{
  "metadata": {
    "title": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
    "source_file": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "summary": "Praise for Designing Machine Learning Systems\nIf you are serious about ML in production, and care about how to design and implement ML systems end to end, this book is essential.\nOne of the best resources that focuses on the first principles behind designing ML systems for production.\nIn a blooming but chaotic ecosystem, this principled view on end-to-end ML is both your map and your compass: a must-read for practitioners inside and outside of Big Tech—especially those working at “reasonable scale.” This book will also appeal to data leaders looking for best practices on how to deploy, manage, and monitor systems in the wild.\nThis is, simply, the very best book you can read about how to build, deploy, and scale machine learning models at a company for maximum impact.\nThis is the book I wish I had read when I started as an ML engineer.\nThe book provides a detailed guide for people building end-to-end machine learning systems.\nChip Huyen has produced an important addition to the canon of machine learning literature—one that is deeply literate in ML fundamentals, but has a much more concrete and practical approach than most.\nThis book will resonate with engineers getting started with ML and with others in any part of the organization trying to understand how ML works.\n—Todd Underwood, Senior Engineering Director for ML SRE, Google, and Coauthor of Reliable Machine Learning\nDesigning Machine Learning Systems An Iterative Process for Production-Ready Applications\nDesigning Machine Learning Systems\nEver since the first machine learning course I taught at Stanford in 2017, many people have asked me for advice on how to deploy ML models at their organizations.\nThese questions can also be specific, such as “I’m convinced that switching from batch prediction to online prediction will give our model a performance boost, but how do I convince my manager to let me do so?” or “I’m the most senior data scientist at my company and I’ve recently been tasked with setting up our first machine learning platform; where do I start?”\nThey are complex because they consist of many different components (ML algorithms, data, business logic, evaluation metrics, underlying infrastructure, etc.) and involve many different stakeholders (data scientists, ML engineers, business leaders, users, even society at large).\nFor example, two companies might be in the same domain (ecommerce) and have the same problem that they want ML to solve (recommender system), but their resulting ML systems can have different model architecture, use different sets of features, be evaluated on different metrics, and bring different returns on investment.\nThis book takes a holistic approach to ML systems.\nWhen I first wrote the lecture notes that laid the foundation for this book, I thought I wrote them for my students to prepare them for the demands of their future jobs as data scientists and ML engineers.\nML in this book refers to both deep learning and classical algorithms, with a leaning toward ML systems at scale, such as those seen at medium to large enterprises and fast-growing startups.",
      "keywords": [
        "Machine Learning Systems",
        "Machine Learning",
        "Designing Machine Learning",
        "Learning Systems",
        "book",
        "effective machine learning",
        "Learning",
        "Machine",
        "Systems",
        "Designing Machine",
        "Chip Huyen",
        "machine learning engineer",
        "machine learning models",
        "Reliable Machine Learning",
        "data"
      ],
      "concepts": [
        "engineer",
        "engineering",
        "learning",
        "designing",
        "machine",
        "models",
        "book",
        "chip",
        "approach",
        "business"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "machine learning",
          "machine",
          "learning",
          "book",
          "ml"
        ],
        "semantic": [],
        "merged": [
          "machine learning",
          "machine",
          "learning",
          "book",
          "ml"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36699361171973877,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.269827+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "summary": "What This Book Is Not\nThis book is not an introduction to ML.\nThere are many books, courses, and resources available for ML theories, and therefore, this book shies away from these concepts to focus on the practical aspects of ML.\nthis book has few code snippets and instead focuses on providing a lot of discussion around trade-offs, pros and cons, and concrete examples.\nNavigating This Book\nThe chapters in this book are organized to reflect the problems data scientists might encounter as they progress through the lifecycle of an ML project.\nChapters 4 to 6 cover the pre-deployment phase of an ML project: from creating the training data and engineering features to developing and evaluating your models in a development environment.\nI debated for a long time on how deep to go into data systems and where to introduce it in the book.\nearly will help us get on the same page to discuss data matters in the rest of the book.\nWhile we cover many technical aspects of an ML system in this book, ML systems are built by people, for people, and can have outsized impact on the life of many.\nIt’d be remiss to write a book on ML production without a chapter on the human side of it, which is the focus of Chapter 11, the last chapter.\nIn this book, we use “data scientist” as an umbrella term to include anyone who works developing and deploying ML models, including people whose job titles might be ML engineers, data engineers, data analysts, etc.\nCode snippets used in this book\nConventions Used in This Book\nThis book is here to help you get your job done.\nIn general, if example code is offered with this book, you may use it in your programs and documentation.\nFor example, writing a program that uses several chunks of code from this book does not require permission.\nSelling or distributing examples from O’Reilly books does require permission.\nIncorporating a significant amount of example code from this book into your product’s documentation does require permission.\nWe have a web page for this book, where we list errata, examples, and any additional information.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\nFor news and information about our books and courses, visit https://oreilly.com.",
      "keywords": [
        "book",
        "data",
        "code",
        "data systems",
        "systems",
        "Chapters",
        "O’Reilly",
        "project",
        "permission",
        "Technical",
        "require permission",
        "Learning",
        "n’t",
        "concepts",
        "tools"
      ],
      "concepts": [
        "book",
        "examples",
        "including",
        "include",
        "technical",
        "learn",
        "deployment",
        "code",
        "coding",
        "permission"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.839,
          "base_score": 0.689,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "permission",
          "require permission",
          "code",
          "does require"
        ],
        "semantic": [],
        "merged": [
          "book",
          "permission",
          "require permission",
          "code",
          "does require"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.385524710604427,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.269908+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 18-25)",
      "start_page": 18,
      "end_page": 25,
      "summary": "This success of deep learning renewed the interest in machine learning (ML) at large.\nMany people, when they hear “machine learning system,” think of just the ML algorithms being used such as logistic regression or different types of neural networks.\nThe first chapter of the book aims to give you an overview of what it takes to bring an ML model to production.\nMachine learning is an approach to (1) learn (2) complex patterns from (3) existing data and use these patterns to make (4) predictions on (5) unseen data.\nA relational database isn’t an ML system because it doesn’t have the capacity to learn.\nFor an ML system to learn, there must be something for it to learn from.\nIn most cases, ML systems learn from data.\nFor example, if you want to build an ML system to learn to predict the rental price for Airbnb listings, you need to provide a dataset where each input is a listing with relevant characteristics (square footage, number of rooms, neighborhood, amenities, rating of that listing, etc.) and the associated output is the rental price of that listing.\nOnce learned, this ML system should be able to predict the price of a new listing given its characteristics.\nML solutions are only useful when there are patterns to learn.\nHowever, there are patterns in how stocks are priced, and therefore companies have invested billions of dollars in building ML systems to learn those patterns.\nInstead of requiring hand-specified patterns to calculate outputs, ML solutions learn patterns from inputs and outputs\nBecause ML learns from data, there must be data for it to learn from.\nIn the zero-shot learning (sometimes known as zero-data learning) context, it’s possible for an ML system to make good predictions for a task without having been trained on data for that task.\nIt’s also possible to launch an ML system without data.\nFor example, in the context of continual learning, ML models can be deployed without having been trained on any data, but they will learn from incoming data\nWithout data and without continual learning, many companies follow a “fake-it-til-you make it” approach: launching a product that serves predictions made by humans, instead of ML models, with the hope of using the generated data to train ML models later.\nML models make predictions, so they can only solve problems that require predictive answers.\nAs predictive machines (e.g., ML models) are becoming more effective, more and more problems are being reframed as predictive problems.\nInstead of computing the exact outcome of a process, which might be even more computationally costly and time-consuming than ML, you can frame the problem as: “What would the outcome of this process look like?” and approximate it using an ML model.\nDue to the way most ML algorithms today learn, ML solutions will especially shine if your problem has these additional following characteristics:\nDespite exciting progress in few-shot learning research, most ML algorithms still require many examples to learn a pattern.",
      "keywords": [
        "system",
        "data",
        "learn",
        "Machine Learning",
        "patterns",
        "n’t",
        "problems",
        "Machine Learning Systems",
        "algorithms",
        "Google Translate",
        "models",
        "systems learn",
        "Machine",
        "solutions",
        "unseen data"
      ],
      "concepts": [
        "learning",
        "predictions",
        "predict",
        "prediction",
        "data",
        "solutions",
        "solution",
        "listings",
        "machine",
        "problems"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "learn",
          "patterns",
          "ml",
          "learning",
          "listing"
        ],
        "semantic": [],
        "merged": [
          "learn",
          "patterns",
          "ml",
          "learning",
          "listing"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35723605966708405,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.269983+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 26-33)",
      "start_page": 26,
      "end_page": 33,
      "summary": "ML solutions often require nontrivial up-front investment on data, compute, infrastructure, and talent, so it’d make sense if we can use these solutions a lot.\nHaving a problem at scale also means that there’s a lot of data for you to collect, which is useful for training ML models.\nBecause ML learns from data, you can update your ML model with new data without having to figure out how the data has changed.\nThe list of use cases can go on and on, and it’ll grow even longer as ML adoption matures in the industry.\nWe’ll go over one case study where the use of ML algorithms can be argued as unethical in the section “Case study I: Automated grader’s biases”.\nFor example, if you can’t build a chatbot to answer all your customers’ queries, it might be possible to build an ML model to predict whether a query matches one of the frequently asked questions.\nML has found increasing usage in both enterprise and consumer applications.\nSince the mid-2010s, there has been an explosion of applications that leverage ML to deliver superior or previously impossible services to consumers.\nWith the explosion of information and services, it would have been very challenging for us to find what we want without the help of ML, manifested in either a search engine or a recommender system.\nTyping on your phone is made easier with predictive typing, an ML system that gives you suggestions on what you might want to say next.\nThe ML use case that drew me into the field was machine translation, automatically translating from one language to another.\nEven though the market for consumer ML applications is booming, the majority of ML use cases are still in the enterprise world.\nEnterprise ML applications tend to have vastly different requirements and considerations from consumer applications.\nAccording to Algorithmia’s 2020 state of enterprise machine learning survey, ML applications in enterprises are diverse, serving both internal use cases (reducing costs, generating customer insights and intelligence, internal processing automation) and external use cases (improving customer experience, retaining customers, interacting with customers) as shown in Figure 1-3.8\nFraud detection is among the oldest applications of ML in the enterprise world.\nBy leveraging ML solutions for anomaly detection, you can have systems that learn from historical fraud transactions and predict whether a future transaction is fraudulent.\nML-based pricing optimization is most suitable for cases with a large number of transactions where demand fluctuates and consumers are willing to pay a dynamic price—for example, internet ads, flight tickets, accommodation bookings, ride-sharing, and events.\nThis can be done through better identifying potential customers, showing better-targeted ads, giving discounts at the right time, etc.—all of which are suitable tasks for ML.\nAn ML system can analyze the ticket content and predict where it should go, which can shorten the response time and improve customer satisfaction.\nAnother popular use case of ML in enterprise is brand monitoring.\nA set of ML use cases that has generated much excitement recently is in health care.\nML in production is very different from ML in research.",
      "keywords": [
        "cases",
        "customer",
        "Machine Learning",
        "system",
        "enterprise",
        "n’t",
        "applications",
        "enterprise machine learning",
        "Learning",
        "prediction",
        "’ll",
        "solutions",
        "problem",
        "Machine Learning Systems",
        "Machine"
      ],
      "concepts": [
        "customers",
        "different",
        "differences",
        "learns",
        "predictions",
        "predicting",
        "prediction",
        "cases",
        "enterprise",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ml",
          "enterprise",
          "applications",
          "customers",
          "customer"
        ],
        "semantic": [],
        "merged": [
          "ml",
          "enterprise",
          "applications",
          "customers",
          "customer"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3765413972629765,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270040+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 34-41)",
      "start_page": 34,
      "end_page": 41,
      "summary": "To edge out a small improvement in performance, researchers often resort to techniques that make models too complex to be useful.\nHaving different, often conflicting, requirements can make it difficult to design, develop, and select an ML model that satisfies all the requirements.\nWant a model that recommends restaurants that users will most likely order from, and they believe they can do so by using a more complex model with more data.\nNotices that every increase in latency leads to a drop in orders through the service, so they want a model that can return the recommended restaurants in less than 100 milliseconds.\n“Recommending the restaurants that users are most likely to click on” and “recommending the restaurants that will bring in the most money for the app” are two different objectives, and in the section “Decoupling objectives”, we’ll discuss how to develop an ML system that satisfies different objectives.\nModel A is the model that recommends the restaurants that users are most likely to click on, and model B is the model that recommends the restaurants that will bring in the most money for the app.\nHowever, if it’s just a nice-to-have requirement, you might still want to consider model A or model B.\nEnsembling combines “multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.” While it can give your ML system a small performance improvement, ensembling tends to make a system too complex to be useful in production, e.g., slower to make predictions or harder to interpret the results.\nDuring the model development process, you might train many different models, and each model does multiple passes over the training data.\nOne corollary of this is that research prioritizes high throughput whereas production prioritizes low latency.\nIn this book, to simplify the discussion and to be consistent with the terminology used in the ML community, we use latency to refer to the response time, so the latency of a request measures the time from when the request is sent to the time a response is received.\nFor example, the average latency of Google Translate is the average time it takes from when a user clicks Translate to when the translation is shown, and the throughput is how many queries it processes and serves a second.\nIf your system always processes one query at a time, higher latency means lower throughput.\nIf the average latency is 10 ms, which means it takes 10 ms to process a query, the throughput is 100 queries/second.\nIf the average latency is 100 ms, the throughput is 10 queries/second.\nHowever, because most modern distributed systems batch queries to process them together, often concurrently, higher latency might also mean higher throughput.\nIf you process 10 queries at a time and it takes 10 ms to run a batch, the average latency is still 10 ms but the throughput is now 10 times higher—1,000 queries/second.\nIf you process 50 queries at a time and it takes 20 ms to run a batch, the average latency now is 20 ms and the throughput is 2,500 queries/second.\nThe difference in latency and throughput trade-off for processing queries one at a time and processing queries in batches is illustrated in Figure 1-4.\nWhen processing queries one at a time, higher latency means lower throughput.\nWhen processing queries in batches, however, higher latency might also mean higher throughput.\nBatching requires your system to wait for enough queries to arrive in a batch before processing them, which further increases latency.\nIn research, you care more about how many samples you can process in a second (throughput) and less about how long it takes for each sample to be processed (latency).\nTo reduce latency in production, you might have to reduce the number of queries you can process on the same hardware at a time.\nIt’s tempting to simplify this distribution by using a single number like the average (arithmetic mean) latency of all the requests within a time window, but this number can be misleading.\nIt’s a common practice to use high percentiles to specify the performance requirements for your system; for example, a product manager might specify that the 90th percentile or 99.9th percentile latency of a system must be below a certain number.",
      "keywords": [
        "model",
        "latency",
        "considered Interpretability",
        "time",
        "queries",
        "Data",
        "throughput",
        "average latency",
        "system",
        "users",
        "Requirements",
        "restaurants",
        "Production",
        "Research",
        "higher latency"
      ],
      "concepts": [
        "latency",
        "latencies",
        "data",
        "requirements",
        "requirement",
        "require",
        "research",
        "production",
        "product",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 37,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.541,
          "base_score": 0.391,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.483,
          "base_score": 0.333,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.459,
          "base_score": 0.309,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "latency",
          "throughput",
          "queries",
          "ms",
          "restaurants"
        ],
        "semantic": [],
        "merged": [
          "latency",
          "throughput",
          "queries",
          "ms",
          "restaurants"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27915387537819364,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270086+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 42-49)",
      "start_page": 42,
      "end_page": 49,
      "summary": "During the research phase, a model is not yet used on people, so it’s easy for researchers to put off fairness as an afterthought: “Let’s try to get state of the art first and worry about fairness when we get to production.” When it gets to production, it’s too late.\nML algorithms don’t predict the future, but encode the past, thus perpetuating the biases in the data and more.\nSince most ML research is still evaluated on a single objective, model performance, researchers aren’t incentivized to work on model interpretability.\nSome might argue that it’s OK to know only the academic side of ML because there are plenty of jobs in research.\nAs ML research and off-the-shelf models become more accessible, more people and organizations would want to find applications for them, which increases the demand for ML in production.\nOn the contrary, ML systems are part code, part data, and part artifacts created from the two.\nInstead of focusing on improving ML algorithms, most companies will focus on improving their data.\nBecause data can change quickly, ML applications need to be adaptive to the changing environment, which might require faster development and deployment cycles.\nWith ML, we have to test and version our data too, and that’s the hard part.\nThe size of ML models is another challenge.\nAs of 2022, it’s common for ML models to have hundreds of millions, if not billions, of parameters, which requires gigabytes of random-access memory (RAM) to load them into memory.\nAs ML models get more complex, coupled with the lack of visibility into their work, it’s hard to figure out what went wrong or be alerted quickly enough when things go wrong.\nThis chapter also highlighted the differences between ML in research and ML in production.\nData scientists and ML engineers working with ML systems in production will likely find that focusing only on the ML algorithms part is far from enough.\nThis book takes a system approach to developing ML systems, which means that we’ll consider all components of a system holistically instead of just looking at ML algorithms.\n1 Mike Schuster, Melvin Johnson, and Nikhil Thorat, “Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System,” Google AI Blog, November 22, 2016, https://oreil.ly/2R1CB.\n5 Andrej Karpathy, “Software 2.0,” Medium, November 11, 2017, https://oreil.ly/yHZrE.\n8 “2020 State of Enterprise Machine Learning,” Algorithmia, 2020, https://oreil.ly/wKMZB.\n2019, by User Action and Operating System,” Statista, 2019, https://oreil.ly/2pTCH.\n13 Marty Swant, “The World’s 20 Most Valuable Brands,” Forbes, 2020, https://oreil.ly/4uS5i.\n14 It’s not unusual for the ML and data science teams to be among the first to go during a\nSee also Sejuti Das’s analysis “How Data Scientists Are Also Susceptible to the Layoffs Amid Crisis,” Analytics India Magazine, May 21, 2020, https://oreil.ly/jobmz.\n“Ensemble learning,” https://oreil.ly/5qkgp.\n16 Julia Evans, “Machine Learning Isn’t Kaggle Competitions,” 2014, https://oreil.ly/p8mZq.\nLeaderboards,” EMNLP, 2020, https://oreil.ly/4Ud8P.\nModels: 6 Lessons Learned at Booking.com,” KDD ’19, August 4–8, 2019, Anchorage, AK, https://oreil.ly/G5QNA.\n22 “Consumer Insights,” Think with Google, https://oreil.ly/JCp6Z.\nFind,” CBS News, November 15, 2019, https://oreil.ly/UiHUB.\non Deep Learning Systems Using Data Poisoning,” arXiv, December 15, 2017, https://oreil.ly/OkAjb.\n33 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv, October 11, 2018, https://oreil.ly/TG3ZW.",
      "keywords": [
        "data",
        "models",
        "Tesla",
        "n’t",
        "production",
        "Andrej Karpathy",
        "systems",
        "applications",
        "Machine Learning",
        "algorithms",
        "Machine Learning Models",
        "research",
        "Machine Learning Systems",
        "Software",
        "Deep Learning Systems"
      ],
      "concepts": [
        "model",
        "data",
        "application",
        "applicants",
        "applications",
        "research",
        "learning",
        "changing",
        "change",
        "algorithms"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "oreil ly",
          "ly",
          "oreil",
          "https oreil",
          "https"
        ],
        "semantic": [],
        "merged": [
          "oreil ly",
          "ly",
          "oreil",
          "https oreil",
          "https"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35850384831547605,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270141+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 50-57)",
      "start_page": 50,
      "end_page": 57,
      "summary": "To reiterate from the first chapter, ML systems design takes a system approach to MLOps, which means that we’ll consider an ML system holistically to ensure that all the components—the business requirements, the data stack, infrastructure, deployment, monitoring, etc.—and their stakeholders can work together to satisfy the specified objectives and requirements.\nIf this system is built for a business, it must be driven by business objectives, which will need to be translated into ML objectives to guide the development of ML models.\nOnce everyone is on board with the objectives for our ML system, we’ll need to set out some requirements to guide the development of this system.\nYou might wonder: with all these objectives, requirements, and processes in place, can I finally start building my ML model yet?\nWe’ll continue this chapter with how to frame your ML problems.\nBusiness and ML Objectives\nWhen working on an ML project, data scientists tend to care about the ML objectives: the metrics they can measure about the performance of their ML models such as accuracy, F1 score, inference latency, etc.\nTheir managers, however, only care about business metrics and, after failing to see how an ML project can help push their business metrics, kill the projects prematurely (and possibly let go of the data science team involved).\nWhat business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users?\nOne of the reasons why predicting ad click-through rates and fraud detection are among the most popular use cases for ML today is that it’s easy to map ML models’ performance to business metrics: every increase in click-through rate results in actual ad revenue, and every fraudulent transaction stopped results in actual money saved.\nMany companies create their own metrics to map business metrics to ML metrics.\nThe effect of an ML project on business objectives can be hard to reason about.\nThe same ML model can also solve their problems faster, which makes them spend less money on your services.\nTo gain a definite answer on the question of how ML metrics influence business metrics, experiments are often needed.\nMany companies do that with experiments like A/B testing and choose the model that leads to better business metrics, regardless of whether this model has better ML metrics.\nYet, even rigorous experiments might not be sufficient to understand the relationship between an ML model’s outputs and business metrics.\nImagine you work for a cybersecurity company that detects and stops security threats, and ML is just a component in their complex process.\nAn ML model is used to detect anomalies in the traffic pattern.\nAccording to a 2020 survey by Algorithmia, among companies that are more sophisticated in their ML adoption (having had models in production for over five years), almost 75% can deploy a model in under 30 days.\nAmong those just getting started with their ML pipeline, 60% take over 30 days to deploy a model (see Figure 2-1).\nHow long it takes for a company to bring a model to production is proportional to how long it has used ML.\nRequirements for ML Systems\nWe’ll discuss how ML systems fail in production in Chapter 8.\nAn ML system might grow in ML model count.\nBecause scalability is such an important topic throughout the ML project workflow, we’ll discuss it in different parts of the book.\nBecause ML systems are part code, part data, and data can change quickly, ML systems need to be able to evolve quickly.",
      "keywords": [
        "system",
        "model",
        "business metrics",
        "business",
        "metrics",
        "data",
        "recommender system",
        "’ll",
        "Systems Design",
        "objectives",
        "requirements",
        "Learning Systems Design",
        "performance",
        "companies",
        "business objectives"
      ],
      "concepts": [
        "data",
        "business",
        "businesses",
        "prediction",
        "predictive",
        "predictions",
        "metrics",
        "process",
        "processes",
        "processing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.837,
          "base_score": 0.687,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "business",
          "business metrics",
          "ml",
          "objectives",
          "metrics"
        ],
        "semantic": [],
        "merged": [
          "business",
          "business metrics",
          "ml",
          "objectives",
          "metrics"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34951482910585735,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270194+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 58-71)",
      "start_page": 58,
      "end_page": 71,
      "summary": "I thought all I had to do was to collect data, train a model, deploy that model, and be done.\nFor example, here is one workflow that you might encounter when building an ML model to predict whether an ad should be shown when users enter a search query:\n7. During error analysis, you realize that your model always predicts that an ad shouldn’t be shown, and the reason is because 99.99% of the data you have have NEGATIVE labels (ads that shouldn’t be shown).\nYour model is now stale, so you need to update it on more recent data.\nFigure 2-2 shows an oversimplified representation of what the iterative process for developing ML systems in production looks like from the perspective of a data scientist or an ML engineer.\nThis process looks different from the perspective of an ML platform engineer or a DevOps engineer, as they might not have as much context into model development and might spend a lot more time on setting up infrastructure.\nA vast majority of ML models today learn from data, so developing ML models starts with engineering data.\nML model development\nWith the initial set of training data, we’ll need to extract features and develop initial models leveraging these features.\nIn Chapter 6, we’ll discuss model selection, training, and evaluation.\nAfter a model is developed, it needs to be made accessible to users.\nWe’ll discuss different ways to deploy an ML model in Chapter 7.\nAn ML problem is defined by inputs, outputs, and the objective function that guides the learning process—none of these three components are obvious from your boss’s request.\nYou can alleviate this bottleneck by developing an ML model to predict which of these four departments a request should go to.\nWe’ll discuss extensively how to extract features from raw data to input into your ML model in Chapter 5.\nIn this section, we’ll focus on two aspects: the output of your model and the objective function that guides the learning process.\nThe output of your model dictates the task type of your ML problem.\nThe most general types of ML tasks are classification and regression.\nClassification models classify inputs into different categories.\nAn example is a house prediction model that outputs the price of a given house.\nA regression model can easily be framed as a classification model and vice versa.\nThe email classification model can become a regression model if we make it output values between 0 and 1, and decide on a threshold to determine which values should be SPAM (for example, if the value is above 0.5, the email is spam), as shown in Figure 2-4.\nWhen there are more than two classes, the problem becomes multiclass classification.\nIn my experience, ML models typically need at least 100 examples for each class to learn to classify that class.\nIn both binary and multiclass classification, each example belongs to exactly one class.\nWhen an example can belong to multiple classes, we have a multilabel classification problem.\nIn multiclass classification, if there are four possible classes [tech, entertainment, finance, politics] and the label for an example is entertainment, you represent this label with the vector [0, 1, 0, 0].\nFor the article classification problem, you can have four models corresponding to four topics, each model outputting whether an article is in that topic or not.\nA naive setup would be to frame this as a multiclass classification task—use the user’s and environment’s features (user demographic information, time, location, previous apps used) as input, and output a probability distribution for every single app on the user’s phone.\nGiven the problem of predicting the app a user will most likely open next, you can frame it as a classification problem.\nIn this framing, for a given user at a given time, there are N predictions to make, one for each app, but each prediction is just a number.\nGiven the problem of predicting the app a user will most likely open next, you can frame it as a regression problem.\nIn this new framing, whenever there’s a new app you want to consider recommending to a user, you simply need to use new inputs with this new app’s feature instead of having to retrain your model or part of your model from scratch.\nTo learn, an ML model needs an objective function to guide the learning process.\nFor supervised ML, this loss can be computed by comparing the model’s outputs with the ground truth labels using a measurement like root mean squared error (RMSE) or cross entropy.\nthis model, given this example, is the cross entropy of [0.45, 0.2, 0.02, 0.33] relative to [0, 0, 0, 1].\nComing up with meaningful objective functions requires algebra knowledge, so most ML engineers just use common loss functions like RMSE or MAE (mean absolute error) for regression, logistic loss (also log loss) for binary classification, and cross entropy for multiclass classification.\nFraming ML problems can be tricky when you want to minimize multiple objective functions.\nYou want to minimize engagement_loss: the difference between each post’s predicted clicks and its actual number of clicks.\nA problem with this approach is that each time you tune α and β—for example, if the quality of your users’ newsfeeds goes up but users’ engagement goes down, you might want to decrease α and increase β— you’ll have to retrain your model.\nAnother approach is to train two different models, each optimizing one loss.\nSo you have two models:\nengagement_model\nMinimizes engagement_loss and outputs the predicted number of clicks of each post\nIn general, when there are multiple objectives, it’s a good idea to decouple them first because it makes model development and maintenance easier.",
      "keywords": [
        "model",
        "classification",
        "data",
        "problem",
        "classification problem",
        "user",
        "multiclass classification",
        "objective function",
        "loss",
        "objective",
        "quality",
        "task",
        "classes",
        "Process",
        "classification task"
      ],
      "concepts": [
        "model",
        "classification",
        "classifications",
        "steps",
        "users",
        "different",
        "difference",
        "labels",
        "problems",
        "predict"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "classification",
          "model",
          "problem",
          "objective",
          "multiclass"
        ],
        "semantic": [],
        "merged": [
          "classification",
          "model",
          "problem",
          "objective",
          "multiclass"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3827485216372241,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270249+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 72-79)",
      "start_page": 72,
      "end_page": 79,
      "summary": "The introduction to his book The Book of Why is entitled “Mind over Data,” in which he emphasizes: “Data is profoundly dumb.” In one of his more controversial posts on Twitter in 2020, he expressed his strong opinion against ML approaches that rely heavily on data and warned that data- centric ML people might be out of a job in three to five years: “ML will not be the same in 3–5 years, and ML folks who continue to follow the current data-centric paradigm will find themselves outdated, if not jobless.\nThe structure allows us to design systems 19 that can learn more from less data.\nMany people in ML today are in the data-over-mind camp.\nWe just have more data.”\nIf you want to use data science, a discipline of which ML is a part of, to improve your products or processes, you need to start with building out your data, both in terms of quality and quantity.\nBoth the research and industry trends in the recent decades show the success of ML relies more and more on the quality and quantity of data.\nWe ended the chapter on a philosophical discussion of the role of data in ML systems.\nHowever, the success of systems including AlexNet, BERT, and GPT showed that the progress of ML in the last decade relies on having access to a large amount of data.\nRegardless of whether data can overpower intelligent design, no one can deny the importance of data in ML.\nNow that we’ve covered the high-level overview of an ML system in production, we’ll zoom in to its building blocks in the following chapters, starting with the fundamentals of data engineering in the next chapter.\n“Pareto optimization,” https://oreil.ly/NdApy. While you’re at it, you might also want to read Jin and Sendhoff’s great paper on applying Pareto optimization for ML, in which the authors claimed that “machine learning is inherently a multiobjective task” (Yaochu Jin and Bernhard Sendhoff, “Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies,” IEEE Transactions on Systems, Man, and Cybernetics—Part C: Applications and Reviews 38, no.\n17 Rich Sutton, “The Bitter Lesson,” March 13, 2019, https://oreil.ly/RhOp9.\n21 Alon Halevy, Peter Norvig, and Fernando Pereira, “The Unreasonable Effectiveness of Data,”\n23 Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson, “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,” arXiv, December 11, 2013, https://oreil.ly/1AdO6.\nLarge data systems, even without ML, are complex.\nIn this chapter, we’ll cover the basics of data engineering that will, hopefully, give you a steady piece of land to stand on as you explore the landscape for your own needs.\nWe’ll start with different sources of data that you might work with in a typical ML project.\nWe’ll continue to discuss data storage engines, also known as databases, for the two major types of processing: transactional and analytical.\nIn the following section of the chapter, we’ll discuss different modes of data passing across processes.\nDuring the discussion of different modes of data passing, we’ll learn about two distinct types of data: historical data in data storage engines, and streaming data in real-time transports.\nThese two different types of data require different processing paradigms, which we’ll discuss in the section “Batch Processing Versus Stream Processing”.\nKnowing how to collect, process, store, retrieve, and process an increasingly growing amount of data is essential to people who want to build ML systems in production.\nIf you’re already familiar with data systems, you might want to move directly to Chapter 4 to learn more about how to sample and generate labels to create training data.\nIf you want to learn more about data engineering from a systems perspective, I recommend Martin Kleppmann’s excellent book Designing Data-Intensive Applications (O’Reilly, 2017).",
      "keywords": [
        "data",
        "systems",
        "data engineering",
        "data science",
        "data systems",
        "Large data systems",
        "’ll",
        "amount of data",
        "business",
        "Data models",
        "Monica Rogati",
        "years",
        "mind versus data",
        "large data",
        "computation"
      ],
      "concepts": [
        "data",
        "processes",
        "process",
        "processing",
        "time",
        "require",
        "requirements",
        "businesses",
        "business",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.839,
          "base_score": 0.689,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ml",
          "systems",
          "data systems",
          "large data",
          "pareto"
        ],
        "semantic": [],
        "merged": [
          "ml",
          "systems",
          "data systems",
          "large data",
          "pareto"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3691371576861989,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270314+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 80-87)",
      "start_page": 80,
      "end_page": 87,
      "summary": "If it’s even remotely possible for users to input wrong data, they are going to do it.\nUser input data requires more heavy-duty checking and processing.\nTherefore, user input data tends to require fast processing.\nThis is the data generated by different components of your systems, which include various types of logs and system outputs such as model predictions.\nBecause logs are system generated, they are much less likely to be malformatted the way user input data is.\nOverall, logs don’t need to be processed as soon as they arrive, the way you would want to process user input data.\nEven though this is system-generated data, it’s still considered part of user data 3 and might be subject to privacy regulations.\nFirst-party data is the data that your company already collects about your users or customers.\nData Formats\nSince your data comes from multiple sources with different access patterns, storing your data isn’t always straightforward and, for some cases, can be costly.\nIt’s important to think about how the data will be used in the future so that the format you use will make sense.\nHow do I store multimodal data, e.g., a sample that might contain both images and texts?\nWhere do I store my data so that it’s cheap and still fast to access?\nThe process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization.\nThere are many, many data serialization formats.\nWhen considering a format to work with, you might want to consider different characteristics such as human readability, access patterns, and whether it’s based on text or binary, which influences the size of its files.\nFor example, your data can be stored in a structured format like the following:\nThe same data can also be stored in an unstructured blob of text like the following:\nOnce you’ve committed the data in your JSON files to a schema, it’s pretty painful to retrospectively go back\nBecause modern computers process sequential data more efficiently than nonsequential data, if a table is row-major, accessing its rows will be faster than accessing its columns in expectation.\nThis means that for row-major formats, accessing data by rows is expected to be faster than accessing data by columns.\nIf we consider each example as a row and each feature as a column, as is often the case in ML, then the row- major formats like CSV are better for accessing examples, e.g., accessing all the examples collected today.\nColumn-major formats like Parquet are better for accessing features, e.g., accessing the timestamps of all your examples.\nColumn-major formats allow flexible column-based reads, especially if your data is large with thousands, if not millions, of features.\nRow-major formats allow faster data writes.\nFor each individual example, it’d be much faster to write it to a file where your data is already in a row-major format.\npandas is built around DataFrame, a concept inspired by R’s Data Frame, which is column- major.\nPeople coming to pandas from NumPy tend to treat DataFrame the way they would ndarray, e.g., trying to access data by rows, and find DataFrame slow.\nI use CSV as an example of the row-major format because it’s popular and generally recognizable by everyone I’ve talked to in tech.\nHowever, some of the early reviewers of this book pointed out that they believe CSV to be a horrible data format.\nCSV and JSON are text files, whereas Parquet files are binary files.\nA program has to know exactly how the data inside the binary file is laid out to make use of the file.\nAs an illustration, I use interviews.csv, which is a CSV file (text format) of 17,654 rows and 10 columns.",
      "keywords": [
        "Data",
        "user input data",
        "text",
        "formats",
        "files",
        "binary",
        "input data",
        "binary files",
        "CSV",
        "user input",
        "text files",
        "Versus Binary Format",
        "logs",
        "Binary Format CSV",
        "JSON"
      ],
      "concepts": [
        "data",
        "formats",
        "text",
        "user",
        "different",
        "access",
        "accessing",
        "logs",
        "log",
        "binary"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 12,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.499,
          "base_score": 0.349,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "",
          "score": 0.492,
          "base_score": 0.342,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 13,
          "title": "",
          "score": 0.483,
          "base_score": 0.333,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.475,
          "base_score": 0.325,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "accessing",
          "files",
          "csv",
          "format",
          "user input"
        ],
        "semantic": [],
        "merged": [
          "accessing",
          "files",
          "csv",
          "format",
          "user input"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25027283190090394,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270361+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 88-98)",
      "start_page": 88,
      "end_page": 98,
      "summary": "Data Models\nData models describe how data is represented.\nThese attributes make up a data model for cars.\nThis is another data model for cars.\nFor example, the way you represent cars in the first data model makes it easier for people looking to buy cars, whereas the second data model makes it easier for police officers to track down criminals.\nRelational Model\nIn this model, data is organized into relations; each relation is a set of tuples.\nData following the relational model is usually stored in file formats like CSV or Parquet.\nYou can join the data from different relations back together, but joining can be expensive for large tables.\nDatabases built around the relational data model are relational databases.\nOnce you’ve put data in your databases, you’ll want a way to retrieve it.\nThe language that you can use to specify the data that you want from a database is called a query language.\nEven though inspired by the relational model, the data model behind SQL has deviated from the original relational model.\ndatabase systems, and normalization means that data is spread out on multiple relations, which makes joining it together even harder.\nYou give the system your data (inputs and outputs) and specify the number of models you want to experiment.\nThe hard part lies in feature engineering, data processing, model evaluation, data shift detection, continual learning, and so on.\nThe relational data model has been able to generalize to a lot of use cases, from ecommerce to finance to social networks.\nThe latest movement against the relational data model is NoSQL.\nOriginally started as a hashtag for a meetup to discuss nonrelational databases, NoSQL has been retroactively reinterpreted as Not Only SQL, Two major types of nonrelational models are the document model and the graph model.\nThe document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare.\nThe graph model goes in the opposite direction, targeting use cases where relationships between data items are common and important.\nas many NoSQL data systems also support relational models.\nDocument model\nFor example, you can convert the book data in Tables 3-3 and 3-4 into three JSON documents as shown in Examples 3-1, 3-2, and 3-3.\n{ \"Title\": \"Harry Potter\", \"Author\": \"J .K. Rowling\", \"Publisher\": \"Banana Press\", \"Country\": \"UK\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$20\"}, {\"Format\": \"E-book\", \"Price\": \"$10\"} ] }\n{ \"Title\": \"Sherlock Holmes\", \"Author\": \"Conan Doyle\", \"Publisher\": \"Guava Press\", \"Country\": \"US\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$30\"}, {\"Format\": \"E-book\", \"Price\": \"$15\"}\n{ \"Title\": \"The Hobbit\", \"Author\": \"J.R.R. Tolkien\", \"Publisher\": \"Banana Press\", \"Country\": \"UK\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$30\"}, ] }\nDocument databases just shift the responsibility of assuming structures from the application that writes the data to the application that reads the data.\nThe document model has better locality than the relational model.\nConsider the book data example in Tables 3-3 and 3-4 where the information about a book is spread across both the Book table and the Publisher table (and potentially also the Format table).\nIn the document model, all information about a book can be stored in a document, making it much easier to retrieve.\nHowever, compared to the relational model, it’s harder and less efficient to execute joins across documents compared to across tables.\nBecause of the different strengths of the document and relational data models, it’s common to use both models for different tasks in the same database systems.\nA database that uses graph structures to store its data is called a graph database.\nIf in document databases, the content of each document is the priority, then in graph databases, the relationships between data items are the priority.\nBecause the relationships are modeled explicitly in graph models, it’s faster to retrieve data based on relationships.\nGiven this graph, you can start from the node USA and traverse the graph following the edges “within” and “born_in” to find all the nodes of the type “person.” Now, imagine that instead of using the graph model to represent this data, we use the relational model.\nMany queries that are easy to do in one data model are harder to do in another data model.\nPicking the right data model for your application can make your life so much easier.\nStructured data follows a predefined data model, also known as a data schema.\nFor example, the data model might specify that each data item consists of two values: the first value, “name,” is a string of at most 50 characters, and the second value, “age,” is an 8-bit integer in the range between 0 and 200.\nOne of the strangest bugs one of my colleagues encountered was when they could no longer use users’ ages with their transactions, and their data schema replaced all the null ages with 0, and their ML model thought the transactions were made by people 0 years old.\nFor example, a text file of logs generated by your ML model is unstructured data.\nFor example, if your storage follows a schema, you can only store data following that schema.\nBut if your storage doesn’t follow a schema, you can store any type of data.",
      "keywords": [
        "Data",
        "data model",
        "model",
        "relational data model",
        "document model",
        "Banana Press",
        "Relational Model",
        "document",
        "Unstructured Data",
        "Banana Press Guava",
        "Doyle Format Paperback"
      ],
      "concepts": [
        "data",
        "models",
        "different",
        "difference",
        "relational",
        "relations",
        "query",
        "queried",
        "queries",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 12,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.595,
          "base_score": 0.445,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "",
          "score": 0.492,
          "base_score": 0.342,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.491,
          "base_score": 0.341,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "document",
          "relational",
          "model",
          "data model",
          "graph"
        ],
        "semantic": [],
        "merged": [
          "document",
          "relational",
          "model",
          "data model",
          "graph"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2495316332886478,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270406+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 99-107)",
      "start_page": 99,
      "end_page": 107,
      "summary": "Data Storage Engines and Processing\nEven though these different transactions involve different types of data, the way they’re processed is similar across applications.\nThis also means that transactional databases might not be efficient for questions such as “What’s the average price for all the rides in September in San Francisco?” This kind of analytical question requires aggregating data in columns across multiple rows of data.\nThis may result in the same data being stored in multiple databases and using different processing engines to solve different types of queries.\nAccording to Wikipedia, online processing means data is immediately available for input/output.\nETL refers to the general purpose processing and aggregating of data into the shape and the format that you want.\nTransform is the meaty part of the process, where most of the data processing is done.\nWhichever application needs data can just pull out raw data from there and process it.” This process of loading data into storage first then processing it later is sometimes called ELT (extract, load, transform).\nThis paradigm allows for the fast arrival of data since there’s little processing needed before data is stored.\nA question arises: how do we pass data between different processes that don’t share memory?\nWhen data is passed from one process to another, we say that the data flows from one process to another, which gives us a dataflow.\nData passing through databases\nData passing through services using requests such as the requests provided by REST and RPC APIs (e.g., POST/GET requests)\nData Passing Through Databases\nThe easiest way to pass data between two processes is through databases, which we’ve discussed in the section “Data Storage Engines and Processing”.\nprocess A to process B, process A can write that data into a database, and process B simply reads from that database.\nSecond, it requires both processes to access data from databases, and read/write from databases can be slow, making it unsuitable for applications with strict latency requirements—e.g., almost all consumer-facing applications.\nData Passing Through Services\nOne way to pass data between two processes is to send data directly through a network that connects these two processes.\nTo pass data from process B to process A, process A first sends a request to process B that specifies the data A needs, and B returns the requested data through the same network.\nThis mode of data passing is tightly coupled with the service-oriented architecture.\nFor B to be able to request data from A, A will also need to be exposed to B as a service.\nBecause the price depends on supply (the available drivers) and demand (the requested rides), the price optimization service needs data from both the driver management and ride management services.\nTheir detailed analysis is beyond the scope of this book, but one major difference is that REST was designed for requests over networks, whereas RPC “tries to make a request to a remote network service look the same as calling a function or method in your programming language.” Because of this, “REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organization, typically within the same data center.”\nIn the last section, we discussed how the price optimization service needs data from the ride and driver management services to predict the optimal price for each ride.\nSimilarly, the ride management service might also want data from the driver management and price optimization services.\nIf we pass data through services as discussed in the previous section, each of these services needs to send requests to the other two services, as shown in Figure 3-8.\nWith only three services, data passing is already getting complicated.\nRequest-driven data passing is synchronous: the target service has to listen to the request for the request to go through.\nIf the price optimization service requests data from the driver management service and the driver management service is down, the price optimization service will keep resending the request until it times out.\nA service that is down can cause all services that require data from it to be down.\nWhat if there’s a broker that coordinates data passing among services?\nInstead of having services request data directly from each other and creating a web of complex interservice data passing, each service only has to communicate with the broker, as shown in Figure 3-9.\nWhichever service wants data from the driver management service can check that broker for the most recent predicted number of drivers.\nTechnically, a database can be a broker—each service can write data to a database and other services that need the data can read from that database.",
      "keywords": [
        "data",
        "Data passing",
        "service",
        "Price optimization service",
        "Driver management service",
        "management service",
        "Processing",
        "databases",
        "process",
        "optimization service",
        "Driver management",
        "price",
        "price optimization",
        "management",
        "ride"
      ],
      "concepts": [
        "service",
        "processing",
        "process",
        "processes",
        "different",
        "difference",
        "transaction",
        "transactions",
        "applications",
        "application"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 13,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.485,
          "base_score": 0.335,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "service",
          "services",
          "price",
          "passing",
          "data passing"
        ],
        "semantic": [],
        "merged": [
          "service",
          "services",
          "price",
          "passing",
          "data passing"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24058655973209644,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270454+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 108-117)",
      "start_page": 108,
      "end_page": 117,
      "summary": "Real-time transports can be thought of as in-memory storage for data passing among services.\nA piece of data broadcast to a real-time transport is called an event.\nHistorical data is often processed in batch jobs—jobs that are kicked off periodically.\nWhen data is processed in batch jobs, we refer to it as batch processing.\nBatch processing has been a research subject for many decades, and companies have come up with distributed systems like MapReduce and Spark to process batch data efficiently.\nWhen you have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that you have streaming data.\nStream processing refers to doing computation on streaming data.\nComputation on streaming data can also be kicked off periodically, but the periods are usually much shorter than the periods for batch jobs (e.g., every five minutes instead of every day).\nComputation on streaming data can also be kicked off whenever the need arises.\nFor example, whenever a user requests a ride, you process your data stream to see what drivers are currently available.\nStream processing, when done right, can give low latency because you can process data as soon as data is generated, without having to first write it into databases.\nWith stream processing, it’s possible to continue computing only the new data each day and joining the new data computation with the older data computation, preventing redundancy.\nYou need infrastructure that allows you to process streaming data as well as batch data and join them together to feed into your ML models.\nTo do computation on data streams, you need a stream computation engine (the way Spark and MapReduce are batch computation engines).\nFor simple streaming computation, you might be able to get away with the built-in stream computation capacity of real-time transports like Apache Kafka, but Kafka stream processing is limited in its ability to deal with various data sources.\nStream processing is more difficult because the data amount is unbounded and the data comes in at variable rates and speeds.\nIn this chapter, we learned it’s important to choose the right format to store our data to make it easier to use the data in the future.\nWe continued the chapter with data storage engines and processing.\nWe studied data storage engines and processing together because traditionally storage is coupled with processing: transactional databases for transactional processing and analytical databases for analytical processing.\nWhen discussing data formats, data models, data storage engines, and processing, data is assumed to be within a process.\nThe most popular mode of data passing for processes is data passing through services.\nA mode of data passing that has become increasingly popular over the last decade is data passing through a real-time transport like Apache Kafka and RabbitMQ.\nAs data in real-time transports have different properties from data in databases, they require different processing techniques, as discussed in the section “Batch Processing Versus Stream Processing”.\nData in databases is often processed in batch jobs and produces static features, whereas data in real-time transports is often processed using stream computation engines and produces dynamic features.\nhttps://oreil.ly/utf7z; Suresh H., “Snowflake Architecture and Key Concepts: A Comprehensive Guide,” Hevo blog, January 18, 2019, https://oreil.ly/GyvKl; Preetam Kumar, “Cutting the Cord: Separating Data from Compute in Your Data Lake with Object Storage,” IBM blog, September 21, 2017, https://oreil.ly/Nd3xD; “The Power of Separating Cloud Compute and Cloud Storage,” Teradata, last accessed April 2022, https://oreil.ly/f82gP.\nTraining Data\nDespite the importance of training data in developing and improving ML models, ML curricula are heavily skewed toward modeling, which is considered by many practitioners the “fun” part of the process.\nBut this is precisely the reason why data scientists and ML engineers should learn how to handle data well, saving us time and headache down the road.\nTraining data, in this chapter, encompasses all the data used in the developing phase of ML models, including the different splits used for training, validation, and testing (the train, validation, test splits).\nThis chapter starts with different sampling techniques to select data for training.\nLike other steps in building ML systems, creating training data is an iterative process.\nAs your model evolves through a project lifecycle, your training data will likely also evolve.\nHistorical data might be embedded with human biases, and ML models, trained on this data, can perpetuate them.\nSampling\nSampling happens in many steps of an ML project lifecycle, such as sampling from all possible real-world data to create training data; sampling from a given dataset to create splits for training, validation, and testing; or sampling from all possible events that happen within your ML system for monitoring purposes.\nIn this section, we’ll focus on sampling methods for creating training data, but these sampling methods can also be used for other steps in an ML project lifecycle.\nOne case is when you don’t have access to all possible data in the real world, the data that you use to train your model is a subset of real-world data, created by one sampling method or another.\nAnother case is when it’s infeasible to process all the data that you have access to—because it requires too much time or resources—so you have to sample that data to create a subset that is feasible to process.\nFor example, when considering a new model, you might want to do a quick experiment with a small subset of your data to see if the new model is promising first before training this new model on all your data.\nUnderstanding different sampling methods and how they are being used in our workflow can, first, help us avoid potential sampling biases, and second, help us choose the methods that improve the efficiency of the data we sample.\nNonprobability sampling is when the selection of data isn’t based on any probability criteria.\nSamples of data are selected based on their availability.\nYou select samples based on quotas for certain slices of data without any randomization.\nThe samples selected by nonprobability criteria are not representative of the real-world data and therefore are riddled with selection biases.\nBecause of these biases, you might think that it’s a bad idea to select data to train ML models using this family of sampling methods.\nUnfortunately, in many cases, the selection of data for ML models is still driven by convenience.\nNonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground.\nIf you randomly select 1% of your data, samples of this rare class will unlikely be selected.\nFor example, to sample 1% of data that has two classes, A and B, you can sample 1% of class A and 1% of class B.\nFor example, if you have three samples, A, B, and C, and want them to be selected with the probabilities of 50%, 30%, and 20% respectively, you can give them the weights 0.5, 0.3, and 0.2.\nFor example, if you know that a certain subpopulation of data, such as more recent data, is more valuable to your model and want it to have a higher chance of being selected, you can give it a higher weight.\nFor example, if in your data, red samples account for 25% and blue samples account for 75%, but you know that in the real world, red and blue have equal probability to happen, you can give red samples weights three times higher than blue samples.\nReservoir sampling is a fascinating algorithm that is especially useful when you have to deal with streaming data, which is usually what you have in production.\nImagine you have an incoming stream of tweets and you want to sample a certain number, k, of tweets to do analysis or train a model on.",
      "keywords": [
        "Data",
        "sampling",
        "Stream Processing",
        "Processing",
        "Batch Processing",
        "training data",
        "Apache Kafka",
        "Data Passing",
        "streaming data",
        "Batch",
        "Stream",
        "data storage engines",
        "Samples",
        "Apache",
        "data storage"
      ],
      "concepts": [
        "data",
        "sampling",
        "sample",
        "processing",
        "process",
        "processes",
        "storage",
        "stream",
        "likely",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 12,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.559,
          "base_score": 0.409,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.492,
          "base_score": 0.492,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.489,
          "base_score": 0.339,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "",
          "score": 0.483,
          "base_score": 0.333,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sampling",
          "stream",
          "processing",
          "computation",
          "real"
        ],
        "semantic": [],
        "merged": [
          "sampling",
          "stream",
          "processing",
          "computation",
          "real"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26974503116768694,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270503+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 118-125)",
      "start_page": 118,
      "end_page": 125,
      "summary": "Labeling\nDespite the promise of unsupervised ML, most ML models in production today are supervised, which means that they need labeled data to learn from.\nThe performance of an ML model still depends heavily on the quality and quantity of the labeled data it’s trained on.\nHe responded: “How long do we need an engineering team for?” Data labeling has gone from being an auxiliary task to being a core function of many ML teams in production.\nWe will then discuss tasks with natural labels, which are tasks where labels can be inferred from the system without requiring human annotations, followed by what to do when natural and hand labels are lacking.\nTo classify whether a comment is spam, you might be able to find 20 annotators on a crowdsourcing platform and train them in 15 minutes to label your data.\nHand labeling means that someone has to look at your data, which isn’t always possible if your data has strict privacy requirements.\nIn many cases, your data might not even be allowed to leave your organization, and you might have to hire or contract annotators to label your data on premises.\nOften, to obtain enough labeled data, companies have to use data from multiple sources and rely on multiple annotators who have different levels of expertise.\nA model trained on data labeled by annotator 1 will perform very differently from a model trained on data labeled by annotator 2.\nYour ML engineers are confident that more data will improve the model performance, so you spend a lot of money to hire annotators to label another million data samples.\nThe reason is that the new million samples were crowdsourced to annotators who labeled data with much less accuracy than the original data.\nIt’s good practice to keep track of the origin of each of your data samples as well as its labels, a technique known as data lineage.\nOn more than one occasion, we’ve discovered that the problem wasn’t with our model, but because of the unusually high number of wrong labels in the data that we’d acquired recently.\nNatural Labels\nTasks with natural labels are tasks where the model’s predictions can be automatically evaluated or\nThe canonical example of tasks with natural labels is recommender systems.\nA recommendation that gets clicked on can be presumed to be good (i.e., the label is POSITIVE) and a recommendation that doesn’t get clicked on after a period of time, say 10 minutes, can be presumed to be bad (i.e., the label is NEGATIVE).\nNatural labels that are inferred from user behaviors like clicks and ratings are also known as behavioral labels.\nEven if your task doesn’t inherently have natural labels, it might be possible to set up your system in a way that allows you to collect some feedback on your model.\nIn a survey of 86 companies in my network, I found that 63% of them work with tasks with natural labels, as shown in Figure 4-3.\nThis doesn’t mean that 63% of tasks that can benefit from ML solutions have natural labels.\nWhat is more likely is that companies find it easier and cheaper to first start on tasks that have natural labels.\nSixty-three percent of companies in my network work on tasks with natural labels.\nIt’s different from explicit labels where users explicitly demonstrate their feedback on a recommendation by giving it a low rating or downvoting it.\nFor tasks with natural ground truth labels, the time it takes from when a prediction is served until when the feedback on it is provided is the feedback loop length.\nTasks with short feedback loops are tasks where labels are generally available within minutes.\nIf you want to extract labels from user feedback, it’s important to note that there are different types of user feedback.\nA short window length means that you can capture labels faster, which allows you to use these labels to detect issues with your model and address those issues as soon as possible.\nHowever, a short window length also means that you might prematurely label a recommendation as bad before it’s clicked on.\nFor tasks with long feedback loops, natural labels might not arrive for weeks or even months.\nLabels with long feedback loops are helpful for reporting a model’s performance on quarterly or yearly business reports.",
      "keywords": [
        "data",
        "labels",
        "Galactic Emperor",
        "natural labels",
        "Galactic Empire",
        "model",
        "feedback",
        "Galactic",
        "tasks",
        "Emperor",
        "natural",
        "n’t",
        "annotators",
        "feedback loops",
        "data samples"
      ],
      "concepts": [
        "labeling",
        "models",
        "annotations",
        "annotators",
        "annotate",
        "annotation",
        "annotating",
        "production",
        "products",
        "differently"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 15,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.421,
          "base_score": 0.421,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.411,
          "base_score": 0.411,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.388,
          "base_score": 0.388,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.387,
          "base_score": 0.387,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "labels",
          "natural",
          "natural labels",
          "feedback",
          "tasks"
        ],
        "semantic": [],
        "merged": [
          "labels",
          "natural",
          "natural labels",
          "feedback",
          "tasks"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25378302057582397,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:10.270554+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 126-134)",
      "start_page": 126,
      "end_page": 134,
      "summary": "No for zero-shot learning Yes for fine-tuning, though the number of ground truths required is often much smaller than what would be needed if you train the model from scratch\nLabels data samples that are most useful to your model\nOne of the most popular open source tools for weak supervision is Snorkel, developed at the Stanford AI Lab. which can be developed with subject matter expertise, to label data.\nMultiple LFs might apply to the same data examples, and they might give conflicting labels.\nIn a study with Stanford Medicine, models trained with weakly supervised labels obtained by a single radiologist after eight hours of writing LFs had comparable performance with models trained on data obtained through almost a year of hand labeling, as shown in Figure 4-5.\nFirst, the models continued improving with more unlabeled data even without more LFs. Second, LFs were being reused across tasks.\nComparison of the performance of a model trained on fully supervised labels (FS) and a model trained with programmatic labels (DP) on CXR and EXR tasks.\nMy students often ask that if heuristics work so well to label data, why do we need ML models?\nOne reason is that LFs might not cover all data samples, so we can train ML models on data programmatically labeled with LFs and use this trained model to generate predictions for samples that aren’t covered by any LF.\nIf weak supervision leverages heuristics to obtain noisy labels, semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels.\nYou start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples.\nAssuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set.\nAnother semi-supervision method assumes that data samples that share similar characteristics share the same labels.\nIn some cases, semi-supervision approaches have reached the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded.\nSemi-supervision is the most useful when the number of training labels is limited.\nOne thing to consider when doing semi-supervision with limited data is how much of this limited data should be used to evaluate multiple candidate models and select the best one.\nOn the other hand, if you use a large amount of data for evaluation, the performance boost gained by selecting the best model based on this evaluation set might be less than the boost gained by adding the evaluation set to the limited training set.\nTransfer learning refers to the family of methods where a model developed for a task is reused as the starting point for a model on a second task.\nFirst, the base model is trained for a base task.\nLanguage modeling is a great candidate because it doesn’t require labeled data.\nIn some cases, such as in zero-shot learning scenarios, you might be able to use the base model on a downstream task directly.\nFine-tuning means making small changes to the base model, such as 19 continuing to train the base model or a part of the base model on data from a given downstream task.\nFor example, to use a language model as the base model for a question answering task, you\nTransfer learning is especially appealing for tasks that don’t have a lot of labeled data.\nEven for tasks that have a lot of labeled data, using a pretrained model as the starting point can often boost the performance significantly compared to training from scratch.\nTransfer learning also lowers the entry barriers into ML, as it helps reduce the up-front cost needed for labeling data to build ML applications.\nActive learning is a method for improving the efficiency of data labels.\nThe hope here is that ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.\nActive learning is sometimes called query learning—though this term is getting increasingly unpopular—because a model (active learner) sends back queries in the form of unlabeled samples to be labeled by annotators (usually humans).\nInstead of randomly labeling data samples, you label the samples that are most helpful to your models according to some metrics or heuristics.\nFor example, in the case of classification problems where your model outputs raw probabilities for different classes, it might choose the data samples with the lowest probabilities for the predicted class.\n(b) A model trained on 30 samples randomly labeled gives an accuracy of 70%.\n(c) A model trained on 30 samples chosen by active learning gives an accuracy of 90%.\nwhich are usually the same model trained with different sets of hyperparameters or the same model trained on different slices of data.\nEach model can make one vote for which samples to label next, and it might vote based on how uncertain it is about the prediction.\nThe samples to be labeled can come from different data regimes.\nstationary distribution where you’ve already collected a lot of unlabeled data and your model chooses samples from this pool to label.\nThey can come from the real-world distribution where you have a stream of data coming in, as in production, and your model chooses samples from this stream of data to label.\nActive learning in this data regime will allow your model to learn more effectively in real time and adapt faster to changing environments.\nClass imbalance typically refers to a problem in classification tasks where there is a substantial difference in the number of samples in each class of the training data.\nClass imbalance can also happen with regression tasks where the labels are continuous.\nML, especially deep learning, works well in situations when the data distribution is more balanced, and usually not so well when the classes are heavily imbalanced, as illustrated in Figure 4-8.\nThe first reason is that class imbalance often means there’s insufficient signal for your model to learn to detect the minority classes.\nIn the case where there is a small number of instances in the minority class, the problem becomes a few-shot learning problem where your model only gets to see the minority class a few times before having to make a decision on it.\nIn the case where there is no instance of the rare classes in your training set, your model might assume these rare classes don’t exist.\nThe second reason is that class imbalance makes it easier for your model to get stuck in a nonoptimal solution by exploiting a simple heuristic instead of learning anything useful about the underlying pattern of the data.\nIf your model learns to always output the majority class, its accuracy is already 99.99%.\nIn this section, we will cover three approaches to handling class imbalance: choosing the right metrics for your problem; data-level methods, which means changing the data distribution to make it less imbalanced; and algorithm-level methods, which means changing your learning method to make it more robust to class imbalance.\nHowever, these are insufficient metrics for tasks with class imbalance because they treat all classes equally, which means the performance of your model on the majority class will dominate these metrics.\nConsider a task with two labels: CANCER (the positive class) and NORMAL (the negative class), where 90% of the labeled data is NORMAL.",
      "keywords": [
        "Class Imbalance",
        "model",
        "data",
        "labels",
        "Imbalance",
        "learning",
        "samples",
        "base model",
        "data samples",
        "Active learning",
        "task",
        "Weak supervision",
        "LFs",
        "model trained",
        "Transfer learning"
      ],
      "concepts": [
        "labels",
        "models",
        "classes",
        "learning",
        "train",
        "sample",
        "sampled",
        "task",
        "predictions"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.648,
          "base_score": 0.498,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 18,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 14,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "class",
          "samples",
          "model",
          "labels",
          "imbalance"
        ],
        "semantic": [],
        "merged": [
          "class",
          "samples",
          "model",
          "labels",
          "imbalance"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2973442439429871,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270615+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 135-142)",
      "start_page": 135,
      "end_page": 142,
      "summary": "The accuracy of model A on the CANCER class is 10% and the accuracy of model B on the CANCER class is 90%.\nF1, precision, and recall are metrics that measure your model’s performance with respect to the positive class in binary classification problems, as they rely on true positive—an outcome where the model correctly predicts the positive class.35\nFor readers needing a refresh, precision, recall, and F1 scores, for binary tasks, are calculated using the count of true positives, true negatives, false positives, and false negatives.\nRecall = True Positive / (True Positive + False Negative)\nF1, precision, and recall are asymmetric metrics, which means that their values change depending on which class is considered the positive class.\nIn our case, if we consider CANCER the positive class, model A’s F1 is 0.17.\nHowever, if we consider NORMAL the positive class, model A’s F1 is 0.95.\nAccuracy, precision, recall, and F1 scores of model A and model B when CANCER is the positive class are shown in Table 4-7.",
      "keywords": [
        "Actual NORMAL Predicted",
        "CANCER Actual NORMAL",
        "NORMAL Predicted CANCER",
        "true positive",
        "positive",
        "Positive Predicted Negative",
        "Actual NORMAL",
        "NORMAL Predicted",
        "Actual CANCER",
        "Predicted CANCER",
        "positive class",
        "CANCER",
        "Model",
        "Predicted"
      ],
      "concepts": [
        "negative",
        "positive",
        "positives",
        "precision",
        "predicted",
        "predictions",
        "classes",
        "metrics",
        "cancer",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 17,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 46,
          "title": "",
          "score": 0.452,
          "base_score": 0.302,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 24,
          "title": "",
          "score": 0.444,
          "base_score": 0.294,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 47,
          "title": "",
          "score": 0.423,
          "base_score": 0.273,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.347,
          "base_score": 0.197,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "positive",
          "cancer",
          "class",
          "positive class",
          "normal"
        ],
        "semantic": [],
        "merged": [
          "positive",
          "cancer",
          "class",
          "positive class",
          "normal"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.15158359514150804,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270670+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 143-150)",
      "start_page": 143,
      "end_page": 150,
      "summary": "Like F1 and recall, the ROC curve focuses only on the positive class and doesn’t show how well your model does on the negative class.\nData-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn.\nResampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes.\nThe simplest way to undersample is to randomly remove instances from the majority class, whereas the simplest way to oversample is to randomly make copies of the minority class until you have a ratio that you’re happy with.\nA popular method of oversampling low-dimensional data is SMOTE (synthetic minority oversampling technique).\nexisting data points within the minority class.\nWhen you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution.\nOversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.\nYou first train your model on the resampled data.\nThis resampled data can be achieved by randomly undersampling large classes until each class has only N instances.\nYou then fine-tune your model on the original data.\nAnother technique is dynamic sampling: oversample the low-performing classes and undersample the high- performing classes during the training process.\nIf data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data, algorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust to class imbalance.\nThe key idea is that if there are two instances, x and x , and the loss resulting from making the wrong prediction on x is higher than x , the model will prioritize making the correct prediction on x over making the correct prediction on x .\nBy giving the training instances we care about higher 2 weight, we can make the model focus more on learning these instances.\nLet L(x;θ) be the loss caused by the instance x for the model with the parameter set θ.\nBack in 2001, based on the insight that misclassification of different classes incurs different costs, Elkan proposed cost-sensitive learning in which the individual loss function is modified to take into account this varying cost.\nThe method started by using a cost matrix to specify C : the cost if class i is classified as class j.\nThe loss caused by instance x of class i will become the weighted average of all possible classifications of instance x.\nWhat might happen with a model trained on an imbalanced dataset is that it’ll bias toward majority classes and make wrong predictions on minority classes.\nWhat if we punish the model for making wrong predictions on minority classes to correct this bias?\nThe loss caused by instance x of class i will become as follows, with Loss(x, j) being the loss when x is classified as class j.\nIn our data, some examples are easier to classify than others, and our model might learn to classify them quickly.\nThe model trained with focal loss (FL) shows reduced loss values compared to the model trained with cross entropy loss (CE).\nTraditionally, these techniques are used for tasks that have limited training data, such as in medical imaging.\nIn computer vision, the simplest data augmentation technique is to randomly modify an image while preserving its label.",
      "keywords": [
        "data",
        "Model",
        "loss",
        "training data",
        "Accuracy Precision Recall",
        "positive",
        "Data Augmentation",
        "loss function",
        "ROC curve",
        "instances",
        "training",
        "positive rate",
        "Accuracy Precision",
        "classes",
        "loss caused"
      ],
      "concepts": [
        "data",
        "classes",
        "sample",
        "sampling",
        "positive",
        "methods",
        "curve",
        "cost",
        "word",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 16,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 47,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 24,
          "title": "",
          "score": 0.48,
          "base_score": 0.33,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 46,
          "title": "",
          "score": 0.413,
          "base_score": 0.263,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.352,
          "base_score": 0.202,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "loss",
          "class",
          "classes",
          "minority",
          "instances"
        ],
        "semantic": [],
        "merged": [
          "loss",
          "class",
          "classes",
          "minority",
          "instances"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.18900412799220476,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270715+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 151-158)",
      "start_page": 151,
      "end_page": 158,
      "summary": "Adding noisy samples to training data can help models recognize the weak spots in their learned decision boundary and improve their performance.\n2 The authors showed that mixup improves models’ generalization, reduces their memorization of corrupt labels, increases their robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\nshowed that by adding images generated using CycleGAN to their original training data, they were able to improve their model’s performance significantly on computed tomography (CT) segmentation tasks.\nMost ML algorithms in use today are supervised ML algorithms, so obtaining labels is an integral part of creating training data.\nWe ended the chapter with a discussion on data augmentation techniques that can be used to improve a model’s performance and generalization for both computer vision and NLP tasks.\nOnce you have your training data, you will want to extract features from it to train your ML models, which we will cover in the next chapter.\n3 Rachel Lerman, “Google Is Testing Its Self-Driving Car in Kirkland,” Seattle Times, February 3, 2016, https://oreil.ly/3IA1V.\n6 “SVM: Weighted Samples,” scikit-learn, https://oreil.ly/BDqbk.\n“Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR Prediction,” arXiv, July 15, 2019, https://oreil.ly/5y2WA.\n3 (2017): 269–82, https://oreil.ly/vFPjk.\n12 Ratner et al., “Snorkel: Rapid Training Data Creation with Weak Supervision.”\n2 (2020): 100019, https://oreil.ly/nKt8E.\n16 Avrim Blum and Tom Mitchell, “Combining Labeled and Unlabeled Data with Co-Training,” in Proceedings of the Eleventh Annual\nConference on Computational Learning Theory (July 1998): 92–100, https://oreil.ly/T79AE.\nLearning Algorithms,” NeurIPS 2018 Proceedings, https://oreil.ly/dRmPV.\nhttps://oreil.ly/DBEbw.\nSurvey of Prompting Methods in Natural Language Processing,” arXiv, July 28, 2021, https://oreil.ly/0lBgn.\nUnderstanding,” arXiv, October 11, 2018, https://oreil.ly/RdIGU; Tom B.\nBrown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al., “Language Models Are Few-Shot Learners,” OpenAI, 2020, https://oreil.ly/YVmrr.\n24 Dana Angluin, “Queries and Concept Learning,” Machine Learning 2 (1988): 319–42, https://oreil.ly/0uKs4.\n29 The Nilson Report, “Payment Card Fraud Losses Reach $27.85 Billion,” PR Newswire, November 21, 2019, https://oreil.ly/NM5zo.\n30 “Job Market Expert Explains Why Only 2% of Job Seekers Get Interviewed,” WebWire, January 7, 2014, https://oreil.ly/UpL8S.\n31 “Email and Spam Data,” Talos Intelligence, last accessed May 2021, https://oreil.ly/lI5Jr.\n32 Nathalie Japkowciz and Shaju Stephen, “The Class Imbalance Problem: A Systematic Study,” 2002, https://oreil.ly/d7lVu.\n33 Nathalie Japkowicz, “The Class Imbalance Problem: Significance and Strategies,” 2000, https://oreil.ly/Ma50Z.\nImbalanced Class Distribution,” 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2017, https://oreil.ly/WeW6J.\nConference on Machine Learning, 2006, https://oreil.ly/s40F3.\n37 Rafael Alencar, “Resampling Strategies for Imbalanced Datasets,” Kaggle, https://oreil.ly/p8Whs.\n448–52, https://oreil.ly/JCxHZ.\n(Workshop on Learning from Imbalanced Datasets II, ICML, Washington, DC, 2003), https://oreil.ly/qnpra; Miroslav Kubat and Stan Matwin, “Addressing the Curse of Imbalanced Training Sets: One-Sided Selection,” 2000, https://oreil.ly/8pheJ.\n42 Hansang Lee, Minseok Park, and Junmo Kim, “Plankton Classification on Imbalanced Large Scale Database via Convolutional Neural Networks with Transfer Learning,” 2016 IEEE International Conference on Image Processing (ICIP), 2016, https://oreil.ly/YiA8p.\nKaseb, Kent Gauen, Ryan Dailey, et al., “Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification,” 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), 2018, https://oreil.ly/D3Ak5.\nProceedings of the Conference on Computer Vision and Pattern, 2019, https://oreil.ly/jCzGH.\nhttps://oreil.ly/Km2dF.\nhttps://oreil.ly/aphzA.\n5 (2019): 828–41, https://oreil.ly/LzN9D.\nhttps://oreil.ly/9v2No; Ian J.\nNetworks,” in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, https://oreil.ly/dYVL8.\nand Semi-Supervised Learning,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, https://oreil.ly/MBQeu.\nhttps://oreil.ly/lIM5E.\n1 (2019): 16884, https://oreil.ly/TDUwm.\nDoesn’t deep learning promise us that we no longer have to engineer features?”\nFigure 5-1 shows an example of classical text processing techniques you can use to handcraft n-gram features for your text.",
      "keywords": [
        "training data",
        "data",
        "Neural networks",
        "learning",
        "Deep Learning",
        "Deep Neural Networks",
        "training",
        "Rapid Training Data",
        "Convolutional Neural Networks",
        "class imbalance",
        "Training Data Creation",
        "data augmentation",
        "labels",
        "features",
        "model"
      ],
      "concepts": [
        "data",
        "learned",
        "label",
        "models",
        "features",
        "samples",
        "sampling",
        "training",
        "generated",
        "generate"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 15,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.511,
          "base_score": 0.361,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ly",
          "oreil",
          "oreil ly",
          "https oreil",
          "https"
        ],
        "semantic": [],
        "merged": [
          "ly",
          "oreil",
          "oreil ly",
          "https oreil",
          "https"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32098050577947007,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270766+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 159-167)",
      "start_page": 159,
      "end_page": 167,
      "summary": "Your model will hopefully learn to extract useful features from this.\nThere are many possible features to use in your model.\nThe process of choosing what information to use and how to extract this information into a format usable by your ML models is feature engineering.\nSome of the possible features about a comment, a thread, or a user to be included in your model\nIn this section, we will discuss several of the most important operations that you might want to consider while engineering features from your data.\nThey include handling missing values, scaling, discretization, encoding categorical features, and generating the old-school but still very effective cross features as well as the newer and exciting positional features.\nHandling Missing Values\nOne of the first things you might notice when dealing with data in production is that some values are missing.\nHowever, one thing that many ML engineers I’ve interviewed don’t know is that not all types of missing values are equal.\nThere are three types of missing values.\nThe income values are missing for reasons related to the values themselves.\nThis is when the reason a value is missing is not due to the value itself, but due to another observed variable.\nIn this example, we might notice that age values are often missing for respondents of the gender “A,” which might be because the people of gender A in this survey don’t like disclosing their age.\nIn this example, we might think that the missing values for the column “Job” might be completely random, not because of the job itself and not because of any other variable.\nWhen encountering missing values, you can either fill in the missing values with certain values (imputation) or remove the missing values (deletion).\nWhen I ask candidates about how to handle missing values during interviews, many tend to prefer deletion, not because it’s a better method, but because it’s easier to do.\nOne way to delete is column deletion: if a variable has too many missing values, just remove that variable.\nFor example, in the example above, over 50% of the values for the variable “Marital status” are missing, so you might be tempted to remove this variable from your model.\nAnother way to delete is row deletion: if a sample has missing value(s), just remove that sample.\nThis method can work when the missing values are completely at random (MCAR) and the number of examples with missing values is small, such as less than 0.1%.\nHowever, removing rows of data can also remove important information that your model needs to make predictions, especially if the missing values are not at random (MNAR).\nFor example, you don’t want to remove samples of gender B respondents with missing income because the fact that income is missing is information itself (missing income might mean higher income, and thus, more correlated to buying a house) and can be used to make predictions.\nOn top of that, removing rows of data can create biases in your model, especially if the missing values are at random (MAR).\nFor example, if you remove all examples missing age values in the data in Table 5-2, you will remove all respondents with gender A from your data, and your model won’t be able to make good predictions for respondents with gender A.\nEven though deletion is tempting because it’s easy to do, deleting data can lead to losing important information and introduce biases into your model.\nIf you don’t want to delete missing values, you will have to impute them, which means “fill them with certain values.” Deciding which “certain values” to use is the hard part.\nOne common practice is to fill in missing values with their defaults.\nFor example, if the temperature value is missing for a data sample whose month value is July, it’s not a bad idea to fill it with the median temperature of July.\nOne time, in one of the projects I was helping with, we discovered that the model was spitting out garbage because the app’s frontend no longer asked users to enter their age, so age values were missing, and the model filled them with 0.\nBut the model never saw the age value of 0 during training, so it couldn’t make reasonable predictions.\nMultiple techniques might be used at the same time or in sequence to handle missing values for a particular set of data.\nRegardless of what techniques you use, one thing is certain: there is no perfect way to handle missing values.\nThe values of the variable Age in our data range from 20 to 40, whereas the values of the variable Annual Income range from 10,000 to 150,000.\nBefore inputting features into models, it’s important to scale them to be similar ranges.\nIf you want your feature to be in an arbitrary range [a, b]—empirically, I find the range [–1, 1] to work better than the range [0, 1]—you can use the following formula: x′= a + (x−min(x))(b−a) max(x)−min(x)\nDuring training, our model has seen the annual income values of “150,000,” “50,000,” “100,000,” and so on.\nThe age variable is discrete, but it might still be useful to group the values into buckets such as follows:\nOne of the features you want to use is the product brand.\nTo address this, you create a category UNKNOWN with the value of 2,000,000 to catch all the brands your model hasn’t seen during training.\nHowever, your model treats them all the same way it treats unpopular brands in the training data.\nFor example, if you want to predict whether a comment is spam, you might want to use the account that posted this comment as a feature, and new accounts are being created all the time.\nThe gist of this trick is that you use a hash function to generate a hashed value of each category.\nBecause you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categories there will be.\nFor example, if you choose a hash space of 18 bits, which corresponds to 2 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143.",
      "keywords": [
        "missing",
        "data",
        "model",
        "features",
        "n’t",
        "brands",
        "income",
        "Feature engineering",
        "data leakage",
        "variable",
        "number",
        "categories",
        "Age",
        "natural language processing",
        "missing income"
      ],
      "concepts": [
        "values",
        "feature",
        "data",
        "model",
        "brand",
        "income",
        "common",
        "hashing",
        "production",
        "products"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 47,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.506,
          "base_score": 0.356,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.504,
          "base_score": 0.354,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "missing",
          "values",
          "missing values",
          "variable",
          "income"
        ],
        "semantic": [],
        "merged": [
          "missing",
          "values",
          "missing values",
          "variable",
          "income"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24843749250443933,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270812+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 168-178)",
      "start_page": 168,
      "end_page": 178,
      "summary": "This technique is useful to model the nonlinear relationships between features.\nIt’s less important in neural networks, but it can still be useful because explicit feature crossing occasionally helps neural networks learn nonlinear relationships faster.\nYou will need a lot more data for models to learn all these possible values.\nAnother caveat is that because feature crossing increases the number of features models use, it can make models overfit to the training data.\nFourier features have been shown to improve models’ performance for tasks that take in coordinates (or positions) as inputs.\nData leakage refers to the phenomenon when a form of the label “leaks” into the set of features used for making predictions, and this same information is not available during inference.\nWhen building models to predict the future stock prices, you want to split your training data by time, such as training your model on data from the first six days and evaluating it on data from the seventh day.\nIf you randomly split your data, prices from the seventh day will be included in your train split and leak into your model the condition of the market on that day.\nTo prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible.\nFor example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test splits as shown in Figure 5-7.\nOne common mistake is to use the entire training data to generate global statistics before splitting it into different splits, leaking the mean and variance of the test samples into the training process, allowing a model to adjust its predictions for the test samples.\nTo avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits.\nData leakage can happen during many steps, from generating, collecting, sampling, splitting, and processing data to feature engineering.\nDo ablation studies to measure how important a feature or a set of features is to your model.\nIf removing a feature causes the model’s performance to deteriorate significantly, investigate why that feature is so important.\nKeep an eye out for new features added to your model.\nIf adding a new feature significantly improves your model’s performance, either that feature is really good or that feature just contains leaked information about labels.\nIf you use the test split in any way other than to report a model’s final performance, whether to come up with ideas for new features or to tune hyperparameters, you risk leaking information from the future into your training process.\nGenerally, adding more features leads to better model performance.\nIn my experience, the list of features used for a model in production only grows over time.\nHowever, more features doesn’t always mean better model performance.\nHaving too many features can be bad both during training and serving your model for the following reasons:\nThe more features you have, the more opportunities there are for data leakage.\nToo many features can increase inference latency when doing online prediction, especially if you need to extract these features from raw data for predictions online.\nIn theory, if a feature doesn’t help a model make good predictions, regularization techniques like L1 regularization should reduce that feature’s weight to 0.\nHowever, in practice, it might help models learn faster if the features that are no longer useful (and even possibly harmful) are removed, prioritizing good features.\nThere are two factors you might want to consider when evaluating whether a feature is good for a model: importance to the model and generalization to unseen data.\nFeature Importance\nThe exact algorithm for feature importance measurement is complex, but intuitively, a feature’s importance to a model is measured by how much that model’s performance deteriorates if that feature or a set of features containing that feature is removed from the model.\nSHAP is great because it not only measures a feature’s importance to an entire model, it also measures each feature’s contribution to a model’s specific prediction.\nFigures 5-8 and 5-9 show how SHAP can help you understand the contribution of each feature to a model’s predictions.\nHow much each feature contributes to a model’s single prediction, measured by SHAP.\nHow much each feature contributes to a model, measured by SHAP.\nOften, a small number of features accounts for a large portion of your model’s feature importance.\nWhen measuring feature importance for a click-through rate prediction model, the ads team at Facebook found out that the top 10 features are responsible for about half of the model’s total feature importance, whereas the last 300 features contribute less than 1% feature importance, as shown in Figure 5-10.\nNot only good for choosing the right features, feature importance techniques are also great for interpretability as they help you understand how your models work under the hood.\nSince the goal of an ML model is to make correct predictions on unseen data, features used for the model should generalize to unseen data.\nFor example, for the task of predicting whether a comment is spam, the identifier of each comment is not generalizable at all and shouldn’t be used as a feature for the model.\nFor example, if you want to build a model to predict whether someone will buy a house in the next 12 months and you think that the number of children someone has will be a good feature, but you can only get this information for 1% of your data, this feature might not be very useful.\nThis rule of thumb is rough because some features can still be useful even if they are missing in most of your data.\nFor example, if a feature appears only in 1% of your data, but 99% of the examples with this feature have POSITIVE labels, this feature is useful and you should use it.\nCoverage of a feature can differ wildly between different slices of data and even in the same slice of data over time.\nIf the coverage of a feature differs a lot between the train and test split (such as it appears in 90% of the examples in the train split but only in 20% of the examples in the test split), this is an indication that your train and test splits don’t come from the same distribution.\nYou might want to investigate whether the way you split your data makes sense and whether this feature is a cause for data leakage.\nIf the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model’s performance.\nYou retrain this model every week, and you want to use the data from the last six days to predict the ETAs (estimated time of arrival) for today.\nIf you include this feature in your model without a clever scheme to encode the days, it won’t generalize to the test split, and might harm your model’s performance.\nOn the other hand, HOUR_OF_THE_DAY is a great feature, because the time in the day affects the traffic too, and the range of values for this feature in the train split overlaps with the test split 100%.\nThe best way to learn is through experience: trying out different features and observing how they affect your models’ performance.\nUse statistics from only the train split, instead of the entire data, to scale your features and handle missing values.\nUnderstand feature importance to your model.\nUse features that generalize well.\nRemove no longer useful features from your models.\nWith a set of good features, we’ll move to the next part of the workflow: training ML models.\nBefore we move on, I just want to reiterate that moving to modeling doesn’t mean we’re done with handling data or feature engineering.\nWe are never done with data and features.\nIn most real-world ML projects, the process of collecting data and feature engineering goes on as long as your models are in production.\nWe need to use new, incoming data to continually improve models, which we’ll cover in Chapter 9.",
      "keywords": [
        "Feature",
        "data",
        "Data Leakage",
        "model",
        "Feature Importance",
        "split",
        "Leakage",
        "test split",
        "embedding",
        "train split",
        "Feature Crossing",
        "Fourier features",
        "feature engineering",
        "importance"
      ],
      "concepts": [
        "feature",
        "data",
        "models",
        "splitting",
        "split",
        "useful",
        "uses",
        "learn",
        "predicting",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 32,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.501,
          "base_score": 0.351,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "",
          "score": 0.498,
          "base_score": 0.348,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.481,
          "base_score": 0.331,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "split",
          "features",
          "model",
          "feature importance"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "split",
          "features",
          "model",
          "feature importance"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24353093912019355,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270854+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 179-186)",
      "start_page": 179,
      "end_page": 186,
      "summary": "Model Development and Offline Evaluation\nIn Chapter 4, we discussed how to create training data for your model, and in Chapter 5, we discussed how to engineer features from that training data.\nThe section that follows discusses different aspects of model development, such as debugging, experiment tracking and versioning, distributed training, and AutoML.\nI expect that most readers already have an understanding of common ML algorithms such as linear models, decision trees, k-nearest neighbors, and different types of neural networks.\nModel Development and Training\nIn this section, we’ll discuss necessary aspects to help you develop and train your model, including how to evaluate different ML models for your problem, creating ensembles of models, experiment tracking and versioning, and distributed training, which is necessary for the scale at which models today are usually trained at.\nWe’ll end this section with the more advanced topic of AutoML—using ML to automatically choose a model best for your problem.\nEvaluating ML Models\nA k-means clustering model might be used to extract features to input into a neural network.\nVice versa, a pretrained neural network (like BERT or GPT-3) might be used to generate embeddings to input into a logistic regression model.\nFor example, if your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem—given a piece of text, classify whether it’s toxic or not—and common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants.\nWhen considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability.\nFor example, a simple logistic regression model might have lower accuracy than a complex neural network, but it requires less labeled data to start, it’s much faster to train, it’s much easier to deploy, and it’s also much easier to explain why it’s making certain predictions.\nWhile helping companies as well as recent graduates get started in ML, I usually have to spend a nontrivial amount of time steering them away from jumping straight into state-of-the-art models.\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets.\nIt doesn’t even mean that this model will perform better than other models on your data.\nIf there’s a solution that can solve your problem that is much cheaper and simpler than state-of-the-art models, use the simpler solution.\nFor example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer.\nImagine an engineer on your team is assigned the task of evaluating which model is better for your problem: a gradient-boosted tree or a pretrained BERT model.\nPart of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture.\nIf an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture.\nBecause the performance of a model architecture depends heavily on the context it’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters, etc.—it’s extremely difficult to make claims that a model architecture is better than another architecture.\nFor example, a tree-based model might work better now because you don’t have a ton of data yet, but two months from now, you might be able to double your amount of training data, and your neural network might perform much better.\nA simple way to estimate how your model’s performance might change with more data is to use learning curves.\nA learning curve of a model is a plot of its performance—e.g., training loss, training accuracy, validation accuracy—against the number of training samples it uses, as shown in Figure 6-1.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations.\nThe team decided to deploy both the collaborative filtering model and the simple neural network.\nThey used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data.\nAfter two weeks, the simple neural network was able to outperform the collaborative filtering model.\nUnderstanding what’s more important in the performance of your ML system will help you choose the most suitable model.\nIn a task where false positives are more dangerous than false negatives, such as fingerprint unlocking (unauthorized people shouldn’t be classified as authorized and given access), you might prefer a model that makes fewer false positives.\nCOVID-19 shouldn’t be classified as no COVID-19), you might prefer a model that makes fewer false negatives.\nAnother example of trade-off is compute requirement and accuracy—a more complex model might deliver higher accuracy but might require a more powerful machine, such as a GPU instead of a CPU, to generate predictions with acceptable inference latency.\nA more complex model can give a better performance, but its results are less interpretable.\nUnderstanding what assumptions a model makes and whether our data satisfies those assumptions can help you evaluate which model works best for your use case.\nEvery model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\nEvery generative model makes the assumption that it’s tractable to compute the probability P(Z|X).\nWhen considering an ML solution to your problem, you might want to start with a system that contains just one model (the process of selecting one model for your problem was discussed earlier in the chapter).\nOne method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions.\nthe task of predicting whether an email is SPAM or NOT SPAM, you might have three different models.",
      "keywords": [
        "Rachel Bogardus Drew",
        "model",
        "Rachel Bogardus",
        "Bogardus Drew",
        "Facts About Marriage",
        "Marriage and Homeownership",
        "Harvard University",
        "Center for Housing",
        "Housing Studies",
        "Studies of Harvard",
        "Neural Network",
        "algorithms",
        "Neural",
        "Data",
        "BERT model"
      ],
      "concepts": [
        "models",
        "prediction",
        "predicting",
        "predictions",
        "feature",
        "solutions",
        "solution",
        "data",
        "evaluation",
        "evaluate"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.604,
          "base_score": 0.454,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.576,
          "base_score": 0.576,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "neural",
          "neural network",
          "network",
          "models"
        ],
        "semantic": [],
        "merged": [
          "model",
          "neural",
          "neural network",
          "network",
          "models"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3658801792473866,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270906+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 187-194)",
      "start_page": 187,
      "end_page": 194,
      "summary": "Therefore, it’s common to choose very different types of models for an ensemble.\nFor example, you might create an ensemble that consists of one transformer model, one recurrent neural network, and one gradient-boosted tree.\nGiven a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps.\nFor example, if 10 classifiers vote SPAM and 6 models vote NOT SPAM, the final prediction is SPAM.\nAn example of a boosting algorithm is a gradient boosting machine (GBM), which produces a prediction model typically from weak decision trees.\nStacking means that you train base learners from the training data then create a meta-learner that combines the outputs of the base learners to output final predictions, as shown in Figure 6-5.\nDuring the model development process, you often have to experiment with many architectures and many different models to choose the best one for your problem.\nAn artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process.\nComparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.\nA large part of training an ML model is babysitting the learning processes.\nIt’s important to track what’s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful.\nThe speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.\nThe values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer), especially if you’re clipping your gradient norms; and weight norm, especially if you’re doing weight decay.\nML systems are part code, part data, so you need to not only version your code but your data as well.\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses data version X to train model A and developer 2 uses data version Y to train model B, it doesn’t make sense to merge data versions X and Y to create Z, since there’s no model corresponding with Z.\nThird, if you use user data to train your model, regulations like General Data Protection Regulation (GDPR) might make versioning this data complicated.\nThe way we have to run so many experiments right now to find the best possible model is the result of us treating ML as a black box.\nHowever, I hope that as the field progresses, we’ll gain more understanding into different models and can reason about what model will work best instead of running hundreds or thousands of experiments.\nDEBUGGING ML MODELS\nML models aren’t an exception.\nDebugging is never fun, and debugging ML models can be especially frustrating for the following three reasons.\nHowever, when making changes to an ML model, you might have to retrain the model and wait until it converges to see whether the bug is fixed, which can take hours.\nThird, debugging ML models is hard because of their cross-functional complexity.\nHere are some of the things that might cause an ML model to fail:\nAs discussed previously, each model comes with its own assumptions about the data and the features it uses.\nA model might fail because the data it learns from doesn’t conform to its assumptions.\nFor example, you use a linear model for the data whose decision boundaries aren’t linear.\nThe model might be a good fit for the data, but the bugs are in the implementation of the model.\nThe model is a great fit for your data, and its implementation is correct, but a poor set of hyperparameters might render your model useless.\nThere are many things that could go wrong in data collection and preprocessing that might cause your models to perform poorly, such as data samples and labels being incorrectly paired, noisy labels, features normalized using outdated statistics, and more.\nToo many features might cause your models to overfit to the training data or cause data leakage.\nToo few features might lack predictive power to allow your models to make good predictions.\nHaving the discipline to follow both the best practices and the debugging procedure is crucial in developing, implementing, and deploying ML models.\nStart with the simplest model and then slowly add more components to see if it helps or hurts the performance.\nCurrently, many people start out by cloning an open source implementation of a state-of-the-art model and plugging in their own data.\nBut if it doesn’t, it’s very hard to debug the system because the problem could have been caused by any of the many components in the model.\nAfter you have a simple implementation of your model, try to overfit a small amount of training data and run evaluation on the same data to make sure that it gets to the smallest possible loss.\nThere are so many factors that contribute to the randomness of your model: weight initialization, dropout, data shuffling, etc.\nRandomness makes it hard to compare results across different experiments —you have no idea if the change in performance is due to a change in the model or a different random seed.\nIt’s common to train a model using data that doesn’t fit into memory.\nIt can also happen with text data if you work for teams that train large language models (cue OpenAI, Google, NVIDIA, Cohere).",
      "keywords": [
        "model",
        "models Probability Ensemble",
        "data",
        "Experiment",
        "Ensemble",
        "Experiment Tracking",
        "models Probability",
        "Probability Ensemble",
        "training",
        "n’t",
        "Tracking",
        "Versioning",
        "code",
        "bagging",
        "training data"
      ],
      "concepts": [
        "models",
        "data",
        "experiment",
        "experiments",
        "version",
        "versions",
        "training",
        "weight",
        "ensemble",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "",
          "score": 0.529,
          "base_score": 0.379,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.524,
          "base_score": 0.374,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.483,
          "base_score": 0.333,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "debugging",
          "models",
          "ensemble",
          "train"
        ],
        "semantic": [],
        "merged": [
          "model",
          "debugging",
          "models",
          "ensemble",
          "train"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29439296935833487,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.270970+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 195-202)",
      "start_page": 195,
      "end_page": 202,
      "summary": "data is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization.\nAccording to the authors of the open source package gradient- checkpointing, “For feed-forward models we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time.” Even when a sample fits into memory, using checkpointing can allow you to fit more samples into a batch, which might allow you to train your model faster.\nIt’s now the norm to train ML models on multiple machines.\nThe most common parallelization method supported by modern ML frameworks is data parallelism: you split your data on multiple machines, train your model on all of them, and accumulate gradients.\nAs each machine produces its own gradient, if your model waits for all of them to finish a run—synchronous stochastic gradient descent (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.\nIf your model updates the weight using the gradient from each machine separately—asynchronous SGD— gradient staleness might become a problem because the gradients from one machine have caused the weights to change before the gradients from another machine have come in.\nAnother problem is that spreading your model on multiple machines can cause your batch size to be very big.\nModel parallelism\nModel parallelism is when different components of your model are trained on different machines, as shown in Figure 6-7.\nData parallelism and model parallelism.\nModel parallelism can be misleading because in some cases parallelism doesn’t mean that different parts of the model in different machines are executed in parallel.\nFor example, if your model is a massive matrix and the matrix is split into two halves on two machines, then these two halves might be executed in parallel.\nHowever, if your model is a neural network and you put the first layer on machine 1 and the second layer on machine 2, and layer 2 needs outputs from layer 1 to execute, then machine 2 has to wait for machine 1 to finish first to run.\nPipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel.\nInstead of paying a group of 100 ML researchers/engineers to fiddle with various models and eventually select a suboptimal one, why not use that money on compute to search for the optimal model?\nWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset.\nshowed in their 2018 paper “On the State of the Art of Evaluation in Neural Language Models” that weaker models with well-tuned hyperparameters can outperform stronger, fancier models.\ngoal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space—the performance of each set evaluated on a validation set.\nPopular methods example, scikit-learn with auto-sklearn, for hyperparameter tuning include random search, AutoML: Methods, Systems, Challenges by the AutoML group at the University of Freiburg dedicates its first chapter (which you can read online for free) to hyperparameter optimization.\nChoose the best set of hyperparameters for a model based on its performance on a validation split, then report the model’s final performance on the test split.\nIf you use your test split to tune hyperparameters, you risk overfitting your model to the test split.\nThis area of research is known as architectural search, or neural architecture search (NAS) for neural networks, as it searches for the optimal model architecture.\nIn a typical ML training process, you have a model and then a learning procedure, an algorithm that helps your model find the set of parameters that minimize a given objective function for a given set of data.\nThe most common learning procedure for neural networks today is gradient descent, which leverages an optimizer to specify how to update a model’s weights given gradient updates.\nFirst, the resulting architectures and learned optimizers can allow ML algorithms to work off-the-shelf on multiple real-world tasks, saving production time and cost, during both training and inferencing.\nFOUR PHASES OF ML MODEL DEVELOPMENT\nBefore we transition to model training, let’s take a look at the four phases of ML model development.\nSimplest machine learning models\nFor your first ML model, you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data.\nOptimizing simple models\nOnce you have your ML framework in place, you can focus on optimizing the simple ML models with different objective functions, hyperparameter search, feature engineering, more data, and ensembles.\nOne common but quite difficult question I often encounter when helping companies with their ML strategies is: “How do I know that our ML models are any good?” In one case, a company deployed ML to detect intrusions to 100 surveillance drones, but they had no way of measuring how many intrusions their system failed to detect, and they couldn’t decide if one ML algorithm was better than another for their needs.\nFor other tasks, you might not be able to evaluate your model’s performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system’s performance.\nIn this section, we’ll discuss methods to evaluate your model’s performance before it’s deployed.\nWe’ll start with the baselines against which we will evaluate our models.\nTable 6-2 shows the F1 and accuracy scores of baseline models making predictions at random.",
      "keywords": [
        "model",
        "machine",
        "Model parallelism",
        "SGD",
        "gradient",
        "Hyperparameter",
        "parallelism",
        "asynchronous SGD",
        "machine learning models",
        "Hyperparameter tuning",
        "batch size",
        "data",
        "machine learning",
        "search",
        "Data parallelism"
      ],
      "concepts": [
        "models",
        "machine",
        "gradient",
        "learning",
        "optimization",
        "optimal",
        "automl",
        "architecture",
        "architectural",
        "layers"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.618,
          "base_score": 0.468,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 39,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "parallelism",
          "gradient",
          "machines",
          "machine"
        ],
        "semantic": [],
        "merged": [
          "model",
          "parallelism",
          "gradient",
          "machines",
          "machine"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3381495266784031,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271030+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 203-210)",
      "start_page": 203,
      "end_page": 210,
      "summary": "The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.\nFor example, for the task of recommending the app a user is most likely to use next on their phone, the simplest model would be to recommend their most frequently used app.\nIn many cases, the goal of ML is to automate what would have been otherwise done by humans, so it’s useful to know how your model performs compared to human experts.\nYour ML model doesn’t always have to be better than existing solutions to be useful.\nHowever, in production, we also want our models to be robust, fair, calibrated, and overall make sense.\nHowever, when they deployed it to actual users, this model’s predictions were close to random.\nTo get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model’s performance.\nThe more sensitive your model is to noise, the harder it will be to maintain it, since if your users’ behaviors change just slightly, such as they change their phones, your model’s performance might degrade.\nModel calibration\nIf a model predicts that team A will beat team B with a 70% probability, and out of the 1,000 times these two teams play together, team A only wins 60% of the time, then we say that this model isn’t calibrated.\nA calibrated model should predict that team A wins with a 60% probability.\nModel calibration is often overlooked by ML practitioners, but it’s one of the most important properties of any predictive system.\nSecond, consider the task of building a model to predict how likely it is that a user will click on an ad.\nYour model predicts that this user will click on ad A with a 10% probability and on ad B with an 8% probability.\nYou don’t need your model to be calibrated to rank ad A above ad B.\nneed your model to be calibrated.\nIf your model predicts that a user will click on ad A with a 10% probability but in reality the ad is only clicked on 5% of the time, your estimated number of clicks will be way off.\nTo measure a model’s calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y.\nIndiscriminately showing all a model’s predictions to users, even the predictions that the model is unsure about, can, at best, cause annoyance and make users lose trust in the system, such as an activity detection system on your smartwatch that thinks you’re running even though you’re just walking a bit fast.\nIf you only want to show the predictions that your model is certain about, how do you measure that certainty?\nSlicing means to separate your data into subsets and look at your model’s performance on each subset separately.\nOne is that their model performs differently on different slices of data when the model should perform the same.\nModel B achieves 95% accuracy on the majority and 95% on the minority, which means its overall accuracy is 95%.\nModel A\nModel B\nAnother problem is that their model performs the same on different slices of data when the model should perform differently.\nFor example, when you build a model for user churn prediction (predicting when a user will cancel a subscription or a service), paid users are more critical than nonpaid users.\nThis means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately.",
      "keywords": [
        "model",
        "distribution Predicting NEGATIVE",
        "Random distribution Meaning",
        "Simple heuristic Forget",
        "system",
        "distribution Meaning",
        "Model calibration",
        "data",
        "performance",
        "Uniform random Predicting",
        "Accuracy Uniform random",
        "model predicts",
        "n’t",
        "heuristic Forget",
        "model performs"
      ],
      "concepts": [
        "model",
        "user",
        "predicting",
        "prediction",
        "data",
        "calibrated",
        "calibration",
        "calibrate",
        "changes",
        "human"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.545,
          "base_score": 0.395,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 47,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.52,
          "base_score": 0.37,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.498,
          "base_score": 0.348,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "calibrated",
          "ad",
          "model performs",
          "model predicts"
        ],
        "semantic": [],
        "merged": [
          "model",
          "calibrated",
          "ad",
          "model performs",
          "model predicts"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28570191226729724,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271081+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 211-218)",
      "start_page": 211,
      "end_page": 218,
      "summary": "Model A\nModel B\nTo make informed decisions regarding what model to choose, we need to take into account its performance not only on the entire data, but also on individual slices.\nSlice-based evaluation can give you insights to improve your model’s performance both overall and on critical data and help detect potential biases.\nThere has been research to systemize the process of finding slices, including Chung et al.’s “Slice Finder: Automated Data Slicing for Model Validation” in 2019 and covered in Sumyea Helal’s “Subgroup Discovery Algorithms: A Survey and Empirical Evaluation” (2016).\nWith the initial models, we can bring to life (in the form of predictions) all our hard work in data and feature engineering, and can finally evaluate our hypothesis (i.e., we can predict the outputs given the inputs).\nAs models today are getting bigger and consuming more data, distributed training is becoming an essential skill for ML model developers, and we discussed techniques for parallelism including data parallelism, model parallelism, and pipeline parallelism.\nIn the next chapter, we’ll go over how to deploy a model.\nWinner Solution-Gilberto Titericz and Stanislav Semenov,” Kaggle, https://oreil.ly/z5od8).\n4 (July 2012): 463–84, https://oreil.ly/ZBlgE; G.\n5 Leo Breiman, “Bagging Predictors,” Machine Learning 24 (1996): 123–40, https://oreil.ly/adzJu.\n6 “Machine Learning Challenge Winning Solutions,” https://oreil.ly/YjS8d.\nhttps://oreil.ly/ysBYO.\n“External memory algorithm,” https://oreil.ly/apv5m).\nPradeep Dubey, “Distributed Deep Learning Using Synchronous Stochastic Gradient Descent,” arXiv, February 22, 2016, https://oreil.ly/ma8Y6.\nJoseph, Randy Katz, and Ion Stoica, “Improving MapReduce Performance in Heterogeneous Environments,” 8th USENIX Symposium on Operating Systems Design and Implementation, https://oreil.ly/FWswd; Aaron Harlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R.\nDeep Networks,” NIPS 2012, https://oreil.ly/EWPun.\n17 Jim Dowling, “Distributed TensorFlow,” O’Reilly Media, December 19, 2017, https://oreil.ly/VYlOP.\nDescent,” 2011, https://oreil.ly/sAEbv.\nModels Are Few-Shot Learners,” arXiv, May 28, 2020, https://oreil.ly/qjg2S.\n20 Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team, “An Empirical Model of Large-Batch Training,” arXiv, December 14, 2018, https://oreil.ly/mcjbV; Christopher J.\nDahl, “Measuring the Effects of Data Parallelism on Neural Network Training,” Journal of Machine Learning Research 20 (2019): 1–49, https://oreil.ly/YAEOM.\nMicro-Batch Pipeline Parallelism,” arXiv, July 25, 2019, https://oreil.ly/wehkx.\nhttps://oreil.ly/5vEsH; “Debate About Science at Organizations like Google Brain/FAIR/DeepMind,” Reddit, https://oreil.ly/2K77r; “Grad Student Descent,” Science Dryad, January 25, 2014, https://oreil.ly/dIR9r; and Guy Zyskind (@GuyZys), “Grad Student Descent: the preferred #nonlinear #optimization technique #machinelearning,” Twitter, April 27, 2015, https://oreil.ly/SW1or.\nLe, “Neural Architecture Search with Reinforcement Learning,” arXiv, November 5, 2016, https://oreil.ly/FhsuQ; Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V.\nLe, “Regularized Evolution for Image Classifier Architecture Search,” AAAI 2019, https://oreil.ly/FWYjn. 29 You can make the search space continuous to allow differentiation, but the resulting architecture has to be converted into a discrete\nTraining More Effective Learned Optimizers, and Using Them to Train Themselves,” arXiv, September 23, 2020, https://oreil.ly/IH7eT.\nMay 29, 2019, https://oreil.ly/gonEn.\n33 Samantha Murphy, “The Evolution of Facebook News Feed,” Mashable, March 12, 2013, https://oreil.ly/1HMXh.\n34 Iveta Ryšavá, “What Mark Zuckerberg’s News Feed Looked Like in 2006,” Newsfeed.org, January 14, 2016, https://oreil.ly/XZT6Q.\n35 Martin Zinkevich, “Rules of Machine Learning: Best Practices for ML Engineering,” Google, 2019, https://oreil.ly/YtEsN.\nhttps://oreil.ly/VYG2j.\nIn Chapters 4 through 6, we have discussed the considerations for developing an ML model, from creating training data, extracting features, and developing the model to crafting metrics to evaluate this model.\nThese considerations constitute the logic of the model—instructions on how to go from raw data into an ML model, as shown in Figure 7-1.\nDifferent aspects that make up the ML model logic\nIn this chapter, we’ll discuss another part in the iterative process: deploying your model.",
      "keywords": [
        "Model",
        "Extracorporeal Shockwave Lithotripsy",
        "Data",
        "kidney stone treatment",
        "stone treatment study",
        "Machine Learning",
        "Clinical Research Edition",
        "British Medical Journal",
        "Machine Learning Research",
        "learning",
        "Group",
        "slices",
        "training",
        "training data",
        "evaluation"
      ],
      "concepts": [
        "model",
        "data",
        "learning",
        "train",
        "slices",
        "slicing",
        "architecture",
        "differently",
        "likely",
        "search"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.538,
          "base_score": 0.538,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.524,
          "base_score": 0.524,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.485,
          "base_score": 0.485,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.48,
          "base_score": 0.48,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.468,
          "base_score": 0.468,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ly",
          "oreil ly",
          "https oreil",
          "oreil",
          "https"
        ],
        "semantic": [],
        "merged": [
          "ly",
          "oreil ly",
          "https oreil",
          "oreil",
          "https"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3709642843497971,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:10.271134+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 219-228)",
      "start_page": 219,
      "end_page": 228,
      "summary": "If you want to deploy a model for your friends to play with, all you have to do is to wrap your predict function in a POST request endpoint using Flask or FastAPI, put the dependencies this predict function needs to run in a container, and push your model and its associated container to a cloud service like AWS or GCP to expose the endpoint:\nThe hard parts include making your model available to millions of users with a latency of milliseconds and 99% uptime, setting up the infrastructure so that the right person can be immediately notified when something goes wrong, figuring out what went wrong, and seamlessly deploying the updates to fix what’s wrong.\nIn many companies, the responsibility of deploying models falls into the hands of the same people who developed those models.\nIn many other companies, once a model is ready to be deployed, it will be exported and handed off to another team to deploy it.\nIn this chapter, we’ll start off with some common myths about ML deployment that I’ve often heard from people who haven’t deployed ML models.\nWe’ll then discuss the two main ways a model generates and serves its predictions to users: online prediction and batch prediction.\nHow a model serves and computes the predictions influences how it should be designed, the infrastructure it requires, and the behaviors that users encounter.\nAs discussed in Chapter 1, deploying an ML model can be very different from deploying a traditional software program.\nThis difference might cause people who have never deployed a model before to either dread the process or underestimate how much time and effort it will take.\nMyth 1: You Only Deploy One or Two ML Models at a Time\nMany people from academic backgrounds I’ve talked to tend to also think of ML production in the context of a single model.\nIn reality, companies have many, many ML models.\nIt needs a model to predict each of the following elements: ride demand, driver availability, estimated time of arrival, dynamic pricing, fraudulent transaction, customer churn, and more.\nWhile many companies still only update their models once a month, or even once a quarter, Weibo’s iteration cycle for updating some of their ML I’ve heard similar numbers at companies like models is 10 minutes.\nOne fundamental decision you’ll have to make that will affect both your end users and developers working on your system is how it generates and serves its predictions to end users: online or batch.\nBatch prediction, which uses only batch features.\nOnline prediction that uses only batch features (e.g., precomputed embeddings).\nOnline prediction that uses both batch features and streaming features.\nTo avoid this confusion, people sometimes prefer the terms “synchronous prediction” and “asynchronous prediction.” However, this distinction isn’t perfect either, because when online prediction leverages a real-time transport to send prediction requests to your model, the requests and predictions technically are asynchronous.\nFigure 7-4 shows a simplified architecture for batch prediction, and Figure 7-5 shows a simplified version of online prediction using only batch features.\nA simplified architecture for online prediction that uses only batch features\nIn batch prediction, only batch features are used.\nprediction, however, it’s possible to use both batch features and streaming features.\nOnline features are more general, as they refer to any feature used for online prediction, including batch features stored in memory.\nA very common type of batch feature used for online prediction, especially session- based recommendations, is item embeddings.\nItem embeddings are usually precomputed in batch and fetched whenever they are needed for online prediction.\nA simplified architecture for online prediction that uses both streaming features and batch features is shown in Figure 7-6.\nSome companies call this kind of prediction “streaming prediction” to distinguish it from the kind of online prediction that doesn’t use streaming features.\nA simplified architecture for online prediction that uses both batch features and streaming features",
      "keywords": [
        "online prediction",
        "prediction",
        "batch prediction",
        "batch features",
        "features",
        "batch",
        "model",
        "online",
        "streaming features",
        "streaming",
        "ONLINE FEATURES",
        "n’t",
        "data",
        "’ll",
        "people"
      ],
      "concepts": [
        "predict",
        "prediction",
        "predictions",
        "models",
        "deploying",
        "likely",
        "features",
        "online",
        "mean",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.75,
          "base_score": 0.6,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.561,
          "base_score": 0.561,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "batch features",
          "batch",
          "prediction",
          "features",
          "online"
        ],
        "semantic": [],
        "merged": [
          "batch features",
          "batch",
          "prediction",
          "features",
          "online"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38161753184638925,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271190+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 229-236)",
      "start_page": 229,
      "end_page": 236,
      "summary": "T a b le 7 - 1 .\nS o m e k e y d if fe r e n c e s b et w e e n b a tc\nh p r e d ic ti o n a n d o n li n e p r e d ic ti o n\nUseful for\nProcessing accumulated data when you don’t need immediate results (such as recommender systems)\nIn many applications, online prediction and batch prediction are used side by side for different use cases.\nFor example, food ordering apps like DoorDash and UberEats use batch prediction to generate restaurant recommendations—it’d take too long to generate these recommendations online because there are many restaurants.\nImagine you run an app where only 2% of your users log in daily—e.g., in 2020, Grubhub had 31 million users and 622,000 daily orders.\nTo people coming to ML from an academic background, the more natural way to serve predictions is probably online.\nThis is likely how most people interact with their models while prototyping.\nThis is also likely easier to do for most companies when first deploying a model.\nYou export your model, upload the exported model to Amazon SageMaker or Google App Engine, and get back an exposed endpoint.\na request that contains an input to that endpoint, it will send back a prediction generated on that input.\nThis limitation can be seen even in more technologically progressive companies like Netflix.\nSay you’ve been watching a lot of horror movies lately, so when you first log in to Netflix, horror movies dominate recommendations.\nNetflix should learn and show you more comedy in your list of their recommendations, right?\nBeing able to detect a fraudulent transaction that happened three hours ago is still better than not detecting it at all, but being able to detect it in real time can prevent the fraudulent transaction from going through.\nA feature you might want to use is the average speed of all the cars in your path in the last five minutes.",
      "keywords": [
        "Batch prediction",
        "Online prediction",
        "prediction",
        "Batch",
        "generate predictions",
        "Online",
        "generate",
        "pipeline",
        "Pipeline Batch prediction",
        "model",
        "n’t",
        "batch pipeline",
        "data",
        "Processing",
        "Stream"
      ],
      "concepts": [
        "predictions",
        "predict",
        "likely",
        "stream",
        "useful",
        "companies",
        "data",
        "pipeline",
        "recommendations",
        "recommending"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 5,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 37,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prediction",
          "batch",
          "recommendations",
          "batch prediction",
          "online"
        ],
        "semantic": [],
        "merged": [
          "prediction",
          "batch",
          "recommendations",
          "batch prediction",
          "online"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37250028492184445,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271245+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 237-244)",
      "start_page": 237,
      "end_page": 244,
      "summary": "Model Compression\nIf the model you want to deploy takes too long to generate predictions, there are three main approaches to reduce its inference latency: make it do inference faster, make the model smaller, or make the hardware it’s deployed on run faster.\nHere, we’ll discuss model compression.\nReaders interested in a comprehensive review might want to check out Cheng et al.’s “Survey of Model Compression and Acceleration for Deep Neural Networks,” which was updated in 2020.19\nHowever, it tends to be specific to certain types of models (e.g., compact convolutional filters are specific to convolutional neural networks) and requires a lot of architectural knowledge to design, so it’s not widely applicable to many use cases yet.\nKnowledge distillation is a method in which a small model (student) is trained to mimic a larger model or ensemble of models (teacher).\nOne example of a distilled network used in production is DistilBERT, which reduces the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster.\nIf you use a pretrained model as the teacher model, training the student network will require less data and will likely be faster.\nOne is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters.\nThis helps with reducing the size of a model because pruning makes a neural network more sparse, and sparse architecture tends to require less storage space than dense structure.\nExperiments show that pruning techniques can reduce the nonzero parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising overall accuracy.\ninto your model.\nIn some cases, pruning can be useful as an architecture search paradigm, and the pruned architecture should be retrained from scratch as a dense model.\nQuantization is the most general and commonly used model compression method.\nQuantization reduces a model’s size by using fewer bits to represent its parameters.\nIf a model has 100M parameters and each requires 32 bits to store, it’ll take up 400 MB.\nThis method is also known as “fixed point.” In the extreme case, some have attempted the 1-bit representation of each weight (binary weight neural networks), e.g., BinaryConnect and XNOR-Net. The authors of the XNOR-Net paper spun off Xnor.ai, a startup that focused on model compression.\nQuantization can either happen during training (quantization aware training), where models are trained in single-precision floating point and then quantized for inference.\nUsing quantization during training means that you can use less memory for each parameter, which allows you to train larger models on the same hardware.\nLatency improvement by various model compression methods.",
      "keywords": [
        "Model Compression",
        "Model",
        "Compression",
        "’ll",
        "pruning",
        "number",
        "parameters",
        "quantization",
        "Neural Networks",
        "inference",
        "’ll discuss",
        "network",
        "training",
        "Model Compression Open",
        "Neural"
      ],
      "concepts": [
        "model",
        "trained",
        "bits",
        "bit",
        "pruning",
        "parameter",
        "inference",
        "performance",
        "perform",
        "architectural"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.511,
          "base_score": 0.361,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model compression",
          "compression",
          "model",
          "quantization",
          "neural"
        ],
        "semantic": [],
        "merged": [
          "model compression",
          "compression",
          "model",
          "quantization",
          "neural"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30066274240305413,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271304+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 245-256)",
      "start_page": 245,
      "end_page": 256,
      "summary": "Another decision you’ll want to consider is where your model’s computation will happen: on the cloud or on the edge.\nCloud services have done an incredible job to make it easy for companies to bring ML models into production.\nML models can be compute-intensive, and compute is expensive.\nEdge computing allows your models to work in situations where there are no internet connections or where the connections are unreliable, such as in rural areas or developing countries.\nRequiring data transfer over the network (sending data to the model on the cloud to make predictions then sending predictions back to the users) might make some use cases impossible.\nTo move computation to the edge, the edge devices have to be powerful enough to handle the computation, have enough memory to store ML models and load them into memory, as well as have enough battery or be connected to an energy source to power the application for a reasonable amount of time.\nBecause of the many benefits that edge computing has over cloud computing, companies are in a race to develop edge devices optimized for different ML use cases.\nWith so many new offerings for hardware to run ML models on, one question arises: how do we make our model run on arbitrary hardware efficiently?\nIn the following section, we’ll discuss how to compile and optimize a model to run it on a certain hardware backend.\nCompiling and Optimizing Models for Edge Devices\nFor a model built with a certain framework, such as TensorFlow or PyTorch, to run on a hardware backend, that framework has to be supported by the hardware vendor.\nMapping from ML workloads to a hardware backend requires understanding and taking advantage of that hardware’s design, and different hardware backends have different memory layouts and compute primitives, as shown in Figure 7-11.\nDeploying ML models to new hardware requires significant manual effort.\nFrom the original code for a model, compilers generate a series of high- and low-level IRs before generating the code native to a hardware backend so that it can run on that hardware backend, as shown in Figure 7-12.\nA series of high- and low-level IRs between the original model code to machine code that can run on a given hardware backend\nHigh-level IRs are usually computation graphs of your ML models.\nAfter you’ve “lowered” your code to run your models into the hardware of your choice, an issue you might run into is performance.\nIn many companies, what usually happens is that data scientists and ML engineers develop models that seem to be working fine in development.\nHowever, when these models are deployed, they turn out to be too slow, so their companies hire optimization engineers to optimize their models for the hardware their models run on.\nThis vision comes together in the AI Engineering team, where our expertise is used to develop AI algorithms and models that are optimized for our hardware, as well as to provide guidance to Mythic’s hardware and compiler teams.\nIn the process of lowering ML model code into machine code, compilers can look at the computation graph of your ML model and the operators it consists of—convolution, loops, cross-entropy—and find a way to speed it up.\nThere are two ways to optimize your ML models: locally and globally.\nLocally is when you optimize an operator or a set of operators of your model.\nThere are standard local optimization techniques that are known to speed up your model, most of them making things run in parallel or reducing memory access on chips.\nnetwork with the computation graph can be fused vertically or horizontally to reduce memory access and speed up the model, as shown in Figure 7-14.\nUsing ML to optimize ML models\nTraditionally, framework and hardware vendors hire optimization engineers who, based on their experience, come up with heuristics on how to best execute the computation graph of a model.\nThis is complicated by the fact that model optimization is dependent on the operators its computation graph consists of.\nHardware vendors like NVIDIA and Google focus on optimizing popular models like ResNet-50 and BERT for their hardware.\nIf you don’t have ideas for good heuristics, one possible solution might be to try all possible ways to execute a computation graph, record the time they need to run, then pick the best one.\nThe pro of this approach is that because the model is trained using the data generated during runtime, it can adapt to any type of hardware it runs on.\nYou optimize your model once for one hardware backend then run it on multiple devices of that same hardware type.\nThis sort of optimization is ideal when you have a model ready for production and target hardware to run inference on.\nWe’ve been talking about how compilers can help us generate machine- native code run models on certain hardware backends.",
      "keywords": [
        "hardware",
        "model",
        "computation graph",
        "run",
        "hardware backend",
        "computation",
        "Edge",
        "Cloud",
        "code",
        "graph",
        "data",
        "edge devices",
        "edge computing",
        "devices",
        "companies"
      ],
      "concepts": [
        "hardware",
        "models",
        "optimized",
        "optimize",
        "optimizing",
        "data",
        "compute",
        "computations",
        "computing",
        "cloud"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 39,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hardware",
          "backend",
          "hardware backend",
          "run",
          "computation"
        ],
        "semantic": [],
        "merged": [
          "hardware",
          "backend",
          "hardware backend",
          "run",
          "computation"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32817762346721924,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271355+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 257-264)",
      "start_page": 257,
      "end_page": 264,
      "summary": "We’ve discussed different ways to deploy a model, comparing online prediction with batch prediction, and ML on the edge with ML on the cloud.\nfor One Large ML Pipeline,” Google, 2020, video, 19:06, https://oreil.ly/HjQm0.\nLearning Models: 6 Lessons Learned at Booking.com,” KDD ’19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (July 2019): 1743–51, https://oreil.ly/Ea1Ke.\n9 “2021 Enterprise Trends in Machine Learning,” Algorithmia, https://oreil.ly/9kdcw.\n14 “Developer Survey Results,” Stack Overflow, 2019, https://oreil.ly/guYIq.\n18 Shuyi Chean and Fabian Hueske, “Streaming SQL to Unify Batch & Stream Processing w/ Apache Flink @Uber,” InfoQ, https://oreil.ly/XoaNu; Yu, “Machine Learning with Flink in Weibo.”\n19 Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang, “A Survey of Model Compression and Acceleration for Deep Neural Networks,” arXiv, June 14, 2020, https://oreil.ly/1eMho. 20 Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman, “Speeding up Convolutional Neural\nNetworks with Low Rank Expansions,” arXiv, May 15, 2014, https://oreil.ly/4Vf4s.\nDally, and Kurt Keutzer, “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size,” arXiv, November 4, 2016, https://oreil.ly/xs3mi.\nHoward, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam, “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,” arXiv, April 17, 2017, https://oreil.ly/T84fD.\nNetwork,” arXiv, March 9, 2015, https://oreil.ly/OJEPW.\nVersion of BERT: Smaller, Faster, Cheaper and Lighter,” arXiv, October 2, 2019, https://oreil.ly/mQWBv.\nTrainable Neural Networks,” ICLR 2019, https://oreil.ly/ychdl.\nState of Neural Network Pruning?” arXiv, March 6, 2020, https://oreil.ly/VQsC3.\nValue of Network Pruning,” arXiv, March 5, 2019, https://oreil.ly/mB4IZ.\nfor Model Compression,” arXiv, November 13, 2017, https://oreil.ly/KBRjy.\nDeep Neural Networks with Binary Weights During Propagations,” arXiv, November 2, 2015, https://oreil.ly/Fwp2G; Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi, “XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,” arXiv, August 2, 2016, https://oreil.ly/gr3Ay.\nVahid Noroozi, and Ravi Gadde, “Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq,” NVIDIA Devblogs, October 9, 2018, https://oreil.ly/WDT1l.\nTPUs,” Google Cloud Blog, August 23, 2019, https://oreil.ly/ZG5p0.\nActivations,” Journal of Machine Learning Research 18 (2018): 1–30; Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko, “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,” arXiv, December 15, 2017, https://oreil.ly/sUuMT.\nCPUs,” Roblox, May 27, 2020, https://oreil.ly/U01Uj.\nBills,” The Information, February 25, 2019, https://oreil.ly/H9ans; Mats Bauer, “How Much Does Netflix Pay Amazon Web Services Each Month?” Quora, 2020, https://oreil.ly/HtrBk.\n38 “2021 State of Cloud Cost Report,” Anodot, https://oreil.ly/5ZIJK.\n39 “Burnt $72K Testing Firebase and Cloud Run and Almost Went Bankrupt,” Hacker News, December 10, 2020, https://oreil.ly/vsHHC; “How to Burn the Most Money with a Single Click in Azure,” Hacker News, March 29, 2020, https://oreil.ly/QvCiI.\n2025,” Statista, https://oreil.ly/BChLN.\n43 Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, et al., “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning,” arXiv, February 12, 2018, https://oreil.ly/vGnkW.\n48 Shashank Prasanna, Prethvi Kashinkunti, and Fausto Milletari, “TensorRT 3: Faster TensorFlow Inference and Volta Support,” NVIDIA Developer, December 4, 2017, https://oreil.ly/d9h98.\n52 Can I Use _____?, https://oreil.ly/slI05.\nNative Code,” USENIX, https://oreil.ly/uVzrX.\nWe’ll start by covering reasons why ML models that perform great during development fail in production.\nThen, we’ll take a deep dive into one especially prevalent and thorny issue that affects almost all ML models in production: data distribution shifts.\nIn the next chapter, we’ll cover how to continually update your models in production to adapt to shifts in data distributions.",
      "keywords": [
        "Neural Networks",
        "Machine Learning Models",
        "Machine Learning",
        "Convolutional Neural Networks",
        "model",
        "Deep Neural Networks",
        "Machine Learning Systems",
        "Training Neural Networks",
        "Learning",
        "Neural",
        "Neural Network Pruning",
        "WASM",
        "System",
        "Learning Models",
        "Networks"
      ],
      "concepts": [
        "data",
        "models",
        "learning",
        "companies",
        "company",
        "cloud",
        "training",
        "power",
        "powerful",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.75,
          "base_score": 0.6,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.643,
          "base_score": 0.643,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.618,
          "base_score": 0.468,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "https oreil",
          "ly",
          "oreil",
          "oreil ly",
          "https"
        ],
        "semantic": [],
        "merged": [
          "https oreil",
          "ly",
          "oreil",
          "oreil ly",
          "https"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39754842671972657,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271410+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 265-272)",
      "start_page": 265,
      "end_page": 272,
      "summary": "performance expectation violations are harder to detect as doing so requires measuring and monitoring the performance of ML models in production.\nTo effectively detect and fix ML system failures in production, it’s useful to understand why a model, after proving to work well during development, would fail in production.\nThey reviewed data from over the previous 15 years to determine the causes and found out that 60 out of these 96 failures happened due to causes not directly related to ML.\nExamples include data collection and processing problems, poor hyperparameters, changes in the training pipeline not correctly replicated in the inference pipeline and vice versa, data distribution shifts that cause a model’s performance to deteriorate over time, edge cases, and degenerate feedback loops.\nIn this chapter, we’ll discuss three new but very common problems that arise after a model has been deployed: production data differing from training data, edge cases, and degenerate feedback loops.\nWhen we say that an ML model learns from the training data, it means that the model learns the underlying distribution of the training data with the goal of leveraging this learned distribution to generate accurate predictions for unseen data—data that it didn’t see during training.\nWhen the model is able to generate accurate predictions for unseen data, 6 we say that this model “generalizes to unseen data.” The test data that we use to evaluate a model during development is supposed to represent unseen data, and the model’s performance on the test data is supposed to give us an idea of how well the model will generalize.\nOne of the first things I learned in ML courses is that it’s essential for the training data and the unseen data to come from a similar distribution.\nIf the unseen data comes from a different distribution, the model might not generalize well.\nCurating a training dataset that can accurately represent the data that a model will encounter in production turns out to be very difficult.\nThere are many different selection and sampling biases, as discussed in Chapter 4, that can happen and make real-world data diverge from training data.\nThis type of divergence leads to a common failure mode known as the train-serving skew: a model that does great in development but performs poorly when deployed.\nAnother common failure mode is that a model does great when first deployed, but its performance degrades over time as the data distribution changes.\nDue to the complexity of ML systems and the poor practices in deploying them, a large percentage of what might look like data shifts on monitoring dashboards are caused by internal errors, such as bugs in the data pipeline,\nmissing values incorrectly inputted, inconsistencies between the features extracted during training and inference, features standardized using statistics from the wrong subset of data, wrong model version, or bugs in the app interface that force users to change their behaviors.\nAn ML model that performs well on most cases but fails on a small number of cases might not be usable if these failures cause catastrophic consequences.\nEdge cases are the data samples so extreme that they cause the model to make catastrophic mistakes.\nEven though edge cases generally refer to data samples drawn from the same distribution, if there is a sudden increase in the number of data samples in which your model doesn’t perform well, it could be an indication that the underlying data distribution has shifted.\nEdge cases refer to performance: an example where a model performs significantly worse than other examples.\nAn outlier can cause a model to perform unusually poorly, which makes it an edge case.\nIn many cases, it might be beneficial to remove outliers as it helps your model to learn better decision boundaries and generalize better to unseen data.\nA degenerate feedback loop can happen when the predictions themselves influence the feedback, which, in turn, influences the next iteration of the model.\nIn ML, a system’s predictions can influence how users interact with the system, and because users’ interactions with the system are sometimes used as training data to the same system, degenerate feedback loops can occur and cause unintended consequences.\nDegenerate feedback loops are especially common in tasks with natural labels from users, such as recommender systems and ads click-through-rate prediction.\nIn the beginning, the rankings of two songs, A and B, might be only marginally different, but because A was originally ranked a bit higher, it showed up higher in the recommendation list, making users click on A more, which made the system rank A even Degenerate feedback loops are one reason why higher.\nThe model finds that feature X accurately predicts whether someone is qualified, so it recommends resumes with feature X.\nthe importance of each feature for the model, as discussed in Chapter 5—can help detect the bias toward feature X in this case.\nLeft unattended, degenerate feedback loops can cause your model to perform suboptimally at best.\nDegenerate loops result from user feedback, and a system won’t have users until it’s online (i.e., deployed to users).\nFor the task of recommender systems, it’s possible to detect degenerate feedback loops by measuring the popularity diversity of a system’s outputs even when the system is offline.\nWe’ve discussed that degenerate feedback loops can cause a system’s outputs to be more homogeneous over time.\nIn the case of recommender systems, instead of showing the users only the items that the system ranks highly for them, we show users random items and use their feedback to determine the true quality of these items.\nWe’ve also discussed that degenerate feedback loops are caused by users’ feedback on predictions, and users’ feedback on a prediction is biased based on where it is shown.\nYou are unsure whether your model is exceptionally good at picking the top song, or whether users click on any song as long as it’s recommended on top.\nThis feature allows your model to learn how much being a top recommendation influences how likely a song is clicked on.\nDuring inference, you want to predict whether a user will click on a song regardless of where the song is recommended, so you might want to set the 1st Position feature to be False.\nThen you look at the model’s predictions for various songs for each user and can choose the order in which to show each song.\nThe first model predicts the probability that the user will see and consider a recommendation taking into account the position at which that recommendation will be shown.\nThe second model then predicts the probability that the user will click on the item given that they saw and considered it.\nIn this section, we’ll zero in onto one especially sticky cause of failures: data distribution shifts, or data shifts for short.\nData distribution shift refers to the phenomenon in supervised learning when the data a model works with changes over time, which causes this model’s predictions to become less accurate as time passes.\nThe distribution of the data the model is trained on is called the source distribution.\nThe distribution of the data the model runs inference on is called the target distribution.",
      "keywords": [
        "degenerate feedback loops",
        "data",
        "software system failures",
        "model",
        "system",
        "data distribution shifts",
        "system failures",
        "degenerate feedback",
        "feedback loops",
        "data distribution",
        "training data",
        "failures",
        "feedback",
        "distribution",
        "software system"
      ],
      "concepts": [
        "data",
        "model",
        "users",
        "distributed",
        "distribution",
        "distributions",
        "failures",
        "recommend",
        "recommendations",
        "items"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 18,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feedback",
          "degenerate",
          "degenerate feedback",
          "loops",
          "feedback loops"
        ],
        "semantic": [],
        "merged": [
          "feedback",
          "degenerate",
          "degenerate feedback",
          "loops",
          "feedback loops"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36780259908413904,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271464+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 273-280)",
      "start_page": 273,
      "end_page": 280,
      "summary": "Even though discussions around data distribution shift have only become common in recent years with the growing adoption of ML in the industry, data distribution shift in systems that learned from data has been studied as early as in 1986.\nTypes of Data Distribution Shifts\nWhile data distribution shift is often used interchangeably with concept drift and covariate shift and occasionally label shift, these are three distinct subtypes of data shift.\nNote that this discussion on different types of data shifts is math-heavy and mostly useful from a research perspective: to develop efficient algorithms to detect and address data shifts requires understanding the causes of those shifts.\nIn production, when encountering a distribution shift, data scientists don’t usually stop to wonder what type of shift it is.\nWe know that in supervised learning, the training data can be viewed as a set of samples from the joint distribution P(X, Y), and then ML usually models P(Y|X).\nCovariate shift is one of the most widely studied forms of data distribution shift.\nMathematically, covariate shift is when P(X) changes, but P(Y|X) remains the same, which means that the distribution of the input changes, but the conditional probability of an output given an input remains the same.\nYou know that the risk of breast cancer is higher for women over the age of 40, over the age of 40 in your training data than in your inference data, so the input distributions differ for your training and inference data.\nDuring model development, covariate shifts can happen due to biases during the data selection process, which could result from difficulty in collecting examples for certain classes.\nCovariate shifts can also happen because the training data is artificially altered to make it easier for your model to learn.\nAs discussed in Chapter 4, it’s hard for ML models to learn from imbalanced datasets, so you might want to collect more samples of the rare classes or oversample your data on the rare classes to make it easier for your model to learn the rare classes.\nCovariate shift can also be caused by the model’s learning process, especially through active learning.\nThis means that the training input distribution is altered by the learning process to differ from the real-world input distribution, and covariate shifts are a by-product.\nThe input distribution into your model has changed, but the probability that a user with a given income level will convert remains the same.\nIf you know in advance how the real-world input distribution will differ from your training input distribution, you can leverage techniques such as importance weighting to train your model to work for the real-world data.\nImportance weighting consists of two steps: estimate the density ratio between the real-world input distribution and the training input distribution, then weight the training data according to this ratio and train an ML model on this weighted data.\nHowever, because we don’t know in advance how the distribution will change in the real world, it’s very difficult to preemptively train your models to make them robust to new, unknown distributions.\nThere has been research that attempts to help models learn representations of latent variables that are invariant across data distributions, but I’m not aware of their adoption in the industry.\nLabel shift, also known as prior shift, prior probability shift, or target shift, is when P(Y) changes but P(X|Y) remains the same.\nRemember that covariate shift is when the input distribution changes.\nWhen the input distribution changes, the output distribution also changes, resulting in both covariate shift and label shift happening at the same time.\nThe probability P(Y|X) reduces for women of all ages, so it’s no longer a case of covariate shift.\nHowever, given a person with breast cancer, the age distribution remains the same, so this is still a case of label shift.\nBecause label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts are similar to covariate shift adaptation methods.\nConcept drift, also known as posterior shift, is when the input distribution remains the same but the conditional distribution of the output given an input changes.\nGeneral Data Distribution Shifts\nWith label shift, P(Y) changes but P(X|Y) remains the same.\nWhen the number of classes changes, your model’s structure might change, both relabel your data and retrain your model from scratch.\nDetecting Data Distribution Shifts\nData distribution shifts are only a problem if they cause your model’s performance to degrade.\nIn research, there have been efforts to understand and detect label shifts without labels from the target distribution.\nHowever, in the industry, most drift detection methods focus on detecting changes in the input distribution, especially the distributions of features, as we discuss in detail in this chapter.\nAs of October 2021, even TensorFlow Extended’s built-in data validation tools use only summary statistics to detect the skew between the training and serving data and shifts between different days of training data.\nIf those metrics differ significantly, the inference distribution might have shifted from the training distribution.\nIf you consider the data from yesterday to be the source population and the data from today to be the target population and they are statistically different, it’s likely that the underlying data distribution has shifted between yesterday and today.\nIf your model’s predictions and labels are one-dimensional (scalar numbers), then the KS test is useful to detect label or prediction shifts.\nFor example, shifts happen at different rates, and abrupt changes are easier to detect than slow, gradual changes.\nTo detect temporal shifts, a common approach is to treat input data to ML applications as time-series data.\nWhen dealing with temporal shifts, the time scale window of the data we look at affects the shifts we can detect.\nIf we use data from day 9 to day 14 as the source distribution, then day 15 looks like a shift.\nAs of today, many companies use the distribution of the training data as the base distribution and monitor the production data distribution at a certain granularity level, such as hourly and daily.\nwindow, the faster you’ll be able to detect changes in your data distribution.\nMore advanced monitoring platforms even attempt a root cause analysis (RCA) feature that automatically analyzes statistics across various time window sizes to detect exactly the time window where a change in data happened.\nAddressing Data Distribution Shifts\nAt one end of the spectrum, we have companies that have just started with ML and are still working on getting ML models into production, so they might not have gotten to the point where data shifts are catastrophic to them.\nThey will then need to adapt their models to the shifted distributions or to replace them with other solutions.\nAt the same time, many companies assume that data shifts are inevitable, so they periodically retrain their models —once a month, once a week, or once a day—regardless of the extent of the shift.\nThe hope here is that if the training dataset is large enough, the model will be able to learn such a comprehensive distribution that whatever data points the model will encounter in production will likely come from this distribution.\nThe second approach, less popular in research, is to adapt a trained model to a target distribution without requiring new labels.\n(2013) used causal interpretations together with kernel embedding of conditional and marginal distributions to correct models’ predictions for both covariate shifts and label shifts without using labels Similarly, Zhao et al.\nThe third approach is what is usually done in the industry today: retrain your model using the labeled data from the target distribution.\nSimilarly, if you consider learning a joint distribution P(X, Y) as a task, then adapting a model trained on one joint distribution for another joint distribution can be framed as a form of transfer learning.\nAddressing data distribution shifts doesn’t have to start after the shifts have happened.",
      "keywords": [
        "Data Distribution Shifts",
        "data",
        "distribution",
        "data distribution",
        "shift",
        "covariate shift",
        "model",
        "distribution shifts",
        "label shift",
        "input distribution",
        "data shifts",
        "label",
        "training data",
        "training input distribution",
        "covariate"
      ],
      "concepts": [
        "shifts",
        "model",
        "statistics",
        "statistical",
        "label",
        "time",
        "different",
        "differences",
        "changes",
        "changed"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 20,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.541,
          "base_score": 0.391,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.473,
          "base_score": 0.473,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "distribution",
          "shift",
          "shifts",
          "covariate",
          "input distribution"
        ],
        "semantic": [],
        "merged": [
          "distribution",
          "shift",
          "shifts",
          "covariate",
          "input distribution"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23194423223711494,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271506+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 281-289)",
      "start_page": 281,
      "end_page": 289,
      "summary": "Within ML-specific metrics, there are generally four artifacts to monitor: a model’s accuracy-related metrics, predictions, features, and raw inputs.\nHowever, the more transformations an artifact has gone through, the more structured it’s become and the closer it is to the metrics you actually care about, which makes it easier to monitor.\nMonitoring accuracy-related metrics\nEven if the feedback can’t be used to infer natural labels directly, it can be used to detect changes in your ML model’s performance.\nYou can monitor predictions for distribution shifts.\nBecause predictions are low dimensional, it’s also easier to compute two-sample tests to detect whether the prediction distribution has shifted.\nChanges in accuracy-related metrics might not become obvious for days or weeks, whereas a model predicting all False for 10 minutes can be detected immediately.\nMonitoring features\nML monitoring solutions in the industry focus on tracking changes in features, both the features that a model uses as inputs and the intermediate transformations from raw inputs into final features.\nFeature monitoring is appealing because compared to raw input data, features are well structured following a predefined schema.\nBeyond basic feature validation, you can also use two-sample tests to detect whether the underlying distribution of a feature or a set of features has shifted.\nWhile tracking features is useful for debugging purposes, it’s not very useful for detecting model performance degradation.\nIn theory, a small distribution shift can cause catastrophic failure, but in practice, an individual feature’s minor changes might not harm the model’s performance at all.\nFeature distributions shift all the time, and most of these changes are benign.\nThe problem of feature monitoring becomes the problem of trying to decide which feature shifts are critical and which are not.\nEven if you detect a harmful change in a feature, it might be impossible to detect whether this change is caused by a change in the underlying input distribution or whether it’s caused by an error in one of the multiple processing steps.\nIf you don’t have a way to version your schemas and map each of your features to its expected schema, the cause of the reported alert might be due to the mismatched schema rather than a change in the data.\nThese concerns are not to dismiss the importance of feature monitoring; changes in the feature space are a useful source of signals to understand the health of your ML systems.\nAs discussed in the previous section, a change in the features might be caused by problems in processing steps and not by changes in data.\nThe raw input data might not be easier to monitor, as it can come from multiple sources in different formats, following multiple structures.\nThe way many ML workflows are set up today also makes it impossible for ML engineers to get direct access to raw input data, as the raw input data is often managed by a data platform team who processes and moves the data to a location like a data warehouse, and the ML engineers can only query for data from that data warehouse where the data is already partially processed.\nTherefore, monitoring raw inputs is often a responsibility of the data platform team, not the data science or ML team.\nSo far, we’ve discussed different types of metrics to monitor, from operational metrics generally used for software systems to ML-specific metrics that help you keep track of the health of your ML models.\nIn the next section, we’ll discuss the toolbox you can use to help with metrics monitoring.\nIt’s common for the industry to herald metrics, logs, and traces as the three pillars of monitoring.\nThey seem to be generated from the perspective of people who develop monitoring systems: traces are a form of logs and metrics can be computed from logs.\nsection, I’d like to focus on the set of tools from the perspective of users of the monitoring systems: logs, dashboards, and alerts.\nAn example use case of ML in log analysis is anomaly detection: to detect abnormal events in your system.\nWhen our monitoring system detects something suspicious, it’s necessary to alert the right people about it.\nSince the mid-2010s, the industry has started embracing the term “observability” instead of “monitoring.” Monitoring makes no assumption about the relationship between the internal state of a system and its outputs.\nThe word “telemetry” comes from the Greek roots tele, meaning “remote,” and metron, meaning “measure.” So telemetry basically means “remote measures.” In the monitoring context, it refers to logs and metrics collected from remote components such as cloud services or applications run on customer devices.\nWhen something goes wrong with an observable system, we should be able to figure out what went wrong by looking at the system’s logs and metrics without having to ship new code to the system.\nObservability allows more fine-grain metrics, so that you can know not only when a model’s performance degrades but also for what types of inputs or what subgroups of users or over what period of time the model degrades.\nFor example, you should be able to query your logs for the answers to questions like: “show me all the users for which model A returned wrong predictions over the last hour, grouped by their zip codes” or “show me the outliers requests in the last 10 minutes” or “show me all the intermediate outputs of this input through the system.” To achieve this, you need to have logged your\nFor example, when a model’s performance degrades over the last hour, being able to interpret which feature contributes the most to all the wrong predictions made over the last hour will help with figuring out what went wrong with the system and how to fix it.\nIn this section, we’ve discussed multiple aspects of monitoring, from what data to monitor and what metrics to keep track of to different tools for monitoring and observability.\nWe discussed three major causes of ML-specific failures: production data differing from training data, edge cases, and degenerate feedback loops.\nTo be able to detect shifts, we need to monitor our deployed systems.\nMonitoring is an important set of practices for any software engineering system in production, not just ML, and it’s an area of ML where we should learn as much as we can from the DevOps world.\nMonitoring is all about metrics.\nWe discussed different metrics we need to monitor: operational metrics—the metrics that should be monitored with any software systems such as latency, throughput, and CPU utilization—and ML-specific metrics.\nMonitoring can be applied to accuracy-related metrics, predictions, features, and/or raw inputs.\nMonitoring is hard because even if it’s cheap to compute metrics, understanding metrics isn’t straightforward.\nIt’s easy to build dashboards to show graphs, but it’s much more difficult to understand what a graph means, whether it shows signs of drift, and, if there’s drift, whether it’s caused by an underlying data distribution change or by errors in the pipeline.\n“Soft error,” https://oreil.ly/4cvNg).\n4 Daniel Papasian and Todd Underwood, “How ML Breaks: A Decade of Outages for One Large ML Pipeline,” Google, July 17, 2020, video, 19:06, https://oreil.ly/WGabN.\n8 John Mcquaid, “Limits to Growth: Can AI’s Voracious Appetite for Data Be Tamed?” Undark, October 18, 2021, https://oreil.ly/LSjVD.\n11 Rodney Brooks, “Edge Cases for Self Driving Cars,” Robots, AI, and Other Stuff, June 17, 2017, https://oreil.ly/Nyp4F; Lance Eliot, “Whether Those Endless Edge or Corner Cases Are the Long-Tail Doom for AI Self-Driving Cars,” Forbes, July 13, 2021, https://oreil.ly/L2Sbp; Kevin McAllister, “Self-Driving Cars Will Be Shaped by Simulated, Location Data,” Protocol, March 25, 2021, https://oreil.ly/tu8hs.\n8 (2011): 1373–86, https://oreil.ly/tGhHi; Daniel Fleder and Kartik Hosanagar, “Blockbuster Culture’s Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity,” Management Science 55, no.\n5 (2009), https://oreil.ly/Zwkh8; Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher, “Managing Popularity Bias in Recommender Systems with Personalized Re-ranking,” arXiv, January 22, 2019, https://oreil.ly/jgYLr.\nwith RecList,” arXiv, November 18, 2021, https://oreil.ly/7GfHk.\nLearning and Evaluation,” arXiv, February 17, 2016, https://oreil.ly/oDPSK.\nMatching,” Journal of Machine Learning Research (2009), https://oreil.ly/s49MI.\nNeurIPS Proceedings 2020, https://oreil.ly/GzJ1r; Gretton et al., “Covariate Shift by Kernel Mean Matching.”\nProceedings of Machine Learning Research 97 (2019): 7523–32, https://oreil.ly/ZxYWD.\n35 Li Bu, Cesare Alippi, and Dongbin Zhao, “A pdf-Free Change Detection Test Based on Density Difference Estimation,” IEEE Transactions on Neural Networks and Learning Systems 29, no.\nMethod,” 2006, https://oreil.ly/Dnv0s.\nDetection on Time-series Data,” arXiv, October 12, 2021, https://oreil.ly/xmdqW.\nProceedings of Machine Learning Research 97 (2019): 7523–32, https://oreil.ly/W78hH.\n45 Some monitoring vendors claim that their solutions are able to detect not only when your model should be retrained, but also what data to retrain\n46 “Amazon Compute Service Level Agreement,” Amazon Web Services, last updated August 24, 2021, https://oreil.ly/5bjx9.\n49 Ian Malpass, “Measure Anything, Measure Everything,” Code as Craft, February 15, 2011, https://oreil.ly/3KF1K.\n50 Andrew Morgan, “Data Engineering in Badoo: Handling 20 Billion Events Per Day,” InfoQ, August 9, 2019, https://oreil.ly/qnnuV.\n51 Charity Majors, “Observability—A 3-Year Retrospective,” The New Stack, August 6, 2019, https://oreil.ly/Logby.\n52 “Log Management Market Size, Share and Global Market Forecast to 2026,” MarketsandMarkets, 2021, https://oreil.ly/q0xgh.",
      "keywords": [
        "system",
        "data",
        "Monitoring",
        "Machine Learning",
        "Feature",
        "Metrics",
        "Feature monitoring",
        "Machine Learning Systems",
        "Machine Learning Research",
        "learning",
        "raw input data",
        "model",
        "software systems",
        "logs",
        "feature validation"
      ],
      "concepts": [
        "monitor",
        "data",
        "metrics",
        "features",
        "log",
        "logs",
        "logged",
        "process",
        "processing",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 32,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 18,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "monitoring",
          "oreil ly",
          "oreil",
          "https oreil",
          "ly"
        ],
        "semantic": [],
        "merged": [
          "monitoring",
          "oreil ly",
          "oreil",
          "https oreil",
          "ly"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32547615742411146,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271564+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 290-297)",
      "start_page": 290,
      "end_page": 297,
      "summary": "This chapter is a continuation of this discussion: how do we adapt our models to data distribution shifts?\nThe answer is by continually updating our ML models.\nThis process is a way to test your systems with live data in production to ensure that your updated model indeed works without catastrophic consequences.\nWhen hearing “continual learning,” many people think of the training paradigm where a model updates itself with every incoming sample in production.\nCompanies that employ continual learning in production update their models in micro-batches.\nStill, the term “continual learning” makes people imagine updating models very frequently, such as every 5 or 10 minutes.\nHowever, continual learning isn’t about the retraining frequency, but the manner in which the model is retrained.\nmeans also allowing stateful training—the model continues training on new data.\nStateful training allows you to update your model with less data.\nTraining a model from scratch tends to require a lot more data than fine-tuning the same model.\nFor example, if you retrain your model from scratch, you might need to use all data from the last three months.\nIn the traditional stateless retraining, a data sample might be reused during multiple training iterations of a model, which means that data needs to be stored.\nIn the stateful training paradigm, each model update is trained using only the fresh data, so a data sample is used only once for training, as shown in Figure 9- 2.\nThe companies that have most successfully used stateful training also occasionally train their model from scratch on a large amount of data to calibrate it.\nAlternatively, they might also train their model from scratch in parallel with stateful training and then combine both updated models using techniques such as parameter server.\nContinual learning is about setting up infrastructure in a way that allows you, a data scientist or ML engineer, to update your models whenever it is needed, whether from scratch or fine-tuning, and to deploy this update quickly.\nAs of today, stateful training is mostly applied for data iteration, as changing your model architecture or adding a new feature still requires training the resulting model from scratch.\nSome people use “online learning” to refer to the specific setting where a model learns from each incoming new sample.\nI also use the term “continual learning” instead of “continuous learning.” Continuous learning refers to the regime in which your model continuously learns with each incoming sample, whereas with continual learning, the learning is done in a series of batches or micro- batches.\nContinuous learning is sometimes used to refer to continuous delivery of ML, which is closely related to continual learning as both help companies to speed up the iteration cycle of their ML models.\nWe discussed that continual learning is about setting up infrastructure so that you can update your models and deploy these changes as fast as you want.\nTo improve performance, your model should learn throughout the day with fresh data.\nThe cold start problem arises when your model has to make predictions for a new user without any historical data.",
      "keywords": [
        "Continual Learning",
        "model",
        "Learning",
        "Stateful Training",
        "data",
        "training",
        "Continual",
        "Stateful",
        "update",
        "existing model",
        "continuous learning",
        "data distribution shifts",
        "Production",
        "online learning",
        "retraining"
      ],
      "concepts": [
        "data",
        "training",
        "continual",
        "continuation",
        "continues",
        "learning",
        "models",
        "make",
        "making",
        "online"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.892,
          "base_score": 0.742,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 28,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "continual",
          "continual learning",
          "learning",
          "stateful",
          "model"
        ],
        "semantic": [],
        "merged": [
          "continual",
          "continual learning",
          "learning",
          "stateful",
          "model"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31312159972700493,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271613+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 298-306)",
      "start_page": 298,
      "end_page": 306,
      "summary": "If your model doesn’t adapt quickly enough, it won’t be able to make recommendations relevant to these users until the next time the model is updated.\nIf you want to update your model every hour, you need new data every hour.\nIf your model needs labeled data to update, as most models today do, this data will need to be labeled as well.\nIn many applications, the speed at which a model can be updated is bottlenecked by the speed at which data is labeled.\nLabel computation can be done with batch processing: e.g., waiting for logs to be deposited into data warehouses first before running a batch job to extract all labels from logs at once.\nIf your model’s speed iteration is bottlenecked by labeling speed, it’s also possible to speed up the labeling process by leveraging programmatic labeling tools like Snorkel to generate fast labels with minimal human intervention.\nGiven that tooling around streaming is still nascent, architecting an efficient streaming-first infrastructure for accessing fresh data and extracting fast labels from real-time transports can be engineering-intensive and costly.\nThe biggest challenge of continual learning isn’t in writing a function to continually update your model—you can do that by writing a script!\nSecond, continual learning makes your models more susceptible to coordinated manipulation and adversarial attack.\nBecause your models learn online from real-world data, it makes it easier for users to input malicious data to trick models into learning wrong things.\nWhen designing the evaluation pipeline for continual learning, keep in mind that evaluation takes time, which can be another bottleneck for model update frequency.\nTo be precise, it only affects matrix-based and tree-based models that want to be updated very fast (e.g., hourly).\nYou can update the neural network model with a data batch of any size.\nHowever, if you want to update the collaborative filtering model, you first need to use\nIt’s much easier to adapt models like neural networks than matrix-based and tree-based models to the continual learning paradigm.\nHowever, there have been algorithms to create tree-based models that can learn from incremental amounts of data, most notably Hoeffding Tree and its variants Hoeffding Window Tree and Hoeffding Adaptive Tree, widespread.\nWhen your model can only see a small subset of data at a time, in theory, you can compute these statistics for each subset of data.\n2. A model to recommend relevant products to users\nBecause your team is focusing on developing new models, updating existing models takes a backseat.\nYou update an existing model only when the following two conditions are met: the model’s performance has degraded to the point that it’s doing more harm than good, and your team has time to update it.\nSome of your models are being updated once every six months.\nThe process of updating a model is manual and ad hoc.\nSomeone else cleans this new data, extracts features from it, retrains that model from scratch on both the old and new data, and then exports the updated model into a binary format.\ndeploys the updated model.\nOftentimes, the code encapsulating data, features, and model logic was changed during the retraining process but these changes failed to be replicated to production, causing bugs that are hard to track down.\nYou have anywhere between 5 and 10 models in production.\nThe ad hoc, manual process of updating models mentioned from the previous stage has grown into a pain point too big to be ignored.\nWhen creating scripts to automate the retraining process for your system, you need to take into account that different models in your system might require different retraining schedules.\nFor example, because the ranking model depends on the embeddings, when the embeddings change, the ranking model should be updated too.\nIf your company has ML models in production, it’s likely that your company already has most of the infrastructure pieces needed for automated retraining.\nHowever, in general, the three major factors that will affect the feasibility of this script are: scheduler, data, and model store.",
      "keywords": [
        "continual learning",
        "data",
        "model",
        "learning",
        "continual",
        "n’t",
        "data warehouses",
        "fresh data",
        "update",
        "batch learning",
        "labels",
        "process",
        "Continual Learning Challenges",
        "stage",
        "challenge"
      ],
      "concepts": [
        "model",
        "data",
        "labels",
        "learn",
        "challenges",
        "evaluation",
        "evaluate",
        "process",
        "processing",
        "make"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 15,
          "title": "",
          "score": 0.648,
          "base_score": 0.498,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 28,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "update",
          "updated",
          "tree",
          "continual"
        ],
        "semantic": [],
        "merged": [
          "model",
          "update",
          "updated",
          "tree",
          "continual"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3549504079427805,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271666+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 307-316)",
      "start_page": 307,
      "end_page": 316,
      "summary": "When creating training data from new data to update your model, remember that the new data has already gone through the prediction service.\nThis prediction service has already extracted features from this new data to input into models for predictions.\nThe main thing you need in this stage is a change in the mindset: retraining from scratch is such a norm—many companies are so used to data scientists handing off a model to engineers to deploy from scratch each time—that many companies don’t think about setting up their infrastructure to enable stateful training.\nThe main thing you need at this stage is a way to track your data and model lineage.\nThis model is updated with new data to create model version 1.1, and so on to create model 1.2.\nThis model is updated with new data to create model version 2.1.\nYou might want to know how these models evolve over time, which model was used as its base model, and which data was used to update it so that you can reproduce and debug it.\nAt stage 3, your models are still updated based on a fixed schedule set out by developers.\nInstead of relying on a fixed schedule, you might want your models to be automatically updated whenever data distributions shift and the model’s performance plummets.\nYou’ll first need a mechanism to trigger model updates.\nYou’ll also need a solid pipeline to continually evaluate your model updates.\nHow Often to Update Your Models\nNow that your infrastructure has been set up to update a model quickly, you started asking the question that has been haunting ML engineers at companies of all shapes and sizes: “How often should I update my models?” Before attempting to answer that question, we first need to figure out how much gain your model will get from being updated with fresh data.\nThe more gain your model can get from fresher data, the more frequently it should be retrained.\nOne way to figure out the gain is by training your model on the data from different time windows in the past and evaluating it on the data from today to see how the performance changes.\nTo measure the value of data freshness, you can experiment with training model version A on the data from January to June 2020, model version B on the data from April to September, and model version C on the data from June to November, then test each of these model\nThe difference in the performance of these versions will give you a sense of the performance gain your model can get from fresher data.\nTo get a sense of the performance gain you can get from fresher data, train your model on data from different time windows in the past and test on data from today to see how the performance changes\nOn the one hand, if you find that iterating on your data doesn’t give you much performance gain, then you should spend your resources on finding a better model.\nOn the other hand, if finding a better model architecture requires 100X compute for training and gives you 1% performance whereas updating the same model on data from the last three hours requires only 1X compute and also gives 1% performance gain, you’ll be better off iterating on data.\nHowever, as your infrastructure matures and the process of updating a model is partially automated and can be done in a matter of hours, if not minutes, the answer to this question is contingent on the answer to the following question: “How much performance gain would I get from fresher data?” It’s important to run experiments to quantify the value of data freshness to your models.\nThe first type of model evaluation you might think about is the good old test splits that you can use to evaluate your models offline, as discussed in Chapter 6.\nHowever, if you update the model to adapt to a new data distribution, it’s not sufficient to evaluate this new model on test splits from the old distribution.\nAssuming that the fresher the data, the more likely it is to come from the current distribution, one idea is to test your model on the most recent data that you have access to.\nSo, after you’ve updated your model on the data from the last day, you might want to test this model on the data from the last hour (assuming that data from the last hour wasn’t included in the data used to update your model).\nThe method of testing a predictive model on data from a specific period of time in the past is known as a backtest.\nIf something went wrong with your data pipeline and some data from the last hour is corrupted, evaluating your model solely on this recent data isn’t sufficient.\nBecause data distributions shift, the fact that a model does well on the data from the last hour doesn’t mean that it will continue doing well on the data\nWe’ll use A/B testing to determine which model is better according to some predefined metrics.\nFirst, A/B testing consists of a randomized experiment: the traffic routed to each model has to be truly random.\nThe gist here is that if your A/B test result shows that a model is better than another with statistical significance, you can determine which model is indeed better.\nSay we run a two- sample test and get the result that model A is better than model B with the p-value of p = 0.05 or 5%, and we define statistical significance as p ≤ 0.5.\nIf you’ve run your A/B test with a lot of samples and the difference between the two tested models is statistically insignificant, maybe there isn’t much difference between these two models, and it’s probably OK for you to use either.",
      "keywords": [
        "model",
        "data",
        "model version",
        "existing model",
        "model store",
        "n’t",
        "model updates",
        "update",
        "performance",
        "’ll",
        "fresher data",
        "training",
        "predictions",
        "retraining",
        "time"
      ],
      "concepts": [
        "model",
        "data",
        "tested",
        "retraining",
        "statistically",
        "statistical",
        "evaluating",
        "evaluation",
        "update",
        "updating"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 38,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.597,
          "base_score": 0.447,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.576,
          "base_score": 0.576,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "gain",
          "fresher",
          "fresher data",
          "model version"
        ],
        "semantic": [],
        "merged": [
          "model",
          "gain",
          "fresher",
          "fresher data",
          "model version"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35325822223955555,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271718+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 317-324)",
      "start_page": 317,
      "end_page": 324,
      "summary": "If not, abort the canary and route all the traffic back to the existing model.\nIf the candidate model’s key metrics degrade significantly, the canary is aborted and all the traffic will be routed to the existing model.\nEach time, a model recommends 10 items users might like.\nEach user will be exposed to the recommendations made by one model.\nWhen we show recommendations from multiple models to users, it’s important to note that the position of a recommendation influences how likely a user will click on it.\nAs of today, the standard method for testing models in production is A/B testing.\nWith A/B testing, you randomly route traffic to each model for\nA/B testing is stateless: you can route traffic to each model without having to know about their current performance.\nWhen you have multiple models to evaluate, each model can be considered a slot machine whose payout (i.e., prediction accuracy) you don’t know.\nBandits allow you to determine how to route traffic to each model for prediction to determine the best model while maximizing prediction accuracy for your users.\nBandit is stateful: before routing a request to a model, you need to calculate all models’ current performance.\nBandits require less data to determine which model is the best and, at the same time, reduce opportunity cost as they route traffic to the better model more quickly.\nbandit algorithm (Thompson Sampling) determined that a model was 5% better than the other with less than 12,000 samples.\nHowever, bandits are a lot more difficult to implement than A/B testing because it requires computing and keeping track of models’ payoffs.\nFor a percentage of time, say 90% of the time or ε = 0.9, you route traffic to the model that is currently the best-performing one, and for the other 10% of the time, you route traffic to a random model.\nIf bandits for model evaluation are to determine the payout (i.e., prediction accuracy) of each model, contextual bandits are to determine the payout of each action.\nContextual bandits, like other bandits, are an amazing technique to improve the data efficiency of your model.\nSome people also call bandits for model evaluation “contextual bandits.” This makes conversations confusing, so in this book, “contextual bandits” refer to exploration strategies to determine the payout of predictions.\nContextual bandits are algorithms that help you balance between showing users the items they will like and showing the items that you want feedback on.\nIn contextual bandits, you can get bandit feedback right away after an action—e.g., after recommending an ad, you get feedback on whether a user has clicked on that recommendation.\nContextual bandits are well researched and have been shown to improve models’ performance significantly (see reports by Twitter and Google).\nHowever, contextual bandits are even harder to implement than model bandits, since the exploration strategy depends on the ML model’s architecture (e.g., whether it’s a decision tree or a neural network), which makes it less generalizable across use cases.\nData scientists tend to evaluate their new model ad hoc using the sets of tests that they like.\nOne data scientist might perform a set of tests and find that model A is better than model B, while another data scientist might report differently.",
      "keywords": [
        "model",
        "Bandits",
        "Contextual bandits",
        "candidate model",
        "user",
        "Canary",
        "traffic",
        "items",
        "existing model",
        "candidate",
        "feedback",
        "Contextual",
        "route traffic",
        "recommendations",
        "Canary Release"
      ],
      "concepts": [
        "models",
        "bandits",
        "feedback",
        "predictions",
        "prediction",
        "recommends",
        "recommendations",
        "interleaving",
        "contextual",
        "exploration"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 5,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 42,
          "title": "",
          "score": 0.537,
          "base_score": 0.387,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.518,
          "base_score": 0.368,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.5,
          "base_score": 0.35,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bandits",
          "contextual",
          "contextual bandits",
          "route",
          "route traffic"
        ],
        "semantic": [],
        "merged": [
          "bandits",
          "contextual",
          "contextual bandits",
          "route",
          "route traffic"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24916787062917228,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271762+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 325-333)",
      "start_page": 325,
      "end_page": 333,
      "summary": "We discussed the four stages a company might go through in the process of modernizing their infrastructure for continual learning: from the manual, training from scratch stage to automated, stateless continual learning.\nWe then examined the question that haunts ML engineers at companies of all shapes and sizes, “How often should I update my models?” by urging them to consider the value of data freshness to their models and the trade- offs between model iteration and data iteration.\nSimilar to online prediction discussed in Chapter 7, continual learning requires a mature streaming infrastructure.\nContinual learning is a problem specific to ML, but it largely requires an infrastructural solution.\nWe’ll discuss infrastructure for ML in the next chapter.\n1 Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou, “Overcoming Catastrophic Forgetting with Hard Attention to the Task,” arXiv, January 4, 2018, https://oreil.ly/P95EZ.\nStart Problem in e-Commerce Recommender Systems,” arXiv, August 5, 2015, https://oreil.ly/GWUyD.\nBianchi, and Giovanni Cassani, “SIGIR 2021 E-Commerce Workshop Data Challenge,” arXiv, April 19, 2021, https://oreil.ly/8QxmS.\nHooked,” Towards Data Science, June 7, 2020, https://oreil.ly/BDWf8.\nHelp Design and Build the Future of Big Data and Stream Processing,” Snowflake blog, October 26, 2020, https://oreil.ly/Knh2Y.\n$100M,” Materialize, September 30, 2021, https://oreil.ly/dqxRb.\nBrooks, “Disparity in Home Lending Costs Minorities Millions, Researchers Find,” CBS News, November 15, 2019, https://oreil.ly/SpZ1N; Lee Brown, “Tesla Driver Killed in Crash Posted Videos Driving Without His Hands on the Wheel,” New York Post, May 16, 2021, https://oreil.ly/uku9S; “A Tesla Driver Is Charged in a Crash Involving Autopilot That Killed 2 People,” NPR, January 18, 2022, https://oreil.ly/WWaRA.\nLess Than a Day,” The Verge, May 24, 2016, https://oreil.ly/NJEVF.\nthe Sixth International Conference on Knowledge Discovery and Data Mining (Boston: ACM Press, 2000), 71–80; Albert Bifet and Ricard Gavaldà, “Adaptive Parameter-free Learning from Evolving Data Streams,” 2009, https://oreil.ly/XIMpl.\n25 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Tanxin Shi, et al., “Practical Lessons from Predicting Clicks on Ads at Facebook,” in ADKDD ’14: Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (August 2014): 1–9, https://oreil.ly/oS16J.\n28 Danilo Sato, “CanaryRelease,” June 25, 2014, MartinFowler.com, https://oreil.ly/YtKJE.\n30 Joshua Parks, Juliette Aurisset, and Michael Ramm, “Innovating Faster on Personalization Algorithms at Netflix Using Interleaving,” Netflix Technology Blog, November 29, 2017, https://oreil.ly/lnvDY.\nBandits,” Towards Data Science, January 22, 2020, https://oreil.ly/MsaAK.\nexemplifies the exploration–exploitation trade-off dilemma (s.v., “Multi-armed bandit,” https://oreil.ly/ySjwo).\nMany data scientists have told me that they know the right things to do for their ML systems, but they can’t do them because their infrastructure isn’t set up in a way that enables them to do so.\nIn this chapter, we’ll discuss how to set up infrastructure right for ML systems.\nBefore we dive in, it’s important to note that every company’s infrastructure needs are different.\nThese companies will likely need to develop their own highly specialized infrastructure.\nCompanies in the middle of the spectrum will likely benefit from generalized ML infrastructure that is being increasingly standardized (see Figure 10-1).\nInfrastructure requirements for companies at different production scales\nAccording to Wikipedia, in the physical world, “infrastructure is the set of fundamental facilities and systems that support the sustainable functionality of households and firms.” In the ML world, infrastructure is the set of fundamental facilities that support the development and maintenance of ML systems.\nThe compute layer provides the compute needed to run your ML workloads such as training a model, computing features, generating features, etc.\nData and compute are the essential resources needed for any ML project, and thus the storage and compute layer forms the infrastructural foundation for any company that wants to apply ML.\nDifferent layers of infrastructure for ML\nthen we’ll discuss resource management, a contentious topic among data scientists—people are still debating whether a data scientist needs to know about this layer or not.\nAn ML platform requires up-front investment from a company, but if it’s done right, it can make the life of data scientists across business use cases at that company so much easier.\nWe’ll discuss the build versus buy decisions in the last part of this chapter, where we’ll also discuss the hope for standardized and unified abstractions for ML infrastructure.\nML systems work with a lot of data, and this data needs to be stored somewhere.",
      "keywords": [
        "data",
        "infrastructure",
        "learning",
        "continual learning",
        "companies",
        "changing data distributions",
        "storage layer",
        "layer",
        "data science",
        "Machine Learning",
        "’ll",
        "’ll discuss",
        "data scientists",
        "streaming",
        "Data Streams"
      ],
      "concepts": [
        "data",
        "infrastructure",
        "learning",
        "streaming",
        "models",
        "scales",
        "likely",
        "layers",
        "search",
        "google"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 34,
          "title": "",
          "score": 0.892,
          "base_score": 0.742,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 35,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 36,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 31,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "oreil ly",
          "ly",
          "https oreil",
          "oreil",
          "https"
        ],
        "semantic": [],
        "merged": [
          "oreil ly",
          "ly",
          "https oreil",
          "oreil",
          "https"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3884972119104223,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271816+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 334-341)",
      "start_page": 334,
      "end_page": 341,
      "summary": "Its most common form is cloud compute managed by a cloud provider such as AWS Elastic Compute Cloud (EC2) or GCP.\nFor example, a CPU core might support two concurrent threads; each thread is used as a compute unit to execute its own job.\nA compute unit can be created for a specific short- lived job such as an AWS Step Function or a GCP Cloud Run—the unit will be eliminated after the job finishes.\nHowever, the compute layer doesn’t always use threads or cores as compute units.\nTo execute a job, you first need to load the required data into your compute unit’s memory, then execute the required operations—addition, multiplication, division, convolution, etc.—on that data.\nTherefore, a compute unit is mainly characterized by two metrics: how much memory it has and how fast it runs an operation.\nhandle more data in memory than a compute unit with only 2 GB, and it is generally more expensive.\nmemory a compute unit has but also how fast it is to load data in and out of memory, so some cloud providers advertise their instances as having “high bandwidth memory” or specify their instances’ I/O bandwidth.\nAs the name suggests, this metric denotes the number of float point operations a compute unit can run per second.\nFor example, if a machine fuses two operations into one and executes this fused operation, because a compute unit is capable of doing a trillion FLOPS doesn’t mean you’ll be able to execute your job at the speed of a trillion FLOPS.\nThe ratio of the number of FLOPS a job can run to the number of FLOPs a compute unit is capable of handling is called utilization.\nBecause thinking about FLOPS is not very useful, to make things easier, when evaluating compute performance, many people just look into the number of cores a compute unit has.\nThis means that instead of setting up their own data centers for storage and compute, companies can pay cloud providers like AWS and Azure for the exact amount of compute they use.\nCloud compute makes it extremely easy for companies to start building without having to worry about the compute layer.\nIt’s convenient to be able to just add more compute or shut down instances as needed—most cloud providers even do that automatically for you—reducing engineering operational overhead.\nMost cloud providers offer limits on the compute resources you can use at a time.\nDue to the cloud’s elasticity and ease of use, more and more companies are choosing to pay for the cloud over building and maintaining their own storage and compute layer.\nWhile leveraging the cloud tends to give companies higher returns than building their own storage and compute layers early on, this becomes less defensible as a company grows.\nThe high cost of the cloud has prompted companies to start moving their workloads back to their own data centers, a process called “cloud repatriation.” Dropbox’s S-1 filing in 2018 shows that the company was able to save $75M over the two years prior to IPO due to their infrastructure optimization overhaul—a large chunk of it consisted of moving their workloads from public cloud to their own data centers.\nMore and more companies are following a hybrid approach: keeping most of their workloads on the cloud but slowly increasing their investment in data centers.\nAs Josh Wills, one of our early reviewers, put it: “Nobody in their right mind intends to use multicloud.” It’s incredibly hard to move data and orchestrate workloads across clouds.",
      "keywords": [
        "compute unit",
        "compute",
        "cloud",
        "unit",
        "compute layer",
        "data",
        "CPU cores",
        "cloud compute",
        "companies",
        "dev environment",
        "CPU",
        "AWS",
        "FLOPS",
        "Environment",
        "Data Centers"
      ],
      "concepts": [
        "cloud",
        "compute",
        "computation",
        "companies",
        "company",
        "engine",
        "uses",
        "useful",
        "operations",
        "operation"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 40,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 41,
          "title": "",
          "score": 0.485,
          "base_score": 0.485,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.469,
          "base_score": 0.469,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "compute",
          "unit",
          "compute unit",
          "cloud",
          "flops"
        ],
        "semantic": [],
        "merged": [
          "compute",
          "unit",
          "compute unit",
          "cloud",
          "flops"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23753455808145904,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271859+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 342-350)",
      "start_page": 342,
      "end_page": 350,
      "summary": "IDEs can be native apps like VS Code or Vim. IDEs can be browser-based, which means they run in browsers, such as AWS Cloud9.\nWith notebooks, you only need to load your data once —notebooks can retain this data in memory—instead of having to load it each time you want to run your code.\nAs shown in Figure 10-5, if your code fails at step 4 in a notebook, you’ll only need to rerun step 4 instead of from the beginning of your program.\nWe had a bash file that a new team member could run to create a new virtual environment—in our case, we use conda for virtual environments— and install the required packages needed to run our code.\nThis means that we still standardize the virtual environment and tools and packages, but now everyone uses the virtual environment and tools and packages on the same type of machine too, provided by a cloud provider.\nWhen using a cloud dev environment, you can use a cloud dev environment that also comes with a cloud IDE like AWS Cloud9 (which has no built-in notebooks) and Amazon SageMaker Studio (which comes with hosted JupyterLab).\nHowever, most engineers I know who use cloud IDEs do so by installing IDEs of their choice, like Vim, on their cloud instances.\nA much more popular option is to use a cloud dev environment with a local IDE.\nFor example, you can use VS Code installed on your computer and connect the local IDE to the cloud environment using a secure protocol like Secure Shell (SSH).\nAmong them, VS Code is a good choice since it allows easy integration with cloud dev instances.\nAt our startup, we chose GitHub Codespaces as our cloud dev environment, but an AWS EC2 or a GCP instance that you can SSH into is also a good option.\nBefore moving to cloud environments, like many other companies, we were worried about the cost—what if we forgot to shut down our instances when not in use and they kept charging us money?\nBecause engineering time is expensive, if a cloud dev environment can help you save a few hours of engineering time a month, it’s worth it for many companies.\nThird, cloud dev environments can help with security.\nOf course, some companies might not be able to move to cloud dev environments also because of security concerns.\nOccasionally, a company has to move their dev environments to the cloud not only because of the benefits, but also out of necessity.\nOf course, cloud dev environments might not work for every company due to cost, security, or other concerns.\nSetting up cloud dev environments also requires some initial investments, and you might need to educate your data scientists on cloud hygiene, including establishing secure connections to the\nHowever, standardization of dev environments might make your data scientists’ lives easier and save you money in the long run.\nYou will have to turn on new instances as needed, and these instances will need to be set up with required tools and packages to execute your workloads.\nWhen a new instance is allocated for your workload, you’ll need to install dependencies using a list of predefined instructions.\nWith Docker, you create a Dockerfile with step-by-step instructions on how to re-create an environment in which your model can run: install this package, download this pretrained model, set environment variables, navigate into a folder, etc.\nIf you run this Docker image, you get back a Docker container.\nFrom this mold, you can create multiple running instances; each is a Docker container.\nFor example, NVIDIA might provide a Docker image that contains TensorFlow and all necessary libraries to optimize TensorFlow for GPUs. If you want to build an application that runs TensorFlow on GPUs, it’s not a bad idea to use this Docker image as your base and install dependencies specific to your application on top of this base image.\nIf you run both parts of the code on the same GPU instances, you’ll need GPU instances with high memory, which can be very expensive.\nInstead, you can run your featurizing code on CPU instances and the model training code on GPU instances.",
      "keywords": [
        "cloud dev environment",
        "dev environment",
        "cloud dev",
        "cloud",
        "run",
        "environment",
        "dev",
        "Docker",
        "Docker image",
        "code",
        "notebooks",
        "instances",
        "Container",
        "IDEs",
        "data"
      ],
      "concepts": [
        "containers",
        "notebooks",
        "cloud",
        "runs",
        "run",
        "running",
        "instances",
        "secure",
        "security",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 39,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.541,
          "base_score": 0.391,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.517,
          "base_score": 0.367,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 42,
          "title": "",
          "score": 0.512,
          "base_score": 0.362,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.457,
          "base_score": 0.307,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "dev",
          "cloud dev",
          "cloud",
          "environment",
          "instances"
        ],
        "semantic": [],
        "merged": [
          "dev",
          "cloud dev",
          "cloud",
          "environment",
          "instances"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2030141994077406,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271901+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 351-358)",
      "start_page": 351,
      "end_page": 358,
      "summary": "In this section, we’ll discuss how to manage resources for ML workflows.\nThere are two key characteristics of ML workflows that influence their resource management: repetitiveness and dependencies.\nThese repetitive processes can be scheduled and orchestrated to run smoothly and cost- effectively using available resources.\nIt doesn’t care about the dependencies between the jobs it runs—you can run job A after job B with cron but you can’t schedule anything complicated like run B if A succeeds and run C if A fails.\nSteps in an ML workflow might have complex dependency relationships with each other.\nMost workflow management tools require you to specify your workflows in a form of DAGs.\nIt takes in the DAG of a workflow and schedules each step accordingly.\nThis means that schedulers need to be aware of the resources available and the resources needed to run each job—the resources needed are either specified as options when you schedule a job or estimated by the scheduler.\nFor instance, if a job requires 8 GB of memory and two CPUs, the scheduler needs to find among the resources it manages an instance with 8 GB of memory and two CPUs and wait until the instance is not executing other jobs to run this job on the instance.\nSchedulers should also optimize for resource utilization since they have information on resources available, jobs to run, and resources needed for each job to run.\nDesigning a general-purpose scheduler is hard, since this scheduler will need to be able to manage almost any number of concurrent machines and workflows.\nIf schedulers are concerned with when to run jobs and what resources are needed to run those jobs, orchestrators are concerned with where to get\nOrchestrators such as HashiCorp Nomad and data science–specific orchestrators including Airflow, Argo, Prefect, and Dagster have their own schedulers.\nWe’ve discussed the differences between schedulers and orchestrators and how they can be used to execute workflows in general.\nReaders familiar with workflow management tools aimed especially at data science like Airflow, Argo, Prefect, Kubeflow, Metaflow, etc.\nAlmost all workflow management tools come with some schedulers, and therefore, you can think of them as schedulers that, instead of focusing on individual jobs, focus on the workflow as a whole.\nOnce a workflow is defined, the underlying scheduler usually works with an orchestrator to allocate resources to run the workflow, as shown in Figure 10-8.\nAfter a workflow is defined, the tasks in this workflow are scheduled and orchestrated\nThere are many articles online comparing different data science workflow management tools.\nThis section isn’t meant to be a comprehensive comparison of those tools, but to give you an idea of different features a workflow management tool might need.\nIf two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow’s DockerOperator, but it’s not that easy to do so.\nThe next generation of workflow orchestrators (Argo, Prefect) were created to address different drawbacks of Airflow.\nYou can run each step in a container, but you’ll still have to deal with Dockerfiles and register your docker with your workflows in Prefect.\nEvery step in an Argo workflow is run in its own container.",
      "keywords": [
        "resources",
        "workflow",
        "Airflow",
        "job",
        "Schedulers",
        "workflow management",
        "DAG",
        "workflow management tools",
        "run",
        "jobs",
        "step",
        "Orchestrators",
        "Prefect",
        "Management",
        "Argo"
      ],
      "concepts": [
        "schedulers",
        "schedule",
        "resource",
        "orchestrators",
        "orchestrated",
        "jobs",
        "job",
        "workflows",
        "steps",
        "airflow"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.57,
          "base_score": 0.57,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 39,
          "title": "",
          "score": 0.485,
          "base_score": 0.485,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.457,
          "base_score": 0.457,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 29,
          "title": "",
          "score": 0.424,
          "base_score": 0.424,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "workflow",
          "schedulers",
          "run",
          "resources",
          "orchestrators"
        ],
        "semantic": [],
        "merged": [
          "workflow",
          "schedulers",
          "run",
          "resources",
          "orchestrators"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3010170471539734,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:10.271948+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 359-366)",
      "start_page": 359,
      "end_page": 366,
      "summary": "# fitA requires a different version of NumPy compared to fitB @conda(libraries={\"scikit-learn\":\"0.21.1\", \"numpy\":\"1.13.0\"}) @step def fitA(self): self.model = fit(self.data, model=\"A\") self.next(self.ensemble)\n@conda(libraries={\"numpy\":\"0.9.8\"}) # Requires 2 GPU of 16GB memory @batch(gpu=2, memory=16000) @step def fitB(self): self.model = fit(self.data, model=\"B\") self.next(self.ensemble)\n@step def ensemble(self, inputs): self.outputs = ( (inputs.fitA.model.predict(self.data) + inputs.fitB.model.predict(self.data)) / 2 for input in inputs ) self.next(self.end)\nTo deploy their recommender systems, they needed to build out tools such as feature management, model management, monitoring, etc.\nHere, I’ll focus on the components that I most often see in ML platforms, which include model development, model store, and feature store.\nYou’ll need to run and serve your models from a compute layer, and usually tools only support integration with a handful of cloud providers.\nIf it’s managed service, your models and likely some of your data will be on its service, which might not work for certain regulations.\nLet’s start with the first component: model deployment.\nModel Deployment\nOnce a model is trained (and hopefully tested), you want to make its predictive capability accessible to users.\nIn Chapter 7, we talked at length on how a model can serve its predictions: online or batch prediction.\nWe also discussed how the simplest way to deploy a model is to push your model and its dependencies to a location accessible in production then expose your model as an endpoint to your users.\nA deployment service can help with both pushing your models and their dependencies to production and exposing your models as endpoints.\nWhen looking into a deployment tool, it’s important to consider how easy it is to do both online prediction and batch prediction with the tool.\nAn open problem with model deployment is how to ensure the quality of a model before it’s deployed.\nModel Store\nIn the section “Model Deployment”, we talked about how, to deploy a model, you have to package your model and upload it to a location accessible in production.\nThe person who was alerted to the problem is a DevOps engineer, who, after looking into the problem, decided that she needed to inform the data scientist who created this model.\nThe model is correct, the feature list is correct, the featurization code is correct, but something is wrong with the data processing pipeline.\nIn this simple example, we assume that the data scientist responsible still has access to the code used to generate the model.\nThe data used to train this model might be pointers to the location where the data is stored or the name/version of your data.\nModel generation code\nVery often, data scientists generate models by writing code in notebooks.\nCompanies with more mature pipelines make their data scientists commit the model generation code into their Git repos on GitHub or GitLab. However, in many companies, this process is ad hoc, and data scientists don’t even check in their notebooks.\nIf the data scientist responsible for the model loses the notebook or quits or goes",
      "keywords": [
        "Model",
        "data",
        "model deployment",
        "prediction",
        "tools",
        "batch prediction",
        "deployment",
        "Batch",
        "run",
        "Python",
        "Kubeflow",
        "data scientists",
        "model deployment tools",
        "model store",
        "online prediction"
      ],
      "concepts": [
        "models",
        "deploy",
        "tools",
        "steps",
        "predictive",
        "predictions",
        "prediction",
        "python",
        "memory",
        "cloud"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "",
          "score": 0.545,
          "base_score": 0.395,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 37,
          "title": "",
          "score": 0.537,
          "base_score": 0.387,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "",
          "score": 0.516,
          "base_score": 0.366,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 40,
          "title": "",
          "score": 0.512,
          "base_score": 0.362,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "",
          "score": 0.506,
          "base_score": 0.356,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "self",
          "model",
          "deployment",
          "model deployment",
          "self self"
        ],
        "semantic": [],
        "merged": [
          "self",
          "model",
          "deployment",
          "model deployment",
          "self self"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23755898556771976,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.271991+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 367-374)",
      "start_page": 367,
      "end_page": 374,
      "summary": "MLflow is the most popular model store, yet it’s far from solving the artifact problem.\nBecause of the lack of a good model store solution, companies like Stitch Fix resolve to build their own model store.\nFigure 10-10 shows the artifacts that Stitch Fix’s model store tracks.\nFeature Store\nits core, there are three main problems that a feature store can help address: feature management, feature transformation, and feature consistency.\nA feature store solution might address one or a combination of these problems:\nA company might have multiple ML models, each model using a lot of 31 features.\nIt’s often the case that features used for one model can be useful for another model.\nThere are many features that these two models can share.\nA feature store can help teams share and discover features, as well as manage roles and sharing settings for each feature.\nHowever, if the computation is expensive, you might want to execute it only once the first time it is required, then store it for feature uses.\nIn this capacity, a feature store acts like a data warehouse.\nDuring development, data scientists might define features and create models using Python.\nWhile it’s generally agreed that feature stores should manage feature definitions and ensure feature consistency, their exact capacities vary from vendor to vendor.\nSome feature stores only manage feature definitions without computing features from data; some feature stores do both.\nPlatforms like SageMaker and Databricks also offer their own interpretations of feature stores.\nfeature store.\nOut of those who use a feature store, half of them build their own feature store.\nAt one extreme, you can outsource all your ML use cases to a company that provides ML applications end-to-end, and then perhaps the only piece of infrastructure you need is for data movement: moving your data from your applications to your vendor, and moving predictions from that vendor back to your users.\nAt the other extreme, if you’re a company that handles sensitive data that prevents you from using services managed by another company, you might need to build and maintain all your infrastructure in-house, even having your own data centers.\nFor example, your compute might be managed by AWS EC2 and your data warehouse managed by Snowflake, but you have your own feature store and your own monitoring dashboards.\nFor example, your team might decide that you need a model store, and you’d have preferred to use a vendor, but there’s no vendor mature enough for your needs, so you have to build your own feature store, perhaps on top of an open source solution.\nAs we’re building out Claypot AI, other founders have actually advised us to avoid selling to big tech companies because, if we do, we’ll get sucked into what they call “integration hell”—spending more time integrating our solution with custom infrastructure instead of building out our core features.",
      "keywords": [
        "Feature Store",
        "Feature",
        "store",
        "model",
        "model store",
        "infrastructure",
        "Stitch Fix",
        "artifacts",
        "companies",
        "Data",
        "build",
        "vendor",
        "Tracking and Versioning",
        "popular model store",
        "company"
      ],
      "concepts": [
        "feature",
        "model",
        "infrastructure",
        "likely",
        "storing",
        "data",
        "management",
        "manage",
        "solution",
        "solutions"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 26,
          "title": "",
          "score": 0.556,
          "base_score": 0.556,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.542,
          "base_score": 0.542,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.488,
          "base_score": 0.488,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.478,
          "base_score": 0.478,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "store",
          "feature",
          "feature store",
          "vendor",
          "model store"
        ],
        "semantic": [],
        "merged": [
          "store",
          "feature",
          "feature store",
          "vendor",
          "model store"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36748054111335093,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:10.272045+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 375-382)",
      "start_page": 375,
      "end_page": 382,
      "summary": "In this chapter, we covered different layers of infrastructure needed for ML systems.\nWe started from the storage and compute layer, which provides vital resources for any engineering project that requires intensive data and compute resources like ML projects.\nOne of the first things a company can do to improve the dev environment is to standardize the dev environment for data scientists and ML engineers working on the same team.\nWe also discussed why ML workflows are different from other software engineering workflows and why they need their own workflow management tools.\n3 The definition for “reasonable scale” was inspired by Jacopo Tagliabue in his paper “You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack,” arXiv, July 15, 2021, https://oreil.ly/YNRZQ.\nEngineering, October 17, 2018, https://oreil.ly/6Ykd3; Kaushik Krishnamurthi, “Building a Big Data Pipeline to Process Clickstream Data,” Zillow, April 6, 2018, https://oreil.ly/SGmNe.\n“Infrastructure,” https://oreil.ly/YaIk8.\nFor example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total” (“Optimize CPU Options,” Amazon Web Services, last accessed April 2020, https://oreil.ly/eeOtd).\nEnterprise Spending on Data Centers,” March 18, 2021, https://oreil.ly/uPx94.\n2021, https://huyenchip.com/2021/09/13/data-science-infrastructure.html; Neil Conway and David Hershey, “Data Scientists Don’t Care About Kubernetes,” Determined AI, November 30, 2020, https://oreil.ly/FFDQW; I Am Developer on Twitter (@iamdevloper): “I barely understand my own feelings how am I supposed to understand kubernetes,” June 26, 2021, https://oreil.ly/T2eQE.\nJohn Wilkes, “Large-Scale Cluster Management at Google with Borg,” EuroSys ’15: Proceedings of the Tenth European Conference on Computer Systems (April 2015): 18, https://oreil.ly/9TeTM.\n30 Neal Lathia, “Building a Feature Store,” December 5, 2020, https://oreil.ly/DgsvA; Jordan\nVolz, “Why You Need a Feature Store,” Continual, September 28, 2021, https://oreil.ly/kQPMb; Mike Del Balso, “What Is a Feature Store?” Tecton, October 20, 2020, https://oreil.ly/pzy0I.\nPlatform,” Uber Engineering, September 5, 2017, https://oreil.ly/XteNy. 32 Some people use the term “feature transformation.”\nIn this chapter, we’ll discuss how users and developers of ML systems might interact with these systems.\nWe’ll first consider how user experience might be altered and affected due to the probabilistic nature of ML models.\nWe’ll end the chapter with how ML systems can affect the society as a whole in the section “Responsible AI”.\nWe’ve discussed at length how ML systems behave differently from traditional software systems.\nSecond, due to this probabilistic nature, ML systems’ predictions are mostly correct, and the hard part is we usually don’t know for what inputs the system will be correct!\nThird, ML systems can also be large and might take an unexpectedly long time to produce a prediction.\nThese differences mean that ML systems can affect user experience differently, especially for users that have so far been used to traditional software.\nIn this section, we’ll discuss three challenges that ML systems pose to good user experience and how to address them.\nML predictions are probabilistic and inconsistent, which means that predictions generated for one user today might be different from what will be generated for the same user the next day, depending on the context of the predictions.\nFor tasks that want to leverage ML to improve users’ experience, the inconsistency in ML predictions can be a hindrance.\nThe applied ML team at Booking.com wanted to use ML to automatically suggest filters that a user might want, based on the filters they’ve used in a given browsing session.\nThe challenge they encountered is that if their ML model kept suggesting different filters each time, users could get confused, especially if they couldn’t find a filter that they had already applied before.\nIn this case, given a set of requirements input by users, you can have the model produce multiple snippets of React code.\nWe’ve talked at length about the effect of an ML model’s inference latency on user experience in the section “Computational priorities”.",
      "keywords": [
        "data",
        "data scientists",
        "systems",
        "predictions",
        "user experience",
        "users",
        "data centers",
        "model",
        "discussed data systems",
        "feature store",
        "correct",
        "Big Data Platform",
        "reasonable scale",
        "experience",
        "Big Data"
      ],
      "concepts": [
        "data",
        "users",
        "different",
        "differences",
        "prediction",
        "predictions",
        "model",
        "engineering",
        "engineers",
        "recommended"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "https",
          "oreil",
          "ly",
          "https oreil",
          "oreil ly"
        ],
        "semantic": [],
        "merged": [
          "https",
          "oreil",
          "ly",
          "https oreil",
          "oreil ly"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3403911806905387,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.272096+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 383-391)",
      "start_page": 383,
      "end_page": 391,
      "summary": "An ML project involves not only data scientists and ML engineers, but also other types of engineers such as DevOps engineers and platform engineers as well as nondeveloper stakeholders like subject matter experts (SMEs).\nWe’ll focus on two aspects: cross- functional teams collaboration and the much debated role of an end-to-end data scientist.\nThey’re not only users but also developers of ML systems.\nHowever, as training ML models becomes an ongoing process in production, labeling and relabeling might also become an ongoing process spanning the entire project lifecycle.\nAn ML system would benefit a lot to have SMEs involved in the rest of the lifecycle, such as problem formulation, feature engineering, error analysis, model evaluation, reranking predictions, and user interface: how to best present results to users and/or to other parts of the system.\nFor example, to help SMEs get more involved in the development of ML systems, many companies are building no-code/low- code platforms that allow people to make changes without writing code.\nTo be able to bring all these areas of expertise into an ML project, companies tend to follow one of the two following approaches: have a separate team to manage all the Ops aspects or include data scientists on the team and have them own the entire process.\nIn this approach, the data science/ML team develops models in the dev environment.\nThen a separate team, usually the Ops/platform/ML engineering team, production i zes the models in prod.\nFor example, the platform team has ideas on how to improve the infrastructure but they can only act on requests from data scientists, but data scientists don’t have to deal with infrastructure so they have less incentives to proactively make changes to it.\nApproach 2: Data scientists own the entire process\nIn this approach, the data science team also has to worry about productionizing models.\nAbout a year ago, I tweeted about a set of skills I thought was important to become an ML engineer or data scientist, as shown in Figure 11-2.\nFor data scientists to own the entire process, we need good tools.\nWhat if I can just tell this tool, “Here’s where I store my data (S3), here are the steps to run my code (featurizing, modeling), here’s where my code should run (EC2 instances, serverless stuff like AWS Batch, Function, etc.), here’s what my code needs to run at each step (dependencies),” and then this tool manages all the infrastructure stuff for me?\nThey need tools that “abstract the data scientists from the complexities of containerization, distributed processing, automatic failover, and other advanced computer science concepts.”\nIn the second half of this chapter, we’ll focus on an even more crucial consideration: how ML systems might affect society and what ML system developers should do to ensure that the systems they develop do more good than harm.\nThe question of how to make intelligent systems responsible is relevant not only to ML systems but also general artificial intelligence (AI) systems.\nTherefore, in this section, we use AI instead of ML.\nResponsible AI is the practice of designing, developing, and deploying AI systems with good intention and sufficient awareness to empower users, to engender trust, and to ensure fair and positive impact to society.\nAs developers of ML systems, you have the responsibility not only to think about how your systems will impact users and society at large, but also to help all stakeholders better realize their responsibilities toward the users by concretely implementing ethics, safety, and inclusivity into your ML systems.\nWe will then propose a preliminary framework for data scientists and ML\nengineers to select the tools and guidelines that best help with making your ML systems responsible.",
      "keywords": [
        "data scientists",
        "data",
        "systems",
        "model",
        "scientists",
        "Team",
        "data science",
        "process long series",
        "main model",
        "code",
        "systems responsible",
        "process",
        "data science team",
        "infrastructure",
        "shorter series"
      ],
      "concepts": [
        "models",
        "data",
        "team",
        "ethics",
        "ethical",
        "process",
        "processing",
        "users",
        "make",
        "making"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "",
          "score": 0.837,
          "base_score": 0.687,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "systems",
          "scientists",
          "ml",
          "team",
          "data scientists"
        ],
        "semantic": [],
        "merged": [
          "systems",
          "scientists",
          "ml",
          "team",
          "data scientists"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3895815030905094,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.272150+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 392-399)",
      "start_page": 392,
      "end_page": 399,
      "summary": "According to Jones and Safak from Ada Lovelace Institute, “Awarding students’ grades based on teacher assessment was originally rejected by Ofqual on the grounds of unfairness between schools, incomparability across generations and devaluing of results because of grade inflation.\nThe fairer option, Ofqual surmised, was to combine previous attainment data and teacher assessment to assign grades, using a particular statistical model—an ‘algorithm.’”\nOfqual stated that their model, tested on 2019 data, had about 60% average accuracy across A-level subjects.\nWhile the model’s accuracy seems low, Ofqual defended their algorithm as being broadly comparable to the accuracy of human graders.\nWhen developing an automated system to grade students, you would’ve thought that the objective of this system would be “grading accuracy for students.”\nHowever, the objective that Ofqual seemingly chose to optimize was “maintaining standards” across schools—fitting the model’s predicted grades to historical grade distributions from each school.\nFor example, if school A had historically outperformed school B in the past, Ofqual wanted an algorithm that, on average, also gives students from school A higher grades than students from school B.\nOfqual prioritized fairness between schools over fairness between students—they preferred a model that gets school-level results right over another model that gets each individual’s grades right.\nDue to this objective, the model disproportionately downgraded high- performing cohorts from historically low-performing schools.\nBias against students from historically low-performing schools is only one of the many biases discovered about this model after the results were brought to the public.\nBecause the model took into account each school’s historical performance, Ofqual acknowledged that their model didn’t have enough data for small schools.\nIt might have been possible to discover these biases through the public release of the model’s predicted grades with fine-grained evaluation to understand their model’s performance for different slices of data—e.g., evaluating the model’s accuracy for schools of different sizes and for students from different backgrounds.\nFor example, they didn’t let the public know that the objective of their system was to maintain fairness between schools until the day the grades were published.\nIt also shows the importance of choosing the right objective to optimize, as the wrong objective (e.g., prioritizing fairness among schools) can not only lead you to choose a model that underperforms for the right objective, but also perpetuate biases.\nSince the development of ML systems relies heavily on the quality of data, it’s important for user data to be collected.\nPractitioners and companies require access to data to discover new use cases and develop new AI-powered products.\nStrava stated that the data used had been anonymized, and “excludes activities that have been marked as private and user-defined privacy zones.”\nSince Strava was used by military personnel, their public data, despite anonymization, allowed people to discover patterns that expose activities of US military bases overseas, including the “forward operating bases in Afghanistan, Turkish military patrols in Syria, and a possible guard patrol in 18 the Russian operating area of Syria.” An example of these discriminating patterns is shown in Figure 11-4.\nFirst, Strava’s default privacy setting was “opt-out,” meaning that it requires users to manually opt out if they don’t want their data to be collected.\nDevelopers of applications that gather user data must understand that their users might not have the technical know-how and privacy awareness to choose the right privacy settings for themselves, and so developers must proactively work to make the right settings the default, even at the cost of gathering less data.",
      "keywords": [
        "Incident Database",
        "model",
        "Ofqual",
        "data",
        "students",
        "schools",
        "grades",
        "public",
        "system",
        "objective",
        "Strava",
        "United Kingdom canceled",
        "biases",
        "privacy",
        "users"
      ],
      "concepts": [
        "grades",
        "grading",
        "data",
        "schools",
        "model",
        "teacher",
        "students",
        "performance",
        "perform",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 48,
          "title": "",
          "score": 0.557,
          "base_score": 0.407,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.462,
          "base_score": 0.312,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 16,
          "title": "",
          "score": 0.452,
          "base_score": 0.302,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 47,
          "title": "",
          "score": 0.439,
          "base_score": 0.289,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 17,
          "title": "",
          "score": 0.413,
          "base_score": 0.263,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "schools",
          "ofqual",
          "grades",
          "students",
          "school"
        ],
        "semantic": [],
        "merged": [
          "schools",
          "ofqual",
          "grades",
          "students",
          "school"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.1316863171025147,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.272197+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 400-407)",
      "start_page": 400,
      "end_page": 407,
      "summary": "Is the data used for developing your model representative of the data your model will handle in the real world?\nIf not, your model might be biased against the groups of users with less data represented in the training data.\nDoes your model use any feature that contains sensitive information?\nDoes your model cause a disparate impact on a subgroup of people?\nDisparate impact occurs “when a selection process has widely different outcomes for different groups, even as it appears to be neutral.” This can happen when a model’s decision relies on information correlated with legally protected classes (e.g., ethnicity, gender, religious practice) even when this information isn’t used in training the model directly.\nAre you performing adequate, fine-grained evaluation to understand your model’s performance on different groups of users?\nFor example, you might want your system to have low inference latency, which could be obtained by model compression techniques like pruning.\nYou might also want your model to have high predictive accuracy, which could be achieved by adding more data.\nYou might also want your model to be fair and transparent, which could require the model and the data used to develop this model to be made accessible for public scrutiny.\nOften, ML literature makes the unrealistic assumption that optimizing for one property, like model accuracy, holds all others static.\nPeople might discuss techniques to improve a model’s fairness with the assumption that this model’s accuracy or latency will remain the same.\nDifferential privacy is a popular technique used on training data for ML models.\nThe trade-off here is that the higher the level of privacy that differential privacy can provide, the lower the model’s accuracy.\nWe learned that it’s possible to reduce a model’s size significantly with minimal cost of accuracy, e.g., reducing a model’s parameter count by 90% with minimal accuracy cost.\nIt’s important to be aware of these trade-offs so that we can make informed design decisions for our ML systems.\nCompanies might decide to bypass ethical issues in ML models to save cost and time, only to\nCreate model cards\nModel cards are short documents accompanying trained ML models that provide information on how these models were trained and evaluated.\ncard paper, “The goal of model cards is to standardize ethical practice and reporting by allowing stakeholders to compare candidate models for deployment across not only traditional evaluation metrics but also along the axes of ethical, inclusive, and fair considerations.”\nThe following list has been adapted from content in the paper “Model Cards for Model Reporting” to show the information you might want to report for your models:\nModel date\nEvaluation data: Details on the dataset(s) used for the quantitative analyses in the card.\nModel cards are a step toward increasing transparency into the development of ML models.\nThey are especially important in cases where people who use a model aren’t the same people who developed this model.\nFor models that update frequently, this can create quite an overhead for data scientists if model cards are created manually.\nTherefore, it’s important to have tools to automatically generate model cards, either by leveraging the model card generation feature of tools like TensorFlow, Metaflow, and scikit-learn or by building this feature in-house.\nFor example, Google has published recommended best practices for responsible AI and IBM has open-sourced AI Fairness 360, which contains a set of metrics, explanations, and algorithms to mitigate bias in datasets and models.\nThe main cons of the second approach is that it’s difficult to hire data scientists who can own the process of developing an ML system end-to-end.",
      "keywords": [
        "model",
        "model cards",
        "data",
        "disparate impact",
        "system",
        "information",
        "Training data",
        "cards",
        "evaluation data",
        "impact",
        "Privacy",
        "generate model cards"
      ],
      "concepts": [
        "model",
        "different",
        "evaluation",
        "evaluated",
        "information",
        "fairness",
        "techniques",
        "ethical",
        "uses",
        "impacted"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 17,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 19,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 24,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 48,
          "title": "",
          "score": 0.459,
          "base_score": 0.309,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cards",
          "model",
          "model cards",
          "information",
          "accuracy"
        ],
        "semantic": [],
        "merged": [
          "cards",
          "model",
          "model cards",
          "information",
          "accuracy"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23400440911992806,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.272242+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 408-416)",
      "start_page": 408,
      "end_page": 416,
      "summary": "Incorporating ethics principles into your modeling and organizational practices will not only help you distinguish yourself as a professional and cutting-edge data scientist and ML engineer but also help your organization gain trust from your customers and users.\nEugeneYan.com, August 9, 2020, https://oreil.ly/A6oPi.\nScience Generalist and the Perils of Division of Labor Through Function,” MultiThreaded, March 11, 2019, https://oreil.ly/m6WWu.\n17, 2018, https://oreil.ly/iYgQs.\nInstitute Blog, 2020, https://oreil.ly/ztTxR.\nAugust 19, 2020, https://oreil.ly/GFRet. 9 Ofqual, “Awarding GCSE, AS & A Levels in Summer 2020: Interim Report,” Gov.uk, August\n13, 2020, https://oreil.ly/r22iz.\n15 “Royal Statistical Society Response to the House of Commons Education Select Committee Call for Evidence: The Impact of COVID-19 on Education and Children’s Services Inquiry,” Royal Statistical Society, June 8, 2020, https://oreil.ly/ernho.\nhttps://oreil.ly/FokAV.\nMashable, January 28, 2018, https://oreil.ly/9ogYx.\nhttps://oreil.ly/mB0GD.\nMilitary Bases,” Wired, January 30, 2018, https://oreil.ly/eJPdj.\n20 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See”; Rosie Spinks, “Using a Fitness App Taught Me the Scary Truth About Why Privacy Settings Are a Feminist Issue,” Quartz, August 1, 2017, https://oreil.ly/DO3WR.\nhttps://oreil.ly/hXwpN.\nVenkatasubramanian, “Certifying and Removing Disparate Impact,” arXiv, July 16, 2015, https://oreil.ly/FjSve.\n“Differential privacy,” https://oreil.ly/UcxzZ.\nModel Accuracy,” arXiv, May 28, 2019, https://oreil.ly/nrJGK.\nCompressed Deep Neural Networks Forget?” arXiv, November 13, 2019, https://oreil.ly/bgfFX.\n“Characterising Bias in Compressed Models,” arXiv, October 6, 2020, https://oreil.ly/ZTI72.\n28 Hooker et al., “Characterising Bias in Compressed Models.”\nStecklein, Jim Dabney, Brandon Dick, Bill Haskins, Randy Lovell, and Gregory Moroney, “Error Cost Escalation Through the Project Life Cycle,” NASA Technical Reports Server (NTRS), https://oreil.ly/edzaB.\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru, “Model Cards for Model Reporting,” arXiv, October 5, 2018, https://oreil.ly/COpah.\n31 Mitchell et al., “Model Cards for Model Reporting.”\ncontinual learning and, Algorithm challenge-Algorithm challenge\narchitectural search, Hard AutoML: Architecture search and learned optimizer\nbase model, fine tuning, Transfer learning\nbaselines, offline model evaluation, Baselines\nchampion model, Continual Learning\nbinary, Binary versus multiclass classification\nhierarchical, Binary versus multiclass classification\nhigh cardinality, Binary versus multiclass classification",
      "keywords": [
        "Binary versus multiclass",
        "Prediction Versus Online",
        "versus multiclass classification",
        "Data Science",
        "Data Science Workflow",
        "Data Science Pin",
        "Batch Prediction Versus",
        "Binary versus",
        "Versus",
        "model",
        "versus multiclass",
        "Architecture search",
        "Batch Prediction",
        "data",
        "Science Pin Factory"
      ],
      "concepts": [
        "data",
        "modeling",
        "learning",
        "baselines",
        "processing",
        "processes",
        "process",
        "bases",
        "binary",
        "classification"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 46,
          "title": "",
          "score": 0.557,
          "base_score": 0.407,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.512,
          "base_score": 0.362,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.471,
          "base_score": 0.321,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 4,
          "title": "",
          "score": 0.463,
          "base_score": 0.313,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "oreil",
          "https oreil",
          "ly",
          "oreil ly",
          "https"
        ],
        "semantic": [],
        "merged": [
          "oreil",
          "https oreil",
          "ly",
          "oreil ly",
          "https"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.18714073179864246,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.272297+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 417-424)",
      "start_page": 417,
      "end_page": 424,
      "summary": "cloud computing, ML on the Cloud and on the Edge, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\nelasticity, Public Cloud Versus Private Data Centers\nmulticloud strategy, Public Cloud Versus Private Data Centers\ncovariate data distribution shift, Covariate shift-Covariate shift\ndata, When to Use Machine Learning, Data\ntraining (see training data)\nunseen data, When to Use Machine Learning\ndata augmentation, Data Augmentation\ndata distribution shifts\nML system failure, Data Distribution Shifts\nfeature change, General Data Distribution Shifts\nlabel schema change, General Data Distribution Shifts\nlabel shift, Types of Data Distribution Shifts, Label shift\ndata engineering, Iterative Process\ndata formats, Data Formats\nmultimodal data, Data Formats\ndata freshness, model updates and, Value of data freshness-Value of data freshness\ndata iteration, Stateless Retraining Versus Stateful Training\nmodel updates and, Model iteration versus data iteration\ndetecting, Detecting Data Leakage\ndata models\ndata normalization, Relational Model\ndata parallelism, distributed training and, Data parallelism-Data parallelism\ndatabases, internal, Data Sources\nlogs, Data Sources\nsmartphones and, Data Sources\nsystem-generated data, Data Sources\ndatabases and dataflow, Data Passing Through Databases\nmessage queue model, Data Passing Through Real-Time Transport",
      "keywords": [
        "Versus Private Data",
        "Data Distribution Shifts",
        "Versus Column-Major Format",
        "Cloud Versus Private",
        "Private Data Centers",
        "Public Cloud Versus",
        "Retraining Versus Stateful",
        "Stateless Retraining Versus",
        "Versus Stateful Training",
        "Row-Major Versus Column-Major",
        "Versus Unstructured Data",
        "Versus Binary Format",
        "Versus Data training",
        "Major Versus Column-Major",
        "General Data Distribution"
      ],
      "concepts": [
        "data",
        "models",
        "learned",
        "formats",
        "shifts",
        "containers",
        "services",
        "failure",
        "training",
        "computing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.95,
          "base_score": 0.8,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "",
          "score": 0.915,
          "base_score": 0.765,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.891,
          "base_score": 0.741,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.604,
          "base_score": 0.454,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "versus",
          "private",
          "versus private",
          "cloud versus",
          "private data"
        ],
        "semantic": [],
        "merged": [
          "versus",
          "private",
          "versus private",
          "cloud versus",
          "private data"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37061367302408244,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273414+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 425-432)",
      "start_page": 425,
      "end_page": 432,
      "summary": "existing data, When to Use Machine Learning\nexperiment artifacts, development and, Model Store\nfeature change, General Data Distribution Shifts\nfeature engineering, Learned Features Versus Engineered Features-Learned Features Versus Engineered Features\nfeature importance, Feature Importance\nNLP (natural language processing) and, Learned Features Versus Engineered Features\npredictive power of features, Detecting Data Leakage\nfeatures\ncomputation, Feature Store\nconsistency, Feature Store\nextracting, Monitoring features\nlearned, Learned Features Versus Engineered Features-Learned Features Versus Engineered Features\nmanagement, Feature Store\nFourier features, Discrete and Continuous Positional Embeddings\ncloud computing and, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\ndevelopment environment layer, Infrastructure and Tooling for MLOps, Development Environment\nML platform layer, Infrastructure and Tooling for MLOps\nstorage and compute layer, Infrastructure and Tooling for MLOps, Infrastructure and Tooling for MLOps, Storage and Compute\nprivate data centers, Public Cloud Versus Private Data Centers- Public Cloud Versus Private Data Centers\npublic cloud, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\ninstances on-demand, Public Cloud Versus Private Data Centers\nmodel development and, Iterative Process\nperformance check, Model Development and Offline Evaluation\nmodel updates and, Model iteration versus data iteration\ndata engineering, Iterative Process\nlabel computation, Fresh data access challenge\nlabel schema change, General Data Distribution Shifts\nlabel shift, Types of Data Distribution Shifts, Label shift\nML algorithms, Evaluating ML Models",
      "keywords": [
        "Cloud Versus Private",
        "Features Versus Engineered",
        "Versus Private Data",
        "Public Cloud Versus",
        "Continuous Positional Embeddings",
        "Private Data Centers",
        "Learned Features Versus",
        "Batch Prediction Versus",
        "Prediction Versus Online",
        "Engineered Features positional",
        "Features positional embeddings",
        "Features-Learned Features Versus",
        "Centers-Public Cloud Versus",
        "Versus Buy cloud",
        "Centers public cloud"
      ],
      "concepts": [
        "feature",
        "data",
        "model",
        "labels",
        "learned",
        "evaluation",
        "evaluating",
        "computation",
        "compute",
        "experiments"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.95,
          "base_score": 0.8,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.875,
          "base_score": 0.725,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "",
          "score": 0.854,
          "base_score": 0.704,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "versus",
          "features",
          "private",
          "public cloud",
          "cloud"
        ],
        "semantic": [],
        "merged": [
          "versus",
          "features",
          "private",
          "public cloud",
          "cloud"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3513721483022561,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273466+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 433-440)",
      "start_page": 433,
      "end_page": 440,
      "summary": "performance metrics, Experiment tracking\nML (machine learning)\nexisting data and, When to Use Machine Learning\nmodel optimization, Using ML to optimize ML models-Using ML to optimize ML models\npredictions and, When to Use Machine Learning\nunseen data, When to Use Machine Learning\nML algorithms, Overview of Machine Learning Systems, Model Development and Offline Evaluation\ndeep learning and, Evaluating ML Models\nlabels, Evaluating ML Models\nversus neural networks, Evaluating ML Models\nML model logic, Model Deployment and Prediction Service\nML models\ndata iteration, Stateless Retraining Versus Stateful Training\ndeployment, Model Deployment\nedge computing, optimization, Compiling and Optimizing Models for Edge Devices-Using ML to optimize ML models\nevaluation, Evaluating ML Models\ndata problems, Versioning\npoor model implementation, Versioning\noptimization, Using ML to optimize ML models-Using ML to optimize ML models\nperformance metrics, Experiment tracking\nselection criteria, Evaluating ML Models\ndata engineering, Iterative Process\ndistributed, Distributed Training-Model parallelism\ndata iteration and, Model iteration versus data iteration\nmodel iteration and, Model iteration versus data iteration\nML system failures\nlabel schema change, General Data Distribution Shifts\nMLOPs, ML systems design and, Overview of Machine Learning Systems- Overview of Machine Learning Systems\nmodel development, Iterative Process\nmodel implementation, failures and, Versioning\nmodel performance, business analysis, Iterative Process",
      "keywords": [
        "Machine Learning Systems",
        "Learning Systems Versus",
        "Machine Learning",
        "data distribution shifts",
        "Machine Learning model",
        "Software System Failures",
        "Machine Learning LFs",
        "Machine Learning production",
        "Machine Learning research",
        "Systems Versus Traditional",
        "Learning Systems MNAR",
        "Mind Versus Data",
        "Learning Systems",
        "system failures data",
        "Learning Systems edge"
      ],
      "concepts": [
        "modeling",
        "data",
        "evaluation",
        "evaluating",
        "evaluate",
        "failures",
        "iterative",
        "iteration",
        "learning",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.891,
          "base_score": 0.741,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.875,
          "base_score": 0.725,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "learning",
          "machine learning",
          "machine",
          "ml",
          "learning systems"
        ],
        "semantic": [],
        "merged": [
          "learning",
          "machine learning",
          "machine",
          "ml",
          "learning systems"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4068788441675996,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273525+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 441-448)",
      "start_page": 441,
      "end_page": 448,
      "summary": "monitoring, Monitoring and Observability, Continual Learning and Test in Production\nNorvig, Peter, Mind Versus Data\non-demand instances, Public Cloud Versus Private Data Centers\non-demand prediction, Batch Prediction Versus Online Prediction\nOne Billion Word Benchmark for Language Modeling, Mind Versus Data\nonline features, Batch Prediction Versus Online Prediction\nonline learning, Stateless Retraining Versus Stateful Training\nonline prediction, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction, Bandits\noverfitting, Data-level methods: Resampling\nSMOTE, Data-level methods: Resampling\nPearl, Judea, Mind Versus Data\nprediction, When to Use Machine Learning, Multiple ways to frame a problem\nasynchronous, Batch Prediction Versus Online Prediction\nbatch prediction, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction\non-demand prediction, Batch Prediction Versus Online Prediction\nonline, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction\nsynchronous, Batch Prediction Versus Online Prediction\nbatch processing, Batch Processing Versus Stream Processing-Batch Processing Versus Stream Processing\nstream processing, Batch Processing Versus Stream Processing-Batch Processing Versus Stream Processing\nproduction, ML and, Machine Learning in Research Versus in Production- Discussion\nstreaming data and, Batch Processing Versus Stream Processing\nregression models, Classification versus regression\ndata normalization, Relational Model\nresampling, Data-level methods: Resampling\ndynamic sampling, Data-level methods: Resampling\noverfitting and, Data-level methods: Resampling\nSMOTE, Data-level methods: Resampling\ntwo-phase learning, Data-level methods: Resampling\nundersampling, Data-level methods: Resampling\nRogati, Monica, Mind Versus Data",
      "keywords": [
        "Prediction Versus Online",
        "Batch Prediction Versus",
        "Online Prediction predictions",
        "Online Prediction-Batch Prediction",
        "Online Prediction batch",
        "Online Prediction streaming",
        "Continuous Positional Embeddings",
        "Unifying Batch Pipeline",
        "Cloud Versus Private",
        "Training online prediction",
        "Prediction-Batch Prediction Versus",
        "Prediction streaming pipeline",
        "Versus Private Data",
        "Versus Online Prediction-Batch",
        "Prediction online learning"
      ],
      "concepts": [
        "data",
        "models",
        "learning",
        "processing",
        "process",
        "predictions",
        "prediction",
        "predictive",
        "objective",
        "baselines"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "",
          "score": 0.903,
          "base_score": 0.753,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "versus",
          "prediction",
          "batch",
          "online",
          "online prediction"
        ],
        "semantic": [],
        "merged": [
          "versus",
          "prediction",
          "batch",
          "online",
          "online prediction"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3857420736078832,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273581+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 449-456)",
      "start_page": 449,
      "end_page": 456,
      "summary": "RPC (remote procedure call), Data Passing Through Services\nsentiment analysis classifier, Learned Features Versus Engineered Features\ndataflow and, Data Passing Through Services-Data Passing Through Services\ndriver management, Data Passing Through Services\nprice optimization, Data Passing Through Services\nride management, Data Passing Through Services\nSGD (stochastic gradient descent), Data parallelism\nerror analysis, Slice-based evaluation\nheuristics based, Slice-based evaluation\nSMOTE (synthetic minority oversampling technique), Data-level methods: Resampling\ndeployment, Software System Failures\ndata leakage and, Scaling before splitting\nstateful training, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nstateless retraining, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nstochastic gradient descent (SGD), Data parallelism\nprivate data centers, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\npublic cloud, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\nstorage engines, Data Storage Engines and Processing\nstreaming data, real-time transport, Batch Processing Versus Stream Processing\nstreaming features, Batch Prediction Versus Online Prediction\nstructured data, Structured Versus Unstructured Data-Structured Versus Unstructured Data\nSutton, Richard, Mind Versus Data\nsynthetic minority oversampling technique (SMOTE), Data-level methods: Resampling\ntext data, Text Versus Binary Format\nstateful, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nstateless retraining, Stateless Retraining Versus Stateful Training- Stateless Retraining Versus Stateful Training\ntraining data, Training Data\ndata distributions, Production data differing from training data\nhand labels, Hand Labels-Data lineage\nn-grams, Learned Features Versus Engineered Features",
      "keywords": [
        "Retraining Versus Stateful",
        "simple random sampling",
        "Versus Stateful Training",
        "Versus Private Data",
        "Stateless Retraining Versus",
        "Versus Column-Major Format",
        "Prediction Service services",
        "Row-Major Versus Column-Major",
        "Learned Features Versus",
        "Cloud Versus Private",
        "Features Versus Engineered",
        "Batch Prediction Versus",
        "Versus Binary Format",
        "Service services dataflow"
      ],
      "concepts": [
        "data",
        "model",
        "sampling",
        "samples",
        "evaluation",
        "label",
        "features",
        "training",
        "learned",
        "storage"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 49,
          "title": "",
          "score": 0.915,
          "base_score": 0.765,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "",
          "score": 0.903,
          "base_score": 0.753,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 50,
          "title": "",
          "score": 0.854,
          "base_score": 0.704,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 21,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "versus",
          "stateless retraining",
          "retraining",
          "stateful",
          "retraining versus"
        ],
        "semantic": [],
        "merged": [
          "versus",
          "stateless retraining",
          "retraining",
          "stateful",
          "retraining versus"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35156933910660654,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273634+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 457-461)",
      "start_page": 457,
      "end_page": 461,
      "summary": "data engineering, Iterative Process\ntwo-phase learning, Data-level methods: Resampling\nunseen data, When to Use Machine Learning\nworkflow management, Data Science Workflow Management\nArgo, Data Science Workflow Management-Data Science Workflow Management\nKubeflow, Data Science Workflow Management\nMetaflow, Data Science Workflow Management\nChip Huyen (https://huyenchip.com) is co-founder and CEO of Claypot AI, developing infrastructure for real-time machine learning.\nPreviously, she was at NVIDIA, Snorkel AI, and Netflix, where she helped some of the world’s largest organizations develop and deploy machine learning systems.\nLinkedIn included her among the 10 Top Voices in Software Development in 2019, and Top Voices in Data Science & AI in 2020.\nThe animal on the cover of Designing Machine Learning Systems is a red- legged partridge (Alectoris rufa), also known as a French partridge.",
      "keywords": [
        "Iterative Process transactional",
        "User Experience consistency",
        "Process transactional processing",
        "Ensuring User Experience",
        "Structured Versus Unstructured",
        "Analytical Processing ACID",
        "Cases user experience",
        "Data Science Workflow",
        "Experience Consistency predictions",
        "Science Workflow Management",
        "Versus Unstructured Data",
        "Smooth Failing user",
        "Predictions smooth failing",
        "Failing user feedback",
        "Iterative Process-Iterative Process"
      ],
      "concepts": [
        "data",
        "learning",
        "partridge",
        "process",
        "processing",
        "management",
        "importance",
        "user",
        "machine",
        "fonts"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 44,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 45,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 2,
          "title": "",
          "score": 0.506,
          "base_score": 0.356,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "workflow management",
          "science workflow",
          "workflow",
          "science",
          "management"
        ],
        "semantic": [],
        "merged": [
          "workflow management",
          "science workflow",
          "workflow",
          "science",
          "management"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30893166529306626,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:10.273687+00:00"
      }
    }
  ],
  "total_chapters": 54,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications_metadata.json",
    "enrichment_date": "2025-12-17T23:02:10.286577+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 5286.021960999278,
    "total_similar_chapters": 270
  }
}