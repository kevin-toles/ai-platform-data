{
  "metadata": {
    "title": "Python Architecture Patterns",
    "source_file": "Python Architecture Patterns_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Introduction to Software Architecture\b",
      "start_page": 1,
      "end_page": 16,
      "summary": "Packt Publishing.\nAN: 3134268 ; Jaime Buelta.; Python Architecture Patterns : Master API Design, Event-driven Structures, and Package Management in Python\nPython Architecture Patterns\nMaster API design, event-driven structures, and  \npackage management in Python\n\"Python\" and the Python Logo are trademarks of the Python Software Foundation.\nPython Architecture Patterns\nCopyright © 2022 Packt Publishing\nNeither the author, nor Packt Publishing or its \ncompanies and products mentioned in this book by the appropriate use of capitals.\nHowever, Packt Publishing cannot guarantee the accuracy of this information.\nContent Development Editor: Alex Patterson\nPython developer for over 10.\nWriting a book is always more than a single person's work.\nPradeep works with Ockham BV, a Belgium-based software development company.\nThe company develops software in the quality and document management systems \nChapter 1: Introduction to Software Architecture\b\nConway's Law – Effects on software architecture\b\nSecurity aspects of software architecture\b\nChapter 2: API Design\b\nAbstractions\b\nUsing the right abstractions\b\nDesigning a RESTful API process\b\nReview of the design and implementation\b\nChapter 3: Data Modeling\b\nTypes of databases\b\nRelational databases\b\nNon-relational databases\b\nDistributed relational databases\b\nSharding\b\nChapter 4: The Data Layer\b\nChapter 5: The Twelve-Factor App Methodology\b\nChapter 6: Web Server Structures\b\nWeb architecture\b\nPython worker\b\nChapter 7: Event-Driven Structures\b\nScheduled tasks\b\nScheduled tasks\b\nChapter 8: Advanced Event-Driven Structures\b\nConnecting the tasks\b\nChapter 9: Microservices vs Monolith\b\nChapter 10: Testing and TDD\b\nStructuring tests\b\nChapter 11: Package Management\b\nThe Python packaging ecosystem\b\nPure Python package\b\nPython package with binary code\b\nChapter 12: Logging\b\nChapter 13: Metrics\b\nChapter 14: Profiling\b\nChapter 15: Debugging\b\nPython introspection tools \b\nChapter 16: Ongoing Architecture\b\nThe evolution of software means that, over time, systems grow to be more and more \ncomplex, and require more and more developers working on them in a coordinated \nThe challenge of software architecture is to plan and design this structure.\ndesigned architecture makes different teams able to interact with each other while at \nThe architecture of a system should be designed in a way that day-to-day software \ncan be adjusted and expanded as well, reshaping the different software elements in a \nIn this book we will see the different aspects of software architecture, from the top \nThe book is \nstructured in four sections, covering all the different aspects in the life cycle:\nDesign before writing any code\nArchitectural patterns to use proven approaches\nImplementation of the design in actual code\nOngoing operation to cover changes, and verification that it's all working as \nDuring the book we will cover different techniques across all these aspects.\nWho this book is for\nThis book is for software developers that want to expand their knowledge of \nsoftware architecture, whether experienced developers that want to expand and \nWe will use code written in Python for the examples.\nWhat this book covers\nChapter 1, Introduction to Software Architecture, presents the topic of what software \narchitecture is and why it is useful, as well as presenting a design example.\nThe first section of the book covers the Design phase, before the software is written:\nChapter 2, API Design, shows the basics of designing useful APIs that abstract the \nChapter 3, Data Modeling, talks about the particularities of storage systems and how to \nChapter 4, The Data Layer, goes over the code handling of the stored data, and how to \nNext, we will present a section that covers the different Architectural patterns \nChapter 5, The Twelve-Factor App Methodology, shows how this methodology includes \ngood practices that can be useful when operating with web services and can be \nChapter 6, Web Server Structures, explains web services and the different elements to \ntake into consideration when settling on both the operative and the software design.\nChapter 7, Event-Driven Structures, describes another kind of system that works \nChapter 8, Advanced Event-Driven Structures, explains more advanced usages for \nasynchronous systems, and some different patterns that can be created.\nChapter 9, Microservices vs Monolith, presents these two architectures for complex \nThe Implementation section of the book covers how the code is written:\nChapter 10, Testing and TDD, talks about the fundaments of testing and how Test \nDriven Development can be used in the coding process.\nChapter 11, Package Management, follows the process of creating reusable parts of code \noperation and requires monitoring at the same time that is adjusted and changed:\nChapter 12, Logging, describes how to record what working systems are doing.\nChapter 13, Metrics, discusses aggregating different values to see how the whole \nChapter 14, Profiling, explains how to understand how code is executed to improve its \nChapter 15, Debugging, covers the process of digging deep into the execution of code \nChapter 16, Ongoing Architecture, describes how to successfully operate architectural \nchanges on running systems.\nTo get the most out of this book\nThe book uses Python language for code examples, and assumes that the \nadvantageous to understand the different challenges software architecture \nThe code bundle for the book is hosted on GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns.\nfrom our rich catalog of books and videos available at https://github.com/",
      "keywords": [
        "Python Architecture Patterns",
        "Python",
        "Software Architecture",
        "Architecture",
        "Packt Publishing",
        "Python Architecture",
        "book",
        "Design",
        "Software",
        "Master API Design",
        "Table of Contents",
        "API Design",
        "printed",
        "8:47",
        "Summary"
      ],
      "concepts": [
        "python",
        "design",
        "architecture",
        "architectural",
        "different",
        "differences",
        "summary",
        "logging",
        "log",
        "logs"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.42,
          "base_score": 0.27,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 5,
          "title": "",
          "score": 0.368,
          "base_score": 0.218,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.344,
          "base_score": 0.194,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chapter",
          "architecture",
          "software",
          "book",
          "design"
        ],
        "semantic": [],
        "merged": [
          "chapter",
          "architecture",
          "software",
          "book",
          "design"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2774236822245011,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.529974+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "API Design\b",
      "start_page": 17,
      "end_page": 74,
      "summary": "example: \"For this recipe, we need to import the requests module.\" \nFor example: \narchitecture is and where it's useful.\nused when defining the architecture of a system and a baseline example of the web \ndifferent elements are accessed, so how software is structured has ramifications for \nan example system that we will be using to present the different patterns and \nConway's Law in software architecture\nSecurity aspects of software architecture\nAt its core, software development is about creating and managing complex systems.\nIn the most simple terms, software architecture defines the structure of a software \na project, but after system growth and a few change requests, the need to think \nIt's easier to make changes following the structure rather than against the \ndifferent teams or perhaps elements that can affect external \nand efficient to change in the future, a smart architectural design \nexample of when to keep certain operations difficult to implement.\nThe considerations for software architecture can be quite numerous and there needs \nSome examples may include:\nhandle a certain number of users, or that the system is fast enough for its use \nA news website requires different update times than a real-time trading \nDivision of tasks, to allow multiple teams, perhaps specialized in different \nUse specific technologies, for example, to allow integration with other \nTo enable successful communication, a good architecture should define boundaries \nthe general approach in this book because it's the best way to explain the different \nelement, or unit, should have a clear function and interface.\nFor example, a common architecture for a typical system could be a web service \nAn Apache web server that handles all the web requests, returns any static \nfiles, like CSS and images, and forwards the dynamic requests to the web \nAs you can see, every different element has a distinct function in the system.\nWhen presented with new features, most use cases will fall \nbetween the elements, as the data stored in the database may need to be changed to \nEach element has different requirements and characteristics:\nThe database needs to be reliable, as it stores all the data.\nThe web server will require some changes for new styling, but that won't \nthe default starting point when designing web-based client/server \nAs we can see, the work balance between elements is very different, as the web \nworker will be the focus for most new work, while the other two elements will be \nThe web server communicates with \nthe external clients via HTTP requests.\nFor example, there can be \nmultiple RESTful interfaces, which is common in microservices.\nThe typical way of looking at different units is as different processes running \nThe Single-Responsibility principle can be applied at different \narchitecture, the higher-level elements are the most important, as \nFor example, you could create a \nThe important characteristic is that in order to create an independent element, the \nAPI needs to be clearly defined and the responsibility needs to be well defined.\nAPIs will be used.\narchitecture described above is an example of that, as most of the \nexplicitly or otherwise, to form the software structure created by an organization.\nThese elements are key for the design of a good \nsoftware architecture.\nThe main thing for the successful application of any software architecture is that \nthe team structure needs to follow the designed architecture quite closely.\nA single software element should \nDifferent \nIf there's a big imbalance in the mapping of work units to teams (for example, too \nelements of the system and to use detailed APIs to overcome the physical barriers \nremote teams to work closely together on the same code base.\nSoftware architecture is tightly related to how different teams \nKeeping this in mind will help you design a successful software architecture so \nIn this book, we will be using an application as an example to demonstrate the \ndifferent elements and patterns presented.\ndivided into different elements for demonstration purposes.\nexample is available on GitHub, and different parts of it will be presented in the \nThe example application is a web application for microblogging, very similar to \nThe architecture of the example system is described in this diagram:\nFigure 1.2: Example architecture\nfor login, logout, writing new micro-posts, and reading other users' micro-\nA public RESTful API, to allow the usage of other clients (mobile, JavaScript, \nThis will authenticate the users using \nbetween the different elements.\nThis package works as a different element.\nSecurity aspects of software architecture\nAn important element to take into consideration when creating an architecture is \nFor example, a banking application needs to be 100 \nexample of this is the storage of passwords.\nWhen the user tries to log in, we receive the input password, \nuser to log in.\nread the passwords of all the users.\nMistakes like displaying the password of a user in status logs.\nTo make things secure, data needs to be structured in a way that's as protected as \npossible from access or even copying, without exposing the real passwords of users.\nusers with the same password but different salts will have different hashes.\n4.\t When a user tries to log in, their input password is added to the salt, and the \nIf it's correct, the user is logged in.\nFor example, you can \nbefore, the architecture defines which aspects are easy and difficult to change and \ncan make some unsafe things impossible to do, like knowing the password of a user, \nfrom the user to keep privacy or reducing the data exposed in internal APIs, for \nexample.\nThis example is presented in a simplified way.\nFor example, the bcrypt function can be applied multiple \nThe problem of mistakenly displaying the password of a user in \nIn this chapter, we looked at what software architecture is and when it is required, as \nWe learned that the underlying structure of software is difficult to change and that \nsuccessful starting point when creating simple web service systems.\nWe described the example that we will use throughout the book to describe the \ndifferent elements and patterns we will present.\nsecurity aspects of software architecture and how creating barriers to accessing data \nIn the next section of the book, we will talk about the different aspects of designing a \nsystem: The interface, or how an element of the system connects to the rest, and data \nstorage, how this element stores information that can be retrieved later.\n1.\t API Design, describing how to create useful, yet flexible, interfaces\n2.\t Data Modeling, with different ways of handling and representing data to \nAPI Design\n(API) design principles.\nWe will see how to start our design by defining useful \nabstractions that will create the foundation for the design.\nWe will look at design approaches and techniques to help create a useful API based \nthis is a critical element for most APIs. We will cover how to create a versioning system for the API, attending to the \ndifferent use cases that can be affected.\nAPI Design\nAlthough the main objective of the chapter is to talk about API interfaces, we will \nalso talk about HTML interfaces to see the differences and how they interact with \nother APIs. Finally, we will describe the design for the example that we will use later in the book.\nVersioning the API\nDesigning the API for the example\nAn API allows us to use a piece of software without totally understanding all the \ndifferent effects; for example, retrieving the time in the system.\nthe time may return very different results.\nIn both cases, the APIs are defining abstractions.\nThis is, of course, a simple example, but APIs can hide a tremendous amount of \nA good example to think about is a program like \nEven when just sending an HTTP request to a URL and printing the returned \nWhile we are mainly talking about APIs, and throughout the book \nmanage a user is an abstraction, as it defines the concept of \"user \nconsistent interface for the user.\nAPI Design\nThis makes a call to www.google.com and displays the headers of the response using \nMaking a remote connection to a server requires a lot of different moving parts:\nRedirection based on the result from the first request, as the server returns \nEach of these steps also makes use of other APIs to perform smaller actions, which \nstrict definition of an API discard stipulates that the end user is a \nFor a successful interface, the root is to create a series of abstractions and present \nthem to the user so that they can perform actions.\ndesigning a new API is, therefore, to decide which are the best abstractions.\nFor example, it's very common to start a user management system by adding \ndifferent flags to the users.\nSo, a user has permission to perform action A, and then \nWhile designing a new API, it is good to try to explicitly describe the inherent \nabstractions that the API uses to clarify them, at least at a high level.\nthe advantage of being able to think about that as a user of the API and see if things \nOne of the most useful viewpoints in the work of software \nthe position of the actual user of the software.\nAPI Design\nby underlying bugs in the code serving the API, or sometimes directly from the way \nOperating systems are good examples of a system that generates good abstractions \nor not being able to create a new connection due to reaching a limit in terms of the \nWhen designing an API, it is important to take this fact into account for several \nThe API \nResources and action abstractions\nA very useful pattern to consider when designing an API is to produce a set \nof resources that can perform actions.\nThis pattern uses two kinds of elements: \nresources and actions.\nResources are passive elements that are referenced, while actions are performed on \nresources.\nFor example, let's define a very simple interface to play a simple game guessing coin \nThe resource and actions may be as follows:\nResource\nAPI Design\nNote how each resource has its own set of actions that can be performed.\nresource).\nActions can require parameters that can be other resources.\nHowever, the abstractions are organized around having a consistent set of resources \nThis way of explicitly organizing an API is useful as it clarifies what is \nneatly into this structure, as \"actions\" can work like resources.\nby Roy Fielding, and it uses HTTP standards as a basis to create a definition of a \nClient-server architecture.\nAll the information related to a particular request should be \ncontained in the request itself, making it independent from the specific \nserver serving the request.\nResource identification in requests, meaning a resource is \nhave all the required information to make changes when they have \nServers can submit code in response to help perform operations or \nHTTP requests.\nAPI Design\nunderstood as interfaces based on HTTP resources using JSON formatted requests.\nresources, as well as HTTP methods and actions to perform on them, using the \nResource\nExample\nelements of the collection, for example, all \nCreate operation.\nCreates a new element of the \nReturns the newly created resource.\nReturns the data from the \nresource, for example, the book with an ID of 1.\nfor the resource.\ncreated.\npartial values for the resource, for example, \nCRUD interfaces facilitate the performance of those actions: Create \nThe key element of this design is the definition of everything as a resource, as we \nthe resources, for example:\n/books/1/cover defines the resource of the cover image from the book with an ID \nFor example, this could be an example of a request and response to retrieve a user:\nto another resource.\nFor simplicity, we will use integer IDs to identify the resources in \nleak information about the number of elements in the system or \npossible to create two at the same time.\nIt is preferable to return full URIs to resources \ninstead of indirect references, such as no-context IDs. For example, when creating a new resource, include the new URI \nAPI Design\nWhen the resource will be directly represented by binary content, it can return the \nFor example, retrieving the \navatar resource may return an image file:\nIn the same way, when creating or updating a new avatar, it should be sent in the \nsame result, while repeating not-idempotent actions will generate different results.\ndoesn't create a problem.\nWhile the original intention of RESTful interfaces was to make use \nof multiple formats, for example, accepting XML and JSON, this \nA clear case of this is the creation of a new element.\ncreations of a new element of a resource list, it will create two new elements.\nexample, submitting two books with the same name and author will create two \ndifferent headers and status codes.\nHeaders include metadata information about the request or response.\nis added automatically, like the size of the body of the request or response.\nThe type of the body of the request, like \nThis can be used in GET requests.\nresource.\nproduce a different result to the first in any case.\nAPI Design\nA well-designed API will make use of headers to communicate proper information, \nfor example, setting Content-Type correctly or accepting cache parameters if possible.\nAnother important detail is to make good use of available status codes.\ninformation possible for each situation will provide a better interface.\nSome common status codes are as follows:\nshould return a body; if it doesn't, use 204 No \n201 Created\nA successful POST request that creates a new resource.\nA successful request that doesn't return a body, for \nexample, a successful DELETE request.\nFor example, the API \nThis status code is only returned if the client \nrequested cached information, for example, using the \nThe request is authenticated, but it can't access this \nresource.\nstatus in that the request is already correctly \nThe resource \nThe requested method cannot be used; for example, \n429 Too Many Requests\nThe server should return this status code if there's a \nlimit to the number of requests the client can do.\nshould return a description or more info in the body, \nThe server is redirecting the request to a different \nIn general, non-descriptive error codes such as 400 Bad Request and 500 Server \nAPI Design\nFor example, a PATCH request to overwrite a parameter should return 400 Bad \nRequest if the parameter is incorrect for any reason, but 404 Not Found if the resource \nIn any error, please include some extra feedback to the user with a reason.\nFor example, the mentioned PATCH may return this body:\nerror codes, multiple messages in case there are multiple possible errors, and also \nDesigning resources\nThe available actions in a RESTful API are limited to CRUD operations.\nresources are the basic construction blocks for the API.\nMaking everything a resource helps to create very explicit APIs and helps with the \nstateless requirement for RESTful interfaces.\nThis is especially useful for 4XX errors as they will help users of the \na request is either provided by the caller or retrieved externally, \nElements that could be activated by creating different actions could be separated \ninto different resources.\nFor example, an interface simulating a pen could require the \nIn some APIs, like an object-oriented one, this could involve creating a pen object \nIn a RESTful API, we need to create different resources for both the pen and its \n# Create a new pen with id 1\n# Create a new open pen for pen 1\n# Update the new open text for the open pen 1\nEither create the text directly, or create a pen \nAPI Design\nResources and parameters\nWhile everything is a resource, some elements make more sense as a parameter \nAny change needs to be submitted to update the resource.\nA typical search endpoint will define a search resource and retrieve its results.\nparameters will be required to define the search, for example:\n# Return every pen in the system\n# Return only red pens\n# Return only red pens, sorted by creation date\nto the API.\ncompose resources that obtain information from multiple sources \nWe will see examples in \nOther kinds of request methods should provide any parameters as \nGET requests are also easy to cache if including the query parameters.\nreturning the same values for each request, given that that's an idempotent request, \nBy convention, all logs that store GET requests will also store the query params, while \nany parameter sent as a header or in the body of the request won't be logged.\nSometimes, that's the reason to create POST operations that typically would be a GET \nrequest, but prefer to set parameters in the body of the request instead of query \nWhile it is possible in the HTTP protocol to set the body in a GET request, \nAnother reason to use POST requests is to allow a bigger space for parameters, as the \nIn a RESTful interface, any LIST request that returns a sensible number of elements \nThis means that the number of elements and pages can be tweaked from the request, \nreturning only a specific page of elements.\nAn example could involve using the parameters page and size, for example:\n# Return only first 10 elements\nAPI Design\nup the downloading of information, doing several small requests instead of one big \nreturning not too much information, being able to retrieve only the relevant \nbetween multiple requests, especially if retrieving many pages.\n# Create a new resource that is added to the first page\nhas now moved to the second, and then there's one element that's not returned.\nNormally, the non-return of the new resource is not that much of a problem, as, after \nThis way, any new resource will be added \nCreating a flexible pagination system increases the usefulness of any API.\nthat your pagination definition is consistent across any different resources.\nDesigning a RESTful API process\nThe best way to start designing a RESTful API is to clearly state the resources and \nResource URI: Note that this may be shared for several actions, differentiated \nby the method (for example, GET to retrieve and DELETE to delete)\nMethods applicable: The HTTP method to use for the action defined in this \nPossible expected errors: Returning status codes depending on specific errors\nDifferent from errors, in case there's a status code that's considered a success \nbut it's not the usual case; for example, a success returns a redirection\nThis will be enough to create a design document that can be understood by other \nengineers and allow them to work on the interface.\nmethods, and to have a quick look at all the different resources that the system has \nhelps to detect missing resource gaps or other kinds of inconsistencies in the API.\nFor resources that return inherently \"new\" elements, like \nretrieve only the new resources since the most recent access.\nAPI Design\nFor example, the API described in this chapter has the following actions:\nIt looks like we forgot to add the action to remove a pen, once created\nThere are a couple of GET actions for retrieving information about the created \nFor example, we can define the \nendpoints to create a new pen and read a pen in the system:\nCreating a new pen:\nDescription: Creates a new pen, specifying the color.\nResource URI: /pens\nResource URI: /pens/<pen id>\nThe most important part is that they are useful; for example, adding \nThe API can also be designed using tools such as Postman (www.\neither design or test/debug existing APIs. While useful, it is good \nto be able to design an API without external tools, in case that's \nAPI Design\nDesigning and defining an API can also enable it to be structured in a standard \nA more structured alternative is to use a tool such as Open API (https://www.\nOpen API is a specification for defining a RESTful API through a \nIt allows the definition of different components that can be repeated, both as input \nways of inheriting or composing from one another, thereby creating a rich interface.\nArchitecture-Patterns/blob/main/pen_example.yaml:\ndescription: \"Pen object that needs to be added to the store\"\nuseful editor and other resources.\nAPI Design\ndifferent errors that can be produced.\nIf the YAML file describes your interface correctly and fully, this can be really useful.\nIn some cases, it could be advantageous to work from the YAML to the API.\nIt's even possible to automatically create skeletons of clients and servers \nin multiple languages, for example, servers in Python Flask or Spring, and clients in \nto use the API, as we can see in the next graphic:\nAPI Design\ndocumentation, even if the design is not started from an Open API YAML file, it's a \nBeing able to log the user properly is critical, and a \nThe most important security issue regarding authentication is to always use \nthe bare minimum required to allow users of your API to send you passwords and \nother sensitive information without the fear that an external user is going to receive \nIn HTML web pages, normally, the flow to authenticate is as follows:\n2.\t The user enters their login and password and sends them to the server.\nNormally, most architectures use HTTPS until the request \nreaches the data center or secure network, and then use HTTP \nrequests in HTTPS require extra processing power.\n5.\t All new requests will send the cookie.\neach access or force the user to log in again from time to time.\nthe user.\nAnother important kind of security problem is cross-site request forgery (CSRF).\nIn this case, the fact that the user is logged in on an external service is exploited by \nFor example, while accessing a forum, a URL from a common bank is called, \nis not understood by older browsers, operations presented to the user by the bank \nshould present a random token, making the user send both the authenticated request \nAPI Design\nevery access, this session ID is queried to the server and the related information is \nOn very big deployments, with many accesses, this can create problems \nThe database where the session ID is stored needs to be accessed \nOne possible solution is to create a rich data token.\nrequired information directly to the cookie; for example, storing the user ID, expiry, \nThis avoids database access, but makes the cookie possible to \nOAuth has become a common standard for authenticating access for APIs, and \nuser and provide them with a token with information allowing the user to log in.\nThe service will receive this token and will log the user:\nprovides certain ideas that can be tweaked to the specific use case.\nThere's a difference between authenticating and authorizing, and \nOAuth uses the concept of scope to return what \nalso include the user information in the returning token to also \nauthenticate the user, returning who the user is.\nAPI Design\nThere's an important difference in terms of whether the system accessing the API \nis the final user directly, or whether it accesses it on behalf of a user.\nAn example of \nneeds to access the data stored for the user in GitHub, such as a code analysis tool.\nuser.\nthe auth provider will present a login page to the user and redirect them with the \nFor example, this could be the sequence of calls for the Authorization Code grant:\nReturn a page with a form to initiate the login with authorizer.com\nReturn 302 Found to https://myservice.com/redirect?code=XXXXX\nIf the system accessing the API is from the end user directly, the Client Credentials \n(user ID) and client_secret (password) to retrieve the authentication token directly.\nThis token will be set in new calls as a header, authenticating the request.\nThis means that there are different ways in which you can \nimplement OAuth, and, crucially, that different authorizers will \nimplement it differently.\nMake new requests setting the header\nWhile OAuth allows you to use an external server to retrieve the access token, that's \nOur example system will use the Client Credentials flow.\nThe returned tokens from the authorization server can contain sufficient information \nAs we've seen, including the user information in the token is \nrequest that is capable of doing the work, but without information \nAPI Design\nThis uses different algorithms, based on the information in the header.\nFor example, to generate a token using pyjwt (https://pypi.org/project/PyJWT/), \nThen, while opening a Python interpreter, to create a token with a payload with \na user ID and an HS256 algorithm to sign it with the \"secret\" secret, you use the \n>>> token = jwt.encode({\"user_id\": \"1234\"}, \"secret\", \n{'user_id': '1234'}\nIncluding the payload information that can be used to identify the user allows \nauthentication of the requests using just the information in the payload, once \nVersioning the API\ncommunicate these changes, it's useful to create some sort of versioning to transmit \nas creating release notes informing users of that fact.\nnormally related to the version of the software, usually with some help from version \ndetailed as the internal one, that is normally not that helpful to users and can \nAPI Design\nFor example, an internal version may distinguish between two different bug fixes, \nAnother good example of when it's useful to make a difference is when the interface \ncould use \"Version 2 interface,\" but this can happen over multiple internal new \nversions, to be tested internally or by a selected group (for example, beta testers).\nFinally, when the \"Version 2 interface\" is ready, it can be activated for all users.\nA common pattern for defining versions is to use semantic versioning.\nversioning describes a method with three increasing integers that carry different \nusers are.\nThis means that software designed to work with v1.2.15 will work with versions \nexample, v1.2.3-rc1 (release candidate) or v1.2.3-dev0 (development version).\nSemantic versioning requires a pretty strict definition of the interface.\ninterface changes often with new features, as happens typically with online \nFor online services, the combination of both will make only a single number useful, \nSemantic versioning works better for cases that require \nmultiple API versions working at the same time, for example:\nexample is databases, such as MySQL.\nexample.\nnumber is set to zero (for example, v0.1.3), making version ",
      "keywords": [
        "API Design",
        "API",
        "Software Architecture",
        "System",
        "Open API",
        "user",
        "pen",
        "design",
        "Architecture",
        "Software",
        "resource",
        "APIs",
        "request",
        "pens",
        "subject"
      ],
      "concepts": [
        "differences",
        "differently",
        "requests",
        "request",
        "resources",
        "useful",
        "uses",
        "likely",
        "operations",
        "operation"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 3,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 5,
          "title": "",
          "score": 0.474,
          "base_score": 0.324,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.391,
          "base_score": 0.241,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "resource",
          "api",
          "resources",
          "user",
          "design"
        ],
        "semantic": [],
        "merged": [
          "resource",
          "api",
          "resources",
          "user",
          "design"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.311926140354631,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530056+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Data Modeling\b",
      "start_page": 75,
      "end_page": 110,
      "summary": "API Design\nDatabases, either relational databases such as MySQL or PostgreSQL, or non-\nThe frontend will make use of interfaces defined by the backend to present the \na typical example being multiple smartphone interfaces for different platforms, but \nthat use the same API to communicate with the backend.\ndifferent, with frontend work requiring more of an eye for design, \nsame time as the backend controllers that handle the data access and business logic.\naccess to the data.\nThe Model manages the data\nthe data.\nof graphic user interfaces and has been used in that area since \nAPI Design\nworks as a way of abstracting the logic of the application from the input and output.\nWhile the strict definition of APIs works for interfaces that are designed to be \nof how to create a successful human interface.\nabout HTML interfaces, aimed at being used by the end user in a browser.\nThe way traditional web interfaces work is through HTTP requests, only using the \nwith some form that submits data to the server.\nFor example, a blog is read way more often than is written, so readers make use of \nAn HTML interface doesn't work in the same way as a RESTful interface because of \nThe main limitation of traditional HTML interfaces is that every change needs to \nperform actions to change the page directly on the browser representation; for \nAPI Design\nthe user.\nAnother example, that reverses the interface somewhat, could be to use the ZIP code \nto use a RESTful API returning JSON to retrieve the expected data for these small \nthe objective of this API is not to replace the HTML interface in its entirety, but \nOther applications go directly to the point of creating an API-first approach and \nIf there's any new data to be required, it will be \naccessed through a specific (typically RESTful) API.\nof the data.\nThis kind of approach is sometimes called API-first as it designs a \nsystem from the API to the representation, instead of creating it the \nAPI Design\nThe technical skill required to create a successful human interface on a \nThe need to design and prepare the API beforehand can result in a slow \nHowever, if the application started with another kind of user \ninterface, like a smartphone application, it could leverage the already existing REST \nAPI to generate an HTML interface that replicates the functionality.\nThe main advantage of this approach is detaching the application from the user \nregular HTML interface, the risk is that any other user interface will tend to conform \ncompromise the design of the API, as the abstractions that are used will likely be \nA whole API-first approach greatly separates the interface, so creating a new \ninterface is as easy to use as the already existing API.\nmultiple interfaces, such as an HTML interface, but also different smartphones \nhappen as a natural step to migrating from a traditional HTML interface to a single-\nHTML interface for the general approach of the interface, with clear pages to \nOn the other, it creates a RESTful API that fills most of the information and \nIn practice, this tends to create a less complete RESTful API, as some of the elements \nDesigning the API for the example\nthe definition for the different interfaces that we will be working on in the example.\nRemember that the example is a microblogging application that will allow users to \nThere are two main interfaces in the example:\nAn HTML interface for allowing users to interact with the service using a \nAPI Design\nIn this chapter, we will describe the design of the second interface.\nUser: A representation of the user of the application.\nMicropost: A small text of up to 255 characters posted by a User.\nCollection: The display of Microposts from a User.\nTimeline: An ordered list of the Microposts by the followed Users.\nSearch: Allow a search by User or by text contained in Microposts.\nGET    /api/user/<username>\nGET    /api/user/<username>/collection\nPOST   /api/user/<username>/collection\nGET    /api/user/<username>/collection/<micropost_id>\nPUT    /api/user/<username>/collection/<micropost_id>\nPATCH  /api/user/<username>/collection/<micropost_id>\nDELETE /api/user/<username>/collection/<micropost_id>\nGET    /api/user/<username>/timeline\nGET    /api/user/<username>/following\nPOST   /api/user/<username>/following\nDELETE /api/user/<username>/following/<username>\nGET    /api/user/<username>/followers\nAPI Design\nRetrieve user:\nResource URI: /api/users/<username>\nRetrieve user's collection:\nDescription: Returns the collection of all microposts from a user, in paginated \nResource URI: /api/users/<username>/collection\nDescription: Create a new micropost.\nResource URI: /api/users/<username>/collection\n403 Forbidden Trying to create a micropost of a different user \nResource URI: /api/users/<username>/collection/<micropost_id>\nAPI Design\nResource URI: /api/users/<username>/collection/<micropost_id>\n403 Forbidden Trying to update a micropost of a different user \nResource URI: /api/users/<username>/collection/<micropost_id>\n403 Forbidden Trying to delete a micropost of a different user \nResource URI: /api/users/<username>/timeline\nAPI Design\nRetrieve the users a user is following:\nDescription: Returns a collection of all users that the selected user is following.\nResource URI: /api/users/<username>/following\nFollow a user:\nDescription: Causes the selected user to follow a different user.\nResource URI: /api/users/<username>/following\nDescription: Stops following a user.\nResource URI: /api/users/<username>/following/<username>\nRetrieve a user's followers:\nDescription: Returns, in paginated form, all followers of this user.\nResource URI: /api/users/<username>/followers\nAPI Design\nThis two-step approach of presenting and designing a new API enables you to \nIn this chapter, we described how the basics of API design are to create a set of \nuseful abstractions that allow users to perform actions without having to care about \nThis definition of an API has evolved to cover RESTful interfaces that follow certain \nbunch of useful standards and techniques when designing RESTful interfaces to \ncreate consistent and complete interfaces, including the OpenAPI tools.\nthrough authentication details as it's a very important element for APIs. We covered the ideas behind versioning and how to create a proper versioning \nschema that's tailored to the specific use case for the API.\nWe described the different options for HTML interfaces to provide a complete \noverview of the different interfaces in web services.\nterms of how an HTML service can be constructed and interact with other APIs. Finally, we presented the design for the RESTful interface for the example, while \nAnother critical element of design is the data structure.\nThis is a critical aspect of the design of any API and \nAPI Design\nData Modeling\nThe core of any application is its data.\nstored data is a crucial part of this cycle, as it allows you to use information that has \nIn this chapter, we will talk about how we can model the stored data from our \napplication and what the different options are to store and structure the data to be \nWe will start by describing the different database options that are available, which \nare critical to understanding their different applications, but we will mostly focus, \nduring the chapter, on relational databases, as they are the most common type.\nwill describe the concept of a transaction to ensure that different changes are applied \nWe will describe different ways that we can increase the scope of a relational \ndatabase by using multiple servers, and what the use cases for each option are.\nthat our data is structured in the best possible way.\nTypes of databases\nDatabase transactions\nData Modeling\nDistributed relational databases\nLet's start with an introduction to the different databases out there.\nTypes of databases\nAll the persistent data from an application should live in a database.\ndiscussed, data is the most critical aspect of any application, and proper handling of \nDatabases have been a critical tool for most of the time software systems have \nThey create an abstraction layer that allows accessing data without \ndatabases allow the structure of the data to be defined without having to worry \nTechnically, databases are collections of data themselves and \nare handled by the database management system (DBMS), the \nsoftware that allows the input and output of data.\nthe word \"database\" is used for both the collection and the \nallow access to multiple databases of the same kind, without being \ndata.\ndatabases to improve the performance or do things in \"the proper \ndatabase complexity have made this role less common, though it's still in use by \nRelational databases: The default standard in databases.\nNon-relational databases: New alternatives to the traditional databases.\nRelational databases\nThese are the most common databases and the first idea that comes to mind when \ntalking about databases.\nThe relational model for databases was developed in the \n1970s, and it's based on creating a series of tables that can be related to each other.\nEach defined table has a number of fields or columns that are fixed and data is \ndatabase, both in knowing how to access it and ensuring that any \ndatabase.\nData Modeling\nThis creates the relation aspect of the database.\nwhich requires an intermediary table to cross over the data.\nDefining types before having data also \ndatabase not being available for some time, or, in the worst-case scenario, data can \nA query can also be executed that searches for data fulfilling certain conditions.\nVirtually all relational databases are interacted with using Structured Query \ndatabases and follow the same concepts that we've described here.\nhow to query the database and how to add or change data contained there.\ncounter can be handled directly by the database to ensure it is \nRelations in relational databases are really constraints.\nRelational databases come from a strict mathematical background, \nhow can be different in different databases.\nUsing a specific relational database and \nWhile relational databases are very mature and flexible and are used in very \nRelational databases are thought to be a \nscalability of relational databases later in this chapter.\nNon-relational databases\nNon-relational databases are a diverse group of DBMSes that do not fit into the \nThis is used sometimes to set up a local database for running tests \nthat's different from the final database that will be in place once the \nto use specific characteristics for a particular database, making it \nNon-relational databases are also called NoSQL, emphasizing \nData Modeling\nWhile there have been non-relational databases even before the introduction \nof relational databases and alongside them, since the 2000s, there has been an \nMost of them aim to address the two main weak spots in relational databases, \nWide-column databases\nGraph databases\nKey-value stores are arguably the simplest of all databases in terms of functionality.\nno way of querying keys in the system; instead, they need to be an input to any \nare normally based on this kind of data store.\nbetween a cache and a database.\ndata already calculated to speed up its retrieval, while a database \nstores raw data.\nIf the data is not in the cache, it can be retrieved \nfrom a different system, but if it's not in the database, either the \ndata is not stored or there has been a big problem.\ndifferent servers.\ncopies for each key and value, though this makes the retrieval of information slower, \nas the multiple copies need to be compared to detect data corruption.\nSome examples of key-value databases are Riak and Redis (if used with durability \n\"record\" in relational databases.\nThey also typically allow embedding more data \nin subfields, something that relational databases normally don't do, relying instead \non creating a relationship and storing that data in a different table.\nqueries can also be constructed to search fields created in the document.\nSo, in our case, we could retrieve the key (ID) ABCDEFG, like in a key-value store; \nData Modeling\ntheir ID, creating a reference, but normally these databases don't allow you to create \nWide-column databases\nWide-column databases are structured with their data separated by columns.\nThey are a bit more capable of being queried than pure key-value stores but require \nmore upfront design work on what kinds of queries are possible in the system.\nThey are aimed at very big database deployments with high availability and \nSome examples of wide-column databases are Apache Cassandra \nNormally, columns are related and can only be queried in a \nGraph databases\nWhile the previous non-relational databases are based on giving up the ability to \nflexibility), graph databases go in the opposite direction.\nThe query capabilities of graph databases are aimed at retrieving information based \nrelational database (obtain the suppliers of the company and their countries), but \nFigure 3.1: Example of data that is typical of graph databases\nSmall databases\nIt's composed of database systems \nData Modeling\na JSON object into a file and recover it when required, for example, client settings \nFor small amounts of data, this structure may work, but it has the limitation that it's \nSQL database, but it's embedded into the system, without requiring external calls.\nThe database is stored in a binary file.\n>>> con = sqlite3.connect('database.db')\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \nconnect to databases.\nIt aims to standardize access to different database backends.\nThis makes it easy to create a higher-level module that can access multiple SQL \ndatabases and swap them with minimal changes.\nDatabase transactions\nStoring data can be a complex operation internally for a database.\nit can include changing the data in a single place, but there are operations that can \nHow broad and possible these operations are highly depends on the database, \nbut they are very similar to relational databases.\nThis characteristic can become a strong requirement for the database in some \nall restrictions that are defined in the database.\nData Modeling\nIt means that the data is \nMost relational databases have the concept of starting a transaction, performing \nThe need for durability means that data needs to be stored on disk \nway that it can't see new updates, which may require temporary data to be stored \nVirtually all relational databases are fully ACID compliant, and that has become a \nScaling the database with multiple servers or nodes with these properties proves \nThis system creates distributed transactions, running on multiple \nMaintaining full ACID transactions in databases with \nThis way of operating allows creating extra verification steps, as \ninside the transaction, data can still be queried and be validated \nDistributed relational databases\nAs we've discussed before, relational databases weren't designed with scalability \ntransactions, but their preferred way of operating is through a single server.\nrelational databases.\non a single node to make the change, and any non-available elements will catch up \nIt is worth noting that a database server can grow vertically, which \ndatabases.\ndistributed database.\nData Modeling\nBecause the replicas contain the whole database, and the only \nThis system is natively supported by most relational databases, especially the most \nand the replicas are pointed at the primary to start copying the data.\nsolved either by keeping the data temporarily, avoiding the need for the query, or by \ndata is consistent.\ndatabase.\ndifferent location than the replica, while a replica requires a good \nData Modeling\nNote that this way of structuring the database may require adapting the application \nlevel to be aware of all the changes and access to different database servers.\ncovered easily and may require specific changes in the application code.\nqueries that don't require up-to-date information, in cases where perhaps a daily \nIf the application has a higher number of writes, the primary-replica structure may \nThis means dividing the data \ninto different databases according to a specific key, so all related data can go to the \nEach of the different partitions is called a shard.\nis horizontal, separating a single table into different servers.\nqueries in a single transaction.\ndata is naturally partitioned, and very bad when queries affecting multiple shards \npartitions between data, so performing cross-shard queries is not required.\nexample, if the data of a user is independent of the rest, which may happen with a \nphoto-sharing application, the user identifier could be a good shard key.\nSome NoSQL databases allow native sharding that will take care of ",
      "keywords": [
        "API",
        "data",
        "API Design",
        "user",
        "Databases",
        "Resource URI",
        "relational databases",
        "username",
        "Model View Controller",
        "HTML interface",
        "Micropost",
        "Bad Request",
        "Found Username",
        "URI",
        "HTML"
      ],
      "concepts": [
        "databases",
        "data",
        "difference",
        "interfaces",
        "likely",
        "api",
        "apis",
        "user",
        "relational",
        "related"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.374,
          "base_score": 0.374,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "databases",
          "data",
          "api",
          "relational",
          "username"
        ],
        "semantic": [],
        "merged": [
          "databases",
          "data",
          "api",
          "relational",
          "username"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2528494896395957,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:03.530074+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "The Data Layer\b",
      "start_page": 111,
      "end_page": 146,
      "summary": "Data Modeling\nAnother property of the shard key is that the data should be ideally portioned in a \nenough distributing of the queries, and having one shard being the bottleneck.\nOn pure shards, the data is all partitioned in shards and the shard key is an input of \nTo ensure that the shards are balanced, each key is hashed in a way that is equally \nIf we have 8 shards, we determine which shard the data is \nThis strategy is only possible if the shard key is always available as input for every \nChanging the number of shards is not an easy task, as the destination for each key is \nWe can create \"virtual shards\" that point to the same server.\n100 shards, and use two servers, initially the virtual shard distribution will be like \nIf the number of servers needs to be increased, the virtual shard structure will change \nThis change to the specific server that corresponds to each shard may require some \ncode change, but it's easier to handle as the shard key calculation won't change.\nEach of the operations requires changing the location of data based on the shard key.\nThis is a costly operation, especially if a lot of data needs to be exchanged.\nSometimes it is not possible to create pure shards and a translation from the input \nis required to determine the shard key.\nlogging in if the shard key is the user ID.\nthat needs to be translated to the user ID to be able to determine the shard to search \nData Modeling\nquery to the shard key.\nFigure 3.5: External tables to translate the input of shard keys\nThis creates a situation where a single shard is responsible for this translation layer.\nthat's not directly the shard key, and that it requires keeping all the information of \nall shards in a single database.\nThis strategy can be used as well to store, directly, what shard key goes to what \nshard, and perform a query instead of a direct operation, as we saw above.\nFigure 3.6: Storing shard keys to shards\nThis has the inconvenience that determining the shard based on the key requires a \nquery in a database, especially with a big database.\nshard of the data in a consistent way, which can be used to adapt the number of \nIf the specific shard, not only the shard key, is stored in this translation table, the \n1.\t Shard key X is assigned to server A in the reference table.\n2.\t Data from server A for shard key X is copied to server B.\ninvolving shard key X is directed to server B yet.\n3.\t Once all the data is copied, the entry for the reference table for shard key X is \n4.\t All queries for shard key X are directed to server B.\n5.\t Data from shard key X in server A can be cleaned.\nthe reference table that can stop or delay the writing of data while the operation \nTable sharding\nAn alternative to sharding by shard key, for smaller clusters, is to separate tables or \nThis means that any query in table X is directed to a specific \nfor unrelated tables, as it's not possible to perform joins between tables in different \na database cluster needs to downscale, as most applications will \nData Modeling\nbetween one or two tables and the rest, for example, if one table stores logs that \nThe data gets stored in multiple servers, so massive amounts of data can be \nstored, without limiting the data that can be stored in a single server\ndeployment will have its problems, sharding requires more work than a \nNative support for sharding is available only in a small number of databases, \nSome queries will be impossible or almost impossible to do once the data is \nThe shard key needs to be selected carefully, as it will have \nproperties, as some operations may need to involve more than one shard.\nsharded database is less flexible.\nAs we've seen, designing, operating, and maintaining a sharded database only \nmakes sense for very big systems, when the number of actions in the database \nFor databases that need to define a schema, the specific design to use is something \nThe best way to start the design of a schema is to draw the different tables, fields, \nand their relationships, if there are foreign keys pointing to other tables.\nThis section will talk specifically about relational databases, as they \nOther databases are \nof a database.\nData Modeling\nEach of the tables can have foreign key relationships with others of different kinds: \nanother table.\nA simple foreign key relationship works in this case, as the Books table will \nFigure 3.8: The key in the first table references multiple rows in the second\nThe reference for the editor in the Books table is \nmodeled as adding all the information into a single table.\nUnder a relational data structure, there's a need for \nIn most cases, the types of fields to store for each of the tables are straightforward, \nFor example, storing a string \nThis extra table may include more information, for example, how \nrelational databases now allow more flexibility in allowing fields \nData Modeling\nThe internal database representation doesn't need to be the same as what's \nFor example, the time stored in the database should \nrestrictive, now the performance improvement is negligible, and storing data \nthe database and using the default time zone of the server \ntimes are stored in the database in UTC.\nFor example, instead of using an integer field to store \nAs we've seen, in relational databases, a key concept is the foreign key one.\nData \ncan be stored in a table and linked to another.\nlimited data can, instead of being stored in a single table, be split in two.\nFor example, let's take a look at this table, initially with the field House as a string:\nEddard Stark\nJaime Lannister\nTo ensure that the data is consistent and there are no errors, the field House can \nThis means that it's stored in a different table, and a FOREIGN KEY \nEddard Stark\nJaime Lannister\nData Modeling\nThis way of operating normalizes the data.\nensures that the data is very consistent and there are no problems, like introducing \nqueries.\nThe data is also more \nCharacter that needs to know the information of the House needs to perform a JOIN \nquery.\nIn the first Characters table, we could generate our query in this way:\nThis query will take longer to execute, as information needs to be compounded \nfrom two tables.\nJOIN from different tables if we add, for example, a PreferredWeapon field and a \nthe Characters table grows in fields.\nIt will also take longer to insert and delete data, as more checks need to be \nNormalized data is also difficult to shard.\nAnother problem is that the database is more difficult to read and operate.\nAlso, complex JOIN queries need to be performed for simple \ndescribing the data in this way.\nuse the Name field on the Houses table.\nEddard Stark\nJaime Lannister\nWe recover our original query, even if the data is \nperform a JOIN query:\nThis trick, anyway, may not avoid the usage of JOIN queries in normal operation.\namount of time that it's taking to perform queries.\nreduce the need to JOIN tables.\nWhere normalizing data \nsplits it into different tables to ensure that all the data is consistent, denormalizing \nregroups information into a single table to avoid the necessity to JOIN tables.\nData Modeling\nFollowing our example above, we want to replace a JOIN query like this:\nEddard Stark\nJaime Lannister\nFor a query similar to this, querying a single table, use something like this:\nTo do so, the data needs to be structured in a single table.\nEddard Stark\nJaime Lannister\nmore space; in a big table with many rows, way more space.\nconcerns for sharding, as now the table can be partitioned on whatever shard key \nunder NoSQL databases, which remove the capability to perform JOIN queries.\nexample, document databases embed data as subfields into a bigger entity.\nData indexing\nproper data from a big table full of information requires performing more internal \nThis leads to creating indexes that allow you to locate data very \nsorted data structure that points to one or more fields of each of the records of the \ndatabase.\nThis index structure is always kept sorted as data in the table changes.\nFor example, a short table may contain this information\nWhile we will describe data indexing in relation to relational \ndatabases.\nData Modeling\nIn the absence of an index, to query what entry has the highest height, the database \nBy creating an index for the Height field, a data structure that is always sorted is kept \nOnce again, if this index doesn't exist, the only way to find these queries is by \nAnother index may be required to cover other fields.\nThe same table \nindices sort the data based on the ordered combination of both fields, for example, \nQuerying in composite indices for only the first part of the index is possible.\nexample, an index of (Height, Name) will always work for querying Height.\nthe database; the SQL query doesn't change at all.\ndetermine how to retrieve the data, and what indexes to use, if any.\nThe primary key of a table is always indexed, as it needs to be a \nIndexes greatly speed up the queries that use them, especially for big tables with \nindexes in a single table will use more space, both in the hard drive and in \nEach time the table changes, all indices in the table need to be adjusted to be \nSmall tables don't really benefit from being indexed.\na full table scan and an indexed search is small if the number of rows is \nthe number of different values that an index contains.\nThe query analyzer needs to run quickly, as determining what the \nto perform a faster query by combining two indices as the data \nData Modeling\nFor example, the Height index in this table has a cardinality of 4.\nImagine a table with a million rows indexed by a field that's the same in all of them.\nNow imagine that we make a query to find a single row in a different field that's not \nwill return every single row in the database.\nFigure 3.11: Returning every single row from a query using an unhelpful index\nthen we need to query them.\nvalue into account to see whether to use the index or not.\nData Modeling\nWe described the different kinds of databases, both relational and non-relational, and \nAs some of the non-relational databases are aimed at dealing \nto scale up relational systems, as that kind of database was not initially designed to \nThe Data Layer\nThe modeling of data when interacting with the application code is as important as \nhow that data is stored in storage.\nIn this chapter, we will describe how to create a software data layer that interacts \nwith storage to abstract the specifics of storing data.\nDriven Design is, how to use an Object-Relational Mapping framework, and more \nLet's start by giving the context of the data design as part of the Model part of the \nThe Data Layer\nAPI Design, the Model layer is the part that's intimately related with the data and \nThe Model abstracts all the data handling.\nThe internal data modeling layer, handling the storage and retrieval of data \nfrom the database.\nThis layer needs to understand the way the data is stored \nThe next layer creates business logic and uses the internal data modeling \nIt's very common to deal with the data layer as a pure extension of the database \nwhich makes good business sense, and the database models, which contain the \nschema of the database.\nThis way, if there's a table, it gets translated into a Model \nthat accesses that table, replicates the fields, etc.\nuser in a table with username, full name, subscription, and password fields.\nA Model can use multiple tables \nFor example, the example of the user above has the following fields in the database \nas columns in a SQL table:\nWe will use a relational database using SQL as our default \nexample, as it is the most common kind of database.\nThe Data Layer\nthe database.\nThis Model transforms the actions from the raw database access to a fully defined \nobject that abstracts the access to the database.\nan object to a table or collection, is called Object-Relational Mapping (ORM).\ncollections or tables in a database, and generating objects in an OOP environment.\nThere are multiple ORM tools available that do the conversion from SQL tables to \nFor example, a low-level access for a query in the \"pens\" table could look like this:\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \naway the differences between different databases, and allows us to retrieve the \nCREATE TABLE statement, we describe the table in code as a class:\nThe operation that in raw SQL is an INSERT is to create a new object and then use \nthe .save() method to persist the data into the database.\nThe Data Layer\nUsing an ORM detaches the database from the code\nIt removes some problems with composing SQL queries, like security issues\nFirst of all, using an ORM detaches the database usage from the code.\nthat a specific database can be changed, and the code will run unchanged.\nbe useful sometimes to run code in different environments or to quickly change to \nuse a different database.\nused in the database backend) to work with the data.\nthe database.\ncomplicated queries that require you to JOIN multiple tables.\nreference to their author that's stored in a different table and stored as a foreign key \nFor each book, make a query to retrieve the author\nWhen the number of books is high, all those extra queries can be very costly.\nThis way, only a single query is generated, which is much more efficient than the \nYou are still required to understand the details of the database to \nThe Data Layer\n>>> query = 'SELECT * FROM Pens WHERE color IN (' + color_list + ')'\nEven worse, if the query is composed using input parameters directly, it can produce \nFor example, let's say that the query presented above is produced when the user \nTo avoid this problem, any input that may be used as part of a SQL query (or any \nthere are limits to the way elements are read for certain queries or results, which \nframework than creating a bespoke SQL query.\ntime to compose the proper SQL query, encode and decode data, and do other \nthere's a good chance that, at some point, a specific, tailored SQL query will need to \nFor example, the following code will work in a similar way \nThe queries \ncreated from the ORM are good for straightforward queries but can \ndatabase in use.\nThe Data Layer\n>>> query\n# Execute the query in a similar way, it will handle all \nin the table for simplicity, but this is not the correct way of addressing them and \nNew columns can be added to a table, so retrieving all columns may change the \nAn ORM will handle this case automatically, but using raw SQL requires you to take \nqueries.\nstatements, in certain cases dynamic queries are still very useful.\nEven if the selected way to access the database is raw SQL statements, it's good to \nlayer should be responsible for storing data, in the proper format in the database, \nIn some cases, any query that's not a stored query \n(a query stored in the database itself beforehand and called with \nThe Data Layer\nand the database table is direct, for example, a user object, this is fine.\nthe data\nAs we've seen before, ORM frameworks directly translate between tables in the \ndatabase and objects.\nThis creates a representation of the data itself, in the way it's \nstored in the database.\nIn most situations, the design of the database will be tightly related to the business \nthe data, as it's stored inside the database.\nmultiple database operations.\nWhile it is possible to directly implement an Account table \nIf the database allows for it, all the operations in a unit of work \ntightly associated with transactions and relational databases and \nnormally is not used in databases that are not capable of creating \nIf we also add an extra Log entry, in a different table, for keeping track \ncorrespond to tables in the database.\n''' This is the model related to a DB table '''\n''' This models stores the operations '''\nThe Data Layer\ndatabase but keeps a relation to the InternalAccount using the unique reference of \nWhenever there's an operation, it requires another account, and then a new Log is \nAccount model is the Unit of Work class.\nThe Data Layer\ndatabase implementation and store any relevant business logic there.\nSometimes a simple CRUD model for the database is not descriptive of how the data \nways to read the data and to write or interact with the data.\nA possibility is that sending data and reading it happen at different ends of a \nthis data is processed or aggregated in a different database.\nFinally, a relational database \nexternal process to the relational database, where it is then represented with a \nrelational model in an ORM way, and then back to the Domain Model.\nmeaning that the Command (write operations) and Query (read operations) are \ndata.\nThe Domain Model may require different methods to deal with the information.\nThe input and output data has a different internal representation, and sometimes it \nIn certain cases, the models and data may be quite different for read and write.\nIn our example, that process would be how the data is stored in the ",
      "keywords": [
        "Data",
        "shard key",
        "database",
        "Data Layer",
        "Data Modeling",
        "shard",
        "query",
        "SQL",
        "ORM",
        "key",
        "data modeling layer",
        "tables",
        "layer",
        "Characters JOIN Houses",
        "SQL query"
      ],
      "concepts": [
        "queries",
        "query",
        "data",
        "tables",
        "database",
        "modeling",
        "operation",
        "operating",
        "operate",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.422,
          "base_score": 0.272,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.418,
          "base_score": 0.268,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.381,
          "base_score": 0.231,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "table",
          "shard",
          "database",
          "query"
        ],
        "semantic": [],
        "merged": [
          "data",
          "table",
          "shard",
          "database",
          "query"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22340991608296396,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530090+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "The Twelve-Factor App Methodology\b",
      "start_page": 147,
      "end_page": 172,
      "summary": "# Create a new sale\nDatabase migrations\nWhile the pace of changes in the database is typically not as fast as other areas, there \nData changes are roughly categorized into two different kinds:\nFormat or schema changes: New elements, like fields or tables, to be added \nor removed; or changes in the format of some fields.\nData changes: Requiring changing the data itself, without modifying the \nFor example, normalizing an address field including the zip code, \nThe basic principle related to changes in the database is backward compatibility.\nThis means that any single change in the database needs to work without any change \nin the code.\nThis allows you to make changes without interrupting the service.\nIf the changes in \nthe database require a change in the code to understand it, the service will have to \nThis is because you can't apply both changes at the same time, and if \nDepending on the database, there are different approaches to data changes.\nFor relational databases, given that they require a fixed structure to be defined, \nany change in the schema needs to be applied to the whole database as a single \nFor other databases that don't force defining a schema, there are ways of updating \nRelational schema changes\nIn relational databases, each individual schema change is applied as a SQL statement \nThe schema change, called a migration, can happen \nMigrations are SQL commands that perform changes in an atomic way.\ninvolve changing the format of tables in the database, but also more operations like \nsupport to create migrations and perform these operations natively.\nFor example, Django will automatically create a migration file by running the \ndetect any change in the models and make the proper changes.\nthe migration.\nMigrations for 'example':\nexample/migrations/0002_auto_20210501_1843.py\nchanges creating the proper migration files.\nThe pending migrations can be applied \nautomatically with the command migrate.\nApply all migrations: admin, auth, contenttypes, example, sessions\nRunning migrations:\nFor more details about Django migrations, check the documentation at https://\nChanging the database without interruption\nThe process to migrate the data, then, needs to happen in the following order:\n1.\t The old code and the old database schema are in place.\n2.\t The database applies a migration that's backward compatible with the old \ncode.\nAs the database can apply this change while in operation, the service is \n3.\t The new code taking advantage of the new schema is deployed.\nThe critical element of this process is step 2, to ensure that the migration is backward \nMost of the usual changes are relatively simple, like adding a new table or column \nThe old code won't make use of \nBut other migrations can be more \nDjango will store in the database the status of the applied \nKeep in mind that, to properly use migrations through Django \nIf you need to apply changes that \ncan't be replicated automatically with a change in the model, like a \ndata migration, you can create an empty migration and fill it with \nof the automatically created Django migrations.\nBut obviously, a change that migrates the code from an integer to a string is going to \n1.\t The old code and the old database schema are in place.\n2.\t The database applies a migration adding a new column, Field2.\nmigration, the value from Field1 is translated into a string and copied.\n3.\t A new version of the code, intermediate code, is deployed.\n4.\t A new migration removing Field1, now unused, can be applied.\n5.\t The new code that is only aware of Field2 can now be deployed safely.\na further migration is deployed changing the name from Field2 to Field1.\ncase, the new code needs to be prepared in advance to use Field2 or, if not present, \nthe application of the migration and the new code, the \ncode will need to check if the column Field1 exists, and if \napplied – if the value in Field1 is different from the one \nA new deployment could be done after that to use only Field1 again:\nFigure 4.3: Migrating from Field1 to Field2\nperform the migration with the format change in Field1, and then start the new \ncode.\nproblem is testing the migration in a database much smaller than the production one.\nThis can create an unexpected problem when running in production, taking much \nDepending on the size of the data, a complex migration may \nBut another problem is the risk of introducing a step, at the start of the new code, \nthat can have problems and bugs, either related to the migration, or unrelated.\nWith this process, after the migration is applied, there's no possibility of using the \nIf there's a bug in the new code, it needs to be fixed and a newer version \npiece of code is less risky than changing two without being able to revert either of \nFor example, a sharded database will need to apply \nIf possible, try to test the migrations of the system with a big \nenough test database that's representative.\nIt's possible that some migrations may need to be \neven possible in some cases that the database will require more \nmemory to allow the migration to run in a reasonable amount of \nMigrations may be reversible, as there could be steps that \nmigration like removing a column is effectively not reversible, as \nThis way migrations need to be applied very carefully and by \nData migrations\nData migrations are changes in the database that don't change the format but change \nThese migrations are produced normally either to correct some error in the data, like \nIn cases like the scale change described above, the process may require more steps \n1.\t Create a migration to set a new column, scale, to all rows, with a default \nAny new row introduced by the old code will automatically \n2.\t Deploy a new version of the code able to work with both inches and \n3.\t Set up another migration to change the value of measurement.\n4.\t Now all the values in the database are in centimeters.\n5.\t Optionally, clean up by deploying a new version of the code that doesn't \nAfter that, a new migration removing the column can also be run.\nAs we discussed before, the key element is to deploy code that's able to work with \nboth database values, the old and the new, and understand them.\nChanges without enforcing a schema\nThis means that, instead of an all-or-nothing change as for relational databases, a \nhere, the code will have to perform the changes over time.\n1.\t The old code and the old database schema are in place.\n2.\t Each of the documents in the database has a version field.\n3.\t The new code contains a Model layer with the migration instructions from \nenough time, it will migrate, document by document, the whole database.\nconcatenated, migrating an old document from different versions \nversion 0 and be migrated to version 1, now including the field.\nmigrate from version 1 to 2, version 2 to 3, etc, if still present in the code.\nupdating and saving it until the whole database is migrated.\nWhile there's still data in the database with the old version, the code needs to be \nto migrate all the data in the background, as it can be done document to document, \nmigration is done, the code can be refactored and cleaned to remove the handling of \nThis process is similar to the one described for data migration, \nthough databases enforcing schemas need to perform migrations \nto change the format.\nIn a schema-less database, the format can be \nchanged at the same time as the value.\nIn the same way, a pure data change, like the example seen \nbefore where it was changing the scale, can be performed without \nthe need for a migration, slowly changing the database as we \nDoing it with a migration ensures a cleaner change, \nthough, and may allow a simultaneous change in format.\nORM frameworks can generate the proper SQL commands to create the database \nthat we can create the ORM Model in code and the ORM framework will make the \nBut sometimes, we need to work with an existing database that was created \nthis case, we need a way to detect the existing schema and use it.\nfields and any new changes.\nIn this scenario, we need to create a Model that \nDetecting a schema from a database\nto automatically detect the schema of the database and work with it.\nThis way of describing the schema in code is called declarative.\n>>> engine = create_engine(\"sqlite:///database.db\")\nThis creates a models.py file that contains the interpretation of the database based on \ncontrol over the code, require a different approach.\nIn other situations, there's a legacy database that was created by a method that \ncode may use the database, but we want to migrate the code so we are up-to-date \nand formats are, and on another, allow the ORM to make controlled changes to the \nWe will see the latter as migrations.\nThe challenge in this case is to create a bunch of Models in the ORM framework that \nIf the database has stored procedures, they need to be either removed or \nStored procedures are code functions inside the database \nlike inserting a new row or changing a column.\nwithout the capacity to change the data that is stored.\nexisting stored procedures into external code is a process \nin the long run that can create problems.\nindex name needs to be changed externally to the compatible name.\nthe Django ORM, which may require you to create a new numeric column to \ncreated schema based on the code Models in the ORM framework can be produced \n1.\t Create a dump of the database schema.\n3.\t Create a single migration with all the required changes for the database.\nThis migration is created normally, with makemigrations.\nthat will be applied by the migration.\nThis generates a database schema that \nthe migration file each time to generate it from scratch.\nmetadata set to not track changes in the database.\nchanges as migrations.\nchanged.\nAfter that, changes can be applied normally by changing the Models and then \nWe described different useful techniques for the code to \ninteract with the database, like the Unit of Work pattern, which is related to the \nWe also discussed how to deal with database changes, both with explicit migrations \nthat change the schema and with more soft changes that migrate the data as the \nFinally, we described different methods to deal with legacy databases, and how to \nChanges to \ndifferences that are not creating any problems.\ntools and techniques to use in both cases, including migrating from one to the other.\ndevelopment environment and while running the tests in a CI system.\nConfiguration allows the same code \nof tests with each new code submission.\nrepository for code, so tests are launched as soon as new changes are forthcoming \nother services that integrate with them and allow operations to be run automatically \ntools to run the tests; for example, a particular version of Python and a \nrun at the same time as there is no dependency between the cases, whereas step 2 \nthe steps need to be able to be run automatically, without any manual intervention.\nexample, elements like databases or other dependencies, if required for tests, need to \nthat allows a database to start up so that it's available in the \nThe external storage source will typically be a database, but it's also common to use \nwith information from the database to return it in the request is \nelements, such as databases and caches, require a different way of operating, as they \nOne of the basic ideas of the Twelve-Factor App is that the code is unique, but it \nThe use of different environments allows testing environments to be set up, where \nAWS S3 is a web service that allows a file to be stored and retrieved \nThis service is very useful for working with files in a scalable way, ",
      "keywords": [
        "Database",
        "code",
        "Sale",
        "Data",
        "migration",
        "migrations",
        "Data Layer",
        "sale return self.",
        "Twelve-Factor App Methodology",
        "system",
        "Twelve-Factor App",
        "ORM",
        "Create",
        "App Methodology",
        "schema"
      ],
      "concepts": [
        "migration",
        "migrate",
        "migrating",
        "changing",
        "changes",
        "databases",
        "code",
        "field",
        "different",
        "differences"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.474,
          "base_score": 0.324,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.368,
          "base_score": 0.218,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.367,
          "base_score": 0.217,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "migration",
          "migrations",
          "database",
          "schema",
          "changes"
        ],
        "semantic": [],
        "merged": [
          "migration",
          "migrations",
          "database",
          "schema",
          "changes"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23900873769065972,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530107+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Web Server Structures\b",
      "start_page": 173,
      "end_page": 218,
      "summary": "Operational configuration: These are parameters that connect different parts \nthe application doesn't change; for example, a change to log only WARNING \nThese parameters are under the control of operations and are normally \nFeature configuration: These parameters change external behavior, enabling \nCreating a comprehensive and easy-to-use local environment is \nsingle service or process, such as a web server, it is relatively easy \nWhile the operational configuration parameters are tightly related to a single \nenvironment and require parameters that are correct for the environment, the feature \nTraditionally, the configuration has been stored in one or more files, typically \nMaking a configuration change is, de facto, a code change.\nStoring all the data in the code base makes it more difficult to create \n\"business release\" at a particular time, deploying new code into a \ncan use these credentials to access all environments, including production.\nfiles inside the code base.\nspecifically in the Configuration factor.\n6.\t Processes.\nExecute the app as a stateless process.\nSet up the services as processes.\nRun one-off admin processes independently.\nCode base, Build, release, run, and Dev/prod parity work around the idea \nof generating a single application that runs in different environments, \nconfiguration and connectivity of different services\nLogs and Admin processes are practical ideas involved with monitoring and \none-off processes\nBuild once, run multiple times\ncode that's changed from one version to another, just configurable options.\nThe aim of the Code base factor is that all the software for an app is a single repo, with \nThis means that the code to deploy is always the same, and only the configuration \nThis allows easy testing of all the configuration changes and does not \nA single code base allows a strict differentiation of the stages in the Build, release, run \nthem change based just on the configuration.\nconfiguration for the selected environment, and sets it ready for execution\nThe run stage finally executes the package in the selected environment\nBecause stages are strictly divided, it's not possible to change the configuration or the \nthat the run stage may need to be executed again in case there's a new server or the \nconfiguration in other factors.\nproduction one, as they use the same building stage, but with proper configuration \nThis factor also makes it possible to use the same (or as close as \npossible) backing services, like databases or queues, to ensure that local development \nenvironments that contain all the required dependencies.\nObtaining a fast and easy process to develop, build, and deploy is critical for \ncan then be separated by environment, something that makes \nsense, as some environments, like production, are more critical \nStoring the configuration as part of the code base \nPerforming tests after the build stage also ensures that the code \nThat is why, in the Config factor, it talks about storing all the configuration for the \nBuild, release, run factor and avoidance of the problems that we described previously \nin storing them in files inside the code base.\nThis is preferred to other alternatives, such as setting different files into the code \nbase describing environments like staging or production, because they allow more \nchanging the code for environments that are not affected; for example, having to \nConfiguration can be obtained in configuration files directly from the environment \nPARAMETER = os.environ['PATH']\nThis code will store in the constant PARAMETER the value of the PATH environment \nwork means that there are a limited number of environments and \nis storing it in a different place to the code base, managed only on \nPARAMETER = os.environ.get('MYENVVAR', 'DEFAULT VALUE')\nin a different format, it needs to be converted, for example:\nNUMBER_PARAMETER = int(os.environ['ENVINTEGERPARAMETER'])\nenvironment variables need to be defined in your environment.\nYou can run Python, adding a local environment, by running $ \nThis makes configuration problems \nAs part of this configuration, any backing services should be defined as well as \nBacking services are external services that the app uses over \nconfiguration, can be changed based on the environment.\nand the database needs to be moved between two networks, we can start the new \ndatabase, perform a new release with a configuration change, and the app will point \nTo allow the concatenation of multiple applications, the Port binding factor ensures \nThis makes it easy to consider each app a backing service.\nFor applications, use HTTP over port 80 when possible.\nSome applications require the combination of several processes working in \nFor example, it is typical for a web server for a Python application, such \nas Django, to use an application server like uWSGI to run it, and then a web server \nlike nginx or Apache to serve it and the static files.\nFigure 5.2: Connecting a web server and application server\ntightly as possible, to avoid the problem of different versions of dependencies being \nFor example, in a pip file, a dependency can be described in different ways:\nrequests\nThe Processes factor talks about making sure that the run stage consists of starting \none or more processes.\nmeaning that all the data needs to be retrieved from an external backing service like \nThe processes \nneed to be able to be started and stopped quickly, and at any time.\nshould be to take not more than a few seconds to have the process up and running.\nFor example, a file upload may use the local hard drive to store \nprocessed, the file should be deleted from the disk.\nThe opposite is to allow the graceful shutdown of the process.\nFor example, for a web request, a graceful shutdown first will curtail the acceptance \ndown the process.\nWeb requests are typically quick to answer, but for other \nprocesses, such as long asynchronous tasks, it may take a long time to stop if they \nBecause the system is created through processes, based on that, we can scale out by \nProcesses are independent and can be run at the same time on \nthe same server or others.\nWorking with Docker containers automatically uses this \nIf the process doesn't \nBe sure that the main process for the container can receive SIGTERM \nprocesses may be longer than for web servers.\nKeep in mind that the same application can use multiple processes that coordinate \namong them to handle different tasks and each process may have a different number \nIn our previous example above, with an nginx server and uWSGI one, \nthe optimal number may be to have a single nginx process for many more times the \nnumber of uWSGI workers.\noperation under a Twelve-Factor App. That allows the size of the entire operation to \nnew nodes as the system grows in load and requests, or it can be done automatically, \nThe Twelve-factor App processes should also be run by some sort of operating \nprocesses remain running, even in the event of a crash, handle graceful manual \nThe traditional deployment process was to set up a physical server \nWith containers, this process is somehow reversed.\noptimization process is still required, with containers, it's more \nknowing that we can use different server sizes or add more servers \nAllow time to perform tests to \nRestarting the processes automatically, combined with a quick start up time and \nLogs are text strings that provide visibility of the behavior of a running app.\ngenerated as the code is being executed, giving information on the different actions \nbut typically frameworks will automatically create logs based on common practices.\nFor example, any web-related software will log requests received, something like \nis mostly handling containers more than processes.\noperating system process manager, the work is performed by a \nthe processes can start without being under the control of a \nThe container will stop if the process is stopped.\nThis kind of log is called an access log and will be generated in different formats.\nstatus code, but it can be configured to return extra information, such as the IP of the \nclient making the request, or the time that it took to process the request.\nApplication logs are also very useful.\nApplication logs are generated inside the code and can be used to communicate \nWeb frameworks prepare the logs, so it's easy to \nFor example, in Django, you can create logs this way:\nAccess logs are also generated by web servers including nginx \nThe Logs factor suggests that logs shouldn't be managed by the process itself.\nThe environment surrounding the process, like the operating system process \nproblem of requiring the logs to be rotated and ensure that there's enough space.\nThis also requires the different processes to coordinate in terms of having a similar \nrequests per hour, and even creating automatic alerts based on certain rules, such \nThe Admin processes factor covers some processes that sometimes need to be run for \nenvironment as the regular processes, using the same code base and configuration.\nThese admin operations should be included as part of the code base to avoid \nIn traditional environments, it may be necessary to log in to a server through ssh to \nallow the execution of this process.\nstarted exclusively to execute the process.\none that runs the application, but called with a different command, so the code and \nenvironment are the same as in the running application.\nthat an error in a production environment can create a serious \nimage that then gets run works very well with the Build, release, run factor and with \nIncluding the build process as part of the \nEach container also works as a Process, which allows scaling by creating multiple \nThe concept of containers makes them easy to start and stop, leaning into the \nas Kubernetes makes it easy to also set up the Backing services factor, and it's also \nIn Docker and orchestrator tools like Kubernetes, it is very easy to set up different \nThis environment configuration, as well as a description of the cluster, can \nbe stored in files, which allow multiple environments to be created easily.\nenvironment, with only small changes in its configuration.\nensuring that the different environments are kept up to date, as demanded by the \nSending information to standard output as per the Logs factor is also a great way \nto store logs as container tools will receive and deal with or redirect those logs \nFinally, the Admin processes can be handled by launching the same container image \nrecommendations for the Twelve-Factor App, as the tools work in the same direction.\nservices that need to be run in the cloud.\nFor web services living in the cloud, we can use the \nCI is the practice of constantly validating any new code by running tests automatically \nWe saw the challenges for configuration, something that the Twelve-Factor App \nalso deals with, and how not every configuration parameter is equal.\nconfiguration, which can help divide and give the proper context to each parameter.\nBuild once, run multiple times, based on the idea of generating a single \npackage that runs in a different environment\nand concepts allow us to easily create Twelve-Factor Apps.\nWeb Server Structures\nWeb servers are the most common servers for remote access at the moment.\nIn this chapter, we will see how web servers are structured, starting by describing \nhow the basic request-response architecture works, and then diving into a LAMP-\nstyle architecture in three layers: the web server itself, the workers executing \nstandardized connection to the web server.\nweb server, uWSGI for the intermediate layer, and the Python Django framework for \nWeb servers\nWeb Server Structures\nLet's start by describing the basis of the request-response architecture.\nThe classical server architecture is heavily based on request-response to \nA client sends a request to a remote server and the server processes it \nAn important element is the time delay between the sending of the request and the \nThis time difference makes it important to handle it properly.\nAnother characteristic of the request-response pattern is that a server cannot call \nthe server only needs to listen for new requests coming.\nFor example, a common request to retrieve \na web page will make one request to retrieve the page and later \nWeb Server Structures\nA crude example of this is a message server implemented only in request-response.\nRequest any new message addressed to them\nEven with these limitations, request-response architecture is the basis of web services \nnew requests makes the architecture simple to implement and quick to evolve, and \nopen, allowing the server to notify the user of new information.\nbase for the web server architecture:\nserver and web worker.\nFrom the point of view of an incoming request, a web request accesses the different \nWeb servers\nThe web server exposes the HTTP port, accepts incoming connections, and redirects \nThe web server \ncan directly serve a request, for example, by directly returning static files, permanent \nWeb Server Structures\nThe primary objective of the web server in the presented architecture is to work as a \nmain/chapter_06_web_server/nginx_example.conf.\nserver {\nfor example, to define different behaviors based on the DNS \nNext, we define where the static files are, both in terms of the external URL, and \nNote the static location needs to be defined before the reverse proxy: \nhandy to detect problems with static files while running in test \nWeb Server Structures\nIt's important in production environments to serve static files directly from the web \nserver, instead of doing them further along the line with the Python worker.\nthis is possible, and a common case when working in a development environment, \nserver is optimized to serve static files.\nproduction through a web server.\nAn alternative is to use an external service to handle files, like AWS S3, that allows \nThe files then will be under a different URL than the service, \nThe service URL is https://example.com/index\nThe static files are in https://mybucket.external-service/static/\nThis way of operating requires you to push the code to the external service as part of \na different path, so static files between deployments are not confused.\ncontent is served from https://mybucket.external-service/static/v1/.\nThe calls to the service, like https://example.com/index, return all their static \nlike a single-page application, changing the static files effectively can be a new \nFor example, adding an optional parameter in any call to overwrite the returned \nCalling https://example.com/index returns the default version, for example, \nCalling https://example.com/index?overwrite_static=v3 returns the \ntest in any environment, including production.\nrequire a lot of versions with very big files to really start to worry \nWeb Server Structures\nThe data can be distributed internally between the different servers from the \nIn any case, using an external service to store the static files will, obviously, remove \nthe need to configure the web server for them.\nLet's continue describing the web server configuration.\nfiles, we need to define a connection to the backend, acting as a reverse proxy.\nA reverse proxy is a proxy server that can redirect a received request towards one or \nIn our example, the backend is the uWSGI process.\nFor example, we have a service where their servers \ncompany has servers in Japan that store a copy of the static content.\nlatency than if the request had to reach a server in Europe, more \nThe web server will be able to communicate with the backend in multiple ways, \nThe socket needs to be coordinated with the way uWSGI is configured.\nsee later, the uWSGI process will create it:\nrequest for a /static request gets detected before checking for / and it's properly \nproxy is only capable of working with web requests.\ndistributing requests across different servers, it can also add some \nfeatures like caching, security, SSL termination (receiving a request \nin HTTPS and connecting to other servers using HTTP), or, in this \nparticular case, receive a web request and transfer it to through a \ndifferent servers, while UNIX sockets are designed to communicate \ncommunication inside the same host and they work like a file, \nallowing you to assign them permissions to control what process \nWeb Server Structures\nThe core of the reverse proxy configuration is the uwsgi_pass clause.\nIn this case, we are adding the Host header, with information about the requested \nIn our configuration, we only use a single backend, as uWSGI will balance between \ndifferent workers.\nserver 192.168.1.117:8080;\nserver 10.0.0.6:8000;\nLater, define the uwsgi_pass to use the cluster.\nuwsgi_params is actually a defined file included by default in \nelements like SERVER_NAME, REMOTE_ADDRESS, etc.\nsingle request can pass through multiple proxies, and each of them \nThere are two different logs that \nnginx (and other web servers) produces:\nError log: The error log tracks possible problems from the web server itself, \nlike not being able to start, configuration problems, etc.\nThis requires nginx to not start as a daemon process, or if it is, capture the standard \nThis directs all the logs into a centralized server that captures the \nerror_log syslog:server=syslog_host:514;\naccess_log syslog:server=syslog_host:514,tag=nginx;\nWeb Server Structures\ntesting, choosing a backend server based on geolocalization of the requester, etc.\nrequests from nginx and redirects them into independent Python workers, in WSGI \nuWSGI will also start and coordinate the different processes, handling the lifecycle \nworkers receiving the requests.\nuWSGI is configured through a uwsgi.ini file.\nblob/main/chapter_06_web_server/uwsgi_example.uni.\nprocesses=1\nWeb Server Gateway Interface (WSGI) is a Python standard to \ndeal with web requests.\nweb servers like Apache and GUnicorn) and from the receiving \nend (virtually every Python web framework, like Django, Flask, or \nInside this file is the definition of the application function, which uWSGI can use to \naddress the internal Python code, in a controlled way.\ndef application(environ, start_response):\nThe first parameter is a dictionary with predefined variables that detail the request \n(like METHOD, PATH_INFO, CONTENT_TYPE, and so on) and parameters related to the \nprotocol or environment (for example, wsgi.version).\nThe second parameter, start_response, is a callable that allows you to set up the \nWeb Server Structures\ntakes some time to process but can be returned without being \nIn any case, the WSGI file is normally created by default by whatever framework is \nFor example, a wsgi.py file created by Django will look like this.\napplication function, and connect it with the rest of the defined code – a great \nInteracting with the web server\nThe socket parameter creates the UNIX socket for the web server to connect to.\nwas discussed before in this chapter, when talking about the web server.\nuWSGI also allows you to use a native HTTP socket, using \nthis option if the web server is not on the same server and needs to \nthe web server, use the option http instead of http-socket, which \nWeb Server Structures\nProcesses\nThe next parameters control the number of processes and how to control them:\nprocesses=1\nThe master parameter creates a master process that ensures that the number of \nworkers is correct, restarting if not, and deals with the process lifecycle, among other \nThe processes parameter is very straightforward and describes how many Python \nThe way uWSGI generates new processes is through pre-forking.\na single process gets started, and after the application is loaded (which may take a \nfor new processes, but at the same time, relays that the setup of the application can \nChoosing the right number of processes is highly dependent on the application \nmultiple cores will be able to run more processes efficiently.\nCPU usage in the application will determine how many processes can be run by the \nparameter lazy-apps will make each worker start from scratch, \nallowing the core to switch to another process meanwhile.\nAn important detail about the created processes is that they deactivate the creation \napplications, there's no need to create independent threads inside each of the \nworkers, and that allows you to deactivate the Python GIL, speeding up the code.\nDuring the time of operation, processes won't stay static.\napplication will need to reload with new code changes regularly.\nparameters are related to how processes are created and destroyed.\nmax-requests specifies the number of requests to be processed by a single worker \nand create another worker from scratch, following the usual process (fork by default, \nallows a single thread to have control of the Python process.\nThis means that, inside a single process, no two threads can run \nWeb Server Structures\nRemember that, based on the Twelve-Factor App, web workers need to be able to be \nuWSGI will also recycle the worker when it's idle, after serving its 5,000th request, so \nrequest, a stampede problem can be created where one after another all the workers \nTo avoid this problem, use the max-requests-delta parameter.\nThis makes the recycling happen at different times, increasing the number of \nrequests.\nIn our example configuration, there's only a single \nworker defined in processes.\nThe master-fifo parameter creates a way to communicate with uWSGI and send \nprocesses and the whole uWSGI.\nis being processed, and when a worker has finished its request, stopping it in an \nFinally, when all workers are done, stop the uWSGI master process.\nto make the different keys expire at slightly different times to avoid \nWeb Server Structures\nIt will also load any new configuration related to uWSGI itself.\nthe time of the operation, the internal uWSGI listen queue may be filled up, causing \nserver, depending on the number of workers and the startup procedure.\nReloading a single server under load may be complicated.\nservers simplifies the process.\ndance, creating copies of the uWSGI configuration in multiple servers and then \nreload the uWSGI configuration but will do with code changes \nThis is common in cloud environments where an extra server can \nrequests from uWSGI after they're routed by the external web server, etc.\nEach framework will interact in a slightly different way with the requests, but in \nWe will use Django as an example.\nabout uWSGI but also about how the whole web stack works.\nWeb Server Structures\nThe View receives the HTTP request and processes it, interacting with the \ndifferent databases, including ones not supported natively by Django (like NoSQL \npath('example/<int:parameter>/<slug:other_parameter>', second_view)\ntransform the defined parameters properly and pass them over to the view.\nexample, the URL example/15/example-slug will create these parameters:\nThere are different types of parameters that can be configured.\nyou need to use wildly different tools, a good alternative can be \nWeb Server Structures\nre_path('example/(?P<parameter>\\d+)/', view)\npath('example/<str:parameter>/', first_view)\npath('example/<int:parameter>/', second_view)\nexample/<int:parameter>/, the new path-defined URL patterns \nany parameters from the URL, and processes it.\nrequest.\npaths, but other distinctions like HTTP method or parameters will need to be \nwith a single parameter, the structure will be similar to the following example: \ndef example_view(request):",
      "keywords": [
        "Web Server Structures",
        "web server",
        "server",
        "Twelve-Factor App Methodology",
        "Twelve-Factor App",
        "web",
        "Server Structures",
        "App",
        "App Methodology",
        "Configuration",
        "code",
        "environment",
        "Processes",
        "process",
        "uWSGI"
      ],
      "concepts": [
        "likely",
        "process",
        "processes",
        "processed",
        "server",
        "different",
        "difference",
        "requests",
        "request",
        "environment"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 5,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.429,
          "base_score": 0.279,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.344,
          "base_score": 0.194,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.309,
          "base_score": 0.309,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "server",
          "processes",
          "web",
          "web server",
          "uwsgi"
        ],
        "semantic": [],
        "merged": [
          "server",
          "processes",
          "web",
          "web server",
          "uwsgi"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2904106140364923,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530123+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Event-Driven Structures\b",
      "start_page": 219,
      "end_page": 254,
      "summary": "def process_data(parameters, form_content):\ndef example_view(request):\nif request.method == 'POST':\ncontent = process_data(request.POST, form_content)\nNote that the display_form function gets called both from example_view and also \nThe key element for passing information is the request parameter.\ntype is HttpRequest, and contains all the information that the user is sending in the \nrequest.\nexample, a request such as:\nThe process will be to create a form with the parameters from the \nWill produce a request.GET value like this:\nit will be filled first by the body of the request, to allow encoding form posts.\ncontent_type with the MIME type of the request.\nrequests.\nIf you need to access all values, use the method getlist:\nThere are also some useful methods to retrieve information from the request, for \nThis method is useful to create full references to return them.\nrequest, allow you to retrieve all the relevant information necessary for processing \nthe request and call the required Models.\nThe HttpResponse class handles the information being returned by the View to the \nThe return from a View function needs to be an HttpResponse object.\ndef my_view(request):\nreturn HttpResponse(content=\"example text\", status_code=200)\nresponse = HttpResponse(content=body)\nthat way, and can be useful for sending big responses over time.\ndef my_view(request):\nreturn HttpResponse(content=\"example text\", status_code=HTTPStatus.\nThe content parameter defines the body of the request.\nthe case, a content_type parameter should be added to adequately label the data \nresponse = HttpResponse(content=img_data, headers=header)\nHttpResponse, for JSON encoded requests, it's better to use JsonResponse, which will \na file-like object and directly filling the headers and content type, including if it needs \ndef my_view(request):\nA typical example of middleware is logging each received request in a standard \nThe middleware will receive the request, produce a log, and hand the \nIt will then fill the request.user object with the \nrequests.\nMiddleware can access the request both when it's received and the response when \nreceived request can generate it before the request is sent to the View.\nLogging middleware that also logs the status code needs to have the \ninformation of the status code, so it will need to do it once the View is \nLogging middleware that logs the time it took to generate the request will \nneed to first register the time when the request was received, and what time \ndef example_middleware(get_response):\ndef middleware(request):\nresponse = get_response(request)\nfinal_response = chain(request)\nrejected request (for example, not adding a proper CSRF) won't be logged.\nThe basic principle behind Django REST framework is to create different classes that \nWe first need to introduce the models to store the information.\nModel stores a string of text and the user that created the micropost.\nYou can use them to create \"virtual objects\" \nWith this information, we create two different views, one for each URL that we need \nWe need to obtain the username and user from the same self.kwargs to be sure to \nThe new object is created combining both the user and the rest of the data, added as \nThe href field requires an extra defined class to create a proper URL reference.\ndef get_url(self, obj, view_name, request, format):\nresult = reverse(view_name, kwargs=url_kwargs, request=request,\nbalancers are not required, and the edge load balancer can handle multiple web \nnumber of requests that it can take.\nof requests.\nWe started by describing the fundamental details of the request-response and web \nnginx as the front web server and uWSGI to handle multiple Python workers that \nWe started with the web server itself, which allows you to serve HTTP, directly \nWe described how Django works to define a web application, and how the requests \nand responses flow through the code, including how the middleware can be used \nway to create RESTful APIs and show how our example introduced in Chapter 2 \ncan be implemented through the views and serializers provided by Django REST \nRequest-response is not the only software architecture that can be used in a system.\nThere can also be requests that don't require an immediate response.\nno interest in a response, as the task can be done without the caller being required \nThis message is called an event, and there are multiple uses for this kind of system.\nof the most popular uses of it: creating asynchronous tasks that are executed in the \nbackground while the caller of the task continues uninterrupted.\nIn the chapter, we will describe the basics of asynchronous tasks, including the \ndetails of queueing systems and how to generate automatically scheduled tasks.\nWe will use Celery as an example of a popular task manager in Python that has \ntasks.\ninterface, including sending new tasks to execute.\nAsynchronous tasks\nSubdividing tasks\nScheduled tasks\ndata and waiting until the other part returns a response, it just sends data and \nThis makes it different from the request-response architecture that we saw in the \nA request-response process will wait until an appropriate response \nInstead, an event containing the request will be sent, and the task will just continue.\nThe difference is that the task itself won't be done in the same moment, so getting \nEvent-driven systems can be implemented with request-response \nThis doesn't make them a pure request-response system.\nFor example, a RESTful API that creates an event and returns an \nan identifier to be able to check the status of any follow-up tasks.\nThis new system is listening to the queue and extracts all the received events to \ndatabases or exposed endpoints, and can even send more events into queues to \nMultiple subscribers can tend the same queue, and they'll be extracting events \nMultiple publishers can also produce events into the same queue.\nThe capacity of the queue will be described by the number of events that can \nwith these tools to create your own way of handling sending messages.\nAsynchronous tasks\nA simple event-driven system is one that allows you to execute asynchronous tasks.\nThe events produced by an event-driven system describe a particular task to execute.\nNormally, each task will require some time to execute, which makes it impractical to \nThe typical example is a web server that needs to respond to the user in a reasonable \nThe solution is to send an event to handle this task, generate a task ID, and return the \ntask ID immediately.\nThe back-end system will then execute the task, which can take as \nMeanwhile, the task ID can be used to monitor the progress of the execution.\nback-end task will update the status of the execution in shared storage, like a \ndatabase, so when it's completed, the web front-end can inform the user.\nBecause the status of the task is stored in a database that's accessible by the front-end \nweb server, the user can ask for the status of the task at any point by identifying it \nthrough the task ID.\nThese operations that take a long time may involve tasks like \nFigure 7.2: Checking the progress of an async task with shared storage\n25% or 50% of the task has been completed.\nwhether a task has been finished or not.\nrequired only if the task is required to return some data.\nsmall results, but if big elements like documents are produced as part of the task, \nA shared database is not the only way to be sure that the web server front-end is \nIn this case, all the information, task IDs, statuses, and results can remain inside the \nFor example, if a task is to generate a report, the back-end will \nRemember that the queue is likely to store the task ID and the \nstatus of the task.\nIt even creates its own events that will be reintroduced into the queue to \nproduce other tasks.\nSubdividing tasks\nIt's entirely possible to generate more tasks from an initial one.\ncreating the right event inside a task and sending it to the right queue.\nThis allows a single task to distribute its load and parallelize its action.\nif a task generates a report and sends it by email to a group of recipients, the task can \nfirst generate the report and then send the emails in parallel by creating new tasks \nadvantage is that individual tasks will be shorter, which makes them easier to \nSome tasks may require creating huge amounts of information in the background, \ntasks are distributed, and their results are returned and combined.\nup with the task taking a longer time.\nBut easy wins are bulk tasks performing similar actions on \nKeep in mind, though, that this will make the initial task finish \nquickly, making the initial task's ID status a bad way to check \nThe initial task \nmay return the IDs of the new tasks if they need to be monitored.\nScheduled tasks\nAsynchronous tasks don't need to be generated directly by a frontend and direct \nSome examples of scheduled tasks include generating daily reports during night \nMost task queues will allow the generation of scheduled tasks, indicating it clearly in \nSome scheduled tasks can be quite big, such as each night sending emails to \nIt's very useful to divide a scheduled task, so a small \nscheduled task is triggered just to add all the individual tasks to the queue that will \nThis distributes the load and allows the task to finish earlier, \nIn the example of sending emails, a single task triggers every night, reading the \nconfiguration and creating a new task for each email found.\nThen the new tasks \nAn important element of asynchronous tasks is the effect that introducing a queue \nAs we've seen, the background tasks are slow, meaning that any worker \nWe will see later in the chapter how to generate a scheduled task \nMeanwhile, more tasks can be introduced, which may mean that the queue starts \nsufficient to handle the average number of tasks introduced in the queue, the queue \nwill build up until it reaches its limit, and new tasks will be rejected.\nBut typically, the load doesn't work like a constant influx of tasks.\ntimes when there are no tasks to execute, and other times when there's a sudden \nspike in the number of tasks to be executed, filling the queue.\nperiod for those spikes, where a task gets delayed because all the workers are busy, \nAn extra difficulty, as we saw with scheduled tasks, is that at a specific time, a \nconsiderable number of tasks can be triggered at the same time.\nthe queue at a particular time, requiring perhaps an hour to digest all the tasks, for \nexample, creating daily reports, ingesting new updates in an external API every 4 \nThis means that, for example, if 100 tasks to create background reports are added, \nthey will block a task to generate a report sent by a user, which will produce a bad \nminutes after the scheduled tasks were fired.\nA possible solution is to use multiple queues, with different workers pulling from \nThis makes those different tasks go to different workers, making it possible \nto reserve capacity for certain tasks to run uninterrupted.\nbackground reports can go to their own dedicated workers, and the user reports \nreports run only once a day, once the 100 tasks are processed, the workers will be \nidle for the rest of the day, even if there's a long queue in the worker serving the user \nFigure 7.6: Regular worker pulling from multiple queues\nIn this case, the user report worker will continue with the same approach, but the \nbackground report worker will pull tasks from both queues.\nthe capacity for background reports, but at the same time, we increase it for the user \nreport tasks when there's available capacity.\nWe reserve capacity for the user report tasks, which are priority, and make the rest of \nthe workers pull from all available tasks, including priority and non-priority tasks.\nTo be able to divide work into these two queues, the tasks need to be divided \nPriority tasks.\nBackground tasks.\ntasks.\nIf too many tasks are labeled as \nregular traffic at the times where there are big spikes in background tasks are \nThat way, a task with priority 3 will be executed before a task with priority \n2, and that before a task with priority 1, and so on.\npriorities is that the workers can be working all the time, without wasting any \nby priority costs more than just assigning tasks to a plain queue.\nto start increasing the priority of tasks over time, especially if multiple \nThe decision on what task should return first could get \nexpectations when developing and creating new tasks.\nThere's always the temptation to generate multiple queues to set \nsome tasks take too long.\nand makes it easy to understand why we want multiple queues.\nWhen having different workers pulling from different queues, the worker could have \ndifferent codebases, making one with priority tasks and another with background \ntasks.\nIt's likely that some tasks or task parts will be either priority or background, \nA unique codebase can handle all kinds of tasks.\nto have a worker that handles both priority and background tasks.\ncodebases will require strict task separation, not using the extra capacity \navailable in the background workers to help with priority tasks.\nNote that for this to work, it will require strict separation of tasks.\nThis may not be adequate when the nature of the tasks may \nFor example, if some of the tasks require big \nsome AI-related tasks) this may require that specific tasks run \nworker for all tasks.\nCloud queues and workers\nIn cloud environments, it's possible that the number of workers extracting events \nDo we have a full queue?\ntask, making the system infinitely scalable.\nThe start-up time can add significant time to the execution of the task, even to \nthe point of being longer than the execution time of the task itself.\ninefficient to spawn one for each task.\nhelp by making it easier to separate between creating a new container and \nEach new worker created costs \nway, allowing extra space to grow and generating extra virtual servers only when \nfinish any remaining tasks, start no new ones, and finish when \nrequirements for latency, traffic, and the speed of creating a new server (if the server \nCelery is the most popular task queue created in Python.\nIt allows us to create new \ntasks easily and can handle the creation of the events that trigger new tasks.\nCelery requires to work to set up a broker, which will be used as a queue to handle \nA good starting point is to create a new server each time the queue \nhas a number of tasks equal to or greater than the number of \nworkers in a single server.\nable to handle those tasks.\ntasks than that, it will create a server that is not quite filled.\nnew server is up before there's a significant queue building up.",
      "keywords": [
        "Web Server Structures",
        "tasks",
        "Web Server",
        "Django REST framework",
        "Server Structures",
        "Server",
        "Django REST",
        "request",
        "user",
        "Queue",
        "Django",
        "system",
        "printed",
        "8:47",
        "subject"
      ],
      "concepts": [
        "task",
        "multiple",
        "request",
        "requests",
        "queueing",
        "content",
        "view",
        "different",
        "difference",
        "response"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 8,
          "title": "",
          "score": 0.586,
          "base_score": 0.436,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.47,
          "base_score": 0.32,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 3,
          "title": "",
          "score": 0.374,
          "base_score": 0.374,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.361,
          "base_score": 0.361,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.355,
          "base_score": 0.355,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tasks",
          "task",
          "request",
          "queue",
          "priority"
        ],
        "semantic": [],
        "merged": [
          "tasks",
          "task",
          "request",
          "queue",
          "priority"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3131005321451545,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530140+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Advanced Event-Driven Structures\b",
      "start_page": 255,
      "end_page": 280,
      "summary": "Using a backend is optional, as tasks don't need to define a return value, and it's very \ncommon that asynchronous tasks don't directly return response data other than the \nstatus of the task.\nfacilities, emails sent during task processing, and pre-caching of values, where there \nWe will use the example to create a task to retrieve, from an external API, \nThe code is divided into two files: celery_tasks.py, which describes the tasks, and \nstart_task.py, which connects with the queue and enqueues a task.\napp = Celery('tasks', broker='redis://localhost')\nThe celery_tasks.py worker defines a main task, obtain_info, and a secondary \ntask, send_email.\napp = Celery('tasks', broker='redis://localhost')\nLet's take a look at the obtain_info task.\nas a Celery task:\n@app.task\nlogger.info('Stating task')\n# Call the /todos endpoint to retrieve all the tasks\nfor task in response.json():\nuser_id = task['userId']\ntask_data = (info, task)\ntask_reminders[user_id].append(task_data)\nfor user_id, reminders in task_reminders.items():\nlogger.info('End task')\nWe wrap the function with INFO logs to provide context to the task execution.\nfor task in response.json():\nuser_id = task['userId']\nThe individual task data is added to a list created to store all the tasks for a user.\ntask_data = (info, task)\ntask_reminders[user_id].append(task_data)\nfor user_id, reminders in task_reminders.items():\ncompose_email takes the information in the task list, which includes a group of user_\ninfo, task_info, extracts the title information for each task_info, then the email \nfrom the matched user_info, and then calls the send_email task:\n# remainders is a list of (user_info, task_info)\n# Retrieve all the titles from each task_info\n# Start the task send_email with the proper info\nAs you can see, the send_email task includes a .delay call, which enqueues this task \nsend_email is another Celery task.\n@app.task\nTriggering tasks\nThe start_task.py script contains all the code to trigger the task.\nscript that imports the task from the other file.\nfrom celery_tasks import obtain_info\nNote that it inherits all the configuration from celery_tasks.py when doing the \nImportantly, it calls the task with .delay().\nThis sends the task to the queue so the \nNote that if you call the task directly with obtain_info(), you'll \nexecute the code directly, instead of submitting the task to the \n$ celery -A celery_tasks worker --loglevel=INFO -c 3\nThis starts the celery_tasks module (the celery_tasks.py file) with the -A \nIt sets the log level to INFO and starts three workers with the -c 3 \n$ celery -A celery_tasks worker --loglevel=INFO -c 3\n.> task events: OFF (enable -E to monitor tasks in this worker)\n[tasks]\n. celery_tasks.obtain_info\n. celery_tasks.send_email\nNote that it displays the two available tasks, obtain_info and send_email.\nwindow, we can send tasks calling the start_task.py script:\n$ python3 start_task.py\nThis will trigger the task in the Celery worker, producing logs (edited for clarity and \n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:54,128: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\n[2021-06-22 20:30:54,141: INFO/ForkPoolWorker-3] Task celery_tasks.\n[2021-06-22 20:30:54,192: INFO/ForkPoolWorker-2] Task celery_tasks.\nthe first task, which corresponds to obtain_info.\nThis task has been executed in the \n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\nWhile this task is being executed, the send_email tasks are also being enqueued and \n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\nIf only one worker is involved, the tasks will be run consecutively, \nWe can see how the send_email tasks start before the end of the obtain_info task, \nand that there are still send_email tasks running after the end of the obtain_info \nScheduled tasks\nInside Celery, we can also generate tasks with a certain schedule, so they can be \nTo do so, we need to define a task and a schedule.\nscheduled_tasks.py file.\napp = Celery('tasks', broker='redis://localhost')\n@app.task\ndef scheduled_task(timing):\nlogger.info(f'Scheduled task executed {timing}')\n'task': 'celery_scheduled_tasks.scheduled_task',\n'task': 'celery_scheduled_tasks.scheduled_task',\nsmall, simple task that just displays when it is executed.\n@app.task\ndef scheduled_task(timing):\nlogger.info(f'Scheduled task executed {timing}')\n'task': 'celery_scheduled_tasks.scheduled_task',\nThe first one defines an execution of the proper task every 15 seconds.\nThe task needs \nto include the module name (celery_scheduled_tasks).\n'task': 'celery_scheduled_tasks.scheduled_task',\nThis crontab object, which is passed as the schedule parameter, executes the task \n$ celery -A celery_scheduled_tasks beat\nWe start the celery_scheduled_tasks worker in the usual way.\n$ celery -A celery_scheduled_tasks worker --loglevel=INFO -c 3\nBut you can see that there's still no incoming tasks.\nwhich is a specific worker that inserts the tasks in the queue:\n$ celery -A celery_scheduled_tasks beat\nOnce celery beat is started, you'll start seeing the tasks being scheduled and \n[2021-06-28 15:13:06,504: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:06,509: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77]\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \n[2021-06-28 15:13:06,511: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[42ed6155-4978-4c39-b307-852561fdafa8] succeeded in \n[2021-06-28 15:13:06,512: INFO/ForkPoolWorker-1] Task celery_scheduled_\ntasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77] succeeded in \n[2021-06-28 15:13:21,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:21,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:13:36,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:13:51,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:51,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:14:00,004: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Task celery_scheduled_\nYou can see that both kinds of tasks are scheduled accordingly.\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \nThe other task happens exactly every 2 minutes.\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \nWhen creating periodic tasks, keep in mind the different priorities, as we described \nThis leads to the way of monitoring how the different tasks are being executed, in a \nexecuted tasks and find and fix problems.\n$ celery --broker=redis://localhost flower -A celery_tasks  --port=5555\nIt is good practice to use a periodic task as a \"heartbeat\" to check \nThis task can be used to \nmonitor that the tasks in the system are flowing as expected, with \n'celery_tasks.obtain_info',\n'celery_tasks.send_email']\nIn this case, we have 11 tasks \ncorresponding to a whole run of start_task.py.\nYou can go to the Tasks tab to see \nthe details of each of the tasks executed, which looks like this:\nFigure 7.9: Tasks page\nYou can see information such as the input parameters, the state of the task, the name \nto trigger the tasks directly with an HTTP request.\nThis can be used to call the tasks \nhttp://localhost:5555/api/task/async-apply/celery_tasks.send_email\n{\"task-id\": \"79258153-0bdf-4d67-882c-30405d9a36f0\"}\nThe task is executed in the worker:\n[2021-06-24 22:35:33,052: INFO/MainProcess] Received task: celery_\ntasks.send_email[79258153-0bdf-4d67-882c-30405d9a36f0]\n[2021-06-24 22:35:33,056: INFO/ForkPoolWorker-2] Task celery_tasks.\nUsing the same API, the status of the task can be retrieved with a GET request:\nGET /api/task/info/{task_id}\n$ curl  http://localhost:5555/api/task/info/79258153-0bdf-4d67-882c-\n{\"uuid\": \"79258153-0bdf-4d67-882c-30405d9a36f0\", \"name\": \"celery_tasks.\nexecute these smaller tasks.\ncertain times to allow the execution of predetermined tasks periodically.\nWe explained how to use Celery, a popular task manager, to create asynchronous \ntasks.\nhow to define a proper worker, and how to generate tasks from a different service.\nWe included a section on how to create scheduled tasks in Celery as well.\nallows us to create tasks by sending HTTP requests, allowing any programming \nquota_info doesn't need to store the information, rather just checking whether the \ngenerate_event('request', info.owner)\ngenerated events, and then stored in the info.\nTo get started, we need to upload the source video to the mock S3 and start the task.",
      "keywords": [
        "task",
        "info",
        "Task celery",
        "Celery",
        "Scheduled task executed",
        "Scheduled task",
        "task executed",
        "info task",
        "Scheduled",
        "email",
        "Received task",
        "user",
        "Event-Driven Structures",
        "tasks beat celery",
        "task executed crontab"
      ],
      "concepts": [
        "tasks",
        "info",
        "celery",
        "null",
        "event",
        "requests",
        "request",
        "scheduled",
        "schedule",
        "worker"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.586,
          "base_score": 0.436,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.336,
          "base_score": 0.336,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.334,
          "base_score": 0.334,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.309,
          "base_score": 0.309,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "task",
          "06",
          "2021 06",
          "2021",
          "info"
        ],
        "semantic": [],
        "merged": [
          "task",
          "06",
          "2021 06",
          "2021",
          "info"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30386877325086864,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530168+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Microservices vs Monolith\b",
      "start_page": 281,
      "end_page": 326,
      "summary": "For the mock S3 service, we will use the same approach, starting a container that \nWe will use the videos bucket for storing all the data.\nWe will define three different Celery workers that will perform three different \ntasks: the base task, image task and video task.\ndifferent queues.\nrequired libraries are available in the requirements.txt file so they can be installed \nThis distinction of specific tasks for different workers is done \ncan run in the same worker, and new events can be reintroduced \nthat may require a change of approach.\nFor example, some of the tasks may require specific hardware \nfor AI processing, use way more RAM or CPU power making it \nLet's start with the first stage of the process, the base task that will redirect to the \nThe main task will receive a path that contains the image.\ntasks for the processing of the video resizing and the extraction of the thumbnail.\nlogger.info('The file is a video, needs to extract thumbnail and '\nvideos_app.send_task('video_tasks.process_video', [path])\nimages_app.send_task('image_tasks.process_video', [path])\nNote that we are creating three different queues here:\nRedis allows us to create different databases easily by referring to them with an \nWe generate events in these queues with the .send_task function.\nTo generate a thumbnail of the video, we need the help of two third-party modules:\nMoviePy. This is a library for working with video.\nBoth libraries are included in the requirements.txt file described earlier in the \n@videos_app.task\ndef process_video(path):\nlogger.info(f'Stating process video {path} for image thumbnail')\nRight after it we generate a temporary file to store the downloaded video.\nThe task to resize the video follows a similar pattern.\nVideo task\nThe video Celery worker pulls from the video queue and performs similar steps to \n@videos_app.task\ndef process_video(path):\nlogger.info(f'Starting process video {path} for image resize')\nThe only difference from the image task is the resizing of the video to a height of 720 \nTo test the system, we need to start all the different elements.\n$ celery -A video_tasks worker --loglevel=INFO\nTo start the process, we need a video to be processed in the system.\nOne possibility to find good, free, videos is to use https://www.\nWe will use the following script to upload the video to the S3 Mock storage and start \nclient.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)\ncelery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_\nfollowing lines generate a simple interface that requests the name of the video to \nif not set yet, uploads the file to SOURCE_VIDEO_PATH, and then sends the task to the \nqueue to start the process:\nclient.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)\ncelery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_\nall the libraries in requirements.txt need to be installed:\nThis worker will create two new tasks:\n[2021-07-08 20:37:57,660: INFO/ForkPoolWorker-2] The file is a video, \nneeds to extract thumbnail and create resized version\n[2021-07-08 20:37:58,532: INFO/ForkPoolWorker-2] Stating process video \n[2021-07-08 20:37:57,982: INFO/ForkPoolWorker-2] Starting process video \n[2021-07-08 20:39:51,171: INFO/ForkPoolWorker-2] Task video_tasks.\nfile /source_video.mp4         size 56807332\nfile /source_video.mp4.png     size 6939007\nfile /source_video.mp4x720.mp4 size 8525077\ndata between different components of a hardware system.\nrequirements of your specific use cases and use a tool that can fulfil \ncan be routed to the proper destination without requiring a different endpoint.\nInternally, though, the bus will contain different logical divisions that allow the \nThis central location makes the configuration of all the different services easy, both \nMore complex systems can be created where the events pass through multiple stages \nand is processed by independent modules.\nevent describing the action will be introduced into a queue to be processed in the \nDifferent modules will analyze user behavior with different objectives \nservice is working over time.\nThese modules are fundamentally about different things and present a different view \ncould be determined that the modules require a different, dedicated team to take \nFigure 8.6: Bus from front end system to different modules\nThe same event will be sent to the bus, and then the different services will receive it.\nNote that the workers in this case can create more events to be introduced.\nexample, any module will be able to create an alert, to which the alert system will be \nFor example, if the inventory is too low, it may require a quick alert at the \nComplex event-driven systems can help you distribute the work between different \nTo properly implement this kind of system, the event needs to use a standard format \nA good idea is to use a simple JSON structure like the following:\nsystem, though it may require some work to configure.\nFor example, there's a library to allow Celery to work under this \nFor example, when a search is produced, an event like this will be created:\nFor example, the economic analysis module will discard any SEARCH event.\nmodules may require further processing.\nYou'll need to use the techniques for CQRS that we discussed in Chapter 3, Data \nIn essence, you'll need to ask differently \nto read and to save new data, as writing new data requires the generation of events; \nThe flexible data structure will allow for new events to be generated, adding more \ninformation and allowing for controlled changes across the modules by enforcing the \nThen the different teams can work in parallel, \nuseful in detaching different elements.\nevent-driven systems makes them not very useful to properly test the reception of \nmodule, and integration test to refer to those that require two or \nAll these steps require configuration to be done in three different systems (the front-\nIdeally, this test will require the system to be able to start \nThat requires every module \nusage of multiple queues and shared storage to generate multiple coordinated tasks, \nWe presented the idea of a bus, a shared access point for all events in the system, and \nchallenges of solving these complex interactions, both in terms of requiring the use of \nthrough events, and the demands in terms of testing at different levels with unit and \nmonolithic and microservices.\nMicroservices vs Monolith\nMonolithic architecture creates a single block \nMicroservices \narchitecture, on the other hand, divides the system into smaller microservices that \ntalk to each other, aiming to allow different teams to take ownership of different \nelements, and helping big teams to work in parallel.\nrequirements in terms of how the work needs to be structured.\nA common pattern is to migrate from an old monolithic architecture to a \nmicroservices one.\nvery useful when it comes to creating microservices, but that can also be applied \nMicroservices vs Monolith\nMonolithic architecture\nThe microservices architecture\nMoving from a monolith to microservices\nLet's start by talking in more depth about monolithic architecture.\nMonolithic architecture\nits usage and starts getting requests for new functionality to complement the existing \nThis process ensures that all the code and functionality are tied together in a single \nresponsibilities to different modules.\nThe defining characteristic of a monolith is that all the calls between modules are \nThe strategy for deploying a new version of the monolith is also easy.\nThe version of the monolith is easy to know, as all the code is part of the same \nThe microservices architecture\nThe microservices architecture was developed as an alternative to having a single \nThis is a monolithic architecture.\nMonolithic architecture is not synonymous with a lack of structure.\nMicroservices vs Monolith\nA system following a microservices architecture is a collection of loosely coupled \n1.\t A collection of specialized services, meaning that there are different and \n2.\t Loosely coupled, so each microservice can be independently deployed and \nEach microservice needs to communicate with others\nsame process, it uses multiple, separate functional parts (each microservice) that \ncommunicate through well-defined APIs. These elements can be in different \nprocesses and typically are moved out from different servers to allow proper scaling \nFigure 9.2: Note that not all microservices will be connected to the storage.\nThe defining characteristic is that the calls between different services are all through \nBecause of this, microservices architecture requires advanced planning and needs to \ndefine clearly the differences between components.\nIn particular, microservices architecture requires a good upfront \ndesign to be sure that the different elements connect together \nA system that follows the microservices architecture doesn't happen organically, but \nThere's a tendency to think that a more evolved architecture, like the microservices \nThe first one is the fact that almost every small application will start as a monolithic \nMicroservices, on the other hand, require the creation of a plan to divide the \nfunctionality carefully into different modules.\ninvestment in the microservices architecture.\nThat said, as monoliths grow, they can start presenting problems just through the \nconstant vigilance to ensure good internal structure, but that requires a lot of work \ndifferent areas into different processes.\ntwo later when changes in the system require adjustments.\nMicroservices vs Monolith\nThe modules can also require different specific knowledge, making it natural \nto assign different team members to different areas.\nownership of the modules, they can have different opinions in terms of code \nservice, the abilities required for training and handling an AI model to categorize the \ndata will be very different, making the module separation natural and productive.\nBoth of them in the same code base may generate problems by trying to work at the \nas each deployment of the monolith carries over every copy of every module.\nexample, the RAM required will be determined for the worst-case scenario across \nexample is the fact that, if any module requires a connection to the database, a new \nIn comparison, using microservices can adjust each service according to its own \nFigure 9.3: Notice that using different microservices allows us to reduce RAM usage by dividing requests into \ndifferent microservices, while in a monolithic application, the worst-case scenario drives RAM utilization\nDeployments also work very differently between monoliths and microservices.\nthe monolithic application needs to be deployed in a single go, every deployment is, \neffectively, a task for the whole team.\nIf the team is small, creating a new deployment \nand ensuring that the new features are properly coordinated between modules and \nMonolith deployments require coordination between modules, meaning that \nthey need to work with each other, which normally leads to teams working \nBy comparison, different microservices are deployed independently.\nIn comparison with the monolith, microservices can be deployed independently, \nteams working on them and allows for faster, continuous deployments that require \nrequired, but the objective of a microservices architecture is \nnecessarily that each microservice can be independently deployed \nother teams.\nMicroservices vs Monolith\nMonolithic applications, because they communicate with other modules through \nmany internal requests made to different microservices.\nrequired to try to avoid repeating external calls and to limit the number of services \nrequirements.\nIn a monolithic application, problems may arise as a result of requiring \ndifferent versions of libraries for different modules.\nversion of Python requires the whole code base to be prepared for that.\nupdates can be complicated as different modules may have different requirements, \nMicroservices, on the other hand, contain their own set of technical requirements, so \nBecause of the external APIs used, different microservices \nspecialized tools for different microservices, tailoring each one for each purpose and \nFor example, a task to process a document needs to \nobtain some user information, which requires calling a different \nmicroservice.\nThe name is required at the start of the document, \nJust because different microservices can be programmed in different \ndifficult for a member of a different team to be able to help, thereby \ncreating more isolated teams.\nAs we see, most of the characteristics of microservices make it more suited for a \nbe split into different teams and coordination needs to be more explicit.\nchange of pace in a big application also requires better ways to deploy and work \nA small team can self-coordinate very well and will be able to work quickly and \nmicroservices architecture only makes sense if there are enough developers such that \ndifferent teams are working in the same system and are required to achieve a good \nWhile the decision of monolith versus microservices is normally discussed in the \nFor example, a program in user space that wants to read from a file needs to \nMicroservices vs Monolith\nabout what architecture is better, given that Linux was created as a monolithic \nA key element of the difference between microservices and monolithic architecture is \nthe difference in the communication structure that they support.\nhappens, the internal structure can become messy, and requires developers with \nIncreasing the size of the development team becomes complicated, as each engineer \nrequires a lot of contextual information, and learning how to navigate the code is \nThe older teammates who have been around can help to train new team \nEach new member of the team will require a significant amount of training \nThe ideal size of a team depends on a lot of different factors, but \nTeams with fewer members create too much overhead in terms of \nmanagement and communication with other teams.\nIf the growing size of the code requires it, this is the time to employ all the techniques \nto work at creating ownership and explicit goals for each team.\nThis allows the teams to work in parallel without too much interference, so the extra \nclear boundaries will help in defining the work for each team.\nbend internal APIs. When moving to a microservices architecture, the division of work becomes way \nThe APIs between teams become hard limitations and there is a need \nfor more work upfront to communicate between teams.\nOwn the microservice completely without other teams coding in the same \nDeploy independently from other teams\nAs the code base will be smaller, new members of the team will be able to learn it \nThis way of working with a small, focused team can \nof the team need to be highly experienced and know their way \nNote this also means that different teams will know less about \nthe internals of other microservices compared with monolithic \ncan create some friction when moving people from one team to \nMicroservices vs Monolith\nway of dividing work was to have different teams, one related to developing new \nrequired for each task are different, after all.\ndivision, which can cause the team responsible for developing new features to be \noperations team finds changes with little reaction time, and identifies bugs without \nthat the same team that develops the software is responsible for deploying it, thereby \nNote that this normally involves creating a multi-functional team with people who \nSometimes, an external team is responsible for creating a set of common \ntools for other teams to use in their operations.\nCommunication within the same team is different from the communication between \ndifferent teams.\nCommunicating with other teams is always more difficult and \nup those teams and integrate them with new people.\nBecause APIs to be used externally from the team are going to be used by \nsense to make them generic and easy to use, as well as creating proper \nIf a new design follows the structure of already existing teams, it will be \nbetween teams require organizational changes.\nTwo teams working in the same service will create problems because each \nmultiple teams.\nteam is in charge of any changes.\nteams, describing their own structured communication, like the API \nThis has also created the need \nto structure communication differently compared with \na team working together in the same room.\nIn any case, team \nwork as a team.\nMicroservices vs Monolith\nMoving from a monolith to microservices\nA usual case is the need to migrate from an existing monolithic architecture to a new \nmicroservices one.\nMigrating to microservices will require a huge amount of effort, actively \nchanging the way the organization operates, and will require a big upfront \nIt will require a good deal of meetings and documentation to plan \nelement of microservices is the interaction between teams, which will \nchange significantly compared with the way of operating in a monolithic \nThis will likely involve changing teams and changing tools.\nThey'll need to be more formal in their interaction with other teams and \nmembers of some teams.\n(we will cover Docker and Kubernetes later in this chapter), so some teams \nbe very different.\nThis requires planning and the need to \nsupport team members until they are comfortable with the new system.\nDividing the existing monolith into different services requires careful \nsituation where practically any change to one service will require a change \ncreates duplication of work, as routinely working on a single feature requires \nmultiple microservices to be changed and deployed.\nMicroservices can be \nThere's an overhead in creating microservices, as there is some work that \nA small development team of up to 10 people \ngrows and independent teams are formed that migrating to microservices \nbe jumping around different microservices.\nuse of different tools and systems.\nMicroservices vs Monolith\nA balance between allowing each team to make their own decisions and \nIf teams have \nSolid communication between teams is required \nHowever, in microservices, it's \nAs we've discussed earlier, each call to a different microservice can increase \nconnecting the microservices.\nIf the code grows, it will make services dependent on \nmicroservices.\nMicroservices vs Monolith\nlook like after breaking the monolith up into multiple microservices.\nEach microservice needs to be considered in isolation, and as part of the rest.\nWhat microservices should be created?\nCan you describe each microservice \nIs there any critical or core microservice that requires more attention \nrequirements.\nHow will the teams be structured to cover the microservices?\nAre microservices independent?\nmicroservices?\nIs there any microservice that is accessed more than others?\nCan microservices be deployed independently from each other?\nprocess if a new change is introduced that requires a change in a dependent \nWhat microservices \nthere any service that requires specific APIs, such as a SOAP connection?\nflow diagrams of requests that need to interact with multiple microservice, so as to \nmicroservice.\nIn general, storage for one microservice should not be shared with \nInstead, one microservice \nFor example, let's imagine that there are two microservices, one that controls reports \nWe can break the microservice's responsibility by allowing the report service \nMicroservices vs Monolith\nInstead, the report service needs to access the user microservice through an API and \nThat way, each microservice is responsible for its own storage and \nEach microservice keeps its own independent storage.\nfrom an API that is not required until later in the process.\nAt this stage, there's no need to design detailed APIs between microservices, but \nsome general ideas on what services handle what data and what the required flows \ndifferent microservices.\nGenerally, though, most microservices won't require their own \nTo create new microservices, there are three possible strategies:\nMicroservices vs Monolith\nAlso, if the functionality covered by this microservice is \nDivide the functionality, copying and pasting code that exists in the monolith \ninto a new microservice structure.\nstructured, this approach is relatively fast, only requiring some internal calls \nfunctionality migrated to the new microservice, and then, one by one, \nThis will inform each microservice plan, although we will need to create a global \nview to determine which microservices to create in what order.\nWhat microservices need to be available first, taking into account \nIt may be necessary to include in the monolith new access \npoints to ensure that a new microservice can call back to \nIt's also possible that the monolith needs to be refactored \nPain points are the code or other elements that are changed \ndiffer.\nAn idea of the training that teams will require and what the new elements \nAny team changes and ownership of the new services.\nInvolve the team \nFinally, we need to act on our plan to start the move from the outdated monolith to \nthe new wonderful land of microservices!\nMicroservices vs Monolith\nTo be able to make the change, from the monolith to the new microservice or \nmicroservices that handle the same functionality, the key tool is to use a load \nthe new microservice is directly replacing the requests.\nThis can be used to migrate the requests from the monolith slowly to the new \nmicroservice that should receive this request.\ncan be configured by a different URL to direct the request to a different service, so \nit can use that small granularity to distribute the load properly across the different \nOnce the new microservice is deployed, the requests \ncan be load-balanced by introducing the new microservice.\nstable between the legacy monolith and the new microservice.\nAt that point, the handling of the requests in the legacy monolith is unused and can \nAny plan will need to be tested with care.\nteam.\nthen start training other teams and spread the knowledge, so everyone \nEnforcing a list of prerequisites for a new microservice to be deployed \nalso to keep a feedback channel, so new teams can share their findings and \nnew microservice will be routine for the team.\nMicroservices vs Monolith\nany new development is done in the microservices.\nNow, teams can take full ownership of their microservices and start taking \nmore ambitious tasks, such as replacing a microservice completely by \nthe architecture by merging or splitting microservices.\nwhere, from now on, you live in a microservices architecture.\nThe traditional way of operating services is to use a server using a full OS, such \nand flexibility, but still requires each server to be managed as an independent \nrequired.\nThis allows, for example, different containers to be run in the same physical machine \nand have each container run a different OS, with different packages, and different ",
      "keywords": [
        "microservices",
        "system",
        "video",
        "Microservices architecture",
        "teams",
        "Monolith",
        "file",
        "Celery",
        "task",
        "Advanced Event-Driven Structures",
        "printed",
        "8:47",
        "subject",
        "code",
        "architecture"
      ],
      "concepts": [
        "different",
        "difference",
        "differ",
        "required",
        "require",
        "microservices",
        "team",
        "videos",
        "service",
        "monolithic"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 8,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.47,
          "base_score": 0.32,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.338,
          "base_score": 0.338,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.322,
          "base_score": 0.322,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.316,
          "base_score": 0.316,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "microservices",
          "microservice",
          "monolith",
          "teams",
          "different"
        ],
        "semantic": [],
        "merged": [
          "microservices",
          "microservice",
          "monolith",
          "teams",
          "different"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31141426103619874,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530186+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Testing and TDD\b",
      "start_page": 327,
      "end_page": 380,
      "summary": "coding abilities and code needs to be tested thoroughly before it can be considered \nWe will see how to approach testing, including the use of Test-Driven Design \n(TDD), a practice that puts testing at the center of the development process.\nChapter 10, Testing and TDD, explaining different approaches to testing, the \nTesting and TDD\nWriting tests allows you to detect problems while the code is fresh and with some \nduring the chapter how to write tests easily, as well as different strategies to write \ndifferent tests for capturing different kinds of problems.\nBe aware that, as with any other code, tests can have bugs as well.\nTesting and TDD\nthe tests first, to ensure that the validation is as independent of the actual code \nWe will also show how to create tests in Python using common unit test frameworks, \nTesting the code\nDifferent levels of testing\nTesting philosophy\nTest-Driven Development\nIntroduction to unit testing in Python\nTesting external dependencies\nLet's start with some basic concepts about testing.\nTesting the code\nThe first question when discussing testing the code is a simple one: What exactly do \nwe mean by testing the code?\ntests but to do informal \"full application runs\" checking that a \nmore careful about the testing.\nthe tests are documented.\nThere are multiple ways that a test can be documented, either by specifying a \nlist of steps to run and expected results or by creating code that runs the test.\nThe main idea is that a test can be analyzed, be run several times by different \nPreferably automated: Tests should be able to be run automatically, with as \nIntegration techniques to run many tests over and over, creating a \"safety \nyou to run tests, which can help.\nFrom a known setup: To be able to run tests in isolation, we need to know \nwhat the status of the system should be before running the test.\nthat the result of a test will not create a certain state that could interfere with \nthe next test.\nBefore and after a test, certain cleanup may be required.\nThis can make running tests in batches slower, compared with not worrying \nand in some cases, the order of tests can create problems.\nFor example, test A creates an entry that test B reads.\ntest B is run in isolation, it will fail as it expects the entry \nAlso, being able to run tests \nTesting and TDD\nDifferent elements of the application: Most tests should not address the \ndifferent levels of testing, but tests should be specific about what are they \nrunning tests takes time, and that time needs to be well spent.\nAny test needs \ncommenting on this important aspect of testing.\nabout the different tests defined by how much of the system is under test, during \neach test.\nThere's an important kind of testing that we are not covering \nwith this definition, which is called exploratory testing.\nThese tests \ntesting can be invaluable in detecting inconsistencies and problems \nBy its nature, this kind of testing cannot be \"designed\" or \nDifferent levels of testing\nAs we described before, tests should cover different elements of the system.\nmeans that a test can address a small or big part of the system (or the whole system), \nWe will define three different levels or kinds of tests, from small to big scopes:\nUnit tests, for tests that check only part of a service\nIntegration tests, for tests that check a single service as a whole\nSystem tests, for tests that check multiple services working together\ntakes to create tests to be sure that they are always worth it.\nUnit tests\nunit test.\nThis kind of test checks the behavior of a small unit of code, not the whole \nThis unit of code could be as small as a single function or test a single API \nintegration and unit tests can be defined side by side, and the \ntest a unit test if it involves a single function or class.\nTesting and TDD\nBecause a unit test checks a small part of the functionality, it can be very easy to set \nTherefore, making new unit tests is quick and can thoroughly \ntest the system, checking that the small individual pieces that make the whole system \nThe objective of unit tests is to check in depth the behavior of a defined feature of a \nare defined as part of the test.\nWe will cover unit tests in more detail later in the \nIntegration tests\nThe next level is the integration test.\nThe main goal of integration testing is to be sure that the different services or \ntests, external requests are simulated, integration tests use the real service.\nThis makes integration tests slower and more expensive \nthan unit tests.\nIntegration tests are great to check that different services work in unison, but there \nIntegration tests are normally not as thorough as unit tests, focusing on checking \nmeaning that the test case should produce no errors or exceptions.\ntests as possible, as the point of the test is to test that the different \nExpected errors and exceptions are normally tested in unit tests, since they are also \nThat doesn't mean that every single integration test should \nthe bulk of the integration tests.\nSystem tests\nSystem tests check that all the different services \nA requirement for this kind of test is that there are actually multiple services in \nIf not, they are not different from tests at the lower levels.\nobjective of these tests is to check that the different services can cooperate, and the \nSystem tests are slow and difficult to implement.\nactually performing any system tests is to run them in the live environment.\nthat case, given the constraints, only a minimum amount of tests should be run, as \nThe tests to run should also exercise the maximum \nSmoke tests should be very clear, well documented, and designed carefully to cover \nthese tests check.\nOf course, smoke tests can be run not only on the live environment \nTesting and TDD\nTesting philosophy\nAs we've seen, testing is a way of ensuring that the behavior of the code is the \nThe objective of testing is to detect possible problems (sometimes \ncreate a specific test that simulates exactly the same problem, but we can also create \na framework that executes tests regularly to have a clear approach to how to detect \nDifferent testing levels have different effects on this cost.\nthat can be detected at the unit test level is going to be cheaper to fix there, and the \nDesigning and running a unit test is easier and faster than \nsystem test.\nIn general, tests are way better at detecting bugs than other defects, \nThe different test levels could be understood as different layers capturing possible \nthe start of the process (design and unit tests while coding), the cheaper it is to create \nSome defects are impossible to detect at the unit test level, like the integration of \nBut having tests is not only a good way of capturing problems once.\nBecause a test \nThis is one of the best arguments for running tests automatically \nContinuous Integration tool will run every test, alerting early \nif there's a problem with some test.\nRegression problems are quite common, so having good test \nSpecific tests \nThese are regression tests, and \nTesting and TDD\nAnother benefit of having good tests that check the behavior of the system is that the \nHow to design a great test\nDesigning good tests requires a certain mindset.\nThe objective of the test is to be sure that the functionality sticks to the expected \nNow, to be able to really put the functionality to the test, the mindset should be to \nWhile approaching the test, we need to check what the limits are of this, trying \nexample, the following tests could be created:\nNote how we are testing different possibilities:\nWe can really create a lot of test cases for simple functionality!\nIn those cases, the question is the same: are those tests adding better \nfunctionality already checked by an existing test (for example, creating a big table \ndividing numbers with a lot of divisions) may depend greatly on the code under test \ntesting as a failure there could be more important.\nThe best test is the test that really stresses the code and ensures \ncode under test is the best way of preparing your code for the real \nthe best preparation for that is to create tests that try as hard as \nTesting and TDD\nNote that tests are done independently from the implementation of the code.\ntest definition is done purely from an external view of the function to test, without \nThis is called black-box testing.\nA heathy test suite \nknowledge of the code itself and approach tests independently.\nto complement it with tests that check functionality that is not apparent from an \nFor example, any external API should test any input with care and \nAPIs. For example, testing what happens when strings are input \nless testing, as the internal code is less likely to abuse the API.\nrequired to test that the input format is incorrect, just to check that \nto create the tests, like a QA team performing tests.\nthis is not a possible approach for unit tests, which will likely be \nBlack-box testing tries to avoid a common problem where the same developer writes \nboth the code and the test and then checks that the interpretation of the feature \nwhich tries to ensure tests are created without the implementation in mind by \nwriting the tests before writing the code.\nStructuring tests\nThis pattern means the test is in three different phases:\nArrange: Prepare the environment for the tests.\nAct: Perform the action that is the objective of the test.\nis to test the functionality from an external perspective.\nDeveloping the ability to be able to create good black-box tests is \nTesting and TDD\nNote that this structure can be used whether the tests are executed through code or \nrun manually, though they'll be used more for automated tests.\nrequiring to run tests in a specific sequence, which is not great for unit test suites, but \ndef test_example():\n# Create the instance of the class to test\nA common different pattern is to group act steps in tests, testing \nmultiple functionalities in a single test.\nFor example, test that \nInstead, to follow the AAA pattern, two tests should be created, the \nIn the same way, if the code to test is purely functional (meaning \nin the class that we want to test.\nThe Act step just generates the action that is under test.\nto_test method for the prepared object with the proper parameter.\nAnother common pattern that appears using the AAA pattern for tests is to create \ncommon functions for testing in Arrange steps.\n# to test requires a lot of things to set up\nreturn object_to_test\ndef test_exampleA():\nobject_to_test = create_basic_environment()\nThe Arrange step is where most of the effort of the test will \nTesting and TDD\ndef test_exampleB():\nobject_to_test = create_basic_environment()\nrepetition, which is a problem when having big test suites.\nHaving big test suites is \nimportant to create good test coverage, as we saw above.\nTest-Driven Development\nTDD consists of putting tests at the center of the developing experience.\n2.\t A new test is written to define the new functionality.\n3.\t The test suite is run to show that it's failing.\nRepetition in tests is, up to a certain point, unavoidable and even \npart of the code because there are changes, the tests need to be \nchanges lightly, as the tests will work as a reminder of the affected \n5.\t The test suite is run to show that the new test is working.\nWrite the tests before writing the code: This prevents the problem of \ncreating a test that is too tightly coupled with the current implementation, \nIt also forces the developer to check that the test actually \nearlier in the How to design a great test section.\nRun the tests constantly: A critical part of the process is running the whole \ntest suite to check that all the functionality in the system is correct.\ndone over and over, every time that a new test is created, but also while \nRunning the tests is an essential part of \nbuilds and grows a test suite that is big and covers the whole functionality of \nThis big test suite creates a safety net that allows you to perform refactors of the \nmean small tests that are specific and need to be thought about before adding \nTesting and TDD\ntests at the center of the development process, hence the name of the practice.\nAnother important advantage of TDD is that putting the focus so heavily on the tests \nmeans that how the code is going to be tested is thought about from the start, which \nwrite, focusing on it being strictly required to pass the test reduces the probability of \nThe requirement to create small tests and work in increments also tends \nbe tested independently.\nThe general flow is to be constantly working with new failing tests, making them \nway of working is the generation of very extensive test suites that cover each detail \nAnother important aspect of TDD is the requirement of speedy tests.\nAs tests are \nof the test suite will make it take longer to run.\nThere's a general threshold where focus gets lost, so running tests taking longer \nObviously, running the whole test suite in under 10 seconds will be extremely \nA full unit test suite for a complex \napplication can consist of 10,000 tests or more!\nThe whole test suite doesn't need to be run all the time.\nInstead, any test runner \nshould allow you to select a range of tests to run, allowing you to reduce the number \nof tests to run on each run while the feature is in development.\nonly the tests that are relevant for the same module, for example.\nrunning a single test, in certain cases, to speed up the result.\nAnyway, as the time taken to run tests is important in TDD, observing the duration \nof tests is important, and generating tests that can run quickly is key to being able \nThis is mainly achieved by creating tests that cover small \n(writing tests after writing the code).\nproblems that may arise through creating tests.\nOf course, at some point, the whole test suite should be run.\nrunning tests, this time automatically once the code is checked out \nThe combination of being able to run a few tests locally \nthe whole test suite running in the background once the code is \nTDD practices work best with unit tests.\ntests may require a big setup that is not compatible with the speed \nTesting and TDD\nexisting code can be difficult to test in this configuration, especially if the developers \nTDD works great for new projects, though, as a test suite for \nOne is the problem of big tests that take too long to run.\nThese tests \nbeginning, as parts of the code will already be written, and perhaps new tests should \nbe added, violating the rule of creating the tests before the code.\nachieve with a preconfigured unit test.\nto be tested before starting to think about the implementation.\nremember to avoid dependencies between tests.\nThis can happen with any test suite, \nbut given the focus on creating new tests, it's a likely problem if the team is starting \nDependencies can be introduced by requiring tests to run in a \nbut a set of ideas and practices that can help you design code that's well tested and \nNot every single test in the system needs to be designed using TDD, \nTo write the code in full TDD fashion, we start with the smallest possible test.\ncreate the smallest skeleton and the first test.\nWe run the test, and get an error with the test failing.\nPython code, but later in the chapter, we'll see how to run tests more efficiently.\nA typical effect on that will be that some tests fail if run \nTesting and TDD\nto pass the first tests.\nLet's run the tests now and you'll see no errors.\nBut now we add tests for the lower edge.\nconsidered the same test, as they're checking that the edge is correct.\nLet's run the tests again.\nWhen running the test, we see that it's running the tests correctly.\nto be sure that the intermediate section is correct, so we add another test.\nTesting and TDD\nThis runs all the tests correctly.\nWe can run the tests all through the process and be sure that the code is correct.\nIntroduction to unit testing in Python\nThere are multiple ways to run tests in Python.\nconcept of creating a testing class that groups several testing methods.\nnew file with the tests written in the proper format, called test_unittest_example.\ndef test_five(self):\n# Note this test is incorrect\ndef test_ten(self):\ndef test_eleven(self):\nWe import the unittest module and the function to test.\ncomes next, which defines the tests.\nTesting and TDD\nThe class TestTDDExample groups the different tests.\nThen, methods that start with test_ will produce the \nindependent tests.\nThis runs the tests automatically if we run the file.\n$ python3 test_unittest_example.py\nFile \".../unittest_example.py\", line 17, in test_seven\nRan 6 tests in 0.001s\nAs you can see, it has run all six tests, and shows any errors.\nthe tests that are being run:\n$ python3 test_unittest_example.py -v\ntest_eleven (__main__.TestTDDExample) ...\ntest_five (__main__.TestTDDExample) ...\ntest_negative (__main__.TestTDDExample) ...\ntest_seven (__main__.TestTDDExample) ...\ntest_ten (__main__.TestTDDExample) ...\nNotice that test_seven is defined incorrectly.\ntest_zero (__main__.TestTDDExample) ...\nFile \".../unittest_example.py\", line 17, in test_seven\nRan 6 tests in 0.001s\nYou can also run a single test or combination of them using the -k option, which \nsearches for matching tests.\n$ python3 test_unittest_example.py -v -k test_ten\ntest_ten (__main__.TestTDDExample) ...\nRan 1 test in 0.000s\nways of testing.\nexecution of each test in the class.\nWe will take a look at it later when testing mocking \nTesting and TDD\nWhile unittest is probably the most popular test framework, it's not the most \nPytest simplifies writing tests even further.\nneeds to structure the tests, adding a bit of boilerplate code, like the test class.\nproblems are not as obvious, but when creating big test suites, the setup of different \ntests can start to get complicated.\nPytest instead simplifies the running and defining of tests, and captures all the \nLet's see how to run the tests defined in the unittest, in the file test_pytest_\ndef test_negative():\nA common pattern is to create classes that inherit from other test \ndef test_zero():\ndef test_five():\ndef test_seven():\ndef test_ten():\ndef test_eleven():\nIf you compare it with the equivalent code in test_unittest_example.py, the code \n$ pytest test_unittest_example.py\n================= test session starts =================\ntest_unittest_example.py ...F..\n______________ TestTDDExample.test_seven ______________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\nTesting and TDD\ntest_unittest_example.py:17: AssertionError\n=============== short test summary info ===============\nFAILED test_unittest_example.py::TestTDDExample::test_seven\nAs with unittest, we can see more information with -v and run a selection of tests \n$ pytest -v test_unittest_example.py\n========================= test session starts =========================\ntest_unittest_example.py::TestTDDExample::test_eleven PASSED      [16%]\ntest_unittest_example.py::TestTDDExample::test_five PASSED        [33%]\ntest_unittest_example.py::TestTDDExample::test_negative PASSED    [50%]\ntest_unittest_example.py::TestTDDExample::test_seven FAILED       [66%]\ntest_unittest_example.py::TestTDDExample::test_ten PASSED         [83%]\ntest_unittest_example.py::TestTDDExample::test_zero PASSED        [100%]\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \n$ pytest test_pytest_example.py -v -k test_ten\n========================= test session starts =========================\ntest_pytest_example.py::test_ten PASSED                           [100%]\nAnd it's totally compatible with unittest defined tests, which allows you to combine \n$ pytest test_unittest_example.py\n========================= test session starts =========================\ntest_unittest_example.py ...F..\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \ntest_ and run inside all the tests.\ncan see it runs both test_unittest_example.py and test_pytest_example.py.\n========================= test session starts =========================\nTesting and TDD\ntest_pytest_example.py ...F..\ntest_unittest_example.py ...F..\n_____________________________ test_seven ______________________________\ndef test_seven():\ntest_pytest_example.py:18: AssertionError\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_pytest_example.py::test_seven - assert 49 == 0\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \nwe need to go back to how to define tests when the code has dependencies.\nTesting external dependencies\nWhen building unit tests, we talked about how it's based around the concept of \nisolating a unit in the code to test it independently.\ncreate small, clear tests.\nCreating small tests also helps in keeping the tests fast.\nIn our example above, we tested a purely functional function, parameter_tdd, that \ninevitably, at some point, you'll need to test something that depends on something \nThe question in this case is should the other component be part of the test or not?\nSome developers think that all unit tests \nshould not be part of the test.\npieces of code that form a unit that it's easier to test in conjunction than separately.\nTesting and TDD\nAnd the tests are in test_dependent.py.\ndef test_negative():\ndef test_zero():\ndef test_twenty_five():\ndef test_hundred():\ndef test_hundred_and_one():\nIn this case, we are completely using the external library and testing it at the same \ntime that we are testing our code.\nneed to be captured to prevent making them while running tests and to have control \nover the returned values, or other big pieces of functionality that should be tested in \nmain/chapter_10_testing_and_tdd.\nfake calls, under the control of the test itself.\nTo be able to mock the code, in our test code, we need to prepare the mock as part of \ndef test_twenty_five(mock_sqrt):\nAgain, in this case, the tests work perfectly fine with the \nbe used for other purposes than testing, though it should be used ",
      "keywords": [
        "TDD",
        "code",
        "run",
        "def test",
        "run tests",
        "Unit tests",
        "assert parameter",
        "test suite",
        "parameter",
        "Integration tests",
        "Python",
        "Assert",
        "Docker",
        "printed"
      ],
      "concepts": [
        "tested",
        "code",
        "coding",
        "run",
        "running",
        "runs",
        "python",
        "different",
        "difference",
        "docker"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.434,
          "base_score": 0.284,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.429,
          "base_score": 0.279,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.42,
          "base_score": 0.27,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.391,
          "base_score": 0.241,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tests",
          "test",
          "testing",
          "test_unittest_example",
          "testtddexample"
        ],
        "semantic": [],
        "merged": [
          "tests",
          "test",
          "testing",
          "test_unittest_example",
          "testtddexample"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30329038370144,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530204+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Package Management\b",
      "start_page": 381,
      "end_page": 422,
      "summary": "For other tests, for example, we can check that the mock was not called, indicating \ndef test_hundred_and_one(mock_sqrt):\nWith that information, the full file for testing, test_dependent_mocked_test.py, will \ndef test_negative(mock_sqrt):\ndef test_zero(mock_sqrt):\ndef test_twenty_five(mock_sqrt):\ndef test_hundred(mock_sqrt):\ndef test_hundred_and_one(mock_sqrt):\ndef test_multiple_returns_mock(mock_sqrt):\ndef test_exception_raised_mock(mock_sqrt):\nMocking is not the only way to handle dependencies for tests.\nexplicit when calling the function under test, so it can be replaced with a testing \nIn essence, it's a way of designing the code that makes dependencies explicit by \nLet's see how this changes the code under test.\ndef test_good_dependency():\ndef test_twenty_five():\nDependency injection, while useful for testing, is not only aimed \npart that the dependent code will do).\nWe can also provoke an error if calling the dependency to ensure that in some tests \nThe code to test becomes, in \nTo use dependency injection principles, the code will need to be written in this way:\nWe can compare how to test both cases, as seen in the file test_dependency_\ninjection_test.py.\ndef test_model(mock_write):\nmock_write.assert_called_with('model.txt', 'test_model')\nCompared to that, the dependency injection example doesn't require a mock through \nWhile in testing this interference is not the same as doing it for regular code \ntwo classes, as we see in test_group_classes.py.\ndef test_negative(self):\ndef test_zero(self):\ndef test_ten(self):\ndef test_eleven(self):\ndef test_five(self):\n$ pytest -v test_group_classes.py\ntest_group_classes.py::TestEdgesCases::test_negative PASSED      [16%]\ntest_group_classes.py::TestEdgesCases::test_zero PASSED          [33%]\ntest_group_classes.py::TestEdgesCases::test_ten PASSED           [50%]\ntest_group_classes.py::TestEdgesCases::test_eleven PASSED        [66%]\ntest_group_classes.py::TestRegularCases::test_five PASSED        [83%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\n$ pytest -k TestRegularCases -v test_group_classes.py\ntest_group_classes.py::TestRegularCases::test_five PASSED        [50%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\n$ pytest -v test_group_classes.py::TestRegularCases\ntest_group_classes.py::TestRegularCases::test_five PASSED        [50%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\nthrough a decorator in the tests, for example, in test_markers.py.\ndef test_five():\ndef test_ten():\ndef test_eleven():\nSee that we are defining a decorator, @pytest.mark.edge, on all the tests that checks \nIf we execute the tests, we can use the parameter -m to run only the ones with a \n$ pytest -m edge -v test_markers.py\ntest_markers.py::test_negative PASSED                            [25%]\ntest_markers.py::test_zero PASSED                                [50%]\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\ntest_markers.py:5\ntest_markers.py:5: PytestUnknownMarkWarning: Unknown pytest.mark.edge \ntest_markers.py:10\n$ pytest -m edge -v test_markers.py\ntest_markers.py::test_negative PASSED                            [25%]\ntest_markers.py::test_zero PASSED                                [50%]\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\nNote that markers can be used across the full test suite, including multiple files.\ncreating a quick test suite with the most important tests to run with the marker \nThe use of fixtures is the preferred way to set up tests in pytest.\nis a context created to set up a test.\nFixtures are used as input for the test functions, so they can be set up and create \nspecific environments for the test to be created.\ndef test_counting():\nDefining the fixture this way will allow us to reuse it easily in different test functions, \ndef test_counting_fixture(prepare_string):\ndef test_counting_fixture2(prepare_string):\n$ pytest -v test_fixtures.py -k counting_fixture --setup-show\ntest_fixtures.py::test_counting_fixture\ntest_fixtures.py::test_counting_fixture (fixtures used: \ntest_fixtures.py::test_counting_fixture2\ntest_fixtures.py::test_counting_fixture2 (fixtures used: \nThe fixture should then create a file, return it, and then remove it as part of the \nfilename = f'./test_file_{time.time()}.txt'\nfilename = f'./test_file_{time.time()}.txt'\ndef test_counting_fixture(prepare_file):\ndef test_counting_fixture2(prepare_file):\nstep deletes the testing files after each test.\n$ pytest -v test_fixtures2.py\ntest_fixtures2.py::test_counting_fixture PASSED                  [50%]\ntest_fixtures2.py::test_counting_fixture2 PASSED                 [100%]\nfile called conftest.py, which will automatically be shared by pytest across all the \ntests.\nthe tests before the code, working in small increments, and running the tests over \nWe continued by presenting ways of creating unit tests in Python, both using the \nWe described how to test external dependencies, something that is critically \nimportant when writing unit tests to isolate functionality.\nPackage Management\nlanguages have their own ways of creating and sharing packages, so the usefulness \nIn this chapter, we will discuss the use of packages, mostly from a Python \nperspective, covering when and how to decide to create a package.\nthe different options available, from a simple structure to packages that include code \nPackage Management\nTrivial packaging in Python\nThe Python packaging ecosystem\nCreating a package\nPython package with binary code\nLet's start by defining what code could be a candidate to create a package.\nOnce the decision to create some code as an independent package has been taken, \npackage.\nAny new package will require time to develop new features and adjustments, \ndependent on how mature the package is and how many new features are \nPackage Management\nWe will focus on creating a new package in Python, but the basics are similar when \ncreating other packages in other languages.\nTrivial packaging in Python\nIn Python, it is easy to create a package to be imported by just adding a subdirectory \nThe structure of the code for a module in Python can be worked out as a subdirectory \nFor example, when creating a module called naive_package \ndirectory contains two files, the submodule.py file with the code, and an empty \nWe have the module.py file, which defines the \nThe __init__.py file, in this case, is not empty, but instead, it imports the \nthe some_function function available as part of the top level of the naive_package \n__init__.py is a special Python file that indicates that the \ndirectory contains Python code and can be imported externally.\nPackage Management\nWe can now create a file to call the module.\nWe'll write the call_naive_package.py \nfile, which needs to be at the same level as the native_package directory:\nfrom naive_package import some_function\nThis file just calls the module-defined function and prints the result:\n$ python3 call_naive_package.py\nmodule can help us understand how to create a package and what the structure of a \nThe first step to detaching a module and creating an independent package \nBut to get a better solution, we will need to be able to create a full Python package \nThe Python packaging ecosystem\nPython has a very active ecosystem of third-party open source packages that \nFor example, to install the package named requests, a package allowing the \npip searches in the Python Package Index automatically to see whether the package \nThe Python Package Index (PyPI, normally pronounced as Pie-P-I, as opposed to Pie-\nPie) is the official source of packages in Python and can be checked at https://pypi.\ndepends on the installation of Python in your system.\nPackage Management\nuseful information, including available packages with partial matches.\nhow to use the package.\nIn any case, given the number of available packages for Python, of varying quality \nPackage Management\nThe next element in the packaging chain is the creation of virtual environments to \nWhen dealing with installing packages, using the default environments in the system \nleads to the packages being installed there.\nThis can lead to problems, as you may install packages that have side effects when \nusing the Python interpreter for other purposes, as dependencies in the packages \nFor example, if the same machine has a Python program that requires the package1 \npackage and another Python program that requires package2, and they are both \nInstalling both package1 and package2 won't \nThe solution to this problem is to create two different environments, so each package \nTo create a new virtual environment, you can use the standard module venv, \nincluded in all installations of Python 3 after 3.3:\nespecially in the dependencies of the packages, or in the \nFor example, package1 requires \ndependency version 5 to be installed, and package2 requires \nThe virtual environment also has its own library, so any installed packages will be \nOnce in the virtual environment, any call to pip will install the packages in the \nWith a proper environment, we can use pip to install the different dependencies.\nPackage Management\nCreating a virtual environment is the first stage, but we need to install all \nrequirements file that defines all dependencies that should be installed.\nworking with a file, normally called requirements.txt, to install dependencies.\nFor example, let's take a look at the following requirements.txt file:\nPython-Architecture-Patterns/blob/main/chapter_11_package_management/\nThe file can be installed in the virtual environment (remember to activate it) using \nNote the format is package==version.\nversion to use for the package, which is the recommended way \nof installing dependencies.\npackage, which will install the latest version, and which can lead \npackaging==21.0\nPackage Management\nPython packages\nThe directory is called site-packages.\nTo distribute it, the subdirectory is packaged into two different files, either Egg files \nor Wheel files.\nImportantly, though, pip can only install Wheel files.\nof Wheel files that are compatible between different versions of Python, \nWheel files can include already compiled binary code.\npython3.9/site-packages/.\nSource packages can also be created.\nOS, so the right Wheel file will be downloaded and installed, if available.\nA Wheel file with a source file can also be created to allow its \npackages.\nRight now, the standard for packaging in Python is Wheel files, and they should be \nEgg files should be limited to older packages that haven't \nWe will see now how to create your own package.\nCreating a package\nEven if, in most cases, we will use third-party packages, at some point, it is possible \nthat you'll need to create your own package.\nTo do so, you need to create a setup.py file, which is the base of the package, \nBase package code will look like this:\npackage\nPackage Management\ninformation about the package.\npackage.\nThe code of the process is the setup.py file.\nname='wheel-package',\ndescription='an example of a package',\nThe setup.py file essentially contains the setuptools.setup function, which defines \nthe package.\nname: The name of the package.\nAn important detail of setup.py is that it is dynamic, so we can use code to \npackage_dir: The subdirectory where the code of the package is located.\ninstall_requires: Any dependency that needs to be installed with your \npackage.\npackages: Using the setuptools.find_packages function, include everything \npackage.\nOnce the file is ready, you can run the setup.py script directly, for example, to check \n$ python setup.py check\nThe setup.py file can be used to install the package in develop mode.\npackage in the current environment in a linked way.\nto the code will be applied directly to the package after the interpreter is restarted, \n(venv) $ python setup.py develop\nwriting src/wheel_package.egg-info/PKG-INFO\nwriting dependency_links to src/wheel_package.egg-info/dependency_\nwriting requirements to src/wheel_package.egg-info/requires.txt\nwriting top-level names to src/wheel_package.egg-info/top_level.txt\nreading manifest file 'src/wheel_package.egg-info/SOURCES.txt'\nUsing venv/lib/python3.9/site-packages\nFinished processing dependencies for wheel-package==0.0.1\n(venv) $ python setup.py develop --uninstall\nRemoving  /venv/lib/python3.9/site-packages/wheel-package.egg-link \nRemoving wheel-package 0.0.1 from easy-install.pth file\nThis step installs the package directly in the current environment and can be used to \nrun tests and validate that the package is working as expected once installed.\nthis is done, we can prepare the package itself.\nPure Python package\nTo create a package, we first need to define what kind of package we want to create.\nwriting src/wheel_package.egg-info/PKG-INFO\nwriting dependency_links to src/wheel_package.egg-info/dependency_\nwriting requirements to src/wheel_package.egg-info/requires.txt\nwriting top-level names to src/wheel_package.egg-info/top_level.txt\nreading manifest file 'src/wheel_package.egg-info/SOURCES.txt'\nwriting manifest file 'src/wheel_package.egg-info/SOURCES.txt'\ncreating wheel-package-0.0.1\ncreating wheel-package-0.0.1/src\ncreating wheel-package-0.0.1/src/submodule\ncreating wheel-package-0.0.1/src/wheel_package.egg-info\ncopying files to wheel-package-0.0.1...\ncopying LICENSE -> wheel-package-0.0.1\ncopying README.md -> wheel-package-0.0.1\ncopying setup.py -> wheel-package-0.0.1\ncopying src/submodule/__init__.py -> wheel-package-0.0.1/src/submodule\ncopying src/submodule/submodule.py -> wheel-package-0.0.1/src/submodule\ncopying src/wheel_package.egg-info/PKG-INFO -> wheel-package-0.0.1/src/\nwheel_package.egg-info\ncopying src/wheel_package.egg-info/SOURCES.txt -> wheel-package-0.0.1/\nsrc/wheel_package.egg-info\ncopying src/wheel_package.egg-info/dependency_links.txt -> wheel-\npackage-0.0.1/src/wheel_package.egg-info\ncopying src/wheel_package.egg-info/requires.txt -> wheel-package-0.0.1/\nsrc/wheel_package.egg-info\ncopying src/wheel_package.egg-info/top_level.txt -> wheel-\npackage-0.0.1/src/wheel_package.egg-info\nWriting wheel-package-0.0.1/setup.cfg\nremoving 'wheel-package-0.0.1' (and everything under it)\nThe dist package is available in the newly created dist subdirectory:\nwheel-package-0.0.1.tar.gz\nTo generate a proper Wheel package, we need to install the wheel module first:\nInstalling collected packages: wheel\n$ python setup.py bdist_wheel\nadding 'wheel_package-0.0.1.dist-info/LICENSE'\nadding 'wheel_package-0.0.1.dist-info/METADATA'\nadding 'wheel_package-0.0.1.dist-info/WHEEL'\nadding 'wheel_package-0.0.1.dist-info/top_level.txt'\nadding 'wheel_package-0.0.1.dist-info/RECORD'\nwheel_package-0.0.1-py3-none-any.whl\nAll these created packages can be installed directly with pip:\n$ pip install dist/wheel-package-0.0.1.tar.gz\nProcessing ./dist/wheel-package-0.0.1.tar.gz\nWheel packages compatible with both Python 2 and Python 3 can \nmost of the new code in Python is using version 3 and we don't \nSuccessfully built wheel-package\nInstalling collected packages: wheel-package\nSuccessfully installed wheel-package-0.0.\n$ pip uninstall wheel-package\nFound existing installation: wheel-package 0.0.1\nUninstalling wheel-package-0.0.1:\nvenv/lib/python3.9/site-packages/submodule/*\nvenv/lib/python3.9/site-packages/wheel_package-0.0.1.dist-info/*\nSuccessfully uninstalled wheel-package-0.0.1\n$ pip install dist/wheel_package-0.0.1-py3-none-any.whl\nProcessing ./dist/wheel_package-0.0.1-py3-none-any.whl\nInstalling collected packages: wheel-package\nSuccessfully installed wheel-package-0.0.\nNote that the dependencies, in this case, requests, are installed automatically as well \nPython code.\npre-compiled packages, which includes compiled code for a target system.\nTo be able to show that, we need to produce some Python module that contains code \nand interact with the Python code.\nCython is a tool that compiles Python code with some extensions in C, so writing a C \nextension is as simple as writing Python code.\nwhether a number is a prime number with the help of the wheel_package_compiled.\nOnce the pyx file is ready, it can be compiled and imported into Python, using \nInstalling collected packages: cython\nNow, using pyximport, we can import the module directly like a py file.\n>>> import wheel_package_compiled",
      "keywords": [
        "Python",
        "Package",
        "def test",
        "code",
        "file",
        "Python package",
        "wheel",
        "Package Management",
        "Python code",
        "math.sqrt def test",
        "Python Package Index",
        "assert parameter",
        "Wheel files"
      ],
      "concepts": [
        "package",
        "packaging",
        "python",
        "pythonic",
        "tested",
        "code",
        "file",
        "module",
        "different",
        "differences"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.521,
          "base_score": 0.371,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.313,
          "base_score": 0.163,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.308,
          "base_score": 0.158,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "package",
          "wheel",
          "wheel package",
          "wheel_package",
          "py"
        ],
        "semantic": [],
        "merged": [
          "package",
          "wheel",
          "wheel package",
          "wheel_package",
          "py"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2565051495725799,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530219+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Logging\b",
      "start_page": 423,
      "end_page": 440,
      "summary": "venv/lib/python3.9/site-packages/Cython/Compiler/Main.py:369: \nFile: wheel_package_\n.pyxbld/temp.macosx-11-x86_64-3.9/pyrex/wheel_package_\n>>> wheel_package_compiled.check_if_prime(5)\nOnce the code is compiled, Cython creates both a wheel_package_compiled.c file, \nwheel_package_compiled.cpython-39-darwin.so\nUsing pyximport is good for local development, but we can create a package that \ncompiles and packages it as part of the build process.\nPython package with binary code\nWe will use the code we created using Cython to show how to build a package that \nWe create a package called wheel_package_compiled that extends the previous \nexample package, wheel_package, with the code presented to be compiled in Cython.\nwheel_package_compiled\n│   ├── wheel_package.py\n│   └── wheel_package_compiled.pyx\nExtension(\"wheel_package_compiled\", [\"src/wheel_package_compiled.\nmain/chapter_11_package_management/wheel_package_\nname='wheel-package-compiled',\nExtension(\"wheel_package_compiled\", [\"src/wheel_package_compiled.\nCompiling src/wheel_package_compiled.pyx because it changed.\n[1/1] Cythonizing src/wheel_package_compiled.pyx\ncreating 'dist/wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_\nadding 'wheel_package_compiled.cpython-39-darwin.so'\nadding 'wheel_package_compiled-0.0.1.dist-info/LICENSE'\nadding 'wheel_package_compiled-0.0.1.dist-info/METADATA'\nadding 'wheel_package_compiled-0.0.1.dist-info/WHEEL'\nadding 'wheel_package_compiled-0.0.1.dist-info/top_level.txt'\nadding 'wheel_package_compiled-0.0.1.dist-info/RECORD'\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\npackage directly includes all the compiled code, so the package will install quickly, \nWhen working with packages that need to be installed in multiple architectures or \nUploading your package to PyPI\nand upload our packages to the official Python repo to allow any project to use it.\nto upload your package first.\nthat, you'll need to create a new API token to allow the package to be uploaded.\nFigure 11.5: You'll need to grant the full scope to upload a new package\nThe next step is to install the twine package, which simplifies the process of \npackage.\nNow we can upload the packages created in the dist subdirectory.\nwheel-package-compiled-0.0.1.tar.gz\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nLet's upload the packages.\nUploading wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nUploading wheel-package-compiled-0.0.1.tar.gz\nhttps://test.pypi.org/project/wheel-package-compiled/0.0.1/\nFor our example, we will use the same package created previously, \nTo do your tests, create your own package with a unique \nThe package is now uploaded!\n(venv2) $ pip install --index-url https://test.pypi.org/simple/ wheel-\npackage-compiled\nCollecting wheel-package-compiled\nDownloading https://test-files.pythonhosted.org/packages/87/c3/88129\n8cdc8eb6ad23456784c80d585b5872581d6ceda6da3dfe3bdcaa7ed/wheel_package_\nDownloading https://test-files.pythonhosted.org/packages/6d/00/8ed\nInstalling collected packages: requests, wheel-package-compiled\nSuccessfully installed requests-2.5.4.1 wheel-package-compiled-0.0.1\nYou can now test the package through the Python interpreter:\n>>> import wheel_package_compiled\n>>> wheel_package_compiled.check_if_prime(5)\nThe package is now installed and ready to use.\nIt is possible that we need to create our own index with our packages.\nYou can create your own private index that can be used to share those packages and \nTo serve the packages, we need to run a PyPI server locally.\nTo run pypiserver, first, install the package using pip and create a directory for \nInstalling collected packages: pypiserver\nto facilitate the uploading of packages without \nWe now need to upload the packages, using twine again, but pointing to our private \nUploading wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nUploading wheel-package-compiled-0.0.1.tar.gz\nThe index is now showing the package available.\nFigure 11.11: Showing the package uploaded\nFigure 11.12: All the uploaded files for the package\nThe files are also uploaded to the package-library directory:\nwheel-package-compiled-0.0.1.tar.gz\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nThe package can now be downloaded and installed, pointing to your private index \n$ pip install --index-url http://localhost:8080 wheel-package-compiled\nCollecting wheel-package-compiled\nDownloading http://localhost:8080/packages/wheel_package_compiled-\n3.2 requests-2.26.0 urllib3-1.26.6 wheel-package-compiled-0.0.1\n>>> import wheel_package_compiled\n>>> wheel_package_compiled.check_if_prime(5)\nWe described the simplest possible package in Python just by structuring code, but \nwithout creating a proper package.\ndescribed the Wheel package, which will be the kind of package that we will create \nNext, we described how to create such a package, creating a setup.py file.\nWe showed how to upload packages to PyPI to distribute publicly (showing how to \nupload to TestPyPI, allowing the upload of packages to be tested) and described how \nto create your own individual index so that you can distribute your own packages \npackage (https://python-poetry.org/) to see how to manage ",
      "keywords": [
        "Package",
        "wheel",
        "Package Management",
        "Python",
        "compiled",
        "create",
        "Wheel package",
        "code",
        "printed",
        "8:47",
        "subject",
        "Cython",
        "compiled Wheel",
        "Management",
        "upload"
      ],
      "concepts": [
        "package",
        "packaging",
        "python",
        "compiled",
        "compiles",
        "install",
        "file",
        "requests",
        "wheel",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.521,
          "base_score": 0.371,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.434,
          "base_score": 0.284,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "package",
          "wheel_package_compiled",
          "wheel",
          "compiled",
          "packages"
        ],
        "semantic": [],
        "merged": [
          "package",
          "wheel_package_compiled",
          "wheel",
          "compiled",
          "packages"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23028203700001587,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530234+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Metrics\b",
      "start_page": 441,
      "end_page": 462,
      "summary": "Logging\nWe will start by understanding how to use logs for monitoring.\nLogging\nLogs allow us to \nUsing logs correctly is deceptively difficult, though.\ntoo little information, or to log the wrong information.\nLog basics\nProducing logs in Python\nDetecting problems through logs\nLog strategies\nAdding logs while developing\nLog limitations\nLogging\nLog basics\nLogs are basically messages produced by the system as it runs.\nNormally, logs are generated as plaintext messages.\nAs well as the main message text, each log contains some metadata about what \nsystem produced the log, what time the log was created, and so on.\nIf the log is in \nAnother important metadata value is the severity of the log.\ncategorize the different logs by their relative importance.\nIt's important to categorize the logs with their proper severity and filter out \nEach logging facility can \nbe configured to only produce logs at one severity level or more.\nIt's possible to add custom log levels instead of the predefined \nmost cases, as the log levels are well understood by all tools and \nof the logs will be generated as part of dealing with a request, which will produce \nseveral logs indicating what the request is doing.\nnormally be undergoing processing at once, the logs will be generated intermixed.\nFor example, consider the following logs:\nThe preceding logs show two different services, as indicated by the different IP \nLogging\nNo matter how we collect logs, a typical system will produce a lot of them, and \nAny log system should be \nIn general, a retention policy based on time (such as keeping logs from \nGenerating log entries is easy, as we will see in the next section, Producing logs in \nProducing logs in Python\nPython includes a standard module to produce logs.\nA basic program to create logs looks like this.\nThis is available as basic_logging.\nyou can collect logs smaller.\nimport logging\n# Generate two logs with different severity levels\nlogging.warning('This is a warning message')\nlogging.info('This is an info message')\nThe .warning and .info methods create logs with the corresponding severity \n$ python3 basic_logging.py\nconfigured not to display INFO logs.\nThe format of the logs is also the default, which \nlogging in Python:\nA handler, which decides how the logs are propagated.\nthe logs through the formatter, as defined above.\nA logger, which produces the logs.\nhow the logs are propagated.\nWith this information, we can configure the logs to specify all the details we want:\nimport logging\n# Create a handler that sends the logs to stdout\nLogging\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.INFO)\n# Generate three logs\nlevelname is the severity of the log, such as INFO, WARNING, or ERROR.\nmessage, finally, is the log message.\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.INFO)\nThis allows us to divide the logs of the application into subsections.\nFinally, we define the level to log as INFO using the .setLevel method.\ndisplay all logs of the level INFO and higher, while those lower won't be.\n$ python3 configured_logging.py\nThe logging module in Python is capable of high levels of configuration.\nwhich rotates the logs based on time, meaning it stores the last \nhttps://docs.python.org/3/howto/logging.html#useful-\nLogging\nDetecting problems through logs\nterms of logs and how we handle them.\nExpected errors are errors that are detected explicitly by creating an ERROR log in the \nFor example, the following code produces an ERROR log when the accessed URL \nimport logging\nlogging.error(f'Error accessing {URL} status code {status_code}')\nThis code, when executed, triggers an ERROR log:\nThe block where the log is generated could perform some \nimport logging\nlogging.info(f'GET {URL}')\nlogging.error(f'Error accessing {URL} status code {status_code}')\nof adding ERROR or WARNING logs when deprecated actions are \nLogging\nmessages as ERROR logs, depending on how the configuration is set up.\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\ntoward the logs and generate a proper 500 status code, indicating \nlogging.info('--- New request ---')\nlogging.info(f'GET {URL}')\nTo be sure that not only is the error logged, but also the full stack trace, we log it \nmessage while logging it with ERROR severity.\nWhen we run the command, we get these logs.\nLogging\nAs you can see, the logs include Traceback, which allows us to detect a specific \nAny unexpected error should be logged as ERROR.\nLog strategies\nA common problem when dealing with logs is deciding on the appropriate severity \nwhich is sometimes difficult when just having access to the logs.\ndevelopers what to do when a given error log is found.\nLog level\nINFO logs show generic \nTrack the number of logs.\nWARNING logs track errors \nTrack the number of logs.\nERROR logs track errors \nCRITICAL logs indicate a \nLogging\nan acceptance that there'll be a certain number of ERROR logs generated.\nWARNING logs are indications that something may not be working as smoothly as \nnumber of logs of this kind.\nINFO logs are just there to give context in the event of a \nIn production situations, ERROR logs will typically be categorized \nas few ERROR logs as possible, so all of them can be meaningful.\nRemember that ERROR logs will include unexpected errors that \nnumber of ERROR logs will increase significantly.\nA common mistake is to generate ERROR logs in actions where there \nsending a bad password, a WARNING log can be created.\npoint in creating an ERROR log when the application is behaving as \nIn web applications, as a rule of thumb, ERROR logs should only \nWith common and shared definitions of log levels across the team, all engineers \nto deal with logs created before the definition, which can require work.\nAdding logs while developing\nAny test runner will capture logs and display it as part of the trace while running \nThis is a good opportunity to check that the expected logs are being generated while \na corresponding log and, while developing the feature, check that they are being \nWhile developing, DEBUG logs can be used to add extra information about the code \ncan help fill in the gaps between INFO logs and help developers to solidify the habit \nof adding logs.\nA DEBUG log may be promoted to INFO if, during tests, it's found to be \nunderstand why it's useful to have logs while developing to make \nLogging\nthis has big implications on the number of generated logs, which can lead to storage \nIt's always a good idea to double-check that the logs are being properly captured and \nAll the configuration to ensure that the logs are \ninvolves capturing unexpected errors and other logs in production and checking that \nLog limitations\nLogs are very useful to understand what's happening in a running system, but they \nLogs are only as good as their messages.\nin making logs useful.\nReviewing the log messages with a critical eye, \nHave an appropriate number of logs.\nToo many logs can confuse a flow, and \nLarge numbers of logs also create problems with storage.\nLogs should work as an indication of the context of the problem, but likely won't \nexample, for a request, make sure to log both the request and its parameters \nseverity logs.\nlogs are generated.\nLogs allow us to follow the execution of a single instance.\nusing a request ID or similar, logs can be grouped by execution, allowing us \nHowever, logs don't directly display \nLogs only work retrospectively.\nWhen a problem in a task is detected, logs \nimportant to analyze critically and refine the information, removing logs that \nIn this chapter, we started by presenting the basic elements of logs.\nhow logs contain messages plus some metadata like a timestamp, and considered \ngroup logs related to the same task.\ncollection of all logs in the system.\nWe then showed how to produce logs in Python using the standard logging module, \nshould be taken when a log of a certain severity is detected, instead of categorizing \nthe logs in terms of \"how critical they are\", which ends up generating vague \nThere are tools available to create metrics based on logs.\nLogging\nWe discussed several habits to improve the usefulness logs by including them in \npresented in logs while writing tests and producing errors, which presents the \nperfect opportunity to ensure that the logs generated work correctly.\nAs well as logging, the other key element of observability is metrics.\nMetrics versus logs\nobservability, logs.\nMetrics versus logs\nAs we saw in the previous chapter, logs are text messages produced as code is \nWhile each recorded log is a text \nNormally, the logs analyzed will all be related to a single task.\nthat metrics are much more lightweight compared with logs.\nin an aggregated way, as information for a single task, like generated logs, may not \nof requests or the number of generated errors.",
      "keywords": [
        "logs",
        "ERROR logs",
        "ERROR",
        "log",
        "INFO",
        "ERROR log",
        "request",
        "INFO logs",
        "code",
        "system",
        "Logging",
        "logs track errors",
        "URL",
        "information",
        "Metrics"
      ],
      "concepts": [
        "logging",
        "log",
        "errors",
        "likely",
        "requests",
        "request",
        "problem",
        "metrics",
        "message",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.422,
          "base_score": 0.272,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.377,
          "base_score": 0.377,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.339,
          "base_score": 0.339,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "logs",
          "logging",
          "log",
          "error",
          "severity"
        ],
        "semantic": [],
        "merged": [
          "logs",
          "logging",
          "log",
          "error",
          "severity"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.349982643544138,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530252+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Profiling\b",
      "start_page": 463,
      "end_page": 500,
      "summary": "Metrics\nFor example, tracking the request time will also count the number of \nDefining which metric is adequate for the specific value to measure is important.\nwe can divide the number of requests that return an error code by the total number \nEvery time there's a metric produced, an event gets pushed toward the metrics \nWe will use some examples with Prometheus, a metrics system that uses the pulling \nGenerating metrics with Prometheus\nPrometheus is a popular metrics system that is well supported and easy to use.\nwill use it as an example during the chapter to show how to collect metrics and how \nAs we saw before, Prometheus uses the pulling approach to metrics generation.\nmeans that any system that produces metrics will run its own internal Prometheus \nis the approach taken by the django-prometheus module, which will automatically \ncollect a lot of common metrics for a Django web service.\ndjango-prometheus\nWe will build up from the Django application code presented \nmain/chapter_13_metrics/microposts.\nMetrics\nRunning setup.py install for django-rest-framework ...\nAccess these pages a couple of times to produce metrics that we can later access.\nConfiguring Django Prometheus\nThe configuration of the django-prometheus module is done in the microposts/\nFirst, add the django-prometheus application to the installed app list which enables \nMetrics\n'django_prometheus',\n'django_prometheus.middleware.PrometheusBeforeMiddleware',\n'django_prometheus.middleware.PrometheusAfterMiddleware',\nCheck the position of django.prometheus.middleware.PrometheusBeforeMiddleware \nand django_prometheus.middleware.PrometheusAfterMiddleware.\nRemember, an important element for the Prometheus system is \nthat each application serves its own metric collection.\npath('', include('django_prometheus.urls')),\nThe path('', include('django_prometheus.urls')) line sets up a /metrics URL \nChecking the metrics\nThe main URL root shows that there's a new endpoint – /metrics:\nWhen accessing the /metrics endpoint, it shows all the collected metrics.\nthere are a lot of metrics that are collected.\nto be collected by a Prometheus metric server.\nmetrics.\nYou can check how some metrics, like django_http_\nMetrics\nFigure 13.3: The raw Prometheus metrics, as collected by the application\nThe next step is to start a Prometheus server that can pull the info and display it.\nStarting a Prometheus server\nThe Prometheus server will pull periodically for metrics to all the configured \napplications that are collecting their metrics.\nPrometheus.\nThe easiest way to start a Prometheus server is to start the official Docker image.\nPacktPublishing/Python-Architecture-Patterns/blob/main/chapter_13_metrics/\nThis is to ensure that the Prometheus server can access the Django application that's \nMetrics\nDouble-check that you can access the metrics in the local IP.\nWith all this information, you are now ready to start the Prometheus server in \nMetrics\nAfter the Prometheus server is up and running, the server is accessible at http://\nPrometheus has its own query system, called PromQL, and ways of operating with \npull approach to metrics.\nFor example, requesting one useful metric, like django_http_requests_latency_\nFigure 13.6: Notice how the prometheus-django-metrics view is called more often, as it is called  \nMetrics\nrate(django_http_requests_latency_seconds_by_view_method_count[1m]) shows \nAs you can see, there's a constant number of requests from prometheus-django-\nmetrics, which is Prometheus requesting the metrics information.\n15:55, at the time where we manually generated some requests to the service.\nsum(rate(django_http_requests_latency_seconds_by_view_method_\nFigure 13.8: Note the bottom value is based on the baseline created by the calls to prometheus-django-metrics\nTo plot times instead, the metric to use is the django_http_requests_latency_\nThe bucket metrics are generated in a way \nMetrics\nhistogram_quantile(0.95, rate(django_http_requests_latency_seconds_by_\nFigure 13.9: Note how the metrics collection is much faster than the user-collection requests\nquantile 0.50 (the maximum time it takes for half of the requests), \nthe quantile 0.90 (the maximum time for most of the requests), and \nquantile 0.99 for the very top time it takes to return a request.\nTo plot times instead, the metric to use is the django_http_requests_latency_\nThe bucket metrics are generated in a way \nMetrics can also be filtered to display only specific labels, and a good number of \nCheck the Prometheus documentation about queries to find out more: https://\nto display the result of several metrics, such as the percentage of \nMetrics\nTraffic: The number of requests flowing through the system per unit of time, \nfor example, the number of requests per minute.\nWhen problems are detected through the metrics, an automatic alert should be \nPrometheus has an alert system that will raise when a defined metric \nCheck out the Prometheus documentation on alerting for more \ninformation: https://prometheus.io/docs/alerting/latest/\nNormally, alerts will be configured when the value of metrics is crossing some \nFor example, the number of errors is higher than X, or the time to return a \nIn this chapter, we described what metrics are and how they compare with logs.\ndescribed how metrics are useful to analyze the general state of the system, while \nPrometheus, a common metrics system that uses the pull approach on how to \nWhile alerts can be generated directly from metrics, there are \nbased on the number of ERROR logs, or more complicated metrics.\nMetrics\nWe set an example of how to generate metrics automatically in Django by installing \nand configuring the django-prometheus module, and how to start a Prometheus \nserver that scrapes the generated metrics.\nNext, we described how to query metrics in Prometheus, introducing PromQL, and \nshowed some common examples of how to display metrics, plot rate to see clearly \nhow the metrics are changing over time, and how to use the histogram_quantile \nFinally, we introduced alerts as a way to be notified when metrics are out of a normal \nmetrics.\nCheck the Prometheus client to see how, for example, for Python: \nhttps://github.com/prometheus/client_python.\nProfiling\nPerhaps some requests are taking too much time, or perhaps the \nthe most time or memory.\nTo get information on what exactly is going on and follow the code flow, we can use \nprofilers to dynamically analyze the code and better understand how the code is \nProfiling code for time\nMemory profiling\nProfiling\nProfiling is a dynamic analysis that instruments code to understand how it runs.\nThe normal application of profiling is to improve the performance of the code under \ncode takes to execute) or memory performance (how much memory the code takes to \nSome code may take too long to execute or use a \nmemory profiler.\nThere are two main kinds of time profilers:\nA deterministic profiler \ndeterministic profilers very detailed, as they can follow up the code on each \nstep, but at the same time, the code is executed slower than without the \nProfiling\nWhile they don't give as detailed information as deterministic profiles, \nStatistical profilers are good tools for \nThe deterministic profilers are tools for analyzing specific use cases in the petri dish \nprofiler.\nA memory profiler records when \nProfiling \nIn the case of Python, memory leaks can be created by three main use cases, from \nIn some respects, statistical profilers are analogous to metrics \nProfiling\nrequire further investigation with specific C profiling tools, which is out of \nof a memory profiler.\nLet's introduce some code and profile it.\nProfiling code for time\nPrime numbers are numbers that are only \ndef check_if_prime(number):\nif number % i == 0:\nThis code will take every number from 2 to the number under test (without \nincluding it), and check whether the number is divisible.\nthe number is not a prime number.\nMemory profiling is typically more complicated and takes more \neffort than time profiling.\nmistakes, we will include the first prime numbers lower than 100 and compare \nprofiling/primes_1.py.\ndef check_if_prime(number):\nif number % i == 0:\nif check_if_prime(number)]\nThe calculation of prime numbers is performed by creating a list of all numbers \nif check_if_prime(number)]\nProfiling\nThe next line asserts that the first prime numbers are the same as the ones defined in \n$ time python3 primes_1.py\nThe easiest, faster way of profiling a module is to directly use the included cProfile \n$ time python3 -m cProfile primes_1.py\n4999    0.754    0.000    0.754    0.000 primes_1.py:7(check_if_\nNote this called the script normally, but also presented the profile analysis.\nncalls: Number of times each element has been called\ntottime: Total time spent on each element, not including sub calls\npercall: Time per call on each element (not including sub calls)\nProfiling\nIn this case, the time is clearly seen to be spent in the check_if_prime function, which \n$ time python3 -m cProfile -o primes1.prof  primes_1.py\ncProfile increases the time it takes to execute the code.\nProfiling\nFigure 14.2: Checking the information about check_if_prime.\nWe can confirm here that the bulk of the time is spent on check_if_prime, but we \nWe will see how to use a profiler that has a higher resolution, analyzing each line \nLine profiler\nTo analyze the check_if_prime function, we need to first install the module line_\nprofiler\n$ pip3 install line_profiler\nAfter it's installed, we will make a small change in the code, and save it as primes_2.\nWe will add the decorator @profile for the check_if_prime function, to indicate \nPatterns/blob/main/chapter_14_profiling/primes_2.py.\n@profile\ndef check_if_prime(number):\nif number % i == 0:\nMost of the time, localizing the function that's taking most \nsituations, the time spent will be on external calls like DB accesses, \nKeep in mind that you should only profile sections of the code \nprofiled in this way, it would take a lot of time to analyze.\nProfiling\nline_profiler.\n$ time kernprof -l primes_2.py\nWrote profile results to primes_2.py.lprof\n$ python3 -m line_profiler primes_2.py.lprof\nFunction: check_if_prime at line 7\n7                                           @profile\nprime(number):\nnumber):\n12  12487503    3749127.0      0.3     54.2          if number % i \nreduce the number of times they're being called.\nThe code will be like this \n(stored in primes_3.py, available at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_14_profiling/primes_3.py):\n@profile\ndef check_if_prime(number):\nif number % i == 0:\n$ time kernprof -l primes_3.py\n$ python3 -m line_profiler primes_3.py.lprof\nFunction: check_if_prime at line 7\nProfiling\n7                                           @profile\nprime(number):\nFor example, for 19, we try these numbers (as 19 is a prime number, it's not \nSo we check only the numbers \nTo apply all of this, we need to tweak the code again and store it in primes_4.py, \nPatterns/blob/main/chapter_14_profiling/primes_4.py:\ndef check_if_prime(number):\nThe code always checks for divisibility by 2, unless the number is 2.\nrange step on 2 integers at  time so that all the numbers are odd, since we started \nProfiling\nFor example, to test a number like 1,000, this is the equivalent code.\nLet's profile the code again.\n$ time kernprof -l primes_4.py\nWrote profile results to primes_4.py.lprof\n$ python3 -m line_profiler primes_4.py.lprof\nFunction: check_if_prime at line 8\n8                                           @profile\nprime(number):\nprimes_3.py and over 12 million in primes_2.py, when we started the line profiling.\nIn many scenarios, profilers will be useful in environments where the system is in ",
      "keywords": [
        "Metrics",
        "Prometheus",
        "number",
        "time",
        "primes",
        "code",
        "Prometheus server",
        "Profiling",
        "prime numbers",
        "8:47",
        "subject",
        "printed",
        "Prometheus metric server",
        "requests",
        "system"
      ],
      "concepts": [
        "metrics",
        "profiling",
        "profiles",
        "times",
        "timing",
        "prometheus",
        "django",
        "prime",
        "likely",
        "number"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.381,
          "base_score": 0.231,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.361,
          "base_score": 0.361,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metrics",
          "prometheus",
          "profiling",
          "number",
          "check_if_prime"
        ],
        "semantic": [],
        "merged": [
          "metrics",
          "prometheus",
          "profiling",
          "number",
          "check_if_prime"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35247363013383526,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530278+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Debugging\b",
      "start_page": 501,
      "end_page": 530,
      "summary": "Profiling\nIf we want to analyze a particular web request, we may need to start a web server, \nproduce a single request, and stop the process to obtain the result.\nExample web server returning prime numbers\nWe will use the final version of the function check_if_prime and create a web service \nthat returns all the primes up to the number specified in the path of the request.\ncode will be the following, and it's fully available in the server.py file on GitHub at \nchapter_14_profiling/server.py.\ndef check_if_prime(number):\nif number % i == 0:\ndef prime_numbers_up_to(up_to):\nif check_if_prime(number)]\nreturn prime_numbers_up_to(param)\nProfiling\nreturn prime_numbers_up_to(param)\nFor any errors, it returns None.\ndef prime_numbers_up_to(up_to):\nif check_if_prime(number)]\nProfiling\nAnd then tested by going to http://localhost:8000/500 to try to get prime numbers \nProfiling the whole process\nWe can profile the whole process by starting it under cProfile and then \nWe start it like this, make a single request to http://\nWe have stored the results in the file server.prof.\ncode was waiting for a new request, and internally doing a poll action.\nfor get_result, which is the root of the interesting bits of our code.\nProfiling\nBut this approach has some problems:\nIf we process two different requests, they will be added into the same file, \nThese problems can be solved by applying the profiler to only the part that is of \ninterest and producing a new file for each request.\nGenerating a profile file per request\nTo be able to generate a different file with information per individual request, \nThis will profile and produce an \nIn the file server_profile_by_request.py, we get the same code as in server.py, but \nfilename = f'profile-{time()}.prof'\nProfiling\nInside, we start a profiler and run the function under it using the runcall function.\nThis line is the core of it – using the profiler generated, we run the original function \nWe also decorate the get_result function, so we start our profiling there.\n@profile_this\nreturn prime_numbers_up_to(param)\nThe full code is available in the file server_profile_by_request.py, available on \nblob/main/chapter_14_profiling/server_profile_by_request.py.\n$ python3 server_profile_by_request.py\nprofile-1633882197.634005.prof \nprofile-1633882200.226291.prof\nFigure 14.6: The profile information of a single request.\na specific request, so it can be profiled individually, with a high level of detail.\nto create a random sample, so only 1 in X calls produces profiled code.\nreduce the overhead of profiling and allow you to completely profile some requests.\nNext, we'll see how to perform memory profiling.\nprofile some requests, instead of detecting what's going on at a \nProfiling\nMemory profiling\nthey use more and more memory as time goes by, normally due to what's called a \nOther problems can also include the fact that the usage of memory may \nTo profile memory and analyze what the objects are that use the memory, we need \nfirst to create some example code.\nWe will generate enough Leonardo numbers.\nThe first Leonardo number is one\nThe second Leonardo number is also one\nWe present the first 35 Leonardo numbers by creating a recursive function and store \nPython-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_1.py.\ndef leonardo(number):\nif number in (0, 1):\nreturn leonardo(number - 1) + leonardo(number - 2) + 1\nNUMBER = 35\n$ time python3 leonardo_1.py\nWe change the code like this, creating the leonardo_2.py file (available on GitHub at \nchapter_14_profiling/leonardo_2.py).\ndef leonardo(number):\nif number in (0, 1):\nif number not in CACHE:\nresult = leonardo(number - 1) + leonardo(number - 2) + 1\nCACHE[number] = result\nreturn CACHE[number]\nNUMBER = 35000\nProfiling\nThis uses a global dictionary, CACHE, to store all Leonardo numbers, speeding up the \n$ time python3 leonardo_2.py\nUsing memory_profiler\nNow that we have our application storing information, let's use a profiler to show \nWe need to install the package memory_profiler.\nprofiler.\n$ pip install memory_profiler\nWe can now add a @profile decorator in the leonardo function (stored in \nleonardo_2p.py, on GitHub at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_14_profiling/leonardo_2p.py), and run \nit using the memory_profiler module.\n$ time python3 -m memory_profiler leonardo_2p.py\n6                                         def leonardo(number):\n13  104.277 MiB    1.914 MiB       34999           CACHE[number] = \n15  104.277 MiB    0.000 MiB      104994       return CACHE[number]\na new Leonardo number and when we store it in the CACHE dictionary.\nWe don't really need to keep all the previous Leonardo numbers in memory at all \nProfiling\nWe create the file leonardo_3.py with the following code, available on GitHub at \nchapter_14_profiling/leonardo_3.py:\n@profile\ndef leonardo(number):\nif number in (0, 1):\nif number not in CACHE:\nresult = leonardo(number - 1) + leonardo(number - 2) + 1\nCACHE[number] = result\nret_value = CACHE[number]\nNUMBER = 35000\nNote we keep the @profile decorator to run the memory profiler again.\nThis code will keep the number of elements in the CACHE dictionary within a limit.\nprofiling.\n$ time python3 -m memory_profiler leonardo_3.py\n6                                         def leonardo(number):\n13   38.441 MiB    0.000 MiB       34999           CACHE[number] = \nCACHE[number]\nProfiling\nThe memory-profiler module is also able to perform more actions, including \nIn this chapter, we described what profiling is and when it's useful to apply it.\ndescribed that profiling is a dynamic tool that allows you to understand how code \nand memory profilers.\nperformance of code and memory profilers analyze the memory used by the code in \nDeterministic profilers instrument the code to detail the flow of the code \nStatistical profilers sample the code at periodic times to provide a \nWe then showed how to profile the code using deterministic profilers, presenting \nThe next step was to see how to profile a process intended to keep running, like a \nWe showed the problems with trying to profile the whole application \nin these cases and described how we can profile each individual request instead \nFinally, we also presented an example to profile memory and see how it's used by \nusing the module memory-profiler.\nIn the next chapter, we will learn more details about how to find and fix problems in \ncertain times or one of each 100 requests.\nGenerally speaking, the cycle for debugging problems has the following steps:\n1.\t Detecting the problem.\ntime on meaningful problems and focus on the most important ones\na way of replicating the problem in a local environment\n4.\t Replicating the problem locally, and getting into the specific details on why it \n5.\t Fixing the problem\nUnderstanding the problem in production\nThe first step is actually detecting the problem.\nDetecting problems can be done in different ways, and some may be more evident \nBased on how problems are detected, we can categorize them into different \nCritical problems that stop the execution of some tasks, but not others\nSerious problems that will stop or cause problems with certain tasks, but \nMild problems, which include tasks containing errors or inaccuracies.\nexample, a task produces an empty result in certain circumstances, or a \nproblem in the UI that doesn't allow calling a functionality\n\"bug.\" The proper tool to fix the problem could be different, but the \na clear view and be efficient by spending time on important problems and not \nFixing bugs is important, not only for the resulting quality of the service, as any user \nThe problem can \nbe ideally replicated into a test, so it can be tested over and over until the problem is \nthe problem affects more than one system, it may be necessary to create integration \nVisually inspecting the code and trying to reason where problems and bugs are is \nBeing able to analyze how, in a particular case, the code is executing \nwith precision is critical for analyzing and fixing problems that are found.\nOnce we are aware that we have a problem in production, we need to understand \nThe most important tools when analyzing why a particular problem is produced are \nto be sure to be able to find problems when required.\nan increase in returned errors can be important to detect that there's an error, but \nexample, if there's a single server that's producing errors, or if it has run out of \nproblem can be complicated in this environment.\nin the chapter about finding a problem in production.\nreplicate a problem.\nAs we saw in Chapter 12, Logging, we can describe error logs \nas detecting two kinds of problems:\nIn this case, we did the work of debugging the error \ncan be an external request that returns an error, a database that cannot be \nstack trace of some sort in the logs detailing the line of code when the error \nweb server will return a 500 error, and a task management system may retry \nerrors, if the external requests are directed to different servers, \nIn any of the two cases, the main tool to detect what the problem was will be logs.\nEither the logs show a known problem that is captured and properly labeled, or the \nlogs show a stack trace that should indicate what specific part of the code is showing \nFinding the element and part of the code that is the source of the error is important \nfor understanding the problem and for debugging the specific problem.\nUnderstanding the problem in production\nLogging a request ID\nOne of the problems when analyzing a large number of logs is correlating them.\nunique identifier per task or request that can trace all logs coming from the same \nproblems in the integration of its different parts.\nOnce the problem can be categorized as a replicable set of steps \nis very important to keep a common request ID that can work to trace the different \nBecause all of them share the same request ID, logs can be filtered by that \nTo achieve this, we can use the module django_log_request_id to create a request ID \ncom/dabapps/django-log-request-id/.\nThe code has been changed to include some extra logs in the microposts/api/views.\npy file (as seen at https://github.com/PacktPublishing/Python-Architecture-\nreturn result\nreturn result\nTo enable the usage of the request ID, we need to properly set up the configuration \nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\n'()': 'log_request_id.filters.RequestIDFilter'\nfilters adds extra information, in this case, our request_id, formatter describes the \nspecific format to use (note that we add request_id as a parameter, which will be \nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\nstate that a new Request ID parameter should be created if not found as a header in ",
      "keywords": [
        "number",
        "Leonardo",
        "Leonardo numbers",
        "problem",
        "MiB",
        "code",
        "request",
        "CACHE",
        "result",
        "memory",
        "Profiling",
        "printed",
        "8:47",
        "subject",
        "param"
      ],
      "concepts": [
        "profiling",
        "profile",
        "problems",
        "logs",
        "logging",
        "log",
        "numbers",
        "returning",
        "time",
        "server"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.377,
          "base_score": 0.377,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.374,
          "base_score": 0.374,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 8,
          "title": "",
          "score": 0.334,
          "base_score": 0.334,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.328,
          "base_score": 0.328,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "leonardo",
          "profile",
          "number",
          "leonardo number",
          "profiling"
        ],
        "semantic": [],
        "merged": [
          "leonardo",
          "profile",
          "number",
          "leonardo number",
          "profiling"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3157967381404457,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:03.530322+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Ongoing Architecture\b",
      "start_page": 531,
      "end_page": 595,
      "summary": "At the same time, you'll see the logs on the server screen:\nWhich, as you can see, added a new request ID element, 66e9f8f1b43140338ddc3ef56\nYou can check the logs in the server again:\nThe request ID can be passed over other services by using the Session included in \nfrom log_request_id.session import Session\nthe chain, like service A or service B.\nIf the default logs are not enough to understand the problem, the next stage in those \ncases is understanding the data related to the problem.\nmay be inspected to follow up on the related data for the task, to see if there's any \nAnalyzing the stored data may require performing ad hoc manual queries to \ndatabases or other kinds of data storage to find out if the related data is consistent or \nIf investigating the data is not enough to be able to understand the problem, it may \nbe necessary to increase the information on the logs.\nBe sure to check the django-log-request-id documentation.\nproduction to be able to understand and replicate the problem \nIn some cases, when investigating a problem in production, it is \npossible that changing the data manually will fix the issue.\nthe data has been possible or how the service should be changed \nThen the code can be \nchanged accordingly to ensure that the problem doesn't happen in \nIf the regular logs and an investigation of the data don't bear fruit, it may be \nnecessary to increase the level of logging with special logs, following the problem.\nAny change in the logs needs to be deployed, which makes it costly and \nThe number of logs in the system will be incremented, which will require \nsystem, this can create pressure on the logging system.\nWhile enabling an extra level of logging, like setting logs to DEBUG level, is technically \npossible, this will probably increase the logs too much, and will make it difficult to \nWith some DEBUG logs, \nhigher to make sure that they are properly logged.\nlike PII should not be logged.\nInstead, try to log surrounding information that can \nhelp find out the problem.\na problem with the algorithm to check the password, instead of logging the \npassword, some code can be added to detect whether there's an invalid character.\nFor example, assuming there's a problem with a password or secret that has an \nThe value in bad_characters can be then logged, as it won't contain the full \ndeployments until finding out the problem.\nRemember that the work is just to be able to reproduce the problem locally, so you \ncan more efficiently investigate and fix the problem locally.\nSometimes the problem \nin Chapter 10, Testing and TDD, tests displaying and then fixing the bug.\nOnce we can detect the problem locally, it is time to go to the next step.\nDebugging locally means exposing and fixing a problem once we have a local \nThe basic steps of debugging are reproducing the problem, knowing what the \nTaking a step back, any debugging process follows the following process:\n1.\t You realize there's a problem\n4.\t You fix the problem\nwithout any secret data with a unit test.\nA great way of creating the reproduction of the problem is with a \ntest, if that's possible.\nCreate a test that fails and then change \nthe code to make it pass.\nKeeping this process in mind is also useful from a local debugging perspective, \npossible, or creating a specific \"experiment\" (some specific code, like a test) to \n4.\t Use the resulting information to iterate the process until the source of the \nNote that this process doesn't necessarily need to be applied to the whole problem.\nIt can be focused on the specific parts of the code that can influence the problem.\nexample, is this setting activated in this case?\ndifferent code path?\nTake small steps and isolate areas of the code so \nit's possible to simplify the code and make it digestible.\nunderstanding when there's a problem in the code is detecting when there \nof a problem is not where an error is raised or obvious, but instead the \nallows you to ignore all code that comes after the problem, and have a clear \nacross different parts, making assumptions about what the rest is returning.\nassumptions from the mind will reduce the amount of code to analyze and \nof the code when it really is in another.\nor a type error that needs to be checked.\nBefore we move on to specific techniques, we need to understand the tools in Python \nAs Python is a dynamic language, it's very flexible and allows you to perform actions \nit can be used while debugging to discover the attributes and methods of any object.\n>>> my_object = {'example': True}\n>>> another_object = {'example'}\nThis can be used to double-check that an object is of the expected type.\nA typical example error is to have a problem because a variable can be either an \nobject or None.\nWhile type is useful in debugging environments, avoid using it directly in your code.\nobjects, so every time that we need to verify if an object is None, it's better to make an \n>>> object = None\n>>> object is None\nKeeping the code simple helps a lot in later debugging problems.\nSimple code is easy to understand and debug.\n>>> if not object:\n>>> object = None\n>>> if not object:\n>>> if object is None:\n>>> object = None\n>>> if object is None:\nobject is None\nFor example, in the following code we see \nhow an object from a class that inherits from another will return that it's an instance \nthe methods and attributes in an object, and it's particularly useful when analyzing \nHelp on C in module __main__ object:\nAll these methods can help you navigate code that's new or under analysis without \ncode.\nDebugging with logs\nA simple yet effective way of detecting what's going on and how the code is being \ncode locally in a test or similar.\nObviously, these print statements need to be removed after the process has \ninstead, as we introduced in Chapter 12, Logging.\nthe code well commented and adding context for developers \nworking in the code, but also in case of debugging in parts \nAnother important advantage is that tests can be run very quickly, as adding more \nlogs is a simple operation, and logs won't interfere with the execution of code.\nThe fact that the logs won't interfere with the code and code can be running \nWhile debugging through logs can be quite convenient, it requires certain knowledge \nAnother problem is that new logs are new code, and they can create problems if \nto fix, but can be an annoyance and require a new run.\nIdeally, these logs will be DEBUG logs, which will only be displayed \nwhen running tests, but won't be produced in a production \nWhile logs can be added and not produced later, it's good practice \nanyway to remove any spurious logs after fixing the bug.\nLogs \nnormally require extensive logs to try to capture the specifics of \nthat problem.\nIn other situations, it's better to stop the execution of the code and take a look at the \nexecution of the script and enter the interpreter, we can run any kind of code and see \nmakes it possible to understand interactively what the code is doing.\nLet's take a look at some code and analyze how it runs.\nThe code can be found on \nbreakpoint is a relatively new addition to Python, available since \nPerhaps you are able to understand what the code does, but let's take a look at it \n$ python3 debug.py\n$ python3 debug.py\nAnother two useful debug commands are s(tep), to get into a function call, and \nr(eturn), to execute the code until the current function returns its execution.\n$ PYTHONBREAKPOINT=IPython.core.debugger.set_trace python3 debug.py\nremote-pdb (https://github.com/ionelmc/python-remote-pdb): Allows you \nwhere there's no good access to the stdout of the process, for example, \nWe investigated similar code and improvements in Chapter 14, \nsetting code to check this, but it has been added as an example and \nquickly run the code without interruptions.\nrunning tests, as we described in Chapter 10, Testing and TDD.\nIn this chapter, we described the general process of detecting and fixing problems.\nimportant to be able to reliably reproduce the problem in order to show all the \nOnce a problem is deemed important, there needs to be an investigation into why \nthis problem is happening.\nThis can be on the running code, and use the available \ntools in production to see if it can be understood why the problem occurs.\nobjective of this investigation is to be able to replicate the problem locally.\nAs the main tool to understand the behavior of the code in production is logs, \nwe talked about creating a request ID that can help us to trace the different calls \nand relate logs from different systems.\nenvironment may have the key to why the problem is occurring there.\nnecessary, the number of logs may need to be increased to extract information from \nWe then moved on to how to debug locally, after replicating the problem, ideally, \ndebugging when required in difficult cases.\nWe introduced some of the tools that help with debugging in Python, which make \nuse of the possibilities that Python presents for introspection.\nlanguage, there are a lot of possibilities, as it's able to execute any code, including all \nWe then talked about how to create logs to debug, which is an improved version \nbetter logs in the long run.\nIn the next chapter, we will talk about the challenges of working in the architecture \nThere are always changes, adjustments, and tweaks that need \nto be performed in order to improve the system: adding new features; improving \nperformance; fixing security problems.\nmore about making changes and improvements.\nsome of the techniques and ideas around making changes in a real working system, \nsystem can be changed continuously while at the same time maintaining service \nLoad testing\nVersioning\nTeamwork aspects of changes\nLet's start by taking a look at why to make changes in the architecture of a system.\nexample, adding an event-driven system to run asynchronous tasks, allowing \nBig API changes, like introducing a new version of an API either internally or \nFor example, adding a new endpoint that works better for other \ninternal systems to perform some action, where the calling services should be \nChanges in the storage system, including all the different ideas that we \nsecurity problem.\nthat is capable of using new security processes because the old one is not \nto use their favorite language to create a service.\ncause problems by complicating maintenance as expertise in this language \nOther kinds of technical debt – for example, refactors that can clean the code \nand make it more readable, or to allow for changing names of components to \nThe challenge is not only to design these changes to achieve the expected results, but \ninterrupted, setting a high bar for any change.\nTo achieve this, changes need to be taken in small steps, taking extra care to ensure \nchanges made, sometimes it's simply not possible to perform big changes without \nThat changed \nIn other cases, like a small new service with very little traffic, customers will either be \nservice.\nIf possible, is a good practice to define maintenance windows to properly set clear \nexpectations about times when the service will or might have a high risk of some sort \nit will happen, not every maintenance window needs to involve downtime – there is \nIt's important to communicate maintenance windows in advance, for example \nService \nrequires the service to be \ntime, as it's better to set expectations with a large maintenance window that can \nWhile scheduled downtime and maintenance windows will help frame the times \nwhere the service is active and what times are riskier for the user, it's still possible \nproduce an error so important that it needs to be taken care of immediately.\nAn incident is defined as a problem that disrupts the service so much that it requires \nDuring incidents, using all monitoring tools available is critical to find the problem \nHow good the monitoring tools are at detecting and understanding problems\nHow fast a change can be introduced in the system, related to how quick it is \nto change a parameter or to deploy new code\n(though it may be necessary to make changes to get a better understanding of the \nproblem, as we saw in Chapter 14, Profiling).\nThis is why these two elements, the observability and the time required to make \na change, are so important.\nto make a change is normally just a minor annoyance, but in a critical situation, it \nservice, or even a problem in one internal service that reduces the \n12, Logging.\nIntroducing changes to the system is tightly related to the \ndifference in how long it takes new code to be ready to deploy.\nproblem has impacted the service.\nand take corrective measures to ensure that the problem doesn't happen again, or at \nto correct the problem?\nExample: The service went down between 08:30 and 9:45 UTC on the 5th of \nDescribe the impact of the problem.\nWhat was the external problem?\nExample: All user requests were returning 500 errors.\nExample: The monitoring system alerted about the problem at 8:35 UTC, after \nActions taken to correct the problem.\nExample: John cleaned the disk space in the database server and restarted the \n8:30 Start of the problem.\ninto the problem.\n9:30 The logs in the database server had filled up the server disk space, \n9:40 Old logs are removed from the server, freeing disk space.\nfixed, will completely remove this problem.\nof a certain tool or metric that was useful when analyzing the problem.\nThe amount of disk space that logs use should be limited in all cases.\nThe most important part of the process.\nproblem.\nAction: Tweak the error alert to change it to alert when there's only one \nIn recent years, an equivalent process to try to foresee problems has been put in \nsimilar that is expected to significantly change the conditions of the system.\nthe problem.\nThe objective of the process is to detect weak spots \nand to try to make sure that problems are not repeated.\nAll that can lead to planning for the different scenarios and running tests to ensure \nWhen doing any premortem analysis, be sure to have enough time to perform the \nnecessary actions and tests to prepare the system.\ncan be an endless task, and as time will be limited, it needs to be focused on the \nBe sure to use as many data-driven \nactions as possible and focus the analysis on real data and not hunches.\nLoad testing\nA key element of preparation in these cases is load testing.\nLoad testing is creating a simulated load that goes to an increased level of traffic.\nLoad testing is typically done not in production environments, but in staging ones, \ncreate a final load test verifying that the configuration in the production environment \nThe basic element of a load test is to simulate a typical user performing actions on \nFor example, a typical user can log in, check a few pages, add some \ninformation, and then log out.\nAn interesting part of load testing analysis in cloud environments \ntimes to simulate the effect of N users, producing enough load to test our system.\nIf necessary, or to perform tweaks, logs can be analyzed to generate an adequate \nLoad tests, though, are sometimes needed when there is no solid \ndata, as they are done typically when new features are introduced, so estimations \nprocess, including readjusted system tests using any existing software.\nThis will help detect possible problems.\nLoad tests \nThe more intensive load tests are, the more problems they'll be able \nLoad tests should also be aimed at creating some headroom in the production cluster \nVersioning\nWhen making changes to any service, a system needs to be in place to track the \ndifferent changes.\nhas changed from last week.\nVersioning means assigning a unique code version to each service or system.\nbeen changed from one version to another.\nIt is also possible to use specific tools aimed at common use cases \nlike HTTP interfaces, for example, Locust (https://locust.\nThis tool allows us to create a web session, simulating a user \nsession explicitly for the load test and is only capable of working \ndeployment, as new code can create new problems.\nthat an incident is produced due to the release of a new version.\nVersion numbers are normally assigned in the source control \nsystem at specific points to precisely track the code at that \nThe point of having a defined version is to have a \nprecise definition of the code under that unique version number.\nversion number that is applicable to multiple iterations of the code \nVersion numbers are about communicating the differences in code when talking \nTraditionally, versions were highly related to packaged software and different \nversions of the software that were sold in boxes, making them marketing versions.\nWhen the internal version was required, a build number was used, which was a \nVersions can not only be applied to whole software, but also to elements of it, as \nAPI version, library versions, etc.\nIn the same way, different versions can be used \neffectively for the same software, such as for creating an internal version for the \ntechnical team but an external version for marketing purposes.\nIn modern software, where the releases are frequent and the version needs to change \noften, this simple method is not adequate, and instead different version schemas are \nSemantic versioning uses two or three numbers, separated by dots.\nThe first number (X) is called the major version.\nAn increase in the major version indicates that the software is not compatible \nAn increase in the minor version means that this version contains new \nfeatures, but they don't break compatibility with older versions.\nWe talked about semantic versioning in Chapter 2, API Design, but \nconcept can be used both for APIs and code releases.\nIt fixes problems, but doesn't change the \nA good example of this kind of versioning is the Python interpreter itself:\nPython 3 was an increase in the major version, and as such, code from \nPython 2 required changes to be run under Python 3\nPython 3.9 introduced new features compared with Python 3.8, for example, \nversion\nSemantic versioning is very popular and it's particularly useful when dealing with \nexpectation, from just the version number, on what to expect from a new change, and \nallows clarity at the time of adding new features.\ncompatibility along the way, only deprecating features after they are old, it works \nthat change very often and are consumed by other parts of the organization, it is \njust using it as a general tool to increase version numbers in a consistent manner to \nprovide an understanding of how the code changes, but without necessarily having \nto force changes in major or minor versions.\nKeep in mind that increasing a major version number can also \nmark changes that would ordinarily appear in minor version \nA change in the major version number will likely \nsemantic versioning for this reason, deciding that instead new \nmajor versions will be small and not change things, and won't \nWhen communicating through external APIs, though, version numbers do not only \nKeep in mind that it can be possible to create a general version of a whole system, \neven if internally its different components have their own independent versions.\ncases like online services, though, that can be tricky or pointless.\nThe key aspect of changing architecture in a running system is the necessity of \nalways keeping backward compatibility in its interfaces and APIs. Backward compatibility means that systems keep their old interfaces working as \nexpected, so any calling system won't be affected by the change.\nAs versioning is so important, a good idea is to allow services to \nself-report their version number via a specific endpoint like /api/\nversion or another easily accessed way to be sure that it's clear \nand can be checked by other dependant services.\ndatabases changes in Chapter 3, Data Modeling.\nimportant in microservices architectures to allow the independent \nThis concept is quite simple, but it has implications on how changes need to be \nChanges should always be additive.\nEven additive changes in externally accessible APIs are difficult.\ncustomers tend to remember the API as it is, so it can be difficult to change \nAdding a new field in a JSON object \nis safer than changing a SOAP definition, which needs to be defined \nNonetheless, for external APIs it could be safer to add new endpoints if \nAPI changes are normally done in stages, creating a new version \nof the API and trying to encourage customers to change to the new and better \nCustomers don't want to change their existing systems to \nadjust to any changes unless there's a good reason, and \nWeb interfaces allow greater flexibility for changes as they \nthe tests without a problem, as the old behavior won't change.\ncoverage of the API functionality is the best way to maintain compatibility.\nIntroducing changes in external interfaces is more complicated and normally \nrequires the definition of stricter APIs and a slower pace of change.\ninterfaces allow greater flexibility, as their changes can be communicated across the \nthe service at any point.\nIncremental changes\nIncremental changes to the system, slowing mutating and adjusting the APIs, can \nBut the changes need to be \nFor example, let's say that we have two services: service A generates an interface \nA good example of how painful a change in APIs can \n2 needed to be changed.\nthere's still code in legacy systems working with Python \nThere's a new feature that needs to be introduced in service A that requires extra \ninformation from the examinees, and requires us to know the number of times that \nWith the current information, that's impossible, but service B can be \nTo do so, the API needs to be extended, so it returns that information:\nOnly after this change is properly done and deployed can service A use it.\n2.\t Deployment of service B with /examinees (v2).\n3.\t Deployment of service A reading and using the new parameter exam_tries.\nThe service works without a problem throughout each \nThis detachment is important because if there's a problem with a \ndeployment, it can be reversed and only affects a single service, \nThe worst situation is to have two changes in services \nthat need to happen at the same time, as a failure in one will affect \nthe problem could be in the interaction between them, and in that \nThis way of operating allows us to implement greater changes, for example, \nchange it for a more appropriate student_id.\nThe process will go like this:\n1.\t Update the returned object to include a new field called student_id, \nreplicating the previous value in service B:\n2.\t Update and deploy service A to use student_id instead of examinee_id.\n4.\t Remove the old field from service B and deploy the service:\n5.\t Remove the old field from service B and deploy the service.\nUse monitoring tools and logs to verify this!\nThis illustrates how we can deploy changes without interrupting the service in \navailable while deploying a new version?\nTo allow continuous releases without service interruption, we need to take the \nbackward-compatible changes and deploy them while the service is still responding.\n1.\t This is the initial stage, where all the instances have version 1 of the service to \n2.\t A new instance with service 2 is created.\nFigure 16.2: New server created\n3.\t The new version is added to the load balancer.\nbackward compatibility, though, this should not cause any problems.\nFigure 16.3: New server included in the load balancer\ninstance in the load balancer, so no new requests will be addressed.\nservice finishes all the already-ongoing requests (remember, no new requests \n6.\t The process can be repeated until all instances are at version 2.\nKubernetes will perform this automatically when rolling out changes to containers.\nWe also saw that web services like nginx or Apache will do as well.\nunder a configuration change.\nquick iteration makes it impossible to create big changes, like a new user interface.\nTo complicate things further, these big changes will likely happen in parallel with \nuntil the new user interface is working correctly.\nThe feature can then be tested in a particular environment, while all the \nThat means that other changes, like bug fixes or performance improvements, are \nAnd the work done on the big new feature is \nof the big new feature are also being released to the production environment, but \nThe final step is to simply enable it through a configuration change.\nTeamwork aspects of changes\nTests need to ensure that both options – the feature active and \nThe test users could be internal to the organization \nThe process of implementing changes in a system has some human elements \nchanges.\nThe pace and acceptance of technical changes in an organization are \nChanges in \norganizations that can quickly change technologies tend to be faster in \nadjusting to organization-wide changes.\nIn the same way, technology changes require support and training, even if \nchange, be sure to have a point of contact where the team can go to resolve \nA change in one will likely \naffect the other, which means that big enough architectural changes will lead \nAt the same time, changes may have winners and losers in the affected teams.\nThis problem can be particularly poignant in team shuffling when people \nare moving around or when creating new teams.\npace of development is to have an efficient team and making changes to \nsecurity updates, but also tasks like upgrading OS versions, dependencies, \nFor example: the OS version will be upgraded \nwithin three to six months of a new LTS version being released.\npredictability, gives clear objectives to follow, and produces continuous \nIn the same way, automatic tools that detect security vulnerabilities make it \nof it, and is manifested with a progressively slower pace of code changes.\nto work with, making the development process more difficult and risking \nAs a general consideration, just keep in mind that changes in architecture need to be \ncarried out by members of the team, and that information needs to be communicated \nAs with any other task where communication is an important \ncomponent, this presents its own challenges and problems, as communicating with \nAny software architecture designer needs to be aware \nrunning while developing and changing it, including its architecture.\nWe started by describing different ways that architecture can require adjustments \nand changes.\nWe then moved on to talk about how to manage changes, including \nexpectations of stability and change.\nWe next went over the different incidents that can happen when problems arise, \nTo deal with this, we next introduced load testing and how it can be used to verify \na versioning system that clearly communicates what version of the software is \nmix this process of releasing bigger features that need to be activated as a whole.\nFinally, we described different aspects of how changes in a system and architecture \ninto account while performing changes to the system, in particular changes that \nSpend less time learning and more time coding with practical eBooks and \nUnderstand when to use the functional or object-oriented approach to \nFetch, clean, and manipulate data, making efficient use of Python's built-in \ntesting\nUse tools and processes to analyze and respond to user requests\nPython Object-Oriented Programming, Fourth Edition\nImplement objects in Python by creating classes and defining methods\nUnderstand when to use object-oriented features, and more importantly, \nLearn to statically type check your dynamic code\nYour review is important to us and the tech community and will help us make sure \nversioning  51\nincremental changes  548-550\ncatastrophic problems  502\ncode profiling, for time  468- 470\ncritical problems  502\nchanging, without interruption  132-135\nwith logs  522, 523\nerror log  185\ntesting  278-280\ntesting  358-361\nexternal versioning\nversus internal versioning  51, 52\ninternal versioning\nversus external versioning  51, 52\nload testing  541, 542\nlogs  185, 421-426\nerror log  185\nproblems, detecting through  430\nmild problems  502\nminor problems  502\nchanges, without enforcing schema  137, 138\ndatabase, detaching from code  116\ntest, designing  336-339\ntesting  334, 336\nrequest ID, logging  507-512\nlogs, producing  426-429\nPython status codes\nsingle code  232\nRelational schema changes  131\nlogging  507-512\nRESTful API process, designing  37-40\nserious problems  502\nservice  322\nservices\nsingle code  232",
      "keywords": [
        "system",
        "code",
        "problem",
        "changes",
        "printed",
        "8:47",
        "subject",
        "Python",
        "Ongoing Architecture",
        "reference link",
        "service",
        "architecture",
        "logs",
        "version",
        "url"
      ],
      "concepts": [
        "version",
        "versions",
        "url",
        "changing",
        "change",
        "logs",
        "logging",
        "log",
        "likely",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.418,
          "base_score": 0.268,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.374,
          "base_score": 0.374,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.355,
          "base_score": 0.355,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "changes",
          "problem",
          "version",
          "service",
          "logs"
        ],
        "semantic": [],
        "merged": [
          "changes",
          "problem",
          "version",
          "service",
          "logs"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2870291722621693,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:03.530339+00:00"
      }
    }
  ],
  "total_chapters": 16,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Python Architecture Patterns_metadata.json",
    "enrichment_date": "2025-12-17T23:08:03.536269+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 3995.398668999769,
    "total_similar_chapters": 71
  }
}