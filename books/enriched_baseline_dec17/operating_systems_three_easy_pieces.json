{
  "metadata": {
    "title": "operating_systems_three_easy_pieces",
    "source_file": "operating_systems_three_easy_pieces_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "OPERATING SYSTEMS\nc⃝2014 by Arpaci-Dusseau Books, Inc. All rights reserved\nThe book is called Operating Systems: Three Easy Pieces, and the title\ngood enough for you in your quest to understand what operating systems (and\nmore generally, systems) are all about.\nThe three easy pieces refer to the three major thematic elements the book is\nthese concepts, we’ll end up discussing most of the important things an operating\nEach major concept is divided into a set of chapters, most of which present a\nOne of our goals in writing this book is to make the paths of history as clear\nsuch a crux of the problem is explicitly called out in the text, and hopefully solved\nthat they are also essential in operating systems.\nRunning real code on real systems is the best way to learn about\noperating systems, so we encourage you to do so when you can.\nsimulations of pieces of the operating system; you should download the home-\nThe most important addendum to this book is a set of projects in which you\nlearn about how real systems work by designing, implementing, and testing your\nunderlies most operating systems, and thus worth adding to your tool-chest of\nThe ﬁrst are systems programming projects; these projects are great for those who\nin three different ways: either all systems programming, all xv6 programming, or\nOPERATING\nSYSTEMS",
      "keywords": [
        "OPERATING SYSTEMS",
        "SYSTEMS",
        "OPERATING",
        "book",
        "EASY PIECES",
        "real operating system",
        "PIECES",
        "projects",
        "called Operating Systems",
        "EASY",
        "real systems",
        "systems programming",
        "text",
        "real",
        "programming"
      ],
      "concepts": [
        "systems",
        "real",
        "pieces",
        "way",
        "ways",
        "present",
        "running",
        "run",
        "called",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 7,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 63,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 2,
          "title": "",
          "score": 0.621,
          "base_score": 0.471,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "operating",
          "systems",
          "operating systems",
          "projects",
          "book"
        ],
        "semantic": [],
        "merged": [
          "operating",
          "systems",
          "operating systems",
          "projects",
          "book"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33176225680182436,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116065+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "summary": "If you are an instructor or professor who wishes to use this book, please feel\nThe (current) proper citation for the book is as follows:\nOperating Systems: Three Easy Pieces\nArpaci-Dusseau and Andrea C.\nArpaci-Dusseau Books, Inc. May, 2014 (Version 0.8)\nOne slightly unusual aspect of the book is that concurrency, a topic at the front\nof many OS books, is pushed off herein until the student has built an understand-\nthis course for nearly 15 years, students have a hard time understanding how the\nYou may have noticed there are no slides that go hand-in-hand with the book.\nThus, when we teach the course,\nwe come to class with a few major ideas and examples in mind and use the board\nexperience, using too many slides encourages students to “check out” of lecture\n(hopefully) more interactive, dynamic, and enjoyable for the students in your class.\nIf you’d like a copy of the notes we use in preparation for class, please drop us\nTo Students\nIf you are a student reading this book, thank you!\nprovide some material to help you in your pursuit of knowledge about operating\ndays (e.g., Hennessy and Patterson [HP90], the classic book on computer architec-\nture) and hope this book will become one of those positive memories for you.\nYou may have noticed this book is free and available online.\nThis book, we hope, is\nthe ﬁrst of a new wave of free materials to help those in pursuit of their education,\nto spend for a book.\nFailing that, it is one free book, which is better than none.\nWe also hope, where possible, to point you to the original sources of much\nof the material in the book: the great papers and persons who have shaped the\nﬁeld of operating systems over the years.\ncome from smart and hard-working people (including numerous Turing-award\nIn doing so, we hopefully can better understand the revolutions that\nOPERATING\nThis section will contain thanks to those who helped us put the book together.\nsend us some feedback and help debug this book.\nat least, have your name in some book.\nThe people who have helped so far include: Abhirami Senthilkumaran*, Adam\nAmur, Huanchen Zhang*, Jake Gillberg, James Perry (U.\neach chapter, to Professor Jerod Weinman (Grinnell) and his entire class for their\nable and detailed reading and comments about the book.\nAlso, many thanks to the hundreds of students who have taken 537 over the\nIn particular, the Fall ’08 class who encouraged the ﬁrst written form of\nYou should totally write a textbook!” comment in our course\nA great debt of thanks is also owed to the brave few who took the xv6 project\nSpring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Tony Gregerson, Michael\nJohnson, John Kjell, Boyan Li, James Loethen, Will McCardell, Ryan Szaroletta, Si-\nAlthough they do not directly help with the book, our graduate students have\nA ﬁnal debt of gratitude is also owed to Aaron Brown, who ﬁrst took this course\nmany years ago (Spring ’09), then took the xv6 lab course (Fall ’09), and ﬁnally was\na graduate teaching assistant for the course for two years or so (Fall ’10 through\nticularly those in xv6 land) and thus has helped better the learning experience for\nOPERATING\nand these notes are certainly here to help with that part of your education; after all,\nWe created these notes to spark your interest in operating systems, to read more\nthus the real point of the educational process: to go forth, to study many new and\n4If this sounds like we are admitting some past history as arsonists, you are probably\nway to understand a modern operating system.\nBasic Books, 1996\nThis book reprints the six easiest chapters of Feynman’s Lectures on Physics, from 1963.\nA book that encouraged each of us at our undergraduate institutions to pursue graduate studies; we later\nboth had the pleasure of working with Patterson, who greatly shaped the foundations of our research\nA great and famous read about the fundamentals of the scientiﬁc process.\nOPERATING\nTo Students .\nA Dialogue on the Book\nIntroduction to Operating Systems\nVirtualization\nProcess API\nProcess States\nInterlude: Process API\nProblem #2: Switching Between Processes .\nOPERATING",
      "keywords": [
        "book",
        "Operating Systems",
        "Systems",
        "Operating",
        "students",
        "System Call",
        "process",
        "Version",
        "references",
        "Easy",
        "Pieces",
        "Easy Pieces",
        "Arpaci-Dusseau",
        "free",
        "years"
      ],
      "concepts": [
        "book",
        "operating",
        "operations",
        "process",
        "processes",
        "student",
        "basic",
        "including",
        "include",
        "references"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "",
          "score": 0.77,
          "base_score": 0.62,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 1,
          "title": "",
          "score": 0.621,
          "base_score": 0.471,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 63,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 7,
          "title": "",
          "score": 0.463,
          "base_score": 0.313,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "students",
          "operating",
          "years",
          "class"
        ],
        "semantic": [],
        "merged": [
          "book",
          "students",
          "operating",
          "years",
          "class"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29701890510784135,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116141+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "summary": "11 Summary Dialogue on CPU Virtualization\n12 A Dialogue on Memory Virtualization\n14 Interlude: Memory API\n14.1 Types of Memory .\n15.2 An Example .\n16.2 Which Segment Are We Referring To?\n18 Paging: Introduction\n18.1 Where Are Page Tables Stored?\n18.2 What’s Actually In The Page Table?\n18.3 Paging: Also Too Slow\n19.1 TLB Basic Algorithm\n19.6 Issue: Replacement Policy\n20 Paging: Smaller Tables\n20.1 Simple Solution: Bigger Pages .\n20.2 Hybrid Approach: Paging and Segments .\n20.3 Multi-level Page Tables .\n20.4 Inverted Page Tables\n20.5 Swapping the Page Tables to Disk .\n21.3 The Page Fault .\n21.4 What If Memory Is Full?\n21.5 Page Fault Control Flow\n22 Beyond Physical Memory: Policies\n23.4 Page Replacement .\n24 Summary Dialogue on Memory Virtualization\n25 A Dialogue on Concurrency\n26.1 An Example: Thread Creation .\n26.3 The Heart of the Problem: Uncontrolled Scheduling\n27.3 Locks .\n28 Locks\n28.1 Locks: The Basic Idea .\n28.4 Evaluating Locks\n28.8 Evaluating Spin Locks\n29 Lock-based Concurrent Data Structures\n29.2 Concurrent Linked Lists\n29.3 Concurrent Queues .\n29.4 Concurrent Hash Table .\n31.2 Binary Semaphores (Locks) .\n32 Common Concurrency Problems\n33 Event-based Concurrency (Advanced)\n33.5 A Problem: Blocking System Calls .\n33.7 Another Problem: State Management .\n34 Summary Dialogue on Concurrency\n37.5 Disk Scheduling .\n38.6 RAID Level 4: Saving Space With Parity\n38.8 RAID Comparison: A Summary .\n39 Interlude: File and Directories\n39.1 Files and Directories .\n39.2 The File System Interface .\n39.3 Creating Files\n39.4 Reading and Writing Files\n39.7 Renaming Files\n39.8 Getting Information About Files .\n39.9 Removing Files\n39.15Making and Mounting a File System\n40 File System Implementation\n40.3 File Organization: The Inode .\n41 Locality and The Fast File System\n41.2 FFS: Disk Awareness Is The Solution\n41.4 Policies: How To Allocate Files and Directories\n41.5 Measuring File Locality .\n41.6 The Large-File Exception .\n42.2 Solution #1: The File System Checker .\n43 Log-structured File Systems\n43.1 Writing To Disk Sequentially .\n43.7 Reading A File From Disk: A Recap .\n44.5 A New Problem: Misdirected Writes\n44.6 One Last Problem: Lost Writes .\n45 Summary Dialogue on Persistence\n47.1 Communication Basics .\n48 Sun’s Network File System (NFS)\n48.1 A Basic Distributed File System .\n48.6 From Protocol to Distributed File System .\n48.9 The Cache Consistency Problem .\n49 The Andrew File System (AFS)\n49.2 Problems with Version 1",
      "keywords": [
        "References",
        "Summary",
        "File System",
        "File",
        "Summary Dialogue",
        "System",
        "Problem",
        "Homework",
        "Distributed File System",
        "Memory",
        "Locks",
        "Page Tables",
        "Dialogue",
        "Disk",
        "CONTENTS"
      ],
      "concepts": [
        "summary",
        "file",
        "problem",
        "disk",
        "locks",
        "references",
        "referring",
        "paging",
        "pages",
        "contents"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 28,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "file",
          "39",
          "files",
          "dialogue",
          "summary dialogue"
        ],
        "semantic": [],
        "merged": [
          "file",
          "39",
          "files",
          "dialogue",
          "summary dialogue"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41123852455077425,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116209+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 25-32)",
      "start_page": 25,
      "end_page": 32,
      "summary": "Simple Example: Code That Loops and Prints\nA Program that Accesses Memory\nRunning The Memory Program Multiple Times .\np2.c: Calling fork() And wait() .\nFIFO Simple Example\nSJF Simple Example\nSTCF Simple Example .\n10.3 Simple List Delete Code .\n13.1 Operating Systems: The Early Days .\n13.2 Three Processes: Sharing Memory .\n13.3 An Example Address Space .\n15.1 A Process And Its Address Space .\n15.2 Physical Memory with a Single Relocated Process .\n16.1 An Address Space (Again) .\n17.5 Free Space With Three Chunks Allocated .\n17.6 Free Space With Two Chunks Allocated .\n18.1 A Simple 64-byte Address Space .\n18.2 64-Byte Address Space Placed In Physical Memory\n18.4 Example: Page Table in Kernel Physical Memory\n18.6 Accessing Memory With Paging\n19.2 Example: An Array In A Tiny Address Space .\n20.1 A 16-KB Address Space With 1-KB Pages .\n20.2 Linear (Left) And Multi-Level (Right) Page Tables .\n20.3 A 16-KB Address Space With 64-byte Pages .\n20.4 Multi-level Page Table Control Flow .\n21.1 Physical Memory and Swap Space .\n22.3 The 80-20 Workload .\n26.1 A Single-Threaded Address Space .\n26.2 Simple Thread Creation Code (t0.c)\n28.2 A Simple Spin Lock Using Test-and-set .\n29.1 A Counter Without Locks .\n29.2 A Counter With Locks .\n30.3 Parent Waiting For Child: Use A Condition Variable\n30.4 The Put and Get Routines (Version 1) .\n30.5 Producer/Consumer Threads (Version 1)\n31.9 A Simple Reader-Writer Lock .\n33.1 Simple Code using select()\n37.2 A Single Track Plus A Head .\n47.1 Example UDP/IP Client/Server Code .",
      "keywords": [
        "Address Space",
        "Space",
        "Control Flow Algorithm",
        "Physical Memory",
        "Memory",
        "Simple",
        "Address",
        "Control Flow",
        "List",
        "Flow Algorithm",
        "Code",
        "Parent Waiting",
        "TLB Control Flow",
        "Lock",
        "Page Table"
      ],
      "concepts": [
        "simple",
        "examples",
        "code",
        "paging",
        "pages",
        "list",
        "thread",
        "memory",
        "space",
        "version"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.901,
          "base_score": 0.751,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 62,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.835,
          "base_score": 0.685,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.833,
          "base_score": 0.683,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "space",
          "address space",
          "address",
          "flow",
          "simple"
        ],
        "semantic": [],
        "merged": [
          "space",
          "address space",
          "address",
          "flow",
          "simple"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4643953356267512,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116275+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "summary": "30.1 Thread Trace: Broken Solution (Version 1)\n30.2 Thread Trace: Broken Solution (Version 2)\n40.3 File Read Timeline (Time Increasing Downward) .\nA Dialogue on the Book\nProfessor: Welcome to this book!\nIt’s called Operating Systems in Three Easy\nPieces, and I am here to teach you the things you need to know about operating\nStudent: Hi Professor!\nProfessor: That’s an easy one.\nIs this going to be hilarious like that book was?\nThat book was great, and I’m glad you’ve read it.\nHopefully this book is more like his notes on Physics.\nsummed up in a book called “Six Easy Pieces”.\nwe’re going to do Three Easy Pieces on the ﬁne topic of Operating Systems.\nStudent: Well, I liked physics, so that is probably good.\nProfessor: They are the three key ideas we’re going to learn about: virtualiza-\nall about how an operating system works, including how it decides what program\nProfessor: Good!\nStudent: I have another question: what’s the best way to learn this stuff?\nA DIALOGUE ON THE BOOK\nown, of course, but here is what I would do: go to class, to hear the professor\nreal problems is the best way to put the ideas within these notes into action.\nProfessor: (surprised) How did you know what I was going to say?!\nProfessor: Well, I think we are going to get along just ﬁne!\nStudent: Professor – just one more question, if I may.\nIf you are taking an undergraduate operating systems course, you should\nalready have some idea of what a computer program does when it runs.\nIf not, this book (and the corresponding course) is going to be difﬁcult\n– so you should probably stop reading this book, or run to the nearest\nWell, a running program does one very simple thing: it executes in-\nthat while a program runs, a lot of other wild things are going on with\neasy to run programs (even allowing you to seemingly run many at the\nsame time), allowing programs to share memory, enabling programs to\nhood to make programs run faster, e.g., executing multiple instructions at once, and even issu-\nwith the simple model most programs assume: that instructions seemingly execute one at a\nOne central question we will answer in this book is quite simple: how\ndoes the operating system virtualize resources?\nis called the operating system (OS)3, as it is in charge of making sure the\nsystem operates correctly and efﬁciently in an easy-to-use manner.\nThat is, the OS takes a physical resource (such as\nrefer to the operating system as a virtual machine.\nmake use of the features of the virtual machine (such as running a pro-\nthe OS provides these calls to run programs, access memory and devices,\nFinally, because virtualization allows many programs to run (thus shar-\nstructions and data (thus sharing memory), and many programs to access\nof the system; it is thus the operating system’s role to manage those re-\n3Another early name for the OS was the supervisor or even the master control program.",
      "keywords": [
        "Thread Trace",
        "Operating Systems",
        "Direction Execution Protocol",
        "OPERATING",
        "Trace",
        "System",
        "Professor",
        "Thread",
        "Book",
        "Execution Protocol",
        "Student",
        "Direction Execution",
        "programs",
        "File",
        "memory"
      ],
      "concepts": [
        "professor",
        "memory",
        "student",
        "program",
        "trace",
        "tracing",
        "operating",
        "operates",
        "read",
        "timeline"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 2,
          "title": "",
          "score": 0.77,
          "base_score": 0.62,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 1,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 7,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.526,
          "base_score": 0.526,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "professor",
          "going",
          "operating",
          "programs"
        ],
        "semantic": [],
        "merged": [
          "book",
          "professor",
          "going",
          "operating",
          "programs"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28131958330206647,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116356+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 41-48)",
      "start_page": 41,
      "end_page": 48,
      "summary": "#include <sys/time.h>\nFigure 2.1: Simple Example: Code That Loops and Prints\nFigure 2.1 depicts our ﬁrst program.\nreturns once it has run for a second.\nLet’s say we save this ﬁle as cpu.c and decide to compile and run it\nNot too interesting of a run – the system begins running the program,\nNote the program will\nwill terminate the program running in the foreground) can we halt the\nprogram.\nNow, let’s do the same thing, but this time, let’s run many different in-\nstances of this same program.\nFigure 2.2: Running Many Programs At Once\nhave only one processor, somehow all four of these programs seem to be\nrunning at the same time!\nprograms to seemingly run at once is what we call virtualizing the CPU,\nOf course, to run programs, and stop them, and otherwise tell the OS\nwhich programs to run, there need to be some interfaces (APIs) that you\nYou might also notice that the ability to run multiple programs at once\nFor example, if two programs want to\n(such as the ability to run multiple programs at once).\nDoing so runs a\ntheir next command, which in this case is another program to run.\ncommands allows us to run multiple programs at the same time in tcsh.\nFigure 2.3: A Program that Accesses Memory\nVirtualizing Memory\nbytes; to read memory, one must specify an address to be able to access\nthe data stored there; to write (or update) memory, one must also specify\nMemory is accessed all the time when a program is running.\ntion of the program is in memory too; thus memory is accessed on each\nLet’s take a look at a program (in Figure 2.3) that allocates some mem-\nThe output of this program can be found here:\n(2134) memory address of p: 00200000\nThe program does a couple of things.\nThen, it prints out the address of the memory (a2), and then\nout what is called the process identiﬁer (the PID) of the running program.\n(24113) memory address of p: 00200000\n(24114) memory address of p: 00200000\nFigure 2.4: Running The Memory Program Multiple Times\nAs the program runs, it slowly updates the\nNow, we again run multiple instances of this same program to see\nWe see from the example that each running\nprogram has allocated memory at the same address (00200000), and yet\nif each running program has its own private memory, instead of sharing\nthe same physical memory with other running programs5.\none running program does not affect the address space of other processes\n(or the OS itself); as far as the running program is concerned, it has phys-\nprogram.\nThe problems of concurrency arose ﬁrst within the operating\nOS is juggling many things at once, ﬁrst running one process, then an-\nFigure 2.5: A Multi-threaded Program\nIndeed, modern multi-threaded programs exhibit the\nprogram (Figure 2.5).\nThe main program creates\nfunction running within the same memory space as other functions, with\nBelow is a transcript of what happens when we run this program with\nThe value of loops\nour own wrapper that calls pthread create() and makes sure that the return code indicates\nHOW TO BUILD CORRECT CONCURRENT PROGRAMS\nmemory space, how can we build a correctly working program?\nWhen the program is run with the value of\nloops set to 1000, what do you expect the ﬁnal value of counter to be?\nvalue of the counter is 2000, as each thread incremented the counter 1000\nIndeed, when the input value of loops is set to N, we would\nexpect the ﬁnal output of the program to be 2N.\nLet’s run the same program, but with higher values for\nIn this run, when we gave an input value of 100,000, instead of getting\nThen, when we run\nthe program a second time, we not only again get the wrong value, but\nalso a different value than the last time.\nIn fact, if you run the program\nkey part of the program above, where the shared counter is incremented,\nFigure 2.6: A Program That Does I/O\nIn system memory,\ndata can be easily lost, as devices such as DRAM store values in a volatile\nUnlike the abstractions provided by the OS for the CPU and memory,\nFor example, when writing a C program, you might\nﬁrst use an editor (e.g., Emacs7) to create and edit the C ﬁle (emacs -nw\nyou might run the new executable (e.g., ./main).\nAnd thus a new program is born!\nThe ﬁle system is the part of the OS in charge of managing persistent data.\nTo accomplish this task, the program makes three calls into the oper-\nsecond, write(), writes some data to the ﬁle; the third, close(), sim-\nply closes the ﬁle thus indicating the program won’t be writing any more\nOS provides a standard and simple way to access devices through its sys-\nand how ﬁle systems manage data persistently atop said devices.\nperformance reasons, most ﬁle systems ﬁrst delay such writes for a while,\ndifferent common operations efﬁcient, ﬁle systems employ many differ-",
      "keywords": [
        "OPERATING SYSTEMS",
        "program",
        "Memory",
        "SYSTEMS",
        "OPERATING",
        "run",
        "OPERATING SYSTEMS prompt",
        "ﬁle system",
        "running program",
        "run multiple programs",
        "INTRODUCTION TO OPERATING",
        "ﬁle",
        "include",
        "cpu",
        "running"
      ],
      "concepts": [
        "memory",
        "program",
        "systems",
        "value",
        "devices",
        "code",
        "run",
        "running",
        "runs",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 9,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "program",
          "run",
          "memory",
          "programs",
          "running"
        ],
        "semantic": [],
        "merged": [
          "program",
          "run",
          "memory",
          "programs",
          "running"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3805319472013705,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116420+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 49-56)",
      "start_page": 49,
      "end_page": 56,
      "summary": "INTRODUCTION TO OPERATING SYSTEMS\nﬁnding the right set of trade-offs is a key to building systems.\nstandable pieces, to write such a program in a high-level language like\nover time, giving you a way to think about pieces of the OS.\nOne goal in designing and implementing an operating system is to\nAnother goal will be to provide protection between applications, as\nwell as between the OS and applications.\nmany programs to run at the same time, we want to make sure that the\ncertainly don’t want an application to be able to harm the OS itself (as\nto protection and thus underlies much of what an OS must do.\noperating systems often strive to provide a high degree of reliability.\noperating systems grow evermore complex (sometimes containing mil-\nlions of lines of code), building a reliable operating system is quite a chal-\nINTRODUCTION TO OPERATING SYSTEMS\ntimes; mobility is increasingly important as OSes are run on smaller and\nDepending in how the system is used, the OS will have\non how to build operating systems are useful in the range of different de-\noperating systems developed.\nideas accumulated in operating systems over time, as engineers learned\noperating systems [BH00].\nEarly Operating Systems: Just Libraries\nUsually, on these old mainframe systems, one program ran at a time,\nOS would do (e.g., deciding what order to run jobs in) was performed by\nthis operator.\nof jobs were set up and then run in a “batch” by the operator.\nOPERATING\nSYSTEMS\nINTRODUCTION TO OPERATING SYSTEMS\nInstead of providing OS routines as a li-\nmake the transition into the OS a more formal, controlled process.\nUser applications run in what\nplications can do; for example, an application running in user mode can’t\nIn kernel mode, the OS has full access to the hardware of the system and\nthus can do things like initiate an I/O request or make more memory\nWhere operating systems really took off was in the era of computing be-\ngot their hands on computers and thus made computer systems do more\njob at a time, the OS would load a number of jobs into memory and switch\nINTRODUCTION TO OPERATING SYSTEMS\noperating systems along a number of directions.\nprotection became important; we wouldn’t want one program to be able\nof the UNIX operating system, primarily thanks to Ken Thompson (and\ngood ideas from different operating systems (particularly from Multics\n[O72], and some from systems like TENEX [B+72] and the Berkeley Time-\nUnfortunately, for operating systems, the PC at ﬁrst represented a\ngreat leap backwards, as early systems forgot (or never knew of) the\ning systems such as DOS (the Disk Operating System, from Microsoft)\ndidn’t think memory protection was important; thus, a malicious (or per-\npainful list of OS features missing in this generation of systems is long,\nputer operating systems started to ﬁnd their way onto the desktop.\nexample, Mac OS X has UNIX at its core, including all of the features\nEven today’s cell phones run operating systems (such as Linux) that are\nOPERATING\nSYSTEMS\nINTRODUCTION TO OPERATING SYSTEMS\nASIDE: THE IMPORTANCE OF UNIX\nIt is difﬁcult to overstate the importance of UNIX in the history of oper-\nating systems.\nalike, also providing a compiler for the new C programming language.\nMaking it easy for programmers to write their own programs, as well as\nveloped in the heyday of OS development have found their way into the\nviding more features and making modern systems even better for users\nINTRODUCTION TO OPERATING SYSTEMS\nSteve Jobs took his UNIX-based NeXTStep operating environment with\nToday’s operating systems\nmake systems relatively easy to use, and virtually all operating systems\nthe OS we won’t cover in the book.\nworking code in the operating system; we leave it to you to take the net-\nFinally, some operating system books talk a great\ndeal about security; we will do so in the sense that the OS must provide\nprotection between running programs and give users the ability to pro-\npersistence via devices and ﬁle systems.\nyou’ll have a new appreciation for how computer systems really work.\nOPERATING\nSYSTEMS\nINTRODUCTION TO OPERATING SYSTEMS\nA fun paper about using multiple ﬁle systems at once to tolerate a mistake in any one of them.\nIn Classic Operating Systems: From Batch Processing to Distributed Systems\nThis essay provides an intro to a wonderful collection of papers about historically signiﬁcant systems.\nTENEX has much of the machinery found in modern operating systems; read more about it to see how\nAnother great intro to how computer systems work.\nThe Atlas pioneered much of what you see in modern systems.\nA nice piece of history on the early development of computer systems and the pioneering efforts of the\nINTRODUCTION TO OPERATING SYSTEMS\n[PP03] “Introduction to Computing Systems:\nOne of our favorite intro to computing systems books.\n[RT74] “The UNIX Time-Sharing System”\nA great summary of UNIX written as it was taking over the world of computing, by the people who\nOPERATING\nSYSTEMS",
      "keywords": [
        "OPERATING SYSTEMS",
        "SYSTEMS",
        "OPERATING",
        "UNIX operating system",
        "UNIX",
        "computer systems",
        "operating systems make",
        "run operating systems",
        "modern operating systems",
        "build operating systems",
        "Disk Operating System",
        "Early Operating Systems",
        "operating system books",
        "Classic Operating Systems",
        "Operating System Support"
      ],
      "concepts": [
        "systems",
        "operating",
        "operator",
        "unix",
        "making",
        "make",
        "time",
        "great",
        "program",
        "programming"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 1,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 63,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.514,
          "base_score": 0.514,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "operating",
          "operating systems",
          "systems",
          "introduction operating",
          "introduction"
        ],
        "semantic": [],
        "merged": [
          "operating",
          "operating systems",
          "systems",
          "introduction operating",
          "introduction"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3214239162992912,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116500+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 57-67)",
      "start_page": 57,
      "end_page": 67,
      "summary": "The Abstraction: The Process\nOS provides to users: the process.\nof processes at the same time.\nruns programs.\nThe OS creates this illusion by virtualizing the CPU.\nprocess, then stopping it and running another, and so forth, the OS can\nsharing of the CPU, allows users to run as many concurrent processes as\nTHE ABSTRACTION: THE PROCESS\ntext switch, which gives the OS the ability to stop running one program\nand start running another on a given CPU; this time-sharing mechanism\nble programs to run on a CPU, which program should the OS run?\nThe Abstraction: A Process\nThe abstraction provided by the OS of a running program is something\nwe will call a process.\nAs we said above, a process is simply a running\nprogram; at any instant in time, we can summarize a process by taking an\nits machine state: what a program can read or update when it is running.\nAlso part of the process’s machine state are registers; many instructions\nthe execution of the process.\nTHE ABSTRACTION: THE PROCESS\nwhich process should the operating system run right now?\nProcess API\nnew process to run the program you have indicated.\nprocesses will run and just exit by themselves when complete; when\n• Wait: Sometimes it is useful to wait for a process to stop running;\nprocess (stop it from running for a while) and then resume it (con-\nabout a process as well, such as how long it has run for, or what\nTHE ABSTRACTION: THE PROCESS\nProcess\naddress space of process\nFigure 4.1: Loading: From Program To Process\nProcess Creation: A Little More Detail\nformed into processes.\nThe ﬁrst thing that the OS must do to run a program is to load its code\nthus, the process of loading a program and static data into memory re-\nIn early (or simple) operating systems, the loading process is done ea-\nthe process lazily, i.e., by loading pieces of code or data only as they are\nTHE ABSTRACTION: THE PROCESS\nother things the OS needs to do before running the process.\nory must be allocated for the program’s run-time stack (or just stack).\nthis memory and gives it to the process.\nThe OS may also create some initial memory for the program’s heap.\nFor example, in UNIX systems, each process\nnewly-created process, and thus the program begins its execution.\nProcess States\nNow that we have some idea of what a process is (though we will\nabout the different states a process can be in at a given time.\nthat a process can be in one of these states arose in early computer systems\nIn a simpliﬁed view, a process can be in one of three states:\n• Running: In the running state, a process is running on a processor.\n• Ready: In the ready state, a process is ready to run but for some\n• Blocked: In the blocked state, a process has performed some kind\nA common example: when a process initiates an I/O\nrequest to a disk, it becomes blocked and thus some other process\nTHE ABSTRACTION: THE PROCESS\nFigure 4.2: Process: State Transitions\nAs you can see in the diagram, a process can be\nmoved between the ready and running states at the discretion of the OS.\nBeing moved from ready to running means the process has been sched-\nuled; being moved from running to ready means the process has been\nOnce a process has become blocked (e.g., by initiating an\nI/O completion); at that point, the process moves to the ready state again\nThe OS is a program, and like any program, it has some key data struc-\nof each process, for example, the OS likely will keep some kind of process\ntion to track which process is currently running.\nin some way, blocked processes; when an I/O event completes, the OS\nshould make sure to wake the correct process and ready it to run again.\ntion the OS tracks about a process.\na stopped process, the contents of its register state.\nWhen a process is\nregisters), the OS can resume running the process.\nTHE ABSTRACTION: THE PROCESS\n// the different states a process can be in\n// Start of process memory\n// Size of process memory\n// for this process\n// Process state\n// Process ID\n// Parent process\n// Switch here to run process\nwill have an initial state that the process is in when it is being created.\nAlso, a process could be placed in a ﬁnal state where it has exited but\nThis ﬁnal state can be useful as it allows other processes\n(usually the parent that created the process) to examine the return code",
      "keywords": [
        "Process",
        "program",
        "running",
        "CPU",
        "state",
        "systems",
        "operating system",
        "operating",
        "run",
        "memory",
        "peach",
        "data",
        "Professor",
        "ready",
        "Abstraction"
      ],
      "concepts": [
        "process",
        "processes",
        "program",
        "running",
        "run",
        "runs",
        "state",
        "data",
        "systems",
        "initialized"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 9,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "process",
          "abstraction process",
          "abstraction",
          "running",
          "ready"
        ],
        "semantic": [],
        "merged": [
          "process",
          "abstraction process",
          "abstraction",
          "running",
          "ready"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3620492587718128,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116560+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 68-77)",
      "start_page": 68,
      "end_page": 77,
      "summary": "THE ABSTRACTION: THE PROCESS\nture in order to keep track of all the running programs in the system.\ntalking about a C structure that contains information about each process.\nIt is quite simply viewed as a running program.\nTHE ABSTRACTION: THE PROCESS\nThis paper introduces one of the ﬁrst microkernels in operating systems history, called Nucleus.\nInterlude: Process API\nIn this interlude, we discuss process creation in UNIX systems.\npresents one of the most intriguing ways to create a new process with\na pair of system calls: fork() and exec().\ncan be used by a process wishing to wait for a process it has created to\nWhat interfaces should the OS present for process creation and con-\nThe fork() system call is used to create a new process [C63].\nMore speciﬁcally, you have a running program whose code looks\nand run it yourself!\nINTERLUDE: PROCESS API\n} else if (rc == 0) { // child (new process)\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\nFigure 5.1: p1.c: Calling fork()\nWhen you run this program (called p1.c), you’ll see the following:\nﬁrst started running, the process prints out a hello world message; in-\nprocess has a PID of 29146; in UNIX systems, the PID is used to name\nThe process calls the fork() system\ncall, which the OS provides as a way to create a new process.\npart: the process that is created is an (almost) exact copy of the calling pro-\nthe program p1 running, and both are about to return from the fork()\nThe newly-created process (called the child, in contrast to the\ncreating parent) doesn’t start running at main(), like you might expect\nthe PID of the newly-created child, the child is simply returned a 0.\nINTERLUDE: PROCESS API\n} else if (rc == 0) { // child (new process)\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\nFigure 5.2: p2.c: Calling fork() And wait()\nthe child process is created, there are now two active processes in the sys-\nor the parent might run at that point.\nmines which process runs at a given moment in time; because the sched-\nit will choose to do, and hence which process will run ﬁrst.\nSo far, we haven’t done much: just created a child that prints out a\nparent to wait for a child process to ﬁnish what it has been doing.\nINTERLUDE: PROCESS API\nIn this example (p2.c), the parent process calls wait() to delay its\nwait() returns to the parent.\nWith this code, we now know that the child will always print ﬁrst.\nHowever, if the parent does happen to run\nthe child has run and exited2.\nThus, even when the parent runs ﬁrst, it\npolitely waits for the child to ﬁnish running, then wait() returns, and\nA ﬁnal and important piece of the process creation API is the exec()\nThis system call is useful when you want to run a program\nFor example, calling fork()\nin p2.c is only useful if you want to keep running copies of the same\nHowever, often you want to run a different program; exec()\nIn this example, the child process calls execvp() in order to run the\nIn fact, it runs wc on\n2There are a few cases where wait() returns before the child exits; read the man page\nINTERLUDE: PROCESS API\n} else if (rc == 0) { // child (new process)\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\nFigure 5.3: p3.c: Calling fork(), wait(), And exec()\nThen the OS simply runs that program, passing in any argu-\nThus, it does not create a new process;\nrather, it transforms the currently running program (formerly p3) into a\ndifferent running program (wc).\nmost as if p3.c never ran; a successful call to exec() never returns.\nprocess?\nessential in building a UNIX shell, because it lets the shell run code after\nenvironment of the about-to-be-run program, and thus enables a variety\nINTERLUDE: PROCESS API\nof ways to design APIs for process creation; however, the combination\nresides, calls fork() to create a new child process to run the command,\ncalls some variant of exec() to run the command, and then waits for the\ncommand to complete by calling wait().\nshell returns from wait() and prints out a prompt again, ready for your\nprompt> wc p3.c > newfile.txt\nIn the example above, the output of the program wc is redirected into\nple: when the child is created, before calling exec(), the shell closes\nput from the soon-to-be-running program wc are sent to the ﬁle instead\nwrites by the child process to the standard output ﬁle descriptor, for ex-\nHere is the output of running the p4.c program:\nprompt> cat p4.output\nINTERLUDE: PROCESS API\nwhen p4 is run, it looks as if nothing has happened; the shell just prints\nHowever, that is not the case; the program p4 did indeed call fork() to\ncreate a new child, and then run the wc program via a call to execvp().\nrected to the ﬁle p4.output.\noutput ﬁle, all the expected output from running wc is found.\nIn this case, the output of one process is connected to an in-\nto that same pipe; thus, the output of one process seamlessly is used as\nto say that the fork()/exec() combination is a powerful way to create",
      "keywords": [
        "PROCESS",
        "Process API",
        "System Call",
        "child",
        "fork",
        "system",
        "pid",
        "child process",
        "int",
        "program",
        "parent",
        "Call",
        "include",
        "wait",
        "Operating systems"
      ],
      "concepts": [
        "process",
        "processes",
        "called",
        "programs",
        "programming",
        "run",
        "running",
        "runs",
        "prompt",
        "way"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "child",
          "process",
          "fork",
          "exec",
          "process api"
        ],
        "semantic": [],
        "merged": [
          "child",
          "process",
          "fork",
          "exec",
          "process api"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37243930033288675,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116622+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 78-89)",
      "start_page": 78,
      "end_page": 89,
      "summary": "faces for interacting with processes in UNIX systems.\nple, using the ps command allows you to see which processes are run-\nparticularly the chapters on Process Control, Process Relationships, and\nThe basic idea is simple: run one process for a little while, then\ncontrol: how can we run processes efﬁciently while retaining control over\nControl is particularly important to the OS, as it is in charge of\nresources; without control, a process could simply run forever and take\nThe OS must virtualize the CPU in an efﬁcient manner, but while re-\ngram running, it creates a process entry for it in a process list, allocates\nOS\nFree memory of process\nwhen we are running a process, how does the operating system stop it\nfrom running and switch to another process, thus implementing the time\nlimits on running programs, the OS wouldn’t be in control of anything\nruns natively on the hardware CPU and thus executes as quickly as one\nBut running on the CPU introduces a problem: what if\nthe process wishes to perform some kind of restricted operation, such\nA process must be able to perform I/O and some other restricted oper-\nations, but without giving the process complete control over the system.\nHow can the OS and hardware work together to do so?\nThe hardware assists the OS by providing different modes of execution.\nIn kernel mode, the OS has access to the full resources of the machine.\nSpecial instructions to trap into the kernel and return-from-trap back to\nOS to tell the hardware where the trap table resides in memory.\ngranting access to a ﬁle, we can’t simply let any user process issue I/Os\nto the disk; if we did, a process could simply read or write the entire disk\nknown as user mode; code that runs in user mode is restricted in what it\nFor example, when running in user mode, a process can’t issue\nthe OS would then likely kill the process.\n(or kernel) runs in.\nthe required work for the calling process.\nThe hardware needs to be a bit careful when executing a trap, in that\nrectly, as well as execute the hardware-speciﬁc trap instruction.\nto be able to return correctly when the OS issues the return-from-trap\ncounter, ﬂags, and a few other registers onto a per-process kernel stack;\ntrap know which code to run inside the OS?\nClearly, the calling process\nThe kernel does so by setting up a trap table at boot time.\nthe OS thus does is to tell the hardware what code to run when certain\nThe OS informs the hardware of\nOS @ run\ntrap into OS\nWe assume each process has a kernel stack where reg-\nthe kernel initializes the trap table, and the CPU remembers its location\nIn the second (when running a process), the kernel sets up a few things\ning a return-from-trap instruction to start the execution of the process;\nthis switches the CPU to user mode and begins running the process.\nWhen the process wishes to issue a system call, it traps back into the OS,\nto the process.\nThe process then completes its work, and returns from\nthe OS).\nProblem #2: Switching Between Processes\nprocesses.\nSwitching between processes should be simple, right?\nOS should just decide to stop one process and start another.\nrunning on the CPU, this by deﬁnition means the OS is not running.\nthe OS is not running, how can it do anything at all?\nway for the OS to take an action if it is not running on the CPU.\nHow can the operating system regain control of the CPU so that it can\nswitch between processes?\nthe OS trusts the processes of the system to behave reasonably.\nProcesses\nthe OS can decide to run some other task.\nThus, you might ask, how does a friendly process give up the CPU in\nMost processes, as it turns out, transfer control of\nthe CPU to the OS quite frequently by making system calls, for example,\nOperating systems often have to deal with misbehaving processes, those\nIn modern systems, the way the OS\nto the OS so it can run other processes.\nApplications also transfer control to the OS when they do something\nOS.\nThe OS will then have control of the CPU again (and likely terminate\nthe offending process).\nThus, in a cooperative scheduling system, the OS regains control of\nWhat can the OS do then?\nA Non-Cooperative Approach: The OS Takes Control\ndo much at all when a process refuses to make system calls (or mistakes)\nand thus return control to the OS.\nHow can the OS gain control of the CPU even if processes are not being\nWhat can the OS do to ensure a rogue process does not take\nprocess is halted, and a pre-conﬁgured interrupt handler in the OS runs.\nAt this point, the OS has regained control of the CPU, and thus can do\nThe addition of a timer interrupt gives the OS the ability to run again\non a CPU even if processes act in a non-cooperative fashion.\nhardware feature is essential in helping the OS maintain control of the\nAs we discussed before with system calls, the OS must inform the\nhardware of which code to run when the timer interrupt occurs; thus,\nsequence, the OS must start the timer, which is of course a privileged\ncontrol will eventually be returned to it, and thus the OS is free to run\ntrap instruction will be able to resume the running program correctly.\nNow that the OS has regained control, whether cooperatively via a sys-\nmade: whether to continue running the currently-running process, or\nIf the decision is made to switch, the OS then executes a low-level\nfor the currently-executing process (onto its kernel stack, for example)\nand restore a few for the soon-to-be-executing process (from its kernel\nBy doing so, the OS thus ensures that when the return-from-trap\ninstruction is ﬁnally executed, instead of returning to the process that was\nrunning, the system resumes execution of another process.\nTo save the context of the currently-running process, the OS will exe-\nPC, as well as the kernel stack pointer of the currently-running process,\nsoon-to-be-executing process.\nOS @ run\nProcess A\nProcess B\nTable 6.3: Limited Direction Execution Protocol (Timer Interrupt)\ncall to the switch code in the context of one process (the one that was in-\nWhen the OS then ﬁnally executes a return-from-trap instruction,\nthe soon-to-be-executing process becomes the currently-running process.\nple, Process A is running and then is interrupted by the timer interrupt.\nIn the timer interrupt handler, the OS decides\nto switch from running Process A to Process B.\nFinally, the OS returns-\nfrom-trap, which restores B’s register state and starts running it.\ncase, the user register state of the running process is implicitly saved by\nthe hardware, using the kernel stack of that process.\nthe OS decides to switch from A to B; in this case, the kernel register state",
      "keywords": [
        "PROCESS",
        "Limited Direct Execution",
        "system call",
        "system",
        "CPU",
        "Direct Execution",
        "kernel",
        "operating system",
        "Limited Direct",
        "call",
        "Control",
        "kernel stack",
        "kernel mode",
        "trap",
        "program"
      ],
      "concepts": [
        "process",
        "processes",
        "run",
        "running",
        "runs",
        "systems",
        "program",
        "programming",
        "hardware",
        "trap"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 9,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "os",
          "process",
          "trap",
          "control",
          "kernel"
        ],
        "semantic": [],
        "merged": [
          "os",
          "process",
          "trap",
          "control",
          "kernel"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3826616515148754,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116683+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 90-102)",
      "start_page": 90,
      "end_page": 102,
      "summary": "is explicitly saved by the software (i.e., the OS), but this time into memory\nprocessor speed over time [O90].\nsame time, particularly useful on multiprocessors.\nyou want to run on the CPU, but ﬁrst make sure to set up the hardware\nRather, you are using a time-tested approach to improving the behavior\ning boot time) setting up the trap handlers and starting an interrupt timer,\na given time?\nAn early paper about time-sharing that refers to using a timer interrupt; the quote that discusses it:\nrun on a real machine, in order to measure some aspect of OS or hardware\nread), and time how long it takes; dividing the time by the number of\nbenchmark does so by running two processes on a single CPU, and set-\nthe second pipe, the OS puts the ﬁrst process in the blocked state, and\nswitches to the other process, which reads from the ﬁrst pipe and then\nWhen the second process tries to read from the ﬁrst\nlmbench can make a good estimate of the cost of a context switch.\nOne difﬁculty in measuring context-switch cost arises in systems with\nBy now low-level mechanisms of running processes (e.g., context switch-\nnumber of simplifying assumptions about the processes running in the\ntimes called jobs, that are running in the system:\n1. Each job runs for the same amount of time.\n2. All jobs arrive at the same time.\n3. All jobs only use the CPU (i.e., they perform no I/O)\n4. The run-time of each job is known.\nlar, it might bother you that the run-time of each job is known: this would\nthere are a number of different metrics that make sense in scheduling.\ngle metric: turnaround time.\nThe turnaround time of a job is deﬁned\nas the time at which the job completes minus the time at which the job\nMore formally, the turnaround time Tturnaround is:\nBecause we have assumed that all jobs arrive at the same time, for now\nYou should note that turnaround time is a performance metric, which\nample, may optimize performance but at the cost of preventing a few jobs\nsystem, A, B, and C, at roughly the same time (Tarrival = 0).\nFIFO has to put some job ﬁrst, let’s assume that while they all arrived\nAssume also that each job runs for 10 seconds.\naverage turnaround time be for these jobs?\nTime\nThus, the average turnaround time for the three jobs is simply 10+20+30\nComputing turnaround time is as easy as that.\nsumption 1, and thus no longer assume that each job runs for the same\namount of time.\nIn particular, let’s again assume three jobs (A, B, and\nC), but this time A runs for 100 seconds while B and C run for 10 each.\nTime\nAs you can see in Figure 7.2, Job A runs ﬁrst for the full 100 seconds\ntime for the system is high: a painful 110 seconds ( 100+110+120\nShortest Job First represents a general scheduling principle that can be\ndeal with our new reality of jobs that run for different amounts of time?\nscheduling of jobs in computer systems.\nTime\nLet’s take our example above but with SJF as our scheduling policy.\nFigure 7.3 shows the results of running A, B, and C.\nage turnaround time.\nSimply by running B and C before A, SJF reduces\nIn fact, given our assumptions about jobs all arriving at the same time,\nulers were developed; such systems would run each job to completion\nbefore considering whether to run a new job.\nperform a context switch, stopping one running process temporarily and\nThus we arrive upon a good approach to scheduling with SJF, but our\nwe can target assumption 2, and now assume that jobs can arrive at any\ntime instead of all at once.\nThis time,\nassume A arrives at t = 0 and needs to run for 100 seconds, whereas B\nand C arrive at t = 10 and each need to run for 10 seconds.\nTime\nAverage turnaround time for these three jobs\n). What can a scheduler do?\nShortest Time-to-Completion First (STCF)\nanisms such as timer interrupts and context switching, the scheduler can\ncertainly do something else when B and C arrive: it can preempt job A\nand decide to run another job, perhaps continuing A later.\nTime\ntion to SJF, known as the Shortest Time-to-Completion First (STCF) or\nPreemptive Shortest Job First (PSJF) scheduler [CK68].\nAny time a new\nwhich has the least time left, and then schedules that one.\nexample, STCF would preempt A and run B and C to completion; only\nwhen they are ﬁnished would A’s remaining time be scheduled.\nThe result is a much-improved average turnaround time: 50 seconds\nonly metric was turnaround time, STCF would be a great policy.\nmetric was born: response time.\nResponse time is deﬁned as the time from when the job arrives in a\nsystem to the ﬁrst time it is scheduled.\nFor example, if we had the schedule above (with A arriving at time 0,\nand B and C at time 10), the response time of each job is as follows: 0 for\njob A, 0 for B, and 10 for C (average: 3.33).\nticularly good for response time.\nIf three jobs arrive at the same time,\nfor example, the third job has to wait for the previous two jobs to run in\nthat is sensitive to response time?\nTime\nFigure 7.6: SJF Again (Bad for Response Time)\nTime\nFigure 7.7: Round Robin (Good for Response Time)\nThe basic idea is simple: instead of running jobs to completion, RR runs\na job for a time slice (sometimes called a scheduling quantum) and then\nswitches to the next job in the run queue.\nFor this reason, RR is sometimes called time-\nNote that the length of a time slice must be a multiple of the\nthe time slice could be 10, 20, or any other multiple of 10 ms.\nthree jobs A, B, and C arrive at the same time in the system, and that\nAn SJF scheduler runs each job to\ntime-slice of 1 second would cycle through the jobs quickly (Figure 7.7).\nThe average response time of RR is:\nsponse time is: 0+5+10\nAs you can see, the length of the time slice is critical for RR.\nit is, the better the performance of RR under the response-time metric.\nHowever, making the time slice too short is problematic: suddenly the\noften (i.e., by performing the operation fewer times), the total cost to the\nFor example, if the time slice is set to 10 ms, and the\ncontext-switch cost is 1 ms, roughly 10% of time is spent context switch-\nthe time slice, e.g., to 100 ms.\ncontext switching, and thus the cost of time-slicing has been amortized.\nto be ﬂushed and new state relevant to the currently-running job to be\nRR, with a reasonable time slice, is thus an excellent scheduler if re-\nsponse time is our only metric.\ntime?\nA, B, and C, each with run-\nning times of 5 seconds, arrive at the same time, and RR is the scheduler\nwith a (long) 1-second time slice.\nturnaround time is our metric.\nRR is doing is stretching out each job as long as it can, by only running\ntime only cares about when jobs ﬁnish, RR is nearly pessimal, even worse\nvides the CPU among active processes on a small time scale, will perform\npoorly on metrics such as turnaround time.\ntrade-off: if you are willing to be unfair, you can run shorter jobs to com-\npletion, but at the cost of response time; if you instead value fairness,\nresponse time is lowered, but at the cost of turnaround time.\noptimizes turnaround time, but is bad for response time.\n(RR) optimizes response time but is bad for turnaround.\nhave two assumptions which need to be relaxed: assumption 3 (that jobs\ndo no I/O), and assumption 4 (that the run-time of each job is known).\noutput each time.",
      "keywords": [
        "time",
        "turnaround time",
        "job",
        "system",
        "response time",
        "Jobs",
        "eax",
        "Scheduling",
        "LIMITED DIRECT EXECUTION",
        "SJF",
        "movl",
        "time slice",
        "average turnaround time",
        "CPU",
        "run"
      ],
      "concepts": [
        "time",
        "scheduler",
        "schedule",
        "jobs",
        "performance",
        "perform",
        "operating",
        "operations",
        "operation",
        "running"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "",
          "score": 0.648,
          "base_score": 0.498,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "time",
          "job",
          "jobs",
          "turnaround",
          "turnaround time"
        ],
        "semantic": [],
        "merged": [
          "time",
          "job",
          "jobs",
          "turnaround",
          "turnaround time"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3198924522773086,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116739+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 103-116)",
      "start_page": 103,
      "end_page": 116,
      "summary": "A scheduler clearly has a decision to make when a job initiates an I/O\nrequest, because the currently-running job won’t be using the CPU dur-\nuler should probably schedule another job on the CPU at that time.\ncourse, it could even decide to run the job at that point.\nOS treat each job?\nB, which each need 50 ms of CPU time.\nThe scheduler runs A ﬁrst, then B after (Figure 7.8).\nscheduler account for the fact that A is broken up into 5 10-ms sub-jobs,\njob and then the other without considering how to take I/O into account\neach CPU burst as a job, the scheduler makes sure processes that are “in-\nWhile those interactive jobs are performing\nI/O, other CPU-intensive jobs run, thus better utilizing the processor.\ntion: that the scheduler knows the length of each job.\nabout the length of each job.\nThe ﬁrst runs the shortest job remaining and\nthus optimizes turnaround time; the second alternates between all jobs\nsaid approach to scheduling a time-shared system.\nulers perform under scheduling metrics such as response time, turnaround\nthree jobs of length 200 with the SJF and FIFO schedulers.\n2. Now do the same but with jobs of different lengths: 100, 200, and\n3. Now do the same, but also with the RR scheduler and a time-slice\n6. What happens to response time with SJF as job lengths increase?\nsponse time, given N jobs?\nScheduling:\nnote, is done by running shorter jobs ﬁrst; unfortunately, the OS doesn’t\ngenerally know how long a job will run for, exactly the knowledge that\nsystem runs, the characteristics of the jobs it is running, and thus make\nHow can we design a scheduler that both minimizes response time for\ninteractive jobs while also minimizing turnaround time without a priori\nknowledge of job length?\nSCHEDULING:\napproaches work when jobs have phases of behavior and are thus pre-\nAt any given time, a job that is ready\nMLFQ uses priorities to decide which job\nshould run at a given time: a job with higher priority (i.e., a job on a\nOf course, more than one job may be on a given queue, and thus have\namong those jobs.\nRather than giving a ﬁxed priority to each job, MLFQ varies the\npriority of a job based on its observed behavior.\nIf, for example, a job repeat-\nMLFQ will keep its priority high, as this is how an interactive process\nIf, instead, a job uses the CPU intensively for long periods\nof time, MLFQ will reduce its priority.\nabout processes as they run, and thus use the history of the job to predict\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\nIn the ﬁgure, two jobs (A and B) are at the highest priority level, while job\nC is in the middle and Job D is at the lowest priority.\nknowledge of how MLFQ works, the scheduler would just alternate time\nslices between A and B because they are the highest priority jobs in the\nsystem; poor jobs C and D would never even get to run – an outrage!\nstand how job priority changes over time.\nof a job (and thus which queue it is on) over the lifetime of a job.\nlonger-running “CPU-bound” jobs that need a lot of CPU time but where\n• Rule 3: When a job enters the system, it is placed at the highest\n• Rule 4a: If a job uses up an entire time slice while running, its pri-\n• Rule 4b: If a job gives up the CPU before the time slice is up, it stays\nExample 1: A Single Long-Running Job\nhas been a long running job in the system.\nto this job over time in a three-queue scheduler.\nFigure 8.2: Long-running Job Over Time\nAs you can see in the example, the job enters at the highest priority\nAfter a single time-slice of 10 ms, the scheduler reduces the job’s\npriority by one, and thus the job is on Q1.\nAfter running at Q1 for a time\nslice, the job is ﬁnally lowered to the lowest priority in the system (Q0),\nExample 2: Along Came A Short Job\nIn this example, there are two jobs: A,\nwhich is a long-running CPU-intensive job, and B, which is a short-running\ninteractive job.\nAssume A has been running for some time, and then B ar-\nning along in the lowest-priority queue (as would any long-running CPU-\nintensive jobs); B (shown in gray) arrives at time T = 100, and thus is\nFigure 8.3: Along Came An Interactive Job\ninserted into the highest queue; as its run-time is short (only 20 ms), B\nshort job or a long-running job, it ﬁrst assumes it might be a short job, thus\ngiving the job high priority.\nIf it actually is a short job, it will run quickly\nand complete; if it is not a short job, it will slowly move down the queues,\njob, for example, is doing a lot of I/O (say by waiting for user input from\nFigure 8.4 shows an example of how this works, with an interactive job\nI/O competing for the CPU with a long-running batch job A (shown in\nThe MLFQ approach keeps B at the highest priority because B\nkeeps releasing the CPU; if B is an interactive job, MLFQ further achieves\nits goal of running interactive jobs quickly.\nCPU fairly between long-running jobs, and letting short or I/O-intensive\ninteractive jobs run quickly.\nteractive jobs in the system, they will combine to consume all CPU time,\nand thus long-running jobs will never receive any CPU time (they starve).\nthe following attack: before the time slice is over, issue an I/O operation\nWhen done right (e.g., by running for 99% of a time slice\ninteractive jobs in the system.\nWhat could we do in order to guarantee that CPU-bound jobs\nThe simple idea here is to periodically boost the priority of all the jobs\n• Rule 5: After some time period S, move all the jobs in the system\nanteed not to starve: by sitting in the top queue, a job will share the CPU\nwith other high-priority jobs in a round-robin fashion, and thus eventu-\nSecond, if a CPU-bound job has become interactive,\na long-running job when competing for the CPU with two short-running\ninteractive jobs.\nno priority boost, and thus the long-running job gets starved once the two\nshort jobs arrive; on the right, there is a priority boost every 50 ms (which\nwe at least guarantee that the long-running job will make some progress,\nRules 4a and 4b, which let a job retain its priority by relinquishing the\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\ngaming, a process can issue an I/O just before a time slice ends and thus\nHow big should the time slice be per queue?\nFor example, most MLFQ variants allow for varying time-slice length\ntime slices; they are comprised of interactive jobs, after all, and thus\nThe low-priority queues, in contrast, contain long-running jobs\nthat are CPU-bound; hence, longer time slices work well (e.g., 100s of\nFigure 8.7 shows an example in which two long-running jobs run\nwe call it Ousterhout’s Law. The Solaris MLFQ implementation – the Time-Sharing scheduling class,\nlifetime, how long each time slice is, and how often to boost the priority of\nOther MLFQ schedulers don’t use a table or the exact rules described\ncalculate the current priority level of a job, basing it on how much CPU\nFor example, some schedulers reserve the highest priority levels\nfor operating system work; thus typical user jobs can never obtain the\nutility nice you can increase or decrease the priority of a job (somewhat)\npriority of a given job.\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\n• Rule 3: When a job enters the system, it is placed at the highest\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\n• Rule 5: After some time period S, move all the jobs in the system\nfor short-running interactive jobs, and is fair and makes progress for long-",
      "keywords": [
        "job",
        "multi-level feedback queue",
        "Priority",
        "time",
        "CPU",
        "MLFQ",
        "jobs",
        "CPU time",
        "feedback queue",
        "queue",
        "SCHEDULING",
        "interactive jobs",
        "system",
        "multi-level feedback",
        "time slice"
      ],
      "concepts": [
        "job",
        "jobs",
        "scheduling",
        "schedule",
        "time",
        "priority",
        "priorities",
        "run",
        "running",
        "cpu"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.792,
          "base_score": 0.642,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "job",
          "priority",
          "jobs",
          "interactive",
          "time"
        ],
        "semantic": [],
        "merged": [
          "job",
          "priority",
          "jobs",
          "interactive",
          "time"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3711620418875552,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116802+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 117-125)",
      "start_page": 117,
      "end_page": 125,
      "summary": "SCHEDULING:\nA great short set of notes by one of the authors on the details of the Solaris scheduler.\nSCHEDULING:\n1. Run a few randomly-generated problems with just two jobs and\nHow would you run the scheduler to reproduce each of the exam-\n4. Craft a workload with two jobs and scheduler parameters so that\nScheduling: Proportional Share\nproportional-share scheduler, also sometimes referred to as a fair-share\nscheduler.\nAn excellent modern example of proportional-share scheduling is found\nHow can we design a scheduler to share the CPU in a proportional\nBasic Concept: Tickets Represent Your Share\nUnderlying lottery scheduling is one very basic concept: tickets, which\nThe percent of tickets that a process has repre-\nthat A has 75 tickets while B has only 25.\nLottery scheduling achieves this probabilistically (but not determinis-\na lottery is straightforward: the scheduler must know how many total\ntickets there are (in our example, there are 100).\nThe scheduler then picks\nSCHEDULING: PROPORTIONAL SHARE\nOne of the most beautiful aspects of lottery scheduling is its use of ran-\nIn a traditional fair-share scheduling algorithm, tracking how\nof tickets each has).\na winning ticket, which is a number from 0 to 991.\nHere is an example output of a lottery scheduler’s winning tickets:\nSCHEDULING: PROPORTIONAL SHARE\nTIP: USE TICKETS TO REPRESENT SHARES\n(and stride) scheduling is that of the ticket.\nthe ticket.\nLottery scheduling also provides a number of mechanisms to manip-\nthe concept of ticket currency.\nets to allocate tickets among their own jobs in whatever currency they\nFor example, assume users A and B have each been given 100 tickets.\nUser A is running two jobs, A1 and A2, and gives them each 500 tickets\nUser B is running only 1 job\ncan temporarily hand off its tickets to another process.\nTo speed up the work, the client can pass the tickets to the server and\ntickets back to the client and all is as before.\ninﬂation, a process can temporarily raise or lower the number of tickets\nSCHEDULING: PROPORTIONAL SHARE\nget a value, between 0 and the total # of tickets\n// loop until the sum of ticket values is > the winner\ncounter = counter + current->tickets;\n// ’current’ is the winner: schedule it...\nFigure 9.1: Lottery Scheduling Decision Code\ncesses of the system (e.g., a list), and the total number of tickets.\nprised of three processes, A, B, and C, each with some number of tickets.\nTo make a scheduling decision, we ﬁrst have to pick a random number\n(the winner) from the total number of tickets (400)2 Let’s say we pick the\nThe code walks the list of processes, adding each ticket value to counter\nWith our example of the winning ticket being 300,\nThen counter would be updated to 150 (B’s tickets), still less than 300\nSCHEDULING: PROPORTIONAL SHARE\nof the tickets.\nTo make the dynamics of lottery scheduling more understandable, we\nagainst one another, each with the same number of tickets (100) and same\ntime, but due to the randomness of lottery scheduling, sometimes one\nOnly as the jobs run for a signiﬁcant number of time slices\ndoes the lottery scheduler approach the desired outcome.\nSCHEDULING: PROPORTIONAL SHARE\nOne problem we have not addressed with lottery scheduling is: how\nto assign tickets to jobs?\ncase, each user is handed some number of tickets, and a user can allocate\ntickets to any jobs they run as desired.\nstride scheduling, a deterministic fair-share scheduler [W95].\nStride scheduling is also straightforward.\na stride, which is inverse in proportion to the number of tickets it has.\nour example above, with jobs A, B, and C, with 100, 50, and 250 tickets,\nnumber by the number of tickets each process has been assigned.\nexample, if we divide 10,000 by each of those ticket values, we obtain\nthis value the stride of each process; every time a process runs, we will\nThe scheduler then uses the stride and pass to determine which pro-\nthe process to run that has the lowest pass value so far; when you run\na process, increment its pass counter by its stride.\nschedule(current);\nIn our example, we start with three processes (A, B, and C), with stride\nﬁrst, any of the processes might run, as their pass values are equally low.\nThen we run B, whose pass value is then\nFinally, we run C, whose pass value is incremented to 40.\nSCHEDULING: PROPORTIONAL SHARE\nTable 9.1: Stride Scheduling: A Trace\nbehavior of the scheduler over time.\nexactly in proportion to their ticket values of 250, 100, and 50.\nscheduling achieves the proportions probabilistically over time; stride\nwhy use lottery scheduling at all?\nWell, lottery scheduling has one nice\nproperty that stride scheduling does not: no global state.\njob enters in the middle of our stride scheduling example above; what\nWith lottery scheduling, there is no global state per process;\nwe simply add a new process with whatever tickets it has, update the\nWe have introduced the concept of proportional-share scheduling and\nbrieﬂy discussed two implementations: lottery and stride scheduling.\nLottery uses randomness in a clever way to achieve proportional share;",
      "keywords": [
        "lottery scheduling",
        "SCHEDULING",
        "Tickets",
        "stride scheduling",
        "lottery",
        "Proportional Share",
        "stride",
        "job",
        "UNIX Operating System",
        "number",
        "Share",
        "process",
        "time",
        "Run",
        "jobs"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "tickets",
        "jobs",
        "job",
        "time",
        "randomly",
        "randomized",
        "current",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.484,
          "base_score": 0.334,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.477,
          "base_score": 0.327,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tickets",
          "scheduling",
          "lottery",
          "stride",
          "lottery scheduling"
        ],
        "semantic": [],
        "merged": [
          "tickets",
          "scheduling",
          "lottery",
          "stride",
          "lottery scheduling"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2598293706341066,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116851+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 126-137)",
      "start_page": 126,
      "end_page": 137,
      "summary": "SCHEDULING: PROPORTIONAL SHARE\nAs a result, proportional-share schedulers are more useful in domains\nSCHEDULING: PROPORTIONAL SHARE\nA paper by one of the authors on how to extend proportional-share scheduling to work better in a\nAn early reference to a fair-share scheduler.\nSCHEDULING: PROPORTIONAL SHARE\nMultiprocessor Scheduling (Advanced)\nThis chapter will introduce the basics of multiprocessor scheduling.\ntime making a single CPU much faster without using (way) too much\nthan a single CPU.\nprogram you wrote) only uses a single CPU; adding more CPUs does not\nrun faster when given more CPU resources.\non virtualization (generally) and CPU scheduling (speciﬁcally).\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nCPU\nFigure 10.1: Single CPU With Cache\nwe’ve discussed a number of principles behind single-processor schedul-\nCRUX: HOW TO SCHEDULE JOBS ON MULTIPLE CPUS\nHow should the OS schedule jobs on multiple CPUs?\nTo understand the new issues surrounding multiprocessor schedul-\ncaches that in general help the processor run programs faster.\ning frequently accessed data in a cache, the system can make the large,\nCPU; the CPU has a small cache (say 64 KB) and a large main memory.\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nCPU\nCPU\nFigure 10.2: Two CPUs With Caches Sharing Memory\nThe ﬁrst time a program issues this load, the data resides in main mem-\nbe reused, puts a copy of the loaded data into the CPU cache.\ngram later fetches this same data item again, the CPU ﬁrst checks for it in\nsystems can make good guesses about which data to put in a cache and\nImagine, for example, that a program running on CPU 1 reads\ncache on CPU 1, the system fetches it from main memory, and gets the\ning its cache with the new value D′; writing the data through all the way\nassume the OS decides to stop running the program and move it to CPU\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nCPU 2’s cache, and thus the system fetches the value from main memory,\nWhen a CPU then sees an update for a data item it holds in its cache,\nfrom its own cache) or update it (i.e., put the new value into its cache\nexample, assume we have a shared queue being accessed on multiple\nMULTIPROCESSOR SCHEDULING (ADVANCED)\ncally, as the number of CPUs grows, access to a synchronized shared data\nOne ﬁnal issue arises in building a multiprocessor cache scheduler,\nparticular CPU, builds up a fair bit of state in the caches (and TLBs) of the\nCPU.\nthe caches on that CPU.\nIf, instead, one runs a process on a different CPU\nCPU thanks to the cache coherence protocols of the hardware).\nmultiprocessor scheduler should consider cache afﬁnity when making its\nSingle-Queue Scheduling\nreuse the basic framework for single processor scheduling, by putting all\njobs that need to be scheduled into a single queue; we call this single-\nqueue multiprocessor scheduling or SQMS for short.\nexisting policy that picks the best job to run next and adapt it to work on\nmore than one CPU (where it might pick the best two jobs to run, if there\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nTo ensure the scheduler works correctly on multiple CPUs,\nqueue (say, to ﬁnd the next job to run), the proper outcome arises.\nThe second main problem with SQMS is cache afﬁnity.\nlet us assume we have ﬁve jobs to run (A, B, C, D, E) and four processors.\nOur scheduling queue thus looks like this:\nOver time, assuming each job runs for a time slice and then another\njob is chosen, here is a possible job schedule across CPUs:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nBecause each CPU simply picks the next job to run from the globally-\nshared queue, each job ends up bouncing around from CPU to CPU, thus\nTo handle this problem, most SQMS schedulers include some kind of\nto run on the same CPU if possible.\nimagine the same ﬁve jobs scheduled as follows:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nsors, with only job E migrating from CPU to CPU, thus preserving afﬁn-\nscheduler, which by deﬁnition has only a single queue.\nMulti-Queue Scheduling\nBecause of the problems caused in single-queue schedulers, some sys-\ntems opt for multiple queues, e.g., one per CPU.\nmulti-queue multiprocessor scheduling (or MQMS).\nEach queue will likely follow a particular scheduling disci-\nWhen a job enters the system, it is placed on exactly one scheduling\nfound in the single-queue approach.\n(labeled CPU 0 and CPU 1), and some number of jobs enter the system:\nGiven that each CPU has a scheduling queue\nnow, the OS has to decide into which queue to place each job.\nDepending on the queue scheduling policy, each CPU now has two\nCPU 1\nCPU 0\nber of queues, and thus lock and cache contention should not become a\nMULTIPROCESSOR SCHEDULING (ADVANCED)\njobs stay on the same CPU and thus reap the advantage of reusing cached\nproblem, which is fundamental in the multi-queue based approach: load\nfollowing scheduling queues:\nCPU 1\nCPU 0\nThe scheduling\nCPU 0\nCPU 1\nSo what should a poor multi-queue multiprocessor scheduler do?\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nHow should a multi-queue multiprocessor scheduler handle load im-\nhave a situation where one CPU is idle and the other has some jobs.\nCPU 0\nCPU 1",
      "keywords": [
        "CPU",
        "Multiprocessor Scheduling",
        "CPUs",
        "SCHEDULING",
        "single CPU",
        "multiple CPUs",
        "Cache",
        "CPU scheduling",
        "Multiprocessor",
        "jobs",
        "system",
        "data",
        "job",
        "memory",
        "single"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "cpu",
        "jobs",
        "job",
        "cache",
        "caching",
        "share",
        "sharing",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 15,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 13,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cpu",
          "cpu cpu",
          "scheduling",
          "multiprocessor",
          "queue"
        ],
        "semantic": [],
        "merged": [
          "cpu",
          "cpu cpu",
          "scheduling",
          "multiprocessor",
          "queue"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.356008037754201,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116913+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "summary": "Linux Multiprocessor Schedulers\nproached to building a multiprocessor scheduler.\ncussed before), changing a process’s priority over time and then schedul-\nproportional-share approach (more like Stride scheduling, as discussed\nWe have seen various approaches to multiprocessor scheduling.\nA dissertation that covers a lot of the details of how modern Linux multiprocessor scheduling works.\nA tech report on this cool scheduling idea, from Ion Stoica, now a professor at U.C. Berkeley and world\nProfessor: So, Student, did you learn anything?\nStudent: Well, Professor, that seems like a loaded question.\nprofessor a break, will you?\nhow the OS virtualizes the CPU.\nProfessor: Good, good!\nStudent: All those interactions do seem a little complicated though; how can I\nProfessor: Well, that’s a good question.\nStudent: Sounds good.\nProfessor: Well, did you get some sense of the philosophy of the OS in your\nStudent: Hmm...\nIt seems like the OS is fairly paranoid.\nPerhaps that is why we think of the OS\nProfessor: Yes indeed – sounds like you are starting to put it together!\nStudent: Thanks.\nStudent: Some lessons to be learned there for sure.\nStudent: Well, that you can build a smart scheduler that tries to be like SJF and\nBuilding up a real scheduler seems\nProfessor: Indeed it is.\nwhich scheduler to use; see the Linux battles between CFS, BFS, and the O(1)\nscheduler, for example.\nProfessor: Probably not.\nscheduler is good at turnaround time, it’s bad at response time, and vice versa.\nStudent: That’s a little depressing.\nProfessor: Good engineering can be that way.\nStudent: I really liked the notion of gaming the scheduler; it seems like that\nProfessor: It looks like I might have created a monster!\nProfessor Frankenstein\nProfessor: I guess so.\nA Dialogue on Memory Virtualization\nStudent: So, are we done with virtualization?\nProfessor: No!\nStudent: Hey, no reason to get so excited; I was just asking a question.\nStudents\nVirtualizing memory is\nmodern virtual memory manager.\nStudent: Neat!\nFor understanding virtual memory, start with this: every address generated\nby a user program is a virtual address.\nsome hardware help, the OS will turn these pretend virtual addresses into real\nA DIALOGUE ON MEMORY VIRTUALIZATION\nStudent: OK, I think I can remember that...\nprogram is virtual, every address from a user program is virtual, every ...\nProfessor: What are you mumbling about?\nStudent: Oh nothing....\nAnyway, why does the OS want\nProfessor: Mostly ease of use: the OS will give each program the view that it\nvariable?” because the virtual address space of the program is large and has lots\nStudent: Why else?\nStudent: Probably not.\nStudent: Maybe we should.\nFrom the perspective of memory, early machines didn’t provide much\nThe OS was a set of routines (a library, really) that sat in memory (start-\nning program (a process) that currently sat in physical memory (starting\nat physical address 64k in this example) and used the rest of memory.\nOS.\nLife was sure easy for OS developers in those days, wasn’t it?",
      "keywords": [
        "professor",
        "Student",
        "Memory",
        "scheduler",
        "n’t",
        "MULTIPROCESSOR SCHEDULING",
        "program",
        "Good",
        "Linux Multiprocessor Schedulers",
        "Virtual",
        "address",
        "MULTIPROCESSOR",
        "SYSTEMS",
        "Linux",
        "BFS"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "student",
        "professor",
        "memory",
        "likes",
        "liked",
        "virtual",
        "process",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "",
          "score": 0.792,
          "base_score": 0.642,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 16,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "student",
          "professor",
          "multiprocessor",
          "scheduler",
          "virtual"
        ],
        "semantic": [],
        "merged": [
          "student",
          "professor",
          "multiprocessor",
          "scheduler",
          "virtual"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3575618089026769,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.116972+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 146-153)",
      "start_page": 146,
      "end_page": 153,
      "summary": "THE ABSTRACTION: ADDRESS SPACES\nProcess C\nFigure 13.2: Three Processes: Sharing Memory\n[DV66], in which multiple processes were ready to run at a given time,\nand the era of time sharing was born [S59, L60, M62, M83].\nOne way to implement time sharing would be to run one process for\nmemory), load some other process’s state, run it for a while, and thus\nwe’d rather do is leave processes in memory while switching between\nthem, allowing the OS to implement time sharing efﬁciently (Figure 13.2).\nTHE ABSTRACTION: ADDRESS SPACES\nFigure 13.3: An Example Address Space\nAssuming a single CPU, the OS chooses to run one of the processes\ning multiple programs to reside concurrently in memory makes protec-\nworse, write some other process’s memory.\nThe Address Space\nrequires the OS to create an easy to use abstraction of physical memory.\nWe call this abstraction the address space, and it is the running program’s\nstraction of memory is key to understanding how memory is virtualized.\nThe address space of a process contains all of the memory state of the\nheap is used for dynamically-allocated, user-managed memory, such as\nTHE ABSTRACTION: ADDRESS SPACES\nIn the example in Figure 13.3, we have a tiny address space (only 16\nThe program code lives at the top of the address space (starting at\n0 in this example, and is packed into the ﬁrst 1K of the address space).\nCode is static (and thus easy to place in memory), so we can place it at\nNext, we have the two regions of the address space that may grow\nyou could arrange the address space in a different way if you’d like (as\nnice way to divide the address space like this works anymore, alas).\nOf course, when we describe the address space, what we are describ-\ning is the abstraction that the OS is providing to the running program.\nThe program really isn’t in memory at physical addresses 0 through 16KB;\ncesses A, B, and C in Figure 13.2; there you can see how each process is\nloaded into memory at a different address.\nTHE CRUX: HOW TO VIRTUALIZE MEMORY\naddress space for multiple running processes (all sharing memory) on\nWhen the OS does this, we say the OS is virtualizing memory, because\nthe running program thinks it is loaded into memory at a particular ad-\nat address 0 (which we will call a virtual address), somehow the OS, in\n320KB (where A is loaded into memory).\nTHE ABSTRACTION: ADDRESS SPACES\nOperating systems strive to isolate processes from\nmemory isolation, the OS further ensures that running programs cannot\nthe OS.\nmemory.\nThe OS will not only virtualize memory, though; it will do so\nOne major goal of a virtual memory (VM) system is transparency2.\nThe OS should implement virtual memory in a way that is invisible to\nthat memory is virtualized; rather, the program behaves as if it has its\ning programs run much more slowly) and space (i.e., not using too much\nmemory for structures needed to support virtualization).\ning time-efﬁcient virtualization, the OS will have to rely on hardware\nprotect processes from one another as well as the OS itself from pro-\nof any other process or the OS itself (that is, anything outside its address\nTHE ABSTRACTION: ADDRESS SPACES\nASIDE: EVERY ADDRESS YOU SEE IS VIRTUAL\n(some large number, often printed in hexadecimal), is a virtual address.\nthat out too, and yes, if you can print it, it also is a virtual address.\nfact, any address you can see as a programmer of a user-level program\nis a virtual address.\nvirtualizing memory, that knows where in the physical memory of the\nprint out an address in a program, it’s a virtual one, an illusion of how\nthings are laid out in memory; only the OS (and the hardware) knows the\nWhen run on a 64-bit Mac OS X machine, we get the following output:\nFrom this, you can see that code comes ﬁrst in the address space, then\nthe heap, and the stack is all the way at the other end of this large virtual\nAll of these addresses are virtual, and will be translated by the OS\nnisms needed to virtualize memory, including hardware and operating\nfree space and which pages to kick out of memory when you run low on\nvirtual memory system really works3.\nTHE ABSTRACTION: ADDRESS SPACES\nsparse, private address space to programs, which hold all of their instruc-\ntake each of these virtual memory references, and turn them into physi-\ncal addresses, which can be presented to the physical memory in order to\nThe OS will do this for many processes at\nTHE ABSTRACTION: ADDRESS SPACES\nA great early example of a system that swapped program memory to the “drum” when the program\nTHE ABSTRACTION: ADDRESS SPACES",
      "keywords": [
        "Address Space",
        "ADDRESS",
        "Memory",
        "Space",
        "large address space",
        "Time Sharing",
        "Program",
        "physical memory",
        "virtual address",
        "System",
        "Operating systems",
        "ABSTRACTION",
        "code",
        "Time",
        "virtual memory"
      ],
      "concepts": [
        "program",
        "programming",
        "memory",
        "process",
        "processes",
        "processing",
        "spaces",
        "address",
        "addresses",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 10,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 6,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 9,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 14,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "address",
          "memory",
          "abstraction address",
          "abstraction",
          "address space"
        ],
        "semantic": [],
        "merged": [
          "address",
          "memory",
          "abstraction address",
          "abstraction",
          "address space"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44651107133754747,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117037+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 154-161)",
      "start_page": 154,
      "end_page": 161,
      "summary": "In this interlude, we discuss the memory allocation interfaces in UNIX\nCRUX: HOW TO ALLOCATE AND MANAGE MEMORY\nIn UNIX/C programs, understanding how to allocate and manage\nTypes of Memory\nIn running a C program, there are two types of memory that are allo-\nThe ﬁrst is called stack memory, and allocations and deallocations\nthis reason it is sometimes called automatic memory.\nDeclaring memory on the stack in C is easy.\nneed some space in a function func() for an integer, called x.\nsuch a piece of memory, you just do something like this:\ncompiler deallocates the memory for you; thus, if you want some infor-\nIt is this need for long-lived memory that gets us to the second type\nof memory, called heap memory, where all allocations and deallocations\nint *x = (int *) malloc(sizeof(int));\ncalls malloc(), it requests space for an integer on the heap; the routine\nheap memory presents more challenges to both users and systems.\nThe malloc() Call\nThe manual page shows what you need to do to use malloc; type man\ncompiler check whether you are calling malloc() correctly (e.g., passing\nThis invocation of malloc() uses the\nint *x = malloc(10 * sizeof(int));\nply asking how big a pointer to an integer is, not how much memory we\nYou might also notice that malloc() returns a pointer to type void.\ncasts the return type of malloc() to a pointer to a double.\ndoing.” By casting the result of malloc(), the programmer is just giving\nAs it turns out, allocating memory is the easy part of the equation;\nknowing when, how, and even if to free memory is the hard part.\nheap memory that is no longer in use, programmers simply call free():\nint *x = malloc(10 * sizeof(int));\nThe routine takes one argument, a pointer that was returned by malloc().\nby the user, and must be tracked by the memory-allocation library itself.\nThere are a number of common errors that arise in the use of malloc()\nCorrect memory management has been such a problem, in fact, that\nmany newer languages have support for automatic memory manage-\nto allocate memory (usually new or something similar to allocate a new\nForgetting To Allocate Memory\nMany routines expect memory to be allocated before you call them.\nMEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY.\nNot Allocating Enough Memory\nA related error is not allocating enough memory, sometimes called a buffer\nIn other cases, the malloc library\nallocated a little extra space anyhow, and thus your program actually\nForgetting to Initialize Allocated Memory\nForgetting To Free Memory\nAnother common error is known as a memory leak, and it occurs when\nyou forget to free memory.\nas the OS itself), this is a huge problem, as slowly leaking memory even-\ntually leads one to run out of memory, at which point a restart is required.\nThus, in general, when you are done with a chunk of memory, you should\nhelp here: if you still have a reference to some chunk of memory, no\ngarbage collector will ever free it, and thus memory leaks remain a prob-\nNote that not all memory need be freed, at least, in certain cases.\nexample, when you write a short-lived program, you might allocate some\nspace using malloc().\nprocess, including any memory it has allocated.\nFreeing Memory Before You Are Done With It\nSometimes a program will free memory before it is ﬁnished using it; such\nmemory (e.g., you called free(), but then called malloc() again to\nallocate something else, which then recycles the errantly-freed memory).\nFreeing Memory Repeatedly\nPrograms also sometimes free memory more than once; this is known as\nine, the memory-allocation library might get confused and do all sorts of\nyour memory-related problems.\ncalls when discussing malloc() and free().\nare used by the memory-allocation library; if you try to use them, you\nFinally, you can also obtain memory from the operating system via the\nanonymous memory region within your program – a region which is not\nThis memory can then\nThere are a few other calls that the memory-allocation library sup-\nFor example, calloc() allocates memory and also zeroes it be-\nfore returning; this prevents some errors where you assume that memory\nadd something to it: realloc() makes a new larger region of memory,",
      "keywords": [
        "Memory",
        "malloc",
        "Memory API",
        "free",
        "call",
        "free memory",
        "program",
        "space",
        "sizeof",
        "pointer",
        "Interlude",
        "heap memory",
        "API",
        "heap",
        "n’t"
      ],
      "concepts": [
        "memory",
        "allocation",
        "allocate",
        "allocations",
        "allocated",
        "called",
        "correctly",
        "free",
        "programs",
        "space"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 20,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.644,
          "base_score": 0.494,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "malloc",
          "free memory",
          "free",
          "allocate"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "malloc",
          "free memory",
          "free",
          "allocate"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34634608954683466,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117096+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 162-173)",
      "start_page": 162,
      "end_page": 173,
      "summary": "their address spaces in whatever way they would like, thus making the\nreferred to as hardware-based address translation, or just address trans-\nWith address translation, the hardware transforms each\ntual address provided by the instruction to a physical address where the\nThus, on each and every memory\nreference, an address translation is performed by the hardware to redirect\nOf course, the hardware alone cannot virtualize memory, as it just pro-\nSpeciﬁcally, we will assume for now that the user’s address space must\nImagine there is a process whose address space as indicated in Figure\nIn virtualizing memory, the hardware\nwill interpose on each memory access, and translate each virtual address\nissued by the process to a physical address where the desired informa-\naddress of x has been placed in the register ebx, and then loads the value\nat that address into the general-purpose register eax using the movl in-\nthe process’s address space; the three-instruction code sequence is located\n• Execute this instruction (load from address 15 KB)\n• Execute this instruction (no memory reference)\n• Execute this instruction (store to address 15 KB)\nFigure 15.1: A Process And Its Address Space\nFrom the program’s perspective, its address space starts at address 0\nHowever, to virtualize memory, the OS\nwants to place the process somewhere else in physical memory, not nec-\nthis process in memory in a way that is transparent to the process?\ncan provide the illusion of a virtual address space starting at 0, when in\nreality the address space is located at some other physical address?\nFigure 15.2: Physical Memory with a Single Relocated Process\nAn example of what physical memory might look like once this pro-\ncess’s address space has been placed in memory is found in Figure 15.2.\nIn the ﬁgure, you can see the OS using the ﬁrst slot of physical memory\ninto the slot starting at physical memory address 32 KB.\nTo gain some understanding of hardware-based address translation,\naddress space anywhere we’d like in physical memory, and do so while\nensuring that the process can only access its own address space.\nwhere in physical memory it should be loaded and sets the base register\nphysical address 32 KB and thus sets the base register to this value.\nwhen any memory reference is generated by the process, it is translated\nphysical address = virtual address + base\nrewrites its addresses to the desired offset in physical memory.\nFor example, if an instruction was a load from address 1000 into a reg-\nister (e.g., movl 1000, %eax), and the address space of the program\nwas loaded starting at address 3000 (and not 0, as the program thinks),\nprocess’s address space is achieved.\naddresses and thus illegally access other process’s or even OS memory; in\naddress space to another location [M65].\nEach memory reference generated by the process is a virtual address;\nthe hardware in turn adds the contents of the base register to this address\nand the result is a physical address that can be issued to the memory\nof 32 KB (32768) to get a physical address of 32896; the hardware then\nfetches the instruction from that physical address.\nthe load from virtual address 15 KB, which the processor takes and again\nadds to the base register (32 KB), getting the ﬁnal physical address of\nTransforming a virtual address into a physical address is exactly the\ntechnique we refer to as address translation; that is, the hardware takes a\nvirtual address the process thinks it is referencing and transforms it into\na physical address which is where the data actually resides.\naddress spaces even after the process has started running, the technique\nNamely, a base register is used to transform virtual addresses (gen-\nerated by the program) into physical addresses.\nthe memory reference is within bounds to make sure it is legal; in the sim-\na process generates a virtual address that is greater than the bounds, or\naddresses generated by the process are legal and within the “bounds” of\npart of the processor that helps with address translation the memory\nIn one way (as above), it holds the size of the address space,\nand thus the hardware checks the virtual address against it ﬁrst before\nend of the address space, and thus the hardware ﬁrst adds the base and\nthen makes sure the address is within bounds.\nTo understand address translation via base-and-bounds in more detail,\nImagine a process with an address space of\nsize 4 KB (yes, unrealistically small) has been loaded at physical address\n• Virtual Address 0 →Physical Address 16 KB\nThe OS must track which parts of free memory are not in use, so as to\nbe able to allocate memory to processes.\nbase address to the virtual address (which can rightly be viewed as an\noffset into the address space) to get the resulting physical address.\nbounds to implement a simple virtual memory.\nbase-and-bounds approach to virtualizing memory.\nFirst, The OS must take action when a process is created, ﬁnding space\nfor its address space in memory.\neach address space is (a) smaller than the size of physical memory and\n(often called a free list) to ﬁnd room for the new address space and then\nAn example of what physical memory might look like can be found\nphysical memory for itself, and that it has relocated the process from the\nexample above into the slot starting at physical memory address 32 KB.\ning all of its memory for use in other processes or the OS.\ntion of a process, the OS thus puts its memory back on the free list, and\na different physical address in memory.\nthe base-and-bounds pair when it switches between processes.\nvalues of the base and bounds registers to memory, in some per-process\npossible for the OS to move an address space from one location in mem-\nTo move a process’s address space, the OS\nﬁrst deschedules the process; then, the OS copies the address space from\nbase register (in the process structure) to point to the new location.\nbase-and-bounds registers; if a process, running in user mode, attempts\nWith address translation, the OS can control each and\nevery memory access from a process, ensuring the accesses stay within\nthe bounds of the address space.\ncess, turning virtual addresses (the process’s view of memory) into phys-\nbase register to the virtual address and check that the address generated\nby the process is in bounds.\nOS and hardware combine to ensure no process can generate memory\nreferences outside its own address space.",
      "keywords": [
        "Address",
        "address space",
        "Address Translation",
        "physical address",
        "MEMORY",
        "physical memory",
        "process",
        "virtual address",
        "physical memory address",
        "physical",
        "space",
        "Translation",
        "hardware",
        "memory address",
        "bounds"
      ],
      "concepts": [
        "memory",
        "address",
        "addresses",
        "hardware",
        "virtual",
        "programming",
        "program",
        "process",
        "processes",
        "base"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.912,
          "base_score": 0.762,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "address",
          "physical",
          "memory",
          "address space",
          "physical address"
        ],
        "semantic": [],
        "merged": [
          "address",
          "physical",
          "memory",
          "address space",
          "physical address"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35616074214537335,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117152+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 174-184)",
      "start_page": 174,
      "end_page": 184,
      "summary": "pages), the relocated process is using physical memory from 32 KB to\neralization of base and bounds known as segmentation, which we will\n2A different solution might instead place a ﬁxed-sized stack within the address space,\nA terriﬁc paper about how you can use compiler support to bound memory references from a program,\nvirtual addresses are within bounds?\nmum value that bounds can be set to, such that the address space\nspaces (-a) and physical memories (-p).\ning from 0 up to the maximum size of the address space.\nSegmentation\nSo far we have been putting the entire address space of each process in\nical memory when we relocate the entire address space somewhere in\nphysical memory; thus, the simple approach of using a base and bounds\nto run a program when the entire address space doesn’t ﬁt into memory;\nTHE CRUX: HOW TO SUPPORT A LARGE ADDRESS SPACE\nHow do we support a large address space with (potentially) a lot of\nine, however, a 32-bit address space (4 GB in size); a typical program will\naddress space be resident in memory.\nSegmentation: Generalized Base/Bounds\nlogical segment of the address space?\nSEGMENTATION\nFigure 16.1: An Address Space (Again)\naddress space, we have three logically-different segments: code, stack,\nof those segments in different parts of physical memory, and thus avoid\nﬁlling physical memory with unused virtual address space.\nsegment, we can place each segment independently in physical memory.\nFor example, see Figure 16.2; there you see a 64-KB physical memory\nwith those three segments within it (and 16KB reserved for the OS).\nSEGMENTATION\nFigure 16.2: Placing Segments In Physical Memory\nin physical memory, and thus large address spaces with large amounts of\nabove; each bounds register holds the size of a segment.\nSegment\nYou can see from the table that the code segment is placed at physical\naddress 32KB and has a size of 2KB and the heap segment is placed at\nLet’s do an example translation, using the address space in Figure 16.1.\nAssume a reference is made to virtual address 100 (which is in the code\nsegment).\nthe hardware will add the base value to the offset into this segment (100 in\nthis case) to arrive at the desired physical address: 100 + 32KB, or 32868.\nIt will then check that the address is within bounds (100 is less than 2KB),\nﬁnd that it is, and issue the reference to physical memory address 32868.\nSEGMENTATION\nThe term segmentation fault or violation arises from a memory access\non a segmented machine to an illegal address.\nIf we just add the virtual address 4200 to the base\nof the heap (34KB), we get a physical address of 39016, which is not the\ncorrect physical address.\nthe heap, i.e., which byte(s) in this segment the address refers to.\nthe heap starts at virtual address 4KB (4096), the offset of 4200 is actually\nregister physical address (34K or 34816) to get the desired result: 34920.\nware detects that the address is out of bounds, traps into the OS, likely\nWhich Segment Are We Referring To?\nThe hardware uses segment registers during translation.\nknow the offset into a segment, and to which segment an address refers?\nis to chop up the address space into segments based on the top few bits\nIn our example above, we have three segments; thus we need two\naddress to select the segment, our virtual address looks like this:\nSegment\nthe virtual address is in the code segment, and thus uses the code base\nand bounds pair to relocate the address to the correct physical location.\nIf the top two bits are 01, the hardware knows the address is in the heap,\nSegment\nSEGMENTATION\nwhich segment we are referring to.\nhardware simply takes the ﬁrst two bits to determine which segment reg-\nister to use, and then takes the next 12 bits as the offset into the segment.\nsimply check if the offset is less than the bounds; if not, the address is ille-\nThus, if base and bounds were arrays (with one entry per segment),\nphysical address:\nif (Offset >= Bounds[Segment])\nPhysAddr = Base[Segment] + Offset\nonly have three segments (code, heap, stack), one segment of the address\nThus, some systems put code in the same segment as\nthe heap and thus use only one bit to select which segment to use [LL82].\nmines the segment by noticing how the address was formed.\nan instruction fetch), then the address is within the code segment; if the\nsegment; any other address must be in the heap.\nThe stack has been relocated to physical address 28KB in the di-\nthe segment grows (a bit, for example, that is set to 1 when the segment\nSEGMENTATION\nSegment\nTable 16.2: Segment Registers (With Negative-Growth Support)\nLet’s take an example stack virtual address and trans-\nIn this example, assume we wish to access virtual address 15KB, which\nshould map to physical address 27KB.\nmust subtract the maximum segment size from 3KB: in this example, a\n(28KB) to arrive at the correct physical address: 27KB.\nless than the segment’s size.\ncertain memory segments between address spaces.\nBasic support adds a few bits per segment,\nAs you can see, the code segment is\nset to read and execute, and thus the same physical segment in memory\ncould be mapped into multiple virtual address spaces.\nSEGMENTATION\nSegment\nTable 16.3: Segment Register Values (with Protection)\nfew segments (i.e., code, stack, heap); we can think of this segmentation\nSupporting many segments requires even further hardware support,\nwith a segment table of some kind stored in memory.\ncode and data into separate segments which the OS and hardware would\nPieces of the address space are relocated into physical memory as the\nthe entire address space.\nus to ﬁt more address spaces into physical memory.\neach process has its own virtual address space, and the OS must make\nWhen a new address space is created, the OS has to be\nable to ﬁnd space in physical memory for its segments.\nassumed that each address space was the same size, and thus physical\nSEGMENTATION\nsegment.\ngion of memory, change their segment register values to point to the",
      "keywords": [
        "address space",
        "ADDRESS",
        "segment",
        "physical memory",
        "virtual address",
        "physical address",
        "memory",
        "space",
        "physical",
        "entire address space",
        "segments",
        "bounds",
        "virtual address space",
        "base",
        "offset"
      ],
      "concepts": [
        "segmentation",
        "segment",
        "address",
        "addresses",
        "addressed",
        "memory",
        "memories",
        "support",
        "hardware",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 20,
          "title": "",
          "score": 0.913,
          "base_score": 0.763,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 18,
          "title": "",
          "score": 0.912,
          "base_score": 0.762,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.871,
          "base_score": 0.721,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.833,
          "base_score": 0.683,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 17,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "address",
          "physical",
          "address space",
          "segmentation",
          "space"
        ],
        "semantic": [],
        "merged": [
          "address",
          "physical",
          "address space",
          "segmentation",
          "space"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3738009642439005,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117227+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 185-192)",
      "start_page": 185,
      "end_page": 192,
      "summary": "allocating memory in variable-sized chunks.\nmentation can better support sparse address spaces, by avoiding the huge\npotential waste of memory between logical segments of the address space.\nHowever, as we learned, allocating variable-sized segments in mem-\nsized, free memory gets chopped up into odd-sized pieces, and thus sat-\nisfying a memory-allocation request can be difﬁcult.\nspace.\nlogical segment, the entire heap must still reside in memory in order to be\nTry reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little\nA great survey paper on memory allocators.\n• First let’s use a tiny address space to translate some addresses.\n• Let’s say we have a tiny 16-byte address space in a 128-byte physical\nFree-Space Management\ncess’s heap) or the OS itself (managing portions of the address space of a\nSpeciﬁcally, we will discuss the issues surrounding free-space\nManaging free space can cer-\neasy when the space you are managing is divided into ﬁxed-sized units;\nWhere free-space management becomes more difﬁcult (and interest-\ning) is when the free space you are managing consists of variable-sized\nunits; this arises in a user-level memory-allocation library (as in malloc()\nand free()) and in an OS managing physical memory when using seg-\nexists is known as external fragmentation: the free space gets chopped\nisfy the request, even though the total amount of free space exceeds the\nfree\nfree\nfree space available is 20 bytes; unfortunately, it is fragmented into two\nthough there are 20 bytes free.\nFREE-SPACE MANAGEMENT\nCRUX: HOW TO MANAGE FREE SPACE\nHow should free space be managed, when satisfying variable-sized re-\nfound in user-level memory-allocation libraries.\nfree().\nThe space that this library manages is known historically as the heap,\nand the generic data structure used to manage free space in the heap is\nsome kind of free list.\nchunks of space in the managed region of memory.\ntrack free space.\nproblem of internal fragmentation; if an allocator hands out chunks of\nspace in such a chunk is considered internal fragmentation (because the\nwaste occurs inside the allocated unit) and is another example of space\nWe’ll also assume that once memory is handed out to a client, it cannot\ncalls malloc() and is given a pointer to some space within the heap,\ning call to free().\nThus, no compaction of free space is possible, which\nFREE-SPACE MANAGEMENT\nmentation; see the chapter on segmentation for details.\nFinally, we’ll assume that the allocator manages a contiguous region\nfor example, a user-level memory-allocation library might call into the\nof space.\nond, we’ll show how one can track the size of allocated regions quickly\ninside the free space to keep track of what is free and what isn’t.\nA free list contains a set of elements that describe the free space still re-\nfree\nfree\nThe free list for this heap would have two elements on it.\nscribes the ﬁrst 10-byte free segment (bytes 0-9), and one entry describes\nthe other free segment (bytes 20-29):\nbe satisﬁed easily by either of the free chunks.\nAssume we have a request for just a single byte of memory.\n2Once you hand a pointer to a chunk of memory to a C program, it is generally difﬁcult\nFREE-SPACE MANAGEMENT\na free chunk of memory that can satisfy the request and split it into two.\nthe 1-byte allocated region) and the list would end up looking like this:\nfree region is now just 93.\nwhen requests are smaller than the size of any particular free chunk.\ning of free space.\nTake our example from above once more (free 10 bytes,\nused 10 bytes, and another free 10 bytes).\nGiven this (tiny) heap, what happens when an application calls free(10),\nthus returning the space in the middle of the heap?\nfree space back into our list without too much thinking, we might end up\nNote the problem: while the entire heap is now free, it is seemingly\nbytes, a simple list traversal will not ﬁnd such a free chunk, and return\nWhat allocators do in order to avoid this problem is coalesce free space\nfree chunk in memory, look carefully at the addresses of the chunk you\nare returning as well as the nearby chunks of free space; if the newly-\nfreed space sits right next to one (or two, as in this example) existing free",
      "keywords": [
        "free space",
        "free",
        "space",
        "memory",
        "address space",
        "SEGMENTATION",
        "free chunk",
        "chunk",
        "MANAGE FREE SPACE",
        "bytes",
        "address",
        "list",
        "systems",
        "heap",
        "size"
      ],
      "concepts": [
        "segmentation",
        "segment",
        "frees",
        "management",
        "manage",
        "allocation",
        "allocators",
        "allocated",
        "byte",
        "spaces"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.913,
          "base_score": 0.763,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "",
          "score": 0.772,
          "base_score": 0.622,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.77,
          "base_score": 0.62,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 17,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "free",
          "free space",
          "space",
          "heap",
          "chunk"
        ],
        "semantic": [],
        "merged": [
          "free",
          "free space",
          "space",
          "heap",
          "chunk"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3563194005848031,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117281+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 193-203)",
      "start_page": 193,
      "end_page": 203,
      "summary": "FREE-SPACE MANAGEMENT\nFigure 17.1: An Allocated Region Plus Header\nTracking The Size Of Allocated Regions\nbeing freed and thus incorporate the space back into the free list.\nIn this example, we are examining an allocated block of size 20\nThe header minimally contains the size of the allocated region (in this\nFREE-SPACE MANAGEMENT\nthe user calls free(ptr), the library then uses simple pointer arithmetic\nlast sentence: the size of the free region is the size of the header plus the\nsize of the space allocated to the user.\nof memory, the library does not search for a free chunk of size N; rather,\nit searches for a free chunk of size N plus the size of the header.\nEmbedding A Free List\nThus far we have treated our simple free list as a conceptual entity; it is\njust a list describing the free chunks of memory in the heap.\nwe build such a list inside the free space itself?\nIn a more typical list, when allocating a new node, you would just call\nthe list inside the free space itself.\nTo manage this as a free list, we ﬁrst have to initialize said\nelement of the free list inside that space.\n// mmap() returns a pointer to a chunk of free space\nFREE-SPACE MANAGEMENT\nFigure 17.3: A Heap With One Free Chunk\nThe 100 bytes now allocated\nThe free 3980 byte chunk\nFigure 17.4: A Heap: After One Allocation\nNow, let’s imagine that a chunk of memory is requested, say of size\nas described above), and the remaining free chunk.\nThus, upon the request for 100 bytes, the library allocated 108 bytes\nout of the existing one free chunk, returns a pointer (marked ptr in the\nFREE-SPACE MANAGEMENT\n100 bytes still allocated\n100 bytes still allocated\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.5: Free Space With Three Chunks Allocated\nallocated space for later use upon free(), and shrinks the one free node\nThe free list remains\nprogram returns some memory via free()?\nFREE-SPACE MANAGEMENT\n100 bytes still allocated\n(now a free chunk of memory)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.6: Free Space With Two Chunks Allocated\nIn this example, the application returns the middle chunk of allocated\nThe library immediately ﬁgures out the size of the free region, and\nthen adds the free chunk back onto the free list.\nthe head of the free list, the space now looks like this (Figure 17.6).\nAnd now we have a list that starts with a small free chunk (100 bytes,\npointed to by the head of the list) and a large free chunk (3764 bytes).\nFREE-SPACE MANAGEMENT\n(now free)\n(now free)\n(now free)\nThe free 3764-byte chunk\nFigure 17.7: A Non-Coalesced Free List\nAnd yes, the free space\nWithout coalescing, you might end up with a free list that is highly\nAlthough all of the memory is free, it is\nFREE-SPACE MANAGEMENT\nMost traditional allocators start with a small-sized heap and then re-\ngrow the heap, and then allocate the new chunks from there.\nbasic strategies for managing free space.\nnately, because the stream of allocation and free requests can be arbitrary\nThe best ﬁt strategy is quite simple: ﬁrst, search through the free list and\nﬁnd chunks of free memory that are as big or bigger than the requested\npass through the free list is enough to ﬁnd the correct block to return.\nthe free list.\nWorst ﬁt tries to thus leave big chunks free instead of lots of\nFREE-SPACE MANAGEMENT\never, a full search of free space is required, and thus this approach can be\nspace is kept free for subsequent requests.\nfree list with a small objects.\nThus, how the allocator manages the free\ndering; by keeping the list ordered by the address of the free space, coa-\nfree space throughout the list more uniformly, thus avoiding splintering\nEnvision a free list with\nAssume an allocation request of size 15.\nfree space that can accommodate the request.\nThe resulting free list:\nFREE-SPACE MANAGEMENT\nalso ﬁnding the ﬁrst free block that can satisfy the request.\nﬁrst-ﬁt only examines free chunks until it ﬁnds one that ﬁts, thus reducing\nyou think about a little more than just best-ﬁt allocation).\nlist just to manage objects of that size; all other requests are forwarded to\nmemory dedicated for one particular size of requests, fragmentation is\nmuch less of a concern; moreover, allocation and free requests can be\nfree lists of a given size and serve memory allocation and free requests\nWhen a given cache is running low on free space, it requests\nFREE-SPACE MANAGEMENT\nThe slab allocator also goes beyond most segregated list approaches\nby keeping free objects on the lists in a pre-initialized state.\nfree space recursively divides free space by two until a block that is big\nHere is an example of a 64KB free space\nFREE-SPACE MANAGEMENT\nWhen returning the 8KB block to the free list, the allocator\nchecks whether the “buddy” 8KB is free; if so, it coalesces the two blocks\naddresses of the blocks in the free space above.\nabout memory allocators.",
      "keywords": [
        "free",
        "free list",
        "free space",
        "free chunk",
        "size",
        "list",
        "chunk",
        "space",
        "Free Chunk size",
        "header",
        "memory",
        "FREE-SPACE MANAGEMENT",
        "heap",
        "Allocated",
        "small free chunk"
      ],
      "concepts": [
        "allocated",
        "allocators",
        "allocation",
        "allocate",
        "free",
        "size",
        "list",
        "chunk",
        "header",
        "requests"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 20,
          "title": "",
          "score": 0.772,
          "base_score": 0.622,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 17,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "free",
          "free space",
          "list",
          "space",
          "free list"
        ],
        "semantic": [],
        "merged": [
          "free",
          "free space",
          "list",
          "space",
          "free list"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33542356581290916,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117367+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 204-211)",
      "start_page": 204,
      "end_page": 211,
      "summary": "we split up our address space into ﬁxed-sized units we call a page.\nsize, with 16 byte pages (real address spaces are much bigger, of course,\n(page 3)\n(page 2)\n(page 1)\n(page 0 of the address space)\npage frame 7\npage frame 6\npage frame 5\npage frame 4\npage frame 3\npage frame 2\npage frame 1\npage frame 0 of physical memory\npage 3 of AS\npage 0 of AS\npage 2 of AS\npage 1 of AS\nFigure 18.2: 64-Byte Address Space Placed In Physical Memory\nThus, we have an address space that is split into four pages (0 through\nWith paging, physical memory is also split into some number of pages\nas well; we sometimes will call each page of physical memory a page\nprocesses uses the address space; we won’t, for example, have to make\naddress space from above into our 8-page physical memory, it simply\nple above, the OS has placed virtual page 0 of the address space (AS) in\nTo record where each virtual page of the address space is placed in\nknown as a page table.\nThe major role of the page table is to store address\ntranslations for each of the virtual pages of the address space, thus letting\nabove (Figure 18.2), the page table would thus have the following entries:\nIt is important to remember that this page table is a per-process data\nTo translate this virtual address that the process generated, we have to\nﬁrst split it into two components: the virtual page number (VPN), and\nthe offset within the page.\nFor this example, because the virtual address\nspace of the process is 64 bytes, we need 6 bits total for our virtual address\nThus, our virtual address:\nThe page size is 16 bytes in a 64-byte address space; thus we need to\nbe able to select 4 pages, and the top 2 bits of the address do just that.\nThus, we have a 2-bit virtual page number (VPN).\nWhen a process generates a virtual address, the OS and hardware\namine this virtual address and see how it breaks down into a virtual page\nWith our virtual page number, we can now index\nour page table and ﬁnd which physical page that virtual page 1 resides\nIn the page table above the physical page number (PPN) (a.k.a. physical frame number or PFN) is 7 (binary 111).\nWhere Are Page Tables Stored?\nimagine a typical 32-bit address space, with 4-KB pages.\nneed 4 bytes per page table entry (PTE) to hold the physical translation\nfor each page table!\nInstead, we store the page table for each process in memory somewhere.\npage frame 7\npage frame 6\npage frame 5\npage frame 0 of physical memory\npage 3 of AS\npage 0 of AS\npage 2 of AS\npage 1 of AS\npage table:\nFigure 18.4: Example: Page Table in Kernel Physical Memory\nLet’s assume for now that the page tables live in physical memory that\nThe page table is just a\ndata structure that is used to map virtual addresses (or really, virtual page\nnumbers) to physical addresses (physical page numbers).\nthe page-table entry (PTE) at that index in order to ﬁnd the desired PFN.\nsimply marking all the unused pages in the address space invalid, we\nFigure 18.5: An x86 Page Table Entry (PTE)\nmemory and allow for the pages of processes that aren’t actively being\nthe page has been modiﬁed since it was brought into memory.\nFigure 18.5 shows an example page table entry from the x86 architec-\nwhich determines if user-mode processes can access the page; a few bits\nto use such page tables in the OS), can be challenging at ﬁrst.\nWith page tables in memory, we already know that they might be too\nTo do so, the hardware must know where the page table is for the\npage table.\n110000) which picks out the VPN bits from the full virtual address; SHIFT\nbits down to form the correct integer virtual page number.\n010000; the shift turns it into 01, or virtual page 1, as desired.\n// Form the address of the page-table entry (PTE)\nFigure 18.6: Accessing Memory With Paging",
      "keywords": [
        "page table",
        "address",
        "address space",
        "virtual address",
        "page frame",
        "virtual page",
        "virtual page number",
        "Memory",
        "physical memory",
        "virtual",
        "VPN",
        "physical",
        "pages",
        "page table entry",
        "physical page"
      ],
      "concepts": [
        "paging",
        "bits",
        "bit",
        "address",
        "addresses",
        "memory",
        "allocator",
        "allocation",
        "allocate",
        "offset"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.979,
          "base_score": 0.829,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.95,
          "base_score": 0.8,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.901,
          "base_score": 0.751,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 19,
          "title": "",
          "score": 0.871,
          "base_score": 0.721,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "virtual",
          "page table",
          "page frame",
          "frame"
        ],
        "semantic": [],
        "merged": [
          "page",
          "virtual",
          "page table",
          "page frame",
          "frame"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3898721371215025,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117426+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 212-221)",
      "start_page": 212,
      "end_page": 221,
      "summary": "In general, a page table\nstores virtual-to-physical address translations, thus letting the system\nknow where each page of an address space actually resides in physical\neral there is one page table per process in the system.\nof the page table is either determined by the hardware (older systems) or\nor store), paging requires us to perform one extra memory reference in\norder to ﬁrst fetch the translation from the page table.\nusing paging.\ntual memory address of the location of the array; this address is computed\nabout where in virtual memory the code snippet and array are found, as\nwell as the contents and location of the page table.\nFor this example, we assume a virtual address space of size 64 KB\nWe also assume a page size of 1 KB.\nAll we need to know now are the contents of the page table, and its\npage table and that it is located at physical address 1 KB (1024).\nAs for its contents, there are just a few virtual pages we need to worry\nFirst, there is the virtual page the\nBecause the page size is 1 KB, virtual address 1024 resides\non the the second page of the virtual address space (VPN=1, as VPN=0 is\nthe ﬁrst page).\nLet’s assume this virtual page maps to physical frame 4\nThe virtual pages for this decimal range is VPN=39 ...\none to the page table to ﬁnd the physical frame that the instruction resides\nthe mov instruction; this adds another page table access ﬁrst (to translate\nthe array virtual address to the correct physical one) and then the array\nPage Table (PA)\nFigure 18.7: A Virtual (And Physical) Memory Trace\nnally, the topmost graph shows page table memory accesses in light gray\n(just physical, as the page table in this example resides in physical mem-\nand ﬁve page table accesses to translate those four fetches and one explicit\nfragmentation, as paging (by design) divides memory into ﬁxed-sized\nslower machine (with many extra memory accesses to access the page\ntable) as well as memory waste (with memory ﬁlled with page tables in-\nThe Atlas pioneered the idea of dividing memory into ﬁxed-sized pages and in many senses was an early\npaging-linear-translate.py, to see if you understand how simple\nvirtual-to-physical address translation works with linear page tables.\nlinear page tables change size given different parameters.\nthe size of linear page tables as different parameters change.\nFirst, to understand how linear page table size changes as the ad-\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0\nThen, to understand how linear page table size changes as page size\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0\nHow should page-table size change as the address space grows?\nthe page size grows?\nand change the number of pages that are allocated to the address\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100\npaging-linear-translate.py -P 8\npaging-linear-translate.py -P 8k -a 32k\npaging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3\nPaging: Faster Translations (TLBs)\nUsing paging as the core mechanism to support virtual memory can lead\nphysical memory, paging logically requires an extra memory lookup for\nextra memory reference that paging seems to require?\nhardware cache of popular virtual-to-physical address translations; thus,\nmemory reference, the hardware ﬁrst checks the TLB to see if the desired\nwithout having to consult the page table (which has all translations).\nvirtual address translation, assuming a simple linear page table (i.e., the\npage table is an array) and a hardware-managed TLB (i.e., the hardware\nhandles much of the responsibility of page table accesses; we’ll explain\nPAGING: FASTER TRANSLATIONS (TLBS)\nvirtual page number (VPN) from the virtual address (Line 1 in Figure 19.1),\nand form the desired physical address (PA), and access memory (Lines\npage table to ﬁnd the translation (Lines 11–12), and, assuming that the\nneeded to access the page table (Line 12).\nincurred; the page table must be accessed to ﬁnd the translation, and an\nextra memory reference (or more, with more complex page tables) results.\nTLB misses lead to more memory accesses.\nPAGING: FASTER TRANSLATIONS (TLBS)\nexample, let’s assume we have an array of 10 4-byte integers in memory,\nvirtual address space, with 16-byte pages; thus, a virtual address breaks\ndown into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there\nare 16 bytes on each of those pages).\nFigure 19.2 shows the array laid out on the 16 16-byte pages of the sys-\nonto the next page (VPN=07), where the next four entries (a[3] ...\nare located on the next page of the address space (VPN=08).",
      "keywords": [
        "PAGE TABLE",
        "VPN",
        "memory",
        "TLB",
        "virtual address",
        "address",
        "virtual",
        "array",
        "PAGING",
        "memory reference",
        "memory accesses",
        "address space",
        "virtual memory",
        "pages",
        "page table memory"
      ],
      "concepts": [
        "paging",
        "pages",
        "instruction",
        "instructions",
        "array",
        "addresses",
        "translations",
        "translation",
        "translate",
        "lines"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.979,
          "base_score": 0.829,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.902,
          "base_score": 0.752,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.828,
          "base_score": 0.678,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 18,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "page table",
          "paging",
          "linear translate",
          "paging linear"
        ],
        "semantic": [],
        "merged": [
          "page",
          "page table",
          "paging",
          "linear translate",
          "paging linear"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3497144774822185,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117478+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 222-230)",
      "start_page": 222,
      "end_page": 230,
      "summary": "the TLB for a valid translation.\ngram accesses the array, the result will be a TLB miss.\nThe next access is to a[1], and there is some good news here: a TLB\ninto the TLB.\nother TLB miss.\nwill hit in the TLB, as they all reside on the same page in memory.\nFinally, access to a[7] causes one last TLB miss.\nin physical memory, and updates the TLB accordingly.\nhardware looks in the TLB for their translations, two more hits result.\nLet us summarize TLB activity during our ten accesses to the array:\nThus, our TLB hit rate,\nis the ﬁrst time the program accesses the array, TLB performance gains\nﬁrst access to an element on a page yields a TLB miss.\ntypes of dense, array-based accesses achieve excellent TLB performance,\nOne last point about TLB performance: if the program, soon after this\nter result, assuming that we have a big enough TLB to cache the needed\nTLB hit rate would be high because of temporal locality, i.e., the quick\ndo), the TLB hit rate will likely be high.\nHardware caches, whether for instructions, data, or address translations\n(as in our TLB) take advantage of locality by keeping copies of memory in\nYou might be wondering: if caches (like the TLB) are so great, why don’t\nWho Handles The TLB Miss?\nOne question that we must answer: who handles a TLB miss?\nwould handle the TLB miss entirely.\nrect page-table entry and extract the desired translation, update the TLB\n(Success, TlbEntry) = TLB_Lookup(VPN)\n// TLB Hit\n// TLB Miss\nRaiseException(TLB_MISS)\nOn a TLB miss, the hardware sim-\nthe OS that is written with the express purpose of handling TLB misses.\nWhen run, the code will lookup the translation in the page table, use spe-\ncial “privileged” instructions to update the TLB, and return from the trap;\nat this point, the hardware retries the instruction (resulting in a TLB hit).\nreturning from a TLB miss-handling trap, the hardware must resume ex-\nstruction run again, this time resulting in a TLB hit.\nSecond, when running the TLB miss-handling code, the OS needs to be\nsolutions exist; for example, you could keep TLB miss handlers in physi-\nlation), or reserve some entries in the TLB for permanently-valid transla-\ncode itself; these wired translations always hit in the TLB.\nmuch on a miss; it raises an exception, and the OS TLB miss handler does\nLet’s look at the contents of the hardware TLB in more detail.\nTLB might have 32, 64, or 128 entries and be what is called fully associa-\nin the TLB, and that the hardware will search the entire TLB in parallel to\nA typical TLB entry might look like this:\nlation could end up in any of these locations (in hardware terms, the TLB\nASIDE: TLB VALID BIT ̸= PAGE TABLE VALID BIT\nA common mistake is to confuse the valid bits found in a TLB with\nThe usual response when an invalid page is accessed is to trap to the OS,\nA TLB valid bit, in contrast, simply refers to whether a TLB entry has a\ninitial state for each TLB entry is to be set to invalid, because no address\nthe TLB is slowly populated, and thus valid entries soon ﬁll the TLB.\nThe TLB valid bit is quite useful when performing a context switch too,\nBy setting all TLB entries to invalid, the\nFor example, the TLB commonly\nbe accessed (as in the page table).\nTLB Issue: Context Switches\nSpeciﬁcally, the TLB contains virtual-to-physical\nprocess (P1) is running, it assumes the TLB might be caching translations\nboth processes were in the TLB, the contents of the TLB would be:\nIn the TLB above, we clearly have a problem: VPN 10 translates to\nHOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH\nWhen context-switching between processes, the translations in the TLB\nproach is to simply ﬂush the TLB on context switches, thus emptying\ntion; with a hardware-managed TLB, the ﬂush could be enacted when the\nsets all valid bits to 0, essentially clearing the contents of the TLB.\nBy ﬂushing the TLB on each context switch, we now have a working\nlations in the TLB.\nmust incur TLB misses as it touches its data and code pages.\nTLB.\nIf we take our example TLB from above and add ASIDs, it is clear\nprocesses can readily share the TLB: only the ASID ﬁeld is needed to dif-\nHere is a depiction of a TLB\nThus, with address-space identiﬁers, the TLB can hold translations\nentries of the TLB are remarkably similar.\nAs with any cache, and thus also with the TLB, one more issue that we\na new entry in the TLB, we have to replace an old one, and thus the\nTHE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY\nWhich TLB entry should be replaced when we add a new TLB entry?\ncause pessimal behavior, e.g., think of a loop accessing n+1 pages, a TLB\nFigure 19.4: A MIPS TLB Entry\nA Real TLB Entry\nbits of this TLB entry can be seen in Figure 19.4.\nThe MIPS R4000 supports a 32-bit address space with 4KB pages.\nHowever, as you can see in the TLB, there are only 19 bits for the\nThere are a few other interesting bits in the MIPS TLB.\nbit (G), which is used for pages that are globally-shared among processes.\nCoherence (C) bits, which determine how a page is cached by the hardware\nthe page has been written to (we’ll see the use of this later); a valid bit\nslots of the TLB to reserve for the OS; the OS uses these reserved map-\nBecause the MIPS TLB is software managed, there needs to be instruc-\nwhich probes the TLB to see if a particular translation is in there; TLBR,\nwhich reads the contents of a TLB entry into registers; TLBWI, which re-\nplaces a speciﬁc TLB entry; and TLBWR, which replaces a random TLB\nThe OS uses these instructions to manage the TLB’s contents.\nuser process could do if it could modify the contents of the TLB (hint: just\nTLB, accessing a particular page of memory may be costly, particularly if\nthat page isn’t currently mapped by your TLB.\nrandomly accessing your address space, particular if the number of pages\naccessed exceeds the TLB coverage, can lead to severe performance penal-\nthe TLB as the source of many performance problems, we name this law\nBy providing a small, dedicated on-chip TLB as an address-translation\nto access the page table in main memory.\nIn particular, if the number of pages a program accesses in a short\nperiod of time exceeds the number of pages that ﬁt into the TLB, the pro-\ngram will generate a large number of TLB misses, and thus run quite a\nTLB can be increased.\nOne other TLB issue worth mentioning: TLB access can easily be-",
      "keywords": [
        "TLB",
        "TLB miss",
        "TLB entry",
        "TLB hit",
        "TLB VALID BIT",
        "TLBS",
        "TLB hit rate",
        "MIPS TLB",
        "TLB VALID",
        "FASTER TRANSLATIONS",
        "MIPS TLB Entry",
        "hit",
        "TLB miss handler",
        "hardware",
        "page table"
      ],
      "concepts": [
        "paging",
        "pages",
        "translations",
        "translation",
        "translates",
        "accessed",
        "accesses",
        "access",
        "hardware",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 27,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 25,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tlb",
          "tlb miss",
          "miss",
          "tlb entry",
          "hit"
        ],
        "semantic": [],
        "merged": [
          "tlb",
          "tlb miss",
          "miss",
          "tlb entry",
          "hit"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38105382515248265,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117540+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 231-249)",
      "start_page": 231,
      "end_page": 249,
      "summary": "that touches 4 or fewer pages, each access should be a TLB hit, and thus\nNumber Of Pages\nof accessing each page.\nPaging: Smaller Tables\nWe now tackle the second problem that paging introduces: page tables\na linear page table.\nAs you might recall1, linear page tables get pretty\npages and a 4-byte page-table entry.\n212 ); multiply by the page-table size and\nyou see that our page table is 4MB in size.\none page table for every process in the system!\ndreds of megabytes of memory just for page tables!\nCRUX: HOW TO MAKE PAGE TABLES SMALLER?\nSimple array-based page tables (usually called linear page tables) are\nmake page tables smaller?\nWe could reduce the size of the page table in one simple way: use\nour linear page table and thus a total size of 1MB per page table, a factor\nproblem should be clear: simple linear (array-based) page tables are too big.\nPAGING: SMALLER TABLES\nfor multiple page sizes is not to save page table space, however; it is to\nof four reduction in size of the page table (not surprisingly, the reduction\nHybrid Approach: Paging and Segments\nsegmentation in order to reduce the memory overhead of page tables.\n16KB address space with 1KB pages (Figure 20.1); the page table for this\nPAGING: SMALLER TABLES\nFigure 20.1: A 16-KB Address Space With 1-KB Pages\nThis example assumes the single code page (VPN 0) is mapped to\nthe two stack pages at the other end of the address space (VPNs 14 and\nfrom the picture, most of the page table is unused, full of invalid entries.\npage table of a 32-bit address space and all the potential wasted space in\nTable 20.1: A Page Table For 16-KB Address Space\nPAGING: SMALLER TABLES\nThus, our hybrid approach: instead of having a single page table for\nIn this example, we might thus have three page tables, one for the\nsegment itself but rather to hold the physical address of the page table of that\nThe bounds register is used to indicate the end of the page table\n(i.e., how many valid pages it has).\nspace with 4KB pages, and an address space split into four segments.\near page table for that segment; thus, each process in the system now has\nthree page tables associated with it.\nmust be changed to reﬂect the location of the page tables of the newly-\nthe VPN as follows to form the address of the page table entry (PTE):\nsaw before with linear page tables.\nuse of one of three segment base registers instead of the single page table\nusing its ﬁrst three pages (0, 1, and 2), the code segment page table will\nPAGING: SMALLER TABLES\nthe linear page table; unallocated pages between the stack and the heap\nno longer take up space in a page table (just to mark them as not valid).\nheap, for example, we can still end up with a lot of page table waste.\nmost of memory is managed in page-sized units, page tables now can be\nlook for better approaches to implementing smaller page tables.\nMulti-level Page Tables\nproblem: how to get rid of all those invalid regions in the page table in-\npage table, as it turns the linear page table into something like a tree.\nThe basic idea behind a multi-level page table is simple.\nthe page table into page-sized units; then, if an entire page of page-table\nentries (PTEs) is invalid, don’t allocate that page of the page table at all.\nTo track whether a page of the page table is valid (and if valid, where it\nis in memory), use a new structure, called the page directory.\nThe page\ntable is, or that the entire page of the page table contains no valid pages.\nlinear page table; even though most of the middle regions of the address\nspace are not valid, we still have to have page-table space allocated for\nthose regions (i.e., the middle two pages of the page table).\nis a multi-level page table.\nPAGING: SMALLER TABLES\nLinear Page Table\nMulti-level Page Table\nThe Page Directory\nFigure 20.2: Linear (Left) And Multi-Level (Right) Page Tables\nthe page table as valid (the ﬁrst and last); thus, just those two pages of the\npage table reside in memory.\nwhat a multi-level table is doing: it just makes parts of the linear page\npages of the page table are allocated with the page directory.\nThe page directory, in a simple two-level table, contains one entry per\npage of the page table.\nat least one of the pages of the page table that the entry points to (via the\nPFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE,\nMulti-level page tables have some obvious advantages over approaches\nble only allocates page-table space in proportion to the amount of address\nSecond, if carefully constructed, each portion of the page table ﬁts\nContrast this to a simple (non-paged) linear page table2, which\ntire linear page table must reside contiguously in physical memory.\na large page table (say 4MB), ﬁnding such a large chunk of unused con-\n2We are making some assumptions here, i.e., that all page tables reside in their entirety in\nPAGING: SMALLER TABLES\nstructure, we add a level of indirection through use of the page directory,\nwhich points to pieces of the page table; that indirection allows us to place\npage-table pages wherever we would like in physical memory.\ninformation from the page table (one for the page directory, and one for\nthe PTE itself), in contrast to just one load with a linear page table.\nthe multi-level table is a small example of a time-space trade-off.\nOS handling the page-table lookup (on a TLB miss), doing so is undoubt-\nedly more involved than a simple linear page-table lookup.\noverheads; in the case of a multi-level table, we make page-table lookups\nTo understand the idea behind multi-level page tables better, let’s do an\nImagine a small address space of size 16 KB, with 64-byte pages.\nA linear page table would have 28 (256) entries, even\nFigure 20.3: A 16-KB Address Space With 64-byte Pages\nPAGING: SMALLER TABLES\npages of the address space are unused.\nTo build a two-level page table for this address space, we start with\nour full linear page table and break it up into page-sized units.\nThus, our page table is 1KB (256 × 4 bytes) in size.\nhave 64-byte pages, the 1-KB page table can be divided into 16 64-byte\nindex ﬁrst into the page directory and then into the page of the page table.\nLet’s ﬁrst index into the page directory.\nOur page table in this example\nThe page directory needs one\nentry per page of the page table; thus, it has 16 entries.\nPage Directory Index\nthe VPN, we can use it to ﬁnd the address of the page-directory entry\nPAGING: SMALLER TABLES\ntable entry (PTE) from the page of the page table pointed to by this page-\npage table using the remaining bits of the VPN:\nPage Directory Index\nPage Table Index\nThis page-table index (PTIndex for short) can then be used to index\ninto the page table itself, giving us the address of our PTE:\nbegin with the page directory for this example (left side of Table 20.2).\nIn the ﬁgure, you can see that each page directory entry (PDE) de-\nscribes something about a page of the page table for the address space.\npage table), we have the ﬁrst page of 16 page table entries for the ﬁrst 16\nof this portion of the page table.\nPage Directory\nTable 20.2: A Page Directory, And Pieces Of Page Table\nPAGING: SMALLER TABLES\nThis page of the page table contains the mappings for the ﬁrst 16\nof those pages.\nThe other valid page of page table is found inside PFN 101.\nThis page\nof allocating the full sixteen pages for a linear page table, we allocate only\nthree: one for the page directory, and two for the chunks of the page table\npage directory.\nof the page table located at address 101.\nof the VPN (1110) to index into that page of the page table and ﬁnd\ntells us that page 254 of our virtual address space is mapped at physi-\nYou should now have some idea of how to construct a two-level page\ntable, using a page directory which points to pages of the page table.\ntimes two levels of page table is not enough!\nIn our example thus far, we’ve assumed that multi-level page tables only\nhave two levels: a page directory and then pieces of the page table.\naddress space, and a small (512 byte) page.\nRemember our goal in constructing a multi-level page table: to make\neach piece of the page table ﬁt within a single page.\nconsidered the page table itself; however, what if the page directory gets\nPAGING: SMALLER TABLES\nmake all pieces of the page table ﬁt within a page, we start by determining\nhow many page-table entries ﬁt within a page.\nGiven our page size of 512\nWhen we index into a page of the page table,\nPage Directory Index\nPage Table Index\nIf our page directory has 214\npiece of the multi-level page table ﬁt into a page vanishes.\nPage Table Index\nNow, when indexing the upper-level page directory, we use the very\nIf valid, the second level of the page directory is consulted by\ncan be formed by using the page-table index combined with the address\npage table, we once again present the control ﬂow in algorithmic form\nlevel page table access occurs, the hardware ﬁrst checks the TLB; upon\nPAGING: SMALLER TABLES\n// first, get page directory entry\n// PDE is valid: now fetch PTE from page table\nFigure 20.4: Multi-level Page Table Control Flow\na hit, the physical address is formed directly without accessing the page\nour traditional two-level page table: two additional memory accesses to\nInverted Page Tables\nAn even more extreme space savings in the world of page tables is\nfound with inverted page tables.\ntables (one per process of the system), we keep a single page table that\nhas an entry for each physical page of the system.\nthis physical page.\nMore generally, inverted page tables illustrate what we’ve said from\nthe beginning: page tables are just data structures.\nMulti-level and inverted page tables are just two\nPAGING: SMALLER TABLES\nSwapping the Page Tables to Disk\nwe have assumed that page tables reside in kernel-owned physical mem-\nEven with our many tricks to reduce the size of page tables, it is still\nThus, some systems place such page tables in kernel virtual memory,\nthereby allowing the system to swap some of these page tables to disk\nWe have now seen how real page tables are built; not necessarily just",
      "keywords": [
        "page table",
        "linear page table",
        "page directory",
        "multi-level page table",
        "Page Table Index",
        "pages",
        "PAGE TABLES SMALLER",
        "address space",
        "Tables",
        "Smaller Tables",
        "linear page",
        "TLB",
        "two-level page table",
        "Inverted Page Tables",
        "Page Directory Index"
      ],
      "concepts": [
        "paging",
        "pages",
        "tables",
        "space",
        "systems",
        "memory",
        "entries",
        "entry",
        "size",
        "make"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.767,
          "base_score": 0.617,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "page table",
          "table",
          "tables",
          "page directory"
        ],
        "semantic": [],
        "merged": [
          "page",
          "page table",
          "table",
          "tables",
          "page directory"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2603507588249353,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117589+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 250-260)",
      "start_page": 250,
      "end_page": 260,
      "summary": "PAGING: SMALLER TABLES\nWe have yet to ﬁnd a good ﬁrst reference to the multi-level page table.\nPAGING: SMALLER TABLES\npage table works.\n• With a linear page table, you need a single register to locate the\npage table, assuming that hardware does the lookup upon a TLB\nHow many registers do you need to locate a two-level page\nyou think memory references to the page table will behave in the\naddress space of every running process ﬁts into memory.\nThus far, we have assumed that all pages reside in physical memory.\nThus, in our memory\nTHE CRUX: HOW TO GO BEYOND PHYSICAL MEMORY\nthan physical memory itself.\nthe ability to swap out some pages, as early machines clearly could not\nhold all the pages needed by all processes at once.\nto such space as swap space, because we swap pages out of memory to it\nand swap pages into memory from it.\nthe OS can read from and write to the swap space, in page-sized units.\ndo so, the OS will need to remember the disk address of a given page.\nthe maximum number of memory pages that can be in use by a system at\npage physical memory and an 8-page swap space.\nmemory, with the rest located in swap space on disk.\n(Proc 3) has all of its pages swapped out to disk, and thus clearly isn’t\nThe code pages from this\nMemory\nFigure 21.1: Physical Memory and Swap Space\nor, as in modern systems, one page at a time when needed).\ncan safely re-use the memory space for these code pages, knowing that it\nchinery higher up in the system in order to support swapping pages to\nlocates the page table in memory (using the page table base register)\nand looks up the page table entry (PTE) for this page using the VPN\nIf the page is valid and present in physical memory, the\nIf we wish to allow pages to be swapped to disk, however, we must\nPTE, it may ﬁnd that the page is not present in physical memory.\npage is present in physical memory and everything proceeds as above; if\nit is set to zero, the page is not in memory but rather on disk somewhere.\nThe act of accessing a page that is not in physical memory is commonly\nreferred to as a page fault.\nFor example, a page fault\nhere, i.e., a page-not-present fault, but sometimes can refer to illegal mem-\n(to a page mapped into the virtual address space of a process, but simply\ncalled a page miss.\nIn this case, a page that a process wants\nto access is missing from memory; the hardware does the only thing it\nUpon a page fault, the OS is invoked to service the page fault.\nular piece of code, known as a page-fault handler, runs, and must service\nthe page fault, as we now describe.\nThe Page Fault\nmanaged TLBs (where the hardware looks in the page table to ﬁnd the\neither type of system, if a page is not present, the OS is put in charge to\nhandle the page fault.\nThe appropriately-named OS page-fault handler\nVirtually all systems handle page faults in\nIf a page is not present and has been swapped to disk, the OS will need\nto swap the page into memory in order to service the page fault.\nquestion arises: how will the OS know where to ﬁnd the desired page?\nmany systems, the page table is a natural place to store such information.\nthe PFN of the page for a disk address.\nWhen the OS receives a page fault\nfor a page, it looks in the PTE to ﬁnd the address, and issues the request\nto disk to fetch the page into memory.\nASIDE: WHY HARDWARE DOESN’T HANDLE PAGE FAULTS\nOS to handle a page fault?\nFirst, page\nto be able to handle a page fault, the hardware would have to understand\nswap space, how to issue I/Os to the disk, and a lot of other details which\nmance and simplicity, the OS handles page faults, and even hardware\nWhen the disk I/O completes, the OS will then update the page table\nto mark the page as present, update the PFN ﬁeld of the page-table entry\n(PTE) to record the in-memory location of the newly-fetched page, and\ncould alternately update the TLB upon when servicing the page fault,\nmemory at the translated physical address.\npage fault is being serviced.\nthe I/O (page fault) of one process and the execution of another is yet\nWhat If Memory Is Full?\nis plenty of free memory in which to page in a page from swap space.\nThus, the OS might like to ﬁrst page out one or more pages to make room\nfor the new page(s) the OS is about to bring in.\ngram to run at disk-like speeds instead of memory-like speeds; in cur-\nRaiseException(PAGE_FAULT)\nFigure 21.2: Page-Fault Control Flow Algorithm (Hardware)\nPage Fault Control Flow\nsecond what the OS does upon a page fault.\nFirst, that the page was both present and valid (Lines 18–21); in\npage fault handler must be run; although this was a legitimate page for\nmemory.\nThird (and ﬁnally), the access could be to an invalid page, due\nroughly must do in order to service the page fault.\na physical frame for the soon-to-be-faulted-in page to reside within; if\nrun and kick some pages out of memory, thus freeing them for use here.\n// no free page found\n// update page table with present\nFigure 21.3: Page-Fault Control Flow Algorithm (Software)\nto read in the page from swap space.\ncompletes, the OS updates the page table and retries the instruction.\nTo keep a small amount of memory free, most operating systems thus\nhelp decide when to start evicting pages from memory.\nas follows: when the OS notices that there are fewer than LW pages avail-\nor group a number of pages and write them out at once to the swap parti-\nTo work with the background paging thread, the control ﬂow in Figure\npages available.\nmemory than is physically present within a system.\nmore complexity in page-table structures, as a present bit (of some kind)\nmust be included to tell us whether the page is present in memory or not.\nWhen not, the operating system page-fault handler runs to service the\npage fault, and thus arranges for the transfer of the desired page from\ndisk to memory, perhaps ﬁrst replacing some pages in memory to make",
      "keywords": [
        "Memory",
        "Physical Memory",
        "page fault",
        "page table",
        "TLB",
        "pages",
        "Physical",
        "fault",
        "page physical memory",
        "Virtual Memory",
        "VPN",
        "swap space",
        "space",
        "Systems",
        "disk"
      ],
      "concepts": [
        "paging",
        "pages",
        "memory",
        "hardware",
        "disk",
        "runs",
        "run",
        "swap",
        "swapped",
        "operating"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.95,
          "base_score": 0.8,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.902,
          "base_score": 0.752,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 28,
          "title": "",
          "score": 0.886,
          "base_score": 0.736,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 27,
          "title": "",
          "score": 0.873,
          "base_score": 0.723,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "fault",
          "page fault",
          "memory",
          "swap"
        ],
        "semantic": [],
        "merged": [
          "page",
          "fault",
          "page fault",
          "memory",
          "swap"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3981431751399402,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117649+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 261-275)",
      "start_page": 261,
      "end_page": 275,
      "summary": "and assign it to the faulting page.\nIn such a case, this memory pressure forces the OS to start paging\nDeciding which page\n(or pages) to evict is encapsulated within the replacement policy of the\nTHE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT\nHow can the OS decide which page (or pages) to evict from memory?\nall the pages in the system, it can rightly be viewed as a cache for virtual\nmemory pages in the system.\npolicy for this cache is to minimize the number of cache misses; that is,\ndesired page.\nber of cache hits, i.e., the number of times a page that is read or written\nKnowing the number of cache hits and misses let us calculate the av-\n4KB, with 256-byte pages.\npages.\nof each of the ﬁrst ten pages of the address space (the page number being\nLet us further assume that every page except virtual page 3 are already\ncompute the hit rate (the percent of references found in memory): 90%,\nThe Optimal Replacement Policy\nThe optimal replacement policy\nthe page that will be accessed furthest in the future is the optimal policy,\nabout it like this: if you have to throw out some page, why not throw\nessentially saying that all the other pages in the cache are more important\nto the other pages before you refer to the one furthest out.\nof virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1.\nof optimal, assuming a cache that ﬁts three pages.\nﬁrst three accesses are misses, as the cache begins in an empty state; such\nThen we refer again to pages 0 and 1, which both hit in the cache.\nwe reach another miss (to page 3), but this time the cache is full; a re-\nWhich begs the question: which page should\nWith the optimal policy, we examine the future for each page\nThus the optimal policy has an easy choice: evict page 2, resulting in\npages 0, 1, and 3 in the cache.\nassociativity; it does not arise in the OS page cache because such caches\nmemory a page can be placed.\nwe get to page 2, which we evicted long ago, and suffer another miss.\nHere the optimal policy again examines the future for each page in the\ncache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which\nThe example shows page 3 getting\npage 1 and the trace completes.\nWe can also calculate the hit rate for the cache: with 6 hits and 5 misses,\npage), resulting in a 85.7% hit rate.\nway to decide which page to evict.\nA Simple Policy: FIFO\noptimal and employed very simple replacement policies.\nsome systems used FIFO (ﬁrst-in, ﬁrst-out) replacement, where pages\nWe again begin our trace with three compulsory misses to pages 0,\nNext, page 3 is referenced, causing\na miss; the replacement decision is easy with FIFO: pick the page that\nTable 22.2: Tracing the FIFO Policy\nwith the ﬁrst-in page on the left), which is page 0.\naccess is to page 0, causing another miss and replacement (of page 1).\nthen hit on page 3, but miss on 1 and 2, and ﬁnally hit on 3.\nmine the importance of blocks: even though page 0 had been accessed\nBelady (of the optimal policy) and colleagues found an interesting refer-\nrate changed when moving from a cache size of 3 to 4 pages.\nTable 22.3: Tracing the Random Policy\nAnother Simple Policy: Random\nAnother similar replacement policy is Random, which simply picks a\nrandom page to replace under memory pressure.\nTable 22.4: Tracing the LRU Policy\nUnfortunately, any policy as simple as FIFO or Random is likely to\nhave a common problem: it might kick out an important page, one that\nFIFO kicks out the page that was ﬁrst\npaged back in.\nThus, FIFO, Random, and similar policies are not likely to\nif a program has accessed a page in the near past, it is likely to access it\nOne type of historical information a page-replacement policy could\nuse is frequency; if a page has been accessed many times, perhaps it\nused property of a page is its recency of access; the more recently a page\nthus try to use history to ﬁgure out which pages are important, and keep\nthose pages in memory when it comes to eviction time.\nused page when an eviction must take place.\nUsed (LRU) policy replaces the least-recently-used page.\nis known as spatial locality, which states that if a page P is accessed,\nit is likely the pages around it (say P −1 or P + 1) will also likely be\nThe second is temporal locality, which states that pages that\ncan see how LRU can use history to do better than stateless policies such\nIn the example, LRU evicts page 2 when it ﬁrst has\nto replace a page, because 0 and 1 have been accessed more recently.\nthen replaces page 0 because 1 and 3 have been accessed more recently.\nis to a random page within the set of accessed pages.\nample, the workload accesses 100 unique pages over time, choosing the\nnext page to refer to at random; overall, 10,000 pages are accessed.\nexperiment, we vary the cache size from very small (1 page) to enough\nto hold all the unique pages (100 page), in order to see how each policy\npolicy you are using; LRU, FIFO, and Random all perform the same, with\nwhich policy you use; all policies (even optimal) converge to a 100% hit\nexhibits locality: 80% of the references are made to 20% of the pages (the\n“hot” pages); the remaining 20% of the references are made to the re-\na total 100 unique pages again; thus, “hot” pages are referred to most of\nthe time, and “cold” pages the remainder.\npages; as those pages have been referred to frequently in the past, they\ntial” workload, as in it, we refer to 50 pages in sequence, starting at 0,\nthen 1, ..., up to page 49, and then we loop, repeating those accesses, for a\ntotal of 10,000 accesses to 50 unique pages.\nshows the behavior of the policies under this workload.\nworkload, kick out older pages; unfortunately, due to the looping nature\nof the workload, these older pages are going to be accessed sooner than\nthe pages that the policies prefer to keep in cache.\na cache of size 49, a looping-sequential workload of 50 pages results in\nimportant pages.\nSpeciﬁcally, upon each page access (i.e., each memory\nsome data structure to move this page to the front of the list (i.e., the\nContrast this to FIFO, where the FIFO list of pages is only\naccessed when a page is evicted (by removing the ﬁrst-in page) or when\na new page is added to the list (to the last-in side).\npages have been least- and most-recently used, the system has to do some\nFor example, a machine could update, on each page access,\na time ﬁeld in memory (for example, this could be in the per-process page\nical page of the system).\nThus, when a page is accessed, the time ﬁeld\npage, the OS could simply scan all the time ﬁelds in the system to ﬁnd the\nleast-recently-used page.\nUnfortunately, as the number of pages in a system grows, scanning a\nhuge array of times just to ﬁnd the absolute least-recently-used page is\nThis machine has 1 million pages, and thus\nﬁnding the LRU page will take a long time, even at modern CPU speeds.\npage to replace?\nCRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY\nwhich was implemented in the ﬁrst system with paging, the Atlas one-\nThere is one use bit per page of the system, and the\nuse bits live in memory somewhere (they could be in the per-process page\nWhenever a page is\nImagine all the pages of the system arranged in\nA clock hand points to some particular page to begin with\nchecks if the currently-pointed to page P has a use bit of 1 or 0.\nimplies that page P was recently used and thus is not a good candidate\nThus, the clock hand is incremented to the next page\nuntil it ﬁnds a use bit that is set to 0, implying this page has not been\nrecently used (or, in the worst case, that all pages have been and that we\nhave now searched through the entire set of pages, clearing all the bits).\nuse bits and then differentiates between which pages have use bits of 1\nlooking for an unused page.\nvariant randomly scans pages when doing a replacement; when it en-\ncounters a page with a reference bit set to 1, it clears the bit (i.e., sets it\nto 0); when it ﬁnds a page with the reference bit set to 0, it chooses it as\nConsidering Dirty Pages\nsideration of whether a page has been modiﬁed or not while in memory.\nThe reason for this: if a page has been modiﬁed and is thus dirty, it must\nVM systems prefer to evict clean pages over dirty pages.\nThis bit is set any time a page is written, and thus can be\nincorporated into the page-replacement algorithm.\nfor example, could be changed to scan for pages that are both unused\nand clean to evict ﬁrst; failing to ﬁnd those, then for unused pages that",
      "keywords": [
        "LRU",
        "pages",
        "Hit",
        "hit rate",
        "PHYSICAL MEMORY",
        "MEMORY",
        "FIFO",
        "Cache",
        "Miss",
        "policy",
        "Random",
        "Policies",
        "optimal policy",
        "Optimal",
        "rate"
      ],
      "concepts": [
        "paging",
        "pages",
        "paged",
        "policies",
        "memory",
        "misses",
        "miss",
        "cache",
        "caching",
        "random"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 28,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.873,
          "base_score": 0.723,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.834,
          "base_score": 0.684,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 29,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "policy",
          "optimal",
          "cache",
          "fifo"
        ],
        "semantic": [],
        "merged": [
          "page",
          "policy",
          "optimal",
          "cache",
          "fifo"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37139861197167756,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117707+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 276-284)",
      "start_page": 276,
      "end_page": 284,
      "summary": "BEYOND PHYSICAL MEMORY: POLICIES\nPage replacement is not the only policy the VM subsystem employs\ndecide when to bring a page into memory.\nthe page selection policy (as it was called by Denning [D70]), presents\nFor most pages, the OS simply uses demand paging, which means the\nOS brings the page into memory when it is accessed, “on demand” as\nOf course, the OS could guess that a page is about to be used,\nexample, some systems will assume that if a code page P is brought into\nmemory, that code page P +1 will likely soon be accessed and thus should\nAnother policy determines how the OS writes pages out to disk.\nsystems instead collect a number of pending writes together in memory\nset of running processes simply exceeds the available physical memory?\nIn this case, the system will constantly be paging, a condition sometimes\nsets (the pages that they are using actively) ﬁt in memory and thus can\nSome current systems take more a draconian approach to memory\nFor example, some versions of Linux run an out-of-memory\nBEYOND PHYSICAL MEMORY: POLICIES\nWe have seen the introduction of a number of page-replacement (and\nthe evolution of page-replacement algorithms continues.\npaging is prohibitive.\nBEYOND PHYSICAL MEMORY: POLICIES\n[BNS69] “An Anomaly in Space-time Characteristics of Certain Programs Running in a Paging\n[C69] “A Paging Experiment with the Multics System”\nDenning’s early and famous survey on virtual memory systems.\nthree components intuitively based on the cause of the misses (page 49).”\nBEYOND PHYSICAL MEMORY: POLICIES\nAlthough Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the\nuse bits in large memories was not a problem the authors solved.\nBEYOND PHYSICAL MEMORY: POLICIES\nThis simulator, paging-policy.py, allows you to play around with\ndifferent page-replacement policies.\n• For a cache of size 5, generate worst-case address reference streams\ngenerate a virtual page reference stream.\nreference into a virtual page-number reference (done by masking\nThe VAX/VMS Virtual Memory System\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nMemory Management Hardware\nThe VAX-11 provided a 32-bit virtual address space per process, di-\nIn the ﬁrst half of process space (known as P0),\nof pages in the VAX hardware (512 bytes).\nreasons, has the fundamental problem of making simple linear page ta-\nwas to make sure that VMS would not overwhelm memory with page\nThe system reduced the pressure page tables place on memory in two\nFirst, by segmenting the user address space into two, the VAX-11\nprovides a page table for each of these regions (P0 and P1) per process;\nthus, no page-table space is needed for the unused portion of the address\npage table for that segment, and the bounds holds its size (i.e., number of\npage-table entries).\nSecond, the OS reduces memory pressure even further by placing user\npage tables (for P0 and P1, thus two per process) in kernel virtual mem-\nThus, when allocating or growing a page table, the kernel allocates\nspace out of its own virtual memory, in segment S.\nder severe pressure, the kernel can swap pages of these page tables out to\ndisk, thus making physical memory available for other uses.\nPutting page tables in kernel virtual memory means that address trans-\ndress in P0 or P1, the hardware has to ﬁrst try to look up the page-table\nentry for that page in its page table (the P0 or P1 page table for that pro-\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nPage 0: Invalid\nFigure 23.1: The VAX/VMS Address Space\nsystem page table (which lives in physical memory); with that transla-\ntion complete, the hardware can learn the address of the page of the page\ntable, and then ﬁnally learn the address of the desired memory access.\nA Real Address Space\naddress space of just user code, user data, and user heap, but as we can\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nA process generates a virtual address of 0, by\nThe page table is consulted, and the entry for VPN 0\nFor example, the code segment never begins at page 0.\nThis page,\ndress space is support for debugging, which the inaccessible zero page\nPerhaps more importantly, the kernel virtual address space (i.e., its\ndata structures and code) is a part of each user address space.\npropriate page tables of the soon-to-be-run process; however, it does not\nstructures are mapped into each user address space.\nThe kernel is mapped into each address space for a number of reasons.\nin physical memory, it would be quite hard to do things like swap pages\nof the page table to disk; if the kernel were given its own address space,\nthe OS does not want user applications reading or writing OS data or\npages to enable this.\nin the page table, what privilege level the CPU must be at in order to\naccess a particular page.",
      "keywords": [
        "VMS Virtual Memory",
        "Virtual Memory System",
        "MEMORY",
        "Virtual Memory",
        "address space",
        "PHYSICAL MEMORY",
        "VMS operating system",
        "systems",
        "page table",
        "Memory System",
        "address",
        "VMS",
        "space",
        "Virtual",
        "user address space"
      ],
      "concepts": [
        "pages",
        "paging",
        "memory",
        "memories",
        "systems",
        "policies",
        "policy",
        "address",
        "addresses",
        "referred"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 27,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.886,
          "base_score": 0.736,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 29,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.774,
          "base_score": 0.624,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "memory",
          "virtual",
          "page table",
          "vms"
        ],
        "semantic": [],
        "merged": [
          "page",
          "memory",
          "virtual",
          "page table",
          "vms"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3795619809841963,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117766+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 285-293)",
      "start_page": 285,
      "end_page": 293,
      "summary": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\nPage Replacement\nThe page table entry (PTE) in VAX contains the following bits: a valid\nlocation of the page in physical memory.\nwithout hardware support for determining which pages are active.\ncess has a maximum number of pages it can keep in memory, known as\nEach of these pages is kept on a FIFO list; when\na process exceeds its RSS, the “ﬁrst-in” page is evicted.\nchance lists where pages are placed before getting evicted from memory,\nspeciﬁcally a global clean-page free list and dirty-page list.\nP exceeds its RSS, a page is removed from its per-process FIFO; if clean\n(not modiﬁed), it is placed on the end of the clean-page list; if dirty (mod-\niﬁed), it is placed on the end of the dirty-page list.\nIf another process Q needs a free page, it takes the ﬁrst free page off\npage before it is reclaimed, P reclaims it from the free (or dirty) list, thus\nPage Clustering\nAnother optimization used in VMS also helps overcome the small page\nSpeciﬁcally, with such small pages, disk I/O during swap-\nlarge batches of pages together from the global dirty list, and writes them\nin most modern systems, as the freedom to place pages anywhere within\nswap space lets the OS group pages, perform fewer and bigger writes,\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nsome notion of which pages are in use in a system.\nsome understanding of which pages are actively being used in a system,\nmark all of the pages in the page table as inaccessible (but keep around\nthe information as to which pages are really accessible by the process,\nperhaps in the “reserved OS ﬁeld” portion of the page table entry).\na process accesses a page, it will generate a trap into the OS; the OS will\nthen check if the page really should be accessible, and if so, revert the\npage to its normal protections (e.g., read-only, or read-write).\nof a replacement, the OS can check which pages remain marked inacces-\nsible, and thus get an idea of which pages have not been recently used.\nstill obtaining a good idea of page usage.\nOS also must not be too passive in such marking, or all pages will end up\nreferenced; the OS will again have no good idea which page to evict.\nzeroing of pages.\nof adding a page to your address space, say in your heap.\nimplementation, the OS responds to a request to add a page to your heap\nby ﬁnding a page in physical memory, zeroing it (required for security;\notherwise you’d be able to see what was on the page from when some\nsetting up the page table to refer to that physical page as desired).\nnaive implementation can be costly, particularly if the page does not get\npage is added to your address space; it puts an entry in the page table\nthat marks the page inaccessible.\npage, a trap into the OS takes place.\nof the page table entry) that this is actually a demand-zero page; at this\npoint, the OS then does the needed work of ﬁnding a physical page, ze-\nnever accesses the page, all of this work is avoided, and thus the virtue of\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nOS needs to copy a page from one address space to another, instead of\ncopying it, it can map it into the target address space and mark it read-\nIf both address spaces only read the page, no\nIf, however, one of the address spaces does indeed try to write to the\npage, it will trap into the OS.\nThe OS will then notice that the page is a\nmap this new page into the address space of the faulting process.\nprocess then continues and now has its own private copy of the page.\nlibrary can be mapped copy-on-write into the address spaces of many\nprocesses, saving valuable memory space.\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10”\nAn early time-sharing OS where a number of good ideas came from.\n[BJ81] “Converting a Swap-Based System to do Paging\nin an Architecture Lacking Page-Reference Bits”\n[RL81] “Segmented FIFO Page Replacement”\nhow virtual memory works?\nPage tables are usually quite\nStudent: I also now understand that the page table is one of those data structures\nWe started with simple structures, like arrays (a.k.a. linear page tables), and advanced all the way up to multi-level tables (which look\nlike trees), and even crazier things like pageable page tables in kernel virtual\nProfessor: Indeed.\nStudent: And here’s one more important thing: I learned that the address trans-\nStudent: Well, it’s certainly fun to study, and good to know how page replace-\nStudent: Thanks professor!",
      "keywords": [
        "VIRTUAL MEMORY SYSTEM",
        "VMS VIRTUAL MEMORY",
        "VIRTUAL MEMORY",
        "MEMORY",
        "MEMORY SYSTEM",
        "VMS",
        "pages",
        "address space",
        "VMS VIRTUAL",
        "page table",
        "SYSTEM",
        "address",
        "FIFO Page Replacement",
        "VIRTUAL",
        "FIFO"
      ],
      "concepts": [
        "pages",
        "paged",
        "paging",
        "professor",
        "memory",
        "memories",
        "student",
        "work",
        "systems",
        "virtual"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 28,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 27,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 23,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.801,
          "base_score": 0.651,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "vms",
          "vms virtual",
          "virtual",
          "vax"
        ],
        "semantic": [],
        "merged": [
          "page",
          "vms",
          "vms virtual",
          "virtual",
          "vax"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.381426365033807,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117823+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 294-301)",
      "start_page": 294,
      "end_page": 301,
      "summary": "Professor: Well, imagine we have a peach –\nStudent: (interrupting) Peaches again!\nProfessor: (interrupting) This has nothing to do with that – I just like peaches.\nProfessor: Yes. Student: OK, let me think.\nStudent: I did?\nProfessor: Indeed.\nas it turns out, there are certain types of programs that we call multi-threaded\napplications; each thread is kind of like an independent agent running around\nBut these threads access\nwe don’t coordinate access to memory between threads, the program won’t work\nStudent: I see.\nProfessor: Indeed there are...\ncess: that of a thread.\ning fetched from and executed), a multi-threaded program has more than\nthread is very much like a separate process, except for one difference:\nThe state of a single thread is thus very similar to that of a process.\nuses for computation; thus, if there are two threads that are running on\nthreads is quite similar to the context switch between processes, as the\nto store the state of each thread of a process.\nthough, in the context switch we perform between threads as compared\nOne other major difference between threads and processes concerns\n(which we can now call a single-threaded process), there is a single stack,\nFigure 26.1: A Single-Threaded Address Space\nHowever, in a multi-threaded process, each thread runs independently\nInstead of a single stack in the address space, there will be one per\nthread.\nLet’s say we have a multi-threaded process that has two threads\nwhat is sometimes called thread-local storage, i.e., the stack of the rele-\nvant thread.\nAn Example: Thread Creation\nLet’s say we wanted to run a program that created two threads, each\nThe main program creates two threads, each of which will run the\nOnce a thread is created, it may start running right away (depending\nAfter creating the two threads\n(T1 and T2), the main thread calls pthread join(), which waits for a\nparticular thread to complete.\nrc = pthread_create(&p2, NULL, mythread, \"B\"); assert(rc == 0);\n// join waits for the threads to finish\nFigure 26.2: Simple Thread Creation Code (t0.c)\ndirection, and each column shows when a different thread (the main one,\nor Thread 1, or Thread 2) is running.\nwhich thread the scheduler decides to run at a given point.\nonce a thread is created, it may run immediately, which would lead to the\ndecided to run Thread 2 ﬁrst even though Thread 1 was created earlier;\nthere is no reason to assume that a thread that is created ﬁrst will run ﬁrst.\nTable 26.3 shows this ﬁnal execution ordering, with Thread 2 getting to\nstrut its stuff before Thread 1.\nAs you might be able to see, one way to think about thread creation\ncreates a new thread of execution for the routine that is being called, and\nAs you also might be able to tell from this example, threads make life",
      "keywords": [
        "thread",
        "address space",
        "Professor",
        "Concurrency",
        "program",
        "Student",
        "address",
        "space",
        "stack",
        "process",
        "running",
        "Thread Creation",
        "single",
        "run",
        "Peaches"
      ],
      "concepts": [
        "thread",
        "student",
        "running",
        "run",
        "professor",
        "programs",
        "programming",
        "concurrency",
        "concurrent",
        "spaces"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 32,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 42,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "threads",
          "threaded",
          "professor",
          "thread created"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "threads",
          "threaded",
          "professor",
          "thread created"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34415033216522734,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117877+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 302-309)",
      "start_page": 302,
      "end_page": 309,
      "summary": "Thread 1\nThread2\ncreates Thread 1\ncreates Thread 2\nThread 1\nThread2\ncreates Thread 1\ncreates Thread 2\nThread 1\nThread2\ncreates Thread 1\ncreates Thread 2\n// a counter, but it shows the problem nicely.\n// Just launches two threads (pthread_create)\nprintf(\"main: begin (counter = %d)\\n\", counter);\n// join waits for the threads to finish\nprintf(\"main: done with both (counter = %d)\\n\", counter);\nThe simple thread example we showed above was useful in showing\nhow threads are created and how they can run in different orders depend-\nthough, is how threads interact when they access shared data.\nLet us imagine a simple example where two threads wish to update a\nwe wrap the thread creation and join routines to simply exit on failure;\nmakes sure the return code is 0; if it isn’t, Pthread create() just prints\nthreads, we just use a single piece of code, and pass the thread an argu-\nment (in this case, a string) so we can have each thread print a different\ntrying to do: add a number to the shared variable counter, and do so 10\nmain: begin (counter = 0)\nmain: done with both (counter = 20000000)\nUnfortunately, when we run this code, even on a single processor, we\nmain: begin (counter = 0)\nmain: done with both (counter = 19345221)\nmain: begin (counter = 0)\nmain: done with both (counter = 19221041)\nassembly instructions make up the program.\nunderstand the low-level code to update a counter (as in our example),\nwe run objdump (Linux) to see the assembly code:\ncase, we wish to simply add a number (1) to counter.\nThus, the code\nadd $0x1, %eax\nThis example assumes that the variable counter is located at address\ncode, and is thus about to increment counter by one.\nof counter (let’s say it’s 50 to begin with) into its register eax.\neax=50 for Thread 1.\nthe OS saves the state of the currently running thread (its PC, its registers\nincluding eax, etc.) to the thread’s TCB.\nNow something worse happens: Thread 2 is chosen to run, and it en-\nthe value of counter and putting it into its eax (remember: each thread\n(after instruction)\nThread 1\nThread 2\n%eax counter\nadd $0x1, %eax\nadd $0x1, %eax\ncounter is still 50 at this point, and thus Thread 2 has eax=50.\nthen assume that Thread 2 executes the next two instructions, increment-\ncounter (address 0x8049a1c).\nFinally, another context switch occurs, and Thread 1 resumes running.\nmov instruction executes, and saves the value to memory; the counter is\nPut simply, what has happened is this: the code to increment counter\nhas been run twice, but counter, which started at 50, is now only equal\nA “correct” version of this program should have resulted in counter\nused to nice, RISC-like instruction sets: x86 has variable-length instruc-\nthe counter starts at value 50, and trace through this example to make\ndepend on the timing execution of the code.\nBecause multiple threads executing this code can result in a race con-\ndition, we call this code a critical section.\ncode that accesses a shared variable (or more generally, a shared resource)\nand must not be concurrently executed by more than one thread.\nThis property guarantees that if one thread is executing within the critical\nAssume this instruction adds a value to a memory location, and the\nhardware guarantees that it executes atomically; when the instruction\nfrom the hardware: when an interrupt occurs, either the instruction has\nadd $0x1, %eax\nsuch an instruction.\nbe able to build multi-threaded code that accesses critical sections in a\n• A critical section is a piece of code that accesses a shared resource,\n• A race condition arises if multiple threads of execution enter the\nwhich threads ran when.\n• To avoid these problems, threads should use some kind of mutual\nexclusion primitives; doing so guarantees that only a single thread\nprograms use them to get the desired results?\nof interaction occurs between threads, that of accessing shared variables\nand the need to support atomicity for critical sections.\ncommon in multi-threaded programs.\nLater, with multi-threaded processes, application program-\nthe code that updates to these shared structures (e.g., a bitmap for alloca-",
      "keywords": [
        "Thread",
        "counter",
        "main",
        "eax",
        "creates Thread",
        "code",
        "Thread Trace",
        "prints",
        "runs prints",
        "instruction",
        "mov",
        "CONCURRENCY",
        "begin",
        "run",
        "critical section"
      ],
      "concepts": [
        "thread",
        "instructions",
        "instruction",
        "counter",
        "program",
        "running",
        "runs",
        "run",
        "result",
        "main"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 32,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.757,
          "base_score": 0.607,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 30,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "counter",
          "thread",
          "eax",
          "creates thread",
          "threads"
        ],
        "semantic": [],
        "merged": [
          "counter",
          "thread",
          "eax",
          "creates thread",
          "threads"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3593005864834246,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117931+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 310-319)",
      "start_page": 310,
      "end_page": 319,
      "summary": "has to go into writing multi-threaded programs.\nMorgan Kaufmann, September 1992\nThis program, x86.py, allows you to see how different thread inter-\nTthis speciﬁes a single thread, an interrupt every 100 instructions,\nthread and make sure you understand what it does, like this:\n5. Now run with multiple iterations and threads:\nDo you understand why the code in each thread loops three times?\n./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0\nyou tell, just by looking at the thread interleaving, what the ﬁnal\nvalue of x will be?\n7. Now use a ﬁxed interrupt interval to explore the program further.\n2000 being used by the threads?\nHow do the threads behave?\nWhat is thread 0 doing?\nInterlude: Thread API\nThis chapter brieﬂy covers the main portions of the thread API.\nCRUX: HOW TO CREATE AND CONTROL THREADS\nThread Creation\nThe ﬁrst thing you have to be able to do to write a multi-threaded\nprogram is to create new threads, and thus some kind of thread creation\nthread,\nfour arguments: thread, attr, start routine, and arg.\nthread, is a pointer to a structure of type pthread t; we’ll use this\nstructure to interact with this thread, and thus we need to pass it to\nINTERLUDE: THREAD API\nThe second argument, attr, is used to specify any attributes this thread\nfunction should this thread start running in?\n(start routine), which is passed a single argument of type void * (as\nvalue of type void * (i.e., a void pointer).\nint pthread_create(..., // first two args are the same\nIf instead the routine took a void pointer as an argument, but returned\nint pthread_create(..., // first two args are the same\nto the function where the thread begins execution.\na void pointer as an argument to the function start routine allows us\nto pass in any type of argument; having it as a return value allows the\nthread to return any type of result.\nHere we just create a thread\nThe thread, once created, can simply cast its argument\nOnce you create a thread, you really have another\nsame address space as all the currently existing threads in the program.\nThread Completion\nThe example above shows how to create a thread.\nhappens if you want to wait for a thread to complete?\nint pthread_join(pthread_t thread, void **value_ptr);\nINTERLUDE: THREAD API\nrc = pthread_create(&p, NULL, mythread, &args);\nFigure 27.1: Creating a Thread\nand is used to specify which thread to wait for.\nyou passed into the thread library during creation; if you held onto it,\nyou can now use it to wait for the thread to stop running.\nThe second argument is a pointer to the return value you expect to get\npointer to void; because the pthread join() routine changes the value\nof the passed in argument, you need to pass in a pointer to that value, not\njust the value itself.\nIn the code, a single thread\nTo return values, the myret t type is used.\nOnce the thread is\nﬁnished running, the main thread, which has been waiting inside of the\npthread join() routine1, then returns, and we can access the values\nreturned from the thread, namely whatever is in myret t.\nexample, if we just create a thread with no arguments, we can pass NULL\nin as an argument when the thread is created.\ninto pthread join() if we don’t care about the return value.\nSecond, if we are just passing in a single value (e.g., an int), we don’t\n1Note we use wrapper functions here; speciﬁcally, we call Malloc(), Pthread join(), and\nINTERLUDE: THREAD API\nPthread_create(&p, NULL, mythread, &args);\nPthread_join(p, (void **) &m);\nFigure 27.2: Waiting for Thread Completion\nvalues are returned from a thread.\nwhich refers to something allocated on the thread’s call stack.\nINTERLUDE: THREAD API\nPthread_create(&p, NULL, mythread, (void *) 100);\nPthread_join(p, (void **) &m);\nFigure 27.3: Simpler Argument Passing to a Thread\nthe stack is so easy to use, after all!), and thus, passing back a pointer to\na thread, followed by an immediate call to pthread join(), is a pretty\nstrange way to create a thread.\ncreating more than just one thread and waiting for it to complete, other-\nWe should note that not all code that is multi-threaded uses the join\nFor example, a multi-threaded web server might create a number\nof worker threads, and then use the main thread to accept requests and\nHowever, a parallel program that creates threads\nBeyond thread creation and join, probably the next most useful set of",
      "keywords": [
        "thread",
        "pthread",
        "void",
        "Thread API",
        "int",
        "CREATE",
        "int pthread",
        "argument",
        "run",
        "arg",
        "program",
        "pthread join",
        "routine",
        "systems",
        "myarg"
      ],
      "concepts": [
        "thread",
        "programming",
        "programs",
        "uses",
        "useful",
        "arguments",
        "argument",
        "atomic",
        "including",
        "include"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 30,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.699,
          "base_score": 0.549,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "argument",
          "void",
          "thread api",
          "join"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "argument",
          "void",
          "thread api",
          "join"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33614813516565745,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.117984+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 320-329)",
      "start_page": 320,
      "end_page": 329,
      "summary": "pthread_mutex_t lock;\npthread_mutex_lock(&lock);\npthread_mutex_unlock(&lock);\nThe intent of the code is as follows: if no other thread holds the lock\nwhen pthread mutex lock() is called, the thread will acquire the lock\nIf another thread does indeed hold the lock,\nthe thread trying to grab the lock will not return from the call until it has\nacquired the lock (implying that the thread holding the lock has released\ninside the lock acquisition function at a given time; only the thread with\nthe lock acquired, however, should call unlock.\nAll locks must be properly\nWith POSIX threads, there are two ways to initialize locks.\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nint rc = pthread_mutex_init(&lock, NULL);\ncode when calling lock and unlock.\nvoid Pthread_mutex_lock(pthread_mutex_t *mutex) {\nint rc = pthread_mutex_lock(mutex);\nThe lock and unlock routines are not the only routines that pthreads\nhas to interact with locks.\nturns failure if the lock is already held; the timedlock version of acquir-\ndeﬁnitely) in a lock acquisition routine can be useful, as we’ll see in future\ncase with POSIX threads, is the presence of a condition variable.\nTo use a condition variable, one has to in addition have a lock that is\nthis lock should be held.\nThe ﬁrst routine, pthread cond wait(), puts the calling thread to\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nPthread_mutex_lock(&lock);\nPthread_cond_wait(&init, &lock);\nPthread_mutex_unlock(&lock);\nIn this code, after initialization of the relevant lock and condition3,\na thread checks to see if the variable initialized has yet been set to\nPthread_mutex_lock(&lock);\nPthread_mutex_unlock(&lock);\ning thread to sleep, releases the lock when putting said caller to sleep.\nImagine if it did not: how could the other thread acquire the lock and\npthread cond wait() re-acquires the lock, thus ensuring that any time\nthe waiting thread is running between the lock acquire at the beginning\ntween two threads, instead of a condition variable and associated lock.\ncreation, building mutual exclusion via locks, and signaling and waiting\nAbove all else, any code to lock or signal between\n• Initialize locks and condition variables.\nfrom, threads.\n• Always use condition variables to signal between threads.\nLocks\nwith the introduction of something referred to as a lock.\nannotate source code with locks, putting them around critical sections,\nTo use a lock, we add\nlock(&mutex);\nA lock is just a variable, and thus to use one, you must declare a lock\nThis lock variable (or just\nther available (or unlocked or free) and thus no thread holds the lock, or\nacquired (or locked or held), and thus exactly one thread holds the lock\nLOCKS\nin the data type as well, such as which thread holds the lock, or a queue\nuser of the lock.\nThe semantics of the lock() and unlock() routines are simple.\ning the routine lock() tries to acquire the lock; if no other thread holds\nthe lock (i.e., it is free), the thread will acquire the lock and enter the crit-\nical section; this thread is sometimes said to be the owner of the lock.\nanother thread then calls lock() on that same lock variable (mutex in\nthis example), it will not return while the lock is held by another thread;\nwhile the ﬁrst thread that holds the lock is in there.\nIf no other threads are waiting for the lock (i.e., no other\nthread has called lock() and is stuck therein), the state of the lock is\nIf there are waiting threads (stuck in lock()),\nlock’s state, acquire the lock, and enter the critical section.\na lock around a section of code, the programmer can guarantee that no\nThus locks\nPthread Locks\nThe name that the POSIX library uses for a lock is a mutex, as it is used\nThus, when you see the following POSIX threads code, you\nour wrappers that check for errors upon lock and unlock):\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nPthread_mutex_lock(&lock);\n// wrapper for pthread_mutex_lock()\nPthread_mutex_unlock(&lock);\nis used any time any critical section is accessed (a coarse-grained locking\ndifferent locks, thus allowing more threads to be in locked code at once\nLOCKS\nBuilding A Lock\nBy now, you should have some understanding of how a lock works,\nBut how should we build a lock?\nThe Crux: HOW TO BUILD A LOCK\nHow can we build an efﬁcient lock?\nlocking library.\nEvaluating Locks\nThe ﬁrst is whether the lock does\nlock work, preventing multiple threads from entering a critical section?\nDoes each thread contending for the lock get\nby using the lock.\nand threads on each contending for the lock?",
      "keywords": [
        "lock",
        "THREAD",
        "pthread",
        "mutex",
        "THREAD API",
        "code",
        "pthread cond",
        "POSIX threads",
        "pthread mutex lock",
        "cond",
        "pthread mutex",
        "critical section",
        "Condition",
        "section",
        "variable"
      ],
      "concepts": [
        "locks",
        "thread",
        "pthreads",
        "code",
        "initialization",
        "initialized",
        "initialize",
        "programs",
        "programming",
        "variables"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lock",
          "thread",
          "locks",
          "threads",
          "mutex"
        ],
        "semantic": [],
        "merged": [
          "lock",
          "thread",
          "locks",
          "threads",
          "mutex"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22790260634731785,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118038+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 330-344)",
      "start_page": 330,
      "end_page": 344,
      "summary": "LOCKS\nvoid lock() {\ninterruption, a thread can be sure that the code it executes will execute\nLOCKS\n// 1->thread wants to grab lock\n(thread 0 or 1?)\nvoid lock() {\nLOCKS\nvoid init(lock_t *mutex) {\n// 0 -> lock is available, 1 -> held\nvoid unlock(lock_t *mutex) {\nstand how test-and-set works, let’s ﬁrst try to build a simple lock without\nthe lock is held or not.\nvariable to indicate whether some thread has possession of a lock.\nﬁrst thread that enters the critical section will call lock(), which tests\nto 1 to indicate that the thread now holds the lock.\nthe critical section, the thread calls unlock() and clears the ﬂag, thus\nIf another thread happens to call lock() while that ﬁrst thread is in\nThread 1\nThread 2\ncall lock()\ncall lock()\nLOCKS\nfact that the way a thread waits to acquire a lock that is already held:\nSpin-waiting wastes time waiting for another thread to release a lock.\nBuilding A Working Spin Lock\nLOCKS\n} lock_t;\nlock->flag = 0;\nwhile (TestAndSet(&lock->flag, 1) == 1)\nlock->flag = 0;\nFigure 28.2: A Simple Spin Lock Using Test-and-set\nmore powerful instruction is enough to build a simple spin lock, as we\nwhere a thread calls lock() and no other thread currently holds the lock;\ning thread, which is testing the value of ﬂag, will not get caught spinning\nin the while loop and will acquire the lock.\ncally set the value to 1, thus indicating that the lock is now held.\nthe thread is ﬁnished with its critical section, it calls unlock() to set the\nthe lock held (i.e., flag is 1).\nIn this case, this thread will call lock() and\nwill return the old value at ﬂag, which is 1 (because the lock is held),\nthread will spin and spin until the lock is ﬁnally released.\nthus acquire the lock and enter the critical section.\nBy making both the test (of the old lock value) and set (of the new\nvalue) a single atomic operation, we ensure that only one thread acquires\nthe lock.\nto as a spin lock.\nIt is the simplest type of lock to build, and simply spins,\nWithout preemption, spin locks don’t make much sense on\nLOCKS\nEvaluating Spin Locks\nGiven our basic spin lock, we can now evaluate how effective it is\nviously yes: the spin lock only allows a single thread to enter the critical\nThus, we have a correct lock.\nHow fair is a spin lock to a waiting thread?\nThe answer here, unfortunately, is bad news: spin locks don’t pro-\nSpin locks are not fair and may lead to starvation.\nWhat are the costs of using a spin lock?\nIn the ﬁrst, imagine threads competing for the lock on a single\nFor spin locks, in the single CPU case, performance overheads can\nbe quite painful; imagine the case where the thread holding the lock is\nquire the lock.\nIn this case, each of those threads will spin for the duration\nHowever, on multiple CPUs, spin locks work reasonably well (if the\nboth contending for a lock.\nIf Thread A (CPU 1) grabs the lock, and then\nSpinning to wait for a lock held on another processor\nLOCKS\nWith the compare-and-swap instruction, we can build a lock in a man-\nwhile (CompareAndSwap(&lock->flag, 0, 1) == 1)\nso, atomically swaps in a 1 thus acquiring the lock.\nacquire the lock while it is held will get stuck spinning until the lock is\nlock\\n\"\nHowever, if we just build a simple spin lock with it, its behavior is iden-\ntical to the spin lock we analyzed above.\nLOCKS\nwhile (LoadLinked(&lock->flag) == 1)\nif (StoreConditional(&lock->flag, 1) == 1)\nlock->flag = 0;\nAs a challenge to yourself, try thinking about how to build a lock using\nThe lock() code is the only interesting piece.\nFirst, a thread spins\nwaiting for the ﬂag to be set to 0 (and thus indicate the lock is not held).\nOnce so, the thread tries to acquire the lock via the store-conditional; if it\nsucceeds, the thread has atomically changed the ﬂag’s value to 1 and thus\nlock() and executes the load-linked, returning 0 as the lock is not held.\nthread enters the lock code, also executing the load-linked instruction,\nthreads will succeed in updating the ﬂag to 1 and thus acquire the lock;\nLOCKS\nother thread updated the value of ﬂag between its load-linked and store-\nconditional) and thus have to try to acquire the lock again.\nlock and unlock code looks like what you see in Figure 28.6.\ncombination to build a lock.\na thread wishes to acquire a lock, it ﬁrst does an atomic fetch-and-add\nLOCKS\n} lock_t;\nlock->ticket = 0;\nlock->turn\nwhile (lock->turn != myturn)\nFetchAndAdd(&lock->turn);\nFigure 28.6: Ticket Locks\nfront of it have passed through the critical section and released the lock).\non test-and-set (for example) could spin forever even as other threads\nacquire and release the lock.\nOur simple hardware-based locks are simple (only a few lines of code)\none thread (thread 0) is in a critical section and thus has a lock held, and\n0 is run again, which releases the lock, and ﬁnally (the next time it runs,\nlock.\nThus, any time a thread gets caught spinning in a situation like this,\nspinning and waiting for a single thread to release the lock.\nLOCKS\nHow can we develop a lock that doesn’t needlessly waste time spin-\nHardware support got us pretty far: working locks, and even (as with\nsection, and threads start to spin endlessly, waiting for the interrupt (lock-\nspin, instead give up the CPU to another thread.\nThink about the example with two threads on one CPU; in this case,\nother thread will run and ﬁnish its critical section.\nvoid lock() {\nFigure 28.7: Lock With Test-and-set And Yield\nLOCKS\npattern before the thread holding the lock gets to run again.\nA thread\nthe scheduler makes a bad choice, a thread runs that must either spin\nwaiting for the lock (our ﬁrst approach), or yield the CPU immediately\nare waiting to enter the lock.\nthe old test-and-set idea with an explicit queue of lock waiters to make a\nmore efﬁcient lock.\nYou might notice how the guard is used, basically as a spin-lock around\nthe ﬂag and queue manipulations the lock is using.\ndoesn’t avoid spin-waiting entirely; a thread might be interrupted while\nacquiring or releasing the lock, and thus cause other threads to spin-wait\nited (just a few instructions inside the lock and unlock code, instead of the\nSecond, you might notice that in lock(), when a thread can not ac-\nthread), set guard to 0, and yield the CPU.\nLOCKS\n} lock_t;\n; //acquire guard lock by spinning\nm->flag = 1; // lock is acquired\nvoid unlock(lock_t *m) {\n; //acquire guard lock by spinning\nm->flag = 0; // let go of lock; no one wants it\nunpark(queue_remove(m->q)); // hold lock (for next thread!)\nFigure 28.8: Lock With Queues, Test-and-set, Yield, And Wakeup\njust pass the lock directly from the thread releasing the lock to the next\nthread acquiring it; ﬂag is not set to 0 in-between.\nlock) could lead to trouble, for example, if that thread then released the\nlock.\nLOCKS\nThe code modiﬁcation, inside of lock(), is quite small:\norder to build a more efﬁcient lock in a thread library.\nif the lock is negative, it is held (because the high bit is set and that bit\nwith only one thread acquiring and releasing a lock, very little work is\ndone (the atomic bit test-and-set to lock and an atomic add to release the\nlock).\nTwo-Phase Locks\nlock.\nA two-phase lock realizes that spinning can be useful, particularly\nif the lock is about to be released.\nSo in the ﬁrst phase, the lock spins for\nLOCKS\nvoid mutex_lock (int *mutex) {\nFigure 28.9: Linux-based Futex Locks\nHowever, if the lock is not acquired during the ﬁrst spin phase, a sec-\nThe Linux lock above is a form of such\na lock, but it only spins once; a generalization of this could spin in a loop\nalways, making a single general-purpose lock, good for all possible use\nthe exact code to perform such locking is usually highly tuned.",
      "keywords": [
        "lock",
        "thread",
        "Spin Lock",
        "void lock",
        "thread calls lock",
        "call lock",
        "Simple Spin Lock",
        "Spin",
        "CPU",
        "flag",
        "code",
        "critical section",
        "int",
        "lock held",
        "critical"
      ],
      "concepts": [
        "locks",
        "thread",
        "spin",
        "spinning",
        "code",
        "instruction",
        "instructions",
        "interrupts",
        "interruption",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lock",
          "spin",
          "thread",
          "locks",
          "spin lock"
        ],
        "semantic": [],
        "merged": [
          "lock",
          "spin",
          "thread",
          "locks",
          "spin lock"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33353194992253793,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118092+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 345-354)",
      "start_page": 345,
      "end_page": 354,
      "summary": "LOCKS\nA landmark paper introducing a different approach to building concurrent data structures.\nLOCKS\nLock-based Concurrent Data Structures\nAdding locks to a data structure to make it us-\nCRUX: HOW TO ADD LOCKS TO DATA STRUCTURES\nWhen given a particular data structure, how should we add locks to\nthat the data structure yields high performance, enabling many threads\nConcurrent Counters\nOne of the simplest data structures is a counter.\nconcurrent counter in Figure 29.1.\nAs you can see, the non-synchronized counter is a trivial data structure,\nLOCK-BASED CONCURRENT DATA STRUCTURES\n} counter_t;\nvoid init(counter_t *c) {\nvoid increment(counter_t *c) {\nvoid decrement(counter_t *c) {\nint get(counter_t *c) {\nFigure 29.1: A Counter Without Locks\n} counter_t;\nvoid init(counter_t *c) {\nPthread_mutex_init(&c->lock, NULL);\nvoid increment(counter_t *c) {\nPthread_mutex_lock(&c->lock);\nPthread_mutex_unlock(&c->lock);\nvoid decrement(counter_t *c) {\nPthread_mutex_lock(&c->lock);\nPthread_mutex_unlock(&c->lock);\nint get(counter_t *c) {\nPthread_mutex_lock(&c->lock);\nPthread_mutex_unlock(&c->lock);\nFigure 29.2: A Counter With Locks\nThis concurrent counter is simple and works correctly.\ndata structures: it simply adds a single lock, which is acquired when call-\nLOCK-BASED CONCURRENT DATA STRUCTURES\nSloppy Counters\nAt this point, you have a working concurrent data structure.\nbenchmark in which each thread updates a single shared counter a ﬁxed\nthe counter one million times.\nthread can complete the million counter updates in a tiny amount of time\n(roughly 0.03 seconds), having two threads each update the counter one\nLOCK-BASED CONCURRENT DATA STRUCTURES\nglobal counter.\nlocal counters and one global one.\nalso locks: one for each local counter, and one for the global counter.\nlocal lock.\nBecause each CPU has its own local counter, threads across\nCPUs can update local counters without contention, and thus counter\nHowever, to keep the global counter up to date (in case a thread wishes\ncounter, by acquiring the global lock and incrementing it by the local\ncounter’s value; the local counter is then reset to zero.\nmore scalable the counter, but the further off the global value might be\nOne could simply acquire all the local locks and\nupdating their local counters L1 ...\nThe global counter value (G) is\nthreshold S, the local value is transferred to the global counter and the\nlocal counter is reset.\nsloppy counters with a threshold S of 1024.\ntime taken to update the counter four million times on four processors is\nLOCK-BASED CONCURRENT DATA STRUCTURES\n// global lock\nand locks\n} counter_t;\nvoid init(counter_t *c, int threshold) {\nlock and transfer local values to it\nvoid update(counter_t *c, int threadID, int amt) {\npthread_mutex_lock(&c->llock[threadID]);\nif (c->local[threadID] >= c->threshold) { // transfer to global\npthread_mutex_lock(&c->glock);\nint get(counter_t *c) {\npthread_mutex_lock(&c->glock);\nFigure 29.4: Sloppy Counter Implementation\nthreads each incrementing the counter 1 million times on four CPUs. If S\nA rough version of such a sloppy counter is found in Figure 29.4.\nLOCK-BASED CONCURRENT DATA STRUCTURES\nFigure 29.5: Scaling Sloppy Counters\nThe former works because part of the lookup actually need not be locked;\nupdating the shared list does a lock need to be held.\nLOCK-BASED CONCURRENT DATA STRUCTURES\nlock;\npthread_mutex_init(&L->lock, NULL);\npthread_mutex_lock(&L->lock);\npthread_mutex_unlock(&L->lock);\npthread_mutex_unlock(&L->lock);\npthread_mutex_lock(&L->lock);\npthread_mutex_unlock(&L->lock);\npthread_mutex_unlock(&L->lock);\nFigure 29.6: Concurrent Linked List\nLOCK-BASED CONCURRENT DATA STRUCTURES\npthread_mutex_init(&L->lock, NULL);\nvoid List_Insert(list_t *L, int key) {\npthread_mutex_lock(&L->lock);\npthread_mutex_unlock(&L->lock);\npthread_mutex_lock(&L->lock);\npthread_mutex_unlock(&L->lock);\nlist, you instead add a lock per node of the list.\nlist, the code ﬁrst grabs the next node’s lock and then releases the current\ntice, it is hard to make such a structure faster than the simple single lock\nlock, performing an operation, and releasing it.",
      "keywords": [
        "concurrent data structures",
        "counter",
        "lock",
        "data structures",
        "concurrent data",
        "Lock-based Concurrent Data",
        "mutex",
        "list",
        "data",
        "local counter",
        "local",
        "pthread",
        "structure",
        "concurrent",
        "Concurrent Linked List"
      ],
      "concepts": [
        "locks",
        "counters",
        "listed",
        "concurrency",
        "concurrently",
        "thread",
        "structures",
        "times",
        "value",
        "local"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.757,
          "base_score": 0.607,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lock",
          "counter",
          "counter_t",
          "concurrent",
          "local"
        ],
        "semantic": [],
        "merged": [
          "lock",
          "counter",
          "counter_t",
          "concurrent",
          "local"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3483814782435125,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118158+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 355-364)",
      "start_page": 355,
      "end_page": 364,
      "summary": "LOCK-BASED CONCURRENT DATA STRUCTURES\nconcurrent may not be important.\nA general design tip, which is useful in concurrent code as well as\nConcurrent Queues\nconcurrent data structure: add a big lock.\nInstead, we’ll take a look at a slightly more concurrent queue designed\nIf you study this code carefully, you’ll notice that there are two locks,\nlocks is to enable concurrency of enqueue and dequeue operations.\nthe common case, the enqueue routine will only access the tail lock, and\ndequeue only the head lock.\nQueues are commonly used in multi-threaded applications.\nthe type of queue used here (with just locks) often does not completely\nqueue, that enables a thread to wait if the queue is either empty or overly\nLOCK-BASED CONCURRENT DATA STRUCTURES\npthread_mutex_init(&q->headLock, NULL);\npthread_mutex_init(&q->tailLock, NULL);\nvoid Queue_Enqueue(queue_t *q, int value) {\npthread_mutex_lock(&q->tailLock);\npthread_mutex_lock(&q->headLock);\nFigure 29.8: Michael and Scott Concurrent Queue\nConcurrent Hash Table\ndata structure, the hash table.\nThis concurrent hash table is straightforward, is built using the con-\nLOCK-BASED CONCURRENT DATA STRUCTURES\nFigure 29.9: A Concurrent Hash Table\nfor its good performance is that instead of having a single lock for the en-\ntire structure, it uses a lock per hash bucket (each of which is represented\nof comparison, is the performance of a linked list (with a single lock).\nAs you can see from the graph, this simple concurrent hash table scales\nSimple Concurrent List\nConcurrent Hash Table\nLOCK-BASED CONCURRENT DATA STRUCTURES\nWhen building a concurrent data structure, start with the most basic ap-\nMany operating systems added a single lock when transitioning to multi-\nthe big kernel lock (BKL), and was the source of performance problems\nRead the Linux and Solaris kernel books\nWe have introduced a sampling of concurrent data structures, from\nuse traditional locks at all; such non-blocking data structures are some-\nLOCK-BASED CONCURRENT DATA STRUCTURES\nProfessor Scott and his students have been at the forefront of concurrent algorithms and data structures\nAvailable: www.cs.tau.ac.il/˜shanir/concurrent-data-structures.pdf\nA short but relatively comprehensive reference on concurrent data structures.\na parent thread might wish to check whether a child thread has completed\nbefore continuing (this is often called a join()); how should such a wait\nPthread_create(&c, NULL, child, NULL); // create child\n// XXX how to wait for child?\nFigure 30.1: A Parent Waiting For Its Child\nput the parent to sleep until the condition we are waiting for (e.g., the\nPthread_create(&c, NULL, child, NULL); // create child\nFigure 30.2: Parent Waiting For Child: Spin-based Approach\nTHE CRUX: HOW TO WAIT FOR A CONDITION\nIn multi-threaded programs, it is often useful for a thread to wait for\nshould a thread wait for a condition?\nTo wait for a condition to become true, a thread can make use of what\nqueue that threads can put themselves on when some state of execution\n(i.e., some condition) is not as desired (by waiting on the condition);\nmore) of those waiting threads and thus allow them to continue (by sig-\nlike this: pthread cond t c;, which declares c as a condition variable\nThe wait() call\nthus wants to wake a sleeping thread waiting on this condition.\npthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);\nPthread_mutex_lock(&m);\nPthread_mutex_lock(&m);\nPthread_cond_wait(&c, &m);\nPthread_create(&p, NULL, child, NULL);\nFigure 30.3: Parent Waiting For Child: Use A Condition Variable\nmutex as a parameter; it assumes that this mutex is locked when wait()\nThe responsibility of wait() is to release the lock and put the\nother thread has signaled it), it must re-acquire the lock before returning\ncessor) and thus immediately calls into thr join() to wait for the child\nIn this case, it will acquire the lock, check if the child\nis done (it is not), and put itself to sleep by calling wait() (hence releas-\nand call thr exit() to wake the parent thread; this code just grabs the\nlock, sets the state variable done, and signals the parent thus waking it.\nFinally, the parent will run (returning from wait() with the lock held),\ndone to 1, calls signal to wake a sleeping thread (but there is none, so\nThe parent then runs, calls thr join(), sees\nthat done is 1, and thus does not wait and returns.\nof just an if statement when deciding whether to wait on the condition.\nPthread_mutex_lock(&m);\nPthread_mutex_lock(&m);\nPthread_cond_wait(&c, &m);\nchild runs immediately and calls thr exit() immediately; in this case,\nthe child will signal, but there is no thread asleep on the condition.\nthe parent runs, it will simply call wait and be stuck; no thread will ever\nthe state variable done; it records the value the threads are interested in\nThe sleeping, waking, and locking all are built around it.\nthat one does not need to hold a lock in order to signal and wait.\nPthread_cond_wait(&c);\nBut just before it calls wait to go to sleep, the parent\ndone to 1 and signals, but no thread is waiting and thus no thread is",
      "keywords": [
        "CONCURRENT DATA STRUCTURES",
        "Concurrent Hash Table",
        "CONCURRENT DATA",
        "pthread",
        "DATA STRUCTURES",
        "CONCURRENT",
        "condition",
        "Hash Table",
        "child",
        "lock",
        "parent",
        "Hash",
        "Concurrent Hash",
        "NULL",
        "LOCK-BASED CONCURRENT DATA"
      ],
      "concepts": [
        "lock",
        "concurrent",
        "concurrency",
        "structures",
        "operations",
        "operating",
        "queues",
        "parent",
        "void",
        "conditions"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 37,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "concurrent",
          "child",
          "lock",
          "concurrent data",
          "hash"
        ],
        "semantic": [],
        "merged": [
          "concurrent",
          "child",
          "lock",
          "concurrent data",
          "hash"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3246755551852634,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118211+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 365-372)",
      "start_page": 365,
      "end_page": 372,
      "summary": "best to hold the lock while signaling when using condition variables.\nThe converse of this tip, i.e., hold the lock when calling wait, is not just\nputting the caller to sleep, and (c) re-acquires the lock just before return-\nducer/consumer or bounded-buffer problem.\nThe Producer/Consumer (Bound Buffer) Problem\nknown as the producer/consumer problem, or sometimes as the bounded\nbuffer problem, which was ﬁrst posed by Dijkstra [D72].\nthis very producer/consumer problem that led Dijkstra and his co-workers\nImagine one or more producer threads and one or more consumer\nProducers produce data items and wish to place them in a buffer;\nconsumers grab data items out of the buffer consume them in some way.\nmulti-threaded web server, a producer puts HTTP requests into a work\nqueue (i.e., the bounded buffer); consumer threads take requests out of\nA bounded buffer is also used when you pipe the output of one pro-\nproducer; the wc process is the consumer; between them is an in-kernel\nbounded buffer; you, in this example, are just the happy user.\nint buffer;\nbuffer = value;\nreturn buffer;\nFigure 30.5: Producer/Consumer Threads (Version 1)\nBecause the bounded buffer is a shared resource, we must of course\nThe ﬁrst thing we need is a shared buffer, into which a producer puts\ndata, and out of which a consumer takes data.\na value into the shared buffer, and to get a value out of the buffer.\nThe put() routine assumes the buffer is empty\nshared buffer and marks it full by setting count to 1.\ndoes the opposite, setting the buffer to empty (i.e., setting count to 0)\nDon’t worry that this shared buffer has just a\nthe buffer to either put data into it or get data out of it.\nPthread_cond_wait(&cond, &mutex); // p3\nPthread_cond_wait(&cond, &mutex); // c3\nFigure 30.6: Producer/Consumer: Single CV and If Statement\nthis should be obvious: only put data into the buffer when count is zero\n(i.e., when the buffer is empty), and only get data from the buffer when\ncount is one (i.e., when the buffer is full).\ncode such that a producer puts data into a full buffer, or a consumer gets\nwe’ll call the producer threads, and the other set which we’ll call con-\nFigure 30.5 shows the code for a producer that puts an\ninteger into the shared buffer loops number of times, and a consumer\nthat gets the data out of that shared buffer (forever), each time printing\nout the data item it pulled from the shared buffer.\nNow imagine that we have just a single producer and a single consumer.\nthem, as put() updates the buffer, and get() reads from it.\n(broken) ﬁrst try (Figure 30.6), we have a single condition variable cond\nBuffer now full\nBuffer full; sleep\nLet’s examine the signaling logic between producers and consumers.\nWhen a producer wants to ﬁll the buffer, it waits for it to be empty (p1–\nThe consumer has the exact same logic, but waits for a different\nWith just a single producer and a single consumer, the code in Figure\ntwo consumers), the solution has two critical problems.\nAssume there are two consumers (Tc1 and Tc2) and\none producer (Tp).\nFirst, a consumer (Tc1) runs; it acquires the lock (c1),\nchecks if any buffers are ready for consumption (c2), and ﬁnding that\nnone are, waits (c3) (which releases the lock).\nThen the producer (Tp) runs.\nbuffers are full (p2), and ﬁnding that not to be the case, goes ahead and\nﬁlls the buffer (p4).\nThe producer then signals that a buffer has been\nCritically, this moves the ﬁrst consumer (Tc1) from sleeping\non a condition variable to the ready queue; Tc1 is now able to run (but\nThe producer then continues until realizing the buffer\nHere is where the problem occurs: another consumer (Tc2) sneaks in\nand consumes the one existing value in the buffer (c1, c2, c4, c5, c6, skip-\nping the wait at c3 because the buffer is full).\nthen calls get() (c4), but there are no buffers to consume!\nhave somehow prevented Tc1 from trying to consume because Tc2 snuck\nin and consumed the one value in the buffer that had been produced.\nPthread_cond_wait(&cond, &mutex); // p3\nPthread_cond_wait(&cond, &mutex); // c3\nFigure 30.7: Producer/Consumer: Single CV and While\nThe problem arises for a simple reason: after the producer woke Tc1,\nbut before Tc1 ever ran, the state of the bounded buffer changed (thanks to\nSignaling a thread only wakes them up; it is thus a hint that the state\nbuffer), but there is no guarantee that when the woken thread runs, the\nabout why this works; now consumer Tc1 wakes up and (with the lock\nbuffer is empty at that point, the consumer simply goes back to sleep\nThe corollary if is also changed to a while in the producer (p2).\nBuffer now full\ncurs when two consumers run ﬁrst (Tc1 and Tc2), and both go to sleep\nThen, a producer runs, put a value in the buffer, wakes one of the\nconsumers (say Tc1), and goes back to sleep.\nNow we have one consumer\nready to run (Tc1), and two threads sleeping on a condition (Tc2 and Tp).\nThe consumer Tc1 then wakes by returning from wait() (c3), re-checks\nthe condition (c2), and ﬁnding the buffer full, consumes the value (c4).\nThis consumer then, critically, signals on the condition (c5), waking one\nthread that is sleeping.\nBecause the consumer has emptied the buffer, it clearly should wake\nthe producer.\nHowever, if it wakes the consumer Tc2 (which is deﬁnitely\nSpeciﬁcally, the consumer Tc2 will wake up and ﬁnd the buffer\nThe producer Tp, which has a value\nto put into the buffer, is left sleeping.\nThe other consumer thread, Tc1,\nA consumer\nshould not wake other consumers, only producers, and vice-versa.\nFigure 30.8: Producer/Consumer: Two CVs and While\nThe Single Buffer Producer/Consumer Solution\nThe solution here is once again a small one: use two condition variables,\nIn the code above, producer threads wait on the condition empty, and\nConversely, consumer threads wait on ﬁll and signal empty.\nBy doing so, the second problem above is avoided by design: a consumer\ncan never accidentally wake a consumer, and a producer can never acci-\ndentally wake a producer.\nThe Final Producer/Consumer Solution\nWe now have a working producer/consumer solution, albeit not a fully\nefﬁciency; speciﬁcally, we add more buffer slots, so that multiple values\ncan be produced before sleeping, and similarly multiple values can be\nconsumed before sleeping.\nWith just a single producer and consumer, this\nproducers or consumers (or both), it even allows concurrent producing\nor consuming to take place, thus increasing concurrency.\nint buffer[MAX];\nbuffer[fill] = value;\nint tmp = buffer[use];\nPthread_cond_wait(&empty, &mutex); // p3\nThe ﬁrst change for this ﬁnal solution is within the buffer structure\nslightly change the conditions that producers and consumers check in or-\nA producer only sleeps if all buffers are cur-\nrently ﬁlled (p2); similarly, a consumer only sleeps if all buffers are cur-\nAnd thus we solve the producer/consumer problem.",
      "keywords": [
        "Buffer",
        "mutex",
        "consumer",
        "Producer",
        "sleep",
        "Ready",
        "CONDITION VARIABLES",
        "Pthread",
        "CONDITION",
        "Running",
        "int",
        "cond",
        "LOCK",
        "void",
        "count"
      ],
      "concepts": [
        "buffer",
        "consumer",
        "consume",
        "producer",
        "threads",
        "problem",
        "counts",
        "condition",
        "conditions",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 39,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 38,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 40,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "consumer",
          "producer",
          "buffer",
          "tc1",
          "producer consumer"
        ],
        "semantic": [],
        "merged": [
          "consumer",
          "producer",
          "buffer",
          "tc1",
          "producer consumer"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34459316700215653,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118269+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 373-387)",
      "start_page": 373,
      "end_page": 387,
      "summary": "further reason to re-check the condition a thread is waiting on.\nAs you might see in the code, when a thread calls into the memory\nHowever, our code above has a problem: which waiting\nnately, when it calls signal to wake a waiting thread, it might not wake\nthe correct waiting thread, Tb, which is waiting for only 10 bytes to be\nthe code in the ﬁgure does not work, as the thread waking other threads\npthread cond broadcast(), which wakes up all waiting threads.\nneedlessly wake up many other waiting threads that shouldn’t (yet) be\nThose threads will simply wake up, re-check the condition, and\nThe Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to\nSemaphores\none can use semaphores as both locks and condition variables.\nHow can we use semaphores instead of locks and condition variables?\nIs it straightforward to build a semaphore out of locks and condition\nsemaphores?\nsem wait() and sem post()1.\nBecause the initial value of the semaphore\nthe semaphore, we must ﬁrst initialize it to some value, as the code in\n1Historically, sem wait() was ﬁrst called P() by Dijkstra (for the Dutch word “to probe”)\nSEMAPHORES\nsem_t s;\nFigure 31.1: Initializing A Semaphore\nsemaphore is shared between threads in the same process.\ninteract with it, sem wait() or sem post().\nsem wait() and sem post(), there is the obvious need for managing\ncan see that sem wait() will either return right away (because the value\nof the semaphore was one or higher when we called sem wait()), or it\nOf course, multiple calling threads may call into sem wait(), and thus\nSecond, we can see that sem post() does not wait for some particular\ncondition to hold like sem wait() does.\nvalue of the semaphore and then, if there is a thread waiting to be woken,\nber of waiting threads [D68b].\nWe will soon use locks and condition variables to do just this.\nint sem_wait(sem_t *s) {\ndecrement the value of semaphore s by one\nwait if value of semaphore s is negative\nincrement the value of semaphore s by one\nif there are one or more threads waiting, wake one\nFigure 31.2: Semaphore: Deﬁnitions of Wait and Post\nSEMAPHORES\nsem_init(&m, 0, X); // initialize semaphore to X; what should X be?\nsem_wait(&m);\nFigure 31.3: A Binary Semaphore, a.k.a. a Lock\nBinary Semaphores (Locks)\nWe are now ready to use a semaphore.\nwhich we are already familiar: using a semaphore as a lock.\ncritical section of interest with a sem wait()/sem post() pair.\ncal to making this work, though, is the initial value of the semaphore m\nLooking back at deﬁnition of the sem wait() and sem post() rou-\nthread (Thread 0) calls sem wait(); it will ﬁrst decrement the value of\nThen, it will wait only if the value is\nnot greater than or equal to 0; because the value is 0, the calling thread\nIf no other thread tries to acquire the lock while Thread 0 is inside\nthe critical section, when it calls sem post(), it will simply restore the\nvalue of the semaphore to 1 (and not wake any waiting thread, because\nit has called sem wait() but not yet called sem post()), and another\nthread (Thread 1) tries to enter the critical section by calling sem wait().\nIn this case, Thread 1 will decrement the value of the semaphore to -1, and\nThread 0 runs again, it will eventually call sem post(), incrementing the\nvalue of the semaphore back to zero, and then wake the waiting thread\n(Thread 1), which will then be able to acquire the lock for itself.\nThread 1 ﬁnishes, it will again increment the value of the semaphore,\nValue of Semaphore\nThread 0\nThread 1\ncall sem wait()\nsem wait() returns\nTable 31.1: Thread Trace: Single Thread Using A Semaphore\nSEMAPHORES\nThread 0\nThread 1\ncall sem wait()\nsem wait() returns\ncall sem wait()\nsem wait() returns\nTable 31.2: Thread Trace: Two Threads Using A Semaphore\nthe table shows the scheduler state of each thread: Running, Ready (i.e.,\nmultiple threads queue up waiting for a lock.\nThus we are able to use semaphores as locks.\nthan discussed here; we instead use the generalized semaphore as a lock.\nSemaphores As Condition Variables\nSemaphores are also useful when a thread wants to halt its progress\nwaiting for a condition to become true.\nIn this pattern of usage, we often ﬁnd a thread waiting for something to\nsignaling that it has happened, thus waking the waiting thread.\nthe waiting thread (or threads) is waiting for some condition in the pro-\nSEMAPHORES\nsem_t s;\nsem_post(&s); // signal here: child is done\nsem_wait(&s); // wait here for child\nFigure 31.4: A Parent Waiting For Its Child\nthread and then wants to wait for it to complete its execution (Figure\nthe code, the parent simply calls sem wait() and the child sem post()\nto wait for the condition of the child ﬁnishing its execution to become\nthis semaphore be?\ncall sem wait() before the child has called sem post(); we’d like the\nparent to wait for the child to run.\nThe parent runs, decrements the semaphore (to -1), then waits (sleeping).\nWhen the child ﬁnally runs, it will call sem post(), increment the value\nof the semaphore to 0, and wake the parent, which will then return from\nsem wait() and ﬁnish the program.\ntion before the parent gets a chance to call sem wait().\nthe child will ﬁrst call sem post(), thus incrementing the value of the\nsemaphore from 0 to 1.\nwill call sem wait() and ﬁnd the value of the semaphore to be 1; the\nparent will thus decrement the value (to 0) and return from sem wait()\nSEMAPHORES\ncall sem wait()\nsem wait() returns\nTable 31.3: Thread Trace: Parent Waiting For Child (Case 1)\ncall sem wait()\nsem wait() returns\nTable 31.4: Thread Trace: Parent Waiting For Child (Case 2)\nIn this example, the producer ﬁrst waits for a buffer to become empty\nin order to put data into it, and the consumer similarly waits for a buffer\nImagine again there are two threads, a producer and a consumer.\ncalling sem wait(&full).\nSEMAPHORES\nsem_t empty;\nsem_t full;\nsem_wait(&empty);\nsem_wait(&full);\nthe call will decrement full (to -1), block the consumer, and wait for\nanother thread to call sem post() on full, as desired.\nsem wait(&empty) routine.\nproducer will put a data value into the ﬁrst entry of buffer (line P2).\nproducer will then continue on to P3 and call sem post(&full), chang-\ning the value of the full semaphore from -1 to 0 and waking the consumer\nSEMAPHORES\never, it would block, as the empty semaphore’s value is 0.\nsem wait(&full) (line c1) and ﬁnd that the buffer was indeed full and\nbinary semaphore and add some locks.\nquires the mutex (line c0), and then calls sem wait() on the full semaphore\nUnfortunately, the ﬁrst thing it does is call sem wait() on the binary\nmutex semaphore (line p0).\nSEMAPHORES\nsem_t empty;\nsem_t full;\nsem_wait(&mutex);\nsem_wait(&empty);\nsem_post(&mutex);\nsem_wait(&mutex);\nsem_wait(&full);\nsem_post(&mutex);\nwaiting for the someone to signal full.\nis waiting for the mutex.\nSEMAPHORES\nsem_t empty;\nsem_t full;\nsem_wait(&empty);\nsem_wait(&mutex);\nsem_post(&mutex);\nsem_wait(&full);\nsem_wait(&mutex);\nsem_post(&mutex);\nSEMAPHORES\nsem_t lock;\n// binary semaphore (basic lock)\nsem_init(&rw->lock, 0, 1);\nsem_wait(&rw->lock);\nsem_wait(&rw->writelock); // first reader acquires writelock\nsem_post(&rw->lock);\nsem_wait(&rw->lock);\nsem_post(&rw->writelock); // last reader releases writelock\nsem_post(&rw->lock);\nsem_wait(&rw->writelock);\nsem wait() on the writelock semaphore, and then ﬁnally releasing\nthe lock by calling sem post().\nallowed to acquire the read lock too; however, any thread that wishes to\nacquire the write lock will have to wait until all readers are ﬁnished; the\nthus enables a waiting writer to acquire the lock.",
      "keywords": [
        "sem",
        "sem wait",
        "sem post",
        "call sem wait",
        "call sem post",
        "thread",
        "semaphore",
        "call sem",
        "wait",
        "post",
        "line",
        "lock",
        "Running",
        "Ready sem post",
        "ready"
      ],
      "concepts": [
        "thread",
        "semaphores",
        "lock",
        "runs",
        "running",
        "run",
        "void",
        "waiting",
        "line",
        "reader"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 39,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 37,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sem",
          "sem wait",
          "semaphore",
          "wait",
          "post"
        ],
        "semantic": [],
        "merged": [
          "sem",
          "sem wait",
          "semaphore",
          "wait",
          "post"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30653049046900605,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118342+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 388-398)",
      "start_page": 388,
      "end_page": 398,
      "summary": "the lock once a writer is waiting.\nsemaphores in an interesting and useful way.\nOne of the most famous concurrency problems posed, and solved, by\nDijkstra, is known as the dining philosopher’s problem [DHO71].\nIn order to eat, a philosopher needs two forks, both\nforks, and the synchronization problems that ensue, are what makes this\na problem we study in concurrent programming.\nputforks() such that there is no deadlock, no philosopher starves and\nnever gets to eat, and concurrency is high (i.e., as many philosophers can\nWhen philosopher p wishes to refer to the fork on their left, they sim-\nSimilarly, the fork on the right of a philosopher p is\nthe one case where the last philosopher (p=4) tries to grab the fork on\nWe’ll also need some semaphores to solve this problem.\nsem_wait(forks[left(p)]);\nsem_wait(forks[right(p)]);\nsem_post(forks[left(p)]);\nsem_post(forks[right(p)]);\nWe attempt our ﬁrst solution to the problem.\nsemaphore (in the forks array) to a value of 1.\nsee the problem that arises?\nThe problem is deadlock.\nIf each philosopher happens to grab the fork\non their left before any philosopher can grab the fork on their right, each\nall the forks are acquired, and all the philosophers are stuck waiting for\na fork that another philosopher possesses.\nThe simplest way to attack this problem is to change how forks are ac-\nhimself solved the problem.\nsem_wait(forks[right(p)]);\nsem_wait(forks[left(p)]);\nsem_wait(forks[left(p)]);\nsem_wait(forks[right(p)]);\nBecause the last philosopher tries to grab right before left, there is no\nsituation where each philosopher grabs one fork and is stuck waiting for\npthread_mutex_t lock;\nMutex_init(&s->lock);\nCond_wait(&s->cond, &s->lock);\nFigure 31.12: Implementing Zemaphores with Locks and CVs\ncondition variables, to build our own version of semaphores called ...\nAs you can see from the ﬁgure, we use just one lock and one condition\nvariable, plus a state variable to track the value of the semaphore.\nvalue of the semaphore, when negative, reﬂects the number of waiting\nOne could view semaphores as a generalization of locks and condition\nﬁculty of realizing a condition variable on top of a semaphore, perhaps\nCuriously, building locks and condition variables out of semaphores\nbuilding condition variables out of semaphores is more challenging than\nbook on concurrency and programming with semaphores [D08].\ning of both semaphores in speciﬁc and concurrency in general.\nof concurrent programming; Birrell, for example, is known for (among other things) writing various\nin concurrent code.\nThe introduction of the reader-writer problem, and a simple solution.\nCommunications of the ACM, volume 11(3): pages 147148, March 1968\nCommunications of the ACM, volume 11(5), pages 341346, 1968\nwhat the problems were in concurrent code.\nPresents numerous concurrency problems, including the Dining Philosophers.\nabout this problem is also quite informative.\nHill’s dissertation work, for those obsessed with caching in early systems.\nCommon Concurrency Problems\nother types of common concurrency bugs (i.e., non-deadlock bugs).\nthis chapter, we take a brief look at some example concurrency problems\nfound in real code bases, to better understand what problems to look out\nAnd thus our problem:\nCRUX: HOW TO HANDLE COMMON CONCURRENCY BUGS\nConcurrency bugs tend to come in a variety of common patterns.\nrency bugs manifest in complex, concurrent programs?\nunderstand what types of bugs arise in practice.\nIn the study, the authors examine concurrency bugs that have been found\nderstand what types of problems actually occur in mature code bases.\nCOMMON CONCURRENCY PROBLEMS\nTable 32.1 shows a summary of the bugs Lu and colleagues studied.\nyou can see that the number of bugs studied from each application; while\nOpenOfﬁce only had 8 total concurrency bugs, Mozilla had nearly 60.\nWe now dive into these different classes of bugs (non-deadlock, dead-\nFor the ﬁrst class of non-deadlock bugs, we use\ndeadlock bugs, we discuss the long line of work that has been done in\nNon-Deadlock Bugs\nNon-deadlock bugs make up a majority of concurrency bugs, accord-\nBut what types of bugs are these?\ndeadlock bugs found by Lu et al.: atomicity violation bugs and order\nviolation bugs.\nAtomicity-Violation Bugs\nThe ﬁrst type of problem encountered is referred to as an atomicity vi-\nIn the example, two different threads access the ﬁeld proc info in\nThe ﬁrst thread checks if the value is non-NULL and\nthen prints its value; the second thread sets it to NULL.\nCOMMON CONCURRENCY PROBLEMS\nassumption is broken, the code will not work as desired.\nIn this solution, we simply add locks around the shared-variable ref-\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_lock(&lock);\npthread_mutex_unlock(&lock);\npthread_mutex_lock(&lock);\npthread_mutex_unlock(&lock);\nOrder-Violation Bugs\nAnother common type of non-deadlock bug found by Lu et al.\nyou can ﬁgure out why the code below has a bug in it.\nAs you probably ﬁgured out, the code in Thread 2 seems to assume\nCOMMON CONCURRENCY PROBLEMS\nThe ﬁx to this type of bug is generally to enforce ordering.\npthread_mutex_lock(&mtLock);\n// wait for the thread to be initialized...\npthread_mutex_lock(&mtLock);\nIn this ﬁxed-up code sequence, we have added a lock (mtLock) and\nWhen the initialization code runs, it sets the state of mtInit\ncondition variables (or semaphores) can come to the rescue.",
      "keywords": [
        "bugs",
        "SEMAPHORES",
        "lock",
        "Common Concurrency Problems",
        "concurrency problems",
        "philosopher",
        "problems",
        "thread",
        "forks",
        "concurrency bugs",
        "concurrency",
        "pthread",
        "mutex",
        "code",
        "common concurrency bugs"
      ],
      "concepts": [
        "concurrent",
        "bugs",
        "bug",
        "semaphores",
        "locking",
        "thread",
        "worked",
        "philosophers",
        "implement",
        "implementation"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 38,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 37,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bugs",
          "forks",
          "concurrency",
          "philosopher",
          "deadlock"
        ],
        "semantic": [],
        "merged": [
          "bugs",
          "forks",
          "concurrency",
          "philosopher",
          "deadlock"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3061357941204953,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118394+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 399-406)",
      "start_page": 399,
      "end_page": 406,
      "summary": "Non-Deadlock Bugs: Summary\nA large fraction (97%) of non-deadlock bugs studied by Lu et al.\nDeadlock Bugs\narises in many concurrent systems with complex locking protocols is known\nas deadlock.\nDeadlock occurs, for example, when a thread (say Thread\n1) is holding a lock (L1) and waiting for another one (L2); unfortunately,\nthe thread (Thread 2) that holds lock L2 is waiting for L1 to be released.\nlock(L1);\nlock(L2);\nlock(L2);\nlock(L1);\nNote that if this code runs, deadlock does not necessarily occur; rather,\nit may occur, if, for example, Thread 1 grabs lock L1 and then a context\nAt that point, Thread 2 grabs L2, and tries to\nThus we have a deadlock, as each thread is waiting for the\nwrite code so as to handle deadlock in some way?\nrecover from deadlock?\nWhy Do Deadlocks Occur?\nlocks in the same order, the deadlock would never arise.\nlocks happen?\nLock L1\nLock L2\nInternally, because the method needs to be multi-thread safe, locks for\nThe routine acquires said locks in some arbitrary order (say v1\nConditions for Deadlock\nFour conditions need to hold for a deadlock to occur [C+71]:\nthey require (e.g., a thread grabs a lock).\n• Hold-and-wait: Threads hold resources allocated to them (e.g., locks\neach thread holds one more resources (e.g., locks) that are being\none approach to handling the deadlock problem.\nis used frequently) is to write your locking code such that you never in-\nlock acquisition.\nFor example, if there are only two locks in the system (L1\nand L2), we can prevent deadlock by always acquiring L1 before L2.\nstrict ordering ensures that no cyclical wait arises; hence, no deadlock.\ndeadlock.\nThe hold-and-wait requirement for deadlock can be avoided by acquiring\nall locks at once, atomically.\nlock(prevention);\nlock(L1);\nlock(L2);\nBy ﬁrst grabbing the lock prevention, this code guarantees that no\nuntimely thread switch can occur in the midst of lock acquisition and thus\ndeadlock can once again be avoided.\nany thread grabs a lock, it ﬁrst acquires the global prevention lock.\nexample, if another thread was trying to grab locks L1 and L2 in a dif-\nlock while doing so.\nwhen calling a routine exactly which locks must be held and to acquire\nas all locks must be acquired early on (at once) instead of when they are\nlock acquisition often gets us into trouble because when waiting for one\nlock we are holding another.\nroutine will grab the lock (if it is available) or return -1 indicating that the\ngrab that lock.\nSuch an interface could be used as follows to build a deadlock-free,\nlock(L1);\nlocks in the other order (L2 then L1) and the program would still be dead-\nlock free.\nfor example, if after acquiring L1, the code had allocated some memory,\nexplicit locking.\nInstead of acquiring a lock, doing the update, and then releasing it, we\nno lock is acquired, and no deadlock can arise (though livelock is still a\ncourse, we could solve this by surrounding this code with a lock acquire\nlock(listlock);\nIn this solution, we are using locks in the traditional manner1.\nDeadlock Avoidance via Scheduling\nInstead of deadlock prevention, in some scenarios deadlock avoidance\nAvoidance requires some global knowledge of which locks\nules said threads in a way as to guarantee no deadlock can occur.\n1 (T1) grabs locks L1 and L2 (in some order, at some point during its\nlocks at all.\nWe can show these lock acquisition demands of the threads\nnot run at the same time, no deadlock could ever arise.\nthough T3 grabs lock L2, it can never cause a deadlock by running con-\ncurrently with other threads because it only grabs one lock.\nfor the same resources (again, locks L1 and L2), as indicated by the fol-\nIn particular, threads T1, T2, and T3 all need to grab both locks L1 and\nguarantees that no deadlock could ever occur:\nhave been possible to run these tasks concurrently, the fear of deadlock\nThus, avoidance of deadlock via scheduling is\nThe ﬁrst type, non-deadlock bugs, are surprisingly\ntion in practice is to be careful, develop a lock acquisition total order,\nand thus prevent deadlock from occurring in the ﬁrst place.\nwithout any locks whatsoever.",
      "keywords": [
        "COMMON CONCURRENCY PROBLEMS",
        "Deadlock",
        "lock",
        "thread",
        "CONCURRENCY PROBLEMS",
        "COMMON CONCURRENCY",
        "CONCURRENCY",
        "code",
        "lock acquisition",
        "systems",
        "Bugs",
        "COMMON",
        "PROBLEMS",
        "occur",
        "Non-Deadlock Bugs"
      ],
      "concepts": [
        "deadlock",
        "locking",
        "thread",
        "problems",
        "order",
        "ordering",
        "code",
        "prevent",
        "prevention",
        "free"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 33,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "deadlock",
          "lock",
          "l1",
          "l2",
          "locks"
        ],
        "semantic": [],
        "merged": [
          "deadlock",
          "lock",
          "l1",
          "l2",
          "locks"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2892883504776558,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118443+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 407-414)",
      "start_page": 407,
      "end_page": 414,
      "summary": "Herlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs.\nEvent-based Concurrency (Advanced)\nconcurrent applications is to use threads.\ngramming is often used in both GUI-based applications [O96] as well as\nThis style, known as event-based\nThe problem that event-based concurrency addresses is two-fold.\nHOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS\nHow can we build a concurrent server without using threads, and thus\nThe Basic Idea: An Event Loop\nThe basic approach we’ll use, as stated above, is called event-based\nEVENT-BASED CONCURRENCY (ADVANCED)\nevent it is and do the small amount of work it requires (which may in-\nclude issuing I/O requests, or scheduling other events for future han-\nevent-based server looks like.\nPseudocode for an event loop\nfor (e in events)\n(by calling getEvents() in the code above) and then, for each event re-\nan event-based server determine which events are taking place, in par-\nevent server tell if a message has arrived for it?\nWith that basic event loop in mind, we next must address the question\nfd_set *restrict readfds,\nThe actual description from the man page: select() examines the I/O de-\nEVENT-BASED CONCURRENCY (ADVANCED)\ncall must read from disk in order to complete, it might block, waiting for\nthe I/O request that has been sent to the disk to return.\nwith threads), but are essential in the event-based approach, as a call that\nchecked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor\nOn return, select() replaces the given descriptor sets with\nselect() returns the total number of ready descriptors in all the sets.\nset the timeout to NULL, which causes select() to block indeﬁnitely,\nevent loop, which simply checks for incoming packets, reads from sockets\nFD ZERO() macro to ﬁrst clear the set of ﬁle descriptors, and then uses\nFD SET() to include all of the ﬁle descriptors from minFD to maxFD in\nEVENT-BASED CONCURRENCY (ADVANCED)\n// initialize the fd_set to all zero\nfd_set readFDs;\nBy then using FD ISSET() in a loop, the event server can see\ngeneral ﬂow of event-based servers [PDZ99, WCB01].\nWith a single CPU and an event-based application, the problems found\none event is being handled at a time, there is no need to acquire or release\nlocks; the event-based server cannot be interrupted by another thread be-\nthreaded programs do not manifest in the basic event-based approach.\nEVENT-BASED CONCURRENCY (ADVANCED)\nTIP: DON’T BLOCK IN EVENT-BASED SERVERS\nEvent-based servers enable ﬁne-grained control over scheduling of tasks.\nblocked event-based server, frustrated clients, and serious questions as to\nThus far, event-based programming sounds great, right?\na simple loop, and handle events as they arise.\nBut there is an issue: what if an event requires that\nread a ﬁle from disk and return its contents to the requesting client (much\nTo service such a request, some event han-\nBoth the open() and read() calls may issue I/O requests to the stor-\nWith a thread-based server, this\nis no issue: while the thread issuing the I/O request suspends (waiting\nfor the I/O to complete), other threads can run, thus enabling the server\nWith an event-based approach, however, there are no other threads to\nrun: just the main event loop.\nissues a call that blocks, the entire server will do just that: block until the\nWhen the event loop blocks, the system sits idle, and thus\nobeyed in event-based systems: no blocking calls are allowed.\nduced new ways to issue I/O requests to the disk system, referred to\nto issue an I/O request and return control immediately to the caller, be-\nfore the I/O has completed; additional interfaces enable an application to\nEVENT-BASED CONCURRENCY (ADVANCED)\nTo issue an asynchronous read to a ﬁle, an application should ﬁrst\nchronous call to read the ﬁle; on Mac OS X, this API is simply the asyn-\nint aio_read(struct aiocb *aiocbp);\nThis call tries to issue the I/O; if successful, it simply returns right\naway and the application (i.e., the event-based server) can continue with\nto aio error() to determine whether said I/O has yet completed.\nan I/O has completed; if a program has tens or hundreds of I/Os issued\nTo remedy this issue, some systems provide an approach based on the",
      "keywords": [
        "Event-based Concurrency",
        "Event",
        "Event-based",
        "server",
        "CONCURRENCY",
        "Event Loop",
        "System",
        "event-based server",
        "select",
        "call",
        "system call",
        "AIO",
        "read",
        "Loop",
        "descriptors"
      ],
      "concepts": [
        "event",
        "servers",
        "concurrency",
        "concurrent",
        "select",
        "threads",
        "aio",
        "paper",
        "blocking",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 42,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 31,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 35,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 34,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "event",
          "event based",
          "based",
          "server",
          "loop"
        ],
        "semantic": [],
        "merged": [
          "event",
          "event based",
          "based",
          "server",
          "loop"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35767591783056774,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118504+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 415-422)",
      "start_page": 415,
      "end_page": 422,
      "summary": "EVENT-BASED CONCURRENCY (ADVANCED)\nSpeciﬁcally, a signal can be delivered to an application; doing so stops the\napplication from whatever it is doing to run a signal handler, i.e., some code in\nthe application to handle that signal.\nto signal names is common); if your program is conﬁgured to catch that signal,\nured to handle that signal, some default behavior is enacted; for SEGV, the process\nand run the handler code handle():\nThere is a lot more to learn about signals, so much that a single page, much\nEVENT-BASED CONCURRENCY (ADVANCED)\nIn systems without asynchronous I/O, the pure event-based approach\ndescribe a hybrid approach in which events are used to process network\npackets, and a thread pool is used to manage outstanding I/Os. Read\nAnother issue with the event-based approach is that such code is gen-\nerally more complicated to write than traditional thread-based code.\nreason is as follows: when an event handler issues an asynchronous I/O,\nit must package up some program state for the next event handler to use\nthread-based programs, as the state the program needs is on the stack of\nis fundamental to event-based programming [A+02].\nwhich a thread-based server needs to read from a ﬁle descriptor (fd) and,\nonce complete, write the data that it read from the ﬁle to a network socket\nAs you can see, in a multi-threaded program, doing this kind of work\nis trivial; when the read() ﬁnally returns, the code immediately knows\nIn an event-based system, life is not so easy.\ncomplete, how does the event-based server know what to do?\nneeded information to ﬁnish processing this event in some data struc-\nture; when the event happens (i.e., when the disk I/O completes), look\nup the needed information and process the event.\nWhen the disk I/O completes, the event han-\nEVENT-BASED CONCURRENCY (ADVANCED)\nWhat Is Still Difﬁcult With Events\nThere are a few other difﬁculties with the event-based approach that\nCPU to multiple CPUs, some of the simplicity of the event-based ap-\nthe event server has to run multiple event handlers in parallel; when do-\nern multicore systems, simple event handling without locks is no longer\nAnother problem with the event-based approach is that it does not\nexample, if an event-handler page faults, it will block, and thus the server\nA third issue is that event-based code can be hard to manage over time,\nif a routine changes from non-blocking to blocking, the event handler\nevent-based servers, a programmer must always be on the lookout for\nsuch changes in the semantics of the APIs each event uses.\ncurrency based on events.\nEvent-based servers give control of schedul-\nbest; thus, both threads and events are likely to persist as two different\nRead some research papers (e.g., [A+02, PDZ99, vB+03, WCB01]) or bet-\nter yet, write some event-based code, to learn more.\nEVENT-BASED CONCURRENCY (ADVANCED)\nThis gem of a paper is the ﬁrst to clearly articulate some of the difﬁculties of event-based concurrency,\nconcurrency management into a single application!\nA great talk about how threads aren’t a great match for GUI-based applications (but the ideas are more\nlanguage and toolkit that made it 100x easier to develop GUI-based applications than the state of the\nA paper about how to make threads work at extreme scale; a counter to all the event-based work ongoing\nA nice twist on event-based serving that combines threads, queues, and event-based hanlding into one\nProfessor: Indeed it is.\nStudent: (gasps) Professors can be ...\nBut if concurrent code is so hard to think about,\nand so hard to get right, how are we supposed to write correct concurrent code?\nand use well-known and tried-and-true ways to manage thread interactions.\nStudent: I see – why add threads if you don’t need them?\nTime to write some more concurrent code...",
      "keywords": [
        "EVENT-BASED",
        "EVENT-BASED CONCURRENCY",
        "code",
        "Read",
        "signal",
        "event",
        "systems",
        "CONCURRENCY",
        "program",
        "write",
        "threads",
        "event-based approach",
        "simple",
        "event-based code",
        "signal handler"
      ],
      "concepts": [
        "thread",
        "program",
        "programming",
        "based",
        "student",
        "event",
        "professor",
        "systems",
        "signals",
        "simple"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 41,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 30,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 36,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 32,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 37,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "event",
          "event based",
          "based",
          "signal",
          "handler"
        ],
        "semantic": [],
        "merged": [
          "event",
          "event based",
          "based",
          "signal",
          "handler"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36927199479185496,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118565+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 423-434)",
      "start_page": 423,
      "end_page": 434,
      "summary": "I/O Devices\ntence), we ﬁrst introduce the concept of an input/output (I/O) device and\ndevices are connected to the system via a general I/O bus, which in many\nics and some other higher-performance I/O devices might be found here.\nThese connect the slowest devices to\nmuch room to plug devices and such into it.\nI/O DEVICES\nother slow devices on a peripheral bus are manifold; in particular, you\nA Canonical Device\nLet us now look at a canonical device (not a real one), and use this\ndevice to drive our understanding of some of the machinery required\nto make device interaction efﬁcient.\ndevice has two important components.\nThus, all devices have some speciﬁed interface\nThe second part of any device is its internal structure.\ning the abstraction the device presents to the system.\nVery simple devices\nmore complex devices will include a simple CPU, some general purpose\nmemory, and other device-speciﬁc chips to get their job done.\nlines of ﬁrmware (i.e., software within a hardware device) to implement\nI/O DEVICES\nFigure 36.2: A Canonical Device\nIn the picture above, the (simpliﬁed) device interface is comprised of\ntus of the device; a command register, to tell the device to perform a cer-\ntain task; and a data register to pass data to the device, or get data from\nthe device.\ncan control device behavior.\n; // wait until device is not busy\n(Doing so starts the device and executes the command)\n; // wait until device is done with your request\nIn the ﬁrst, the OS waits until the device is\ncall this polling the device (basically, just asking it what is going on).\nond, the OS sends some data down to the data register; one can imagine\ntake place to transfer a disk block (say 4KB) to the device.\nCPU is involved with the data movement (as in this example protocol),\nto the command register; doing so implicitly lets the device know that\nFinally, the OS waits for the device to ﬁnish by again polling it\nthe (potentially slow) device to complete its activity, instead of switching\nI/O DEVICES\nHow can the OS check device status without frequent polling, and\nthus lower the CPU overhead required to manage the device?\nof polling the device repeatedly, the OS can issue a request, put the call-\nWhen the device\nample, by reading data and perhaps an error code from the device) and\na repeated 1 on the CPU line), and then issues an I/O request to the disk\nthe status of the device repeatedly until the I/O is complete (indicated by\nThe disk services the request and ﬁnally Process 1 can run again.\nIf instead we utilize interrupts and allow for overlap, the OS can do\nIn this example, the OS runs Process 2 on the CPU while the disk ser-\nWhen the disk request is ﬁnished, an interrupt\nimagine a device that performs its tasks very quickly: the ﬁrst poll usually\nﬁnds the device to be done with task.\na device is fast, it may be best to poll; if it is slow, interrupts, which allow\nI/O DEVICES\nreally make sense for slow devices.\nIf the speed of the device is not known, or sometimes\nlittle while and then, if the device is not yet ﬁnished, uses interrupts.\ndevice which needs to raise an interrupt ﬁrst waits for a bit before deliv-\nto transfer a large chunk of data to a device, the CPU is once again over-\nIn the timeline, Process 1 is running and then wishes to write some data to\nto the device explicitly, one word at a time (marked c in the diagram).\nWhen the copy is complete, the I/O begins on the disk and the CPU can\nI/O DEVICES\ndevices by hand.\nA DMA engine is essentially a very speciﬁc device\nwithin a system that can orchestrate transfers between devices and main\nTo transfer data to the device, for example, the\nmemory, how much data to copy, and which device to send it to.\nMethods Of Device Interaction\nincorporate devices into modern systems.\ntually communicates with the device!\nTHE CRUX: HOW TO COMMUNICATE WITH DEVICES\nHow should the hardware communicate with a device?\nOver time, two primary methods of device communication have de-\nway for the OS to send data to speciﬁc device registers and thus allow the\nI/O DEVICES\nmunicate with devices.\nFor example, to send data to a device, the caller\ndevice.\nThe OS controls devices, and\nThe second method to interact with devices is known as memory-\nWith this approach, the hardware makes device registers\nthe OS issues a load (to read) or store (to write) the address; the hardware\nthen routes the load/store to the device instead of main memory.\nFitting Into The OS: The Device Driver\nOne ﬁnal problem we will discuss: how to ﬁt devices, each of which\nTHE CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS\nHow can we keep most of the OS device-neutral, thus hiding the de-\ntails of device interactions from major OS subsystems?\nhow a device works.\nWe call this piece of software a device driver, and\nany speciﬁcs of device interaction are encapsulated within.\nit is using; it simply issues block read and write requests to the generic\nblock layer, which routes them to the appropriate device driver, which\nI/O DEVICES\nDevice Driver [SCSI, ATA, etc.]\ndevices, which have very rich error reporting; because other block de-\nInterestingly, because device drivers are needed for any device you\nover 70% of OS code is found in device drivers [C01]; for Windows-based\nhas millions of lines of device-driver code.\nstallation, most of that code may not be active (i.e., only a few devices are\nregisters are available by reading or writing to speciﬁc “I/O addresses”\nI/O DEVICES\nThe basic protocol to interact with the device is as follows, assuming\nRead Status Register (0x1F7) until drive\nby issuing read/write to command register.\n• Data transfer (for writes): Wait until drive status is READY and\nDRQ (drive request for data); write data to data port.\nor issues it directly to the disk (via ide start request()); in either\nI/O DEVICES\nstatic void ide_start_request(struct buf *b) {\nide_start_request(b);\nif ((ide_queue = b->qnext) != 0) // start next request\nused to send a request (and perhaps data, in the case of a write) to the\ndisk; the in and out x86 instructions are called to read and write device\nide wait ready(), to ensure the drive is ready before issuing a request\nreads data from the device (if the request is a read, not a write), wakes the\nprocess waiting for the I/O to complete, and (if there are more requests\nin the I/O queue), launches the next I/O via ide start request().",
      "keywords": [
        "device",
        "CPU",
        "IDE",
        "Data",
        "system",
        "disk",
        "request",
        "register",
        "Write",
        "Device Driver",
        "device registers",
        "process",
        "interrupt",
        "read",
        "IDE Disk"
      ],
      "concepts": [
        "devices",
        "interrupting",
        "disk",
        "cpu",
        "data",
        "error",
        "writing",
        "write",
        "professor",
        "block"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 44,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.588,
          "base_score": 0.438,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 46,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "device",
          "devices",
          "data device",
          "request",
          "data"
        ],
        "semantic": [],
        "merged": [
          "device",
          "devices",
          "data device",
          "request",
          "data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34529252478680655,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118626+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 435-443)",
      "start_page": 435,
      "end_page": 443,
      "summary": "A nice summary of a simple IDE disk drive’s interface and how to build a device driver for it.\nHard Disk Drives\nwe dive into more detail about one device in particular: the hard disk\nis worth understanding the details of a disk’s operation before building\nCRUX: HOW TO STORE AND ACCESS DATA ON DISK\nHow do modern hard-disk drives store data?\nLet’s start by understanding the interface to a modern disk drive.\nThe sectors are numbered from 0 to n −1 on a disk with n\nThus, we can view the disk as an array of sectors; 0 to n −1 is\ndisk, the only guarantee drive manufactures make is that a single 512-\nHARD DISK DRIVES\nFigure 37.1: A Disk With Just A Single Track\nThere are some assumptions most clients of disk drives make, but\ncalled this the “unwritten contract” of disk drives [SG04].\nLet’s start to understand some of the components of a modern disk.\nA disk may have one\ntime of a single rotation, e.g., a drive that rotates at 10,000 RPM means\nus to either sense (i.e., read) the magnetic patterns on the disk or to in-\naccomplished by the disk head; there is one such head per surface of the\nThe disk head is attached to a single disk arm, which moves across\nthe surface to position the head over the desired track.\nA Simple Disk Drive\nLet’s understand how disks work by building up a model one track at\nAssume we have a simple disk with a single track (Figure 37.1).\nHARD DISK DRIVES\nFigure 37.2: A Single Track Plus A Head\nThis track has just 12 sectors, each of which is 512 bytes in size (our\nwant to be able to read or write those sectors, and thus we need a disk\nhead, attached to a disk arm, as we now see (Figure 37.2).\nIn the ﬁgure, the disk head, attached to the end of the arm, is posi-\nSingle-track Latency: The Rotational Delay\ntrack disk, imagine we now receive a request to read block 0.\nthe disk service this request?\nmust just wait for the desired sector to rotate under the disk head.\nple, if the full rotational delay is R, the disk has to incur a rotational delay\n2 to wait for 0 to come under the read/write head (if we start at\nA worst-case request on this single track would be to sector 5, causing\nMultiple Tracks: Seek Time\nSo far our disk just has a single track, which is not too realistic; modern\nmore realistic disk surface, this one with three tracks (Figure 37.3, left).\n(which contains sectors 24 through 35); the next track over contains the\nnext set of sectors (12 through 23), and the outermost track contains the\nHARD DISK DRIVES\nFigure 37.3: Three Tracks Plus A Head (Right: With Seek)\nTo understand how the drive might access a given sector, we now trace\nTo service this read, the drive has to ﬁrst move the disk arm to the cor-\nSeeks, along with rotations, are one of the most costly disk operations.\nAfter the seek, the disk arm has positioned the head over the right\ntrack.\ntrack, and the platter of course has rotated, in this case about 3 sectors.\nThus, sector 9 is just about to pass under the disk head, and we must\nWhen sector 11 passes under the disk head, the ﬁnal phase of I/O\nesting details about how hard drives operate.\nkind of track skew to make sure that sequential reads can be properly\nHARD DISK DRIVES\ntrack to another, the disk needs time to reposition the head (even to neigh-\ntrack but the desired next block would have already rotated under the\nhead, and thus the drive would have to wait almost the entire rotational\nThese tracks are often referred to as multi-zoned disk drives,\nFinally, an important part of any modern disk drive is its cache, for\ncan use to hold data read from or written to the disk.\nreading a sector from the disk, the drive might decide to read in all of the\nsectors on that track and cache them in its memory; doing so allows the\ndrive to quickly respond to any subsequent requests to the same track.\nactually been written to disk?\ndisk in a certain order for correctness, write-back caching can lead to",
      "keywords": [
        "Disk",
        "Hard Disk Drives",
        "Disk Drives",
        "Track",
        "disk head",
        "drive",
        "Hard Disk",
        "modern disk drive",
        "head",
        "sectors",
        "disk arm",
        "Single Track",
        "read",
        "device",
        "IDE disk drive"
      ],
      "concepts": [
        "track",
        "disk",
        "drives",
        "sectors",
        "devices",
        "hardly",
        "interrupts",
        "ideas",
        "write",
        "writing"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 46,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 45,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 43,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "disk",
          "head",
          "track",
          "drive",
          "drives"
        ],
        "semantic": [],
        "merged": [
          "disk",
          "head",
          "track",
          "drive",
          "drives"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2898977413871324,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118678+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 444-451)",
      "start_page": 444,
      "end_page": 451,
      "summary": "HARD DISK DRIVES\nliseconds, a single rotation of a disk takes.\nonly the RPM of the disk, or rotations per minute.\ntalking about a 10K RPM disk (i.e., it rotates 10,000 times per minute).\nexample, you will often be given the transfer rate of a disk, e.g.,\n1 Request =\nRequest\nI/O Time: Doing The Math\nNow that we have an abstract model of the disk, we can use a little\nanalysis to better understand disk performance.\nHARD DISK DRIVES\nTable 37.1: Disk Drive Specs: SCSI Versus SATA\nTo get a better feel for I/O time, let us perform the following calcu-\nlocations on the disk.\nquential workloads, we need to make a few assumptions about the disk\nLet’s look at a couple of modern disks from Seagate.\nin many ways nicely summarize two important components of the disk\ndrives are engineered to spin as fast as possible, deliver low seek times,\nlocation on disk, we can calculate how long each such read would take.\nHARD DISK DRIVES\nTIP: USE DISKS SEQUENTIALLY\nWhen at all possible, transfer data to and from disks in a sequential man-\nyour careless random I/Os. The average seek time (4 milliseconds) is just taken as the average time\nOn average, the disk will encounter a half rotation and thus 2 ms is the\nTo compute the rate of I/O, we just divide the size of the transfer\nby the average time, and thus arrive at RI/O for the Cheetah under the\nis a single seek and rotation before a very long transfer.\nTable 37.2: Disk Drive Performance: SCSI Versus SATA\nHARD DISK DRIVES\nIn many books and papers, you will see average disk-seek time cited\nas being roughly one-third of the full seek time.\nImagine the disk as a set of tracks, from 0 to N.\nseek distance between any two tracks x and y is thus computed as the\nTo compute the average seek distance, all you need to do is to ﬁrst add\nx=0\nThen, divide this by the number of different possible seeks: N 2.\nx=0\nx=0\n2 x)\n(N 2) to compute the average seek distance: ( N3\naverage seek distance on a disk, over all possible seeks, is one-third the\nHARD DISK DRIVES\nFigure 37.5: SSTF: Scheduling Requests 21 And 2\nDisk Scheduling\ndeciding the order of I/Os issued to the disk.\nset of I/O requests, the disk scheduler examines the requests and decides\nknown, with disk scheduling, we can make a good guess at how long\na “job” (i.e., disk request) will take.\nthe rotational delay of a request, the disk scheduler can know how long\nleast time to service ﬁrst.\nThus, the disk scheduler will try to follow the\nSSTF: Shortest Seek Time First\nOne early disk scheduling approach is known as shortest-seek-time-ﬁrst\n(SSTF) (also called shortest-seek-ﬁrst or SSF).\nI/O requests by track, picking requests on the nearest track to complete\n(outer track), we would then issue the request to 21 ﬁrst, wait for it to\nSSTF works well in this example, seeking to the middle track ﬁrst and\nwhich schedules the request with the nearest block address next.\nHARD DISK DRIVES\nRequests to any other\nCRUX: HOW TO HANDLE DISK STARVATION\ncalled SCAN, simply moves across the disk servicing requests in order\nLet us call a single pass across the disk a sweep.\na request comes for a block on a track that has already been serviced on\nstead of sweeping in one direction across the disk, the algorithm sweeps\nIn disks, it just\nCRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS\nHARD DISK DRIVES\nBefore discussing shortest positioning time ﬁrst or SPTF scheduling (some-\nWhat it depends on here is the relative time of seeking as compared\nIf, in our example, seek time is much higher than rotational\nsense to seek further to service request 8 on the outer track than it would\nto perform the shorter seek to the middle track to service 16, which has to\nrotate all the way around before passing under the disk head.\nOn modern drives, as we saw above, both seek and rotation are roughly\ntrack boundaries are or where the disk head currently is (in a rotational\nHARD DISK DRIVES\nof basic disk operation, scheduling, and related topics.\nsue is this: where is disk scheduling performed on modern systems?\nissue it to the disk.\nDisks were simpler then, and so was life.\nimplement SPTF accurately; inside the disk controller, all relevant details\nall to disk; the disk then uses its internal knowledge of head position and\nAnother important related task performed by disk schedulers is I/O\nFor example, imagine a series of requests to read blocks 33,\nordering that the scheduler does is performed upon the merged requests.\nber of requests sent to the disk and thus lowers overheads.\nshould the system wait before issuing an I/O to disk?\nthink that the disk, once it has even a single I/O, should immediately\nissue the request to the drive; this approach is called work-conserving, as\nthe disk will never be idle if there are requests to serve.\non anticipatory disk scheduling has shown that sometimes it is better to\nBy waiting, a new and “better” request may arrive at the disk, and thus",
      "keywords": [
        "HARD DISK DRIVES",
        "DISK",
        "DISK DRIVES",
        "HARD DISK",
        "time",
        "Seek",
        "Requests",
        "DRIVES",
        "Disk Scheduling",
        "Request",
        "Average Seek",
        "seek time",
        "SSTF",
        "Disk Drive Performance",
        "track"
      ],
      "concepts": [
        "disk",
        "request",
        "requests",
        "scheduling",
        "schedule",
        "times",
        "rotation",
        "rotations",
        "rotates",
        "drives"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 46,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 44,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.589,
          "base_score": 0.439,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 43,
          "title": "",
          "score": 0.517,
          "base_score": 0.367,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "disk",
          "seek",
          "drives",
          "disk drives",
          "hard disk"
        ],
        "semantic": [],
        "merged": [
          "disk",
          "seek",
          "drives",
          "disk drives",
          "hard disk"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2760398499935747,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118738+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 452-475)",
      "start_page": 452,
      "end_page": 475,
      "summary": "We have presented a summary of how disks work.\nA cool paper showing how waiting can improve disk scheduling: better requests may be on their way!\nA more modern take on disk scheduling.\nrunning some requests against a disk run with -z 10,20,30 (the\n8. Scheduling windows determine how many sector requests a disk\n(RAIDs)\nWhen we use a disk, we sometimes wish it to be faster; I/O operations\nuse a disk, we sometimes wish it to be larger; more and more data is being\nCRUX: HOW TO MAKE A LARGE, FAST, RELIABLE DISK\nDisks better known as RAID [P+88], a technique to use multiple disks in\nExternally, a RAID looks like a disk: a group of blocks one can read\nRAIDs offer a number of advantages over a single disk.\ndisks.\ntiple disks (without RAID techniques) makes the data vulnerable to the\nloss of a single disk; with some form of redundancy, RAIDs can tolerate\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\ninstall a SCSI-based RAID storage array instead of a SCSI disk, and the\nthat use them, i.e., a RAID just looks like a big disk to the host system.\na disk with a RAID and not change a single line of software; the operat-\n(hopefully) reliable disk.\nJust as with a single disk, it presents itself as\nWhen a ﬁle system issues a logical I/O request to the RAID, the RAID\nHowever, as a simple example, consider a RAID\nthat keeps two copies of each block (each one on a separate disk); when\nwriting to such a mirrored RAID system, the RAID will have to perform\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nized logic to perform parity calculations (useful in some RAID levels, as\noperate the RAID.\nTo understand RAID and compare different approaches, we must have\nIn this model, a disk can be in\nWith a working disk, all\nSpeciﬁcally, when a disk has failed, we assume that this is\nFor example, in a RAID array, we would assume that the\ndisk has failed.\ngle block becoming inaccessible upon an otherwise working disk (some-\nHow To Evaluate A RAID\nbuilding a RAID.\nﬁrst axis is capacity; given a set of N disks, how much useful capacity is\navailable to systems that use the RAID?\nHow many disk faults can\nonly that an entire disk can fail; in later chapters (i.e., on data integrity),\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\ning), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-based re-\nRAID Level 0: Striping\nHowever, RAID level 0, or striping as it is better known,\nThe simplest form of striping will stripe blocks across the disks of the\nsystem as follows (assume here a 4-disk array):\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nblock (each of say size 4KB) is placed on each disk before moving on to\nwe could arrange the blocks across disks as in Table 38.2:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nIn this example, we place two 4KB blocks on each disk before moving\non to the next disk.\nThus, the chunk size of this RAID array is 8KB, and\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nThis problem arises in all RAID arrays; simply put, given a logical\nblock to read or write, how does the RAID know exactly which physical\ngiven a logical block address A, the RAID can easily compute the desired\nDisk\n= A % number_of_disks\nOffset = A / number_of_disks\nﬁrst RAID above that a request arrives for block 14.\ndisk 2.\nchunk size implies that many ﬁles will get striped across many disks, thus\npositioning time to access blocks across multiple disks increases, because\nsitioning time of a single disk.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nBack To RAID-0 Analysis\nFrom the perspective of capacity, it is perfect: given N disks, striping de-\nlivers N disks worth of useful capacity.\nFinally, performance is excellent: all disks are utilized, often in\nEvaluating RAID Performance\nIn analyzing RAID performance, one can consider two different perfor-\ntency of a single I/O request to a RAID is useful as it reveals how much\nBecause RAIDs are often used in high-performance\nand that each request is to a different random location on disk.\ndifferent performance characteristics from a disk.\na disk operates in its most efﬁcient mode, spending little time seeking and\nthis difference in our analysis, we will assume that a disk can transfer\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nSpeciﬁcally, lets calculate S and R given the following disk charac-\nAlso, assume the following disk\nBack To RAID-0 Analysis, Again\nsingle-block request should be just about identical to that of a single disk;\nafter all, RAID-0 will simply redirect that request to one of its disks.\nof disks) multiplied by S (the sequential bandwidth of a single disk).\na large number of random I/Os, we can again use all of the disks, and\nwith other RAID levels.\nRAID Level 1: Mirroring\ndisk, of course.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nblock, the RAID keeps two physical copies of it.\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nand disk 3 do as well; the data is striped across these mirror pairs.\nblock copies across the disks.\nstriping (RAID-0) arrays, and then mirrors (RAID-1) on top of them.\nWhen reading a block from a mirrored array, the RAID has a choice: it\nthe RAID, it is free to read it from either disk 2 or disk 3.\na block, though, no such choice exists: the RAID must update both copies\ncould proceed to disks 2 and 3 at the same time.\nRAID-1 Analysis\nThus, with N disks, the useful capacity of mirroring is N/2.\nure of any one disk.\nImagine, in the ﬁgure above, that disk 0 and\ndisk 2 both failed.\ndisk; all the RAID-1 does is direct the read to one of its copies.\naverage) will be slightly higher than a write to a single disk.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nany multi-disk RAID system, known as the consistent-update problem\nThe problem occurs on a write to any RAID that has to up-\nassume we are considering a mirrored disk array.\nImagine the write is issued to the RAID, and then the RAID decides that\nThe RAID then issues\nthe write to disk 0, but just before the RAID can issue the request to disk\nassume that the request to disk 0 completed (but clearly the request to\ndisk 1 did not, as it was never issued).\non disk 1 is the old.\nkind to ﬁrst record what the RAID is about to do (i.e., update two disks\nOne last note: because logging to disk on every write is prohibitively\nWhen writing out to disk sequentially, each logical write must\n0 (in the ﬁgure above), the RAID internally would write it to both disk\n0 and disk 1.\n3 to disk 3.\nWe continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1,\nall disks, we are achieving the full bandwidth of the array.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nIn fact, each disk receives a request\nThus, each disk will only\nRandom reads are the best case for a mirrored RAID.\nThus, for random reads, RAID-1 delivers N · R MB/s.\nwrites to two different physical disks, the bandwidth of many small re-\nRAID Level 4: Saving Space With Parity\nWe now present a different method of adding redundancy to a disk ar-\nIn a ﬁve-disk RAID-4 system, we might observe the following layout:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nblocks on each disk; how do we apply XOR to a bunch of blocks to com-\nRAID-4 Analysis\nFrom a capacity standpoint, RAID-4 uses 1\nour useful capacity for a RAID group is (N-1).\nReliability is also quite easy to understand: RAID-4 tolerates 1 disk\nSequential read performance can utilize all of the disks\nexcept for the parity disk, and thus deliver a peak effective bandwidth of\nWhen writing a big chunk of data to disk,\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nRAID-4 can perform a simple optimization known as a full-stripe write.\nsent to the RAID as part of a write request (Table 38.4).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\nTable 38.4: Full-stripe Writes In RAID-4\nIn this case, the RAID can simply calculate the new value of P0 (by\nperforming an XOR across the blocks 0, 1, 2, and 3) and then write all of\nthe blocks (including the parity block) to the ﬁve disks above in parallel\nefﬁcient way for RAID-4 to write to disk.\nof sequential writes on RAID-4 is easy; the effective bandwidth is also\nEven though the parity disk is constantly in use during\nacross the data disks of the system but not the parity disk.\nthe new data and new parity to their respective disks, also in parallel.\ndisks, and thus in larger RAIDs requires a high number of reads to com-\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nThus, for each write, the RAID has to perform 4 physical\nsubmitted to the RAID; how many can RAID-4 perform in parallel?\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\nTable 38.5: Example: Writes To 4, 13, And Respective Parity Blocks\nNow imagine there were 2 small writes submitted to the RAID-4 at\nThe data for those disks is on disks 0 and 1, and thus the read and write\nis with the parity disk; both the requests have to read the related parity\nissue is now clear: the parity disk is a bottleneck under this type of work-\nbased RAIDs. Thus, even though the data disks could be accessed in\nwrites to the system will be serialized because of the parity disk.\nthe parity disk has to perform two I/Os (one read, one write) per logical\nI/O, we can compute the performance of small random writes in RAID-4\nby computing the parity disk’s performance on those two I/Os, and thus\nRAID-4 throughput under random small writes\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\na single read (assuming no failure) is just mapped to a single disk, and\nthus its latency is equivalent to the latency of a single disk request.\ntwice that of a single disk (with some differences because we have to wait\nRAID Level 5: Rotating Parity\nRAID-4, except that it rotates the parity block across drives (Figure 38.6).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\nTable 38.6: RAID-5 With Rotated Parity\nthe disks, in order to remove the parity-disk bottleneck for RAID-4.\nRAID-5 Analysis\nare sequential read and write performance.\n(whether a read or a write) is also the same as RAID-4.\nthe disks.\nRAID-4, as it allows for parallelism across requests.\nblock 1 and a write to block 10; this will turn into requests to disk 1 and\ndisk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for\nof four loss is due to the fact that each RAID-5 write still generates 4 total\nI/O operations, which is simply the cost of using parity-based RAID.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nRAID-0\nRAID-1\nRAID-4\nRAID-5\nTable 38.7: RAID Capacity, Reliability, and Performance\nwrite problem altogether [HLM94]; in those cases, RAID-4 is sometimes\nseek time is a little higher than when writing to just a single disk, because\nthe seek time is the max of two seeks (one on each disk).\nwrite performance to two disks will generally be a little less than random\nwrite performance of a single disk.\nAlso, when updating the parity disk\nin RAID-4/5, the ﬁrst read of the old parity will likely cause a full seek\nis useful for understanding tradeoffs across RAID levels.\nrepresent the time that a request to a single disk would take.\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nFor example, there are many other RAID designs, including Levels 2\nThere is also what the RAID does when a disk fails; some-\nWe have discussed RAID.\nFor example, mirrored RAID is simple, reliable, and generally provides\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nOur own work analyzing how often disks actually corrupt your data.\n[CL95] “Striping in a RAID level 5 disk array”\nA nice analysis of some of the important parameters in a RAID-5 disk array.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”\nThough not the ﬁrst paper on a RAID system with two disks for parity, it is a recent and highly-\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n1. Use the simulator to perform some basic RAID mapping tests.\nFor RAID-5, see if you can ﬁgure out\nthe RAID level.\nmance of 100 random reads to the RAID, while varying the RAID\nlevels, using 4 disks.\n6. Do the same as above, but increase the number of disks.\nthe performance of each RAID level scale as the number of disks\nHow does the performance of each RAID level scale now?\nwith RAID level, and when doing reads versus writes?",
      "keywords": [
        "RAID",
        "DISK",
        "RAID level",
        "Inexpensive Disks",
        "Redundant Arrays",
        "parity disk",
        "RAID system",
        "single disk",
        "block",
        "parity",
        "Arrays of Inexpensive",
        "write",
        "system",
        "ﬁrst RAID level",
        "RAID array"
      ],
      "concepts": [
        "raids",
        "disk",
        "blocks",
        "different",
        "difference",
        "write",
        "writing",
        "performance",
        "perform",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 44,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 45,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "raid",
          "disk",
          "disks",
          "disk disk",
          "raids"
        ],
        "semantic": [],
        "merged": [
          "raid",
          "disk",
          "disks",
          "disk disk",
          "raids"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3572816756564464,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118799+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 476-485)",
      "start_page": 476,
      "end_page": 485,
      "summary": "The ﬁrst is the ﬁle.\nA ﬁle is simply a linear array of bytes,\neach of which you can read or write.\nEach ﬁle has some kind of low-level\nﬁle is often referred to as its inode number.\nabout inodes in future chapters; for now, just assume that each ﬁle has an\nresponsibility of the ﬁle system is simply to store such data persistently\nA directory, like a ﬁle,\nFor example, let’s say there is a ﬁle with the low-level name “10”,\nof separator to name subsequent sub-directories until the desired ﬁle or\nroot directory /, and then created a ﬁle bar.txt in the directory foo,\nferent locations in the ﬁle-system tree (e.g., there are two ﬁles named\nﬁle system.\nYou may also notice that the ﬁle name in this example often has two\nname, whereas the second part of the ﬁle name is usually used to indi-\nIn UNIX systems, the ﬁle system thus provides a uniﬁed way to access\nLet’s now discuss the ﬁle system interface in more detail.\nWe’ll start with the most basic of operations: creating a ﬁle.\nit the O CREAT ﬂag, a program can create a new ﬁle.\nple code to create a ﬁle called “foo” in the current working directory.\nple, the program creates the ﬁle (O CREAT), can only write to that ﬁle\nwhile opened in this manner (O WRONLY), and, if the ﬁle already exists,\nThe older way of creating a ﬁle is to call creat(), as follows:\nBecause open() can create a ﬁle,\nOne important aspect of open() is what it returns: a ﬁle descriptor.\nﬁle descriptor is just an integer, private per process, and is used in UNIX\nsystems to access ﬁles; thus, once a ﬁle is opened, you use the ﬁle de-\nscriptor to read or write the ﬁle, assuming you have permission to do so.\nIn this way, a ﬁle descriptor is a capability [L84], i.e., an opaque handle\nthink of a ﬁle descriptor is as a pointer to an object of type ﬁle; once you\nhave such an object, you can call other “methods” to access the ﬁle, like\nread() and write().\nWe’ll see just how a ﬁle descriptor is used below.\nReading and Writing Files\nOnce we have some ﬁles, of course we might like to read or write them.\nLet’s start by reading an existing ﬁle.\nline, we might just use the program cat to dump the contents of the ﬁle\nthe ﬁle foo, which then contains the word “hello” in it.\nto see the contents of the ﬁle.\nﬁle foo?\nat each call; -e trace=open,close,read,write only traces calls to\nThe ﬁrst thing that cat does is open the ﬁle for reading.\nof things we should note about this; ﬁrst, that the ﬁle is only opened for\nsucceeds and returns a ﬁle descriptor, which has the value of 3.\nThese are represented by ﬁle descriptors 0, 1, and 2,\nThus, when you ﬁrst open another ﬁle (as cat does above),\nit will almost certainly be ﬁle descriptor 3.\nedly read some bytes from a ﬁle.\nThe ﬁrst argument to read() is the ﬁle\ndescriptor, thus telling the ﬁle system which ﬁle to read; a process can of\nthe operating system to know which ﬁle a particular read refers to.\ncall to the write() system call, to the ﬁle descriptor 1.\nThe cat program then tries to read more from the ﬁle, but since there\nare no bytes left in the ﬁle, the read() returns 0 and the program knows\nthat this means it has read the entire ﬁle.\nﬁle descriptor.\nThe ﬁle is thus closed, and the reading of it thus complete.\nWriting a ﬁle is accomplished via a similar set of steps.\nFirst, a ﬁle\nto a ﬁle, perhaps of a program you wrote yourself, or by tracing the dd\nThus far, we’ve discussed how to read and write ﬁles, but all access\nhas been sequential; that is, we have either read a ﬁle from the beginning\nciﬁc offset within a ﬁle; for example, if you build an index over a text\nThe ﬁrst argument is familiar (a ﬁle descriptor).\nwithin the ﬁle.\nAs you can tell from this description, for each ﬁle a process opens, the\ning to understand disks and how the ﬁle systems atop them work.\nA disk seek occurs when a read or write\nissued to the disk is not on the same track as the last read or write, and\nthe fact that calling lseek() to read or write from/to random parts of a\nﬁle, and then reading/writing to those random parts, will indeed lead to\nwrite will begin reading from or writing to within the ﬁle.\nof the abstraction of an open ﬁle is that it has a current offset, which\nbytes takes place, N is added to the current offset; thus each read or write\nMost times when a program calls write(), it is just telling the ﬁle\nThe ﬁle system, for performance reasons, will buffer such writes\nTo support these types of applications, most ﬁle systems provide some\n(i.e., not yet written) data to disk, for the ﬁle referred to by the speciﬁed\nﬁle descriptor.\nthe ﬁle foo, writes a single chunk of data to it, and then calls fsync()\ncontains the ﬁle foo.\nis on disk, but that the ﬁle, if newly created, also is durably a part of the\nOnce we have a ﬁle, it is sometimes useful to be able to give a ﬁle a\nwith mv command; in this example, the ﬁle foo is renamed bar:\nname of the ﬁle (old) and the new name (new).\nif the system crashes during the renaming, the ﬁle will either be named\nthat require an atomic update to ﬁle state.\nImagine that you are using a ﬁle ed-\nThe ﬁle’s\nﬁle to guarantee that the new ﬁle has the original contents plus the line\nﬁle metadata and contents are on the disk, rename the temporary ﬁle to\nthe original ﬁle’s name.\natomic ﬁle update is achieved.\nBeyond ﬁle access, we expect the ﬁle system to keep a fair amount of\ninformation about each ﬁle it is storing.\nTo see the metadata for a certain ﬁle, we can use stat()\nThese calls take a pathname (or ﬁle descriptor) to a ﬁle and ﬁll in a\nYou can see that there is a lot of information kept about each ﬁle, in-\nAs it turns out, each ﬁle system usually keeps this type of information",
      "keywords": [
        "ﬁle",
        "ﬁle system",
        "ﬁle descriptor",
        "system",
        "system call",
        "ﬁles",
        "write",
        "call",
        "read",
        "foo",
        "ﬁle foo",
        "UNIX ﬁle system",
        "File",
        "Directories",
        "directory"
      ],
      "concepts": [
        "write",
        "writing",
        "directories",
        "directory",
        "calling",
        "open",
        "file",
        "time",
        "read",
        "device"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.853,
          "base_score": 0.703,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 48,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "ﬁle descriptor",
          "descriptor",
          "read",
          "foo"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "ﬁle descriptor",
          "descriptor",
          "read",
          "foo"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3072458873887366,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118852+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 486-493)",
      "start_page": 486,
      "end_page": 493,
      "summary": "INTERLUDE: FILE AND DIRECTORIES\nthink of an inode as a persistent data structure kept by the ﬁle system that\ndoes rm use to remove a ﬁle?\nAs you can see, unlink() just takes the name of the ﬁle to be removed,\nthan just ﬁles, but also directories.\nMaking Directories\nrectly; because the format of the directory is considered ﬁle system meta-\nﬁles, directories, or other object types within it.\nIn this way, the ﬁle system\neponymous mkdir program can be used to create such a directory.\n1Some ﬁle systems call these structures similar, but slightly different, names, such as\nINTERLUDE: FILE AND DIRECTORIES\nyou issue the command, accidentally, from the root directory of a ﬁle sys-\ntem, thus removing every ﬁle and directory from it.\nNow that we’ve created a directory, we might wish to read one too.\nInstead of just opening a directory as if it were a ﬁle, we instead use\nof a directory.\nand print out the name and inode number of each ﬁle in the directory.\nINTERLUDE: FILE AND DIRECTORIES\nmay want to call stat() on each ﬁle to get more information on each,\nDeleting Directories\nUnlike ﬁle deletion,\nﬁle system tree, through a system call known as link().\nyou “link” a new ﬁle name to an old one, you essentially create another\nway to refer to the same ﬁle.\nINTERLUDE: FILE AND DIRECTORIES\nHere we created a ﬁle with the word “hello” in it, and called the ﬁle\nWe then create a hard link to that ﬁle using the ln program.\nthis, we can examine the ﬁle by either opening file or file2.\nrectory you are creating the link to, and refers it to the same inode number\nrefer to the same ﬁle.\ning out the inode number of each ﬁle:\nBy passing the -i ﬂag to ls, it prints out the inode number of each ﬁle\n(as well as the ﬁle name).\nWhen you create a ﬁle, you are really doing two things.\nSecond, you are linking a human-readable name to that ﬁle, and\nputting that link into a directory.\nAfter creating a hard link to a ﬁle, to the ﬁle system, there is no dif-\nference between the original ﬁle name (file) and the newly created ﬁle\ndata about the ﬁle, which is found in inode number 67158084.\nThus, to remove a ﬁle from the ﬁle system, we call unlink().\nexample above, we could for example remove the ﬁle named file, and\nstill access the ﬁle without difﬁculty:\nThe reason this works is because when the ﬁle system unlinks ﬁle, it\nINTERLUDE: FILE AND DIRECTORIES\n(sometimes called the link count) allows the ﬁle system to track how\nmany different ﬁle names have been linked to this particular inode.\nname (the ﬁle that is being deleted) to the given inode number, and decre-\ndoes the ﬁle system also free the inode and related data blocks, and thus\ntruly “delete” the ﬁle.\nYou can see the reference count of a ﬁle using stat() of course.\nsee what it is when we create and delete hard links to a ﬁle.\nple, we’ll create three links to the same ﬁle, and then delete them.\nThere is one other type of link that is really useful, and it is called a\nwill create a cycle in the directory tree); you can’t hard link to ﬁles in\nTo create such a link, you can use the same program ln, but with the\ninal ﬁle can now be accessed through the ﬁle name file as well as the\nsymbolic link name file2.\nINTERLUDE: FILE AND DIRECTORIES\nlink is actually a ﬁle itself, of a different type.\nregular ﬁles and directories; symbolic links are a third type the ﬁle system\n(4 bytes in this case), as well as what the link points to (the ﬁle named\nformed is by holding the pathname of the linked-to ﬁle as the data of the\nlink ﬁle.\nBecause we’ve linked to a ﬁle named file, our link ﬁle file2\nIf we link to a longer pathname, our link ﬁle would be\ncat: file2: No such file or directory\noriginal ﬁle named file causes the link to point to a pathname that no\nINTERLUDE: FILE AND DIRECTORIES\nlying ﬁle systems.\nThis task is accomplished via ﬁrst making ﬁle systems,\npartition, e.g., /dev/sda1) a ﬁle system type (e.g., ext3), and it simply\nwrites an empty ﬁle system, starting with a root directory, onto that disk\nAnd mkfs said, let there be a ﬁle system!\nHowever, once such a ﬁle system is created, it needs to be made ac-\nand b, each of which in turn holds a single ﬁle named foo.\nwish to mount this ﬁle system at the mount point /home/users.\nIf successful, the mount would thus make this new ﬁle system avail-\nHowever, note how the new ﬁle system is now accessed.\nthe contents of the root directory, we would use ls like this:\na number of separate ﬁle systems, mount uniﬁes all ﬁle systems into one\nINTERLUDE: FILE AND DIRECTORIES\nThe ﬁle system interface in UNIX systems (and indeed, in any system)",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "FILE",
        "prompt",
        "system",
        "link",
        "directory",
        "DIRECTORIES",
        "file prompt",
        "ﬁle named file",
        "ﬁles",
        "inode",
        "system call",
        "link ﬁle",
        "remzi"
      ],
      "concepts": [
        "directories",
        "directory",
        "prompt",
        "file",
        "types",
        "links",
        "named",
        "names",
        "naming",
        "making"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 47,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "link",
          "directories",
          "file",
          "directory"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "link",
          "directories",
          "file",
          "directory"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2306756728295526,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118900+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 494-511)",
      "start_page": 494,
      "end_page": 511,
      "summary": "pseudo ﬁle system.\nwhich simply calls the stat() system call on a given ﬁle or di-\nPrint out ﬁle size, number of blocks allocated, reference\nthe ﬁle names.\n3. Tail: Write a program that prints out the last few lines of a ﬁle.\nﬁle, reads in a block of data, and then goes backwards until it ﬁnds\nat the end of the ﬁle to print.\neach ﬁle and directory in the ﬁle system tree, starting at a given\nIn this chapter, we introduce a simple ﬁle system implementation, known\nThis ﬁle system is a simpliﬁed\nyou will ﬁnd in many ﬁle systems today.\nsome aspect of the ﬁle system work better (though we will want to pay at-\nAll of these ﬁle\nThus, the way we will be learning about ﬁle systems is\nthrough case studies: ﬁrst, a simple ﬁle system (vsfs) in this chapter to\nHow can we build a simple ﬁle system?\nTo think about ﬁle systems, we usually suggest thinking about two\nprobably understand how the ﬁle system basically works.\nThe ﬁrst is the data structures of the ﬁle system.\ntypes of on-disk structures are utilized by the ﬁle system to organize its\nThe ﬁrst ﬁle systems we’ll see (including vsfs below)\nFor ﬁle systems, your mental\nstructures store the ﬁle system’s data and metadata?\na process opens a ﬁle?\nmore sophisticated ﬁle systems, like SGI’s XFS, use more complicated\nThe second aspect of a ﬁle system is its access methods.\nIf you understand the data structures and access methods of a ﬁle sys-\ntures of the vsfs ﬁle system.\ndisk into blocks; simple ﬁle systems use just one block size, and that’s\nThus, our view of the disk partition where we’re building our ﬁle sys-\na ﬁle system.\nIn fact, most of the space in any ﬁle system is (and should be) user data.\ninformation about each ﬁle.\ntion, ﬁle system usually have a structure called an inode (we’ll read more\nlooks like this picture, assuming that we use 5 of our 64 blocks for inodes\nAssuming 256 bytes per inode, a 4-KB block can hold 16\ninodes, and our ﬁle system above contains 80 total inodes.\nﬁle system, built on a tiny 64-block partition, this number represents the\nmaximum number of ﬁles we can have in our ﬁle system; however, do\nnote that the same ﬁle system, built on a larger disk, could simply allocate\nOur ﬁle system thus far has data blocks (D), and inodes (I), but a few\nelement in any ﬁle system.\nand yet we only have 80 inodes and 56 data blocks.\nvery simple ﬁle system.\nthis particular ﬁle system, including, for example, how many inodes and\ndata blocks are in the ﬁle system (80 and 56, respectively in this instance),\nwhere the inode table begins (block 3), and so forth.\nThus, when mounting a ﬁle system, the operating system will read\nvolume to the ﬁle-system tree.\nOne of the most important on-disk structures of a ﬁle system is the\ninode; virtually all ﬁle systems have a structure similar to this.\nThe inode is the generic name that is used in many ﬁle systems to de-\nscribe the structure that holds the metadata for a given ﬁle, such as its\nAs we’ll see, design of the inode is one key part of ﬁle\nthis for every ﬁle they track, but perhaps call them different things (such\nother simple ﬁle systems), given an i-number, you should directly be able\nample, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks)\nTo read inode number 32, the ﬁle system would ﬁrst calculate the offset\nupon the correct byte address of the desired block of inodes: 20KB.\ninodes that contains inode 32, the ﬁle system would issue a read to sector\nﬁle: its type (e.g., regular ﬁle, directory, etc.), its size, the number of blocks\ncan this ﬁle be read/written/executed?\nwho owns this ﬁle?\nhow many bytes are in this ﬁle?\nwhat time was this ﬁle last accessed?\nwhat time was this ﬁle created?\nwhat time was this ﬁle last modiﬁed?\nhow many hard links are there to this ﬁle?\nhow many blocks have been allocated to this ﬁle?\nﬁle version (used by NFS)\nﬁle acl\nallocated to it, protection information (such as who owns the ﬁle, as well\nas who can access it), some time information, including when the ﬁle was\ndata blocks reside on disk (e.g., pointers of some kind).\nthe ﬁle system that isn’t pure user data is often referred to as such.\npointer refers to one disk block that belongs to the ﬁle.\nis limited: for example, if you want to have a ﬁle that is really big (e.g.,\nIf a ﬁle grows\nanother 1024 pointers; the ﬁle can grow to be (12 + 1024) · 4K or 4144KB.\na pointer for every block of a ﬁle, all one needs is a pointer and a length\nto specify the on-disk location of a ﬁle.\nwhen allocating a ﬁle.\nThus, extent-based ﬁle systems often allow for\nﬁle allocation.\nﬂexible but use a large amount of metadata per ﬁle (particularly for large\nproach to pointing to ﬁle blocks.\nmodate a ﬁle of just over 4 GB in size (i.e., (12 + 1024 + 10242) × 4 KB).\nMany ﬁle systems use a multi-level index, including commonly-used\nthe original UNIX ﬁle system.\nOther ﬁle systems, including SGI XFS and\nstudied ﬁle systems and how they are used, and virtually every time they\none, to point to the ﬁrst block of the ﬁle.\nAs you might have guessed, linked ﬁle allocation performs poorly for\nsome workloads; think about reading the last block of a ﬁle, for example,\nsimply D’s next pointer, i.e., the address of the next block in a ﬁle which\neffectively do random ﬁle accesses, simply by ﬁrst scanning through the\n(in memory) table to ﬁnd the desired block, and then accessing (on disk)\nstructure of what is known as the ﬁle allocation table, or FAT ﬁle system.\nYes, this classic old Windows ﬁle system, before NTFS [C94], is based on a\na standard UNIX ﬁle system too; for example, there are no inodes per se,\nbut rather directory entries which store metadata about a ﬁle and refer\ndirectly to the ﬁrst block of said ﬁle, which makes creating hard links\nAverage ﬁle size is growing\nEven as disks grow, ﬁle systems remain ˜50% full\nIn vsfs (as in many ﬁle systems), directories have a simple organiza-\nFor each ﬁle or directory in a given directory, there is a string\nand a number in the data block(s) of the directory.\nFor example, assume a directory dir (inode number 5) has three ﬁles\nﬁle systems treat directories as a special type of ﬁle.\nblocks pointed to by the inode (and perhaps, indirect blocks); these data\nblocks live in the data block region of our simple ﬁle system.\nA ﬁle system must track which inodes and data blocks are free, and\nwhich are not, so that when a new ﬁle or directory is allocated, it can ﬁnd\nThus free space management is important for all ﬁle systems.\nSome early ﬁle systems used free lists, where a single pointer in the super\nModern ﬁle systems use more sophisticated data structures.\nFor example, when we create a ﬁle, we will have to allocate an inode\nfor that ﬁle.\nThe ﬁle system will thus search through the bitmap for an in-\node that is free, and allocate it to the ﬁle; the ﬁle system will have to mark\ndata block is allocated.\ndata blocks for a new ﬁle.\nFor example, some Linux ﬁle systems, such\nwhen a new ﬁle is created and needs data blocks; by ﬁnding such a se-\nquence of free blocks, and then allocating them to the newly-created ﬁle,\nthe ﬁle system guarantees that a portion of the ﬁle will be on the disk and\nof reading or writing a ﬁle.\nFor the following examples, let us assume that the ﬁle system has been\nelse (i.e., inodes, directories) is still on the disk.\na ﬁle (e.g., /foo/bar, read it, and then close it.\nlet’s assume the ﬁle is just 4KB in size (i.e., 1 block).\ntem ﬁrst needs to ﬁnd the inode for the ﬁle bar, to obtain some basic in-\nthe ﬁle system must be able to ﬁnd the inode, but all it has right now is\nThe ﬁle system must traverse the pathname and thus\nAll traversals begin at the root of the ﬁle system, in the root directory\nUsually, we ﬁnd the i-number of a ﬁle\nwhat it is when the ﬁle system is mounted.\nIn most UNIX ﬁle systems,\nblock that contains inode number 2 (the ﬁrst inode block).\ndata blocks, which contain the contents of the root directory.\nthus use these on-disk pointers to read through the directory, in this case\ning the inode of foo and then read in its directory data, ﬁnally ﬁnding the\ninto memory; the FS can then do a ﬁnal permissions check, allocate a ﬁle\nfrom the ﬁle.\nwill thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd\nthe location of such a block; it may also update the inode with a new last-\nThe read will further update the in-memory open ﬁle table\nfor this ﬁle descriptor, updating the ﬁle offset such that the next read will\nread the second ﬁle block, etc.\nreading a ﬁle, and not allocating any new blocks, that the bitmap will still\nThe inodes, directories, and\nquest; there is no need to make sure a block is allocated when the inode\nAt some point, the ﬁle will be closed.\nin order to ﬁnally locate the inode of the ﬁle.\nblock requires the ﬁle system to ﬁrst consult the inode, then read the\nblock, and then update the inode’s last-accessed-time ﬁeld with a write.\npath, we have to read its inode as well as its data.\nmight have to read many data blocks to ﬁnd the desired entry.\ncan get pretty bad when reading a ﬁle; as you’re about to ﬁnd out, writing\nout a ﬁle (and especially, creating a new one) is even worse.\nWriting to a ﬁle is a similar process.\nFirst, the ﬁle must be opened (as\nThen, the application can issue write() calls to update the ﬁle\nUnlike reading, writing to the ﬁle may also allocate a block (unless\nﬁle, each write not only has to write data to disk but has to ﬁrst decide\nwhich block to allocate to the ﬁle and thus update other structures of the\nThus, each write to a ﬁle logically\nTo create a ﬁle, the ﬁle\nthe directory containing the new ﬁle.\ninode itself (to initialize it), one to the data of the directory (to link the\nhigh-level name of the ﬁle to its inode number), and one read and write\nAll that just to create a ﬁle!\nLet’s look at a speciﬁc example, where the ﬁle /foo/bar is created,\nthe open() (which creates the ﬁle) and during each of three 4KB writes.\nwork it is to create the ﬁle: 10 I/Os in this case, to walk the pathname\nand then ﬁnally create the ﬁle.\nEven the simplest of operations like opening, reading, or writing a ﬁle\nclearly be a huge performance problem, most ﬁle systems aggressively\nImagine the open example above: without caching, every ﬁle open\n(one to read the inode of the directory in question, and at least one to read\nsystem would literally perform hundreds of reads just to open the ﬁle!\nEarly ﬁle systems thus introduced a ﬁx-sized cache to hold popular\nModern systems integrate virtual memory pages and ﬁle\nallocated more ﬂexibly across virtual memory and ﬁle system, depending\nNow imagine the ﬁle open example with caching.\ngenerate a lot of I/O trafﬁc to read in directory inode and data, but sub-\nsequent ﬁle opens of that same ﬁle (or ﬁles in the same directory) will\nFirst, by delaying writes, the ﬁle system can batch\nanother ﬁle is created, the ﬁle system saves an I/O by delaying the write\nfor example, if an application creates a ﬁle and then deletes it, delaying\nthe writes to reﬂect the ﬁle creation to disk avoids them entirely.\nFor the reasons above, most modern ﬁle systems buffer writes in mem-\nthe ﬁle system altogether1.\nThere needs to be some information about each ﬁle (metadata), usually\nof ﬁle that store name→inode-number mappings.\nare needed too; for example, ﬁle systems often use a structure such as a\nbitmap to track which inodes or data blocks are free or allocated.\nThe terriﬁc aspect of ﬁle system design is its freedom; the ﬁle systems\nto optimize some aspect of the ﬁle system.\nFor example, when a new ﬁle",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "File System Implementation",
        "system",
        "File System",
        "inode",
        "simple ﬁle system",
        "System Implementation",
        "UNIX ﬁle system",
        "data",
        "data blocks",
        "block",
        "Read",
        "ﬁles",
        "FILE"
      ],
      "concepts": [
        "blocks",
        "directories",
        "directory",
        "systems",
        "write",
        "writing",
        "disk",
        "read",
        "data",
        "allocated"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 47,
          "title": "",
          "score": 0.853,
          "base_score": 0.703,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 48,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "inode",
          "ﬁle systems",
          "block",
          "blocks"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "inode",
          "ﬁle systems",
          "block",
          "blocks"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3668308843562131,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.118960+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 512-520)",
      "start_page": 512,
      "end_page": 520,
      "summary": "An excellent recent analysis of how ﬁle systems are actually used.\nSidebotham, Michael J.\nSome details on ext2, a very simple Linux ﬁle system based on FFS, the Berkeley Fast File System.\nA nice paper about NetBSD’s integration of ﬁle-system buffer caching and the virtual-memory page\nﬁle system: everything is a tree.\nWe should have a chapter on this ﬁle system too.\nUse this tool, vsfs.py, to study how ﬁle system state changes as var-\nThe ﬁle system begins in an empty state, with\nperformed, thus slowly changing the on-disk state of the ﬁle system.\nabout the inode and data-block allocation algorithms, in terms of\n3. Now reduce the number of data blocks in the ﬁle system, to very\nWhat types of ﬁles end up in the ﬁle system in this highly-\nthe ﬁnal state of the ﬁle system likely to be?\nhimself Ken Thompson wrote the ﬁrst ﬁle system.\n“old UNIX ﬁle system”, and it was really simple.\nThe super block (S) contained information about the entire ﬁle system:\nall the inodes for the ﬁle system.\nby data blocks.\nThe good thing about the old ﬁle system was that it was simple, and\nand got worse over time, to the point where the ﬁle system was delivering\nThe main issue was that the old UNIX ﬁle system treated the disk like it\nFor example, the data blocks of\na ﬁle were often very far away from its inode, thus inducing an expensive\nseek whenever one ﬁrst read the inode and then the data blocks of a ﬁle\ning to a bunch of blocks spread across the disk, and as ﬁles got allocated,\ncally contiguous ﬁle would be accessed by going back and forth across\nfour ﬁles (A, B, C, and D), each of size 2 blocks:\nwish to allocate a ﬁle E, of size four blocks:\nUNIX ﬁle system, and it hurt performance.\non-disk data to place ﬁles contiguously and make free space one or a few\nHow can we organize ﬁle system data structures so as to improve per-\nHow do we make the ﬁle system “disk aware”?\nA group at Berkeley decided to build a better, faster ﬁle system, which\nthe ﬁle system structures and allocation policies to be “disk aware” and\nto the ﬁle system (the same APIs, including open(), read(), write(),\nVirtually all modern ﬁle systems adhere to the ex-\ndisk into a bunch of groups known as cylinder groups (some modern ﬁle\nsystems like Linux ext2 and ext3 just call them block groups).\nformance; by placing two ﬁles within the same group, FFS can ensure that\nThus, FFS needs to have the ability to allocate ﬁles and directories\nsuper block (S) is found in each group for reliability reasons (e.g., if one\ngets corrupted or scratched, you can still mount and access the ﬁle system\nfree space in a ﬁle system because it is easy to ﬁnd a large chunk of free\nspace and allocate it to a ﬁle, perhaps avoiding some of the fragmentation\nproblems of the free list in the old ﬁle system.\nFinally, the inode and data block regions are just like in the previous\nvery simple ﬁle system.\n/foo/bar.txt and that the ﬁle is one block long (4KB).\nThe ﬁle is new,\nThe ﬁle also has data in it and\nthus it too must be allocated; the data bitmap and a data block will thus\nIn particular, when creating a new ﬁle, we must also place the ﬁle in the\nﬁle-system hierarchy; thus, the directory must be updated.\njust to create a new ﬁle!\nWith this group structure in place, FFS now has to decide how to place\nbe placed into different block groups.\ncate a bunch of ﬁles), and put the directory data and inode in that group.\nthe number of free data blocks).\nto allocate the data blocks of a ﬁle in the same group as its inode, thus\npreventing long seeks between inode and data (as in the old ﬁle sys-\nto analyze some traces of ﬁle system access and see if indeed there is\naway” ﬁle accesses were from one another in the directory tree.\nample, if ﬁle f is opened, and then re-opened next in the trace (before\ndirectory tree is zero (as they are the same ﬁle).\nIf a ﬁle f in directory\ndir (i.e., dir/f) is opened, and followed by an open of ﬁle g in the same\ndirectory (i.e., dir/g), the distance between the two ﬁle accesses is one,\nas they share the same directory but are not the same ﬁle.\npercentage of ﬁle opens that were of that difference along the y-axis.\nsee that about 7% of ﬁle accesses were to the ﬁle that was opened previ-\nously, and that nearly 40% of ﬁle accesses were to either the same ﬁle or\nInterestingly, another 25% or so of ﬁle accesses were to ﬁles that had a\nHowever, because eventually every ﬁle shares a common ancestor (e.g.,\nIn FFS, there is one important exception to the general policy of ﬁle\nﬁle would entirely ﬁll the block group it is ﬁrst placed within (and maybe\nsubsequent “related” ﬁles from being placed within this block group, and\nthus may hurt ﬁle-access locality.\nblocks are allocated into the ﬁrst block group (e.g., 12 blocks, or the num-\nchunk of the ﬁle (e.g., those pointed to by the ﬁrst indirect block) in an-\nchunk of the ﬁle is placed in yet another different block group, and so on.\nthe large-ﬁle exception, a single large ﬁle would place all of its blocks into\nWe use a small example of a ﬁle with 10 blocks to",
      "keywords": [
        "ﬁle system",
        "Fast File System",
        "ﬁle",
        "FILE SYSTEM",
        "SYSTEM",
        "UNIX ﬁle system",
        "ﬁles",
        "FFS",
        "data",
        "Fast File",
        "FILE SYSTEM IMPLEMENTATION",
        "ﬁle system data",
        "FILE",
        "block",
        "group"
      ],
      "concepts": [
        "blocks",
        "file",
        "directory",
        "directories",
        "performance",
        "performed",
        "locality",
        "disk",
        "operating",
        "operations"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 47,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "group",
          "block",
          "blocks",
          "ﬁles"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "group",
          "block",
          "blocks",
          "ﬁles"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4024411517403492,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119023+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 521-532)",
      "start_page": 521,
      "end_page": 532,
      "summary": "Here is the depiction of FFS without the large-ﬁle exception:\nWith the large-ﬁle exception, we might see something more like this, with\nthe ﬁle spread across the disk in chunks:\nThe astute reader will note that spreading blocks of a ﬁle across the\nof sequential ﬁle access (e.g., when a user or application reads chunks 0\nof our time transferring data from disk and just a relatively little time\nof peak disk performance), we would thus need to spend 10 ms transfer-\nin the same group as the inode; each subsequent indirect block, and all\nWith a block\n1024 blocks of the ﬁle (4 MB) were placed in separate groups, the lone\nexception being the ﬁrst 48-KB of the ﬁle as pointed to by direct pointers.\nout, many ﬁles were 2 KB or so in size back then, and using 4-KB blocks,\ntle blocks that the ﬁle system could allocate to ﬁles.\nsmall ﬁle (say 1 KB in size), it would occupy two sub-blocks and thus not\nallocating 512-byte blocks to it until it acquires a full 4-KB of data.\npoint, FFS will ﬁnd a 4-KB block, copy the sub-blocks into it, and free the\nwrites and then issue them in 4-KB chunks to the ﬁle system, thus avoid-\nA second neat thing that FFS introduced was a disk layout that was\nA problem arose in FFS when a ﬁle was placed on consecutive sectors of\nﬁrst issue a read to block 0; by the time the read was complete, and FFS\nFFS has enough time to request the next block before it went past the\ndisk how many blocks it should skip in doing layout in order to avoid the\nLong ﬁle\nﬁle or directory on a system and thus are much more ﬂexible.\nThe introduction of FFS was a watershed moment in ﬁle system his-\ntory, as it made clear that the problem of ﬁle management was one of the\nSince that time, hundreds of new ﬁle systems have developed, but still\ntoday many ﬁle systems take cues from FFS (e.g., Linux ext2 and ext3 are\nAs we’ve seen thus far, the ﬁle system manages a set of data structures to\nﬁle system.\nmemory of a running program), ﬁle system data structures must persist,\ndata despite power loss (such as hard disks or ﬂash-based SSDs).\nof power losses and crashes, updating a persistent data structure can be\nquite tricky, and leads to a new and interesting problem in ﬁle system\ncrashes or loses power after one write completes, the on-disk structure\nﬁle systems need to solve:\nTHE CRUX: HOW TO UPDATE THE DISK DESPITE CRASHES\nin time, how do we ensure the ﬁle system keeps the on-disk image in a\nat some methods ﬁle systems have used to overcome it.\nexamining the approach taken by older ﬁle systems, known as fsck or the\nﬁle system checker.\nWe’ll need to use a workload that updates on-disk structures in some\ndata block to an existing ﬁle.\nissuing a single 4KB write to the ﬁle before closing it.\nLet’s also assume we are using standard simple ﬁle system structures\non the disk, similar to ﬁle systems we have seen before.\nincludes an inode bitmap (with just 8 bits, one per inode), a data bitmap\n(also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and\nspread across four blocks), and data blocks (8 total, numbered 0 to 7).\nHere is a diagram of this ﬁle system:\nData Blocks\nsingle allocated data block (data block 4), also marked in the data bitmap.\nIn this simpliﬁed inode, the size of the ﬁle is 1 (it has one block al-\nlocated), the ﬁrst direct pointer points to block 4 (the ﬁrst data block of\nthe ﬁle, Da), and all three other direct pointers are set to null (indicating\nWhen we append to the ﬁle, we are adding a new data block to it, and\nthus must update three on-disk structures: the inode (which must point\nnew data block Db, and a new version of the data bitmap (call it B[v2]) to\nindicate that the new data block has been allocated.\nmust write to disk.\nThe updated data bitmap (B[v2]) now looks like this: 00001100.\nthere is the data block (Db), which is just ﬁlled with whatever it is users\nWhat we would like is for the ﬁnal on-disk image of the ﬁle system to\nData Blocks\nrate writes to the disk, one each for the inode (I[v2]), bitmap (B[v2]), and\ndata block (Db).\nor buffer cache) for some time ﬁrst; then, when the ﬁle system ﬁnally\ndecides to write them to disk (after say 5 seconds or 30 seconds), the ﬁle\na crash may occur and thus interfere with these updates to the disk.\n• Just the data block (Db) is written to disk.\non disk, but there is no inode that points to it and no bitmap that\nﬁle-system crash consistency1.\n• Just the updated inode (I[v2]) is written to disk.\ninode points to the disk address (5) where Db was about to be writ-\npointer, we will read garbage data from the disk (the old contents\nFurther, we have a new problem, which we call a ﬁle-system incon-\nThe on-disk bitmap is telling us that data block 5 has not\nment in the ﬁle system data structures is an inconsistency in the\ndata structures of the ﬁle system; to use the ﬁle system, we must\n• Just the updated bitmap (B[v2]) is written to disk.\nbitmap indicates that block 5 is allocated, but there is no inode that\nnever be used by the ﬁle system.\nblocks to disk.\n• The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not\nIn this case, the ﬁle system metadata is completely con-\nsistent: the inode has a pointer to block 5, the bitmap indicates that\nthe ﬁle system’s metadata.\n• The inode (I[v2]) and the data block (Db) are written, but not the\nrect data on disk, but again have an inconsistency between the in-\nneed to resolve the problem before using the ﬁle system.\n• The bitmap (B[v2]) and data block (Db) are written, but not the\nthe inode and the data bitmap.\nHowever, even though the block\nwhich ﬁle it belongs to, as no inode points to the ﬁle.\nthat can occur to our on-disk ﬁle system image because of crashes: we can\nhave inconsistency in ﬁle system data structures; we can have space leaks;\nideally is move the ﬁle system from one consistent state (e.g., before the\nﬁle got appended to) to another atomically (e.g., after the inode, bitmap,\nand new data block have been written to disk).\ndo this easily because the disk only commits one write at a time, and\nEarly ﬁle systems took a simple approach to crash consistency.\nall problems; consider, for example, the case above where the ﬁle system\nlooks consistent but the inode points to garbage data.\nis to make sure the ﬁle system metadata is internally consistent.\nIt is run before the ﬁle system\nis mounted and made available (fsck assumes that no other ﬁle-system\nactivity is on-going while it runs); once ﬁnished, the on-disk ﬁle system\nmostly doing sanity checks such as making sure the ﬁle system size\n• Free blocks: Next, fsck scans the inodes, indirect blocks, double\ncurrently allocated within the ﬁle system.\nFor example, fsck makes sure that each allocated inode has\na valid type ﬁeld (e.g., regular ﬁle, directory, symbolic link, etc.).\n• Inode links: fsck also veriﬁes the link count of each allocated in-\nticular ﬁle.\nlink counts for every ﬁle and directory in the ﬁle system.\ntwo different inodes refer to the same block.\nremoves (clears) the pointer from the inode or indirect block.\ncreated by the ﬁle system itself.\nof the ﬁle system; making sure such a piece of code works correctly in all",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "SYSTEM",
        "ﬁle system data",
        "data block",
        "inode",
        "data",
        "FAST FILE SYSTEM",
        "disk",
        "FFS",
        "block",
        "FILE SYSTEM",
        "system data structures",
        "on-disk ﬁle system",
        "FSCK"
      ],
      "concepts": [
        "disk",
        "blocks",
        "directories",
        "directory",
        "systems",
        "problem",
        "crash",
        "crashes",
        "writes",
        "bitmap"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 47,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 52,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "block",
          "inode",
          "data",
          "disk"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "block",
          "inode",
          "data",
          "disk"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38502260936226207,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119099+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 533-546)",
      "start_page": 533,
      "end_page": 546,
      "summary": "CRASH CONSISTENCY: FSCK AND JOURNALING\nSolution #2: Journaling (or Write-Ahead Logging)\nIn ﬁle systems, we usually call write-ahead logging jour-\n[H87], though many modern ﬁle systems use the idea, including Linux\nBy writing the note to disk, you are guaranteeing that if a crash takes\nWe’ll now describe how Linux ext3, a popular journaling ﬁle system,\nincorporates journaling into the ﬁle system.\ntures are identical to Linux ext2, e.g., the disk is divided into block groups,\nThus, an ext2 ﬁle system (without journaling) looks like this:\nAssuming the journal is placed within the same ﬁle system image\nthe ﬁle system), an ext3 ﬁle system with a journal looks like this:\nJournal\nData Journaling\nLet’s look at a simple example to understand how data journaling works.\nData journaling is available as a mode with the Linux ext3 ﬁle system,\n‘inode (I[v2]), bitmap (B[v2]), and data block (Db) to disk again.\nwriting them to their ﬁnal disk locations, we are now ﬁrst going to write\nthem to the log (a.k.a. journal).\nJournal\ning update to the ﬁle system (e.g., the ﬁnal addresses of the blocks I[v2],\nthe journal, e.g., “this update wishes to append data block Db to ﬁle X”,\ning update in the journal), we issue the writes I[v2], B[v2], and Db to\ntheir disk locations as seen above; if these writes complete successfully,\n1. Journal write: Write the transaction, including a transaction-begin\nblock, all pending data and metadata updates, and a transaction-\nend block, to the log; wait for these writes to complete.\n2. Checkpoint: Write the pending metadata and data updates to their\nIn our example, we would write TxB, I[v2], B[v2], Db, and TxE to the\nWhen these writes complete, we would complete the update\nthe journal.\nASIDE: FORCING WRITES TO DISK\nTo enforce ordering between two disk writes, modern ﬁle systems have\ntwo writes, A and B, was easy: just issue the write of A to the disk, wait\nfor the disk to interrupt the OS when the write is complete, and then issue\nreporting), a disk will inform the OS the write is complete when it simply\nreach the disk after previous writes; thus ordering between writes is not\nthe barrier will reach disk before any writes issued after the barrier.\nlowing reason: given such a big write, the disk internally may perform\nthe disk internally may (1) write TxB, I[v2], B[v2], and TxE and only later\n(2) write Db. Unfortunately, if the disk loses power between (1) and (2),\nJournal\nNamely, the ﬁle system ﬁrst has to write out the transaction-begin block\nand contents of the transaction; only after these writes complete can the\nﬁle system send the transaction-end block to disk.\nWhen writing a transaction to the journal,\nDoing so enables the ﬁle system to write the entire transaction at\nthe transaction, it can conclude that a crash occurred during the write\nof the transaction and thus discard the ﬁle-system update.\nsmall tweak in the write protocol and recovery system, a ﬁle system can\nThus, every time you write to disk on many Linux-based systems, a little\nTo avoid this problem, the ﬁle system issues the transactional write in\nFirst, it writes all blocks except the TxE block to the journal,\nissuing these writes all at once.\nWhen these writes complete, the journal\nJournal\nWhen those writes complete, the ﬁle system issues the write of the TxE\nJournal\nThus, our current protocol to update the ﬁle system, with each of its three\n1. Journal write: Write the contents of the transaction (including TxB,\nmetadata, and data) to the log; wait for these writes to complete.\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for write to complete; transaction is said to be\n3. Checkpoint: Write the contents of the update (metadata and data)\ncomplete, the ﬁle system can recover the update as follows.\nsystem boots, the ﬁle system recovery process will scan the log and look\nthus replayed (in order), with the ﬁle system again attempting to write\nout the blocks in the transaction to their ﬁnal on-disk locations.\nBy recovering the committed transactions in the journal, the ﬁle system\nthe journal for each of our two ﬁle creations; because the ﬁles are in the\nblock, this means that if we’re not careful, we’ll end up writing these same\nTo remedy this problem, some ﬁle systems do not commit each update\nto disk one at a time (e.g., Linux ext3); rather, one can buffer all updates\nto write these blocks to disk (say, after a timeout of 5 seconds), this single\nThus, by buffering updates, a ﬁle system can avoid excessive write\nWe thus have arrived at a basic protocol for updating ﬁle-system on-disk\nThe ﬁle system buffers updates in memory for some time;\nwhen it is ﬁnally time to write to disk, the ﬁle system ﬁrst carefully writes\nout the details of the transaction to the journal (a.k.a. write-ahead log);\nafter the transaction is complete, the ﬁle system checkpoints those blocks\nJournal\nTo address these problems, journaling ﬁle systems treat the log as a\ncircular data structure, re-using it over and over; this is why the journal is\nTo do so, the ﬁle system must take\nand newest transactions in the log in a journal superblock; all other space\nJournal\nJournal\nIn the journal superblock (not to be confused with the main ﬁle system\n1. Journal write: Write the contents of the transaction (containing TxB\nand the contents of the update) to the log; wait for these writes to\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction is\n3. Checkpoint: Write the contents of the update to their ﬁnal locations\nwithin the ﬁle system.\nThus we have our ﬁnal data journaling protocol.\nproblem: we are writing each data block to the disk twice, which is a\nyou ﬁgure out a way to retain consistency without writing data twice?\nMetadata Journaling\nwrite to disk, we are now also writing to the journal ﬁrst, thus doubling\nFurther, between writes to the journal and writes to the main\nBecause of the high cost of writing every data block to disk twice, peo-\ndata journaling (as in Linux ext3), as it journals all user data (in addition\nto the metadata of the ﬁle system).\nten to the journal.\nJournal\nThe data block Db, previously written to the log, would instead be\nwritten to the ﬁle system proper, avoiding the extra write; given that most\nI/O trafﬁc to the disk is data, not writing data twice substantially reduces\nquestion, though: when should we write data blocks to disk?\nshould we write Db to disk?\nAs it turns out, the ordering of the data write does matter for metadata-\nonly journaling.\nFor example, what if we write Db to disk after the trans-\nproach has a problem: the ﬁle system is consistent but I[v2] may end up\nThe ﬁle system will then\nBecause Db is not in the log, the ﬁle system will replay\nwrites to I[v2] and B[v2], and produce a consistent ﬁle system (from the\next3) write data blocks (of regular ﬁles) to the disk ﬁrst, before related\n1. Data write: Write data to ﬁnal location; wait for completion\n2. Journal metadata write: Write the begin block and metadata to the\nlog; wait for writes to complete.\n3. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction (in-\n4. Checkpoint metadata: Write the contents of the metadata update\nBy forcing the data write ﬁrst, a ﬁle system can guarantee that a pointer\next3) is more popular than full data journaling.\nFinally, note that forcing the data write to complete (Step 1) before\nissuing writes to the journal (Step 2) is not required for correctness, as\nwrites as well as the transaction-begin block and metadata to the journal;\nof the journal commit block (Step 3).\nusing some form of metadata journaling (and thus data blocks for ﬁles\nare not journaled).\nJournal\nnew ﬁle (say foobar), which ends up reusing the same block (1000) that\nits data; note, however, because metadata journaling is in use, only the\ninode of foobar is committed to the journal; the newly-written data in\nblock 1000 in the ﬁle foobar is not journaled.\nJournal\nJournal\nTable 42.1: Data Journaling Timeline\nthe log, including the write of directory data in block 1000; the replay\nthus overwrites the user data of current ﬁle foobar with old directory\nout of the journal.\njournal.\nshows the protocol when journaling data as well as metadata, whereas\nFor example, in the data journaling protocol (42.1), the writes\nany order; however, the write to the transaction end block (TxE) must not\ning writes to data and metadata blocks cannot begin until the transaction\nthat the data write can logically be issued at the same time as the writes\nJournal\nto the transaction begin and the contents of the journal; however, it must\nFinally, note that the time of completion marked for each write in the\nThis approach carefully orders all writes to the ﬁle sys-\nFor example, by writing a pointed-to data block to disk before\ning more about this technique when we discuss the log-structured ﬁle\ndisk and thus the ﬁle is consistent; if not, the ﬁle is inconsistent, and an\ntimes a journal protocol has to wait for disk writes to complete.\nwrites to disk as possible and uses a generalized form of the transaction\nThus, many ﬁle systems now use journaling.\nJournaling\nthis reason, many modern ﬁle systems use journaling.\nboth ﬁle system metadata as well as user data.\nZFS uses copy-on-write and journaling, actually, as in some cases, logging writes to disk will perform\nThe ﬁrst work (that we know of) that applied write-ahead logging (a.k.a. journaling) to a ﬁle system.\nA paper mostly focused on studying how ﬁle systems react to disk failures.\nAn early paper we wrote analyzing how journaling ﬁle systems work.\nOur own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to\nbe written to disk before B, ﬁrst write A, then send a lot of “dummy” writes to disk, hopefully causing\nTweedie did much of the heavy lifting in adding journaling to the Linux ext2 ﬁle system; the result,\ncompatibility, e.g., you can just add a journaling ﬁle to an existing ext2 ﬁle system and then mount it",
      "keywords": [
        "ﬁle system",
        "system",
        "ﬁle",
        "write",
        "CRASH CONSISTENCY",
        "JOURNALING",
        "journaling ﬁle systems",
        "Linux ﬁle system",
        "disk",
        "data",
        "File System",
        "ﬁle system metadata",
        "CRASH",
        "block",
        "ﬁle system checker"
      ],
      "concepts": [
        "journal",
        "write",
        "writing",
        "disks",
        "data",
        "logging",
        "log",
        "systems",
        "block",
        "updating"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "journal",
          "ﬁle",
          "journaling",
          "transaction",
          "write"
        ],
        "semantic": [],
        "merged": [
          "journal",
          "ﬁle",
          "journaling",
          "transaction",
          "write"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3267942411581238,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119156+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 547-558)",
      "start_page": 547,
      "end_page": 558,
      "summary": "As more data is cached, disk trafﬁc\nto create a new ﬁle of size one block: one for a new inode, one to\nupdate the inode bitmap, one to the directory data block that the\nﬁle is in, one to the directory inode to update it, one to the new data\nblock that is apart of the new ﬁle, and one to the data bitmap to\nAn ideal ﬁle system would thus focus on write performance, and try\nupdate on-disk metadata structures frequently.\ning to disk, LFS ﬁrst buffers all updates (including metadata!) in an in-\nmemory segment; when the segment is full, it is written to disk in one\nlong, sequential transfer to an unused part of the disk, i.e., LFS never\noverwrites existing data, but rather always writes segments to free loca-\nBecause segments are large, the disk is used efﬁciently, and perfor-\nHow can a ﬁle system turns all writes into sequential writes?\nwhere on disk.\nFor writes, however, the ﬁle system always has a choice,\nWriting To Disk Sequentially\nﬁle-system state into a series of sequential writes to disk?\nImagine we are writing a data block\nWriting the data block to disk might result in the following\nHowever, when a user writes a data block, it is not only data that gets\nwritten to disk; there is also other metadata that needs to be updated.\nIn this case, let’s also write the inode (I) of the ﬁle to disk, and have it\npoint to the data block D.\nWhen written to disk, the data block and inode\nThis basic idea, of simply writing all updates (such as data blocks,\ninodes, etc.) to the disk sequentially, sits at the heart of LFS.\nUnfortunately, writing to disk sequentially is not (alone) enough to\nthe disk at address A + 1 (the next block address in sequential order),\nthe disk has rotated; when you issue the second write, it will thus wait\nthe second write to the disk surface).\nthat simply writing to disk in sequential order is not enough to achieve\nTo achieve this end, LFS uses an ancient technique known as write\nBefore writing to the disk, LFS keeps track of updates in\nThe large chunk of updates LFS writes at one time is referred to by\nwrites.\nThus, when writing to disk, LFS buffers updates in an in-memory\nsegment, and then writes the segment all at once to the disk.\nthe segment is large enough, these writes will be efﬁcient.\nfour block writes to ﬁle j; the second is one block being added to ﬁle k.\nLFS then commits the entire segment of seven blocks to disk at once.\nresulting on-disk layout of these blocks is as follows:\nbuffer before writing to disk?\nbefore writing when running on such a disk?\nThe time to write out this chunk of data (Twrite) is the positioning time\nTo understand how we ﬁnd an inode in LFS, let us brieﬂy review how\nto ﬁnd an inode in a typical UNIX ﬁle system.\nas FFS, or even the old UNIX ﬁle system, ﬁnding inodes is easy, because\nFor example, the old UNIX ﬁle system keeps all inodes at a ﬁxed por-\nﬁnd a particular inode, you can calculate its exact disk address simply by\ninodes all throughout the disk!\nbetween inode numbers and the inodes through a data structure called\nto disk, the imap is updated with its new location.\nAnd certainly the inode map in LFS is a virtualization of inode numbers.\nexample, or inodes in LFS) without having to change every reference to\ndisk); doing so allows LFS to keep track of the locations of inodes across\n(i.e., there would be more disk seeks, between each update and the ﬁxed\nInstead, LFS places chunks of the inode map right next to where it is\nblock to a ﬁle k, LFS actually writes the new data block, its inode, and a\npiece of the inode map all together onto the disk, as follows:\nimap tells LFS that the inode k is at disk address A1; this inode, in turn,\ntells LFS that its data block D is at address A0.\nhave some ﬁxed and known location on disk to begin a ﬁle lookup.\nLFS has just such a ﬁxed place on disk for this, known as the check-\ninodes point to ﬁles (and directories) just like typical UNIX ﬁle systems.\nthe beginning of the disk, at address 0), and a single imap chunk, inode,\nimap chunks, and of course many more inodes, data blocks, etc.\nwhat must happen to read a ﬁle from disk.\nThe ﬁrst on-disk data structure we must read is the\ndresses) to the entire inode map, and thus LFS then reads in the entire in-\nnumber of a ﬁle, LFS simply looks up the inode-number to inode-disk-\nTo read a block from the ﬁle, at this point, LFS proceeds exactly\nﬁle from disk; the entire imap is cached and thus the extra work LFS does\nﬁle systems, in that a directory is just a collection of (name, inode number)\nFor example, when creating a ﬁle on disk, LFS must both write\nRemember that LFS will do so sequentially on the disk\naccessing ﬁle foo (with inode number f), you would ﬁrst look in the\nThere is one other serious problem in LFS that the inode map solves,\nin any ﬁle system that never updates in place (such as LFS), but rather\nmoves updates to new locations on the disk.\nSpeciﬁcally, whenever an inode is updated, its location on disk changes.\nLFS cleverly avoids this problem with the inode map.\nYou may have noticed another problem with LFS; it keeps writing\nnewer version of a ﬁle, its inode, and in fact all data to new parts of the\ndisk.\nThis process, while keeping writes efﬁcient, implies that LFS leaves\nolder versions of ﬁle structures all over the disk, scattered throughout the\ndisk.\nferred to by inode number k, which points to a single data block D0.\nnow overwrite that block, generating both a new inode and a new data\nThe resulting on-disk layout of LFS would look something like this\nof imap would also have to be written to disk to point to the new inode):\nIn the diagram, you can see that both the inode and data block have\nblock, a number of new structures must be persisted by LFS, thus leaving\nold versions of said blocks on the disk.\nold data block is still pointed to by the inode.\nSo what should we do with these older versions of inodes, data blocks,\nHowever, LFS instead keeps only the latest live version of a ﬁle; thus\nof ﬁle data, inodes, and other structures, and clean them; cleaning should\nthus make blocks on disk free again for use in a subsequent writes.\nthat enables large writes to disk in LFS.\ncleaner simply went through and freed single data blocks, inodes, etc.,\nto write to disk sequentially and with high performance.\nnumber of old (partially-used) segments, determines which blocks are\nlive within these segments, and then write out a new set of segments\nwith just the live blocks within them, freeing up the old ones for writing.\nthe N segments to disk in new locations.\nfreed and can be used by the ﬁle system for subsequent writes.\nhow can LFS tell which blocks within a segment are live, and which are\nGiven a data block D within an on-\ndisk segment S, LFS must be able to determine whether D is live.\nSpeciﬁcally, LFS includes, for each data block D, its inode number\nas the segment summary block.\nFor a block D located on disk at address A, look\nin the segment summary block and ﬁnd its inode number N and offset\nT. Next, look in the imap to ﬁnd where N lives and read N from disk\nthe offset T, look in the inode (or some indirect block) to see where the\ninode thinks the Tth block of this ﬁle is on disk.\naddress A, LFS can conclude that the block D is live.\nBy also recording the version number in the on-disk segment,\nwriting to disk?\nDuring normal operation, LFS buffers writes in a segment, and then\nwrites the segment to disk.\nLFS organizes these writes in a log, i.e., the\noperations (write to a segment, write to the CR).\natomically, LFS actually keeps two CRs, one at either end of the disk, and\nupdating the CR with the latest pointers to the inode map and other infor-\nBecause LFS writes the CR every 30\nare, LFS updates the ﬁle system accordingly and thus recovers much of\nLFS introduces a new approach to updating the disk.\nwriting ﬁles in places, LFS always writes to an unused portion of the\ncient writing, as LFS can gather all updates into an in-memory segment",
      "keywords": [
        "LFS",
        "ﬁle system",
        "disk",
        "inode",
        "Log-structured File Systems",
        "ﬁle",
        "data block",
        "File Systems",
        "block",
        "UNIX ﬁle system",
        "system",
        "Inode Map",
        "data",
        "writes",
        "inode number"
      ],
      "concepts": [
        "lfs",
        "writes",
        "writing",
        "disk",
        "segment",
        "segments",
        "block",
        "update",
        "updated",
        "old"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 47,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lfs",
          "inode",
          "disk",
          "ﬁle",
          "block"
        ],
        "semantic": [],
        "merged": [
          "lfs",
          "inode",
          "disk",
          "ﬁle",
          "block"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38515839128325174,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119517+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 559-566)",
      "start_page": 559,
      "end_page": 566,
      "summary": "LOG-STRUCTURED FILE SYSTEMS\na similar copy-on-write approach to writing to disk, and thus the intel-\nLOG-STRUCTURED FILE SYSTEMS\nLOG-STRUCTURED FILE SYSTEMS\nA paper that showed the LFS performance sometimes has problems, particularly for workloads with\n[SO90] “Write-Only Disk Caches”\nHow should systems ensure that the data written to storage is pro-\nDisk Failure Modes\nAs you learned in the chapter about RAID, disks are not perfect, and\nIn early RAID systems, the model of failure was\nquite simple: either the entire disk is working, or it fails completely, and\ndisk failure makes building RAID relatively simple [S90].\nmodern disks exhibit.\nin great detail [B+07, B+08], modern disks will occasionally seem to be\nconsideration: latent-sector errors (LSEs) and block corruption.\nTable 44.1: Frequency of LSEs and Block Corruption\nLSEs arise when a disk sector (or group of sectors) has been damaged\nFor example, if the disk head touches the surface for some\nin-disk error correcting codes (ECC) are used by the drive to determine\nwhether the on-disk bits in a block are good, and in some cases, to ﬁx\ntion to ﬁx the error, the disk will return an error when a request is issued\nThere are also cases where a disk block becomes corrupt in a way not\ndetectable by the disk itself.\nFor example, buggy disk ﬁrmware may write\na block to the wrong location; in such a case, the disk ECC indicates the\nrupted when it is transferred from the host to the disk across a faulty\nbus; the resulting corrupt data is stored by the disk, but it is not what\nthe are silent faults; the disk gives no indication of the problem when\ndescribes this more modern view of disk failure as\nthe fail-partial disk failure model [P+05].\nIn this view, disks can still fail\never, disks can also seemingly be working and have one or more blocks\nThus, when accessing a seemingly-working disk, once in a while\nit may either return an error when trying to read or write a given block\nor block corruption over the course of the study (about 3 years, over\n1.5 million disk drives).\n• LSEs increase with disk size\n• Most disks with LSEs have less than 50\n• Disks with LSEs are more likely to develop additional LSEs\n• Disk scrubbing is useful (most LSEs were found this way)\n• Chance of corruption varies greatly across different drive models\n• Workload and disk size have little impact on corruption\n• Most disks with corruption only have a few corruptions\n• Corruption is not independent with a disk or across disks in RAID\nery to detect and recovery from both LSEs and block corruption.\nGiven these two new modes of partial disk failure, we should now try\nHow should a storage system handle latent sector errors?\ntries to access a block, and the disk returns an error, the storage system\nOne particularly interesting problem arises in RAID-4/5 systems\nwhen both full-disk faults and LSEs occur in tandem.\nan entire disk fails, the RAID tries to reconstruct the disk (say, onto a\nhot spare) by reading through all of the other disks in the parity group\nis encountered on any one of the other disks, we have a problem: the\nFor example, NetApp’s RAID-DP has the equivalent of two parity disks\nthe form of an extra disk for the second parity block.\nDetecting Corruption: The Checksum\nvia data corruption.\nwhen corruption arises, and thus leads to disks returning bad data?\nCRUX: HOW TO PRESERVE DATA INTEGRITY DESPITE CORRUPTION\nUnlike latent sector errors, detection of corruption is a key problem.\nThe primary mechanism used by modern storage systems to preserve\nif data has somehow been corrupted or altered by storing the checksum",
      "keywords": [
        "LOG-STRUCTURED FILE SYSTEMS",
        "disk",
        "system",
        "Data",
        "FILE SYSTEMS",
        "storage system",
        "Data Integrity",
        "LOG-STRUCTURED FILE",
        "block",
        "LSEs",
        "FILE",
        "corruption",
        "Disk Failure",
        "File System Design",
        "ﬁle system"
      ],
      "concepts": [
        "disk",
        "data",
        "systems",
        "drive",
        "raid",
        "blocks",
        "corruption",
        "corrupt",
        "corruptions",
        "errors"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 52,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 46,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 44,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 45,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "disk",
          "corruption",
          "lses",
          "disks",
          "disk failure"
        ],
        "semantic": [],
        "merged": [
          "disk",
          "corruption",
          "lses",
          "disks",
          "disk failure"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27463401854618497,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119571+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 567-574)",
      "start_page": 567,
      "end_page": 574,
      "summary": "Common Checksum Functions\nA number of different functions are used to compute checksums, and\nOne simple checksum function that some use is based on exclusive\nWith XOR-based checksums, the checksum is computed sim-\nply by XOR’ing each chunk of the data block being checksummed, thus\ndisk sector or block, but it will serve for the example).\nto see what the resulting checksum will be: simply perform an XOR over\neach column to get the ﬁnal checksum value:\nXOR is a reasonable checksum but has its limitations.\ntwo bits in the same position within each checksummed unit change, the\nchecksum will not detect the corruption.\ninvestigated other checksum functions.\nAnother simple checksum function is addition.\nchecksum is known to be almost as strong as the CRC (described next),\nOne ﬁnal commonly-used checksum is known as a cyclic redundancy\nsume you wish to compute the checksum over a data block D.\nfect checksum: it is possible two data blocks with non-identical contents\nwill have identical checksums, something referred to as a collision.\nfact should be intuitive: after all, computing a checksum is taking some-\nIn choosing a good checksum function, we are thus\nChecksum Layout\nNow that you understand a bit about how to compute a checksum, let’s\nnext analyze how to use checksums in a storage system.\nwe must address is the layout of the checksum, i.e., how should check-\nThe most basic approach simply stores a checksum with each disk sec-\nGiven a data block D, let us call the checksum over that\ndata C(D).\nThus, without checksums, the disk layout looks like this:\nWith checksums, the layout adds a single checksum for every block:\nBecause checksums are usually small (e.g., 8 bytes), and disks only can\n8 bytes per sector can be used to store the checksum.\nout a way to store the checksums packed into 512-byte blocks.\nIn this scheme, the n checksums are stored together in a sector, fol-\nlowed by n data blocks, followed by another checksum sector for the next\nblock D1, it has to read in the checksum sector containing C(D1), update\nC(D1) in it, and then write out the checksum sector as well as the new\ndata block D1 (thus, one read and two writes).\none checksum per sector) just performs a single write.\nUsing Checksums\nally understand how to use the checksums.\nWhen reading a block D, the\nclient (i.e., ﬁle system or storage controller) also reads its checksum from\ndisk Cs(D), which we call the stored checksum (hence the subscript Cs).\nThe client then computes the checksum over the retrieved block D, which\nwe call the computed checksum Cc(D).\npares the stored and computed checksums; if they are equal (i.e., Cs(D)\n== Cc(D), the data has likely not been corrupted, and thus can be safely\nchecksum reﬂects the value of the data at that time).\na corruption, which our checksum has helped us to detect.\narises in disk and RAID controllers which write the data to disk correctly,\nHow should a storage system or disk controller detect misdirected\nWhat additional features are required from the checksum?\nto each checksum.\nthe checksum C(D) as well as the disk and sector number of the block,\nSpeciﬁcally, if the client is reading block 4 on disk\n10 (D10,4), the stored information should include that disk number and\nexample of what this added information would look like on a two-disk\nchecksums are usually small (e.g., 8 bytes) whereas the blocks are much\nredundancy on disk: for each block, the disk number is repeated within\nThe obvious question here is: do any of our checksumming strategies\nfrom above (e.g., basic checksums, or physical identity) help to detect\nmatching checksum, and the physical ID used above (disk number and\nHow should a storage system or disk controller detect lost writes?\nWhat additional features are required from the checksum?\nby immediately reading back the data after a write, a system can ensure\nthat the data indeed reached the disk surface.\nSome systems add a checksum elsewhere in the system to detect lost\nThus, even if the write to a data block itself is lost, the check-\nchecksums actually get checked?\nsystem, and checking whether checksums are still valid, the disk system\nOverheads Of Checksumming\nstorage medium) itself; each stored checksum takes up room on the disk,\nbyte checksum per 4 KB data block, for a 0.19% on-disk space overhead.\nchecksums as well as the data itself.\nthe checksum and then discards it once done, this overhead is short-lived\nOnly if checksums are kept in memory (for\nchecksum over each block, both when the data is stored (to determine\nthe value of the stored checksum) as well as when it is accessed (to com-\npute the checksum again and compare it against the stored checksum).\nthat use checksums (including network stacks), is to combine data copy-\ning and checksumming into one streamlined activity; because the copy is\nuser buffer), combined copying/checksumming can be quite effective.\nBeyond CPU overheads, some checksumming schemes can induce ex-\ntra I/O overheads, particularly when checksums are stored distinctly from\nWe have discussed data protection in modern storage systems, focus-\ning on checksum implementation and usage.\nDifferent checksums protect\n[B+08] “An Analysis of Data Corruption in the Storage Stack”\nA great simple tutorial on checksums, available to you for the amazing cost of free.\n[F82] “An Arithmetic Checksum for Serial Transmissions”\nFletcher’s original work on his eponymous checksum.\nOf course, he didn’t call it the Fletcher checksum,\nThis work of ours, joint with colleagues at NetApp, explores how different checksum schemes work (or\ndon’t work) in protecting data.\nOur paper on how disks have partial failure modes, which includes a detailed study of how ﬁle systems\ncommunity, thus helping to yield a new more robust group of ﬁle systems to store your data.",
      "keywords": [
        "Checksum",
        "DATA",
        "DATA INTEGRITY",
        "block",
        "disk",
        "data block",
        "system",
        "storage system",
        "stored checksum",
        "PROTECTION",
        "data protection",
        "write",
        "INTEGRITY AND PROTECTION",
        "FREE LUNCH",
        "INTEGRITY"
      ],
      "concepts": [
        "checksum",
        "disks",
        "data",
        "block",
        "systems",
        "overheads",
        "corruption",
        "corrupted",
        "storage",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 52,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.564,
          "base_score": 0.414,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 51,
          "title": "",
          "score": 0.525,
          "base_score": 0.375,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "checksum",
          "checksums",
          "block",
          "data",
          "disk"
        ],
        "semantic": [],
        "merged": [
          "checksum",
          "checksums",
          "block",
          "data",
          "disk"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2414858877117383,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119619+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 575-582)",
      "start_page": 575,
      "end_page": 582,
      "summary": "Student: Wow, ﬁle systems seem interesting(!), and yet complicated.\nStudent: I also learned about cool things like disk scheduling, and about data\nProfessor: I like those topics too.\nStudent: And I also liked all the thought that has gone into building technology-\nProfessor: Good question!\nSYSTEMS\nProfessor: And thus we reach our ﬁnal little piece in the world of operating\nsystems: distributed systems.\nsystems.\nall-knowing professor?\ntime to get the peach.\nStudent: This peach analogy is working less and less for me.\nProfessor: So anyhow, forget about the peaches.\nBuilding distributed systems\nis hard, because things fail all the time.\nIt’s like the whole world is working against you!\nStudent: But I use distributed systems all the time, right?\nProfessor: Yes!\nStudent: Well, it seems like they mostly work.\nProfessor: Yes, it is amazing.\nas to ensure that even though some machines have failed, the entire system stays\nProfessor: It does seem so.\nDistributed Systems\nDistributed systems have changed the face of the world.\ndistributed systems interesting.\nA number of new challenges arise when building a distributed system.\nThe major one we focus on is failure; machines, disks, networks, and\nsoftware all fail from time to time, as we do not (and likely, will never)\nwe build a modern web service, we’d like it to appear to clients as if it\nHOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL\nYes, machines fail;\nDISTRIBUTED SYSTEMS\nlinks and machines, and lack of buffer space for incoming packets all lead\nbuild reliable services atop such unreliable networks, we must consider\nin a distributed system: communication.\nwithin a distributed system communicate with one another?\nThe central tenet of modern networking is that communication is fun-\nwork link or packet router or even the remote host, are somehow dam-\nMore fundamental however is packet loss due to lack of buffering\narrives at a router; for the packet to be processed, it must be placed in\nDISTRIBUTED SYSTEMS\nint sd = UDP_Open(20000);\nint rc = UDP_FillSockAddr(&addr, \"machine.cs.wisc.edu\", 10000);\nrc = UDP_Write(sd, &addr, message, BUFFER_SIZE);\nint rc = UDP_Read(sd, &addr2, buffer, BUFFER_SIZE);\nint sd = UDP_Open(10000);\nint rc = UDP_Read(sd, &s, buffer, BUFFER_SIZE);\nrc = UDP_Write(sd, &s, reply, BUFFER_SIZE);\nFigure 47.1: Example UDP/IP Client/Server Code\npacket loss again arises.\nThus, packet loss is fundamental in networking.\ncations know how to deal with packet loss, it is sometimes useful to let\nthem communicate with a basic unreliable messaging layer, an example\ncommunication endpoint; processes on other machines (or on the same\nmachine) send UDP datagrams to the original process (a datagram is a\nDISTRIBUTED SYSTEMS\nif (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) {\nint UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) {\nint UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nreturn sendto(sd, buffer, n, 0, (struct sockaddr *) addr, addrLen);\nint UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nreturn recvfrom(sd, buffer, n, 0, (struct sockaddr *) addr,\nbegin building distributed systems!\nUDP is a great example of an unreliable communication layer.\ndestination and not worry about packet loss, we need more.\nwe need reliable communication on top of an unreliable network.",
      "keywords": [
        "UDP",
        "distributed systems",
        "systems",
        "int",
        "distributed",
        "Student",
        "struct sockaddr",
        "buffer",
        "addr",
        "Professor",
        "int UDP",
        "struct",
        "time",
        "sockaddr",
        "COMMUNICATION"
      ],
      "concepts": [
        "professor",
        "systems",
        "student",
        "networks",
        "communication",
        "communicate",
        "machines",
        "distribution",
        "distributed",
        "messages"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 61,
          "title": "",
          "score": 0.66,
          "base_score": 0.66,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 1,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 63,
          "title": "",
          "score": 0.61,
          "base_score": 0.46,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 2,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sd",
          "distributed",
          "int",
          "distributed systems",
          "addr"
        ],
        "semantic": [],
        "merged": [
          "sd",
          "distributed",
          "int",
          "distributed systems",
          "addr"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29918348816108276,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119671+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 583-591)",
      "start_page": 583,
      "end_page": 591,
      "summary": "Before sending a message\nfrom one machine to another, compute a checksum over the bytes of the\nmessage.\nThen send both the message and the checksum to the desti-\nAt the destination, the receiver computes a checksum over the\nincoming message as well; if this computed checksum matches the sent\nchecksum, the receiver can feel some assurance that the data likely did\nReliable Communication Layers\nTo build a reliable communication layer, we need some new mech-\nexample in which a client is sending a message to a server over an unreli-\nknow that the receiver has actually received the message?\nThe idea is simple: the sender sends a message to the re-\nceiver; the receiver then sends a short message back to acknowledge its\n[send message]\nReceiver\n[receive message]\nFigure 47.3: Message Plus Acknowledgment\nWhen the sender receives an acknowledgment of the message, it can\nthen rest assured that the message did indeed receive the original mes-\nHowever, what should the sender do if it does not receive an ac-\n[send message;\nReceiver\n[receive message]\nFigure 47.4: Message Plus Acknowledgment: Dropped Request\nWhen the sender sends a message, the sender now sets a timer\nhas been received, the sender concludes that the message has been lost.\nThe sender then simply performs a retry of the send, sending the same\nmessage again with hopes that this time, it will get through.\napproach to work, the sender must keep a copy of the message around,\nexample, it is not the original message that gets lost, but the acknowledg-\nno ack was received, and thus a timeout and retry are in order.\nthe perspective of the receiver, it is quite different: now the same message\nare aiming for a reliable message layer, we also usually want to guarantee\nthat each message is received exactly once by the receiver.\nTo enable the receiver to detect duplicate message transmission, the\nsender has to identify each message in some unique way, and the receiver\nneeds some way to track whether it has already seen each message be-\nreceives the data.\nThus, the sender receives the ack but the message is not\nsender could generate a unique ID for each message; the receiver could\n[send message;\nReceiver\n[receive message]\n[receive message]\nFigure 47.5: Message Plus Acknowledgment: Dropped Reply\nthe sender and receiver agree upon a start value (e.g., 1) for a counter\nWhenever a message is sent, the current\nvalue of the counter is sent along with the message; this counter value\nAfter the message is sent, the sender\nof the incoming message from that sender.\nsage (N) matches the receiver’s counter (also N), it acks the message and\nis the ﬁrst time this message has been received.\nments its counter (to N + 1), and waits for the next message.\nIf the ack is lost, the sender will timeout and re-send message N.\ntime, the receiver’s counter is higher (N +1), and thus the receiver knows\nit has already received this message.\nThus it acks the message but does\nThe most commonly used reliable communication layer is known as\nGiven a basic messaging layer, we now approach the next question\ncorrectly is an important aspect of using timeouts to retry message sends.\nIf the timeout is too small, the sender will re-send messages needlessly,\nspective of a single client and server, is thus to wait just long enough to\nHowever, there are often more than just a single client and server in a\nmany clients sending to a single server, packet loss at the server may be\na different adaptive manner; for example, after the ﬁrst timeout, a client\nmemory (DSM) systems enable processes on different machines to share\nhandler sends a message to some other machine to fetch the page, install\ntical impact; nobody builds reliable distributed systems using DSM today.\nfunction arguments and results into messages by automating it.\nwishing to use this RPC service would link with this client stub and call\ninto it in order to make RPCs. Internally, each of these functions in the client stub do all of the work\nappears as a function call (e.g., the client calls func1(x)); internally, the\n• Create a message buffer.\nA message buffer is usually just a con-\n• Pack the needed information into the message buffer.\nmessage.\n• Send the message to the destination RPC server.\nit operate correctly, are handled by the RPC run-time library, de-\n• Unpack the message.\nduring this time, a main thread keeps receiving other requests, and per-\nThe run-time library handles much of the heavy lifting in an RPC system;\nmost performance and reliability issues are handled herein.\nmachine running the desired RPC service, as well as the port number it is\nSpeciﬁcally, should the RPC system use a reliable pro-\nlike to reliably receive a reply.\nUnfortunately, building RPC on top of a reliable communication layer\nsion above how reliable communication layers work: with acknowledg-\nThus, when the client sends an RPC request\nthe reply to the client, the client acks it so that the server knows it was\nreceived.\nof a reliable communication layer, two “extra” messages are sent.\nlayer, but does add the responsibility of providing reliability to the RPC\nThere are some other issues an RPC run-time must handle as well.\n(from the receiver to sender) when the reply isn’t immediately generated;\nthis lets the client know the server received the request.\ntime has passed, the client can periodically ask whether the server is still\nworking on the request; if the server keeps saying “yes”, the client should\nThe run-time must also handle procedure calls with large arguments,\none larger logical whole); if not, the RPC run-time may have to implement\nan excellent example: reliable ﬁle transfer between two machines.\nexample, say we build a reliable communication protocol and use it to\nthat every byte sent by a sender will be received in order by the receiver,\nback the ﬁle on the receiver disk, compute a checksum, and compare that\nRPC packages often handle this by providing a well-deﬁned endian-\nchine sending or receiving a message matches the endianness of XDR,\nmessages are just sent and received as expected.\nthe message must be converted.",
      "keywords": [
        "message",
        "DISTRIBUTED SYSTEMS",
        "RPC",
        "RPC system",
        "SYSTEMS",
        "sender",
        "client",
        "server",
        "receiver",
        "DISTRIBUTED",
        "call",
        "send message",
        "receive message",
        "checksum",
        "DSM systems"
      ],
      "concepts": [
        "received",
        "receive",
        "receives",
        "message",
        "messaging",
        "systems",
        "reliable",
        "reliability",
        "reliably",
        "server"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 59,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 55,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 61,
          "title": "",
          "score": 0.464,
          "base_score": 0.314,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.438,
          "base_score": 0.288,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.42,
          "base_score": 0.27,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "message",
          "receiver",
          "sender",
          "rpc",
          "received"
        ],
        "semantic": [],
        "merged": [
          "message",
          "receiver",
          "sender",
          "rpc",
          "received"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.17672758801534239,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119719+00:00"
      }
    },
    {
      "chapter_number": 58,
      "title": "Segment 58 (pages 592-602)",
      "start_page": 592,
      "end_page": 602,
      "summary": "calls on servers; the RPC package handles all of the gory details, includ-\nOne of the ﬁrst uses of distributed client/server computing was in the\nrealm of distributed ﬁle systems.\nnumber of client machines and one server (or a few); the server stores the\ndata on its disks, and clients request data through well-formed protocol\nClient 0\nClient 1\nClient 2\nClient 3\nServer\nFigure 48.1: A Generic Client/Server System\nAs you can see from the picture, the server has the disks, and clients\nThus, if you access a ﬁle on one machine\nA simple client/server distributed ﬁle system has more components\nOn the client side, there are\nside ﬁle system.\nﬁle system (such as open(), read(), write(), close(), mkdir(),\nThus, to client\nThe role of the client-side ﬁle system is to execute the actions needed\nFor example, if the client issues a read()\nrequest, the client-side ﬁle system may send a message to the server-side\nﬁle system (or, more commonly, the ﬁle server) to read a particular block;\nthe ﬁle server will then read the block from disk (or its own in-memory\ncache), and send a message back to the client with the requested data.\nThe client-side ﬁle system will then copy the data into the user buffer\nClient-side File System\nFile Server\nimportant pieces of software in a client/server distributed ﬁle system: the\nclient-side ﬁle system and the ﬁle server.\nbetween the client and the server; if the network acts strangely (for ex-\nample, if it becomes partitioned and clients and servers are working but\nclients and servers would use to communicate.\nIn a multiple-client, single-server environment,\ncurrently open at each client, or the current ﬁle pointer position for a ﬁle,\nSimply put, the server does not track anything about what clients are\nFigure 48.3: Client Code: Reading From A File\nNow imagine that the client-side ﬁle system opens the ﬁle by sending\na protocol message to the server saying “open the ﬁle ’foo’ and give me\nThe ﬁle server then opens the ﬁle locally on its side\nclient application uses that descriptor to call the read() system call; the\nclient-side ﬁle system then passes the descriptor in a message to the ﬁle\nserver, saying “read some bytes from the ﬁle that is referred to by the\nthe client and the server (Ousterhout calls this distributed state [O91]).\nthe server crashes after the ﬁrst read completes, but before the client\nthe client then issues the second read.\ntion, the client and server would have to engage in some kind of recovery\nprotocol, where the client would make sure to keep enough information\nImagine, for example, a client that opens a ﬁle\nThe open() uses up a ﬁle descriptor on the server; how\ncan the server know it is OK to close a given ﬁle?\nclient would eventually call close() and thus inform the server that the\nﬁle should be closed.\nHowever, when a client crashes, the server never\nto close the ﬁle.\nwould require the server to track open ﬁles); however, the client appli-\nstanding the ﬁle handle.\nmany of the protocol requests include a ﬁle handle.\nYou can think of a ﬁle handle as having three important components: a\nthree items comprise a unique identiﬁer for a ﬁle or directory that a client\nThe volume identiﬁer informs the server which ﬁle sys-\ntem the request refers to (an NFS server can export more than one ﬁle\nsystem); the inode number tells the server which ﬁle within that partition\nber is reused, the server ensures that a client with an old ﬁle handle can’t\nthe LOOKUP protocol message is used to obtain a ﬁle handle, which is\ndirectory) plus its attributes are passed back to the client from the server.\nFor example, assume the client already has a directory ﬁle handle for\nthrough the NFS mount protocol, which is how clients and servers ﬁrst\nIf an application running on the client opens the ﬁle\n/foo.txt, the client-side ﬁle system sends a lookup request to the server,\nﬁle handle (and attributes) for foo.txt will be returned.\nOnce a ﬁle handle is available, the client can issue READ and WRITE\nprotocol messages on a ﬁle to read or write the ﬁle, respectively.\nREAD protocol message requires the protocol to pass along the ﬁle handle\nof the ﬁle along with the offset within the ﬁle and number of bytes to read.\nThe server then will be able to issue the read (after all, the handle tells the\nserver which volume and which inode to read from, and the offset and\ncount tells it which bytes of the ﬁle to read) and return the data to the\nexcept the data is passed from the client to the server, and just a success\nturned into a ﬁle system across the client-side ﬁle system and the ﬁle\nserver.\nThe client-side ﬁle system tracks open ﬁles, and generally trans-\nserver simply responds to each protocol message, each of which has all\nFor example, let us consider a simple application which reads a ﬁle.\nmakes, and what the client-side ﬁle system and ﬁle server do in respond-\ndescriptor to an NFS ﬁle handle as well as the current ﬁle pointer.\nenables the client to turn each read request (which you may have noticed\nread protocol message which tells the server exactly which bytes from\nthe ﬁle to read.\nﬁle position; subsequent reads are issued with the same ﬁle handle but a\nWhen the ﬁle\nis opened for the ﬁrst time, the client-side ﬁle system sends a LOOKUP\nClient\nServer\nreturn ﬁle descriptor to application\nget NFS ﬁle handle (FH)\nupdate ﬁle position (+bytes read)\nTable 48.1: Reading A File: Client-side And File Server Actions",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "client-side ﬁle system",
        "Network File System",
        "system",
        "File System",
        "client",
        "distributed ﬁle system",
        "server",
        "ﬁle handle",
        "File",
        "NFS",
        "ﬁle server",
        "read",
        "NFS ﬁle handle"
      ],
      "concepts": [
        "servers",
        "client",
        "read",
        "protocol",
        "returns",
        "network",
        "request",
        "requested",
        "open",
        "including"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 61,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 59,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 46,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "client",
          "server",
          "ﬁle handle",
          "client ﬁle"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "client",
          "server",
          "ﬁle handle",
          "client ﬁle"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33980029493979536,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119777+00:00"
      }
    },
    {
      "chapter_number": 59,
      "title": "Segment 59 (pages 603-610)",
      "start_page": 603,
      "end_page": 610,
      "summary": "Handling Server Failure with Idempotent Operations\nWhen a client sends a message to the server, it sometimes does not re-\nrequest, the client sets a timer to go off after a speciﬁed time period.\nIf the server\nreplies, all is well and the client has neatly handled the problem.\nidempotent, as they only read information from the ﬁle server and do not\nMore interestingly, WRITE requests are also idempotent.\nfor example, a WRITE fails, the client can simply retry it.\nClient\nServer\nClient\nServer\nCase 3: Reply lost on way back from Server\nClient\nServer\nWRITE request was simply lost (Case 1 above), the client will retry it, the\nserver will perform the write, and all will be well.\nif the server happened to be down while the request was sent, but back\nFinally, the server may in fact receive the WRITE\nrequest, issue the write to its disk, and send a reply.\nlost (Case 3), again causing the client to re-send the request.\nserver receives the request again, it will simply do the exact same thing:\nIf the client this time\nThus, in NFS, if the ﬁle server\nImproving Performance: Client-side Caching\nall read and write requests across the network can lead to a big perfor-\nthe sub-heading above, is client-side caching.\nThe NFS client-side ﬁle\nsystem caches ﬁle data (and metadata) that it has read from the server in\nThe cache also serves as a temporary buffer for writes.\nWhen a client\napplication ﬁrst writes to a ﬁle, the client buffers the data in client mem-\nory (in the same cache as the data it read from the ﬁle server) before writ-\nthe data in the client-side ﬁle system’s cache); only later does the data get\nwritten out to the ﬁle server.\nThus, NFS clients cache data and performance is usually great and\nsort of system with multiple client caches introduces a big and interesting\nThe cache consistency problem is best illustrated with two clients and\nImagine client C1 reads a ﬁle F, and keeps a copy of the\nﬁle in its local cache.\nServer S\nFinally, there is a third client, C3, which has not yet accessed the ﬁle F.\nmay buffer its writes in its cache for a time before propagating them to the\nserver; in this case, while F[v2] sits in C2’s memory, any access of F from\nanother client (say C3) will fetch the old version of the ﬁle (F[v1]).\nby buffering writes at the client, other clients may get stale versions of the\ncase, C2 has ﬁnally ﬂushed its writes to the ﬁle server, and thus the server\nprogram running on C1 reads ﬁle F, it will get a stale version (F[v1]) and\nspeciﬁcally, when a ﬁle is written to and subsequently closed by a client\napplication, the client ﬂushes all updates (i.e., dirty pages in the cache)\nto the server.\nSecond, to address the stale-cache problem, NFSv2 clients ﬁrst check\nto see whether a ﬁle has changed before using its cached contents.\ncally, when opening a ﬁle, the client-side ﬁle system will issue a GETATTR\nrequest to the server to fetch the ﬁle’s attributes.\nﬁle was fetched into the client cache, the client invalidates the ﬁle, thus\nremoving it from the client cache and ensuring that subsequent reads will\ngo to the server and retrieve the latest version of the ﬁle.\nhand, the client sees that it has the latest version of the ﬁle, it will go\ncache problem, they realized a new problem; suddenly, the NFS server\nrequests to the server to make sure no one else had changed the ﬁle.\nclient thus bombards the server, constantly asking “has anyone changed\nto each client.\nA client would still validate a ﬁle before accessing it, but\nThe attributes for a particular ﬁle were placed in the cache when the ﬁle\nclient and then soon deleted, it would still be forced to the server.\nAnd thus we have described the oddity that is NFS client caching.\nImplications on Server-Side Write Buffering\nOur focus so far has been on client caching, and that is where most\ncess to the client on a WRITE protocol request could result in incorrect\nThe answer lies in our assumptions about how clients handle server\nImagine the following sequence of writes as issued by a client:\nwrites were issued to the server as three distinct WRITE protocol mes-\nAssume the ﬁrst WRITE message is received by the server and\nthe second write is just buffered in memory, and the server also reports\nit success to the client before forcing it to disk; unfortunately, the server\nthe third write request, which also succeeds.\nThus, to the client, all the requests succeeded, but we are surprised\nBecause the server told the client that the second write was\nTo avoid this problem, NFS servers must commit each write to stable\nables the client to detect server failure during a write, and thus retry until\nNFS server that can perform writes quickly; one trick they use is to ﬁrst\nof having to write to disk right away; the second trick is to use a ﬁle sys-\nclient, single-server system can complicate things.\nserver caching can be tricky: writes to the server must be forced to stable\nBrian Pawlowski, Chet Juszczak, Peter Staubach, Carl Smith, Diane Lebel, Dave Hitz",
      "keywords": [
        "NETWORK FILE SYSTEM",
        "NFS",
        "client",
        "Server",
        "ﬁle",
        "FILE SYSTEM",
        "WRITE",
        "NETWORK FILE",
        "NFS Cache Consistency",
        "SUN’S NETWORK FILE",
        "cache",
        "NFS clients cache",
        "NFS File Server",
        "SYSTEM",
        "NFS server"
      ],
      "concepts": [
        "client",
        "server",
        "write",
        "writing",
        "caching",
        "caches",
        "request",
        "requests",
        "version",
        "versions"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 57,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 52,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 54,
          "title": "",
          "score": 0.55,
          "base_score": 0.4,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "client",
          "server",
          "ﬁle",
          "nfs",
          "cache"
        ],
        "semantic": [],
        "merged": [
          "client",
          "server",
          "ﬁle",
          "nfs",
          "cache"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.267440206170601,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119831+00:00"
      }
    },
    {
      "chapter_number": 60,
      "title": "Segment 60 (pages 611-622)",
      "start_page": 611,
      "end_page": 622,
      "summary": "design a distributed ﬁle system such that a server can support as many\ntween clients and servers.\nIn NFS, for example, the protocol forces clients\nclients a server can respond to and thus limit scalability.\nIn AFS, cache\nclient will generally receive the latest consistent copy from the server.\nOne of the basic tenets of all versions of AFS is whole-ﬁle caching on\nthe local disk of the client machine that is accessing a ﬁle.\nopen() a ﬁle, the entire ﬁle (if it exists) is fetched from the server and\nstored in a ﬁle on your local disk.\nwrite() operations are redirected to the local ﬁle system where the ﬁle is\nback to the server.\nan entire ﬁle) and does so in client memory (not local disk).\nopen(), the AFS client-side code (which the AFS designers call Venus)\nwould send a Fetch protocol message to the server.\nple, /home/remzi/notes.txt) to the ﬁle server (the group of which\nsired ﬁle, and ship the entire ﬁle back to the client.\nwould then cache the ﬁle on the local disk of the client (by writing it to\ncalls are strictly local in AFS (no communication with the server occurs);\nthey are just redirected to the local copy of the ﬁle.\nand write() calls act just like calls to a local ﬁle system, once a block\nis accessed, it also may be cached in client memory.\nclient memory to cache copies of blocks that it has in its local disk.\nnally, when ﬁnished, the AFS client checks if the ﬁle has been modiﬁed\nback to the server with a Store protocol message, sending the entire ﬁle\nThe next time the ﬁle is accessed, AFSv1 does so much more efﬁ-\nSpeciﬁcally, the client-side code ﬁrst contacts the server (using\nthe TestAuth protocol message) in order to determine whether the ﬁle\nIf not, the client would use the locally-cached copy, thus\nsion of the protocol only cached ﬁle contents; directories, for example,\nWith many clients accessing the server at once, the designers of AFS\nThus, servers spent much of their time telling clients\nwhether it was OK to used their cached copies of a ﬁle.\nThe two problems above limited the scalability of AFS; the server CPU\nclient/server interactions.\nto the client that the server will inform the client when a ﬁle that the\nclient is caching has been modiﬁed.\nclient no longer needs to contact the server to ﬁnd out if a cached ﬁle is\nRather, it assumes that the ﬁle is valid until the server tells it\nthe NFS ﬁle handle) instead of pathnames to specify which ﬁle a client\nAn FID in AFS consists of a volume identiﬁer, a ﬁle\nﬁle, the client would walk the pathname, one piece at a time, caching the\nFor example, if a client accessed the ﬁle /home/remzi/notes.txt,\ndirectory, but home and its children were in AFS), the client would ﬁrst\nFetch the directory contents of home, put them in the local-disk cache,\nremzi, put it in the local-disk cache, and setup a callback on the server\nFinally, the client would Fetch notes.txt, cache this regular\nﬁle in the local disk, setup a callback, and ﬁnally return a ﬁle descriptor\ndirectory or ﬁle, the AFS client would establish a callback with the server,\nthus ensuring that the server would notify the client of a change in its\nServer\nwrite remzi to local disk cache\nwrite notes.txt to local disk cache\nperform local read() on cached copy\nif ﬁle has changed, ﬂush to server\nTable 49.1: Reading A File: Client-side And File Server Actions\nremzi/notes.txt generates many client-server messages (as described\nﬁle notes.txt, and thus subsequent accesses are entirely local and require\ncached at the client, AFS behaves nearly identically to a local disk-based\nﬁle system.\nbe just as fast as accessing a ﬁle locally.\nWhen discussing distributed ﬁle systems, much is made of the cache con-\nnot solve all problems with regards to ﬁle access from multiple clients.\nof a ﬁle?\nWith cache staleness, the question is: once the server has a new\nBecause of callbacks and whole-ﬁle caching, the cache consistency pro-\nBetween different machines, AFS makes updates visible at the server\nA client opens a ﬁle, and then writes to it (perhaps\nWhen it is ﬁnally closed, the new ﬁle is ﬂushed to the server\n(and thus visibile); the server then breaks callbacks for any clients with\ncached copies, thus ensuring that clients will no longer read stale copies\nof the ﬁle; subsequent opens on those clients will require a re-fetch of the\nnew version of the ﬁle from the server.\nIn this case, writes to a ﬁle are immediately visible to\nother local processes (i.e., a process does not have to wait until a ﬁle is\nServer\nchines are modifying a ﬁle at the same time, AFS naturally employs what\nupdate the entire ﬁle on the server last and thus will be the “winning”\nﬁle, i.e., the ﬁle that remains on the server for others to see.\na ﬁle that was generated in its entirety either by one client or the other.\nof individual blocks may be ﬂushed out to the server as each client is up-\ndating the ﬁle, and thus the ﬁnal ﬁle on the server could end up as a mix\nits cache state, and the server (Server), all operating on a single ﬁle called,\nmessages; for example, imagine C1 had ﬁle F cached on its local disk, and\nclients caching the ﬁle to remove it from their local caches.\nthe next access to ﬁle F, C1 should ﬁrst ask the server (with a TestAuth\nprotocol message) whether its cached copy of ﬁle F is still valid; if so, C1\ntimely manner, or risk a client accessing a stale ﬁle.\nit is up and running again, or by having clients check that the server is\nwith NFS, clients hardly noticed a server crash.\nIndeed, each server could\ncause in the common case, all ﬁle accesses were local; ﬁle reads usually\nclient created a new ﬁle or wrote to an existing one was there need to send\na Store message to the server and thus update the ﬁle with new contents.\ncommon ﬁle-system access scenarios with NFS.\nAFS\nAFS/NFS\n1. Small ﬁle, sequential read\n2. Small ﬁle, sequential re-read\n5. Large ﬁle, sequential read\n6. Large ﬁle, sequential re-read\n7. Large ﬁle, single read\n8. Small ﬁle, sequential write\n9. Large ﬁle, sequential write\nLarge ﬁle, single write\nwork to the remote server for a ﬁle block takes Lnet time units.\nsmall ﬁle sequential read) roughly takes on either NFS or AFS.\nreading a ﬁle (e.g., Workloads 1, 3, 5), the time to fetch the ﬁle from the re-\nAFS would be slower in this case, as it has to write the ﬁle to local disk;\nhowever, those writes are buffered by the local (client-side) ﬁle system\nthat AFS reads from the local cached copy would be slower, again be-\ncause AFS stores the cached copy on disk.\nhere from local ﬁle system caching; reads on AFS would likely hit in the\nclient-side memory cache, and performance would be similar to NFS.\nBecause AFS has a large local disk cache, it will\nonly can cache blocks in client memory; as a result, if a large ﬁle (i.e., a ﬁle\nbigger than local memory) is re-read, the NFS client will have to re-fetch\nthe entire ﬁle from the remote server.\nthe ﬁle to the local cached copy; when the ﬁle is closed, the AFS client\nwill force the writes to the server, as per the protocol.\nwrites in client memory, perhaps forcing some blocks to the server due\nto client-side memory pressure, but deﬁnitely writing them to the server\nwhen the ﬁle is closed, to preserve NFS ﬂush-on-close consistency.\nmight think AFS would be slower here, because it writes all data to local\nHowever, realize that it is writing to a local ﬁle system; those writes\nto disk, and thus AFS reaps the beneﬁts of the client-side OS memory\nFourth, we note that AFS performs worse on a sequential ﬁle over-\nwrite are also creating a new ﬁle; in this case, the ﬁle exists, and is then\nthe client ﬁrst fetches the old ﬁle in its entirety, only to subsequently over-\nperform much better on NFS than AFS (Workloads 7, 11).\nthe AFS protocol fetches the entire ﬁle when the ﬁle is opened; unfortu-\nmodiﬁed, the entire ﬁle is written back to the server, doubling the per-\nNFS client would also have to read the block ﬁrst.\nThe designers of AFS, given their experience in measuring how ﬁle sys-\nNFS, like most UNIX ﬁle systems, has much less\nAFS shows us how distributed ﬁle systems can be built quite differ-\nularly important; by minimizing server interactions (through whole-ﬁle\ncaching and callbacks), each server can support many clients and thus\nwith CIFS (the Windows-based distributed ﬁle system protocol), NFS",
      "keywords": [
        "AFS",
        "Andrew File System",
        "ﬁle",
        "server",
        "ﬁle system",
        "NFS",
        "local ﬁle system",
        "File System",
        "System",
        "client",
        "Andrew File",
        "local",
        "AFS client",
        "File",
        "cache"
      ],
      "concepts": [
        "server",
        "cached",
        "clients",
        "read",
        "writing",
        "writes",
        "protocol",
        "differs",
        "difference",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 53,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 61,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 59,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "server",
          "afs",
          "client",
          "local"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "server",
          "afs",
          "client",
          "local"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3499631319571939,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119890+00:00"
      }
    },
    {
      "chapter_number": 61,
      "title": "Segment 61 (pages 623-630)",
      "start_page": 623,
      "end_page": 630,
      "summary": "[B+91] “Measurements of a Distributed File System”\nAn early paper measuring how people use distributed ﬁle systems.\n[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications”\n[H+88] “Scale and Performance in a Distributed File System”\nThe long journal version of the famous AFS system, still in use in a number of places throughout the\nworld, and also probably the earliest clear thinking on how to build distributed ﬁle systems.\n[S+85] “The ITC Distributed File System: Principles and Design”\nM. Satyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A.\nSpector, M.J. West\nThe older paper about a distributed ﬁle system.\nStudent: Well, that was quick.\nProfessor: Yes, distributed systems are complicated and cool and well worth\nStudent: That’s too bad; I wanted to learn more!\nProfessor: Like what?\nStudent: Well, everything can fail.\nStudent: But by having lots of these things (whether disks, machines, or what-\nStudent: Some basic techniques like retrying are really useful.\nStudent: And you have to think carefully about protocols: the exact bits that\nStudent: Thanks!\nProfessor: Well thank you very much too.\nStudent: So is this the end of the book?\nStudent: Me neither.\nStudent: Go ahead.\nProfessor: No, after you.\nStudent: Please, professors ﬁrst.\nProfessor: No, please, after you.\nStudent: I don’t know how.\nProfessor: Me too.\nabstraction, iv, 112, 395\naddress, 7\naddress-based ordering, 164\naddress-translation cache, 183\nasynchronous I/O, 377\natomic exchange, 296\nattribute cache, 571\nautomatic memory management, 122\naverage memory access time, 228\nback pointer, 508\nbase, 133, 135, 204\nbase and bounds, 133\nBerkeley Systems Distribution, 17\nblock corruption, 436, 527\nblock groups, 481\nBlocked, 29\nblocked, 67, 221\nblocks, 462\nbuffer cache, 493\nC-SCAN, 413\ncache, 183, 227, 407\ncache afﬁnity, 97\ncache coherence, 96\ncache consistency problem, 569\ncache hits, 227\ncache misses, 227\ncache replacement, 192\ncached, 560\ncaches, 94\ncaching, 569\nCircular SCAN, 413\nclient-side ﬁle system, 560\ncode sharing, 146\ndangling pointer, 124\ndata, 391\ndata bitmap, 463, 481, 492\ndata journaling, 498, 503\ndata region, 462\ndata structures, 32, 461\ndirect I/O, 474\nDirect Memory Access (DMA), 394\ndirect pointers, 466\ndisk, 28\ndisk address, 218\ndisk head, 404\nDisk Operating System, 16\ndisk scheduler, 412\ndisks, 389\ndistributed shared memory, 550\nerror correcting codes, 528\nevent-based concurrency, 373\nﬁle, 441\nﬁle allocation table, 468\nﬁle descriptor, 444\nﬁle descriptors, 29\nﬁle handle, 563, 578\nﬁle identiﬁer, 578\nﬁle offset, 446\nﬁle server, 560\nﬁle system, 11, 12, 15\nﬁle system checker, 492\nﬁle-level locking, 580\nﬁle-system inconsistency, 494\nﬂush-on-close, 570\nfree space management, 469\nfree-space management, 153\nhardware caches, 94\nhardware-based address translation, 130\nI/O, 11\nI/O bus, 389\nI/O instructions, 394\nindirect pointer, 466\ninput/output (I/O) device, 389\ninstruction pointer, 26\ninterrupt, 378, 392\ninterrupts, 578\ninvalid frees, 124\nlimited direct execution, 45, 55, 105, 129",
      "keywords": [
        "ANDREW FILE SYSTEM",
        "FILE SYSTEM",
        "Distributed File System",
        "ANDREW FILE",
        "Student",
        "SYSTEM",
        "FILE",
        "ﬁle",
        "Distributed File",
        "Professor",
        "Distributed",
        "distributed ﬁle systems",
        "ﬁle system",
        "cache",
        "distributed ﬁle"
      ],
      "concepts": [
        "professor",
        "cache",
        "cached",
        "distributed",
        "distribution",
        "student",
        "pointer",
        "base",
        "disks",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 60,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 58,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.66,
          "base_score": 0.66,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 49,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "student",
          "distributed",
          "cache",
          "file"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "student",
          "distributed",
          "cache",
          "file"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33298155994632683,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.119947+00:00"
      }
    },
    {
      "chapter_number": 62,
      "title": "Segment 62 (pages 631-638)",
      "start_page": 631,
      "end_page": 638,
      "summary": "load-linked, 300\nlock, 291\nlock-free, 96\nlocked, 291\nlocking, 55, 97, 98\nlocks, 262, 283\nLog-structured File System, 512\nman pages, 42\nmanage memory, 130\nmanual pages, 42\nmemory bus, 389\nmemory leak, 124\nmemory management unit (MMU), 135\nmemory protection, 16\nmemory-management unit, 183\nmount protocol, 564\nmulti-level page table, 187, 205\nmulti-queue multiprocessor scheduling, 99\nmulti-threaded, 9, 262, 263\nmulti-threaded programs, 37\nmultiprocessor scheduling, 93, 94\nnon-blocking data structures, 322\nOperating Systems in Three Easy Pieces,\npage, 169\npage cache, 493\npage daemon, 223\npage directory, 205, 209\npage directory entries, 206\npage fault, 219, 220, 224\npage frame, 170\npage frame number, 206\npage in, 221\npage miss, 220\npage out, 221\npage replacement, 174\npage selection, 240\npage table, 170, 176\npage table base register, 219\npage table entry (PTE), 172, 219\npage-directory index, 208\npage-fault handler, 220, 224\npage-replacement policy, 221\npage-table base register, 175, 187\npage-table index, 209\npaging, 28, 153, 169, 179, 381\npaging out, 227\nphysical logging, 498\nphysical memory, 7\nphysically-indexed cache, 194\npreemptive scheduler, 298\nProcess Control Block, 32\nprocess control block, 137\nprocess control block (PCB), 263\nprocess list, 30, 32\nprotocol, 575\nrandom-access memory, 194\nread-after-write, 535\nrecovery protocol, 562\nremote procedure call, 551\nrotates, 434\nrotations per minute (RPM), 404\nrun-time library, 551\nrun-time stack, 29\nschedule, 474\nscheduled, 30\nscheduler, 37, 52\nscheduler state, 344\nscheduling metric, 60\nscheduling policies, 59\nscheduling policy, 26\nset, 298\nsets, 296\nshadow paging, 522\nshortest access time ﬁrst, 414\nshortest positioning time ﬁrst, 414\nShortest Time-to-Completion First, 64\nshortest-seek-ﬁrst, 412\nshortest-seek-time-ﬁrst, 412\nsingle-queue multiprocessor scheduling,\nsmall-write problem, 433, 511\nsoftware-managed TLB, 188\nstride scheduling, 88\nsystems programming, iv\nTCP/IP, 549\ntest-and-set, 297\nthread, 262, 263\nthread control blocks (TCBs), 263\nthread-local, 264\nthreads, 9, 93, 112\nticket lock, 302\nTime sharing, 26\ntime sharing, 25, 45, 46, 110\ntime-sharing, 26\ntime-space trade-off, 207\ntime-space trade-offs, 207\ntimeout/retry, 548\ntwo-phase lock, 307\nuniﬁed page cache, 474\nvirtual, 4, 23, 130\nvirtual address, 112, 114, 134\nvirtual address space, 8\nvirtual memory, 263\nvirtual page number (VPN), 171\nvirtual-to-physical address translations, 176\nvirtualization, iii, 1, 4, 8, 23\nvirtualized, 90, 269\nvirtualizes, 13\nvirtualizing, 25\nvirtualizing memory, 8, 112\nvirtually-indexed cache, 194\nwait-free synchronization, 300\nworking sets, 240\nwrite back, 407\nwrite buffering, 474, 513, 569\nwrite through, 407\nwrite-ahead log, 429\nwrite-ahead logging, 492, 497\nCalling lseek() Does Not Perform A Disk Seek, 447\nData Structure – The Free List, 136\nData Structure – The Page Table, 176\nData Structure – The Process List, 32\nEvery Address You See Is Virtual, 114\nForcing Writes To Disk, 499\nFree Space Management, 470\nMultiple Page Sizes, 202\nOptimizing Log Writes, 500\nPreemptive Schedulers, 63\nRTFM – Read The Man Pages, 42\nThe RAID Consistent-Update Problem, 429\nTLB Valid Bit ̸= Page Table Valid Bit, 190",
      "keywords": [
        "System",
        "bit",
        "memory",
        "File System",
        "system calls",
        "page table",
        "VALID BIT",
        "operating",
        "time",
        "RAID",
        "scheduling",
        "process",
        "TLB",
        "structure",
        "Linux"
      ],
      "concepts": [
        "pages",
        "paging",
        "write",
        "memory",
        "time",
        "block",
        "scheduling",
        "schedule",
        "caches",
        "caching"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 29,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 26,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "page table",
          "263",
          "scheduling",
          "shortest"
        ],
        "semantic": [],
        "merged": [
          "page",
          "page table",
          "263",
          "scheduling",
          "shortest"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39997924482895447,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.120009+00:00"
      }
    },
    {
      "chapter_number": 63,
      "title": "Segment 63 (pages 639-643)",
      "start_page": 639,
      "end_page": 643,
      "summary": "Avoid Premature Optimization (Knuth’s Law), 322\nBe Wary Of Locks and Control Flow, 319\nComparing Against Optimal is Useful, 229\nKnow And Use Your Tools, 269\nMeasure Then Build (Patterson’s Law), 577\nReboot Is Useful, 56\nUse strace (And Similar Tools), 445\nUse A Level Of Indirection, 516\nUse Advice Where Possible, 80\nUse Atomic Operations, 274\nUse Caching When Possible, 187\nUse Checksums For Integrity, 547\nUse Disks Sequentially, 410\nUse Hybrids, 205\nUse Protected Control Transfer, 47\nUse Randomness, 84\nUse The Timer Interrupt To Regain Control, 52\nUse Tickets To Represent Shares, 85\nUse Time Sharing (and Space Sharing), 26\nUse While (Not If) For Conditions, 337\nHow To Build A Distributed File System, 560\nHow To Build A Lock, 293\nHow To Build Concurrent Servers Without Threads, 373\nHow To Build Correct Concurrent Programs, 10\nHow To Develop Scheduling Policy, 59\nHow To Efﬁciently Virtualize The CPU With Control, 45\nHow To Make A Large, Fast, Reliable Disk, 421\nHow To Use Semaphores, 341\nbased version of zplot, a simple and useful tool developed by R.\nstudy of PostScript to good use and developed zplot.",
      "keywords": [
        "Law",
        "File System",
        "Control",
        "Build",
        "Virtualize Memory",
        "Avoid Premature Optimization",
        "Knuth ’s Law",
        "Data",
        "Avoid Voo-doo Constants",
        "Reduce File System",
        "Ousterhout ’s Law",
        "System",
        "Simple File System",
        "Disk",
        "Handle"
      ],
      "concepts": [
        "useful",
        "data",
        "control",
        "based",
        "file",
        "disks",
        "tools",
        "concurrency",
        "concurrent",
        "avoid"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 62,
          "title": "",
          "score": 0.644,
          "base_score": 0.644,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 1,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 56,
          "title": "",
          "score": 0.61,
          "base_score": 0.46,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 7,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 2,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "use",
          "law",
          "control",
          "build",
          "file"
        ],
        "semantic": [],
        "merged": [
          "use",
          "law",
          "control",
          "build",
          "file"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35485902731568414,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:07:57.120067+00:00"
      }
    }
  ],
  "total_chapters": 63,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "operating_systems_three_easy_pieces_metadata.json",
    "enrichment_date": "2025-12-17T23:07:57.132129+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4787.334126998758,
    "total_similar_chapters": 315
  }
}