{
  "metadata": {
    "title": "LLM-Engineers-Handbook",
    "source_file": "LLM-Engineers-Handbook_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 2-10)",
      "start_page": 2,
      "end_page": 10,
      "summary": "LLM Engineer’s Handbook\nLLM Engineer’s Handbook\nNeither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \nPackt Publishing has endeavored to provide trademark information about all of the companies and products \nLLMs by writing this book and making sure that as many people as possible can not only use \ndorse The LLM Engineer’s Handbook.\nFrom data engineering and model fine-tuning to advanced topics like RAG pipelines \nbest practices make it an invaluable resource for anyone serious about mastering LLM engineering.\nIn an era where AI is reshaping industries at breakneck speed, The LLM Engineer’s Handbook stands \na book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building \nchannel on production-grade ML that provides posts, articles, and open-source courses to help \nothers build real-world ML systems.\nPolytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML.\nRany ElHousieny is an AI solutions architect and AI engineering manager with over two decades \nof experience in AI, NLP, and ML.\ndeployment of AI models, authoring multiple articles on AI systems architecture and ethical AI de-\nTheir commitment to AI advancements made my experience of reviewing \nChapter 1: Understanding the LLM Twin Concept and Architecture  \nUnderstanding the LLM Twin concept                                                                                  2\nWhat is an LLM Twin?\nWhy building an LLM Twin matters • 3\nPlanning the MVP of the LLM Twin product                                                                          6\nDefining the LLM Twin MVP • 7\nBuilding ML systems with feature/training/inference pipelines                                            8\nThe problem with building ML systems • 8\nThe solution – ML pipelines for ML systems • 13\nThe training pipeline • 14\nThe inference pipeline • 14\nDesigning the system architecture of the LLM Twin                                                            16\nListing the technical details of the LLM Twin architecture • 16\nHow to design the LLM Twin architecture using the FTI pipeline design • 17",
      "keywords": [
        "LLM Twin",
        "LLM",
        "LLM Twin architecture",
        "LLM Engineer",
        "Packt Publishing",
        "LLM Twin Concept",
        "LLM Twin MVP",
        "LLM Twin product",
        "Labonne LLM Engineer",
        "Twin",
        "book",
        "LLMs",
        "Publishing",
        "LLM Twin matters",
        "Engineer"
      ],
      "concepts": [
        "editor",
        "production",
        "products",
        "engineer",
        "llm",
        "models",
        "pipelines",
        "building",
        "book",
        "twin"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 4,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineer",
          "llm",
          "llm engineer",
          "twin",
          "llm twin"
        ],
        "semantic": [],
        "merged": [
          "engineer",
          "llm",
          "llm engineer",
          "twin",
          "llm twin"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3245111069941553,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037563+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "summary": "Feature pipeline • 19\nTraining pipeline • 21\nInference pipeline • 22\nOrchestrator • 33\n• 51\nImplementing the LLM Twin’s data collection pipeline • 61\n• 66\nThe crawlers • 69\nTroubleshooting • 94\n• 100\nHallucinations • 101\nIngestion pipeline • 104\nRetrieval pipeline • 105\nGeneration pipeline • 105\n• 107\n• 111\n• 115\nRetrieval • 122\n• 128\nBatch pipelines • 130\n• 138\nOrchestration • 138\nSettings • 139\nOVM • 154\nThe handlers • 162\nData quantity • 180\nData curation • 182\nData deduplication • 184\nData decontamination • 185\nData exploration • 189\nData generation • 191\nData augmentation • 193\nWhen to fine-tune • 206\nFull fine-tuning • 211\nLoRA • 213\nQLoRA • 215\nOptimizers • 218\nPreference data • 230\nData quantity • 232\nData generation and evaluation • 233\nEvaluating preferences • 235\nRagas • 272\nARES • 274\nData parallelism • 299\nPipeline parallelism • 300\nBringing everything together into the RAG inference pipeline • 346\nData • 357\n• 373\nDevOps • 403\nMLOps • 405\nLLMOps • 410\nGuardrails • 411",
      "keywords": [
        "LLM Twin",
        "RAG Inference Pipeline",
        "RAG Feature Pipeline",
        "data",
        "Inference pipeline",
        "LLM",
        "pipeline",
        "Table of Contents",
        "RAG",
        "RAG Inference",
        "Twin",
        "Inference",
        "RAG Feature",
        "Feature pipeline",
        "advanced RAG"
      ],
      "concepts": [
        "pipeline",
        "evaluation",
        "evaluating",
        "evaluations",
        "rag",
        "optimization",
        "optimized",
        "optimizations",
        "inference",
        "summary"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "",
          "score": 0.9,
          "base_score": 0.75,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 56,
          "title": "",
          "score": 0.809,
          "base_score": 0.809,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.608,
          "base_score": 0.608,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.582,
          "base_score": 0.582,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pipeline",
          "data",
          "rag",
          "inference",
          "pipeline 105"
        ],
        "semantic": [],
        "merged": [
          "pipeline",
          "data",
          "rag",
          "inference",
          "pipeline 105"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3912174344943976,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037629+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 19-27)",
      "start_page": 19,
      "end_page": 27,
      "summary": "Run the pipelines on AWS • 428\nLLM Twin’s CI/CD pipeline flow • 434\nThe CI pipeline • 438\nThe CD pipeline • 442\nTest out the CI/CD pipeline • 445\nThe CT pipeline • 446\nTrigger downstream pipelines • 449\nPrompt monitoring • 451\n• 465\nTest examples • 465\nModel metrics • 469\nrange of disciplines, from data preparation and model fine-tuning to inference optimization and \nBy incorporating MLOps practices, LLM projects can \nThe LLM Engineer’s Handbook is a comprehensive guide to applying best practices to the new \nfield of LLM engineering.\nThe book covers topics \nsuch as data engineering, supervised fine-tuning, model evaluation, inference optimization, and \nTo illustrate these concepts in action, an end-to-end project called the LLM Twin will be developed \nusing various aspects of LLM engineering and MLOps.\nfine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines.\nThey will learn how to evaluate LLM performance, align models with human preferences, and \ndeploy LLM-based applications.\nThe book also covers essential MLOps principles and practices, \nenabling readers to build scalable, reproducible, and robust LLM applications.\nWho this book is for\nFor those already working with machine learning , this book will enhance your skills in imple-\nWhat this book covers\nChapter 1, Understanding the LLM Twin Concept and Architecture, introduces the LLM Twin project, \nwhich is used throughout the book as an end-to-end example of a production-level LLM appli-\nal-world LLM applications, such as an orchestrator, experiment tracker, prompt monitoring and \nChapter 3, Data Engineering, shows the implementation of a data collection pipeline that scrapes \nChapter 4, RAG Feature Pipeline, introduces RAG fundamental concepts, such as embeddings, the \nRAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software \nmodels and LLM systems.\nChapter 9, RAG Inference Pipeline, explores advanced RAG techniques by implementing methods \nimplementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to \nChapter 10, Inference Pipeline Deployment, introduces ML deployment strategies, such as online, \nasynchronous and batch inference, which will help in architecting and deploying the LLM Twin \nMLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML \nIt also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.\nTo get the most out of this book\ngramming is particularly beneficial, as the book’s examples and code snippets are predominantly \nstrictly necessary, as the book provides explanations for many fundamental AI and ML concepts.\nFamiliarity with version control systems like Git is assumed, as this book has a GitHub reposi-\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of ",
      "keywords": [
        "LLM Twin",
        "LLM",
        "LLM Twin project",
        "RAG Inference Pipeline",
        "book",
        "LLM engineering",
        "LLM Twin Concept",
        "pipeline",
        "RAG",
        "Twin",
        "LLMs",
        "Inference Pipeline",
        "implementing LLM Twin",
        "inference",
        "RAG Feature Pipeline"
      ],
      "concepts": [
        "chapters",
        "llm",
        "pipelines",
        "rag",
        "model",
        "engineering",
        "engineer",
        "preface",
        "practices",
        "introduces"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.844,
          "base_score": 0.694,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.803,
          "base_score": 0.653,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "llm",
          "twin",
          "llm twin",
          "pipeline"
        ],
        "semantic": [],
        "merged": [
          "book",
          "llm",
          "twin",
          "llm twin",
          "pipeline"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43156851244503047,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037686+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 28-36)",
      "start_page": 28,
      "end_page": 36,
      "summary": "Understanding the LLM Twin \nlarge language model (LLM) product.\nwill show you how to build an LLM Twin, an AI character that learns to write like a particular \nperson by incorporating its style, voice, and personality into an LLM.\nMost of the concepts learned while implementing your LLM Twin can be applied in other LLM-\nIn our case, what exactly is an LLM Twin, \nused to build the LLM system.\nUnderstanding the LLM Twin Concept and Architecture\nmuch ML knowledge, it is critical to go through them to understand “how” to build the product \nUnderstanding the LLM Twin concept\nPlanning the MVP of the LLM Twin product\nDesigning the system architecture of the LLM Twin\nUnderstanding the LLM Twin concept\nThe concept of an LLM Twin is new.\nWhat is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \ninto an LLM.\nInstead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \nNaturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nThus, this LLM will not be you; \nIt is essential to understand that an LLM reflects the data it was trained on.\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various \nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\nDo we have enough digital data to project ourselves into an LLM?\n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nWhy building an LLM Twin matters\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nthe skeleton of our main idea to the LLM Twin and let it do the grunt work.\nUnderstanding the LLM Twin Concept and Architecture\non the concrete features in the Planning the MVP of the LLM Twin product section).\nect ourselves into a content-writing LLM Twin that will help us automate our writing process.\nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter?\nAlso, it is critical to understand that building an LLM Twin is entirely moral.\nThe LLM will be \nEveryone will have their own LLM Twin with restricted access.\nWhat’s the difference between a co-pilot and an LLM Twin?\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nwrites like you is your LLM Twin co-pilot.\nThe key of the LLM Twin stands in the following:\nHow we feed the data into the LLM\nThe solution is to build an LLM system that encapsulates and automates all the following steps \nUnderstanding the LLM Twin Concept and Architecture\nLLM fine-tuning\ninterface, it can be integrated into the LLM Twin system we will learn to build.\nPlanning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nDefining the LLM Twin MVP\nAs a thought experiment, let’s assume that instead of building this project for this book, we want \nin defining our LLM Twin MVP and what features we want to pick.\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\nFine-tune an open-source LLM using the collected data\nHave a simple web interface to interact with the LLM Twin and be able to do the following:\nThat will be the LLM Twin MVP.\nEven if we focus only on the core features of the LLM Twin defined in this section, we \nwill build the product with the latest LLM research and best software engineering ",
      "keywords": [
        "LLM Twin",
        "LLM Twin Concept",
        "LLM Twin MVP",
        "LLM Twin product",
        "LLM",
        "Twin",
        "LLM Twin system",
        "Twin Concept",
        "LLM Twin co-pilot",
        "LLM Twin matters",
        "content-writing LLM Twin",
        "LLM Twin stands",
        "LLM Twin defined",
        "Twin MVP",
        "Twin product"
      ],
      "concepts": [
        "llm",
        "product",
        "production",
        "content",
        "data",
        "write",
        "writing",
        "twin",
        "follow",
        "generating"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llm twin",
          "twin",
          "llm",
          "product",
          "mvp"
        ],
        "semantic": [],
        "merged": [
          "llm twin",
          "twin",
          "llm",
          "product",
          "mvp"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.269737630206441,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037733+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 37-46)",
      "start_page": 37,
      "end_page": 46,
      "summary": "Understanding the LLM Twin Concept and Architecture\nBuilding ML systems with feature/training/inference \npipelines\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \npattern at the core of the architecture, known as the feature/training/inference (FTI) architecture.\nThis section will present a general overview of the FTI pipeline design and how it can structure \nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\nBuilding production-ready ML systems is much more than just training a model.\ngineering point of view, training the model is the most straightforward step in most use cases.\nHowever, training a model becomes complex when deciding on the correct architecture and \ndata science team is often responsible for training the model.\nwhen productionizing an ML model.\non a monolithic batch architecture that couples the feature creation, model training, and infer-\nthe ML world: the training-serving skew.\nThe training-serving skew happens when the features \npassed to the model are computed differently at training and inference time.\nFigure 1.2: Monolithic batch pipeline architecture\nIt’s hard to share the work between multiple teams between the features, training, and \nwhole state through the client request so the features can be computed and passed to the model.\nFigure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/\nBut here is where the FTI pipeline architectures kick in.\nThe solution – ML pipelines for ML systems\ncan follow to compute the features, train the model, and make predictions.\ncritical steps that any ML system requires, the pattern is known as the FTI pipeline.\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, \nAs shown in Figure 1.5, we have the feature, training, and inference pipelines.\nFigure 1.5: FTI pipelines architecture\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nThe feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference.\nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match.\nThe training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \na train model or models.\nfeature stores, but this time, the model is the first-class citizen.\nversion, track, and share the model with the inference pipeline.\naspects of how the model was trained.\nversion used to train the model.\nThus, we will always know what data the model was trained on.\nThe inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nAdditionally, the features, labels, and models are \nwe can quickly change the connections between the model and features.\nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nThe training pipeline queries the features store for features and labels and outputs a \nThe inference pipeline uses the features from the feature store and the model from the \nFor example, the feature pipeline \ntraining pipeline can be composed of the training and evaluation components.\nlines interact with each other through the feature store and model registries.\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how \nstand how we can solve them by designing our LLM system using the FTI architecture.\nListing the technical details of the LLM Twin architecture\nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML \nSystems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder \nfti-pipelines.\nHow can we apply the FTI pipeline design to implement the preceding list of requirements?\nHow to design the LLM Twin architecture using the FTI \npipeline design\nthree, as the FTI pipeline design clearly states?” That is a great question.\nWe must also implement the data pipeline along the three feature/training/inference \npipelines.\nThe data engineering team owns the data pipeline\nThe ML engineering team owns the FTI pipelines.",
      "keywords": [
        "LLM Twin",
        "LLM Twin architecture",
        "LLM Twin Concept",
        "FTI pipeline",
        "model",
        "features",
        "LLM",
        "pipeline",
        "FTI pipeline design",
        "FTI",
        "Architecture",
        "training",
        "Twin",
        "system",
        "FTI pipeline architectures"
      ],
      "concepts": [
        "data",
        "architecture",
        "pipeline",
        "model",
        "chapters",
        "inference",
        "infer",
        "training",
        "follow",
        "users"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 4,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fti",
          "training",
          "fti pipeline",
          "pipeline",
          "features"
        ],
        "semantic": [],
        "merged": [
          "fti",
          "training",
          "fti pipeline",
          "pipeline",
          "features"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3836737609447724,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037787+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 47-58)",
      "start_page": 47,
      "end_page": 58,
      "summary": "Understanding the LLM Twin Concept and Architecture\nThis includes defining the data collection and FTI pipelines.\nData collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nThe output of this component will be a NoSQL DB, which will act as our data warehouse.\nattach an additional ETL in the data collection pipeline, and everything else will work without \nFeature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nIt is critical to highlight that the data collection pipeline is designed to crawl data \nexample for this book, we agreed to make our collected data available for learning \nUnderstanding the LLM Twin Concept and Architecture\nHere are some custom properties of the LLM Twin’s feature pipeline:\nIt processes three types of data differently: articles, posts, and code\nquery the vector DB for new data points without any vector search logic.\nThe training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \nTo conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines.\nTraining pipeline\nThe training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with \nit, and stores the tuned LLM weights in a model registry.\ndataset is available in the logical feature store, we will trigger the training pipeline, consume the \nthe best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate.\nThe proposed LLM is then stored in the model registry.\nmodel is ultimately tagged as accepted and deployed to the production inference pipeline.\nHow do you implement an LLM agnostic pipeline?\ndata collection pipeline to crawl data every week.\nUnderstanding the LLM Twin Concept and Architecture\nThen, we can trigger the feature pipeline when new data is available in the data warehouse and \nInference pipeline\nIt loads a fine-tuned LLM from the model registry, and from the logical feature \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\nit is done, in our system, it is easier and cheaper to use a logical feature store based on a vector \ncollection and feature pipeline are mostly CPU-based and do not require powerful machines.\ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it.\nThe data and feature pipelines will be scaled horizon-\nIt processes the data as requested, and the training is modular and can \nThe inference pipeline \narchitecture of the LLM Twin to fit all our technical requirements.\nFrom MLOps to ML Systems with Feature/Training/Inference \nPipelines.\nUnderstanding the LLM Twin Concept and Architecture\nMLOps: Continuous delivery and automation pipelines in machine learning.\n5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024.\nhttps://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-\nimplementing and deploying the LLM Twin project.\nTwin use case by implementing a data collection ETL that crawls data from the internet.\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage \nmultiple Python versions, create a virtual environment, and install the pinned dependencies re-\ncode yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\nPython ecosystem and project installation\nBy the end of this chapter, you will be aware of all the tools we will use across the book.\nwill have learned how to install the LLM-Engineers-Handbook repository, set up the rest of the \ntools, and use them if you run the code while reading the book.\nPython ecosystem and project installation\nAny Python project needs three fundamental tools: the Python interpreter, dependency manage-\nAll the code within the book is tested with Python 3.11.8.\nversion (Python 3.11.8) to run the LLM Twin project using pyenv, making the installation process \nInstead of installing multiple global Python versions, we recommend managing them using pyenv, \na Python version management tool that lets you manage multiple Python versions between \nAfter you have installed pyenv, you can install the latest version of Python 3.11, using pyenv, as \nNow list all installed Python versions to see that it was installed correctly:\nBecause we defined a .python-version file within the repository, pyenv will know to pick up \npython --version\nTo create the .python-version file, you must run pyenv local 3.11.8 once.\nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nPoetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nFor example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements.\nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nYou can install Poetry from here: https://python-poetry.org/docs/.\nWe use Poetry 1.8.3 \nOnce Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nas follows: poetry run <your command>.\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nversions, the poetry.lock file ensures that all project installations use the same versions of each \nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nfile that Poetry already uses for dependencies.\nYou can install Poe the Poet as a Poetry plugin, as follows:",
      "keywords": [
        "LLM Twin",
        "LLM Twin Concept",
        "LLM",
        "LLM Twin architecture",
        "Python",
        "LLM Twin project",
        "data",
        "Data collection pipeline",
        "Poetry",
        "pipeline",
        "feature store",
        "Python version",
        "LLM Twin MVP",
        "data collection",
        "Twin"
      ],
      "concepts": [
        "python",
        "data",
        "poetry",
        "llm",
        "pipelines",
        "tools",
        "file",
        "versions",
        "version",
        "require"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.877,
          "base_score": 0.727,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 10,
          "title": "",
          "score": 0.821,
          "base_score": 0.821,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.794,
          "base_score": 0.644,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.767,
          "base_score": 0.617,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "python",
          "poetry",
          "python version",
          "pipeline",
          "llm"
        ],
        "semantic": [],
        "merged": [
          "python",
          "poetry",
          "python version",
          "pipeline",
          "llm"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44048900650396555,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037848+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "summary": "Now that we have installed our Python project, let’s present the MLOps tools we will use in the \nuse case, such as model registries and orchestrators, but only provide a quick idea of what they \nRun poetry poe local-infrastructure-up to locally spin up ZenML (http://127.0.0.1:8237/) \nYou can read more details on how to run everything locally in the LLM-Engineers-Handbook re-\nFigure 2.1: Hugging Face model registry example\nMost ML tools provide model registry features.\nZenML: orchestrator, artifacts, and metadata\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that \nThus, ZenML’s main features are orchestrating ML \npipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts \nyou to run ZenML on multiple infrastructure options.\nFor example, in our LLM Twin use case, we used the AWS stack:\ndetails on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production-\nThe local version of the ZenML server comes installed as a Python package.\npoetry install, it installs a ZenML debugging server that you can use locally.\nwill show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\nHow does ZenML work as an orchestrator?\nIt works with pipelines and steps.\nA function becomes a ZenML pipeline by being \nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nIn the code snippet below, we defined a ZenML pipeline that queries \nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\nWe will focus only on the ZenML features used throughout the book, such as orches-\nguide: https://docs.zenml.io/user-guide/starter-guide.\nYou can run the pipeline with the following CLI command: poetry poe run-digital-data-etl.\nTo visualize the pipeline run, you can go to your ZenML dashboard (at http://127.0.0.1:8237/) \nFigure 2.2: ZenML Pipelines dashboard\npipeline runs, as seen in Figure 2.3.\nAlso, you can see the stack used to run the pipeline, where the default stack is the one used to \nrun your ML pipelines locally.\nFigure 2.3: ZenML digital_data_etl pipeline dashboard.\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or \nis still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure \nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, \nlet’s quickly look at how to define a ZenML step.\nor_create_user() step, which works just like a normal Python function but is decorated with \nfrom zenml import get_step_context, step\nWithin a ZenML step, you can define any Python logic your use case needs.\nthen glue multiple steps together within a main function decorated with @pipeline.\nWe also defined the pipelines and steps folders, where \nmodule, we only aggregated ZenML steps to glue them into the final pipeline.\nsign, we can easily swap ZenML with another orchestrator or use our application logic in other ",
      "keywords": [
        "ZenML",
        "Hugging Face",
        "pipeline",
        "LLM Twin",
        "Hugging Face model",
        "run",
        "pipeline run",
        "user",
        "etl pipeline run",
        "model registry",
        "ZenML pipeline",
        "LLM",
        "etl pipeline",
        "step",
        "Face model registry"
      ],
      "concepts": [
        "zenml",
        "pipelines",
        "steps",
        "user",
        "models",
        "quickly",
        "tooling",
        "run",
        "runs",
        "running"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 8,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 52,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "zenml",
          "run",
          "pipeline",
          "pipeline run",
          "dashboard"
        ],
        "semantic": [],
        "merged": [
          "zenml",
          "run",
          "pipeline",
          "pipeline run",
          "dashboard"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3601240635655649,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037898+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 67-75)",
      "start_page": 67,
      "end_page": 75,
      "summary": "As mentioned in the previous section, ZenML transforms any step output into an artifact.\nthe machine learning lifecycle, such as datasets, trained models, checkpoints, or logs.\nFor example, when wrapping your dataset with an artifact, \nyou can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, \nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an ar-\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\nA more interesting example of an artifact and its metadata is the generated dataset artifact.\nFigure 2.9, we can visualize the metadata of the instruct_datasets artifact, which was auto-\ninstruction datasets are in Chapter 5.\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\ncan precompute and attach to the artifact’s metadata anything you consider helpful for dataset \nname=\"instruct_datasets\",\nstep_context.add_output_metadata(output_name=\"instruct_datasets\", \nmetadata=_get_metadata_instruct_dataset(datasets))\ndef _get_metadata_instruct_dataset(datasets: InstructTrainTestSplit) -> \ninstruct_dataset_categories = list(datasets.train.keys())\ncategory: instruct_dataset.num_samples for category, instruct_\ntest_num_samples = {category: instruct_dataset.num_samples for \n\"data_categories\": instruct_dataset_categories,\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\nHow to run and configure a ZenML pipeline\nAll the ZenML pipelines can be called from the run.py file, accessed at tools/run.py in our GitHub \nFor example, to call the digital_data_etl pipeline to crawl Maxime’s content, \npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul\n\"run_name\": f\"digital_data_etl_run_{dt.now().\ndigital_data_etl.with_options()(**run_args_etl)\nample, the configs/digital_data_etl_maxime_labonne.yaml configuration file looks as follows:\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nFigure 2.10: ZenML pipeline configs\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss \nFigure 2.11: Comet ML training metrics example\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your \nFigure 2.12: Comet ML system metrics example\nexperiment tracker, we made the training experiments tracked with Comet ML public while ",
      "keywords": [
        "artifact",
        "ZenML",
        "Tooling and Installation",
        "metadata",
        "dataset",
        "instruct",
        "data",
        "etl",
        "pipeline",
        "digital",
        "Comet",
        "config",
        "run",
        "etl pipeline",
        "step"
      ],
      "concepts": [
        "zenml",
        "dataset",
        "train",
        "tooling",
        "models",
        "artifacts",
        "run",
        "running",
        "experiments",
        "experiment"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 52,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 10,
          "title": "",
          "score": 0.526,
          "base_score": 0.526,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.515,
          "base_score": 0.365,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "artifact",
          "zenml",
          "run",
          "digital_data_etl",
          "etl"
        ],
        "semantic": [],
        "merged": [
          "artifact",
          "zenml",
          "run",
          "digital_data_etl",
          "etl"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3406491644163537,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.037965+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 76-83)",
      "start_page": 76,
      "end_page": 83,
      "summary": "You can read more on Opik at https://github.com/comet-ml/opik.\nDatabases for storing unstructured and vector data\nWe also want to present the NoSQL and vector databases we will use within our examples.\nMongoDB: NoSQL database\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before \nQdrant: vector database\nhttps://superlinked.com/vector-db-comparison, which compares all the top vector databases \nPreparing for AWS\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have \none), an AWS access key, and the CLI.\nAlso, we will look into what SageMaker is and why we use it.\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in \nBut for our MVP, AWS, it’s the perfect option as it provides robust features for every-\nSetting up an AWS account, an access key, and the CLI\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is \nby redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/\nAfter successfully creating an AWS account, you can access the AWS console at http://console.\naws.amazon.com.\nNext, we must generate access keys to access AWS programmatically.\nfirst to create an IAM user with administrative access as described in this AWS official tutorial: \nhttps://docs.aws.amazon.com/streams/latest/dev/setting-up.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html.\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key>\nAlso, be cautious with who you share them, as they could be used to access your AWS \naccount and manipulate various AWS resources.\nThe last step is to install the AWS CLI and configure it with your newly created access keys.\ncan install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/\nAfter installing the AWS CLI, you can configure it by running aws configure.\nof our AWS configuration:\naws_access_key_id = *************\naws_secret_access_key = ************\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://\ndocs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html.\nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nis an ML platform used to train and deploy ML models.\nSageMaker is a fully managed machine learning service by AWS that enables developers and data \nAll the cloud services used across the book stick to their freemium option, except AWS.\nThus, if you use a personal AWS account, you will be responsible for AWS costs as you \non our tests, the AWS costs can vary between $50 and $100 using the specifications \nSee the AWS documentation on setting up billing alarms to monitor your costs \nat https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\nand to deploy our custom LLM Twin model as a REST API that can be accessed in real time from \nWhy AWS SageMaker?\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, \nsuch as AWS Bedrock.\nIt provides pre-trained models, which you can access directly \npre-trained models and APIs provided by Amazon Bedrock.\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying \ncomfortable working with cloud platforms such as AWS.\nregarding costs, following a pay-as-you-go pricing model similar to most AWS services.\ndeployed resources on AWS, such as online EC2 instances.\nployment, use EKS, AWS’s Kubernetes self-managed service.\nFinally, we explored the process of setting up an AWS account, generating an access \nkey, and configuring the AWS CLI for programmatic access to the AWS cloud.\ndeep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM \nhttps://github.com/comet-ml/opik\nhttps://neptune.ai/blog/ml-experiment-tracking\ncom/resources/basics/databases/nosql-explained\nneptune.ai/blog/ml-model-registry\nhttps://www.pinecone.io/learn/vector-database/\nhttps://superlinked.com/vector-db-comparison",
      "keywords": [
        "AWS",
        "AWS CLI",
        "AWS account",
        "AWS SageMaker",
        "access",
        "AWS access key",
        "vector database",
        "SageMaker",
        "LLM Twin",
        "vector",
        "key",
        "database",
        "NoSQL database",
        "Bedrock",
        "access key"
      ],
      "concepts": [
        "tool",
        "managed",
        "manage",
        "models",
        "best",
        "databases",
        "uses",
        "options",
        "machine",
        "cloud"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 49,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 46,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 10,
          "title": "",
          "score": 0.504,
          "base_score": 0.504,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "aws",
          "aws account",
          "access",
          "account",
          "cli"
        ],
        "semantic": [],
        "merged": [
          "aws",
          "aws account",
          "access",
          "account",
          "cli"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2499378190047673,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038007+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 84-97)",
      "start_page": 84,
      "end_page": 97,
      "summary": "design and implement the data collection pipeline to gather the raw data we will use in all our \nThus, implementing our data pipeline will connect the dots \nimplement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, \nWe will show you how to implement various crawling methods, standardize the data, \nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture \nFinally, we will explore how to run the data collection pipeline using ZenML and query the col-\nDesigning the LLM Twin’s data collection pipeline\nImplementing the LLM Twin’s data collection pipeline\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, \nDesigning the LLM Twin’s data collection pipeline\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL \nunderstanding how our data collection pipeline maps to an ETL process.\nWe will crawl data from platforms like Medium, \nFor our project, we use MongoDB as our NoSQL data warehouse.\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\nWe want to design an ETL pipeline that inputs a user and a list of links as input.\ncrawls each link individually, standardizes the collected content, and saves it under that specific \nauthor in a MongoDB data warehouse.\nHence, the signature of the data collection pipeline will look as follows:\nOutput: A list of raw documents stored in the NoSQL data warehouse\nWe will use user and author interchangeably, as in most scenarios across the ETL pipeline, a \nHowever, within the data warehouse, we have only \nThe ETL pipeline will detect the domain of each link, based on which it will call a specialized \ncrawler.\nWe implemented four different crawlers for three different data categories, as seen in \nFigure 3.2: The relationship between the crawlers and the data categories\nMedium crawler: Used to collect data from Medium.\nlogs in to Medium and crawls the HTML of the article’s link.\nsafety net when the link’s domain isn’t associated with the other supported crawlers.\nexample, when providing a Substack link, it will default to the custom article crawler, but \nGitHub crawler: This collects data from GitHub. It outputs a repository document.\nLinkedIn crawler: This is used to collect data from LinkedIn. It outputs multiple post \ndata from the MongoDB data warehouse, cleans it further, processes it into features, and stores it \ndata warehouse.\nThus, the data collection pipeline can write data for MongoDB, and the feature \nWhy did we use MongoDB as a data warehouse?\nMongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of docu-\ntured data: text crawled from the internet.\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s \nImplementing the LLM Twin’s data collection pipeline\nThus, let’s start by looking into the ZenML digital_data_etl pipeline.\nthis time, we will dig deeper into the implementation, explaining how the data collection works \ntation of each crawler used to collect data from various sites and the MongoDB documents used \nIn the code snippet below, we can see the implementation of the ZenML digital_data_etl\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user \nin our repository at pipelines/digital_data_etl.py.\nfrom steps.etl import crawl_links, get_or_create_user\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nlast_step = crawl_links(user=user, links=links)\nFigure 3.3 shows a run of the digital_data_etl pipeline on the ZenML dashboard.\nphase is to explore the get_or_create_user and crawl_links ZenML steps individually.\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\nWe will move on to the crawl_links ZenML step, which collects the data from the provided links.\nfrom llm_engineering.application.crawlers.dispatcher import \nthis function, a crawler dispatcher is initialized and configured to handle specific domains such \ndef crawl_links(user: UserDocument, links: list[str]) -> \nIt attempts to crawl and extract data for each link, updating the count of \nsuccessfull_crawl, crawled_domain = _crawl_link(dispatcher, link, \nstep_context.add_output_metadata(output_name=\"crawled_links\", \nappropriate crawler based on the link’s domain.\nextraction and returns a tuple indicating the crawl’s success and the link’s domain:\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: \ncrawler = dispatcher.get_crawler(link)\ncrawler_domain = urlparse(link).netloc\ncrawler.extract(link=link, user=user)\nreturn (True, crawler_domain)\nreturn (False, crawler_domain)\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) \nAs seen in the abovementioned _crawl_link() function, the CrawlerDispatcher class knows \nwhat crawler to initialize based on each link’s domain.\n3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers.\nThe CrawlerDispatcher class knows how to extract the domain of each link and initialize the \nproper crawler that collects the data from that site.\ncrawlers\nimporting our crawler classes:\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, Linke-\nwe will use the key of the dictionary as the domain pattern to match future links with a crawler:\ndef register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n= crawler\nreturn crawler()\nThe next step in understanding how the data collection pipeline works is analyzing each crawler ",
      "keywords": [
        "Data",
        "data collection pipeline",
        "LLM Twin",
        "data collection",
        "data warehouse",
        "Data Engineering",
        "user",
        "ETL pipeline",
        "pipeline",
        "crawler",
        "LLM",
        "data collection ETL",
        "ETL",
        "link",
        "collection pipeline"
      ],
      "concepts": [
        "data",
        "crawler",
        "importing",
        "pipelines",
        "user",
        "link",
        "step",
        "domain",
        "metadata",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.821,
          "base_score": 0.821,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.646,
          "base_score": 0.646,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 18,
          "title": "",
          "score": 0.593,
          "base_score": 0.443,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.576,
          "base_score": 0.576,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 52,
          "title": "",
          "score": 0.564,
          "base_score": 0.564,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "crawler",
          "link",
          "data",
          "data collection",
          "collection"
        ],
        "semantic": [],
        "merged": [
          "crawler",
          "link",
          "data",
          "data collection",
          "collection"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.399012578883696,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038063+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 98-105)",
      "start_page": 98,
      "end_page": 105,
      "summary": "Before exploring each crawler’s implementation, we must present their base class, which defines \nEach class implements the extract()\ncrawler.extract(link=link, user=user)\nBase classes\nclass BaseCrawler(ABC):\ndef extract(self, link: str, **kwargs) -> None: ...\ndefines a model attribute at the class level that represents the data category document type used \nWe also extend the BaseCrawler class with a BaseSeleniumCrawler class, which implements \nfrom selenium.webdriver.chrome.options import Options\nNext, we define the BaseSeleniumCrawler class for use cases where we need Selenium to collect \nFor the Selenium-based crawlers to work, you must install Chrome on your machine \nclass BaseSeleniumCrawler(BaseCrawler, ABC):\ndef __init__(self, scroll_limit: int = 5) -> None:\noptions.add_argument(\"--headless=new\")\noptions.add_argument(\"--log-level=3\")\noptions.add_argument(\"--disable-notifications\")\noptions.add_argument(\"--disable-extensions\")\noptions.add_argument(\"--ignore-certificate-errors\")\noptions.add_argument(f\"--user-data-dir={mkdtemp()}\")\noptions.add_argument(f\"--data-path={mkdtemp()}\")\noptions.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver \nand creates a new instance of the Chrome driver with the specified options:\nself.set_extra_driver_options(options)\nThe BaseSeleniumCrawler class includes placeholder methods for set_extra_driver_options()\ndef set_extra_driver_options(self, options: Options) -> None:\ncontent to load, and repeats the process until it reaches the end of the page or the scroll limit is \ndef scroll_page(self) -> None:\nlast_height = self.driver.execute_script(\"return document.body.\nnew_height = self.driver.execute_script(\"return document.body.\nif new_height == last_height or (self.scroll_limit and \nWe’ve understood what the base classes of our crawlers look like.\nYou can find the implementation of the above crawlers in the GitHub repository at \nThe GithubCrawler class is designed to scrape GitHub repositories, extending the functionality \nclass GithubCrawler(BaseCrawler):\nNext, we implement the extract() method, where the crawler first checks if the repository has \ndef extract(self, link: str, **kwargs) -> None:\nold_model = self.model.find(link=link)\nlogger.info(f\"Repository already exists in the database: {link}\")\nIf the repository is new, the crawler extracts the repository name from the link.\nlogger.info(f\"Starting scrapping GitHub repository: {link}\")\nlogger.info(f\"Finished scrapping GitHub repository: {link}\")\nIt leverages the AsyncHtmlLoader class to read the entire HTML from a link and the \nHtml2TextTransformer class to extract the text from that HTML.\nfrom langchain_community.document_transformers.html2text import \nfrom .base import BaseCrawler\nwe don’t need to log in or use the scrolling functionality provided by Selenium.\nclass CustomArticleCrawler(BaseCrawler):\ndef extract(self, link: str, **kwargs) -> None:\nold_model = self.model.find(link=link)\nlogger.info(f\"Article already exists in the database: {link}\")\nclass, which returns a list of documents.\ngate the whole logic to these two classes, we don’t control how the content is extracted and parsed.\nWe get the page content from the extracted document, plus relevant metadata such as the title, \nWe then create a new instance of the article model, populating it with the extracted content.",
      "keywords": [
        "link",
        "Options",
        "Selenium",
        "scroll",
        "options.add",
        "argument",
        "data",
        "content",
        "Data Engineering",
        "repository",
        "extract",
        "crawler",
        "Chrome",
        "BaseCrawler",
        "ChromeDriver"
      ],
      "concepts": [
        "imports",
        "options",
        "link",
        "classes",
        "content",
        "crawlers",
        "user",
        "data",
        "base",
        "based"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 25,
          "title": "",
          "score": 0.511,
          "base_score": 0.361,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.444,
          "base_score": 0.294,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "options",
          "link",
          "options add_argument",
          "add_argument",
          "class"
        ],
        "semantic": [],
        "merged": [
          "options",
          "link",
          "options add_argument",
          "add_argument",
          "class"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20212497640496704,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038104+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 106-115)",
      "start_page": 106,
      "end_page": 115,
      "summary": "MediumCrawler class\nThe code begins by importing essential libraries and defining the MediumCrawler class, which \nfrom llm_engineering.domain.documents import ArticleDocument\nclass MediumCrawler(BaseSeleniumCrawler):\nWithin the MediumCrawler class, we leverage the set_extra_driver_options() method to extend \ndef extract(self, link: str, **kwargs) -> None:\ninstance, populates it with the extracted content and user information provided via kwargs, and \nThe last step is understanding how the document classes \nWe had to implement three document classes to structure our data categories.\nThese classes \nIt is best practice to structure your data in classes instead of dictionaries, as the attributes we \nour data items with classes, we can ensure each attribute is as expected.\nArticleDocument class\nPostDocument class\nRepositoryDocument class\nThese are not simple Python data classes or Pydantic models.\nall the document classes without repeating any code, we used the Object-Document Mapping\nand document classes.\nFor example, using SQLAlchemy, we defined a User ORM with the ID and name fields.\nORM is mapped to the users table within the SQL database.\nall the CRUD operations on top of the User class.\n# Define a class that maps to the users table.\nclass User(Base):\nUsing the User ORM, we can quickly insert or query users directly from Python without writing a \nshows how to save an instance of the User ORM to a SQLite database:\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and \nwork with NoSQL databases, the data structure is centered on collections, which store JSON-like \nTo conclude, ODM simplifies working with document-based NoSQL databases and maps ob-\nImplementing the ODM class\nThis section will explore how to implement an ODM class from scratch.\nclasses.\nHence, we will implement a base ODM class called NoSQLBaseDocument, from which all \nthe other documents will inherit to interact with the MongoDB data warehouse.\nNext, we define a type variable T bound to the NoSQLBaseDocument class.\nPython’s generic module, allowing us to generalize the class’s types.\nThe class can be found in our repository at llm_engineering/domain/base/nosql.\nWithin the NoSQLBaseDocument class, an id field is defined as a UUID4, with a default factory \nThe class also implements the __eq__ and __hash__ methods to allow \nif not isinstance(value, self.__class__):\nThe class provides methods for converting between MongoDB documents and class instances.\nfrom_mongo() class method transforms a dictionary retrieved from MongoDB into an instance of \nthe class.\ndef from_mongo(cls: Type[T], data: dict) -> T:\nreturn cls(**dict(data, id=id))\nThe save() method allows an instance of the model to be inserted into a MongoDB collection.\nretrieves the appropriate collection, converts the instance into a MongoDB-compatible document \ncollection = _database[self.get_collection_name()]\ncollection.insert_one(self.to_mongo(**kwargs))\nThe get_or_create() class method attempts to find a document in the database matching the \nIf a matching document is found, it is converted into an instance of the class.\nIf not, a new instance is created with the filter options as its initial data and saved to the database:\ndef get_or_create(cls: Type[T], **filter_options) -> T:\ninstance = collection.find_one(filter_options)\nreturn cls.from_mongo(instance)\nnew_instance = cls(**filter_options)\nThe bulk_insert() class method allows multiple documents to be inserted into the database \ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\nThe find() class method searches for a single document in the database that matches the given \ninstance = collection.find_one(filter_options)\nSimilarly, the bulk_find() class method retrieves multiple documents matching the filter options.\nIt converts each retrieved MongoDB document into a model instance, collecting them into a list:\ninstances = collection.find(filter_options)\nreturn [document for instance in instances if (document := cls.\nFinally, the get_collection_name() class method determines the name of the MongoDB collec-\nexception will be raised specifying that the subclass should define a nested Settings class:\n\"Document should define an Settings configuration class with ",
      "keywords": [
        "user",
        "Data",
        "database",
        "ORM",
        "instance",
        "User ORM",
        "Python",
        "ODM",
        "ODM class",
        "Data Engineering",
        "class method",
        "document",
        "options",
        "type",
        "link"
      ],
      "concepts": [
        "data",
        "importing",
        "python",
        "method",
        "documents",
        "user",
        "collecting",
        "collections",
        "collection",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 11,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 20,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "class",
          "document",
          "instance",
          "class method",
          "orm"
        ],
        "semantic": [],
        "merged": [
          "class",
          "document",
          "instance",
          "class method",
          "orm"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3216984125159456,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038188+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 116-123)",
      "start_page": 116,
      "end_page": 123,
      "summary": "We can configure each subclass using the nested Settings class, such as defining the collection \nData categories and user document classes\nThese are the concrete classes that define our data categories.\nYou’ve seen these classes used across the chapter when working with articles, repositories, and \nWe define an enum class, where we centralize all our data category types.\nThe class can be found in the repository at llm_engineering/domain/types.py.\nNoSQLBaseDocument ODM class.\nclass Document(NoSQLBaseDocument, ABC):\nFinally, specific document types are defined by extending the Document class.\nclass ArticleDocument(Document):\nFinally, we define the UserDocument class, which is used to store and query all the users from the \nBy implementing the NoSQLBaseDocument ODM class, we had to focus solely on the fields and \nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML \nup with the ODM class and data category documents.\nThe last step is to run the data collection \npipeline and ingest raw data into the MongoDB data warehouse.\nZenML orchestrates the data collection pipeline.\nThus, leveraging ZenML, the data collection \nWe configured a different pipeline run for each author.\nPaul Iusztin’s or Maxime Labonne’s data.\nTo call the data collection pipeline to collect Maxime’s \ndata, for example, you can run the following CLI command:\npoetry poe run-digital-data-etl-maxime\nFigure 3.5 shows the user output artifact generated by this data collection pipeline.\nwe collected the links in this specific run.\nFigure 3.5: Example of the user output artifact after running the data collection pipeline using \nfrom which we collected data, the total number of links crawled for each domain, and the number \nFigure 3.6: Example of the crawled_links output artifact after running the data collection \nNow, we can download the crawled_links artifact anywhere in our code by running the following \nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML \n- https://medium.com/decodingml/sota-python-streaming-pipelines-for-\n- https://decodingml.substack.com/p/real-time-feature-pipelines-\nTo run the pipeline using Paul’s configuration, we call the following poe command:\npoetry poe run-digital-data-etl-paul\nfigured a command that calls the data collection pipeline for all the supported authors:\npoetry poe run-digital-data-etl\nWe can easily query the MongoDB data warehouse using our ODM classes.\nquery all the articles collected for Paul Iusztin:\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any \nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB \nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML ",
      "keywords": [
        "data collection pipeline",
        "Data",
        "data collection",
        "collection pipeline",
        "Paul Iusztin",
        "pipeline",
        "Data Engineering",
        "str class Settings",
        "collection",
        "run",
        "Paul",
        "user",
        "ODM",
        "class Settings",
        "ODM class"
      ],
      "concepts": [
        "data",
        "run",
        "running",
        "mongodb",
        "users",
        "importing",
        "documents",
        "zenml",
        "link",
        "collection"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 20,
          "title": "",
          "score": 0.773,
          "base_score": 0.623,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 10,
          "title": "",
          "score": 0.576,
          "base_score": 0.576,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "collection",
          "data collection",
          "data",
          "collection pipeline",
          "class"
        ],
        "semantic": [],
        "merged": [
          "collection",
          "data collection",
          "data",
          "collection pipeline",
          "class"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3089916680471248,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038238+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 124-132)",
      "start_page": 124,
      "end_page": 132,
      "summary": "Figure 3.7: Fix Selenium issues when crawling raw data\nproceed to the fine-tuning and inference sections without running the data collection ETL code.\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM \nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions \nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking \nprogrammatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.\nRAG Feature Pipeline\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications.\nRAG’s \ncore responsibility is to inject custom data into the large language model (LLM) to perform a \nthe LLM on data it wasn’t trained on (e.g., private or new data).\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and \nadvanced RAG system.\nThen, we will continue exploring LLM Twin’s RAG feature pipeline archi-\nFinally, we will go through a practical example by implementing the LLM Twin’s RAG \nUnderstanding RAG\nExploring the LLM Twin’s RAG feature pipeline architecture\nImplementing the LLM Twin’s RAG feature pipeline\nRAG Feature Pipeline\nUnderstanding RAG\nRAG enhances the accuracy and reliability of generative AI models with information fetched from \nGeneration: Use the augmented prompt with an LLM for generation\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized \naccess to the newest data or any other external sources on which it wasn’t trained.\nThe model is trained on data up to October 2023.\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and \nWhy use RAG?\nWe briefly explained the importance of using RAG in generative AI applications earlier.\nnecessary information into the prompt to answer the initial user question.\nthe augmented prompt to the LLM for the final answer.\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high \nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced con-\nRAG will act as the single source of truth for the generated answer.\nevaluate if the LLM’s answer is based on the external data or not.\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or \nDirectly injecting the necessary data to respond to user questions into the \nprompts that are fed to the LLM is enough to generate correct and valuable answers.\nright data into the prompt based on the user’s questions?\nof RAG in the next sections.\nRAG Feature Pipeline\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and \nan LLM to generate answers\npopulate the vector DB with external data.\nThe generation pipelines use a prompt template, user input, and retrieved context to \nThe prompt is passed to an LLM to generate the answer.\nYou must implement RAG in your generative AI application when you need access to any type of \nhave to implement a RAG strategy in your generative AI project.",
      "keywords": [
        "RAG feature pipeline",
        "RAG",
        "LLM",
        "LLM Twin",
        "RAG feature",
        "data",
        "feature pipeline",
        "RAG system",
        "pipeline",
        "answer",
        "Understanding RAG",
        "Selenium",
        "advanced RAG",
        "data collection pipeline",
        "RAG ingestion pipeline"
      ],
      "concepts": [
        "rag",
        "data",
        "pipeline",
        "generative",
        "generate",
        "answer",
        "answered",
        "python",
        "world",
        "retrieves"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.877,
          "base_score": 0.727,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.785,
          "base_score": 0.635,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 33,
          "title": "",
          "score": 0.763,
          "base_score": 0.613,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rag",
          "data",
          "llm",
          "pipeline",
          "rag feature"
        ],
        "semantic": [],
        "merged": [
          "rag",
          "data",
          "llm",
          "pipeline",
          "rag feature"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44120237169753246,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038296+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 133-140)",
      "start_page": 133,
      "end_page": 140,
      "summary": "Ultimately, it loads the embedded chunks into a vector DB (or other similar \nsource and embedding model.\npass the document’s content to an embedding model, this is necessary to ensure it doesn’t \nso, at the retrieval time, you will add only the essential data to the prompt.\nThe embedding component uses an embedding model to take the chunk’s content (text, \non embeddings in the What are embeddings?\nThe loading module takes the embedded chunks along with a metadata document.\nmetadata will contain essential information such as the embedded content, the URL to \nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, \nThe next step is to retrieve relevant data from the vector store correctly.\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the \nvector DB for similar vectors to the user’s input.\nThe primary function of the retrieval step is to project the user’s input into the same vector space \nas the embeddings used as an index in the vector DB.\nilar entries by comparing the embeddings from the vector storage with the user’s input vector.\nMost of the time, the cosine distance works well in non-linear complex vector spaces.\ndata and the embedding model you use.\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vec-\npreprocess the user input in the same way you processed the raw documents in the RAG ingestion \nThis means you must clean, chunk (if necessary), and embed the user’s input using the \nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and \nprompt += prompt_template.format(context=retrieved_context, user_\nyour RAG system are the embeddings of the external data, usually stored in vector DBs, the em-\non what embeddings are and how they are computed.\nWhat are embeddings?\nsentations of objects encoded as vectors in a continuous vector space, such as words, images, or \nFigure 4.2: What are embeddings?\nA popular method is visualizing the embeddings to understand and evaluate their geometrical \nAs the embeddings often have more than 2 or 3 dimensions, usually between 64 \nFigure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)\nmatical technique used to reduce the number of input variables or features in a data-\nWhy embeddings are so powerful\nEmbeddings come in handy when we want to feed words, images, or audio data into models.\nFor instance, when working with transformer models, you tokenize all your text input, where \nthe input to the transformer is a sequence of embeddings, which can be easily and confidently \nBased on this example, you can use embeddings to encode any categorical variable and feed it to \nSecondly, embedding your input reduces the size of its dimension and condenses all of its se-\nimages, where a CNN encoder module maps the high-dimensional meaning into an embedding, \nthat it can lead to a high-dimensional feature space if the categorical variable has \nEmbeddings help us encode categorical variables while controlling the output vec-\ninto the final vector embedding, a numerical image representation.\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\nHow are embeddings created?\nEmbeddings are created by deep learning models that understand the context and semantics of \nyour input and project it into a continuous vector space.\nVarious deep learning models can be used to create embeddings, varying by the data input type.\nembedding model.\nFor example, when working with text data, one of the early methods used to create embeddings \nture to smartly project your input into a dense vector space that can later be used as embeddings.\nprovides a user-friendly interface, making the embedding process straightforward and efficient.",
      "keywords": [
        "RAG Feature Pipeline",
        "embeddings",
        "RAG ingestion pipeline",
        "data",
        "vector",
        "input",
        "user",
        "prompt",
        "RAG Feature",
        "RAG",
        "embedding model",
        "Pipeline",
        "RAG ingestion",
        "Feature Pipeline",
        "vector space"
      ],
      "concepts": [
        "data",
        "embedded",
        "vector",
        "input",
        "prompt",
        "learning",
        "retrieval",
        "retrieve",
        "retrieved",
        "method"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.794,
          "base_score": 0.644,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.749,
          "base_score": 0.599,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "embeddings",
          "vector",
          "input",
          "embedding",
          "user input"
        ],
        "semantic": [],
        "merged": [
          "embeddings",
          "vector",
          "input",
          "embedding",
          "user input"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37986205277632956,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038347+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 141-148)",
      "start_page": 141,
      "end_page": 148,
      "summary": "ed the embeddings for three sentences, and, ultimately, computed the cosine similarity between \n\"The dog is swimming.\"\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings, embeddings)\nLLM-Engineering/blob/main/code_snippets/08_text_embeddings.py.\nThe best-performing embedding model can change with time and your specific use case.\nof the audio, such as a spectrogram, and then apply image embedding models to those visuals.\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the \nThis allows you to find similar images using a sentence as input, or the other \nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences.\n# Output: tensor([[0.3068, 0.3300, 0.1719]])\nblob/main/code_snippets/08_text_image_embeddings.py.\nmost digital data categories, such as words, sentences, documents, images, videos, and graphs.\nbetween two different data categories, such as the distance between the vector of a sentence and \nThese models are designed to project both data types into the same vector space, \nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popu-\nlar in information retrieval tasks, such as semantic search for text, code, images, and audio, and \nThe last step to fully understanding how RAG works is to examine vector DBs and how they \nleverage embeddings to retrieve data.\nMore on vector DBs\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embed-\nTraditional scalar-based DBs struggle with the complexity of vector data, making vector \nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ \nVector DBs support CRUD operations, metadata \nHow does a vector DB work?\nVector DBs are different.\nUnder the hood, a vector DB uses \nIndexing vectors: Vectors are indexed using data structures optimized for high-dimen-\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those \nmost similar to the input vector.\nVector DBs can filter results based on metadata before or after the vector search.\n(along with the vector index), so it contains a metadata index user for filtering operations.\nAlgorithms for creating the vector index\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\nLSH: LSH maps similar vectors into buckets.\nest neighbor searches by focusing on a subset of the data, reducing the computational \nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making \nVector DBs also share common characteristics with standard DBs to ensure high performance, \nIs the retrieved context enough to answer the user’s question?\ndata and generate answers relative to the user’s question.\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data \nRetrieval: This stage revolves around improving the embedding models and metadata \nfiltering to improve the vector search step.\nData indexing: It is part of the RAG ingestion pipeline.\nthe cleaning or chunking modules to preprocess the data for better indexing.\nbedding it and retrieving the chunks from the vector DB.\nAs we index our data using embeddings that semantically represent the content of a chunked \ndocument, most of the data indexing techniques focus on better preprocessing and structuring \nthe data to improve retrieval efficiency, such as:\nOptimizing index structures: It is based on different data index methods, such as various \nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used ",
      "keywords": [
        "RAG Feature Pipeline",
        "vector",
        "vector DBs",
        "data",
        "RAG",
        "RAG Feature",
        "embeddings",
        "similarity",
        "DBs",
        "advanced RAG",
        "sentences",
        "text",
        "Feature Pipeline",
        "image",
        "vector index"
      ],
      "concepts": [
        "data",
        "vector",
        "model",
        "retrieve",
        "retrieved",
        "rag",
        "embeddings",
        "index",
        "images",
        "similarity"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 22,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 33,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.479,
          "base_score": 0.329,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vector",
          "dbs",
          "vector dbs",
          "index",
          "embeddings"
        ],
        "semantic": [],
        "merged": [
          "vector",
          "dbs",
          "vector dbs",
          "index",
          "embeddings"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20888422036018106,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038388+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 149-163)",
      "start_page": 149,
      "end_page": 163,
      "summary": "RAG Feature Pipeline\ngories of data and query each category differently.\ncan retrieve additional context from a vector DB using vector search queries, a standard \nRAG Feature Pipeline\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly \nThus, as with any data processing pipeline, no method \nImproving the embedding models used in the RAG ingestion pipeline to encode the \nthe semantic similarity between the query and the indexed data.\nIt differs from a hybrid search in that you retrieve the data \nRAG Feature Pipeline\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the \nthe data using a similarity distance between the embeddings and refine the retrieved \ndata you work with.\nRAG Feature Pipeline\nFor example, if you work with multi-modal data such as text and images, most of the techniques \nvector indexing, adjusting user queries for more accurate searches, enhancing the embedding \nin mind, you can effectively optimize your RAG workflow for data processing and retrieval\nExploring the LLM Twin’s RAG feature pipeline \nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a \nThe inference pipeline queries the vector DB for relevant context and ultimately generates \nwe get our raw data.\nRAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) \nfrom our MongoDB data warehouse.\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and \nRAG Feature Pipeline\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and \nlogical feature store while processing the data accordingly.\nHaving the data in a feature store \nThe LLM Twin inference pipeline will query it for \nThe training pipeline will use the cleaned data from the feature store (stored \nas artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked doc-\nThat is why we are designing a feature pipeline and not only a RAG ingestion \npipeline.\nIt clearly states that it takes raw data as input and then outputs features and optional \nbetween the data warehouse and the feature store goes into the feature pipeline namespace, con-\nin cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under \nthe feature pipeline umbrella as the artifacts are part of the logical feature store.\nwould be implementing a data validation pipeline on top of the raw data or computed features.\nAnother important observation to make is that text data stored as strings are not considered \nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse.\nThe data \nwarehouse is populated by the data collection ETL pipeline presented in Chapter 3.\nDesigning the architecture of the RAG feature pipeline\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM \nWe will use a batch design scheduled to poll data from the MongoDB data \nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\nRAG Feature Pipeline\nBatch pipelines\nA batch pipeline in data systems refers to a data processing method where data is collected, pro-\nproach differs from real-time or streaming data processing, where data is processed continuously \nScheduled processing: Data processing is scheduled at regular intervals, for example, \nDuring this time, the collected data is processed in bulk.\nData loading: After processing, the data is loaded into the target system, such as a DB, data \nwarehouse, data lake, or feature store.\nThis processed data is then available for analysis, \nBatch pipelines are particularly useful when dealing with large volumes of data that do not require \nEfficiency: Batch processing can handle large volumes of data more efficiently than re-\nComplex processing: Batch pipelines can perform complex data transformations and \nBatch versus streaming pipelines\nWhen implementing feature pipelines, you have two main design choices: batch and streaming.\nbatch architecture over a streaming one for our LLM Twin use case.\nTable 4.1 compares batch and streaming pipelines based on multiple criteria such as processing \nBatch pipeline\nStreaming pipeline\nProcesses data at regular \nProcesses data \ndata more efficiently, \ncomplex data transformations \nimmediate data processing \nand feature pipelines.\nTable 4.1: Batch versus streaming pipelines\nRAG Feature Pipeline\nFor example, streaming pipelines are extremely powerful in social media recommender systems \nBy implementing a streaming pipeline, you update \nthe features of specific users in real time, which are then passed to a chain of models that predict \nactions as they occur, not after a few minutes or hours as a batch pipeline would process them.\ndations periodically, such as every night, based on historical user behavior data using a batch \nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load \ndata for different use cases.\nThe ETL design is widespread in data pipelines used to move data \nSome practical use cases include aggregating data for analytics, where \nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline \nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming \nDoes not require immediate data processing: Even if syncing the data warehouse and \nThe whole data \nRAG Feature Pipeline\nbatch) and the quantity of data you have to process (small versus big data).\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\nIn the Change data capture: syncing the data warehouse and feature store section later in this chapter, \nMost of the RAG feature pipelines are composed of five core steps.\nRAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\nAt the extraction step, you usually aggregate all the data you need ",
      "keywords": [
        "RAG Feature Pipeline",
        "Feature Pipeline",
        "RAG Feature",
        "data",
        "Pipeline",
        "LLM Twin",
        "Feature",
        "RAG",
        "feature store",
        "data warehouse",
        "batch pipeline",
        "RAG ingestion pipeline",
        "LLM",
        "batch",
        "streaming pipelines RAG"
      ],
      "concepts": [
        "data",
        "query",
        "queries",
        "process",
        "processing",
        "processes",
        "pipeline",
        "search",
        "searches",
        "feature"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.833,
          "base_score": 0.683,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.785,
          "base_score": 0.635,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "feature",
          "pipeline",
          "batch",
          "streaming"
        ],
        "semantic": [],
        "merged": [
          "data",
          "feature",
          "pipeline",
          "batch",
          "streaming"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3653777547753808,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038438+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 164-171)",
      "start_page": 164,
      "end_page": 171,
      "summary": "Chunking: You must adopt various chunking strategies based on each data category \nThat is why you usually chunk a document based on your data \nData loading: The final step combines the embedding of a chunked document and its \nfor the features, we also push the cleaned documents (before chunking) to Qdrant.\npush data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB.\nRAG Feature Pipeline\nChange data capture: syncing the data warehouse and feature \ndata lakes, data warehouses, and feature stores getting out of sync.\nChange data capture (CDC) \nThe syncing issues also apply when building a feature pipeline.\nhow to sync the data warehouse with the feature store to have data fresh enough for your par-\nWhat happens if a record is deleted from the data warehouse?\nthe feature store?\nWhat if we want to process only the new or updated items from the data warehouse and \ndata changes.\nWith that in mind, there are different methods to detect changes in data.\nsource DB, captures all data changes, and requires no schema modification.\nin our RAG feature pipeline to sync the data warehouse and feature store more optimally when \nin the source DB; it just pulls everything from the data warehouse.\nFor more details on CDC, I recommend What is Change Data Capture?\nRAG Feature Pipeline\nWhy is the data stored in two snapshots?\nWe store two snapshots of our data in the logical feature store:\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the Mon-\nanother summarization use case where we must clean and preprocess the data differently.\ndata warehouse is generic and is modeled to specific applications only in downstream compo-\nBased on these factors, we decided to keep the cleaned data in Qdrant, \npipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, \nThis is a reminder that our logical feature store comprises the Qdrant vector \nZenML will orchestrate the batch RAG feature pipeline.\nit after the ETL data collection pipeline finishes.\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration \nImplementing the LLM Twin’s RAG feature pipeline\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied every-\nThe cleaning, chunking, and embedding logic for all our data categories\nThus, let’s start with the Settings class and ZenML pipeline.\nRAG Feature Pipeline\nZenML pipeline and steps\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline.\nEngineers-Handbook/blob/main/pipelines/feature_engineering.py:\nfrom llm_engineering.interfaces.orchestrator.steps import feature_\ndef feature_engineering(author_full_names: list[str]) -> None:\nraw_documents = fe_steps.query_data_warehouse(author_full_names)\nlast_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\nembedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\nlast_step_2 = fe_steps.load_to_vector_db(embedded_documents)\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps \nFigure 4.12: Feature pipeline DAG in the ZenML dashboard\nRAG Feature Pipeline\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically.\nauthors of this book as we want to populate the feature store with data from all of us (available \nfeature_engineering.with_options(config_path=\"…/feature_engineering.yaml\")\nThus, we can call the feature engineering pipeline calling the \ncan run the feature pipeline using the following poe command:\npoetry poe run-feature-engineering-pipeline\nall the feature engineering pipeline steps is available on GitHub at \"steps/feature_engineering\".\nWe will begin with the first step, which involves querying the data warehouse for new content ",
      "keywords": [
        "RAG Feature Pipeline",
        "Feature Pipeline",
        "RAG Feature",
        "data",
        "Feature",
        "feature engineering pipeline",
        "Pipeline",
        "data warehouse",
        "feature store",
        "RAG",
        "embedding model",
        "Qdrant",
        "RAG feature engineering",
        "Change data capture",
        "model"
      ],
      "concepts": [
        "data",
        "pipeline",
        "step",
        "updates",
        "updated",
        "based",
        "useful",
        "change",
        "changing",
        "approach"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 10,
          "title": "",
          "score": 0.593,
          "base_score": 0.443,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.58,
          "base_score": 0.58,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.506,
          "base_score": 0.506,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "pipeline",
          "data",
          "feature pipeline",
          "rag feature"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "pipeline",
          "data",
          "feature pipeline",
          "rag feature"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3101519555567473,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038492+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 172-179)",
      "start_page": 172,
      "end_page": 179,
      "summary": "It fetches all the raw data for the user from the data warehouse and extends the documents\nlist to include these user documents.\ndocuments = []\nlogger.info(f\"Querying data warehouse for user: {author_full_\nuser_documents = [doc for query_result in results.values() for doc \nstep_context.add_output_metadata(output_name=\"raw_documents\", \nmetadata=_get_metadata(documents))\nreturn documents\nThe _get_metadata() function takes the list of queried documents and authors and counts the \ndef _get_metadata(documents: list[Document]) -> dict:\nmetadata[collection][\"authors\"] = list()\nmetadata[collection][\"num_documents\"] = metadata[collection].\nmetadata[collection][\"authors\"].append(document.author_full_name)\nFor example, in Figure 4.13, we accessed the metadata tab of the query_data_warehouse()\ndocuments from three authors.\nCleaning the documents\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a \ndef clean_documents(\n) -> Annotated[list, \"cleaned_documents\"]:\ncleaned_documents = []\nstep_context.add_output_metadata(output_name=\"cleaned_documents\", \nmetadata=_get_metadata(cleaned_documents))\nreturn cleaned_documents\nThe computed metadata is similar to what we logged in the query_data_warehouse() step.\nChunk and embed the cleaned documents\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to \ncleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\nmetadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": \nlen(cleaned_documents)}\nchunks = ChunkingDispatcher.dispatch(document)\nmetadata[\"chunking\"])\nmetadata[\"num_chunks\"] = len(embedded_chunks)\nstep_context.add_output_metadata(output_name=\"embedded_documents\", \nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step.\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized \nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details \nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding \nto group all the documents based on their data category.",
      "keywords": [
        "documents",
        "metadata",
        "step",
        "RAG Feature Pipeline",
        "data",
        "query",
        "Document",
        "list",
        "user",
        "chunks",
        "Feature Pipeline",
        "RAG Feature",
        "cleaned",
        "data warehouse",
        "embedding"
      ],
      "concepts": [
        "documents",
        "document",
        "metadata",
        "chunking",
        "step",
        "domain",
        "querying",
        "queried",
        "list",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 25,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 11,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metadata",
          "documents",
          "cleaned_documents",
          "chunking",
          "embedding"
        ],
        "semantic": [],
        "merged": [
          "metadata",
          "documents",
          "cleaned_documents",
          "chunking",
          "embedding"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.259004356605614,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038541+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 180-188)",
      "start_page": 180,
      "end_page": 188,
      "summary": "We decided to create a base class for each state of the document, resulting in having the following \nbase abstract classes:\nclass CleanedDocument(VectorBaseDocument, ABC)\nclass Chunk(VectorBaseDocument, ABC)\nclass EmbeddedChunk(VectorBaseDocument, ABC)\nNote that all of them inherit the VectorBaseDocument class, which is our custom OVM implemen-\nmakes the class abstract.\nThat is why base classes are always marked as abstract.\nEach base abstract class from above (which models the state) will have a subclass that adds \nabstract classes.\nWe will implement a specific document class for each data category and state com-\nThus, structuring the classes after the state allows us to plug another data \ncategory by inheriting these base abstract classes.\ncleaned document will be saved within the metadata of the vector DB.\nAnother fundamental aspect is the Config internal class, which defines the name of the collection \nwithin the vector DB, the data category of the entity, and whether to leverage the vector index \nclass CleanedDocument(VectorBaseDocument, ABC):\nclass Config:\nclass Config:\nclass Config:\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embed-\nclass Chunk(VectorBaseDocument, ABC):\nclass EmbeddedChunk(VectorBaseDocument, ABC):\nVectorBaseDocument OVM class.\nOur OVM base class is called VectorBaseDocument.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef from_record(cls: Type[T], point: Record) -> T:\nif cls._has_class_attribute(\"embedding\"):\npayload[\"embedding\"] = point.vector or None\nThe VectorBaseDocument class inherits from Pydantic’s BaseModel and helps us structure \nFor example, the from_record() method of the Chunk() class, \nThe from_record() method adapts a data point from Qdrant’s format to our internal structure \nThe bulk_insert() method maps each document to a point.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \ncls._bulk_insert(documents)\ncls._bulk_insert(documents)\nlogger.error(f\"Failed to insert documents in '{cls.get_\ndef _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \nThe collection name is inferred from the Config class defined in the subclasses inheriting the OVM:\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef get_collection_name(cls: Type[T]) -> str:\nNow, we must define a method that lets us read all the records from the vector DB (without using \nThe function below scrolls the Qdrant vector DB, which returns a list of data \nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ndocuments, next_offset = cls._bulk_find(limit=limit, **kwargs)\nlogger.error(f\"Failed to search documents in '{cls.get_\nreturn documents, next_offset\ndef _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \nreturn documents, next_offset\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef search(cls: Type[T], query_vector: list, limit: int = 10, \ndocuments = cls._search(query_vector=query_vector, \nlogger.error(f\"Failed to search documents in '{cls.get_\nreturn documents\ndef _search(cls: Type[T], query_vector: list, limit: int = 10, ",
      "keywords": [
        "vector",
        "collection",
        "VectorBaseDocument",
        "RAG Feature Pipeline",
        "ABC",
        "documents",
        "VectorBaseDocument class",
        "type",
        "str class Config",
        "str",
        "OVM",
        "Qdrant",
        "class Config",
        "offset",
        "data"
      ],
      "concepts": [
        "classes",
        "document",
        "documents",
        "vector",
        "type",
        "typed",
        "typing",
        "collection",
        "search",
        "offset"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.773,
          "base_score": 0.623,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.501,
          "base_score": 0.501,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.451,
          "base_score": 0.451,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vectorbasedocument",
          "class",
          "cls",
          "abc",
          "cls type"
        ],
        "semantic": [],
        "merged": [
          "vectorbasedocument",
          "class",
          "cls",
          "abc",
          "cls type"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2633051224638313,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038584+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 189-197)",
      "start_page": 189,
      "end_page": 197,
      "summary": "on to the dispatchers who clean, chunk, and embed the documents.\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, \nA handler can either clean, chunk, or embed a document.\nBased on its data category, it instantiates and calls a handler that \napplies the cleaning logic specific to that data point:\ndef dispatch(cls, data_model: NoSQLBaseDocument) -> \nhandler = cls.cleaning_factory.create_handler(data_category)\nclean_model = handler.clean(data_model)\n\"Data cleaned successfully.\",\nreturn clean_model\ncleaning handler based on the document’s data category:\ndef create_handler(data_category: DataCategory) -> \nYou have a single class that cleans any document, which \ncorrect handler based on the data category of the input document.\nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.\nInitially, we knew we wanted to clean the data, but as we knew the data category only at \ncleaning handler for its data type.\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, \nIn total, we will have nine Handler classes that follow the next \nUntil now, we have just modeled our entities and how the data flows in our appli-\nWe haven’t written a single piece of cleaning, chunking, or embedding code.\nThe cleaning handlers\ndef clean(self, data_model: DocumentT) -> CleanedDocumentT:\ndef clean(self, data_model: PostDocument) -> CleanedPostDocument:\ncontent=clean_text(\" #### \".join(data_model.content.\ndef clean(self, data_model: ArticleDocument) -> \ndef clean(self, data_model: RepositoryDocument) -> \ncontent=clean_text(\" #### \".join(data_model.content.\nThe handlers input a raw document domain entity, clean the content, and return a cleaned docu-\nAll the handlers use the clean_text() function to clean the text.\nthe same cleaning technique for all the data categories.\nto further optimize and create a different cleaning function for each data category.\ncleaned data used for fine-tuning will be accessed from the logical feature store, making it the \nThe chunking handlers\nchunking logic.\nThe handler takes cleaned documents as input and returns chunk entities.\n\"chunk_size\": 500,\ndef chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\nThe handler’s chunk() method inputs cleaned article documents and returns a list of article chunk \nIt uses the chunk_text() function to split the cleaned content into chunks.\nThe chunking \nThe chunk_id \na list of chunk entities and return them.\ndef chunk(self, data_model: CleanedArticleDocument) -> \ndata_models_list = []\ncleaned_content = data_model.content\ncontent=chunk,\ndocument_id=data_model.id,\nreturn data_models_list\nIt groups sentences into a single chunk until the max_length limit is reached.\ndef chunk_article(text: str, min_length: int, max_length: int) -> \ncurrent_chunk = \"\"\nif len(current_chunk) + len(sentence) <= max_length:\ncurrent_chunk += sentence + \" \"\ncurrent_chunk = sentence + \" \"\nengineering/application/preprocessing/chunking_data_handlers.py, have a similar struc-\nThe chunk_text() function is a two-step process that has \nAt this point, we also apply the chunk_overlap logic, as we want to do it only after we ",
      "keywords": [
        "data",
        "chunk",
        "RAG Feature Pipeline",
        "data category",
        "handler",
        "RAG Feature",
        "Feature Pipeline",
        "clean",
        "category",
        "text",
        "model",
        "length",
        "content",
        "document",
        "code"
      ],
      "concepts": [
        "classes",
        "clean",
        "chunk",
        "handlers",
        "returns",
        "documents",
        "document",
        "data",
        "pattern",
        "content"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 20,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 11,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "handler",
          "data_model",
          "clean",
          "chunk",
          "cleaning"
        ],
        "semantic": [],
        "merged": [
          "handler",
          "data_model",
          "clean",
          "chunk",
          "cleaning"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3417048319223507,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038634+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 198-207)",
      "start_page": 198,
      "end_page": 207,
      "summary": "tokens_per_chunk=embedding_model.max_input_length,\nmodel_name=embedding_model.model_id,\nparameters and the embedding model’s max input length.\nWe took this approach because, when calling the embedding model, \nWe implemented an embed() method, in case you want to run the inference on a single data point, \ngathers their content into a list, passes them to the embedding model, and maps the results to an \nThe mapping is done through the map_model() abstract method, \nembedding_model = EmbeddingModelSingleton()\nAbstract class for all embedding data handlers.\nAll data transformations logic for the embedding step is done here\ndef embed(self, data_model: ChunkT) -> EmbeddedChunkT:\nreturn self.embed_batch([data_model])[0]\ndef embed_batch(self, data_model: list[ChunkT]) -> \nembedding_model_input = [data_model.content for data_model in \ndata_model]\nembeddings = embedding_model(embedding_model_input, to_list=True)\nself.map_model(data_model, cast(list[float], embedding))\nfor data_model, embedding in zip(data_model, embeddings, \nreturn embedded_chunk\ndef map_model(self, data_model: ChunkT, embedding: list[float]) -> \nAs you can see, we only have to implement the map_model() method, which \ntakes a chunk of input and computes the embeddings in batch mode.\ndef map_model(self, data_model: ArticleChunk, embedding: list[float]) \nid=data_model.id,\ncontent=data_model.content,\nplatform=data_model.platform,\nlink=data_model.link,\ndocument_id=data_model.document_id,\nauthor_id=data_model.author_id,\nauthor_full_name=data_model.author_full_name,\n\"embedding_model_id\": embedding_model.model_id,\n\"embedding_size\": embedding_model.embedding_size,\n\"max_input_length\": embedding_model.max_input_length,\nmodel.\nThe SentenceTransformer() class is initialized with the model_id defined in the Settings class, \nallowing us to quickly test multiple embedding models just by changing the configuration file \nThat is why I am not insisting at all on what embedding model to use.\nclass, which can quickly be configured, you can experiment with multiple embedding models \nmodel_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\ndevice: str = settings.RAG_MODEL_DEVICE,\nself._model_id = model_id\nself._model = SentenceTransformer(\nself._model_id,\nself._model.eval()\ndef model_id(self) -> str:\nreturn self._model_id\ndummy_embedding = self._model.encode(\"\")\nreturn self._model.max_seq_length\nreturn self._model.tokenizer\nembeddings = self._model.encode(input_text)\nlogger.error(f\"Error generating embeddings for {self._model_\nThe embedding model class implements the singleton pattern (https://refactoring.guru/\nOtherwise, you risk loading the model in memory every time you use it or loading it \nmodel to find the size of its output.\nembedding techniques based on the data category of each document.\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June.\nLearning transferable visual models from \nSFT refines the model’s capabilities using carefully curated pairs of instructions and correspond-\nThe importance of SFT lies in its ability to bridge the gap between a model’s general language \nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning \nThis careful review helps ensure that the dataset is accurate and useful for training the model.\ninputs of the model, used as context during fine-tuning.\nthe model.\nDuring fine-tuning, you can choose to train the model on the instructions and answers, \nIn this case, “inputs” contain the data the model \nof the model.",
      "keywords": [
        "model",
        "embedding model",
        "embedding",
        "RAG Feature Pipeline",
        "data",
        "RAG",
        "multiple embedding models",
        "RAG Feature",
        "embedding model class",
        "list",
        "input",
        "self.",
        "instruction",
        "Feature Pipeline",
        "Pipeline"
      ],
      "concepts": [
        "embedding",
        "data",
        "model",
        "learned",
        "returns",
        "input",
        "pipeline",
        "pattern",
        "rag",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 16,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.607,
          "base_score": 0.457,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data_model",
          "embedding",
          "self",
          "embedding_model",
          "_model"
        ],
        "semantic": [],
        "merged": [
          "data_model",
          "embedding",
          "self",
          "embedding_model",
          "_model"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35498626110988263,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038687+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 208-215)",
      "start_page": 208,
      "end_page": 215,
      "summary": "Table 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\nTo build an instruction dataset, we want to curate data that is representative of how the model will \nOnce we have gathered enough samples, our goal is to filter them to only keep high-quality \ndata.\nIn this context, high-quality data can be described through three main dimensions:\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering \nBy sampling data in a representative \nIn the following sections, we will see techniques to filter and evaluate instruction samples ac-\nit with high-quality data.\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the \nfor example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in \nquality of the data is a crucial factor, and a high number of samples is always desirable.\naimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, \nGeneral-purpose models cover more topics, which requires additional samples.\nsource community, models like OpenHermes and Dolphin use around one million samples.\nsamples to create a good general-purpose instruct model.\nfor a specific purpose require fewer samples.\nHere, we differentiate task-specific models from \nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs.\nTask-specific models are designed to excel at a particular function, such as translation, summari-\nThe data required for task-specific fine-tuning is generally more manageable, \nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge \nThe data requirements for domain-specific fine-tuning can vary widely depending on the com-\nor hospitality, might need fewer samples, more in line with task-specific fine-tuning.\nThe key factors determining the data needs for domain-specific models are the “size” of the \nthat domain in the model’s pre-training data.\ntraining data may require less fine-tuning, while those that are more specialized or underrep-\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and \ndomain-specific models.\nFor task-specific models, data curation often involves collecting examples \nDomain-specific data curation can be more challenging.\nrelevance of this data is crucial, as it directly impacts the model’s ability to understand and gen-\nerful models by providing a few examples of the desired task within the input prompt.\nIn practice, the line between task-specific and domain-specific models can sometimes blur.\ninstance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused \nnext step consists of refining the quality of the samples through rule-based filtering, data dupli-\ncation, data decontamination, and data quality evaluation.\nRule-based filtering is a systematic approach to data quality control that relies on explicit, pre-\ndefined rules to evaluate and filter data samples.\nprimary goal of rule-based filtering is to maintain a high standard of data quality by removing \nFormat checking is recommended for datasets that include structured data or follow specific \nticularly important for datasets containing code samples, JSON structures, or other formatted \nfiltering reduces the need for manual intervention and enables continuous data quality monitoring.\nas data patterns and quality standards evolve, rules need regular review and updates to remain \nData deduplication\nDataset diversity is fundamental to training models that can generalize well to new, unseen data.\nBiased performance: Overrepresented data points may skew the model’s performance \ntion removes identical samples through a straightforward process involving data normalization, \nData decontamination is the process of ensuring that the training dataset does not contain samples \nremove any training samples that are identical to those in the evaluation sets.\nmethods to identify and remove training samples that are very similar to evaluation samples, \nAnother aspect of data decontamination is filtering out samples that may have been derived from \nthe same source as evaluation data.\nthe data they use) to identify and exclude data from specific sources that are known to be used \ninstruction dataset during the data deduplication stage.\nensure that we only remove samples from the instruction dataset, which can be \nevaluation sets in the data deduplication stage to fully automate this process.\nData quality evaluation\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process \nTraditional methods of data quality assessment include human annotation, which generally \nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample.\nWith domain-specific datasets, \nyou might want to use domain-specific models instead of better, general-purpose LLMs. Com-",
      "keywords": [
        "data",
        "samples",
        "models",
        "data quality",
        "domain-specific models",
        "quality",
        "datasets",
        "data quality evaluation",
        "Fine-Tuning",
        "domain-specific",
        "filtering",
        "evaluation",
        "rule-based filtering",
        "data decontamination",
        "Instruction"
      ],
      "concepts": [
        "data",
        "model",
        "sample",
        "sampling",
        "specific",
        "dataset",
        "instruction",
        "instructions",
        "instruct",
        "rule"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "samples",
          "data",
          "quality",
          "specific",
          "specific models"
        ],
        "semantic": [],
        "merged": [
          "samples",
          "data",
          "quality",
          "specific",
          "specific models"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3336924257747012,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038741+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 216-225)",
      "start_page": 216,
      "end_page": 225,
      "summary": "Instruction\nYou are a data quality evaluator.\nA score of 1 means that the answer is terrible and irrelevant to the instruction.\ninstruction.\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\nReward models are another way to re-purpose LLMs for data quality evaluation.\ncan be broadly defined as models that take an instruction and answer pair and return a score as \nThis allows for a more fine-grained approach to data quality evaluation.\nof chosen and rejected answers for each instruction.\nstruction data quality, it is a good resource for finding models capable of differentiating between \nClassifiers or encoder-only models can be trained to perform data quality evaluation.\nThis model was designed as a quality filter for pretraining data but a similar \napproach can be taken to evaluate instruction samples at scale.\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\nLet’s consider the task of building an instruction dataset about various programming languages.\nData generation\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary.\nWhile data can be \nSynthetic data generation using LLMs offers a more efficient \nThe process of synthetic data generation typically begins with the preparation of a set of carefully \nThe quality of synthetically generated data largely depends on the prompts and techniques \ndiverse, relevant, and high-quality instruction-response pairs.\ncific instructions, examples, and constraints to ensure the generated data aligns with the desired \nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality.\nmay include generating an initial set of questions or instructions, followed by generating corre-\ngenerated data.\nThis includes factors such as the complexity of the instructions, the length of the \nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps \nHowever, synthetic data generation also comes with challenges.\npotential for the generated data to inherit biases or errors from the underlying language model \ngenerated data.\nAnother consideration is the need for the generated data to be sufficiently diverse and challeng-\nAdvanced techniques in synthetic data generation often focus on \nthe quality of data samples.\nUnlike data generation, we use pre-existing instruction samples \nWhile it is possible to upsample pairs of instructions and answers, data \nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple \nThe evolved instructions can then be used to generate \nIn-depth evolving focuses on enhancing the complexity of existing instructions.\nComplicating input: This involves adding more complex data formats or structures to \nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset.\nIt generates entirely new instructions inspired by existing ones, focusing on creating more rare \nto evolve as input, and a powerful model like GPT-4o will return a more complex version of the \noriginal instruction.\nPlease follow the steps below to rewrite the given “#Instruction#” into a more complex \nchange the language of the instruction!\nin Step 1 to make the #Instruction# more complex.\nEnsure that the #Rewritten Instruction# is only a more complex \nversion of the #Instruction#.\nStep 3 #Rewritten Instruction#:\nStep 4 #Finally Rewritten Instruction#:\n#Instruction#:\n{Instruction}\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language \nof instruction quality.\ndiverse instructions and models to generate a wide range of responses.\nscores for these responses across multiple dimensions such as instruction-following, truthfulness, \ning and diverse instruction dataset.\nBy refining and evolving existing instructions and answers, \nthe resulting dataset can better train models to handle complex, multi-step tasks, and improve \nCreating our own instruction dataset\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter \n3. To create a high-quality instruction dataset, we need to address two main issues: the unstruc-\nof pairs of instructions and answers.\nBacktranslation refers to the process of providing the expected answer as output and generat-\ninto chunks and generate three instruction-answer pairs for each chunk.\nspecific templates or instructions, there’s no guarantee that the model will consistently adhere ",
      "keywords": [
        "Instruction",
        "data",
        "Rewritten Instruction",
        "instruction dataset",
        "data quality",
        "data quality evaluation",
        "Synthetic data generation",
        "Data generation",
        "models",
        "dataset",
        "Finally Rewritten Instruction",
        "Synthetic data",
        "quality",
        "generated data",
        "answer"
      ],
      "concepts": [
        "instruction",
        "instruct",
        "instructions",
        "models",
        "generative",
        "generation",
        "generate",
        "answer",
        "diverse",
        "diversity"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 23,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "data",
          "data quality",
          "quality",
          "instructions"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "data",
          "data quality",
          "quality",
          "instructions"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3326963634752049,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038793+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 226-234)",
      "start_page": 226,
      "end_page": 234,
      "summary": "will use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON \nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\nallow us to interact with a model to generate the instruction data, and datasets will format \ndatasets==2.20.0\nimport json\nfrom datasets import Dataset\nThe raw data we have is a JSON file.\nWe create a Hugging Face dataset from this JSON file \nby extracting specific fields from each article: id, content, platform, author_id, author \ndef load_articles_from_json(file_path: str) -> Dataset:\ndata = json.load(file)\nreturn Dataset.from_dict(\nIf we simply load our dataset as a pandas dataframe, it returns the following table.\nsubstack.com/p/\nsubstack.com/p/\nsubstack.com/p/\nNow that we can load our articles, we need to chunk them before turning them into pairs \nof instructions and answers.\nproper formatting, we cannot extract paragraphs or headlines for every article in our raw \ndataset.\nInstead, we will extract sentences using a regex to get chunks between 1,000 \nThe extract_substrings function processes each article in the dataset by first cleaning the \ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nfor article in dataset[\"content\"]:\nNext, we want to create instruction-answer pairs from the extracted chunks of text.\ndata = json.loads(json_str)\npairs = [(pair['instruction'], pair['answer'])\nfor pair in data['instruction_answer_pairs']]\nNow that we have a set of extracts from the articles with a reasonable length, we can use \nan LLM to transform them into pairs of instructions and answers.\nIn our example, we want to create instructions like “Write a paragraph about X topic” and \nwe need to provide an extract that will ground the model’s responses.\nalso choose to generate five instruction-answer pairs for each extract.\nof our function for instruction generation, including our prompt.\ndef generate_instruction_answer_pairs(\nextract: str, client: OpenAI\nprompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer pairs.\nEach instruction \\\ninstructions.\nProvide your response in JSON format with the following structure:\n\"instruction_answer_pairs\": [\n{{\"instruction\": \"...\", \"answer\": \"...\"}},\nel into generating the expected instructions.\n4o mini model in JSON mode and a maximum of 1,200 tokens in the answer.\nparsed using the InstructionAnswerSet class to return pairs of instructions and answers.\ngenerates instruction-answer pairs based on the given \ndataset, then uses concurrent processing via Python’s ThreadPoolExecutor to efficiently \ngenerate instruction-answer pairs for each extract.\ndef create_instruction_dataset(\ndataset: Dataset, client: OpenAI, num_workers: int = 4\n) -> Dataset:\nextracts = extract_substrings(dataset)\ninstruction_answer_pairs = []\nfutures = [executor.submit(generate_instruction_answer_\npairs, extract, client)\ninstruction_answer_pairs.extend(future.result())\ninstructions, answers = zip(*instruction_answer_pairs)\nreturn Dataset.from_dict(\n{\"instruction\": list(instructions), \"output\": list(answers)}\nWe can create our instruction dataset by calling this function.\ndata, creates the instruction dataset, splits it into training and testing sets, and pushes \nthe result to the Hugging Face Hub. def main(dataset_id: str) -> Dataset:\nLoad the raw data\nraw_dataset = load_articles_from_json(\"cleaned_documents.json\")\nprint(\"Raw dataset:\")\nprint(raw_dataset.to_pandas())\ninstruction_dataset = create_instruction_dataset(raw_dataset, \nprint(\"Instruction dataset:\")\nprint(instruction_dataset.to_pandas())\nfiltered_dataset = instruction_dataset.train_test_split(test_\nreturn filtered_dataset\nDataset({\nfeatures: ['instruction', 'output'],\ndataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are \nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub",
      "keywords": [
        "dataset",
        "data",
        "instruction",
        "JSON",
        "pairs",
        "instruction dataset",
        "item",
        "extract",
        "raw data",
        "answer",
        "Hugging Face Hub",
        "text",
        "str",
        "raw",
        "Hugging Face"
      ],
      "concepts": [
        "data",
        "extracting",
        "instructions",
        "importance",
        "important",
        "pairs",
        "returns",
        "answers",
        "openai",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 12,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 13,
          "title": "",
          "score": 0.534,
          "base_score": 0.384,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 11,
          "title": "",
          "score": 0.511,
          "base_score": 0.361,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "dataset",
          "instruction",
          "json",
          "pairs",
          "instructions"
        ],
        "semantic": [],
        "merged": [
          "dataset",
          "instruction",
          "json",
          "pairs",
          "instructions"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2791432140718087,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038838+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 235-244)",
      "start_page": 235,
      "end_page": 244,
      "summary": "Supervised Fine-Tuning\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instruc-\nThe goal of SFT is to turn a base model, which can only perform next-token \nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage \nWhen to fine-tune\nPrompt engineering can be used with either open-weight or closed-source models.\nIf enough data is available, fine-tuning \nFigure 5.8 – Basic flowchart to determine when fine-tuning is an option on a technical level\nyour data”) and customizability (the fine-tuned model is unique).\nmodels in this book, several LLM providers offer automated fine-tuning services.\nleverages pre-existing knowledge in the base model’s weights and refocuses the parameters for \nEven worse, a study showed that fine-tuning a model on new knowledge could result in more \nSupervised Fine-Tuning\nInstruction datasets are stored in a particular format to organize instructions and answers.\nIf you choose to re-train a model \non raw text, this is a type of fine-tuning generally called “continual pre-training.”\nconversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better \nto the model.\nSince base models are not designed to follow instructions, they \nThis means that you can choose any template when you fine-tune \nIf you want to fine-tune an instruct model (not recommended), you need to use \nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and \ninput by the model during fine-tuning.\nsystem and user part as shown in Figure 5.6, and prompt the model to answer by adding <|im_\nBecause the model has been fine-tuned with this template, it understands that the next tokens \nhow fine-tuned models acquire instruction-following capabilities.\nSupervised Fine-Tuning\nof such templates, including Alpaca, which is both the name of an instruction dataset format \n<start_of_turn>model\nParameter-efficient fine-tuning techniques\nfine-tuning, LoRA, and QLoRA.\nFull fine-tuning\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every \nparameter in the base model.\nLike pre-training, SFT uses next-token prediction as its training \nmain difference between continual pre-training and full fine-tuning.\nMemory usage depends on several factors, including model size, training techniques, and op-\nmodel, these are typically the weights in the attention mechanisms, feed-forward layers, \nmodel parameter.\nagation and are used to update the model parameters.\nSupervised Fine-Tuning\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning.\nModel \nwith model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the \nHowever, memory requirements remain substantial for large models even with \nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive \nadditional complexity and its high computational requirements, parameter-efficient techniques \nare often preferred to full fine-tuning to create task and domain-specific models.\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the compu-\na cornerstone technique in LLM fine-tuning.\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced \nFaster fine-tuning process\nPreservation of pre-trained model weights (non-destructive)\nwith limited computational resources, effectively democratizing the process of LLM fine-tuning.\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently.\nInstead of directly modifying the original weight matrix 𝑊𝑊 , LoRA introduces two smaller matrices, \nFigure 5.10 – LoRA adds the two trainable matrices 𝐴𝐴 and 𝐵𝐵 and keeps the pre-trained weights \nSupervised Fine-Tuning\nLoRA can be applied to various parts of the model architecture.\nLoRA’s application to other key components of the model.\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-\nto full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable \nparameters, LoRA drastically reduces the number compared to full fine-tuning.\nLoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-\nIntroduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses the chal-\nallows developers to fine-tune models on relatively small, widely available GPUs. The core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit \nof updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-\nrank matrices (adapters) to specific layers of the model.\ntraining, while the original model weights remain unchanged.\nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory \nFor example, for a 7B model, QLoRA reduces peak memory usage from 14 GB \nDuring fine-tuning, the memory savings increase \nthe cost of increased training time, with QLoRA being about 30% slower than LoRA.\nmodel performance, QLoRA shows only minor differences compared to LoRA.\nif training speed is crucial and sufficient memory is available, LoRA might be the preferred choice.",
      "keywords": [
        "model",
        "Fine-Tuning",
        "LoRA",
        "SFT",
        "capital of France",
        "Memory",
        "instruction",
        "France",
        "parameter",
        "techniques",
        "base model",
        "full fine-tuning",
        "end",
        "instruction dataset",
        "training"
      ],
      "concepts": [
        "models",
        "memory",
        "instructions",
        "instruct",
        "tuning",
        "parameters",
        "layers",
        "training",
        "weights",
        "techniques"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 28,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fine",
          "fine tuning",
          "tuning",
          "lora",
          "model"
        ],
        "semantic": [],
        "merged": [
          "fine",
          "fine tuning",
          "tuning",
          "lora",
          "model"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3559654079735848,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038889+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 245-252)",
      "start_page": 245,
      "end_page": 252,
      "summary": "project, available hardware, and the need to balance memory usage, training speed, and model \nTraining parameters\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly \nspecific task and model.\nThe learning rate scheduler adjusts the learning rate throughout the training process.\nlater stages to fine-tune the model more precisely.\nThe specific values and decay schedule depend on your model \nand dataset, but a common approach is to use a warmup period (e.g., 5% of total steps) where the \nmodel converges.\nThe batch size determines the number of samples processed before the model’s weights are up-\nTypical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, \ntraining speed, as they provide a better approximation of the true gradient of the entire dataset.\nFor instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while \nupdate to the model’s parameters.\nmodels or limited GPU memory.\nbut your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps \nand then update the model as if you had processed all 32 samples at once.\nchoosing the number of steps, consider the trade-off between training speed and memory usage.\nMore accumulation steps allow for larger effective batch sizes but increase the time required for \nmulation steps, your effective batch size would be 4 * 2 * 4 = 32 samples.\nThe maximum sequence length determines the longest input the model can process.\nThis parameter directly impacts batch size and memory usage; a batch \nbatch size with a max length of 512 would only contain 6,144 tokens.\nthis parameter with your GPU capabilities and the nature of your training data to optimize per-\nPacking maximizes the utilization of each training batch.\ning to tokens from different samples within the same packed sequence.\npasses through the entire training dataset.\nFor LLM fine-tuning, the typical range is 1 to 10 epochs, \ntask complexity, dataset size, and model architecture.\nMore epochs allow the model to refine its \nFor example, a large model fine-\ntuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger \ntraining and implement early stopping if the model’s performance plateaus or degrades.\nOptimizers adjust the model’s parameters to minimize the loss function.\nmemory (but it doesn’t improve training speed).\nweight decay regularization, often leading to better training stability and model performance.\nIn situations involving extremely large models or limited GPU memory, paged \nmodel to learn simpler, more generalizable features.\nfor the model to capture important patterns in the data.\nmodel architecture and dataset, so it’s generally a good practice to experiment with different \nGradient checkpointing is a technique that reduces memory consumption during training by stor-\nLet’s now fine-tune an open-source model on our custom dataset.\nThere are many efficient open-weight models we can leverage for task or domain-specific use \nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and \nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, \nensure that the model has the necessary capabilities to perform well on the intended \nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta.\nThere are specialized tools and libraries to fine-tune models.\ntraining (2-5x) and reduce memory use (up to 80% less memory).\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nLet’s now load the model to fine-tune and its corresponding tokenizer.\nWe’ll use LoRA in this example because of faster training and higher quality, but you can \nmodel, tokenizer = FastLanguageModel.from_pretrained(\nNow that the model is loaded, we can define our LoRA configuration.\nmodel,\nbecause the model might not correctly learn the chat template.\nthe 100,000 samples of this dataset, we will specify we only want 10,000 in the train split.\ntence (EOS) token at the end of each message to ensure that the model learns to output \ndataset = dataset.map(format_samples, batched=True, remove_\n8. Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for val-\ndataset = dataset.train_test_split(test_size=0.05)\nThe model is now ready to be trained.\nfor our training.\nIn addition, we provide the model, tokenizer, LoRA configuration, and \nWe train \nthis model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for \nmodel=model,\ntrain_dataset=dataset[\"train\"],\nper_device_train_batch_size=2,",
      "keywords": [
        "model",
        "Batch size",
        "Learning rate",
        "training",
        "Batch",
        "effective batch size",
        "dataset",
        "memory",
        "size",
        "Learning",
        "samples",
        "GPU memory",
        "tokens",
        "rate",
        "GPU"
      ],
      "concepts": [
        "model",
        "training",
        "dataset",
        "important",
        "memory",
        "performance",
        "performing",
        "tokens",
        "batched",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 36,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 35,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.524,
          "base_score": 0.524,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.493,
          "base_score": 0.493,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 43,
          "title": "",
          "score": 0.489,
          "base_score": 0.489,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "memory",
          "batch",
          "dataset",
          "training"
        ],
        "semantic": [],
        "merged": [
          "model",
          "memory",
          "batch",
          "dataset",
          "training"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3214544156257272,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038937+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 253-265)",
      "start_page": 253,
      "end_page": 265,
      "summary": "Training this model on our concatenated dataset can take a few hours.\nThis forces the model to answer the instruction instead of completing it.\nHere is the answer provided by our model:\nTraining loss: It measures how well the model is performing on the task it’s being trained \na well-fitted model typically shows both training and validation losses decreasing and \nis expected to exist as the model will always perform slightly better on the training data.\namined the instruction data pipeline and how to create high-quality datasets, from curation \na Llama 3.1 8 B model on our custom instruction dataset.\nIn the next chapter, we will use preference alignment techniques to create a new version of Twin-\nWe will generate a new dataset with chosen and rejected answers that will help us \nFine-Tuning with Preference \never, SFT struggles to capture the nuances of human preferences and the long tail of potential \nadvanced techniques for aligning AI systems with human preferences, grouped under the um-\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI \nIn this chapter, we will talk about the type of data that is required by preference alignment algo-\nWe will build our own dataset to modify the writing style of our model, making \nalign the model trained in Chapter 5.\nUnderstanding preference datasets\nHow to create our own preference dataset\nFine-Tuning with Preference Alignment\nBy the end of this chapter, you will be able to create your own preference datasets and align \nUnderstanding preference datasets\nThe principles for creating high-quality preference datasets are the same as those discussed in \nChapter 5 for instruction datasets.\ndatasets: data generation and evaluation.\nPreference data\nPreference datasets lack the standardization of instruction datasets due to varying data require-\nPreference data comprises a collection of responses \nto a given instruction, ranked by humans or language models.\npaired with one preferred answer and one rejected answer.\ngenerate the preferred response rather than the rejected one.\nIn preference datasets, the rejected response is as important as the chosen one.\nrejected response, the dataset would be a simple instruction set.\nto use preference datasets in many contexts.\nHere is a list of examples where preference datasets \nA preference dataset \nallows the model to learn these nuanced aspects by comparing better and worse responses.\nSimple SFT might not capture the subtleties of what makes one response preferable over \nPreference datasets can help the model learn to dis-\nBy using preference datasets, models can learn to generate \nsummaries that are technically correct but less preferable to human readers.\nPreference datasets \nFine-Tuning with Preference Alignment\nPreference datasets can capture human \nPreference \ndatasets can help models learn to produce translations that native speakers prefer, even \nIn all these scenarios, preference datasets enable a more refined training approach.\nsubjective quality assessments and human preferences that extend beyond simple correctness or \nThis method can produce models that generate output that is not only \ntechnically accurate but also better aligned with human judgment and preferences in complex, \nMost preference datasets follow a structure similar to that shown in Table 6.1, with columns for \nan instruction, a preferred answer, and a rejected answer.\nAs with instruction datasets, the required sample count depends on model size \nThis requires preference datasets with millions of samples.\ninvolving multiple rounds of preference alignment, and extensive use of synthetic data.\nTask-specific alignment focuses on improving model performance for a particular function, such \noften be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending \nAn example of an application that requires few samples is instructing the model to state that it \nanswers are responses where the model correctly states that it was trained by you.\nWhen creating preference datasets, data generation and evaluation are closely linked.\ncreate answers and then rate them to make the final dataset.\nGenerating preferences\nBefore making new preference data, it’s good to look at relevant open-source datasets.\nfewer of these compared to instruction datasets, but you can find high-quality preference data-\nWell-known preference datasets include the Anthropic HH-RLHF dataset, which has human \npreferences for helpful and harmless AI responses, and the OpenAI Summarize from Human \nDPO datasets can be created using various methods, each with its own trade-offs between quality, \nHuman-generated, human-evaluated datasets: This method involves hiring people to \napproach can capture nuanced human preferences and is ideal for complex tasks, it’s \nHuman-generated, LLM-evaluated datasets: This method can be useful if you have \npotentially missing nuanced preferences during the LLM evaluation stage.\nFine-Tuning with Preference Alignment\nLLM-generated, human-evaluated datasets: This method offers a good balance between \nLLMs generate multiple responses to prompts, and humans rank \nThis approach is often preferred because humans are generally better at \nresponses while still capturing human preferences effectively.\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both gener-\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of \nto scale, which is why large datasets benefit from LLM evaluation.\nFor instance, it is possible to use a high-quality model to generate preferred \noutputs and a lower-quality or intentionally flawed model to produce less preferred alternatives.\nThis creates a clear distinction in the preference dataset, allowing more effective training of AI \nAnother approach is to compare model-generated outputs with human-written responses, which \ncan provide insights into how well the model aligns with actual human preferences and highlight \nThe data generation is consistent between instruction and preference datasets.\ncapture the varied nature of human preferences.\nalign with human preferences.\nUsing multiple LLMs to generate samples can be better than using just one model.\nopen-source datasets like argilla/Capybara-Preferences, combining GPT-4 with open-weight \nmodels.\nEvaluating preferences\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation\nlines to the LLM, and using the model to select preferred and rejected responses.\nLLM evaluation depends directly on the model’s performance and the provided guidelines.\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or \nFine-Tuning with Preference Alignment\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for ",
      "keywords": [
        "preference datasets",
        "preference",
        "datasets",
        "model",
        "preference alignment",
        "human preferences",
        "LLM",
        "instruction datasets",
        "instruction",
        "human",
        "data",
        "responses",
        "answer",
        "LLM evaluation",
        "Training"
      ],
      "concepts": [
        "model",
        "dataset",
        "preferences",
        "preferred",
        "preferable",
        "prefer",
        "human",
        "data",
        "instruction",
        "instructions"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.751,
          "base_score": 0.601,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "preference",
          "datasets",
          "preference datasets",
          "human",
          "preferences"
        ],
        "semantic": [],
        "merged": [
          "preference",
          "datasets",
          "preference datasets",
          "human",
          "preferences"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3199732309098382,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.038985+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 266-274)",
      "start_page": 266,
      "end_page": 274,
      "summary": "Length bias: Similar to humans, LLM judges often show a preference for longer answers, \nTo mitigate these biases and enhance the quality of preference datasets, several solutions can \nIn the next section, we will create our own preference dataset.\nprocess to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\nCreating our own preference dataset\nIn this section, we will create a preference dataset where the chosen answers are extracts from \nthe text, while rejected answers are generated by the model.\ncode created in Chapter 5, which was designed to generate instruction datasets.\nAs seen in the previous section, preference and instruction datasets rely on the same principles.\nInstead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2).\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\nWe are now ready to implement the preference data generation pipeline:\nclass is designed to handle triples of instructions, generated answers (rejected), and ex-\nanswer'], triple['extracted_answer'])\nfor triple in data['preference_triples']]\nThe load_articles_from_json, clean_text, and extract_substrings functions remain \ndef load_articles_from_json(file_path: str) -> Dataset:\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nfor article in dataset[\"content\"]:\nThe generate_preference_triples function replaces the original generate_instruction_\ntype of instructions we’re interested in, how to extract answers from articles, and how \ndef generate_preference_triples(extract: str, client: OpenAI) -> \ninstruction-answer triples.\n2. A generated answer that attempts to answer the instruction based \nin the extracted answer.\n\"generated_answer\": \"...\",\n\"extracted_answer\": \"...\"\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode.\nThe JSON answers are \ngenerates instruction-answer triples based on the given context.\nEach triple should include an instruction, a generated answer, and \nan extracted answer from the context.\ndef filter_short_answers(dataset: Dataset, min_length: int = 100) -> \nDataset:\nreturn dataset.filter(is_long_enough)\ndef filter_answer_format(dataset: Dataset) -> Dataset:\nreturn dataset.filter(is_valid_format)\nThe create_preference_dataset function replaces the original create_instruction_\ndataset function.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_\nexecutor.submit(generate_preference_triples, extract, \ninstructions, generated_answers, extracted_answers = \n\"rejected\": list(generated_answers),\n\"chosen\": list(extracted_answers)\nraw_dataset = load_articles_from_json(\"cleaned_documents.json\")\nCreate preference dataset\ndataset = create_preference_dataset(raw_dataset, client)\nprint(\"Preference dataset:\")\ndataset = filter_short_answers(dataset)\nFilter answers based on format\ndataset = filter_answer_format(dataset)\nreturn dataset\nThe create_preference_dataset() function generated 2,970 samples.\nTo produce this dataset, we iterated many times over the prompt to generate the data.\nprocess to generate your own preference datasets.",
      "keywords": [
        "dataset",
        "preference",
        "preference dataset",
        "answers",
        "data",
        "Preference Alignment",
        "triples",
        "json",
        "str",
        "text",
        "item",
        "LLM judges",
        "preference data",
        "LLM",
        "extract"
      ],
      "concepts": [
        "dataset",
        "answer",
        "triples",
        "models",
        "functions",
        "function",
        "instructions",
        "returns",
        "preferences",
        "extracts"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 28,
          "title": "",
          "score": 0.751,
          "base_score": 0.601,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 23,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "dataset",
          "preference",
          "answers",
          "preference dataset",
          "dataset dataset"
        ],
        "semantic": [],
        "merged": [
          "dataset",
          "preference",
          "answers",
          "preference dataset",
          "dataset dataset"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3610994970814,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039037+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 275-282)",
      "start_page": 275,
      "end_page": 282,
      "summary": "(RL) with human input to align models with human preferences and values.\nThe origins of RLHF can be traced back to the field of preference-based reinforcement learning\nin 2017 demonstrated the effectiveness of learning reward models from \nhuman preferences and using them to train RL agents.\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a \nreward model from human feedback.\nare used to train a reward model, often using a Bradley-Terry model or similar approaches \nPolicy optimization: With the learned reward model, standard RL algorithms can be \npredicted rewards from the learned model.\nevaluated by humans, leading to refinements in the reward model.\nA key innovation in RLHF is its approach to handling the high cost of human feedback.\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm \nHere, the reward model is used to \nscore the text that is generated by the trained model.\nto the model before training (frozen model).\nlenges due to its iterative nature and reliance on a separate reward model, which can be compu-\nModel is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\nDPO’s core innovation lies in its reformulation of the preference learning problem.\nwhich typically involves training a separate reward model and then using reinforcement learning \nalgorithms like PPO to fine-tune the language model, DPO takes a more direct approach.\nmathematical insight allows DPO to express the preference learning problem directly in terms of \nthe policy, eliminating the need for a separate reward model or complex reinforcement learning \nmodel to assign higher probability to preferred responses and lower probability to non-preferred \nerence model is directly controlled via a beta parameter between 0 and 1.\nThe reference model is \nignored when beta is equal to 0, which means that the trained model can be very different from \nwithout the need for sampling from the model during training or implementing complex RL \nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\nDPO has several advantages over traditional RLHF methods.\nBy eliminating the need for a separate reward model and RL algorithms, DPO \nwith adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated.\nsince we’re only training adapters, the trained model is not modified.\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods.\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically \nThis enables a virtuous cycle where better models produce better training data, which \nIn this section, we will DPO fine-tune the TwinLlama-3 1-8B model we created in Chapter 5.\nthe best hyperparameters, we trained over 20 models and compared their outputs on a set of \nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nimport of DPOConfig and DPOTrainer from TRL, which are specific to DPO training.\nThis step loads our fine-tuned model from Chapter 5.\nthe following, we will perform LoRA DPO fine-tuning for increased speed and quality.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nLet’s now prepare the model for PEFT with the LoRA configuration.\nmodel,\nWe load the llmtwin-dpo dataset (training split), which contains our prompts, chosen, \ndataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")\nThe model and data are now ready, so we can start fine-tuning.\nare a few new parameters, like ref_model and beta.\nwe don’t directly train the model but instead the adapters.\neter controls the importance of the reference model.\ndue to the fact that the trained model used formal language with lower values.\nmodel=model,\nref_model=None,",
      "keywords": [
        "model",
        "reward model",
        "DPO",
        "RLHF",
        "reward",
        "Preference Alignment",
        "separate reward model",
        "human preferences",
        "Preference",
        "Learning",
        "Human",
        "learning reward models",
        "RLHF methods",
        "Human Feedback",
        "preference learning"
      ],
      "concepts": [
        "model",
        "training",
        "learning",
        "preference",
        "prefer",
        "preferred",
        "human",
        "approach",
        "approaches",
        "importance"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 28,
          "title": "",
          "score": 0.536,
          "base_score": 0.386,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "reward",
          "reward model",
          "model",
          "dpo",
          "rlhf"
        ],
        "semantic": [],
        "merged": [
          "reward",
          "reward model",
          "model",
          "dpo",
          "rlhf"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21642319134382515,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039076+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 283-291)",
      "start_page": 283,
      "end_page": 291,
      "summary": "Once the model is trained, we can run it for a quick sanity check.\nIt prepares the model for inference and generates a response to a prompt.\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\nThe trained DPO model returns the following response:\nof pre-trained language models by utilizing labeled data.\ntechnique involves taking a pre-trained model and refining it on \nproviding the model with relevant data and guidance, it can learn to \nWe can compare it with the answer provided by the SFT model:\nSupervised fine-tuning is a method used to enhance a language model \nThis process is designed to align the model's \nThe DPO model provides an answer that is both more accurate and closer to the desired \nIt correctly identifies pre-training language models as source models for \nFinally, the last step consists of saving the trained model locally and pushing it to the \nWe have trained and exported our DPO model.\nOver time, we expect the model to choose the chosen answers and reject the rejected \nA well-trained model’s margin will quickly increase and then plateau.\nAccuracies: This metric represents the percentage of times the model correctly identifies \ndicates that the preference dataset might be too easy for the model.\ncess, involving a reference model.\nmodel, you can experiment with different ranks, beta parameters, learning rates, and number of \nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models \nwords in the text generated by different models (SFT and DPO) with our ground-truth dataset.\nIn this example, we expect the SFT model to output a lot of words that are overrepresented in \nThe distribution output by our DPO model should be a lot closer \nfine-tune our TwinLlama-3.1-8B model from Chapter 5.\ninstructions for training the model, as well as highlighting key differences from SFT.\nmodel is available on the Hugging Face Hub. In the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges \nthe concept of using larger models to evaluate smaller ones (LLM-as-a-judge).\n“Direct Preference Optimization: Your Language Model is Secretly a \nReward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n“Deep reinforcement learning from human preferences.” arXiv preprint \n“Training language models to follow instructions with human feedback.” \nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models.\nModel evaluation\nmodels and RAG systems using different techniques.\nModel evaluation\nIn model evaluation, the objective is to assess the capabilities of a single model without any \nsure that the fine-tuning process actually improved the model.\nML and LLM evaluation to understand the main differences between these two fields.\nthen explore benchmarks for general-purpose, domain-specific, and task-specific models.\nComparing ML and LLM evaluation\nML evaluation is centered on assessing the performance of models designed for tasks like pre-\nhow well a model understands and generates language, ML evaluation is more concerned with \nhow accurately and efficiently a model can process structured data to produce specific outcomes.\nThis difference comes from the nature of the tasks these models handle.\nML models are gener-\nIn particular, we can see three key differences in how these models work, which impact the \nNumerical metrics: Evaluating ML models typically involves measuring objective per-\nselecting and transforming relevant data features before training the model.\nInterpretability: With ML models, it is easier to interpret why a model made certain pre-\nduring the generation process can give insights into the model’s decision-making process.",
      "keywords": [
        "model",
        "evaluation",
        "LLM evaluation",
        "DPO model",
        "LLM",
        "Preference",
        "Preference Alignment",
        "DPO",
        "LLMs",
        "Model evaluation",
        "SFT",
        "SFT model",
        "language models",
        "human",
        "learning"
      ],
      "concepts": [
        "model",
        "evaluation",
        "evaluate",
        "evaluating",
        "evaluations",
        "preference",
        "llm",
        "metrics",
        "trained",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 28,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "evaluation",
          "models",
          "dpo",
          "dpo model"
        ],
        "semantic": [],
        "merged": [
          "model",
          "evaluation",
          "models",
          "dpo",
          "dpo model"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3210010581451652,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039130+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 292-299)",
      "start_page": 292,
      "end_page": 299,
      "summary": "While evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific \nGeneral-purpose LLM evaluations\nGeneral-purpose evaluations refer to metrics dedicated to base and general-purpose fine-tuned \nmodels.\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after \nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model.\npre-training evaluations:\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from \nEvaluating LLMs\nARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science \nMany of these datasets are also used to evaluate general-purpose fine-tuned models.\ncase, we focus on the difference in a given score between the base and the fine-tuned model.\nexample, bad fine-tuning can degrade the knowledge of the model, measured by MMLU.\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks.\nHere, we use the term “fine-tuned model” to designate a model that has been trained with su-\nconnected to the ability of fine-tuned models to understand and answer questions.\nIFEval (instruction following): Assesses a model’s ability to follow instructions with \nAlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is \nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their \nFor example, if you want to fine-tune a model, you want the best base model \nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good \nway to compare different instruct models.\nple, public benchmarks can be gamed by training models on test data or samples that are very \nmultiple evaluations provide a similar answer, you can raise your confidence level about the real \nDomain-specific LLM evaluations\nDomain-specific LLMs don’t have the same scope as general-purpose models.\ncommon applications like a language-specific model or a code model, it is recommended to \nsearch for relevant evaluations and even benchmark suites.\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging \nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\nEvaluating LLMs\nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main \nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world \nThe evaluation focuses on specific \nincluding many general-purpose evaluations.\nthese benchmarks use machine translation, it is better to rely on human-translated evaluations \nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics.\nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language \nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs \nBoth general-purpose and domain-specific evaluations are designed with three main principles.\nTask-specific LLM evaluations\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, \nthey cannot provide insights into how well these models work for a given task.\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation data-\nto evaluate using traditional ML metrics.\nEvaluating LLMs\nby the model.\nThis benchmark can be inspired by general-purpose and domain-specific evaluation \nA common and successful pattern is the use of multiple-choice question answering.\nThere are two main ways of evaluating models with this scheme—text generation and log-like-\nFor example, the model generates a letter (A, B, C, or \nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabil-\nmodels about a particular task, and even expand it to specific domains.\nChapter 5 can be used to evaluate the quality of the answers.\nIt is recommended to use large models for evaluation and to iteratively refine your prompt.\nEvaluating LLMs\nYou are an evaluator who assesses the quality of an answer to an \nPlease provide your evaluation as follows:\n##Evaluation##\n##Evaluation##\nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation",
      "keywords": [
        "model",
        "evaluations",
        "answer",
        "LLMs",
        "benchmarks",
        "fine-tuned models",
        "Leaderboard",
        "Evaluates",
        "LLM evaluations",
        "Evaluates models",
        "general-purpose",
        "MMLU",
        "questions",
        "general-purpose fine-tuned models",
        "instruction"
      ],
      "concepts": [
        "evaluating",
        "evaluation",
        "evaluate",
        "evaluator",
        "answer",
        "answering",
        "benchmark",
        "model",
        "task",
        "instruction"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 26,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 24,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 31,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 28,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 23,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluations",
          "general purpose",
          "purpose",
          "general",
          "evaluates"
        ],
        "semantic": [],
        "merged": [
          "evaluations",
          "general purpose",
          "purpose",
          "general",
          "evaluates"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31390012167594344,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039194+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 300-308)",
      "start_page": 300,
      "end_page": 308,
      "summary": "combine LLM evaluations with other metrics, use multiple judges, and carefully design prompts \nOnce a model has been properly evaluated and works as intended, it might be included within a \nRAG evaluation\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation \nrequires a more comprehensive approach that considers both the model’s generative abilities \nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing \nThe evaluation of RAG systems goes beyond assessing a standalone LLM.\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accura-\nEvaluating LLMs\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected \nIn this section, we will cover two methods to evaluate how well RAG models incorporate external \nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to \nprovide developers with a comprehensive set of tools for RAG evaluation and optimization.\nThis approach ensures a comprehensive evaluation of different \nAdditionally, Ragas can generate conversational samples that simulate chat-based question-and-\nFigure 7.1: Overview of the Ragas evaluation framework\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to \nFaithfulness: This metric measures the factual consistency of the generated answer against \nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the \nIt uses an innovative approach where an LLM is prompted to generate \nContext precision: This metric evaluates whether all the ground-truth relevant items \nEvaluating LLMs\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed \nto evaluate RAG systems.\ntion with fine-tuned classifiers to assess various aspects of RAG performance, including context \nThe ARES framework operates in three main stages: synthetic data generation, classifier training, \nand RAG evaluation.\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world sce-\nfrom the previous stage), test set for evaluation, label columns, and model choice.\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the \nRAG model’s performance.\nUsers provide evaluation datasets, few-shot examples for guiding the \nARES supports various evaluation metrics \nHTML, images, and so on), enabling comprehensive evaluation across different RAG system \nuation and dataset generation.\nmetrics can be combined with ARES’s highly configurable evaluation process and classifier-based \nWhile Ragas may offer more nuanced evaluations based on LLM capabilities, ARES \nprovides consistent and potentially faster evaluations once its classifiers are trained.\nthem offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas \nEvaluating TwinLlama-3.1-8B\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and \nEvaluating LLMs\nIt will take both the instruction and the answer as inputs, and score it on \nIn our evaluation framework, we will use the test split of our instruction dataset to get test in-\nWe will feed them to our models and generate answers.\nThese answers will then be \nevaluated by our judge LLM (GPT-4o-mini), based on a prompt that specifies our criteria.\nGenerating answers\nThe first step consists of efficiently generating answers for each instruction in our test set.\ndition to our two models, we will also use meta-llama/Meta-Llama-3.1-8B-Instruct, the official \nWe import the relevant libraries, including vLLM for fast generation.\n2. We define a function called generate_answers that will process our dataset and generate \ndef generate_answers(model_id, dataset_name):\noutputs = llm.generate(dataset[\"prompt\"], sampling_params)\nThen, we run our generate_answers()\nEvaluating LLMs\ngenerate_answers(model_id, \"mlabonne/llmtwin\")\nNow that we have the answer generation, we can move on to the evaluation process.\nEvaluating answers\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge.\nHere, we will score every generated answer from every model in \n2. We then define the evaluate_answer() function.\nprompt, which sets up the context for evaluating answers based on accuracy and style:\ndef evaluate_answer(\nPlease evaluate the \nquality of a given answer to an instruction based on two criteria:\nthe answer?",
      "keywords": [
        "RAG",
        "RAG evaluation",
        "RAG systems",
        "evaluation",
        "model",
        "answer",
        "dataset",
        "Ragas",
        "LLM",
        "evaluation framework",
        "information",
        "evaluate RAG systems",
        "RAG pipeline",
        "LLMs",
        "Evaluating LLMs"
      ],
      "concepts": [
        "evaluated",
        "evaluation",
        "evaluate",
        "generative",
        "generate",
        "generation",
        "model",
        "answers",
        "rag",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.763,
          "base_score": 0.613,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "rag",
          "rag evaluation",
          "ragas",
          "ares"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "rag",
          "rag evaluation",
          "ragas",
          "ares"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34218662254305504,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039245+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 309-317)",
      "start_page": 309,
      "end_page": 317,
      "summary": "forces that we are interested in answer evaluation based on accuracy and style:\nevaluates answers based on accuracy and style.\nwhy we create an evaluate_batch() function, which returns a list of parsed structured \n(i, evaluate_answer(instr, ans, client))\nWe can now orchestrate the previous code in the evaluate_answers() function.\nthe model ID, number of threads, and batch size as inputs.\ndef evaluate_answers(model_id: str, num_threads: int = 10, batch_\n8. We create batches of instruction-answer pairs from our dataset.\ndataset[\"answers\"][i:i+batch_size])))\nWe perform parallel evaluation of batches of instruction-answer pairs using multiple \nWe use parallel processing to evaluate multiple batches simultaneously, speed-\nevaluate_batch().\nevaluations = [None] * len(dataset)\nfutures = [executor.submit(evaluate_batch, batch, start_\nfor index, evaluation in future.result():\nWe create a new column with the result of the evaluation process.\nthe raw JSON output of the judge model, including scores and explanations:\nif 'evaluation' in dataset.column_names:\ndataset = dataset.remove_columns(['evaluation'])\ndataset = dataset.add_column(\"evaluation\", evaluations)\nand style scores that should have been generated.\nstyle_scores = []\nfor evaluation in dataset['evaluation']:\nWe add two new columns to store the accuracy and style scores for further analysis:\ndataset = dataset.add_column('accuracy', accuracy_scores)\ndataset = dataset.add_column('style', style_scores)\nLet’s push the final dataset with generated answers, evaluations, and scores to the Hug-\ndataset.push_to_hub(f\"mlabonne/{model_id.split('/')\nWe can now call the evaluate_answers() function with the three models we selected:\nevaluate_answers(model_id)\nThere are three elements we can review at the end of this evaluation process: model answers, \nexplanations from evaluations, and scores.\ninstruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\nDiscuss the concept of algorithm bias and its implications.\nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nIf an algorithm is not properly vetted for bias, it may lead \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nTherefore, it is essential to address algorithmic bias proactively to \nmachine learning models that can affect their performance, accuracy, and fairness.\nThis bias \nof algorithm bias is its potential to perpetuate and exacerbate existing social inequalities.\nAlgorithm bias \nFor example, a medical diagnosis algorithm that is biased towards certain demographics \nIn education, an algorithm that is biased towards certain learning \nTo mitigate the effects of algorithm bias, it is essential to address \nbiases in machine learning models and take proactive steps to mitigate their effects.\nthe case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples.\nthe writing style but not the actual content of the answer.\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\nThe answer accurately defines algorithm bias \nto biased algorithms.\nThe answer accurately defines algorithm bias \ncept of algorithm bias without becoming \nThe answer accurately defines algorithm bias \nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and ac-\nHowever, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-\n8B-Instruct, with a score of 2.\nperfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of \nalgorithm bias without becoming overly formal.”\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Style: 2.12\nLlama-3.1-8B-Instruct - Accuracy: 2.62\nLlama-3.1-8B-Instruct - Style: 1.86\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct \nIn this chapter, we explored LLM evaluation with models and RAG systems.\nFinally, we evaluated TwinLlama-3.1-8B with \n“Instruction-Following Evaluation for Large Language Models.” arXiv \ntions in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n“RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv ",
      "keywords": [
        "algorithm bias",
        "evaluation",
        "style",
        "accuracy",
        "bias",
        "algorithm",
        "dataset",
        "model",
        "score",
        "batch",
        "answers",
        "arXiv",
        "evaluate",
        "defines algorithm bias",
        "Evaluating LLMs"
      ],
      "concepts": [
        "evaluating",
        "evaluations",
        "evaluate",
        "model",
        "dataset",
        "score",
        "instruction",
        "instruct",
        "answering",
        "style"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 29,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.53,
          "base_score": 0.53,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 22,
          "title": "",
          "score": 0.49,
          "base_score": 0.49,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 2,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 53,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bias",
          "algorithm bias",
          "algorithm",
          "style",
          "dataset"
        ],
        "semantic": [],
        "merged": [
          "bias",
          "algorithm bias",
          "algorithm",
          "style",
          "dataset"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34390384660702344,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:17.039295+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 318-325)",
      "start_page": 318,
      "end_page": 325,
      "summary": "of tokens generated per second (throughput), and minimizing the memory footprint of LLMs. Indeed, naive deployment approaches lead to poor hardware utilization and underwhelming \nModel optimization strategies\nModel parallelism\nModel quantization\nModel optimization strategies\nThe decoder-only architecture is designed for text-generation tasks.\nFigure 8.1 – Inference process with decoder-only models.\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\nTokenizing the input prompt and passing it through an embedding layer and positional \nGenerating output tokens sequentially, one at a time, using the computed keys and values.\nmultiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs. The real challenge is that the token generation in Step 3 is inherently sequential – to generate \nthe next token, you need to have generated all previous tokens.\na (static) KV cache, continuous batching, speculative decoding, and optimized attention mech-\nWe saw that LLMs generate text token by token, which is slow because each new prediction \nthe model needs the context of tokens 1 through 99.\nInstead of recalculating these pairs for each new token, the model retrieves them \nWhen a new token is generated, only the key and value for that single token need to be computed \nthe model.\nThe size of the KV cache scales with the number of tokens (𝑛𝑛𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 ) and several model dimensions, \nTo configure a model to use a static KV cache with the transformers library, follow these steps:\nWe import the tokenizer and the model we want to optimize:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nTo implement the static cache, we change the cache implementation in the model’s gen-\nmodel.generation_config.cache_implementation = \"static\"\nNow that our KV cache is static, we can compile the model using torch.compile:\nLet’s use the generate() method to get the model’s output and decode it with batch_\noutputs = model.generate(**inputs, do_sample=True, temperature=0.7, \nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\nHowever, decoder-only models pose a particular challenge due to the high variability in input \nAnother powerful optimization technique is speculative decoding, also called assisted generation.\nThe key insight is that even with continuous batching, the token-by-token generation process \nFeed these speculative completions into the full model to validate which predictions \nmatch what the large model would have generated.\nThe result is that, if the small model approximates the large model well, multiple tokens can be \nThe degree of speedup depends on the quality of the small model’s predictions – a 90% match \nIt is crucial that both models use the same tokenizer.\nIf this is not the case, the tokens generated \nNote that, if you have enough VRAM, you can use much larger models like 14B, 32B, \nWe load the tokenizer and both models:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nWe can now use model.generate() with the argument assistant_model to enable specu-\noutputs = model.generate(**inputs, do_sample=True, assistant_\nmodel=draft_model, temperature=0.7, max_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\nprompt_lookup_num_tokens parameter in model.generate():\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4)\nBy combining the static KV cache with torch.compile, implementing continuous batching, and ",
      "keywords": [
        "model",
        "Inference Optimization",
        "Inference",
        "tokens",
        "cache",
        "Optimization",
        "speculative decoding",
        "Text Generation Inference",
        "input",
        "generation",
        "decoding",
        "Model optimization strategies",
        "Model optimization",
        "large model",
        "speculative"
      ],
      "concepts": [
        "models",
        "token",
        "generation",
        "generate",
        "generating",
        "inference",
        "optimization",
        "optimized",
        "optimize",
        "optimal"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 36,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 27,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 43,
          "title": "",
          "score": 0.516,
          "base_score": 0.516,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 37,
          "title": "",
          "score": 0.452,
          "base_score": 0.302,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "token",
          "cache",
          "tokens",
          "tokenizer"
        ],
        "semantic": [],
        "merged": [
          "model",
          "token",
          "cache",
          "tokens",
          "tokenizer"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3144484849764886,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039344+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 326-334)",
      "start_page": 326,
      "end_page": 334,
      "summary": "these speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-\ntunes both the speculation heads and the large model.\nblock-based approach naturally supports memory sharing across multiple output sequences \nblocks reduce redundant computations and memory usage, cutting the memory overhead by \nbetween the GPU’s main memory and its processing units.\nthe backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\nimplementation parameter when loading a model.\nThe techniques presented in this section focused on improving the model’s efficiency in processing \nmultiple GPUs. Model parallelism\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across \nmultiple GPUs. This enables the training and inference of models too large to fit on a single device, \nThere are three main approaches to model parallelism, each involving splitting the model weights \nand computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism.\nData parallelism\nData parallelism (DP) is the simplest type of model parallelism.\nmodel and distributing these replicas across different GPUs (see Figure 8.4).\nFigure 8.4 – Illustration of data parallelism with four GPUs\nbetween GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient.\nthat this technique only works when the model is small enough to fit into a single GPU, leaving \nFor larger models or when memory is \nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for \nPipeline parallelism\nmultiple GPUs. Unlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism parti-\ntions the model’s layers across different GPUs. This approach allows each GPU to handle a specific \nportion of the model, thereby reducing the memory burden on individual GPUs. Figure 8.5 – Illustration of pipeline parallelism with four GPUs\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four \nThe first 25% of the model’s layers might \nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory \nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\nFigure 8.6 shows an example of pipeline parallelism with micro-batching.\nGPU 0 waits for the other GPUs to finish their respective forward computations before starting \nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, Deep-\npipeline parallelism.\n(TP) is another popular technique to distribute the computation of LLM layers across multiple \nIn contrast to pipeline parallelism, TP splits the weight matrices found in individual \nThis enables simultaneous computations, significantly reducing memory bottlenecks and \nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset \nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism \nmodel to process large sequences more effectively.\nGPUs can compute these layers on different slices of the input sequence, avoiding replication of \nThis technique is limited to a few specific layers, but it can provide additional memory \nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined.\n8.8 illustrates how a given model can be split according to each approach:\nFigure 8.8 – Illustration of the different model parallelism techniques\nPipeline parallelism provides \nideal if the primary constraint fits the model in the GPU memory.\nparamount, then prioritizing tensor parallelism and accepting a larger memory footprint may \nIn practice, a model may be split depth-wise into a few pipeline stages, \non reducing the precision of the model’s weights and activations.\ninference of LLMs. In addition to these benefits, larger models with over 30 billion parameters can outperform \na pre-trained model are directly converted to a lower precision format without any retraining.\nperforms quantization during the training or fine-tuning stage, allowing the model to adapt to \nConversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a ",
      "keywords": [
        "pipeline parallelism",
        "model",
        "GPU",
        "parallelism",
        "GPUs",
        "memory",
        "pipeline",
        "Model parallelism",
        "tensor parallelism",
        "multiple GPUs",
        "data parallelism",
        "training",
        "data",
        "Inference",
        "layers"
      ],
      "concepts": [
        "memory",
        "model",
        "parallel",
        "parallelism",
        "processing",
        "processes",
        "process",
        "layers",
        "quantization",
        "computation"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 27,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 35,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 43,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.476,
          "base_score": 0.476,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "parallelism",
          "memory",
          "pipeline parallelism",
          "gpus",
          "model"
        ],
        "semantic": [],
        "merged": [
          "parallelism",
          "memory",
          "pipeline parallelism",
          "gpus",
          "model"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3133855102679926,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039396+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 335-343)",
      "start_page": 335,
      "end_page": 343,
      "summary": "(8-bit integers), can be employed for quantization, further reducing the memory footprint.\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point \nquantization\nAbsmax quantization maps the original weights 𝐗𝐗 to the range [-127, 127] by dividing them by the \nIf we take the same example with a weight of 0.1, we get a scale of \nThe weight of 0.1 would be quantized to round(41.13 ⋅0.1 −5) = −1 , \nIn Python, zero-point quantization can be implemented as follows:\ncan significantly impact the quantization process, leading to reduced precision for other values.\nmixed-precision quantization scheme, where outlier features are processed using FP16, while the \nModels can be directly loaded in 8-bit precision with the transformer library, using LLM.int8(), \nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nTo load a model in NF4 (4-bit precision), you can use the load_in_4bit\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nQuantization with GGUF and llama.cpp\nsigned to perform inference with various LLMs. It is the most popular quantization technique, \nwith many quantized models available on the Hugging Face Hub. Compared to other libraries that rely on hardware-specific closed-source libraries like CUDA, \nmodel loading.\nIQ2_XXS/XS/S/M and Q2_K: 2-bit precision – generally low quality but IQ2 can be usable \nfor large models\nIQ3_XXS/XS/S/M and Q3_K_S/M/L: 3-bit precision – low quality but usable for large models\nIQ4_XS/NL and Q4_K_S/M, Q4_0/1: 4-bit precision – good quality and usable for most \nmodels\nQ5_K_S/M and Q5_0/1: 5-bit precision – high quality\nQ8_0: 8-bit precision – highest quality\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds \nand quantizing them based on the largest weight value in the block (w = q × block_scale ).\nvalues are also quantized in higher precision with 6 bits (w = q × block_scale(6bit) +  block_min(6bit) ).\nFinally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#.\nHere is a practical example of how to quantize a model in the GGUF format.\nFirst, we convert the model into FP16.\nfor every GGUF quantization type.\ncpp and are compatible with different models:\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile \nWe select a format (here, Q4_K_M) and start the quantization.\nqtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\nOnce it’s done, your quantized model is ready.\nrepo_id = f\"{username}/{MODEL_NAME}-GGUF\",\nrepo_type=\"model\",\nrepo_id=f\"{username}/{MODEL_NAME}-GGUF\",\nGGUF models can be used with backends such as llama-cpp-python and frameworks like Lang-\nThis is useful if you want to integrate a quantized model into a broader system.\nalso directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, \nQuantization with GPTQ and EXL2\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two \nquantization formats dedicated to GPUs. This makes them both faster than llama.cpp during \nIt optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) \neach linear layer, prioritizing more important weights with higher bit quantization.\nmodels to run on a single 24 GB GPU with 2.55-bit precision.\nEXL2 models.\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2.\n2. We download the model to quantize by cloning its repo from the Hugging Face Hub:\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\nQuantize the model at a given precision (for example, 4.5):\n-i {MODEL_NAME} \\\nThe quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\nGPTQ models are also supported in TensorRT-LLM.\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face \nHub. Other quantization techniques\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2.\nbriefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization \nQuantization).\nAn interesting trend is the quantization of models into 1- or 2-bit precision.\nlike EXL2, allow extreme quantization, the quality of the models often suffers significantly.\nter explored various optimization techniques, including optimized generation methods, model \nparallelism, and weight quantization.\nWeight quantization, with formats ",
      "keywords": [
        "model",
        "quantization",
        "Hugging Face Hub",
        "GGUF",
        "GPTQ",
        "Hugging Face",
        "scale",
        "Face Hub",
        "precision",
        "Inference Optimization",
        "GGUF models",
        "Inference",
        "weight",
        "weight quantization",
        "quantization techniques"
      ],
      "concepts": [
        "quantization",
        "model",
        "precision",
        "precise",
        "weights",
        "bit",
        "bits",
        "inference",
        "value",
        "libraries"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 35,
          "title": "",
          "score": 0.452,
          "base_score": 0.302,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 27,
          "title": "",
          "score": 0.443,
          "base_score": 0.293,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 36,
          "title": "",
          "score": 0.387,
          "base_score": 0.237,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "quantization",
          "gguf",
          "precision",
          "bit",
          "bit precision"
        ],
        "semantic": [],
        "merged": [
          "quantization",
          "gguf",
          "precision",
          "bit",
          "bit precision"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.1659019548008025,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039443+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 344-356)",
      "start_page": 344,
      "end_page": 356,
      "summary": "Zheng, C.H. Yu, J.E. Gonzalez, H.\nY. Leviathan, M.\nZheng, C.H. Yu, J.E. Gonzalez, H.\nRAG Inference Pipeline\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline \nilar pattern by implementing a retrieval module to query the vector DB.\nHowever, we will write an inference service that inputs the user query and context, builds \nthe prompt, and calls the LLM to generate the answer.\nPython modules, one for retrieval and one for calling the LLM using the user’s input and context \nRAG Inference Pipeline\ninto the advanced RAG retrieval module implementation.\n(and not when calling the LLM), you write most of the RAG inference code.\nUnderstanding the LLM Twin’s RAG inference pipeline\nImplementing the LLM Twin’s RAG inference pipeline\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, \naugment a prompt using the retrieved context, and call an LLM to generate the final answer.\nUnderstanding the LLM Twin’s RAG inference \nBefore implementing the RAG inference pipeline, we want to discuss its software architecture \ninference pipeline starts with the input query, retrieves the context using the retrieval module \n(based on the query), and calls the LLM SageMaker service to generate the final answer.\ntime, the retrieval module is called on demand, within the inference pipeline, on every user request.\nRAG Inference Pipeline\nThe input of the RAG retrieval module is the user’s query, based on which we \nQuery expansion: We expand the initial query to generate multiple queries that reflect \nThus, instead of one query, \nwe will use xN queries.\nSelf-querying: We extract useful metadata from the original query, such as the author’s \ninating redundant data points from the query vector space (making the search more \nFiltered vector search: We embed each query and perform a similarity search to find \nexpanded queries.\nextracted from the self-query step as query filters.\nrelevant angles based on the original query’s different facets.\nchunk based on the relevance and importance relative to the initial user query.\nwhere 1 means the result is entirely relevant to the query.\nBuild the prompt and call the LLM: We map the final list of the most relevant K chunks \nthe retrieved context, and the user’s query.\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced \nPre-retrieval step: Query expansion and self-querying\nRetrieval step: Filtered vector search\nRAG Inference Pipeline\nsteps such as query expansion and self-querying.\nfrom llm_engineering.domain.queries import Query\ndef generate(self, query: Query, *args, **kwargs) -> Any:\nUltimately, we must understand how we modeled the Query domain entity to wrap the user’s \nNext, we define the Query entity class, which inherits from the VectorBaseDocument object-vector \nThus, each query can easily be saved or retrieved \nclass Query(VectorBaseDocument):\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a \ncontent: A string containing input query.\nreturn Query(content=query.strip(\"\\n \"))\ndef replace_content(self, new_content: str) -> \"Query\":\nreturn Query(\nFollowing the Query class, \nclass EmbeddedQuery(Query):\nRAG Inference Pipeline\nThe EmbeddedQuery class extends Query by adding the embedding field.\npipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.\nAdvanced RAG pre-retrieval optimizations: query expansion \nand self-querying\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion \nand self-querying.\nfor query expansion and move to implementing self-querying.\nquery within the query expansion step and to extract the necessary metadata within the self-que-\nQuery expansion\nThe problem in a typical retrieval step is that you query your vector DB using a single vector rep-\nnuances of your query, the retrieved context may not be relevant.\nLLM to generate multiple queries based on your initial question, you create various perspectives \nThese expanded queries, when embedded, target other \nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt \nto guide the LLM in generating these alternative queries.\nThus, after implementing query ex-\npansion, instead of having only one query to search relevant context, you will have xN queries, \nnumber of queries you generate to ensure the retrieval step meets your application requirements.\nfor query expansion:\nfrom llm_engineering.domain.queries import Query\nNext, we define the QueryExpansion class, which generates expanded query versions.\nHandbook/blob/main/llm_engineering/application/rag/query_expanison.py:\ndef generate(self, query: Query, expand_to_n: int) -> list[Query]:\nreturn [query for _ in range(expand_to_n)]\nRAG Inference Pipeline\nlist containing copies of the original query to simulate expansion without actually calling the \nquery_expansion_template = QueryExpansionTemplate()\nprompt = query_expansion_template.create_template(expand_to_n - 1)\nto_n - 1 new queries (excluding the original).\nWe split the result using the separator defined in the template to get individual queries.\nwith a list containing the original query, we append each expanded query after stripping any \nquery expansion.\nThis class defines a prompt instructing the language model to generate multiple versions of ",
      "keywords": [
        "RAG Inference Pipeline",
        "RAG Inference",
        "query",
        "RAG",
        "inference pipeline",
        "advanced RAG",
        "LLM",
        "Query expansion",
        "inference",
        "LLM Twin",
        "RAG retrieval module",
        "advanced RAG techniques",
        "LLM Twin model",
        "pipeline",
        "original query"
      ],
      "concepts": [
        "query",
        "queries",
        "rag",
        "classes",
        "llm",
        "inference",
        "importance",
        "importing",
        "model",
        "pipeline"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.953,
          "base_score": 0.803,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.833,
          "base_score": 0.683,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 33,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.767,
          "base_score": 0.617,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "query",
          "expansion",
          "query expansion",
          "rag inference",
          "rag"
        ],
        "semantic": [],
        "merged": [
          "query",
          "expansion",
          "query expansion",
          "rag inference",
          "rag"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4227441293582212,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039500+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 357-366)",
      "start_page": 357,
      "end_page": 366,
      "summary": "generated queries.\nllm_engineering.application.rag.query_expansion command:\nquery = Query.from_str(\"Write an article about the best types of advanced \nmethod was successful in providing more details and different perspectives of the initial query, \nNow, let’s move to the next pre-retrieval optimization method: self-querying.\nSelf-querying\nThe problem when embedding your query into a vector space is that you cannot guarantee that \nBy embedding the query prompt alone, you can never be sure that the tags \nThe solution is to use self-querying to extract the tags or other critical metadata within the query \nSelf-querying uses an LLM to extract various \nSelf-queries work hand-in-hand with filtered vector searches, which we will explain in the next \nfrom llm_engineering.domain.queries import Query\nHandbook/blob/main/llm_engineering/application/rag/self_query.py:\ndef generate(self, query: Query) -> Query:\nreturn query\nmodel instance (similar to the query expansion implementation).\nand the model into a chain and invoke it with the user’s query.\nresponse = chain.invoke({\"question\": query})\nreturn query\nIf the response is \"none\", it means no user name was found in the query, so we return the origi-\nFinally, we update the query object with the extracted author information and return it:\nquery.author_id = user.id\nquery.author_full_name = user.full_name\nreturn query\nThe updated query now contains the author_id and author_full_name values, which can be \nLet’s look at the SelfQueryTemplate class, which defines the prompt to extract user information:\nIn the SelfQueryTemplate class, we define a prompt instructing the AI model to extract the user \nBy implementing self-querying, we ensure that critical metadata required for our use case is ex-\ncode using the python -m llm_engineering.application.rag.self_query CLI command:\nself_query = SelfQuery()\nquery = self_query.generate(query)\nlogger.info(f\"Extracted author_id: {query.author_id}\")\nlogger.info(f\"Extracted author_full_name: {query.author_full_name}\")\nNow that we understand how self-querying works, let’s explore how it can be used together with \nfiltered vector search within the retrieval optimization step.\nAdvanced RAG retrieval optimization: filtered vector search\nVector search is pivotal in retrieving relevant information based on semantic similarity.\nAs the metadata used within the filtered vector search is often part of the user’s input, we have \nto extract it before querying the vector DB.\nThat’s precisely what we did during the self-query \nThus, as we processed the query within the self-query step, it went into the pre-retrieval \noptimization category, whereas when the filtered vector search optimized the query, it went into \nquery_vector=query_embedding,\nquery_filter= Filter(\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations \nAdvanced RAG post-retrieval optimization: reranking\nbetween the query and chunk embeddings.\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance \nN represents the number of searches after query expansion, while K is the number of chunks \nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which \nilarity between the query and each chunk more accurately than initial retrieval methods based \nReranking works well when combined with query expansion.\nunderstand how reranking works without query expansion:\nof each chunk relative to the query.\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\nfrom llm_engineering.domain.queries import Query\nNext, we define the Reranker class, which is responsible for reranking the retrieved documents \nbased on their relevance to the query:\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an \nrelevance of each document chunk with respect to the query.\ndef generate(self, query: Query, chunks: list[EmbeddedChunk], keep_\nquery_doc_tuples = [(query.content, chunk.content) for chunk in \nscores = self._model(query_doc_tuples)\nscored_query_doc_tuples = list(zip(scores, chunks, strict=False))\nreranked_documents = scored_query_doc_tuples[:keep_top_k]\nThe generate() method takes a query, a list of chunks (document segments), and the number \nCreates pairs of the query content and each chunk’s content\nUses the cross-encoder model to score each pair, assessing how well the chunk matches \nthe query\nExtracts the chunks from the tuples and returns them as the reranked documents",
      "keywords": [
        "query",
        "RAG Inference Pipeline",
        "model",
        "RAG",
        "vector search",
        "user",
        "advanced RAG methods",
        "chunks",
        "advanced RAG",
        "RAG Inference",
        "vector",
        "RAG methods",
        "filtered vector search",
        "query expansion",
        "search"
      ],
      "concepts": [
        "queries",
        "query",
        "model",
        "importing",
        "chunks",
        "retrieve",
        "retrieved",
        "search",
        "searches",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 40,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 53,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "query",
          "vector",
          "self",
          "chunks",
          "search"
        ],
        "semantic": [],
        "merged": [
          "query",
          "vector",
          "self",
          "chunks",
          "search"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2766955991248545,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039543+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 367-374)",
      "start_page": 367,
      "end_page": 374,
      "summary": "how to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\nlook at how to build the final prompt using the retrieved context and user query.\nry expansion, self-querying, reranking, and filtered vector search.\nFigure 9.2: Search logic of the RAG retrieval module\nglues together all the steps required to search results similar to the user’s query.\nthe extracted author details from the self-query step are used within the filtered vector search.\nof N searches), we want to retrieve a maximum of K results.\nThe retrieved list is ≤ K (and not equal to K) when a particular data \nFigure 9.3: Processing the results flow of the RAG retrieval module\ntheir reranking score, and pick the most relevant top K chunks we will use as context for RAG.\nIn the search() method, we convert the user’s input string into a query object.\nSelfQuery instance to extract the author_id and author_full_name from the query:\nquery_model = self._metadata_extractor.generate(query_model)\n\"Successfully extracted the author_id from the query.\",\nauthor_id=query_model.author_id,\nn_generated_queries = self._query_expander.generate(query_model, \n\"Successfully generated queries for search.\",\nWe then perform the search concurrently for all expanded queries using a thread pool.\nsearch_tasks = [executor.submit(self._search, _query_model, k) \nfor _query_model in n_generated_queries]\nAfter retrieving the documents, we rerank them based on their relevance to the original query \nk_documents = self.rerank(query, chunks=n_k_documents, keep_\nEmbeddedQuery, which includes the query’s embedding vector and any extracted metadata:\ndef _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\ndata_category_odm: type[EmbeddedChunk], embedded_query: \nif embedded_query.author_id:\nvalue=str(embedded_query.author_id),\nquery_filter = None\nWe used the same EmbeddingDispatcher to embed the query as in the RAG feature pipeline to \nsame embedding model at ingestion and query time, which is critical for the retrieval step.\nFor instance, if an author_id is present, we use it to filter the search results \npost_chunks = _search_data_category(EmbeddedPostChunk, embedded_\nquery)\nembedded_query)\ncategory(EmbeddedRepositoryChunk, embedded_query)\nFinally, the rerank() method takes the original query and the list of retrieved documents to \ndef rerank(self, query: str | Query, chunks: list[EmbeddedChunk], \nreranked_documents = self._reranker.generate(query=query, \nLeveraging the ContextRetriever class, we can retrieve context from any query with only a few \nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"\ndocuments = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nPaul Iusztin Implement 4 advanced RAG retrieval techniques to optimize \nIntegrate the RAG retrieval module into a \nchunk, embed, and load your data to a vector DBretrieval query your vector ",
      "keywords": [
        "RAG Inference Pipeline",
        "RAG Inference",
        "query",
        "RAG",
        "RAG retrieval module",
        "search",
        "Inference Pipeline",
        "documents",
        "RAG retrieval",
        "advanced RAG",
        "LLM",
        "retrieval module",
        "module RAG Inference",
        "chunks",
        "data"
      ],
      "concepts": [
        "query",
        "queries",
        "rag",
        "search",
        "searches",
        "document",
        "documents",
        "retrieved",
        "retrieve",
        "llm"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 17,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 39,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "query",
          "search",
          "rag",
          "author_id",
          "embedded_query"
        ],
        "semantic": [],
        "merged": [
          "query",
          "search",
          "rag",
          "author_id",
          "embedded_query"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3084886439204958,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039593+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 375-383)",
      "start_page": 375,
      "end_page": 383,
      "summary": "RAG Inference Pipeline\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-\nTo fully implement the RAG flow, we still have to build the prompt using the context from the \nretrieval model and call the LLM to generate the answer.\nIt takes in a user’s query and an optional context, sets up the language model end-\ndef call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\nFor now, what is essential to know is that we use this function to call our fine-tuned LLM.\nusing the user query and retrieved context:\ning relevant documents based on the query, mapping the documents to the context that will be \ndocuments = retriever.search(query, k=3)\nanswer = call_llm_service(query, context)\nindependently without context or use the retrieval module as a query engine on top of your vector \nIn the next chapter, we will see the rag() function in action after we deploy our fine-tuned \nstores all the user prompts and generated answers in memory.\nthe LLM, along with the new user input and context, we also pass the conversation history from \nRAG Inference Pipeline\nupdate the summary on every user prompt and generate an answer.\nAs for each search, we send three queries to the vector DB, one for each data category.\nsecond improvement is to add a router between the query and the search.\nmulti-category classifier that predicts the data categories we must retrieve for that specific query.\nexample, if the user wants to write a theoretical paragraph about RAG for an article, then most \nthe article class, which we can use to decide what collection we must query.\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG \nuse case, we can adapt the router algorithm to optimize the retrieval step.\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector \nsearch (based on embeddings) with a keyword search algorithm, such as BM25.\nSearch algorithms \nused BM25 (or similar methods) to find similar items in a DB before vector search algorithms \nBy merging the methods, hybrid search retrieves results that match the exact \nterms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and \nvector search and BM25 algorithms.\nEach algorithm retrieves a set of relevant documents \nOne last improvement we can make to our RAG system is to use multi-index vector structures \nInstead of using the embeddings of a single field to do the vector search for a particular \nRAG Inference Pipeline\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or \nSuperlinked, we defined a multi-index on the content and platform for our article collection in \nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, \ndata into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\nThis chapter taught us how to build an advanced RAG inference pipeline.\nods we used within the retrieval module, such as query expansion, self-querying, filtered vector \nglues all the advanced RAG components under a single interface, making searching for relevant \nretrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve \nas our RAG inference pipeline.\nlearn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, \nsuperlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-\nRAG Inference Pipeline\n4 Advanced RAG Algorithms You Must Know | Decoding \nhttps://medium.com/decodingml/the-4-advanced-rag-algorithms-you-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\nMulti-attribute search with vector embeddings | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/multi-attribute-semantic-search\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-\nVisualize your RAG Data—Evaluate your Retrieval-Aug-\nvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications.\nhttps://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-",
      "keywords": [
        "RAG Inference Pipeline",
        "RAG Inference",
        "RAG",
        "LLM",
        "Inference Pipeline",
        "search",
        "Inference",
        "query",
        "advanced RAG",
        "vector search",
        "context",
        "advanced RAG inference",
        "fine-tuned LLM",
        "Pipeline",
        "vector"
      ],
      "concepts": [
        "rag",
        "retrieved",
        "retrieval",
        "retrieve",
        "context",
        "query",
        "queries",
        "search",
        "searches",
        "article"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.953,
          "base_score": 0.803,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 53,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 40,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 39,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rag",
          "search",
          "vector",
          "query",
          "retrieval"
        ],
        "semantic": [],
        "merged": [
          "rag",
          "search",
          "vector",
          "query",
          "retrieval"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3900811571725966,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039648+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 384-392)",
      "start_page": 384,
      "end_page": 392,
      "summary": "Deploying the inference pipeline for the large language model (LLM) Twin application is a critical \nthe deployment processes, we must leverage MLOps best practices, such as model registries that \nthree deployment types we can choose from: online real-time inference, asynchronous inference, \nan architectural decision, such as latency, throughput, data, and infrastructure.\nWhen it comes to deploying ML models, the first step is to understand the four requirements \npresent in every ML application: throughput, latency, data, and infrastructure.\nFor example, should your model deployment be optimized for low latency or \nThroughput refers to the number of inference requests a system can process in a given period.\nML models when you expect to process many requests.\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters \nwith multiple high-end GPUs.Latency is the time it takes for a system to process a single inference \nis the average response time from when a user sends a request, and the service provides a result \nMeanwhile, the throughput is the average number of requests the API processes and \nLow-latency systems require optimized and often more costly infrastructure, such as faster pro-\nA lower latency translates to higher throughput when the service processes multiple queries in \nFor example, if the service takes 100 ms to process requests, this translates to \na throughput of 10 requests per second.\nIf the latency reaches 10 ms per request, the throughput \nprocess 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests \nIf you process 60 requests in 200 ms, the latency is 200 ms, while the throughput \nThus, even when batching requests at serving time, it’s essential \nof the processed data.\nData is the foundation of the inference process.\nThe type and size of the data directly impact latency and throughput, as more complex or exten-\nthat supports the deployment and operation of the ML models.\nnecessary resources for deploying, scaling, and maintaining ML models.\nFor high throughput, the systems require scalable infrastructure to manage large data \nvolumes and high request rates, possibly through parallel processing, distributed systems, \nInfrastructure must be optimized to reduce processing time to achieve low latency, such \nlow latency while batching your requests, you often have to sacrifice high throughput \nAs you process fewer requests per second, it results in idle computing, which \nincreases the overall cost of processing a request.\nIt is crucial to design infrastructure to meet specific data requirements.\nLet’s move on to the three deployment architectures we can leverage to serve our models.\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and \nFor example, serving your model in \nOtherwise, you have to serve your model in real-time, which is more infrastruc-\nFigure 10.1: The three fundamental architectures of inference deployment types\nIn real-time inference, we have a simple architecture based on a server that can be accessed \nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which \nimmediately processes the request and returns the result in the same response.\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML \nrecommendation engines in platforms like TikTok. The simplicity of real-time inference, with its direct client-server interaction, makes it an attrac-\ntive option for applications that require immediate responses, like chatbots or real-time recom-\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the \nrequest and places it in a queue for processing.\nUnlike real-time inference, the client doesn’t wait \nInstead, the ML service processes the request asynchronously.\nquires a robust infrastructure that queues the messages to be processed by the ML service later on.\nIt doesn’t have to process all the requests \nduces higher latency, making it less suitable for time-sensitive applications.\nFor example, this is a robust design where you don’t care too much about the latency of the infer-\nBut suppose you carefully design the autoscaling system to process the requests from \nBatch transform is about processing large volumes of data simultaneously, either on a schedule \nIn a batch transform architecture, the ML service pulls data from a storage \nUnlike the asynchronous inference architecture, a batch transform design is optimized for high \nthroughput with permissive latency requirements.\nthis approach can significantly reduce costs, as processing data in big batches is the most eco-\nThe client pulls the results directly from data storage, decoupling its interaction with the ML \nTaking this approach, the client never has to wait for the ML service to process its input, \nsee the data storage, where the results are stored as a large cache, from where the client can take \nHowever, it’s unsuitable for real-time applications due to its high latency and \nTo conclude, we examined the three most common architectures for serving ML models.\nstarted with online real-time inference, which serves clients when they request a prediction.\nences in architecture were mainly based on the interaction between the client and the ML service, ",
      "keywords": [
        "Inference Pipeline Deployment",
        "LLM Twin",
        "Inference",
        "Deployment",
        "requests",
        "LLM Twin service",
        "Inference Pipeline",
        "data",
        "latency",
        "Pipeline Deployment",
        "throughput",
        "inference deployment types",
        "deployment types",
        "LLM",
        "real-time inference"
      ],
      "concepts": [
        "data",
        "latency",
        "inference",
        "infer",
        "model",
        "deployment",
        "processes",
        "process",
        "processed",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 43,
          "title": "",
          "score": 0.653,
          "base_score": 0.653,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "latency",
          "throughput",
          "requests",
          "inference",
          "request"
        ],
        "semantic": [],
        "merged": [
          "latency",
          "throughput",
          "requests",
          "inference",
          "request"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3446523922455852,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039700+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 393-400)",
      "start_page": 393,
      "end_page": 400,
      "summary": "Inference Pipeline Deployment\nBut another aspect to consider is the architecture of the ML service itself, which can be imple-\nFigure 10.2: Monolithic versus microservices architecture in model serving\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-pro-\ncessing steps) are bundled into a single service in a monolithic architecture.\nOne key challenge of a monolithic architecture is the difficulty of scaling components independent-\nThe LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound.\nresource use, with the GPU being idle when the business logic is executed and vice versa.\nMicroservices architecture\nA microservices architecture breaks down the inference pipeline into separate, independent ser-\nvices—typically splitting the LLM service and the business logic into distinct components.\nFor instance, since the LLM service might require more GPU resources \nThis optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus \nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service \nInference Pipeline Deployment\nFigure 10.3: Scaling microservices independently based on compute requirements\nNote that the proposed design for decoupling the ML model and business logic into two services \nThe choice between monolithic and microservices architectures for serving ML models largely \nif the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, \nmore complex systems where different components have varying scaling needs or require distinct \nas GPU-intensive LLM services.\nthe system for keeping these machines busy all the time or quickly scaling down when the GPU \nML and business logic into two different Python modules that don’t interact with each other.\nInference Pipeline Deployment\nif you start with a monolith and down the line you want to move to a microservices architecture, \nExploring the LLM Twin’s inference pipeline \nstrategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to \nthe selection of an online real-time inference deployment architecture.\nOn the monolith versus microservice aspect, we will split the ML service between a REST API \nserver containing the business logic and an LLM microservice optimized for running the given \nAs the LLM requires a powerful machine to run the inference, and we can further optimize \nwith the microservice architecture.\nThe LLM microservice is strictly optimized for the RAG generation component.\nIn summary, our approach involves implementing an online real-time ML service using a micro-\nservice architecture, which effectively splits the LLM and business logic into two distinct services.\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\nInference Pipeline Deployment\nA fine-tuned LLM generated by the training pipeline, which is pulled from our model \nThe prompt is sent to the LLM microservice through an HTTP request.\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast.\nThe LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s \nenable real-time predictions from deployed models.\nYou can deploy multiple models to an endpoint, \nOnce deployed, models are easily accessible via the \nTogether, these components create a robust infrastructure for deploying and managing ML models \ndeploy the inference pipeline.",
      "keywords": [
        "Inference Pipeline Deployment",
        "Inference Pipeline",
        "LLM",
        "LLM microservice",
        "business logic",
        "Inference",
        "Pipeline",
        "model",
        "GPU",
        "architecture",
        "Pipeline Deployment",
        "business",
        "microservices",
        "service",
        "LLM Twin"
      ],
      "concepts": [
        "microservices",
        "requires",
        "required",
        "requirements",
        "services",
        "inference",
        "scaled",
        "scale",
        "llm",
        "deployment"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.653,
          "base_score": 0.653,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.574,
          "base_score": 0.574,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.533,
          "base_score": 0.533,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.527,
          "base_score": 0.527,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 35,
          "title": "",
          "score": 0.516,
          "base_score": 0.516,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "business",
          "business logic",
          "microservices",
          "microservice",
          "inference"
        ],
        "semantic": [],
        "merged": [
          "business",
          "business logic",
          "microservices",
          "microservice",
          "inference"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31064175687623674,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:17.039747+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 401-411)",
      "start_page": 401,
      "end_page": 411,
      "summary": "Inference Pipeline Deployment\nDeploying the LLM Twin service\nwe will deploy the LLM microservice using AWS SageMaker and the business microservice using \nDeploy our fined-tuned LLM Twin model to AWS SageMaker\nWrite an inference client to interact with the deployed model\nImplementing the LLM microservice using AWS SageMaker\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon \nSageMaker as an online real-time inference endpoint.\ninference container, known as the Hugging Face LLM DLC, to deploy our LLM.\nInference Pipeline Deployment\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, \nwill be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP \nWe will start by deploying the \nLLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\nConfiguring SageMaker roles\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and \nroles to access and deploy the SageMaker infrastructure.\nthe deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3.\ndeploy everything related to SageMaker to ensure we modify only the resources associated \nllm_engineering/infrastructure/aws/roles/create_sagemaker_role.py.\nWe will attach this role to the SageMaker deployment, \ndeployment.\nDeploying the LLM Twin model to AWS SageMaker\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which \nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command: poe \ndeploy-inference-endpoint.\nexcept for creating the SageMaker AWS IAMs we created and configured in the previous step.\nInference Pipeline Deployment\nmate the deployment process, starting with the create_endpoint() function.\ntest the CLI command and check the AWS console to see whether the deployment was successful.\nThe SageMaker deployment code is available at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\nFigure 10.5: AWS SageMaker deployment steps\nmain function that deploys the LLM Twin model to AWS SageMaker.\nfunction, which is later passed to the deployment strategy class, along with an instance of the \nresource manager and deployment service:\ndeployment_service = DeploymentService(resource_manager=resource_\nconfig=hugging_face_deploy_config,\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\nendpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\nthe deployment process.\nInference Pipeline Deployment\nNext, we implement the endpoint_config_exists method, checking whether a specific Sage-\nself.sagemaker_client.describe_endpoint_\nSageMaker endpoint:\nself.sagemaker_client.describe_endpoint(EndpointName=endpoint_\nwhich will interface with AWS SageMaker and an instance of the ResourceManager class we \nThe deploy() method is the heart of the DeploymentService class.\nentire process of deploying a model to a SageMaker endpoint.\ndef deploy(\nif self.resource_manager.endpoint_config_exists(endpoint_config_\nself.prepare_and_deploy_model(\nInference Pipeline Deployment\nlogger.info(f\"Successfully deployed/updated model to endpoint \nlogger.error(f\"Failed to deploy model to SageMaker: {e}\")\nThe deploy method begins by checking whether the endpoint configuration already exists using \ndeploy_model() method, which is responsible for the actual deployment of the model to the \nspecified SageMaker endpoint.\nThe prepare_and_deploy_model() method is a static method within the DeploymentService\nThis method is focused on setting up and deploying the Hugging Face model to SageMaker:\ndef prepare_and_deploy_model(\nhuggingface_model.deploy(\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from \nSageMaker designed to handle Hugging Face models.\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy \nThis deployment process involves specifying the type of instance used, the number of \ncludes optional resources for more complex deployments, such as the initial_instance_count\nThe class is initialized only with an instance of a deployment service, \ndef __init__(self, deployment_service):\nhow the Hugging Face model should be deployed to AWS SageMaker:\ndef deploy(\nInference Pipeline Deployment\nlogger.info(\"Starting deployment using Sagemaker Huggingface \nThe parameters passed into the method are crucial to the deployment process:\nrole_arn: The AWS IAM role that provides permissions for the SageMaker deployment.\nendpoint_name and endpoint_config_name: Names for the SageMaker endpoint and its \nresources: Optional resources dictionary used for multi-model endpoint deployments.\nendpoint_type: This can either be MODEL_BASED or INFERENCE_COMPONENT, determining \nThe method delegates the actual deployment process to the deployment_service.\nself.deployment_service.deploy(",
      "keywords": [
        "endpoint",
        "Inference Pipeline Deployment",
        "LLM Twin model",
        "AWS",
        "AWS SageMaker",
        "Hugging Face",
        "LLM",
        "Deployment",
        "LLM Twin",
        "Inference Pipeline",
        "SageMaker",
        "Hugging Face model",
        "Inference",
        "model",
        "config"
      ],
      "concepts": [
        "deployment",
        "deployments",
        "inference",
        "models",
        "endpoint",
        "setting",
        "roles",
        "method",
        "served",
        "serve"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.947,
          "base_score": 0.797,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 46,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 49,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sagemaker",
          "deployment",
          "aws",
          "deploy",
          "endpoint"
        ],
        "semantic": [],
        "merged": [
          "sagemaker",
          "deployment",
          "aws",
          "deploy",
          "endpoint"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3213194919458974,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039797+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 412-421)",
      "start_page": 412,
      "end_page": 421,
      "summary": "sources are leveraged when setting up multi-endpoint configurations that use multiple replicas \nInference Pipeline Deployment\nwhen the model is deployed to a SageMaker endpoint.\ndeploying the endpoint, we suggest modifying them and seeing how the performance of the LLM \nUltimately, let’s review the settings configuring the LLM engine.\npoetry poe deploy-inference-endpoint\nThat’s all you need to deploy an inference pipeline to AWS SageMaker.\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker \nin the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\nFigure 10.6: AWS SageMaker Inference endpoints example\nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-\nInference Pipeline Deployment\nFigure 10.7: AWS SageMaker twin inference endpoint example\nCalling the AWS SageMaker Inference endpoint\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service.\ninference endpoint through HTTP requests, and decode the results in a way the client can work \nAll the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/\ninference.\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\nSageMaker endpoint:\nclass LLMInferenceSagemakerEndpoint(Inference):\ninference_component_name: Optional[str] = None,\nself.inference_component_name = inference_component_name\nOne of the key features of the class is its ability to generate a default payload for inference requests.\ndef _default_payload(self) -> Dict[str, Any]:\n\"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n\"top_p\": settings.TOP_P_INFERENCE,\n\"temperature\": settings.TEMPERATURE_INFERENCE,\nfor inference.\nThe parameters section includes settings that influence the model’s behavior during \nthe application’s settings, ensuring consistency across different inference tasks.\nThe class allows customization of the payload through the set_payload() method, which enables \nthe user to modify the inputs and parameters before sending an inference request:\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = \nInference Pipeline Deployment\nAdditionally, it allows for modifying inference parameters if any are provided.\nUltimately, we leverage the inference() method to call the SageMaker endpoint with the cus-\ndef inference(self) -> Dict[str, Any]:\nif self.inference_component_name not in [\"None\", None]:\ninvoke_args[\"InferenceComponentName\"] = self.inference_\nlogger.exception(\"SageMaker inference failed.\")\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint.\nwe previously presented to send HTTP requests to the AWS SageMaker endpoint.\nThe llm parameter accepts any instance that implements the Inference interface, such \nas the LLMInferenceSagemakerEndpoint class, which is used to perform the inference.\nllm: Inference,\nsent to the LLM by formatting the prompt with the user’s query and context.\nthe model from generating repetitive text, and the temperature setting that controls the ran-\nOnce the payload and parameters are set, the method calls the inference function from \nInference Pipeline Deployment\nself.llm.set_payload(\n\"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n\"temperature\": settings.TEMPERATURE_INFERENCE,\nanswer = self.llm.inference()[0][\"generated_text\"]\npoetry run python -m llm_engineering.model.inference.test\npoetry poe test-sagemaker-endpoint\nmicroservice will send HTTP requests to the LLM microservice defined above and call the RAG \nclasses represent the request and response structure for the FastAPI endpoints:\nllm_service() function wraps the inference logic used to call the SageMaker LLM microservice:\ndef call_llm_service(query: str, context: str | None) -> str:\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\nanswer = InferenceExecutor(llm, query, context).execute()\nAlso, as the LLM inference logic is moved to a different microservice, the call_llm_service()\nanswer = call_llm_service(query, context)\nInference Pipeline Deployment\npoetry poe run-inference-ml-service",
      "keywords": [
        "AWS SageMaker Inference",
        "Inference",
        "Inference Pipeline Deployment",
        "LLM",
        "AWS SageMaker",
        "SageMaker Inference endpoint",
        "endpoint",
        "SageMaker Inference",
        "model",
        "Inference Pipeline",
        "Inference endpoint",
        "AWS",
        "SageMaker",
        "SageMaker endpoint",
        "RAG"
      ],
      "concepts": [
        "infer",
        "inference",
        "setting",
        "sets",
        "parameters",
        "llm",
        "endpoint",
        "classes",
        "query",
        "rag"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.947,
          "base_score": 0.797,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 46,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 49,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "inference",
          "sagemaker",
          "endpoint",
          "sagemaker inference",
          "aws sagemaker"
        ],
        "semantic": [],
        "merged": [
          "inference",
          "sagemaker",
          "endpoint",
          "sagemaker inference",
          "aws sagemaker"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30985016614453986,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039879+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 422-429)",
      "start_page": 422,
      "end_page": 429,
      "summary": "So far, the SageMaker LLM microservice has used a static number of replicas to serve our users, \nOnce you’re done testing your inference pipeline deployment, deleting all your AWS \nSageMaker resources used to deploy the LLM is essential.\nAs almost all AWS re-\nyou’re done testing your SageMaker infrastructure (or any AWS resource).\nwe have provided a script that deletes all the AWS SageMaker resources for you:\nFor example, let’s assume we requested four copies (replicas) with the following \nThe solution is to implement an autoscaling strategy that scales the number \nof replicas up and down dynamically based on various metrics, such as the number of requests.\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints \nscale in and out based on the number of requests.\nreplica so the server remains responsive to new user requests or even scales down to zero if the \nwe have to keep two replicas online, and when the number of requests spikes to 100 per second, \nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference \nSageMaker provides a feature called Application Auto Scaling that allows you to scale \nThe first step in enabling autoscaling for your resources is to register a scalable target with the \nApplication Auto Scaling feature AWS provides.\nresource you intend to scale, as well as setting the boundaries within which the scaling should \nFor instance, when working with SageMaker Inference components, you’ll define the following:\nResource ID: This serves as a unique identifier for the resource you want to scale, typically \nincluding the name of the SageMaker Inference component.\nScalable dimension: This specifies the resources to be scaled, such as the desired number \ning strategies, such as minimum and maximum limits of the number of replicas.\nBy registering a scalable target, you prepare your SageMaker Inference component for future \nOnce your scalable target is registered, the next step is defining how the scaling should occur.\nThis is where creating a scaling policy comes in.\nA scaling policy defines specific rules that trigger \nscaling events.\nIn the context of our SageMaker Inference component, the scalable policy might include the \nthe resource’s capacity to maintain a specific target value for a chosen metric.\nspecifying cooldown periods that control how quickly scaling actions can occur after \nThe scaling policy defines the rules for your scaling-in and scaling-out strategy.\ntarget value, it triggers actions to scale the number of inference component copies up or down, \nOnce defined, Application Auto Scaling creates and manages the necessary \nFor instance, consider an application running on SageMaker.\nGPU usage exceeds the target, the system scales out, adding resources to manage the increased \nConversely, when GPU usage drops below the target, the system scales in, reducing capacity \nOne significant advantage of setting up target tracking policies using Application Auto Scaling is \ndefine scaling adjustments manually.\nMinimum and maximum scaling limits\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish \nyour maximum and minimum scaling limits before creating your scaling policy.\nNext, configure the maximum value, which defines the upper limit of resources your model can \nscale up to.\nThus, you can scale up as much as your application needs within the \nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to \nof instances during scale-in requests and restricts the creation of new replicas during scale-out \nOnce you understand how to configure scaling policies for SageMaker, you can imme-\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMak-\nAWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-\nIf your scaling policy or cooldown period is too sensitive, you may \nfor instance, that you expect your system to support an average of 100 users per minute and scale \nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Infer-\nmicroservice deployed on AWS SageMaker.\nWe also reviewed a popular autoscaling strategy that scales in and out \nbased on a given set of metrics and saw how to implement it in AWS SageMaker.\nuser-in-aws\nhttps://www.oreilly.com/library/view/machine-learning-",
      "keywords": [
        "AWS",
        "inference pipeline deployment",
        "AWS SageMaker",
        "Application Auto Scaling",
        "AWS SageMaker resources",
        "Scaling",
        "SageMaker Inference",
        "AWS SageMaker Inference",
        "inference",
        "SageMaker",
        "AWS Elastic Container",
        "inference pipeline",
        "AWS EKS",
        "SageMaker Inference component",
        "number"
      ],
      "concepts": [
        "scales",
        "scaling",
        "infer",
        "resources",
        "policies",
        "policy",
        "deploy",
        "service",
        "endpoint",
        "define"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 49,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "scaling",
          "sagemaker",
          "aws",
          "sagemaker inference",
          "target"
        ],
        "semantic": [],
        "merged": [
          "scaling",
          "sagemaker",
          "aws",
          "sagemaker inference",
          "target"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3081495919860996,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039935+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 430-437)",
      "start_page": 430,
      "end_page": 437,
      "summary": "MLOps and LLMOps\nmodels (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to \nBut MLOps is not just about these components; it takes an ML \napplication to the next level by automating data collection, training, testing, and deployment.\nThus, the end goal of MLOps is to automate as much as possible and let users focus on the most \nadopt these pre-trained foundational models for their use cases, focusing on LLMOps problems \nMLOps and LLMOps\ndeployment (CI/CD) pipeline to test the integrity of our code and automate the deployment \nprocess, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline \nstarting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging \ninto LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as \nThe path to LLMOps: Understanding its roots in DevOps and MLOps\nDevOps and MLOps\nThen, we will move to MLOps to understand how \nThus, DevOps was born to automate the process of shipping software at scale.\ncifically, DevOps is used in software development, where you want to completely automate your \nSuperior quality and security: DevOps ensures swift software development while main-\nMLOps and LLMOps\nDeployment environments: To thoroughly test your code before shipping it to produc-\nVersion control: Used to track, manage, and version every change made to the source code.\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field \nMLOps\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML.\napplication, such as the data, model, and, finally, the code.\nMLOps and LLMOps\nthe code, modifications in the data, or adjustments to the model.\nFigure 11.2: Relationship between data, model, and code changes\nIn that case, you must train (or fine-tune) a new model, resulting \na few examples that can trigger a change in the data and indirectly in the model:\nAfter deploying the ML model, its performance might decay as time passes, so we need \ndata or re-label it, which generates a new set of models.\nSo, what is MLOps?\nthat makes data and models their first-class citizen while preserving the DevOps methodology.\nLike DevOps, MLOps originates from the idea that isolating ML model development from its \ndeployment process (ML operations) diminishes the system’s overall quality, transparency, and \nMLOps core components\ner on the MLOps core components now that we better understand the field.\ncontrol and CI/CD, MLOps revolves around:\nModel registry: A centralized repository for storing trained ML models (tools: Comet \nFeature store: Preprocessing and storing input data as features for both model training \nML metadata store: This store tracks information related to model training, such as model \nconfigurations, training data, testing data, and performance metrics.\nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools: ZenML, \nMLOps and LLMOps\nmanual processes to automated pipelines through CT and CI/CD.\nretraining and deployment of ML models in response to triggers such as new data, per-\nautomation ensures that our ML systems are robust, scalable, and adaptable to changing \nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, \nCode is tracked using tools like Git, models are \nversioned through model registries, and data versioning can be managed using solutions \nExperiment tracking: As training ML models is an iterative and experimental process \nTesting: MLOps suggests that along with testing your code, you should also test your \ndata and models through unit, integration, acceptance, regression, and stress tests.\nMonitoring: This stage is vital for detecting performance degradation in served ML models \nmodel metrics and detecting drifts, we can maintain the health of ML systems in produc-",
      "keywords": [
        "MLOps",
        "DevOps",
        "LLM Twin",
        "model",
        "data",
        "LLMOps",
        "code",
        "LLM",
        "software",
        "pipeline",
        "LLMs",
        "training",
        "Twin",
        "core",
        "application"
      ],
      "concepts": [
        "model",
        "data",
        "tool",
        "operations",
        "operational",
        "operate",
        "tested",
        "training",
        "version",
        "versions"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.947,
          "base_score": 0.797,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.844,
          "base_score": 0.694,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mlops",
          "devops",
          "llmops",
          "ml",
          "mlops llmops"
        ],
        "semantic": [],
        "merged": [
          "mlops",
          "devops",
          "llmops",
          "ml",
          "mlops llmops"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3567070311953181,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.039984+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 438-445)",
      "start_page": 438,
      "end_page": 445,
      "summary": "MLOps engineering\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description \nan MLOps engineer, you have a lot of work to do on the infrastructure side.\nseen in this section, an MLOps engineer still has to implement things such as experiment tracking, \nthese into the code and the MLOps engineer focus on making them work on their infrastructure.\nMLOps\ndata scientist/ML researcher, ML engineer, and MLOps engineer.\nMLOps and LLMOps\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of \nHowever, the MLOps engineer plays a pivotal role in this \nLLMOps encompasses the practices and processes essential for managing and running LLMs.\nassociated with LLMs. While MLOps addresses the principles and practices of managing various \nML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly \ncomplex training requirements, prompt management, and non-deterministic nature of generating \nWhen training LLMs from scratch, the data and model dimensions of an ML system grow sub-\nstantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns \nsive datasets required for training LLMs. It involves big data techniques for processing, \nThe massive size of LLMs directly impacts model training.\nWhen training an LLM from \noptimizing your processes and infrastructure to support data, model, or tensor parallelism.\nAt its core, LLMOps is MLOps at scale.\nIt uses the same MLOps principles but is applied to big data \nand huge models that require more computing power to train and run.\ntions now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or \nThus, for most LLM applications out there, your development steps will involve the selection of a \nfoundation model, which you further have to optimize by using prompt engineering, fine-tuning, \ndive into some popular components of LLMOps that can improve prompt engineering, fine-tun-\nMLOps and LLMOps\nyour LLM can produce harmful output or receive dangerous input, so you should monitor and \nyour system (model jailbreaking), and accepting violent or unethical prompts.\napplications don’t want to accept violent or unethical queries from users, such as asking \nOutput guardrails: At the output of an LLM response, you want to catch failed outputs \nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data \nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the \nthese tools, you usually want to track the user input, the prompt templates, the input variables, \nthe generated response, the number of tokens, and the latency.\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we \nThus, when it comes to tracking the latency of generating an answer, the final user experience \nTime per Output Token (TPOT): The time it takes to generate each output token\nMLOps and LLMOps\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting \nyour LLMs. Ultimately, you can compute metrics that validate your model’s performance for each input, \nhave multiple intermediate steps from the user query to the final general answer.\nand the final prompt sent to the model.\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated \nthe application can behave unexpectedly if the number of generated tokens suddenly fluctuates \nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong \nDeploying the LLM Twin’s pipelines to the cloud\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud.\nNote that the training and inference pipelines already work with AWS SageMaker.\nSageMaker training and inference components are more costly to run (which we \nMLOps and LLMOps\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, ",
      "keywords": [
        "LLM",
        "MLOps",
        "LLMs",
        "MLOps engineer",
        "training LLMs",
        "LLMOps",
        "data",
        "LLM Twin",
        "training",
        "output",
        "model",
        "LLM systems",
        "input",
        "prompt",
        "system"
      ],
      "concepts": [
        "data",
        "model",
        "prompt",
        "training",
        "output",
        "outputting",
        "tokens",
        "engineers",
        "step",
        "input"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.947,
          "base_score": 0.797,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.803,
          "base_score": 0.653,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mlops",
          "llmops",
          "engineer",
          "mlops engineer",
          "mlops llmops"
        ],
        "semantic": [],
        "merged": [
          "mlops",
          "llmops",
          "engineer",
          "mlops engineer",
          "mlops llmops"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33714745788231076,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040033+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 446-455)",
      "start_page": 446,
      "end_page": 455,
      "summary": "Build a Docker image that contains all the system dependencies, the project dependencies, \nPush the Docker image to ECR, where SageMaker can access it.\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS \nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2.\nBased on the pulled image, it creates a Docker container that executes the pipeline step.\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and \nWe will show you how to create and integrate a free MongoDB cluster into our projects.\nIn the left panel, go to Deployment | Database and click Build a Cluster.\nWhat AWS cloud region should I choose?\nTo test that your newly created MongoDB cluster works fine, we must connect to it from \nThe final step is to return to your project and open your .env file.\ncloud MongoDB cluster we just created.\nThus, to create a Qdrant cluster \nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\nIn the left panel, go to Clusters and click Create.\nClick Create and choose your twin cluster to create a new access token.\nGo back to the Clusters section of Qdrant and open your newly created twin cluster.\nwill have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5>\ncloud Qdrant cluster we just created.\nend data pipeline with the cloud version of MongoDB and Qdrant as follows:\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\nSetting up the ZenML cloud\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process.\nup a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, \nwe will bundle our code in a Docker image to run it in AWS SageMaker.\nLet’s start with setting up the ZenML cloud:\nML cloud tenant, return to the project and run the zenml connect command provided \nTo ensure everything works fine, run a random pipeline from your code.\nGo to the Pipelines section in the left panel of the ZenML dashboard.\nTo ship the code to AWS, you must create a ZenML stack.\nworking locally, ZenML offers a default stack that allows you to quickly develop your code and \ninfrastructure environments, such as local and AWS runs, which we will showcase in this section.\nWith that in mind, let’s create an AWS stack for our project.\nThen, choose AWS as your cloud provider.\nNow ZenML will create a set of IAM roles to give permissions to all the other components \nThen run poetry lock --no-update && poetry install\nleverages CloudFormation (an infrastructure as code, or IaC, tool) to create all the AWS \nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML \nAWS resources or to connect ZenML with your current infrastructure.\nand deploying Docker container images easy.\nCloudFormation is a service that allows you to model and set up your AWS re-\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker \nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing.\nThe last step is to find a way to take our code and run it on top of this infrastructure.\npopular solution is Docker, a tool that allows us to create an isolated environment (a container) \nthat contains everything we need to run our application, such as system dependencies, Python \nWe defined our Docker image at the project’s root in the Dockerfile.\nBefore digging into the code, if you want to build the Docker image your-\nself, ensure that you have Docker installed on your machine.\nby following the instructions provided here: https://docs.docker.com/engine/install.\nPoetry to be installed is specified, and a few environment variables are set to ensure that package \nNext, we install Google Chrome in the container.\nthe stable version of Google Chrome is installed.\nFollowing the Chrome installation, other essential system dependencies are installed.\nPoetry, the dependency management tool, is then installed using pip.\nAfter installation, Poetry \nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20\nWith the dependency files in place, the project’s dependencies are installed using Poetry.\ninstalled directly into the container’s Python environment.\nRUN poetry config virtualenvs.create false && \\\npoetry install --no-root --no-interaction --no-cache --without dev && \nIn the final step, the entire project directory from the host machine is copied into the container’s \nThis step ensures that all the application files are available within the container.\nOne important trick when writing a Dockerfile is to decouple your installation steps from copy-\nproject dependencies but mostly change your code, copying your project files in the last step makes \nIt allows the project to run smoothly in any environment that supports Docker.\nThe last step is to build the Docker image and push it to the ECR created by ZenML.\nDocker image from the root of the project, run the following:\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works \nThe tag of the newly created Docker image is llmtwin.\npoetry poe build-docker-image",
      "keywords": [
        "AWS",
        "Docker image",
        "Docker",
        "Qdrant",
        "ZenML",
        "ZenML cloud",
        "create",
        "step",
        "AWS infrastructure",
        "Qdrant cluster",
        "cloud",
        "run",
        "cluster",
        "MongoDB",
        "poetry"
      ],
      "concepts": [
        "mongodb",
        "zenml",
        "poetry",
        "creates",
        "created",
        "step",
        "install",
        "installations",
        "running",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 9,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 46,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "docker",
          "cluster",
          "cloud",
          "docker image",
          "zenml"
        ],
        "semantic": [],
        "merged": [
          "docker",
          "cluster",
          "cloud",
          "docker image",
          "zenml"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21229952589628442,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040075+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 456-464)",
      "start_page": 456,
      "end_page": 464,
      "summary": "Figure 11.7: AWS ECR example\ndocker tag llmtwin ${AWS_ECR_URL}:latest\ndocker push ${AWS_ECR_URL}:latest\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository.\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\nRun the pipelines on AWS\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps.\nzenml stack set aws-stack\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9.\nWe’ve configured the pipeline to always use the latest Docker image available in ECR.\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they \nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our \nRun the end-to-end-data-pipeline with the \npoetry poe run-end-to-end-data-pipeline\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run.\nthe ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10.\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\nTo find even more details about the runs, you can go to AWS SageMaker.\nTo run other pipelines, you have to update the settings.docker.parent_image\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\na ZenML pipeline on SageMaker\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline \nIf you want to run the pipelines locally again, use the following CLI command:\nprocess and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML.\nAs mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to \nLLM Twin’s CI/CD pipeline flow\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens.\nImplementing a CI pipeline ensures that new features follow the repository’s standards and \nFigure 11.14: CI/CD pipelines flow\nThe CD pipeline runs after the branch is merged.\ninto staging, the CD pipeline takes the code from the staging branch, builds a new Docker im-\nage, and pushes it to the AWS ECR Docker repository.\nWhen running future pipeline runs in \nthe staging environment, it will use the latest Docker image that was built by the CD pipeline.\nfurther manually test the new feature along with what is automatically tested in the CI pipeline.",
      "keywords": [
        "AWS ECR",
        "AWS",
        "AWS ECR Docker",
        "AWS ECR dashboard",
        "AWS ECR ZenML",
        "ECR",
        "ECR URL",
        "AWS ECR repository",
        "pipeline",
        "Docker image",
        "docker",
        "main AWS ECR",
        "LLM Twin",
        "ZenML",
        "ECR Docker repository"
      ],
      "concepts": [
        "zenml",
        "pipeline",
        "running",
        "run",
        "runs",
        "step",
        "process",
        "processing",
        "error",
        "latest"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 49,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 52,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ecr",
          "docker",
          "aws ecr",
          "aws",
          "zenml"
        ],
        "semantic": [],
        "merged": [
          "ecr",
          "docker",
          "aws ecr",
          "aws",
          "zenml"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23801793710937444,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040148+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 465-475)",
      "start_page": 465,
      "end_page": 475,
      "summary": "GitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their \nworkflows directly within a GitHub repository.\ncode directly from GitHub by defining workflows in YAML files.\nrepository’s .github/workflows directory.\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same \ncurrent GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\nmight trigger a workflow every time code is pushed to a specific branch.\nhow GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\nGitHub Actions CI YAML file\nThe YAML file sits under .github/workflows/ci.yaml.\nHence, the CI workflow will automatically run \nThe concurrency section ensures that only one instance of this workflow runs for a given reference \ntrue line ensures that if a new workflow run is triggered before the previous one finishes, the \nThe workflow defines two separate jobs: qa and test.\nThe first job, named QA, is responsible for quality assurance tasks like code checks and format-\nWithin the qa job, the first step is to check out the repository’s code using the \nThis step is necessary to ensure that the job has access to the code \nsteps in the job will run in the correct Python environment.\nThe workflow then installs Poetry using the abatilo/actions-poetry@v2 action, specifying the \nuses: abatilo/actions-poetry@v2\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the \nThe qa job then runs several quality checks on the code.\nrun: poetry poe gitleaks-check\nFollowing the gitleaks check, the workflow runs a linting process to enforce coding standards \nrun: poetry poe lint-check\nThe last step in the qa job is a format check, which ensures that the Python code is properly for-\nrun: poetry poe format-check\nThe second job defined in the workflow is the test job, which also runs on the latest version \nLike the qa job, it starts by checking out the code from the repository and installing \nFinally, the test job runs the project’s tests using the poetry poe test command.\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting \nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository.\ncommit with the message feat: Add Docker image and CD pipeline and ran the two jobs de-\nFigure 11.15: GitHub Actions CI pipeline run example\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the \nPush the Docker image to AWS ECR.\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under .github/workflows/\ntrigger is any push to the repository’s main branch.\nThis workflow will automatically run when \nnew code is pushed to the main branch, usually when a PR is merged into the main branch.\nThe workflow then defines a single job named Build & Push Docker Image:\nname: Build & Push Docker Image\nThe first step within the job is to check out the repository’s code.\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends \nrepository’s secrets to authenticate the workflow with AWS.\nnecessary permissions to push Docker images to the ECR repository.\nuses: aws-actions/configure-aws-credentials@v1\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR.\nuses: aws-actions/amazon-ecr-login@v1\nThe final step in the workflow involves building the Docker image and pushing it to the Ama-\nThis is accomplished using the docker/build-push-action@v6 action.\n- name: Build images & push to ECR\nuses: docker/build-push-action@v6\nTo conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to \nThe Docker image is pushed with latest and the commit’s SHA tag.\nAs seen before, we only have the Build & Push Docker Image job here.\nFigure 11.16: GitHub Actions CD pipeline run example\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\nTest out the CI/CD pipeline\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full \nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point \nThese secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create \nTo trigger the CD pipeline, merge the PR into the main branch.\nFor the AWS_ECR_NAME, you should configure only the name of the repository (e.g., \nAfter the CD GitHub Actions are complete, check the ECR repository to see whether the Docker \nFigure 11.18: GitHub Actions secrets\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out \nactions/security-guides/using-secrets-in-github-actions\nCT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, ",
      "keywords": [
        "GitHub Actions",
        "Actions",
        "Docker image",
        "Docker",
        "workflow",
        "Push Docker Image",
        "GitHub",
        "aws",
        "code",
        "ECR",
        "pipeline",
        "Poetry",
        "GitHub Actions secrets",
        "GitHub Actions workflow",
        "run"
      ],
      "concepts": [
        "actions",
        "code",
        "coding",
        "jobs",
        "run",
        "running",
        "workflows",
        "uses",
        "useful",
        "branch"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.562,
          "base_score": 0.412,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.517,
          "base_score": 0.367,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.509,
          "base_score": 0.359,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.467,
          "base_score": 0.317,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "actions",
          "workflow",
          "docker",
          "github",
          "github actions"
        ],
        "semantic": [],
        "merged": [
          "actions",
          "workflow",
          "docker",
          "github",
          "github actions"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25466239554685516,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040196+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 476-483)",
      "start_page": 476,
      "end_page": 483,
      "summary": "for our pipelines and a way to monitor their execution.\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our \nFigure 11.19: CT pipeline\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines \nand how the pipelines are triggered by each other.\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline.\nREST API triggers: You can call a pipeline by an HTTP request.\nwhen integrating your ML pipelines with other components.\nZenML’s documentation: https://docs.zenml.io/v/docs/how-to/trigger-pipelines/\ntrigger-a-pipeline-from-rest-api.\nScheduled triggers: Another common approach is to schedule your pipeline to run con-\nfollowing example from ZenML, the pipeline is scheduled every hour:\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to lever-\nWhen it finds any, it generates a new config and triggers the pipelines through the REST API.\nother option is implementing the watcher as an additional pipeline and leveraging the schedule \ntriggers to look daily for new data.\nThe conclusion is that once you can manually trigger all your ML pipelines through a single \nTrigger downstream pipelines\ncollection pipeline has finished, it will trigger the feature pipeline.\nWhen the feature pipeline has \nbeen completed successfully, it triggers the dataset generation pipeline, and so on.\nthe logic more complex, like scheduling the generate instruct dataset pipeline to run daily, check-\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything \n@pipeline\nand deploy logic to the parent pipeline to implement an end-to-end flow.\nTo run the end-to-end pipeline, use the following poe command:\npoetry poe run-end-to-end-data-pipeline\npipeline isolated and use triggers to start downstream pipelines.\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\nwe have more, we avoided that limitation by compressing all the steps into a single pipeline.\nly trigger a pipeline from another pipeline, as you can see in the code snippet below where we \ntriggered the feature engineering pipeline after the data collection ETL:\nfrom zenml import pipeline, step\n@pipeline \ntrigger_feature_engineering_pipeline(user)\ndef trigger_feature_engineering_pipeline(user):\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_\n@pipeline\ndef llm_chain(input_text: str) -> str:\ndef llm_chain(input_text):\n\"value\": compute_llm_judge_score(…),\nprompt that already contains the user’s input and context and returns an answer that is usually \nFigure 11.21: Inference pipeline serving architecture",
      "keywords": [
        "pipeline",
        "LLM",
        "data",
        "str",
        "LLM Twin",
        "triggers",
        "user",
        "REST API triggers",
        "ZenML",
        "REST API",
        "author",
        "input",
        "feature",
        "track",
        "links"
      ],
      "concepts": [
        "pipelines",
        "zenml",
        "trigger",
        "triggered",
        "user",
        "data",
        "llm",
        "generated",
        "generates",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 8,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 50,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 45,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 44,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pipeline",
          "trigger",
          "triggers",
          "pipelines",
          "rest api"
        ],
        "semantic": [],
        "merged": [
          "pipeline",
          "trigger",
          "triggers",
          "pipelines",
          "rest api"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31959772484324045,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040248+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 484-491)",
      "start_page": 484,
      "end_page": 491,
      "summary": "def call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\ndef rag(query: str) -> str:\nanswer = call_llm_service(query, context)\npost-processing steps, such as the ContextRetriever search function:\nMLOps and LLMOps\nquery_model = Query.from_str(query)\nquery_model = self._metadata_extractor.generate(query_model)\ndef generate(self, query: str) -> str:\ndef rag(query: str) -> str:\nanswer, prompt = call_llm_service(query, context)\nModel configuration: Here, we should consider both the LLM and other models used \nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such \nFor example, you can add a callback in your training pipeline to trigger \nfrom zenml import get_pipeline_context, pipeline\ndef training_pipeline(…):\nMLOps and LLMOps\nZenML and most orchestrators simplify implementing an alerter, as it’s a critical component \npipeline is, the three core dimensions of an ML application (code, data, model), and that, after \ndeployment, it is more critical than ever to implement a monitoring and alerting layer due to \nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud.\nThe final step was to add LLMOps to our LLM Twin project.\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an \nto any LLM-based application.\nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with \ndata collection and fine-tuning until deploying the LLM microservice and RAG service.\nMLOps: Continuous delivery and automation pipelines in machine learning.\nhttps://cloud.google.com/architecture/mlops-continuous-\nhttps://ml-ops.org/content/mlops-principles\nhttps://ml-ops.org/content/mlops-principles\n(2024c, July 5).\nhttps://ml-ops.org/content/motivation\nmadewithml.com/courses/mlops/monitoring/\nTesting Machine Learning Systems: Code, Data and Models.\nhttps://madewithml.com/courses/mlops/testing/\nMLOps and LLMOps\nUnderstanding LLMOps: Large Language Model Operations.\nGitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps over-\nview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker \nGitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/\nBuilding robust and scalable ML systems requires more than creating powerful models.\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual \nan ML application.\nThe data scientist manually performs each pipeline step, such as data \npreparation and validation, model training, and testing.\nJupyter notebooks to train their models.\nthe data and train the models.\nContinuous training (CT): The next level involves automating model training.\nknown as continuous training, which triggers model retraining whenever required.\npoint, you often automate your data and model validation steps.\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable \nautomatic building, testing, and deployment of data, ML models, and training pipeline \nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can \nquickly move from a manual process to CI/CD/CT.\nOnce they improve the model by tinkering with how the data is processed \nor the model architecture, they push the code to the code repository, which triggers the CI/CD \npipeline to build, test, package, and deploy the new changes to the FTI pipelines.",
      "keywords": [
        "query",
        "llm",
        "model",
        "str",
        "data",
        "pipeline",
        "MLOps",
        "LLM Twin",
        "track def rag",
        "track def",
        "context",
        "application",
        "rag",
        "LLMOps",
        "code"
      ],
      "concepts": [
        "model",
        "data",
        "monitoring",
        "application",
        "applications",
        "zenml",
        "build",
        "llm",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 41,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 39,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 7,
          "title": "",
          "score": 0.585,
          "base_score": 0.585,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mlops",
          "query",
          "str",
          "llmops",
          "zenml"
        ],
        "semantic": [],
        "merged": [
          "mlops",
          "query",
          "str",
          "llmops",
          "zenml"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34982062290385274,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040436+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 492-500)",
      "start_page": 492,
      "end_page": 500,
      "summary": "By now, we understand that the whole ML system changes if the code, model, or data changes.\nwe adopt to track the code, model, and data separately?\nmodels used within your system.\nmodel, such as what data it was trained on, its architecture, performance, latency, and \nVersioning the data isn’t as straightforward as versioning the code and model because it \nsolutions are based on Git-like systems, such as Data Version Control (DVC), that track \ncreating a new version for every change made to your data.\nTraining ML models is an entirely iterative and experimental process.\nmodel.\n4. Testing\nThe same trend is followed when testing ML systems.\nall three dimensions: the data, the model, and the code.\noverall user experience—for example, testing an entire ML pipeline, from data ingestion \nto model training and inference, ensuring the system produces the correct outputs for \nWhat do we test?\nYou want to test that you get an expected output for a given \nTest examples\nWhen we talk about data tests, we mainly refer to data validity.\nThus, by writing integration or system tests for your feature pipeline, \nTesting data validity depends a lot on your application and data type.\nModel tests are the trickiest, as model training is the most non-deterministic process of an ML \nSome standard model test techniques involve checking:\nThe shapes of the input and model output tensors\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which \ntries to adopt the strategy from code testing and treats the model as a black box while looking \nsolely at the input data and expected outputs.\nThis makes the behavioral testing methods model \nA fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models \nquick overview, the paper proposes that you test your model against three types of tests.\na model that extracts the main subject from a sentence as an example:\nmodel(text=\"The advancements in AI are changing the world rapidly.\")\nmodel(text=\"The progress in AI is changing the world rapidly.\")\nexample where we know the outputs should change based on the provided inputs:\nfor example, below is a set of simple examples that we expect the model should always \nData, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/\ntesting/.\nmeans that our ML model will constantly be exposed to a level of degradation.\nbecause the data from production might differ from the data the model was trained on.\nIntuitively, monitoring detects the model’s performance degradation, which triggers an alarm that \nWhy retrain the model?\nAs the model performance degrades due to a drift in the training dataset \nand what it inputs from production, the only solution is to adapt or retrain the model on a new \ndifferent aspects of your application, such as the infrastructure, data, and model.\nModel metrics\nmodel.\nTherefore, moving on to the next layer of metrics that focus on the model’s performance \nwell as essential business metrics influenced by the model, such as ROI and click rate.\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance \nDrifts are proxy metrics that help us detect potential issues with the production model in time \ndata drift →P(X) ≠Pref(X) \nTable A.1: Relationship between data, model, and code changes\nData drift\nmodel cannot handle the changes in feature space, leading to potentially unreliable predictions.\nDrift can result from natural real-life changes or systemic problems like missing data, pipeline \nFigure A.3: Data drift examples\nWhen data begins to drift, the degradation in our model’s performance might not be immediately \nchance to consider retraining before the drift affects the model’s performance.\nIn addition to changes in input data (data drift), we might also encounter shifts in output dis-\nWhile retraining the model can help reduce performance \nand model head to support the new schema of the output class.\nIn addition to changes in input and output data, their relationship can also shift.\nenon, known as concept drift, makes our model ineffective because the patterns it previously \nFor example, this happens when using the model in a different geographic area.",
      "keywords": [
        "model",
        "data",
        "system",
        "drift",
        "data drift",
        "output",
        "code",
        "System tests",
        "metrics",
        "version",
        "performance",
        "inputs",
        "input data"
      ],
      "concepts": [
        "data",
        "model",
        "outputs",
        "version",
        "metrics",
        "performance",
        "perform",
        "features",
        "drift",
        "pipelines"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 42,
          "title": "",
          "score": 0.532,
          "base_score": 0.382,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "drift",
          "data",
          "testing",
          "changes"
        ],
        "semantic": [],
        "merged": [
          "model",
          "drift",
          "data",
          "testing",
          "changes"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25997914992959914,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040484+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 501-509)",
      "start_page": 501,
      "end_page": 509,
      "summary": "How to detect and measure drifts\nA reference window: This is the collection of data points used as a baseline to compare \nagainst the production data distributions for drift identification.\nA test window: This collects data points gathered while the ML system is in production.\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between \nFor example, you can use the Kolmogorov-Smirnov (KS) test to monitor a \nwindow distribution.\nWhen working with text data in an embedding representation, we have to model a multivariate \nthe test and reference windows, apply a dimensionality reduction algorithm, and apply an algo-\nmean of the embeddings of the two windows.\nMonitoring involves the collection and visualization of data, whereas observability provides in-\nOn the other hand, a system is considered observable if it generates meaningful data about its \nTweaking the p-value of the statistical tests that check for drifts.\nA lower p-value means \nThese thresholds and p-values depend on your application.\ntriggered the alarm, with what value, the time it happened, and anything else that makes sense \nand train the model on the newly shifted dataset to solve the drift.\nyou use the same dataset and hyperparameters, you might end up with a model with a differ-\nThis aspect can be solved by always using a seed before generating random \nrandom values or randomly remove data or labels.\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\nOther Books \nIf you enjoyed this book, you may be interested in these other books by Packt:\nOther Books You May Enjoy\nCustomize and scale RAG-driven generative AI systems across domains\nControl and build robust generative AI systems grounded in real-world data\nCombine text and image data for richer, more informative AI responses\nOther Books You May Enjoy\nUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\nUnderstand the implications of LFMs for AI research and industry applications\nOther Books You May Enjoy",
      "keywords": [
        "data",
        "books",
        "system",
        "reference window",
        "model",
        "LLM",
        "drift",
        "window",
        "reference",
        "Enjoy",
        "data points",
        "alarm",
        "production data distributions",
        "LLMs",
        "production"
      ],
      "concepts": [
        "data",
        "monitor",
        "randomly",
        "llm",
        "values",
        "dataset",
        "engineers",
        "engineering",
        "distributions",
        "distribution"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 54,
          "title": "",
          "score": 0.493,
          "base_score": 0.343,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 48,
          "title": "",
          "score": 0.424,
          "base_score": 0.274,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 47,
          "title": "",
          "score": 0.364,
          "base_score": 0.214,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.363,
          "base_score": 0.213,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 5,
          "title": "",
          "score": 0.356,
          "base_score": 0.206,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "books",
          "window",
          "enjoy",
          "data",
          "books enjoy"
        ],
        "semantic": [],
        "merged": [
          "books",
          "window",
          "enjoy",
          "data",
          "books enjoy"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.16377449942913958,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040522+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 510-517)",
      "start_page": 510,
      "end_page": 517,
      "summary": "advanced RAG post-retrieval optimization\nadvanced RAG retrieval optimization\nLLM Twin model, deploying to  375-385\nCI/CD pipeline  462\nCI pipeline, LLM Twin\ndeployment (CI/CD) pipeline  31, 402\nversus LLM Twin  4\ndata collection pipeline  19\ndata deduplication  184, 185\ndata drift  470\ndata evaluation  233\ndata exploration  189-191\ndata generation  191-233\npreference data, evaluating  235-237\npreference data, generating  233, 234\ndata indexing techniques  119\ndata quality evaluation  186-189\ndata tests  466\ndata  357\ndomain-specific LLM evaluations  265-267\ndownstream pipelines\nhuman-generated, LLM-evaluated \nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\ndata drift  470\nend-to-end RAG inference pipeline\nETL pipeline\nconnecting, to feature pipeline  60\nExtract, Transform, Load (ETL) pipeline  55\nfeature pipeline  14, 19, 20\nfeature pipeline  14\ninference pipeline  14\ntraining pipeline  14\nused, for building LLM system  462, 463\nFTI pipeline design\nLLM Twin architecture, designing  17\nFTI pipelines architecture\ninference pipeline  14\ngeneral-purpose LLM evaluations  263-265\nhandlers  162, 163\nhuman-generated, LLM-evaluated \ninference pipeline  22\nversus training pipeline  371, 372\ndata deduplication  184, 185\ndata exploration  189-191\ndata generation  191, 193\ndata quality evaluation  186-189\nlarge language model (LLM)  1, 99, 355, 401\nLLM evaluation  235\nversus, ML evaluation  262, 263\nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\nLLM system\nLLM Twin  2, 5, 6\nCD pipeline  442-444\nCI/CD pipeline flow  434, 435\nCI/CD pipeline, testing  445\nCI pipeline  438\nCT pipeline  446, 448\ninference pipeline deployment \nRAG feature pipeline architecture  127, 139\nLLM Twin architecture  23\ndata collection pipeline  19\ndesigning, with FTI pipeline design  17\nfeature pipeline  19, 20\ninference pipeline  22\ntraining pipeline  21, 22\nLLM Twin model\nLLM Twin RAG feature pipeline\nhandlers  162\nsetting  139\nZenML pipeline and steps  140, 141\nLLM Twin's data collection pipeline\nZenML pipeline and steps  61-65\nLLM Twin service\nLLM Twin's pipelines, cloud deployment  415\nMongoDB, setting up  418, 419\npipelines, running on AWS  428-431\npipeline on SageMaker  432, 433\nZenML, setting up  421-423\nLow-Rank Adaptation (LoRA)  213-215\nminimum viable product (MVP)  6\nvesus, LLM evaluation  262, 263\nML models\nCI/CD pipeline  462\nML pipeline orchestrator  407\nML pipeline automation\nML pipelines\nmodel evaluation  261\ndomain-specific LLM evaluations  265-267\ngeneral-purpose LLM evaluations  263-265\nML, versus LLM evaluation  262, 263\ntask-specific LLM evaluations  267-271\nmodel optimization strategies  290\npipeline parallelism (PP)  300, 301\nmodel tests  466\nsetting up  418, 419\nMongoDB, as data warehouse\nmonolithic batch pipeline architecture  10\nclasses  87-89\npipeline parallelism (PP)  300",
      "keywords": [
        "LLM Twin",
        "LLM",
        "pipeline",
        "data",
        "LLM Twin RAG",
        "LLM Twin architecture",
        "LLM Twin model",
        "Twin",
        "LLM evaluations",
        "LLM Twin pipelines",
        "LLM Twin service",
        "advanced RAG",
        "RAG",
        "LLM Twin data",
        "inference"
      ],
      "concepts": [
        "data",
        "pipelines",
        "llm",
        "models",
        "optimization",
        "optimizations",
        "optimized",
        "optimal",
        "evaluation",
        "evaluating"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 2,
          "title": "",
          "score": 0.809,
          "base_score": 0.809,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.732,
          "base_score": 0.732,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "",
          "score": 0.729,
          "base_score": 0.729,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.633,
          "base_score": 0.633,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.624,
          "base_score": 0.624,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pipeline",
          "llm",
          "llm twin",
          "twin",
          "data"
        ],
        "semantic": [],
        "merged": [
          "pipeline",
          "llm",
          "llm twin",
          "twin",
          "data"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4084493039360073,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:17.040582+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 518-522)",
      "start_page": 518,
      "end_page": 522,
      "summary": "code generation  231\ndata evaluation  233\ndata generation  233\ndata quantity  232\ndata indexing  119\ndata category  151\nPydantic Settings\ntechniques  313, 314\nRAG evaluation  271, 272\nRAG feature pipeline\ndata extraction  134\ndata loading  135\ndata warehouse and feature store, \nembedding  135\nRAG feature pipeline architecture\nbatch pipelines  130\npipelines  130-134\ninference pipeline  127\nraw data  128\nRAG inference pipeline\nReinforcement Learning from Human \npolicy optimization  246\nreward model learning  246\nreinforcement learning (RL)  246\nembeddings  107, 108\nembeddings, creating  111-114\nretrieval-augmented generation (RAG) \npipeline  206, 261\nreward model learning  246\nSFT, techniques\ntechniques  211\nsystem tests  464\ntest job  438\ntest types  465\nsystem tests  464\ntest window  472\ntraining pipeline  14, 21, 22\nversus inference pipeline  371, 372\ntriggers\ndata  463\ntest window  472\nZenML pipeline  140-142\ncleaned documents, chunking  147-150\ncleaned documents, embedding  147-150\ndata warehouse, querying  143-145\nDownload a free PDF copy of this book\nThanks for purchasing this book!\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.",
      "keywords": [
        "data",
        "RAG",
        "pipeline",
        "Index",
        "inference",
        "generation",
        "learning",
        "model",
        "triggers",
        "reference",
        "REST API",
        "query",
        "REST API triggers",
        "RAG feature pipeline",
        "window"
      ],
      "concepts": [
        "pipeline",
        "rag",
        "evaluations",
        "evaluating",
        "index",
        "generating",
        "inference",
        "model",
        "triggers",
        "book"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 2,
          "title": "",
          "score": 0.9,
          "base_score": 0.75,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 56,
          "title": "",
          "score": 0.729,
          "base_score": 0.729,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 3,
          "title": "",
          "score": 0.565,
          "base_score": 0.565,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 14,
          "title": "",
          "score": 0.541,
          "base_score": 0.541,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 6,
          "title": "",
          "score": 0.525,
          "base_score": 0.525,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "246",
          "data",
          "pipeline",
          "window",
          "rag"
        ],
        "semantic": [],
        "merged": [
          "246",
          "data",
          "pipeline",
          "window",
          "rag"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38013587393963816,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:17.040633+00:00"
      }
    }
  ],
  "total_chapters": 57,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "LLM-Engineers-Handbook_metadata.json",
    "enrichment_date": "2025-12-17T23:06:17.051038+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4583.679585000937,
    "total_similar_chapters": 283
  }
}