{
  "metadata": {
    "title": "Architecture Patterns with Python",
    "source_file": "test_book_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Domain Modeling",
      "start_page": 46,
      "end_page": 73,
      "summary": "Domain Modeling\nWe’ll discuss why domain modeling matters, and we’ll look at a few key patterns for modeling domains: Entity, Value Object, and Domain Service.\nother chapters, we’ll build things around the domain model, but you should always be able to find these little shapes at the core.\nWhat Is a Domain Model?\nFor the rest of the book, we’re going to use the term domain model instead.\nDomain-driven design, or DDD, popularized the concept of domain modeling, and it’s been a hugely successful movement in transforming the way people design software by focusing on the core business domain.\nThe terminology used by business stakeholders represents a distilled understanding of the domain model, where complex ideas and processes are boiled down to a single word or phrase.\nWe’re going to use a real-world domain model throughout this book, specifically a model from our current employment.\nTime for some domain modeling.\ndomain model.\nWe need to allocate order lines to batches.\nWhen we’ve allocated an order line to a batch, we will send stock from that specific batch to the customer’s delivery address.\nWhen we allocate x units of stock to a batch, the available quantity is reduced by x.\nWe have a batch of 20 SMALL-TABLE, and we allocate an order line for 2 SMALL-TABLE.\nWe can’t allocate to a batch if the available quantity is less than the quantity of the order line.\nWe should not be able to allocate the line to the batch.\nWe have a batch of 10 BLUE-VASE, and we allocate an order line for 2 BLUE-VASE.\nIf we allocate the order line again to the same batch, the batch should still have an available quantity of 8.\nWe allocate to warehouse stock in preference to shipment batches.\nWe allocate to shipment batches in order of which has the earliest ETA.\nUnit Testing Domain Models\nA first test for allocation (test_batches.py)\ndef test_allocating_to_a_batch_reduces_the_available_quantity(): batch = Batch(\"batch-001\", \"SMALL-TABLE\", qty=20, eta=date.today()) line = OrderLine('order-ref', \"SMALL-TABLE\", 2)\nbatch.allocate(line)\nFirst cut of a domain model for batches (model.py)\nclass Batch: def __init__( self, ref: str, sku: str, qty: int, eta: Optional[date] ): self.reference = ref self.sku = sku self.eta = eta self.available_quantity = qty\ndef allocate(self, line: OrderLine): self.available_quantity -= line.qty\nIf you want to double- check anything, you can see the full working code for each chapter in its branch (e.g., chapter_01_domain_model).\nOur implementation here is trivial: a Batch just wraps an integer available_quantity, and we decrement that value on allocation.\nTesting logic for what we can allocate (test_batches.py)\ndef make_batch_and_line(sku, batch_qty, line_qty): return ( Batch(\"batch-001\", sku, batch_qty, eta=date.today()), OrderLine(\"order-123\", sku, line_qty) )\ndef test_can_allocate_if_available_greater_than_required(): large_batch, small_line = make_batch_and_line(\"ELEGANT-LAMP\", 20, 2) assert large_batch.can_allocate(small_line)\ndef test_cannot_allocate_if_available_smaller_than_required(): small_batch, large_line = make_batch_and_line(\"ELEGANT-LAMP\", 2, 20) assert small_batch.can_allocate(large_line) is False\ndef test_can_allocate_if_available_equal_to_required(): batch, line = make_batch_and_line(\"ELEGANT-LAMP\", 2, 2) assert batch.can_allocate(line)\ndef test_cannot_allocate_if_skus_do_not_match(): batch = Batch(\"batch-001\", \"UNCOMFORTABLE-CHAIR\", 100, eta=None) different_sku_line = OrderLine(\"order-123\", \"EXPENSIVE-TOASTER\", 10) assert batch.can_allocate(different_sku_line) is False\nand a line for the same SKU; and we’ve written four simple tests for a new method can_allocate.\ndef can_allocate(self, line: OrderLine) -> bool: return self.sku == line.sku and self.available_quantity >= line.qty\nThis test is going to require a smarter model (test_batches.py)\ndef test_can_only_deallocate_allocated_lines(): batch, unallocated_line = make_batch_and_line(\"DECORATIVE-TRINKET\", 20, 2) batch.deallocate(unallocated_line) assert batch.available_quantity == 20\nIn this test, we’re asserting that deallocating a line from a batch has no effect unless the batch previously allocated the line.\nFor this to work, our Batch needs to understand which lines have been allocated.\nThe domain model now tracks allocations (model.py)\nclass Batch: def __init__( self, ref: str, sku: str, qty: int, eta: Optional[date] ): self.reference = ref\n@property def allocated_quantity(self) -> int: return sum(line.qty for line in self._allocations)\ndef can_allocate(self, line: OrderLine) -> bool: return self.sku == line.sku and self.available_quantity >= line.qty\nA batch now keeps track of a set of allocated OrderLine objects.\ndef test_allocation_is_idempotent(): batch, line = make_batch_and_line(\"ANGULAR-DESK\", 20, 2) batch.allocate(line) batch.allocate(line) assert batch.available_quantity == 18\nAt the moment, it’s probably a valid criticism to say that the domain model is too trivial to bother with DDD (or even object orientation!).\nBut taking this simple domain model as a placeholder for something more complex, we’re going to extend our simple domain model in the rest of the book and plug it into the real world of APIs and databases\nclass Batch: def __init__(self, ref: Reference, sku: Sku, qty: Quantity): self.sku = sku self.reference = ref self._purchased_quantity = qty\nA value object is any domain object that is uniquely identified by the data it holds; we usually make them immutable:\nAn order line is uniquely identified by its order ID, SKU, and quantity; if we change one of those values, we now have a new line.\nWe use the term entity to describe a domain object that has long-lived identity.\nWe can allocate lines to a batch, or change the\nWe’ve made a model to represent batches, but what we actually need to do is allocate order lines against a specific set of batches that\nEvans discusses the idea of Domain Service operations that don’t have a natural home in an entity or value object.\norder line, given a set of batches, sounds a lot like a function, and we can take advantage of the fact that Python is a multiparadigm language and just make it a function.\nTesting our domain service (test_allocate.py)\ndef test_prefers_current_stock_batches_to_shipments(): in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow) line = OrderLine(\"oref\", \"RETRO-CLOCK\", 10)\nallocate(line, [in_stock_batch, shipment_batch])\ndef test_returns_allocated_batch_ref(): in_stock_batch = Batch(\"in-stock-batch-ref\", \"HIGHBROW-POSTER\", 100, eta=None) shipment_batch = Batch(\"shipment-batch-ref\", \"HIGHBROW-POSTER\", 100, eta=tomorrow) line = OrderLine(\"oref\", \"HIGHBROW-POSTER\", 10) allocation = allocate(line, [in_stock_batch, shipment_batch]) assert allocation == in_stock_batch.reference\nA standalone function for our domain service (model.py)\ndef allocate(line: OrderLine, batches: List[Batch]) -> str: batch = next( b for b in sorted(batches) if b.can_allocate(line) ) batch.allocate(line) return batch.reference\nTo make it work, we implement __gt__ on our domain model:\nIn our conversations with domain experts, we’ve learned about the possibility that an order cannot be allocated because we are out of stock, and we can capture that by using a domain\ndef test_raises_out_of_stock_exception_if_cannot_allocate(): batch = Batch('batch1', 'SMALL-FORK', 10, eta=today) allocate(OrderLine('order1', 'SMALL-FORK', 10), [batch])\nwith pytest.raises(OutOfStock, match='SMALL-FORK'): allocate(OrderLine('order2', 'SMALL-FORK', 1), [batch])\nDomain modeling\nWe won’t bore you too much with the implementation, but the main thing to note is that we take care in naming our exceptions in the ubiquitous language, just as we do our entities, value objects, and services:\nRaising a domain exception (model.py)\ndef allocate(line: OrderLine, batches: List[Batch]) -> str: try: batch = next( ...\nOur domain model at the end of the chapter\n1 DDD did not originate domain modeling.",
      "keywords": [
        "batch",
        "Domain Model",
        "Domain",
        "line",
        "model",
        "allocate",
        "quantity",
        "SKU",
        "Domain Service",
        "def test",
        "order line",
        "Object",
        "stock",
        "order",
        "business"
      ],
      "concepts": [
        "batches",
        "batch",
        "allocating",
        "allocation",
        "allocate",
        "allocations",
        "lines",
        "modeling",
        "domain",
        "businesses"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.49,
          "base_score": 0.34,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.462,
          "base_score": 0.312,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 7,
          "title": "",
          "score": 0.354,
          "base_score": 0.354,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.336,
          "base_score": 0.336,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "batch",
          "line",
          "domain",
          "self",
          "allocate"
        ],
        "semantic": [],
        "merged": [
          "batch",
          "line",
          "domain",
          "self",
          "allocate"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3180102269573265,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533784+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Repository Pattern",
      "start_page": 74,
      "end_page": 102,
      "summary": "We’ll introduce the Repository pattern, a simplifying abstraction over\nFigure 2-1 shows a little preview of what we’re going to build: a Repository object that sits between our domain model and the database.\ngit clone https://github.com/cosmicpython/code.git cd code git checkout chapter_02_repository # or to code along, checkout the previous chapter: git checkout chapter_01_domain_model\nPersisting Our Domain Model\nIn Chapter 1 we built a simple domain model that can allocate orders to batches of stock.\nIf we needed to run a database or an API and create test data, our tests would be harder to write and maintain.\nFor the next few chapters we’re going to look at how we can connect our idealized domain model to external state.\n# call our domain service allocate(line, batches) # then save the allocation back to the database somehow return 201\nWe’ll need a way to retrieve batch info from the database and instantiate our domain model objects from it, and we’ll also need a\nBut we want our domain model to have no dependencies whatsoever.\nWe don’t want infrastructure concerns bleeding over into our domain model and slowing our unit tests or our ability to make changes.\nInstead, as discussed in the introduction, we’ll think of our model as being on the “inside,” and dependencies flowing inward to it; this is what people sometimes call onion architecture (see Figure 2-3).\nLet’s remind ourselves of our domain model (see Figure 2-4): an allocation is the concept of linking an OrderLine to a Batch.\nThe “Normal” ORM Way: Model Depends on ORM\nobjects and domain modeling and the world of databases and relational algebra.\nthe idea that our fancy domain model doesn’t need to know anything\nSQLAlchemy “declarative” syntax, model depends on ORM (orm.py)\nYou don’t need to understand SQLAlchemy to see that our pristine model is now full of dependencies on the ORM and is starting to look\nThe point is the same—our model classes inherit directly from ORM classes, so our model depends on the ORM.\nDjango doesn’t provide an equivalent for SQLAlchemy’s classical mapper, but see Appendix D for examples of how to apply dependency inversion and the Repository pattern to Django.\nInverting the Dependency: ORM Depends on Model\ndomain model, what SQLAlchemy calls a classical mapping:\ndef start_mappers(): lines_mapper = mapper(model.OrderLine, order_lines)\nThe ORM imports (or “depends on” or “knows about”) the domain model, and not the other way around.\nWhen we call the mapper function, SQLAlchemy does its magic to bind our domain model classes to the various tables we’ve defined.\nBut if we never call that function, our domain model classes stay blissfully unaware of the database.\ndef test_orderline_mapper_can_load_lines(session): session.execute( 'INSERT INTO order_lines (orderid, sku, qty) VALUES ' '(\"order1\", \"RED-CHAIR\", 12),' '(\"order1\", \"RED-TABLE\", 13),' '(\"order2\", \"BLUE-LIPSTICK\", 14)' ) expected = [ model.OrderLine(\"order1\", \"RED-CHAIR\", 12), model.OrderLine(\"order1\", \"RED-TABLE\", 13), model.OrderLine(\"order2\", \"BLUE-LIPSTICK\", 14), ] assert session.query(model.OrderLine).all() == expected\ndef test_orderline_mapper_can_save_lines(session): new_line = model.OrderLine(\"order1\", \"DECORATIVE-WIDGET\", 12) session.add(new_line) session.commit()\nonce you’ve taken the step of inverting the dependency of ORM and domain model, it’s only a small additional step to implement another\nabstraction called the Repository pattern, which will be easier to write\nBut we’ve already achieved our objective of inverting the traditional dependency: the domain model stays “pure” and free from\ndomain model doesn’t need to change at all.\nDepending on what you’re doing in your domain model, and especially if you stray far from the OO paradigm, you may find it increasingly\nhard to get the ORM to produce the exact behavior you need, and you may need to modify your domain model.\nThe Repository pattern is an abstraction over persistent storage.\nThis self-imposed simplicity stops us from coupling our domain model to the database.\nHere’s what an abstract base class (ABC) for our repository would\n@abc.abstractmethod def add(self, batch: model.Batch): raise NotImplementedError\n@abc.abstractmethod def get(self, reference) -> model.Batch: raise NotImplementedError\nWe’re using abstract base classes in this book for didactic reasons: we hope they help explain what the interface of the repository abstraction is.\nAs far as our code is concerned, we’re really just swapping the SQLAlchemy abstraction (session.query(Batch)) for a different one (batches_repo.get) that we designed.\nWe will have to write a few lines of code in our repository class each time we add a new domain object that we want to retrieve, but in\nThe Repository pattern would make it easy to make fundamental changes to the way we store things (see Appendix C), and as we’ll see, it is easy to fake out for unit tests.\nUnlike the ORM tests from earlier, these tests are good candidates for staying part of your codebase longer term, particularly if any parts of your domain model mean the object-relational map is nontrivial.\nRepository test for saving an object (test_repository.py)\ndef test_repository_can_save_a_batch(session): batch = model.Batch(\"batch1\", \"RUSTY-SOAPDISH\", 100, eta=None)\nrepo = repository.SqlAlchemyRepository(session) repo.add(batch) session.commit()\nRepository test for retrieving a complex object (test_repository.py)\ndef test_repository_can_retrieve_a_batch_with_allocations(session): orderline_id = insert_order_line(session) batch1_id = insert_batch(session, \"batch1\") insert_batch(session, \"batch2\") insert_allocation(session, orderline_id, batch1_id)\nrepo = repository.SqlAlchemyRepository(session) retrieved = repo.get(\"batch1\")\nexpected = model.Batch(\"batch1\", \"GENERIC-SOFA\", 100, eta=None) assert retrieved == expected # Batch.__eq__ only compares reference assert retrieved.sku == expected.sku assert retrieved._purchased_quantity == expected._purchased_quantity assert retrieved._allocations == { model.OrderLine(\"order1\", \"GENERIC-SOFA\", 12), }\ndef get(self, reference): return self.session.query(model.Batch).filter_by(reference=reference).one()\ndef list(self): return self.session.query(model.Batch).all()\n@flask.route.gubbins def allocate_endpoint(): batches = SqlAlchemyRepository.list() lines = [ OrderLine(l['orderid'], l['sku'], l['qty']) for l in request.params...\nWe bumped into a friend at a DDD conference the other day who said, “I haven’t used an ORM in 10 years.” The Repository pattern and an ORM both act as abstractions in front of raw SQL, so using one behind the other isn’t really necessary.\nExample usage of fake repository (test_api.py)\nWith that in mind, Table 2-1 shows some of the pros and cons of the Repository pattern and our persistence-ignorant model.\nWe have a simple interface between persistent storage and our domain model.\nIt’s easy to make a fake version of the repository for unit testing, or to swap out different storage solutions, because we’ve fully decoupled the model from infrastructure concerns.\nWriting the domain model before thinking about persistence helps us focus on the business problem at hand.\nFigure 2-6 shows the basic thesis: yes, for simple cases, a decoupled domain model is harder work than a simple ORM/ActiveRecord pattern.\nIf your app is just a simple CRUD (create-read-update-delete) wrapper around a database, then you don’t need a domain model or a repository.\nDomain model trade-offs as a diagram\nImagine, for example, if we decide one day that we want to change allocations to live on the OrderLine instead of on the Batch object: if we were using Django, say, we’d have to define and think through the\nAs it is, because our model is just plain old Python objects, we can change a set() to being a new attribute, without needing to think about the database until later.\nOur domain model should be free of infrastructure concerns, so your ORM should import your model, and not the other way around.\nThe Repository pattern is a simple abstraction around permanent storage",
      "keywords": [
        "domain model",
        "Repository Pattern",
        "Repository",
        "model",
        "ORM",
        "domain",
        "batch",
        "Pattern",
        "’ll",
        "database",
        "order",
        "Python",
        "domain model classes",
        "n’t",
        "’re"
      ],
      "concepts": [
        "modeling",
        "batches",
        "batch",
        "session",
        "sessions",
        "tested",
        "repository",
        "repositories",
        "sqlalchemy",
        "orm"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 1,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 13,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 3,
          "title": "",
          "score": 0.462,
          "base_score": 0.312,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "domain model",
          "domain",
          "repository",
          "orm"
        ],
        "semantic": [],
        "merged": [
          "model",
          "domain model",
          "domain",
          "repository",
          "orm"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45407855142061704,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533823+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "A Brief Interlude: On Coupling and Abstractions",
      "start_page": 103,
      "end_page": 121,
      "summary": "And how do they relate to testing?\nImagine we want to write code for synchronizing two file directories, which we’ll call the source and the destination:\nThe code to generate a SHA-1 hash from a file is simple enough:\ndef hash_file(path): hasher = hashlib.sha1() with path.open(\"rb\") as file: buf = file.read(BLOCKSIZE) while buf: hasher.update(buf) buf = file.read(BLOCKSIZE) return hasher.hexdigest()\nWe’ll use this approach throughout the book, because it’s how we write code in the real world: start with a solution to the smallest part of the problem, and then iteratively make the solution richer and better designed.\ndef sync(source, dest): # Walk the source folder and build a dict of filenames and their hashes source_hashes = {} for folder, _, files in os.walk(source): for fn in files: source_hashes[hash_file(Path(folder) / fn)] = fn\n# if there's a file in target that's not in source, delete it if dest_hash not in source_hashes: dest_path.remove()\n# if there's a file in target that has a different path in source, # move it to the correct path elif dest_hash in source_hashes and fn != source_hashes[dest_hash]: shutil.move(dest_path, Path(folder) / source_hashes[dest_hash])\n# for every file that appears in source but not target, copy the file to # the target for src_hash, fn in source_hashes.items(): if src_hash not in seen: shutil.copy(Path(source) / fn, Path(dest) / fn)\nWe have some code and it looks OK, but before we run it on our hard drive, maybe we should test it.\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): try: source = tempfile.mkdtemp() dest = tempfile.mkdtemp()\ncontent = \"I am a very useful file\" (Path(source) / 'my-file').write_text(content)\ndef test_when_a_file_has_been_renamed_in_the_source(): try: source = tempfile.mkdtemp() dest = tempfile.mkdtemp()\ncontent = \"I am a file that was renamed\" source_path = Path(source) / 'source-filename' old_dest_path = Path(dest) / 'dest-filename' expected_dest_path = Path(dest) / 'source-filename' source_path.write_text(content) old_dest_path.write_text(content)\nGetting decent coverage and revealing these bugs means writing more tests, but if they’re all as unwieldy as the preceding ones, that’s going to get real painful real quickly.\nOur high-level code is coupled to low-level details, and it’s making life hard.\nsource_files = {'hash1': 'path1', 'hash2': 'path2'} dest_files = {'hash1': 'path1', 'hash2': 'pathX'}\nNow we could write tests that just use two filesystem dicts as inputs,\nfunction, check what actions have happened,” we say, “Given this abstraction of a filesystem, what abstraction of filesystem actions\nSimplified inputs and outputs in our tests (test_sync.py)\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): src_hashes = {'hash1': 'fn1'} dst_hashes = {} expected_actions = [('COPY', '/src/fn1', '/dst/fn1')] ...\ndef test_when_a_file_has_been_renamed_in_the_source(): src_hashes = {'hash1': 'fn1'} dst_hashes = {'hash1': 'fn2'} expected_actions == [('MOVE', '/dst/fn2', '/dst/fn1')] ...\nThat’s all very well, but how do we actually write those new tests, and how do we change our implementation to make it all work?\ntest it thoroughly without needing to set up a real filesystem.\ndef sync(source, dest): # imperative shell step 1, gather inputs source_hashes = read_paths_and_hashes(source) dest_hashes = read_paths_and_hashes(dest)\n# step 2: call functional core actions = determine_actions(source_hashes, dest_hashes, source, dest)\nThe code to build up the dictionary of paths and hashes is now trivially easy to write:\ndef read_paths_and_hashes(root): hashes = {} for folder, _, files in os.walk(root): for fn in files: hashes[hash_file(Path(folder) / fn)] = fn return hashes\nThe determine_actions() function will be the core of our business logic, which says, “Given these two sets of hashes and filenames, what\nOur tests now act directly on the determine_actions() function:\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): src_hashes = {'hash1': 'fn1'} dst_hashes = {} actions = determine_actions(src_hashes, dst_hashes, Path('/src'), Path('/dst')) assert list(actions) == [('copy', Path('/src/fn1'), Path('/dst/fn1'))] ...\ndef test_when_a_file_has_been_renamed_in_the_source(): src_hashes = {'hash1': 'fn1'} dst_hashes = {'hash1': 'fn2'} actions = determine_actions(src_hashes, dst_hashes, Path('/src'), Path('/dst')) assert list(actions) == [('move', Path('/dst/fn2'), Path('/dst/fn1'))]\nidentifying changes—from the low-level details of I/O, we can easily test the core of our code.\nWith this approach, we’ve switched from testing our main entrypoint function, sync(), to testing a lower-level function, determine_actions().\nBut there’s another option, which is to modify the sync() function so it can be unit tested\ntesting.\ndef sync(reader, filesystem, source_root, dest_root):\nfor sha, filename in src_hashes.items(): if sha not in dest_hashes: sourcepath = source_root / filename destpath = dest_root / filename filesystem.copy(destpath, sourcepath)\nfor sha, filename in dst_hashes.items(): if sha not in source_hashes: filesystem.delete(dest_root/filename)\nAlthough we’re using dependency injection, there is no need to define an abstract base class or any kind of explicit interface.\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): source = {\"sha1\": \"my-file\" } dest = {} filesystem = FakeFileSystem()\nassert filesystem == [(\"COPY\", \"/source/my-file\", \"/dest/my-file\")]\ndef test_when_a_file_has_been_renamed_in_the_source(): source = {\"sha1\": \"renamed-file\" } dest = {\"sha1\": \"original-file\" } filesystem = FakeFileSystem()\nIt means we can write tests like assert foo not in database.\nThe advantage of this approach is that our tests act on the exact same function that’s used by our production code.\nPatching out the dependency you’re using makes it possible to unit test the code, but it does nothing to improve the design.\nTests that use mocks tend to be more coupled to the implementation details of the codebase.\nThat’s because mock tests verify the interactions between things: did we call shutil.copy with the right arguments?\nThis coupling between code and test tends to make tests more brittle, in our experience.\nOveruse of mocks leads to complicated test suites that fail to explain the code.\nFakes are working implementations of the thing they’re replacing, but they’re designed for use only in tests.\nWe like to build our tests around state both in setup and in assertions, and we like to work at the highest level of abstraction 3 possible rather than doing checks on the behavior of intermediary collaborators.\nTests that use too many mocks get overwhelmed with setup code that hides the story we care about.\nThe service layer that we build around it (in Chapter 4) allows us to drive the system edge to edge, and we use dependency injection to provide those services with stateful components, so we can still unit test them.\nWe’ll see this idea come up again and again in the book: we can make our systems easier to test and maintain by simplifying the interface",
      "keywords": [
        "dest",
        "source",
        "path",
        "hashes",
        "code",
        "file",
        "’re",
        "dst",
        "src",
        "def test",
        "filesystem",
        "actions",
        "abstraction",
        "folder",
        "copy"
      ],
      "concepts": [
        "tested",
        "abstractions",
        "abstraction",
        "abstracting",
        "paths",
        "mocks",
        "code",
        "dest",
        "file",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 13,
          "title": "",
          "score": 0.55,
          "base_score": 0.4,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.469,
          "base_score": 0.319,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.462,
          "base_score": 0.312,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.427,
          "base_score": 0.277,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 6,
          "title": "",
          "score": 0.331,
          "base_score": 0.331,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "path",
          "source",
          "dest",
          "file",
          "source_hashes"
        ],
        "semantic": [],
        "merged": [
          "path",
          "source",
          "dest",
          "file",
          "source_hashes"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31686420854789993,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533838+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Our First Use Case: Flask API and Service Layer",
      "start_page": 122,
      "end_page": 149,
      "summary": "Our First Use Case: Flask API and Service Layer\nIn this chapter, we discuss the differences between orchestration logic, business logic, and interfacing code, and we introduce the Service Layer pattern to take care of orchestrating our workflows and defining the use cases of our system.\nWe’ll also discuss testing: by combining the Service Layer with our repository abstraction over the database, we’re able to write fast tests, not just of our domain model but of the entire workflow for a use case.\nFigure 4-2 shows what we’re aiming for: we’re going to add a Flask API that will talk to the service layer, which will serve as the entrypoint to our domain model.\nBecause our service layer depends on the AbstractRepository, we can unit test it by using FakeRepository but run our production code using SqlAlchemyRepository.\nThe service layer will become the main way into our app\nWe have the core of our domain model and the domain service we need to allocate orders, and we have the repository interface for permanent storage.\n1. Use Flask to put an API endpoint in front of our allocate domain service.\n2. Refactor out a service layer that can serve as an abstraction to capture the use case and that will sit between Flask and our domain model.\nBuild some service-layer tests and show how they can use FakeRepository.\n3. Experiment with different types of parameters for our service layer functions; show that using primitive data types allows the service layer’s clients (our tests and our Flask API) to be decoupled from the model layer.\n@pytest.mark.usefixtures('restart_api') def test_api_returns_allocation(add_stock): sku, othersku = random_sku(), random_sku('other') earlybatch = random_batchref(1) laterbatch = random_batchref(2) otherbatch = random_batchref(3)\n@app.route(\"/allocate\", methods=['POST']) def allocate_endpoint(): session = get_session() batches = repository.SqlAlchemyRepository(session).list() line = model.OrderLine( request.json['orderid'], request.json['sku'], request.json['qty'], )\n@pytest.mark.usefixtures('restart_api') def test_allocations_are_persisted(add_stock): sku = random_sku() batch1, batch2 = random_batchref(1), random_batchref(2) order1, order2 = random_orderid(1), random_orderid(2) add_stock([ (batch1, sku, 10, '2011-01-01'), (batch2, sku, 10, '2011-01-02'), ]) line1 = {'orderid': order1, 'sku': sku, 'qty': 10} line2 = {'orderid': order2, 'sku': sku, 'qty': 10} url = config.get_api_url()\n@pytest.mark.usefixtures('restart_api') def test_400_message_for_out_of_stock(add_stock): sku, smalL_batch, large_order = random_sku(), random_batchref(), random_orderid() add_stock([ (smalL_batch, sku, 10, '2011-01-01'), ]) data = {'orderid': large_order, 'sku': sku, 'qty': 20} url = config.get_api_url() r = requests.post(f'{url}/allocate', json=data) assert r.status_code == 400 assert r.json()['message'] == f'Out of stock for sku {sku}'\n@pytest.mark.usefixtures('restart_api') def test_400_message_for_invalid_sku(): unknown_sku, orderid = random_sku(), random_orderid() data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20} url = config.get_api_url() r = requests.post(f'{url}/allocate', json=data) assert r.status_code == 400 assert r.json()['message'] == f'Invalid sku {unknown_sku}'\n@app.route(\"/allocate\", methods=['POST']) def allocate_endpoint(): session = get_session() batches = repository.SqlAlchemyRepository(session).list() line = model.OrderLine( request.json['orderid'], request.json['sku'], request.json['qty'], )\nIntroducing a Service Layer, and Using FakeRepository to Unit Test It\nOur fake repository, an in-memory collection of batches (test_services.py)\nHere’s where it will come in useful; it lets us test our service layer\nUnit testing with fakes at the service layer (test_services.py)\ndef test_returns_allocation(): line = model.OrderLine(\"o1\", \"COMPLICATED-LAMP\", 10) batch = model.Batch(\"b1\", \"COMPLICATED-LAMP\", 100, eta=None) repo = FakeRepository([batch])\ndef test_error_for_invalid_sku(): line = model.OrderLine(\"o1\", \"NONEXISTENTSKU\", 10) batch = model.Batch(\"b1\", \"AREALSKU\", 100, eta=None) repo = FakeRepository([batch])\nOur services module (services.py) will define an allocate() service-layer function.\nIt will sit between our allocate_endpoint() function in the API layer and the allocate() domain service function from our domain model.\nA fake database session (test_services.py)\nA second test at the service layer (test_services.py)\ndef test_commits(): line = model.OrderLine('o1', 'OMINOUS-MIRROR', 10) batch = model.Batch('b1', 'OMINOUS-MIRROR', 100, eta=None) repo = FakeRepository([batch]) session = FakeSession()\ndef allocate(line: OrderLine, repo: AbstractRepository, session) -> str: batches = repo.list() if not is_valid_sku(line.sku, batches): raise InvalidSku(f'Invalid sku {line.sku}') batchref = model.allocate(line, batches) session.commit() return batchref\nNotice one more thing about our service-layer function:\nIf you remember “The Dependency Inversion Principle”, this is what we mean when we say we should “depend on abstractions.” Our high-level module, the service layer, depends on the repository abstraction.\nBut the essentials of the service layer are there, and our Flask app now\nFlask app delegating to service layer (flask_app.py)\nline = model.OrderLine( request.json['orderid'], request.json['sku'], request.json['qty'], ) try: batchref = services.allocate(line, repo, session) except (model.OutOfStock, services.InvalidSku) as e: return jsonify({'message': str(e)}), 400\nAll the orchestration logic is in the use case/service layer, and the domain logic stays in the\n@pytest.mark.usefixtures('restart_api') def test_happy_path_returns_201_and_allocated_batch(add_stock): sku, othersku = random_sku(), random_sku('other') earlybatch = random_batchref(1) laterbatch = random_batchref(2) otherbatch = random_batchref(3)\n@pytest.mark.usefixtures('restart_api') def test_unhappy_path_returns_400_and_error_message(): unknown_sku, orderid = random_sku(), random_orderid() data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20} url = config.get_api_url() r = requests.post(f'{url}/allocate', json=data) assert r.status_code == 400 assert r.json()['message'] == f'Invalid sku {unknown_sku}'\nabout web stuff, which we implement end to end; and tests about orchestration stuff, which we can test against the service layer in memory.\nWe’ve added an E2E test and a few stub service-layer tests for you to get started on GitHub.\nNotice how doing so doesn’t require any change to our service layer or domain layer!\na service layer.\n├── service_layer │ ├── __init__.py │ └── services.py ├── adapters │ ├── __init__.py │ ├── orm.py │ └── repository.py ├── entrypoints │ ├── __init__.py │ └── flask_app.py └── tests ├── __init__.py ├── conftest.py ├── unit │ ├── test_allocate.py │ ├── test_batches.py │ └── test_services.py ├── integration │ ├── test_orm.py │ └── test_repository.py └── e2e └── test_api.py\nCurrently that’s just one file called services.py for our service-layer functions.\nYou could add service-layer exceptions here, and as you’ll see in Chapter 5, we’ll add unit_of_work.py.\nWe’ve defined a clear API for our domain, a set of use cases or entrypoints that can be used by any adapter without needing to know anything about our domain model classes—whether that’s an API, a CLI (see Appendix C), or the tests!\nWe can write tests in “high gear” by using the service layer, leaving us free to refactor the domain model in any way we see fit.\nFigure 4-3 shows the dependencies of our service layer: the domain model and AbstractRepository (the port, in ports and adapters terminology).\nAbstract dependencies of the service layer\nLet’s pause for Table 4-1, in which we consider the pros and cons of having a service layer at all.\nWhen combined with the Repository pattern and FakeRep ository, we have a nice way of writing tests at a higher level than the domain layer; we can test more of our workflow without needing to use integration tests (read on to Chapter 5 for more elaboration on this).\nThe service layer is still tightly coupled to the domain, because its API is expressed in terms of OrderLine objects.\nIn Chapter 5, we’ll fix that and talk about the way that the service layer enables more productive TDD.\nIn Chapter 6, we’ll introduce one more pattern that works closely with the Repository and Service Layer patterns, the Unit of Work pattern, and everything will be absolutely lovely.",
      "keywords": [
        "Service Layer",
        "Service",
        "sku",
        "Layer",
        "api def test",
        "Flask",
        "domain service",
        "API",
        "domain",
        "Flask API",
        "Flask app",
        "Service Layer Back",
        "domain model",
        "def test",
        "random"
      ],
      "concepts": [
        "tested",
        "layer",
        "batches",
        "batch",
        "service",
        "session",
        "repository",
        "repositories",
        "models",
        "allocations"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 13,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 6,
          "title": "",
          "score": 0.567,
          "base_score": 0.567,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 3,
          "title": "",
          "score": 0.469,
          "base_score": 0.319,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "layer",
          "service layer",
          "service",
          "sku",
          "json"
        ],
        "semantic": [],
        "merged": [
          "layer",
          "service layer",
          "service",
          "sku",
          "json"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44532471895293557,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533855+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "TDD in High Gear and Low Gear",
      "start_page": 150,
      "end_page": 163,
      "summary": "In this chapter we’ll discuss the trade-offs involved in moving those tests up to the service-layer level, and some\nOnce you implement domain modeling and the service layer, you really actually can get to a stage where unit tests outnumber integration and end-to-end tests by an order of magnitude.\nLet’s see what this move to using a service layer, with its own service- layer tests, does to our test pyramid:\n$ grep -c test_ test_*.py tests/unit/test_allocate.py:4 tests/unit/test_batches.py:8 tests/unit/test_services.py:3\nShould Domain Layer Tests Move to the Service Layer?\nSince we can test our software against the service layer, we don’t really need tests for the domain model anymore.\nInstead, we could rewrite all of the domain-level tests from Chapter 1 in terms of the service layer:\nRewriting a domain test at the service layer (tests/unit/test_services.py)\n# domain-layer test: def test_prefers_current_stock_batches_to_shipments(): in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow)\n# service-layer test: def test_prefers_warehouse_batches_to_shipments(): in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow) repo = FakeRepository([in_stock_batch, shipment_batch]) session = FakeSession()\nTesting against this API reduces the amount of code that we need to change when we refactor our domain model.\nIf we restrict ourselves to testing only against the service layer, we won’t have any tests that directly interact with “private” methods or attributes on our model objects, which leaves us freer to refactor them.\nIn these cases, we prefer to write tests against services because of the lower coupling and higher coverage.\nFor example, when writing an add_stock function or a cancel_order feature, we can work more quickly and with less coupling by writing tests against the service layer.\nFully Decoupling the Service-Layer Tests from the Domain\nWe still have direct dependencies on the domain in our service-layer tests, because we use domain objects to set up our test data and to\nTests now use primitives in function call (tests/unit/test_services.py)\ndef test_returns_allocation(): batch = model.Batch(\"batch1\", \"COMPLICATED-LAMP\", 100, eta=None) repo = FakeRepository([batch])\nBut our tests still depend on the domain, because we still manually instantiate Batch objects.\nour tests.\n(tests/unit/test_services.py)\ndef test_returns_allocation(): repo = FakeRepository.for_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, eta=None) result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, repo, FakeSession()) assert result == \"batch1\"\nIf we had a service to add stock, we could use that and make our service-layer tests fully expressed in\nTest for new add_batch service (tests/unit/test_services.py)\ndef test_add_batch(): repo, session = FakeRepository([]), FakeSession() services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, repo, session) assert repo.get(\"b1\") is not None assert session.committed\nIn general, if you find yourself needing to do domain-layer stuff directly in your service-layer tests, it may be an indication that your service layer is incomplete.\nShould you write a new service just because it would help remove dependencies from your tests?\nThat now allows us to rewrite all of our service-layer tests purely in\nServices tests now use only services (tests/unit/test_services.py)\ndef test_allocate_returns_allocation(): repo, session = FakeRepository([]), FakeSession() services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, repo, session) result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, repo, session) assert result == \"batch1\"\nOur service-layer tests depend on\nIn the same way that adding add_batch helped decouple our service- layer tests from the model, adding an API endpoint to add a batch would remove the need for the ugly add_stock fixture, and our E2E tests could be free of those hardcoded SQL queries and the direct\nOnce you have a service layer in place, you really can move the majority of your test coverage to unit tests and develop a healthy test\nWrite the bulk of your tests against the service layer\nDon’t be afraid to delete these tests if the functionality is later covered by tests at the service layer.\nIn an ideal world, you’ll have all the services you need to be able to test entirely against the service layer, rather than hacking state via repositories or the database.",
      "keywords": [
        "service layer",
        "service",
        "batch",
        "layer",
        "domain",
        "unit tests",
        "API",
        "def test",
        "repo",
        "TEST PYRAMID",
        "unit",
        "add",
        "eta",
        "sku",
        "session"
      ],
      "concepts": [
        "services",
        "batch",
        "batches",
        "session",
        "api",
        "eta",
        "changed",
        "change",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 13,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 1,
          "title": "",
          "score": 0.49,
          "base_score": 0.34,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 3,
          "title": "",
          "score": 0.427,
          "base_score": 0.277,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tests",
          "layer",
          "service layer",
          "service",
          "repo"
        ],
        "semantic": [],
        "merged": [
          "tests",
          "layer",
          "service layer",
          "service",
          "repo"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4272863774295474,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533873+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Unit of Work Pattern",
      "start_page": 164,
      "end_page": 184,
      "summary": "In this chapter we’ll introduce the final piece of the puzzle that ties together the Repository and Service Layer patterns: the Unit of Work pattern.\nstorage, the Unit of Work (UoW) pattern is our abstraction over the idea of atomic operations.\nlayer to start a session, it talks to the repository layer to initialize SQLAlchemyRepository, and it talks to the service layer to ask it to allocate.\nThe service collaborates with the UoW (we like to think of the UoW as being part of the service layer), but neither the service function itself nor Flask now needs to talk directly to the database.\nLet’s see the unit of work (or UoW, which we pronounce “you-wow”) in action.\nPreview of unit of work in action (src/allocation/service_layer/services.py)\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: batches = uow.batches.list() ...\nbatchref = model.allocate(line, batches) uow.commit()\nWhen we’re done, we commit or roll back our work, using the UoW.\ndef test_uow_can_retrieve_a_batch_and_allocate_to_it(session_factory): session = session_factory() insert_batch(session, 'batch1', 'HIPSTER-WORKBENCH', 100, None) session.commit()\nuow = unit_of_work.SqlAlchemyUnitOfWork(session_factory) with uow: batch = uow.batches.get(reference='batch1') line = model.OrderLine('o1', 'HIPSTER-WORKBENCH', 10) batch.allocate(line) uow.commit()\nWe initialize the UoW by using our custom session factory and get back a uow object to use in our with block.\nAbstract UoW context manager (src/allocation/service_layer/unit_of_work.py)\nThe Real Unit of Work Uses SQLAlchemy Sessions\nThe real SQLAlchemy UoW (src/allocation/service_layer/unit_of_work.py)\ndef commit(self): self.session.commit()\nFinally, we provide concrete commit() and rollback() methods that use our database session.\nFake Unit of Work for Testing\nHere’s how we use a fake UoW in our service-layer tests:\nFake UoW (tests/unit/test_services.py)\ndef test_add_batch(): uow = FakeUnitOfWork() services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, uow) assert uow.batches.get(\"b1\") is not None assert uow.committed\ndef test_allocate_returns_allocation(): uow = FakeUnitOfWork() services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, uow) result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, uow) assert result == \"batch1\" ...\nIn our tests, we can instantiate a UoW and pass it to our service layer, rather than passing a repository and a session.\nOur UoW is much simpler than a session, and we feel comfortable with the service layer being able to start and stop units of work.\nUsing the UoW in the Service Layer\nService layer using UoW (src/allocation/service_layer/services.py)\ndef add_batch( ref: str, sku: str, qty: int, eta: Optional[date], uow: unit_of_work.AbstractUnitOfWork ): with uow: uow.batches.add(model.Batch(ref, sku, qty, eta)) uow.commit()\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: batches = uow.batches.list() if not is_valid_sku(line.sku, batches): raise InvalidSku(f'Invalid sku {line.sku}') batchref = model.allocate(line, batches) uow.commit() return batchref\nOur service layer now has only the one dependency, once again on an abstract UoW.\nIntegration tests for rollback behavior (tests/integration/test_uow.py)\ndef test_rolls_back_uncommitted_work_by_default(session_factory): uow = unit_of_work.SqlAlchemyUnitOfWork(session_factory) with uow: insert_batch(uow.session, 'batch1', 'MEDIUM-PLINTH', 100, None)\ndef test_rolls_back_on_error(session_factory): class MyException(Exception): pass\nuow = unit_of_work.SqlAlchemyUnitOfWork(session_factory) with pytest.raises(MyException): with uow: insert_batch(uow.session, 'batch1', 'LARGE-FORK', 100, None) raise MyException()\nA UoW with implicit commit… (src/allocation/unit_of_work.py)\ndef add_batch(ref: str, sku: str, qty: int, eta: Optional[date], uow): with uow: uow.batches.add(model.Batch(ref, sku, qty, eta)) # uow.commit()\nHere are a few examples showing the Unit of Work pattern in use.\ndef reallocate(line: OrderLine, uow: AbstractUnitOfWork) -> str: with uow: batch = uow.batches.get(sku=line.sku) if batch is None: raise InvalidSku(f'Invalid sku {line.sku}') batch.deallocate(line) allocate(line) uow.commit()\ndef change_batch_quantity(batchref: str, new_qty: int, uow: AbstractUnitOfWork): with uow: batch = uow.batches.get(reference=batchref) batch.change_purchased_quantity(new_qty) while batch.available_quantity < 0: line = batch.deallocate_one() uow.commit()\n└── tests ├── conftest.py ├── e2e │ └── test_api.py ├── integration │ ├── test_orm.py │ ├── test_repository.py │ └── test_uow.py ├── pytest.ini └── unit ├── test_allocate.py ├── test_batches.py └── test_services.py\nThe code, as always, is on GitHub. You could either follow the model we have quite closely, or perhaps experiment with separating the UoW (whose responsibilities are commit(), rollback(), and providing the .batches repository) from the context manager, whose job is to initialize things, and then do the commit or rollback on exit.\nThis pattern is so useful, in fact, that SQLAlchemy already uses a UoW in the shape of the Session object.\nabstracting away the SQLAlchemy session if it already implements the pattern we want?\nThe Unit of Work pattern is an abstraction around data integrity\nIt works closely with the Repository and Service Layer patterns\nEach of our service-layer use cases runs in a single unit of work that succeeds or fails as a block.",
      "keywords": [
        "UoW",
        "session",
        "Unit",
        "Service Layer",
        "Unit of Work",
        "Work",
        "Service",
        "Layer",
        "Work Pattern",
        "commit",
        "sku",
        "batch",
        "context manager",
        "Repository",
        "factory"
      ],
      "concepts": [
        "batches",
        "batch",
        "session",
        "sessions",
        "commit",
        "committed",
        "code",
        "sqlalchemy",
        "unit",
        "allocate"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.567,
          "base_score": 0.567,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 13,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 8,
          "title": "",
          "score": 0.441,
          "base_score": 0.441,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.412,
          "base_score": 0.412,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.381,
          "base_score": 0.381,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "uow",
          "uow batches",
          "commit",
          "session_factory",
          "session"
        ],
        "semantic": [],
        "merged": [
          "uow",
          "uow batches",
          "commit",
          "session_factory",
          "session"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4251360060989764,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:57.533889+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Aggregates and Consistency Boundaries",
      "start_page": 185,
      "end_page": 222,
      "summary": "Aggregates and Consistency Boundaries\nIn this chapter, we’d like to revisit our domain model to talk about invariants and constraints, and see how our domain objects can maintain their own internal consistency, both conceptually and in\nFigure 7-1 shows a preview of where we’re headed: we’ll introduce a new model object called Product to wrap multiple batches, and we’ll make the old allocate() domain service available as a method on Product instead.\nAdding the Product aggregate\nAn order line can be allocated to only one batch at a time.\nThe invariant is that an order line is allocated to either zero or one batch, but never more than one.\nWe need to make sure that our code never accidentally calls Batch.allocate() on two different batches for the same line, and currently, there’s nothing there to explicitly stop us from doing that.\nWe can’t allocate to a batch if the available quantity is less than the quantity of the order line.\nEvery time we update the state of the system, our code needs to ensure that we don’t break the invariant, which is that the available quantity must be greater than or equal to zero.\nWe might even be allocating order lines at the same time as processing changes to the batches themselves.\nAs we start to think about scaling up our app, we realize that our model of allocating lines against all available batches may not scale.\nIf we process tens of thousands of orders per hour, and hundreds of thousands of order lines, we can’t hold a lock over the whole batches table for every single one—we’ll get deadlocks or performance problems at the very least.\nOK, so if we can’t lock the whole database every time we want to allocate an order line, what should we do instead?\nMaintaining our invariants inevitably means preventing concurrent writes; if multiple users can allocate DEADLY-SPOON at the same time, we run the risk of overallocating.\nIt’s safe to allocate two products at the same time because there’s no invariant that covers them both.\nAn aggregate is just a domain object that contains other domain objects and lets us treat the whole collection as a single unit.\nJust as we sometimes use _leading_underscores to mark methods or functions as “private,” you can think of aggregates as being the “public” classes of our model, and the rest of the entities and value objects as “private.”\nOr perhaps we could use Warehouse as our boundary: each warehouse contains many batches, and counting all the stock at the same time could make sense.\nWhen we allocate an order line, we’re interested only in batches that have the same SKU as the order line.\nSo the plan is this: when we want to allocate an order line, instead of Figure 7-2, where we look up all the Batch objects in the world and pass them to the allocate() domain service…\nBefore: allocate against all batches using the domain service\n…we’ll move to the world of Figure 7-3, in which there is a new Product object for the particular SKU of our order line, and it will be in charge of all the batches for that SKU, and we can call a .allocate() method on that instead.\nAfter: ask Product to allocate against its batches\nOur chosen aggregate, Product (src/allocation/domain/model.py)\ndef allocate(self, line: OrderLine) -> str: try: batch = next( b for b in sorted(self.batches) if b.can_allocate(line) ) batch.allocate(line) return batch.reference except StopIteration: raise OutOfStock(f'Out of stock for sku {line.sku}')\nOur Product class holds a reference to a collection of batches for that SKU.\nFinally, we can move the allocate() domain service to be a method on the Product aggregate.\nRather than trying to build a single model (or class, or database) to capture all the use cases, it’s better to have several models, draw boundaries around each context, and handle the translation between different contexts explicitly.\nIn our example, the allocation service has Product(sku, batches), whereas the ecommerce will have Product(sku, description, price, image_url, dimensions, etc...).\nAs a rule of thumb, your domain models should include only the data that they need for performing calculations.\nThe rule that repositories should only return aggregates is the main place where we enforce the convention that aggregates are the only way into our domain model.\nThe ORM layer will need some tweaks so that the right batches automatically get loaded and associated with Product objects.\nWe can just use our FakeRepository and then feed through the new model into our service layer to see how it looks with Product as its main entrypoint:\ndef add_batch( ref: str, sku: str, qty: int, eta: Optional[date], uow: unit_of_work.AbstractUnitOfWork ): with uow: product = uow.products.get(sku=sku) if product is None: product = model.Product(sku, batches=[]) uow.products.add(product) product.batches.append(model.Batch(ref, sku, qty, eta)) uow.commit()\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: product = uow.products.get(sku=line.sku) if product is None: raise InvalidSku(f'Invalid sku {line.sku}') batchref = product.allocate(line) uow.commit() return batchref\nWe’ve mentioned a few times that we’re modeling with aggregates because we want to have high-performance software, but here we are loading all the batches when we only need one.\nFirst, we’re purposefully modeling our data so that we can make a single query to the database to read, and a single update to persist our\nThird, we expect to have only 20 or so batches of each product at a time.\nYou’ve just seen the main top layers of the code, so this shouldn’t be too hard, but we’d like you to implement the Product aggregate starting from Batch, just as we did.\nThe Aggregate pattern is designed to help manage some technical constraints around consistency and performance.\nWe don’t want to hold a lock over the entire batches table, but how will we implement holding a lock over just the rows for a particular SKU?\nOne answer is to have a single attribute on the Product model that acts as a marker for the whole state change being complete and to use\nIf two transactions read the state of the world for batches at the same time, and both want to update the allocations tables, we force both to also try to update the version_number in the products table, in such a way that only one of them can win and the world stays consistent.\nFigure 7-4 illustrates two concurrent transactions doing their read operations at the same time, so they see a Product with, for example, version=3.\nThey both call Product.allocate() in order to modify a state.\nBut we set up our database integrity rules such that only one of them is allowed to commit the new Product with version=4, and the other update is rejected.\nWhat we’ve implemented here is called optimistic concurrency control because our default assumption is that everything will be fine when two users want to make changes to the database.\nIn our example, that would mean locking the whole batches table, or using SELECT FOR UPDATE— we’re pretending that we’ve ruled those out for performance reasons, but in real life you’d want to do some evaluations and measurements of your own.\nBoth threads load the product at version 1 and allocate stock.\nWhen we retry the operation, Bob’s order loads the product at version 2 and tries to allocate again.\n1. version_number lives in the domain; we add it to the Product constructor, and Product.allocate() is responsible for incrementing it.\nThe version number isn’t strictly a domain concern, so instead our service layer could assume that the current version number is attached to Product by the repository, and the service layer will increment it before it does the commit().\nThe repository has access to version numbers for any products it retrieves, and when the UoW does a commit, it can increment the version number for any products it knows about, assuming them to have changed.\nhaving to assume that all products have changed, so we’ll be incrementing version numbers when we don’t have to.\nOur chosen aggregate, Product (src/allocation/domain/model.py)\ndef allocate(self, line: OrderLine) -> str: try: batch = next( b for b in sorted(self.batches) if b.can_allocate(line) ) batch.allocate(line) self.version_number += 1 return batch.reference\nWhat’s important is that the Product database row is modified whenever we make a change to the Product aggregate.\nThe version number is a simple, human-comprehensible way to model a thing that changes on every write, but it could equally be a random UUID every time.\nNow to make sure we can get the behavior we want: if we have two concurrent attempts to do allocation against the same Product, one of them should fail, because they can’t both update the version number.\ndef try_to_allocate(orderid, sku, exceptions): line = model.OrderLine(orderid, sku, 10) try: with unit_of_work.SqlAlchemyUnitOfWork() as uow: product = uow.products.get(sku=sku) product.allocate(line) time.sleep(0.2)\ndef test_concurrent_updates_to_version_are_not_allowed(postgres_session_factory): sku, batch = random_sku(), random_batchref() session = postgres_session_factory() insert_batch(session, batch, sku, 100, eta=None, product_version=1) session.commit()\n[[version]] = session.execute( \"SELECT version_number FROM products WHERE sku=:sku\", dict(sku=sku), ) assert version == 2 [exception] = exceptions assert 'could not serialize access due to concurrent update' in str(exception)\norders = list(session.execute( \"SELECT orderid FROM allocations\" \" JOIN batches ON allocations.batch_id = batches.id\" \" JOIN order_lines ON allocations.orderline_id = order_lines.id\" \" WHERE order_lines.sku=:sku\", dict(sku=sku), )) assert len(orders) == 1 with unit_of_work.SqlAlchemyUnitOfWork() as uow: uow.session.execute('select 1')\nSELECT FOR UPDATE produces different behavior; two concurrent transactions will not be allowed to do a read on the same rows at the same time:\ndef get(self, sku): return self.session.query(model.Product) \\ .filter_by(sku=sku) \\ .with_for_update() \\ .first()\nbusiness circumstances and storage technology choices, but we’d like to bring this chapter back to the conceptual idea of an aggregate: we explicitly model an object as being the main entrypoint to some subset of our model, and as being in charge of enforcing the invariants and business rules that apply across all of those objects.\nChoosing aggregates is just the next level up: it lets you decide which of your domain model classes are the public ones, and which aren’t.\nModeling our operations around explicit consistency boundaries helps us avoid performance problems with our ORM.\nPutting the aggregate in sole charge of state changes to its subsidiary models makes the system easier to reason about, and makes it easier to control invariants.\nAGGREGATES AND CONSISTENCY BOUNDARIES RECAP\nAggregates are your entrypoints into the domain model\nAggregates are in charge of a consistency boundary\nAn aggregate’s job is to be able to manage our business rules about invariants as they apply to a group of related objects.\nIt’s the aggregate’s job to check that the objects within its remit are consistent with each other and with our rules, and to reject changes that would break the rules.\nWe’ve seen how to build a domain model that’s exercised by a set of high-level unit tests.\nWe don’t want to lock our entire system whenever we make a change, so we have to choose which parts are consistent with one another.\nIn Part II, we’ll zoom out and talk about a bigger topic: if aggregates are our boundary, and we can update only one at a time, how do we model processes that cross consistency boundaries?\n2 time.sleep() works well in our use case, but it’s not the most reliable or efficient way to reproduce concurrency bugs.\nWe’ll see how our Service Layer and Unit of Work patterns allow us to reconfigure our app to run as an asynchronous message processor, and how event-driven systems help us to decouple aggregates and applications from one another.",
      "keywords": [
        "Product",
        "SKU",
        "aggregate",
        "Product aggregate",
        "n’t",
        "batches",
        "version",
        "allocate",
        "domain",
        "version numbers",
        "domain model",
        "model",
        "time",
        "update",
        "number"
      ],
      "concepts": [
        "batches",
        "batch",
        "aggregates",
        "concurrency",
        "concurrent",
        "allocate",
        "allocated",
        "allocation",
        "allocations",
        "product"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 9,
          "title": "",
          "score": 0.746,
          "base_score": 0.596,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 10,
          "title": "",
          "score": 0.455,
          "base_score": 0.305,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.454,
          "base_score": 0.454,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "product",
          "sku",
          "version",
          "batches",
          "aggregate"
        ],
        "semantic": [],
        "merged": [
          "product",
          "sku",
          "version",
          "batches",
          "aggregate"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.42842552584163146,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533905+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Events and the Message Bus",
      "start_page": 223,
      "end_page": 246,
      "summary": "Events and the Message Bus\nWe’ll start by doing the simplest, most expeditious thing, and talk about why it’s exactly this kind of decision that leads us to the Big Ball of Mud. Then we’ll show how to use the Domain Events pattern to separate side effects from our use cases, and how to use a simple Message Bus pattern for triggering behavior based on those events.\nThe code for this chapter is in the chapter_08_events_and_message_bus branch on GitHub:\nrequest.json['qty'], ) try: uow = unit_of_work.SqlAlchemyUnitOfWork() batchref = services.allocate(line, uow) except (model.OutOfStock, services.InvalidSku) as e: send_mail( 'out of stock', 'stock_admin@made.com', f'{line.orderid} - {line.sku}' ) return jsonify({'message': str(e)}), 400\nEmail-sending code in our model isn’t lovely either (src/allocation/domain/model.py)\nline = OrderLine(orderid, sku, qty) with uow: product = uow.products.get(sku=line.sku) if product is None: raise InvalidSku(f'Invalid sku {line.sku}') try: batchref = product.allocate(line) uow.commit() return batchref except model.OutOfStock: email.send_mail('stock@made.com', f'Out of stock for {line.sku}') raise\nThe Model Records Events\nWe’ll use a message bus to respond to events and invoke a new operation.\nWe always name events in the\ndomain/events.py):\nEvent classes (src/allocation/domain/events.py)\nThe Model Raises Events\nan event.\nHere’s what it will look like from the outside; if we ask Product to allocate but it can’t, it should raise an event:\nTest our aggregate to raise events (tests/unit/test_product.py)\ndef test_records_out_of_stock_event_if_cannot_allocate(): batch = Batch('batch1', 'SMALL-FORK', 10, eta=today) product = Product(sku=\"SMALL-FORK\", batches=[batch]) product.allocate(OrderLine('order1', 'SMALL-FORK', 10))\nThe model raises a domain event (src/allocation/domain/model.py)\nexcept StopIteration: self.events.append(events.OutOfStock(line.sku)) # raise OutOfStock(f'Out of stock for sku {line.sku}') return None\nIn general, if you’re implementing domain events, don’t raise exceptions to describe the same domain concept.\nAs you’ll see later when we handle events in the Unit of Work pattern, it’s confusing to have to reason about events and exceptions together.\nThe Message Bus Maps Events to Handlers\nA message bus basically says, “When I see this event, I should invoke the following handler function.” In other words, it’s a simple publish-\nHandlers are subscribed to receive events, which we publish to the bus.\nSimple message bus (src/allocation/service_layer/messagebus.py)\ndef send_out_of_stock_notification(event: events.OutOfStock): email.send_mail( 'stock@made.com', f'Out of stock for {event.sku}', )\nOption 1: The Service Layer Takes Events from the Model and Puts Them on the Message Bus\nOur domain model raises events, and our message bus will call the\nWe need something to catch events from the model\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: product = uow.products.get(sku=line.sku) if product is None: raise InvalidSku(f'Invalid sku {line.sku}') try: batchref = product.allocate(line) uow.commit() return batchref finally: messagebus.handle(product.events)\nBut now, instead of depending directly on an email infrastructure, the service layer is just in charge of passing events from the model up to the message bus.\nimplementation, and we have several systems that work like this one, in which the service layer explicitly collects events from aggregates and passes them to the message bus.\nOption 2: The Service Layer Raises Its Own Events\nAnother variant on this that we’ve used is to have the service layer in charge of creating and raising events directly, rather than having them raised by the domain model:\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: product = uow.products.get(sku=line.sku) if product is None: raise InvalidSku(f'Invalid sku {line.sku}') batchref = product.allocate(line) uow.commit()\nOption 3: The UoW Publishes Events to the Message Bus\nSo it’s a good place to spot events and pass them to the message bus:\nThe UoW meets the message bus (src/allocation/service_layer/unit_of_work.py)\ndef commit(self): self._commit() self.publish_events()\ndef publish_events(self): for product in self.products.seen: while product.events: event = product.events.pop(0) messagebus.handle(event)\nAfter committing, we run through all the objects that our repository has seen and pass their events to the message bus.\n@abc.abstractmethod def _get(self, sku) -> model.Product: raise NotImplementedError\nFor the UoW to be able to publish new events, it needs to be able to ask the repository for which Product objects have been used during this session.\nAfter the UoW and repository collaborate in this way to automatically keep track of live objects and process their events, the service layer can be totally free of event-handling concerns:\ndef allocate( orderid: str, sku: str, qty: int, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(orderid, sku, qty) with uow: product = uow.products.get(sku=line.sku) if product is None: raise InvalidSku(f'Invalid sku {line.sku}') batchref = product.allocate(line) uow.commit() return batchref\nDomain events are a great way to model the real world, and we can use them as part of our business language when modeling with stakeholders.\nThe message bus is an additional thing to wrap your head around; the implementation in which the unit of work raises events for us is neat but also magic.\nWhat’s more, that hidden event- handling code executes synchronously, meaning your service-layer function doesn’t finish until all the handlers for any events are finished.\nEvents are useful for more than just sending email, though.\nDOMAIN EVENTS AND THE MESSAGE BUS RECAP\nYou can think of a message bus as a dict that maps from events to their consumers.\nOption 1: Service layer raises events and passes them to message bus\nThe simplest way to start using events in your system is to raise them from handlers by calling bus.handle(some_new_event) after you commit your unit of work.\nOption 2: Domain model raises events, service layer passes them to message bus\nThe logic about when to raise an event really should live with the model, so we can improve our system’s design and testability by raising events from the domain model.\nIt’s easy for our handlers to collect events off the model objects after commit and pass them to the bus.\nOption 3: UoW collects events from aggregates and passes them to message bus\nAdding bus.handle(aggregate.events) to every handler is annoying, so we can tidy up by making our unit of work responsible for raising events that were raised by loaded objects.",
      "keywords": [
        "Message Bus",
        "Events",
        "Service Layer",
        "Message",
        "Bus",
        "Domain Events",
        "domain model",
        "Domain",
        "product",
        "Model Raises Events",
        "Service",
        "sku",
        "Model",
        "layer",
        "Raises Events"
      ],
      "concepts": [
        "events",
        "product",
        "production",
        "allocate",
        "allocation",
        "allocated",
        "allocations",
        "model",
        "classes",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 10,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 9,
          "title": "",
          "score": 0.464,
          "base_score": 0.464,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.452,
          "base_score": 0.452,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.448,
          "base_score": 0.448,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 6,
          "title": "",
          "score": 0.441,
          "base_score": 0.441,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "events",
          "bus",
          "product",
          "message bus",
          "sku"
        ],
        "semantic": [],
        "merged": [
          "events",
          "bus",
          "product",
          "message bus",
          "sku"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40245801101242573,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:57.533920+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Going to Town on the Message Bus",
      "start_page": 247,
      "end_page": 275,
      "summary": "An event we’ll call BatchQuantityChanged should lead us to change the quantity on the batch, yes, but also to apply a business rule: if the new quantity drops to less than the total already allocated, we need to deallocate those orders from that batch.\nThen each one will require a new allocation, which we can capture as an event called AllocationRequired.\nPerhaps you’re already anticipating that our internal message bus and events can help implement this requirement.\nWe could define a service called change_batch_quantity that knows how to adjust batch quantities and also how to deallocate any excess order lines, and then each deallocation can emit an AllocationRequired event that can be forwarded to the existing allocate service, in separate transactions.\nImagining an Architecture Change: Everything Will Be an Event Handler\nInternal events (which might be raised as a side effect of a service-layer function) and their handlers (which in turn call service-layer functions)\nour API calls as capturing events, the service-layer functions can be event handlers too, and we no longer need to make a distinction\nservices.allocate() could be the handler for an AllocationRequired event and could emit Allocated events as its output.\nservices.add_batch() could be the handler for a BatchCreated event.\nAn event called BatchQuantityChanged can invoke a handler called change_batch_quantity().\nAnd the new AllocationRequired events that it may raise can be passed on to services.allocate() too, so there is no conceptual difference between a brand-new allocation coming from the API and a reallocation that’s internally triggered by a deallocation.\n1. We refactor our service layer into event handlers.\nIn particular, the existing services.allocate() function will become the handler for an event called AllocationRequired.\n3. Our implementation will conceptually be very simple: a new handler for BatchQuantityChanged events, whose implementation will emit AllocationRequired events, which in turn will be handled by the exact same handler for allocations that the API uses.\nmoving the responsibility for putting new events on the message bus\ndef add_batch( event: events.BatchCreated, uow: unit_of_work.AbstractUnitOfWork ): with uow: product = uow.products.get(sku=event.sku) ...\ndef allocate( event: events.AllocationRequired, uow: unit_of_work.AbstractUnitOfWork ) -> str: line = OrderLine(event.orderid, event.sku, event.qty) ...\ndef add_batch( - ref: str, sku: str, qty: int, eta: Optional[date], - uow: unit_of_work.AbstractUnitOfWork + event: events.BatchCreated, uow: unit_of_work.AbstractUnitOfWork ): with uow: - product = uow.products.get(sku=sku) + product = uow.products.get(sku=event.sku) ...\ndef allocate( - orderid: str, sku: str, qty: int, - uow: unit_of_work.AbstractUnitOfWork + event: events.AllocationRequired, uow: unit_of_work.AbstractUnitOfWork ) -> str: - line = OrderLine(orderid, sku, qty) + line = OrderLine(event.orderid, event.sku, event.qty) ...\nThe Message Bus Now Collects Events from the UoW\nOur event handlers now need a UoW.\ndef handle(event: events.Event, uow: unit_of_work.AbstractUnitOfWork): queue = [event] while queue: event = queue.pop(0) for handler in HANDLERS[type(event)]: handler(event, uow=uow) queue.extend(uow.collect_new_events())\nAfter each handler finishes, we collect any new events that have been generated and add them to the queue.\ndef publish_events(self): + def collect_new_events(self): for product in self.products.seen: while product.events: - event = product.events.pop(0) - messagebus.handle(event) + yield product.events.pop(0)\nAnd the UoW no longer actively puts events on the message bus; it just makes them available.\nOur tests now operate by creating events and putting them on the message bus, rather than invoking service-layer functions directly:\nHandler tests use events (tests/unit/test_handlers.py)\ndef test_for_new_product(self): uow = FakeUnitOfWork() - services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, uow) + messagebus.handle( + events.BatchCreated(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None), uow + ) assert uow.products.get(\"CRUNCHY-ARMCHAIR\") is not None\ndef test_returns_allocation(self): uow = FakeUnitOfWork() - services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, uow) - result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, uow) + messagebus.handle( + events.BatchCreated(\"batch1\", \"COMPLICATED-LAMP\", 100, None), uow + ) + result = messagebus.handle( + events.AllocationRequired(\"o1\", \"COMPLICATED-LAMP\", 10), uow + ) assert result == \"batch1\"\nThis means we need to put in a temporary hack on our message bus to let it return events:\nMessage bus returns results (src/allocation/service_layer/messagebus.py)\ndef handle(event: events.Event, uow: unit_of_work.AbstractUnitOfWork): + results = [] queue = [event] while queue: event = queue.pop(0) for handler in HANDLERS[type(event)]: - handler(event, uow=uow) + results.append(handler(event, uow=uow)) queue.extend(uow.collect_new_events()) + return results\n@app.route(\"/allocate\", methods=['POST']) def allocate_endpoint(): try: - batchref = services.allocate( - request.json['orderid'], - request.json['sku'], - request.json['qty'], - unit_of_work.SqlAlchemyUnitOfWork(), + event = events.AllocationRequired( + request.json['orderid'], request.json['sku'], request.json['qty'], ) + results = messagebus.handle(event, unit_of_work.SqlAlchemyUnitOfWork())\nWhat used to be service-layer functions are now event handlers.\nin Figure 9-4: we’ll receive as our inputs some new BatchQuantityChanged events and pass them to a handler, which in turn might emit some AllocationRequired events, and those in turn will go back to our existing handler for reallocation.\nOur New Event\nThe event that tells us a batch quantity has changed is simple; it just needs a batch reference and a new quantity:\nNew event (src/allocation/domain/events.py)\ndef test_changes_available_quantity(self): uow = FakeUnitOfWork() messagebus.handle( events.BatchCreated(\"batch1\", \"ADORABLE-SETTEE\", 100, None), uow ) [batch] = uow.products.get(sku=\"ADORABLE-SETTEE\").batches assert batch.available_quantity == 100\nmessagebus.handle(events.BatchQuantityChanged(\"batch1\", 50), uow)\ndef test_reallocates_if_necessary(self): uow = FakeUnitOfWork() event_history = [ events.BatchCreated(\"batch1\", \"INDIFFERENT-TABLE\", 50, None), events.BatchCreated(\"batch2\", \"INDIFFERENT-TABLE\", 50, date.today()), events.AllocationRequired(\"order1\", \"INDIFFERENT-TABLE\", 20), events.AllocationRequired(\"order2\", \"INDIFFERENT-TABLE\", 20), ] for e in event_history: messagebus.handle(e, uow) [batch1, batch2] = uow.products.get(sku=\"INDIFFERENT-TABLE\").batches assert batch1.available_quantity == 10 assert batch2.available_quantity == 50\nmessagebus.handle(events.BatchQuantityChanged(\"batch1\", 25), uow)\ndef change_batch_quantity( event: events.BatchQuantityChanged, uow: unit_of_work.AbstractUnitOfWork ): with uow: product = uow.products.get_by_batchref(batchref=event.ref) product.change_batch_quantity(ref=event.ref, qty=event.qty) uow.commit()\nWe add the new method to the model, which does the quantity change and deallocation(s) inline and publishes a new event.\ndef change_batch_quantity(self, ref: str, qty: int): batch = next(b for b in self.batches if b.reference == ref) batch._purchased_quantity = qty while batch.available_quantity < 0: line = batch.deallocate_one() self.events.append( events.AllocationRequired(line.orderid, line.sku, line.qty) ) ...\nHANDLERS = { events.BatchCreated: [handlers.add_batch], events.BatchQuantityChanged: [handlers.change_batch_quantity], events.AllocationRequired: [handlers.allocate], events.OutOfStock: [handlers.send_out_of_stock_notification],\nOptionally: Unit Testing Event Handlers in Isolation with a Fake Message Bus\nIt uses the real message bus, and it tests the whole flow, where the BatchQuantityChanged event handler triggers deallocation, and emits new AllocationRequired events, which in turn are handled by their own handlers.\nDepending on the complexity of your chain of events, you may decide that you want to test some handlers in isolation from one another.\nIn our case, we actually intervene by modifying the publish_events() method on FakeUnitOfWork and decoupling it from the real message bus, instead making it record what events it\nFake message bus implemented in UoW (tests/unit/test_handlers.py)\nNow when we invoke messagebus.handle() using the FakeUnitOfWorkWithFakeMessageBus, it runs only the handler for that event.\nevents.AllocationRequired(\"order2\", \"INDIFFERENT-TABLE\", 20), ] for e in event_history: messagebus.handle(e, uow) [batch1, batch2] = uow.products.get(sku=\"INDIFFERENT-TABLE\").batches assert batch1.available_quantity == 10 assert batch2.available_quantity == 50\nmessagebus.handle(events.BatchQuantityChanged(\"batch1\", 25), uow)\nclass MessageBus(AbstractMessageBus): HANDLERS = { events.OutOfStock: [send_out_of_stock_notification],\nclass FakeMessageBus(messagebus.AbstractMessageBus): def __init__(self): self.events_published = [] # type: List[events.Event] self.handlers = { events.OutOfStock: [lambda e: self.events_published.append(e)] }\nHandlers can also raise other events.",
      "keywords": [
        "Message Bus",
        "event",
        "Message",
        "UoW",
        "Handler",
        "Bus",
        "event handlers",
        "batch",
        "quantity",
        "unit",
        "sku",
        "change",
        "API",
        "Fake Message Bus"
      ],
      "concepts": [
        "event",
        "handler",
        "batch",
        "classes",
        "allocated",
        "allocation",
        "allocate",
        "allocations",
        "chapters",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 7,
          "title": "",
          "score": 0.746,
          "base_score": 0.596,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 10,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 8,
          "title": "",
          "score": 0.464,
          "base_score": 0.464,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "events",
          "event",
          "uow",
          "allocationrequired",
          "handler"
        ],
        "semantic": [],
        "merged": [
          "events",
          "event",
          "uow",
          "allocationrequired",
          "handler"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4598123376071418,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533936+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Commands and Command Handler",
      "start_page": 276,
      "end_page": 290,
      "summary": "Commands and Events\nLike events, commands are a type of message—instructions sent by one part of a system to another.\nWe usually represent commands with dumb data structures and can handle them in much the same way as events.\nThe differences between commands and events, though, are important.\ncommands.\ndon’t know who’s handling an event, senders should not care whether the receivers succeeded or failed.\nEvents versus commands\nEvent\nCommand\ncommands.Allocate will replace events.AllocationRequired.\ncommands.CreateBatch will replace events.BatchCreated.\ncommands.ChangeBatchQuantity will replace events.BatchQuantityChanged.\nWe want to treat events and commands similarly, but not exactly the same.\nDispatch events and commands differently (src/allocation/service_layer/messagebus.py)\nMessage = Union[commands.Command, events.Event]\ndef handle(message: Message, uow: unit_of_work.AbstractUnitOfWork): results = [] queue = [message] while queue: message = queue.pop(0) if isinstance(message, events.Event): handle_event(message, queue, uow) elif isinstance(message, commands.Command): cmd_result = handle_command(message, queue, uow) results.append(cmd_result) else: raise Exception(f'{message} was not an Event or Command') return results\nIt still has a main handle() entrypoint that takes a message, which may be a command or an event.\nWe dispatch events and commands to two different helper functions, shown next.\ndef handle_event( event: events.Event, queue: List[Message], uow: unit_of_work.AbstractUnitOfWork ): for handler in EVENT_HANDLERS[type(event)]: try: logger.debug('handling event %s with handler %s', event, handler) handler(event, uow=uow) queue.extend(uow.collect_new_events()) except Exception: logger.exception('Exception handling event %s', event) continue\nuow: unit_of_work.AbstractUnitOfWork ): logger.debug('handling command %s', command) try: handler = COMMAND_HANDLERS[type(command)] result = handler(command, uow=uow) queue.extend(uow.collect_new_events()) return result except Exception: logger.exception('Exception handling command %s', command) raise\nWe also change the single HANDLERS dict into different ones for commands and events.\nDiscussion: Events, Commands, and Error Handling\nHow am I supposed to make sure the system is in a consistent state?” If we manage to process half of the events during messagebus.handle before an out-of-memory error kills our process, how do we mitigate problems caused by the lost messages?\nLet’s start with the worst case: we fail to handle an event, and the\nBy definition, we don’t require two aggregates to be immediately consistent, so if we fail to process an event and update only a single\nsplitting messages into commands and events.\ncommand.\nWe don’t require the event handlers to succeed in order for the command to be\nUsing the techniques we’ve already discussed in this book, we decide that we want to build a new History aggregate that records orders and can raise domain events when rules are met.\ndef update_customer_history(uow, event: OrderCreated): with uow: history = uow.order_history.get(event.customer_id) history.record_order(event.order_id, event.order_amount) uow.commit() # raises CustomerBecameVIP\nOur first handler creates an order for the customer and raises a domain event OrderCreated.\nUsing this code, we can gain some intuition about error handling in an event-driven system.\nHopefully we’ve convinced you that it’s OK for events to fail independently from the commands that raised them.\nLet’s look again at the handle_event method from our message bus:\ndef handle_event( event: events.Event, queue: List[Message], uow: unit_of_work.AbstractUnitOfWork ): for handler in EVENT_HANDLERS[type(event)]: try: logger.debug('handling event %s with handler %s', event, handler) handler(event, uow=uow) queue.extend(uow.collect_new_events()) except Exception: logger.exception('Exception handling event %s', event) continue\nHandling event CustomerBecameVIP(customer_id=12345) with handler <function congratulate_vip_customer at 0x10ebc9a60>\ndef handle_event( event: events.Event, queue: List[Message], uow: unit_of_work.AbstractUnitOfWork ):\nwith attempt: logger.debug('handling event %s with handler %s', event, handler) handler(event, uow=uow) queue.extend(uow.collect_new_events()) except RetryError as retry_failure: logger.error( 'Failed to handle event %s times, giving up!, retry_failure.last_attempt.attempt_number ) continue\nAgain, the Unit of Work and Command Handler patterns mean that each attempt starts from a consistent state and won’t leave things half-finished.\nIn this book we decided to introduce the concept of events before the concept of commands, but other guides often do it the other way\nYou’ll sometimes see people use the name Command Handler pattern to describe what we’re doing with Events,\nSplitting commands and events: the trade-offs\nTreating commands and events differently helps us understand which things have to succeed and which things we can tidy up later.\nThe semantic differences between commands and events can be subtle.",
      "keywords": [
        "event",
        "Commands",
        "message",
        "Handler",
        "commands Event Command",
        "Command Handler",
        "Exception handling event",
        "handling event",
        "uow",
        "message bus",
        "handle",
        "order",
        "customer",
        "system",
        "Exception Handling"
      ],
      "concepts": [
        "commands",
        "events",
        "handler",
        "message",
        "handles",
        "customer",
        "different",
        "differences",
        "orders",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 9,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 8,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 7,
          "title": "",
          "score": 0.455,
          "base_score": 0.305,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "event",
          "commands",
          "events",
          "command",
          "uow"
        ],
        "semantic": [],
        "merged": [
          "event",
          "commands",
          "events",
          "command",
          "uow"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40459454267512884,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533950+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Event-Driven Architecture: Using Events to Integrate Microservices",
      "start_page": 291,
      "end_page": 309,
      "summary": "application will receive events from external sources via an external message bus (we’ll use Redis pub/sub queues as an example) and publish its outputs, in the form of events, back there as well.\nat breaking up the system might have looked like Figure 11-2 (notice that we’ve named our system after a noun, Batches, instead of Allocation).\nallocate stock, the Orders service drives the Batches system, which drives Warehouse; but in order to handle problems at the warehouse, our Warehouse system drives Batches, which drives Orders.\nIn our first example of a distributed ball of mud, we see Connascence of Execution: multiple components need to know the correct order of work for an operation to be successful.\nWhen thinking about error conditions here, we’re talking about Connascence of Timing: multiple things have to happen, one after another, for the operation to work.\nbatches, we think about a system for ordering and a system for allocating, and so on.\nOther services can listen to those events to trigger the next\ntemporally coupled HTTP API calls, we want to use asynchronous messaging to integrate our systems.\nWe want our BatchQuantityChanged messages to come in as external messages from upstream systems, and we want our system to publish Allocated events for downstream systems to listen to.\nWe’ll need some way of getting events out of one system and into another, like our message bus,\nConcerns like message ordering, failure handling, and idempotency all need to be thought through.\nOur new flow will look like Figure 11-6: Redis provides the BatchQuantityChanged event that kicks off the whole process, and our Allocated event is published back out to Redis again at the end.\nexisting API to create batches, and then we’ll test both inbound and outbound messages:\n(tests/e2e/test_external_events.py)\ndef test_change_batch_quantity_leading_to_reallocation(): # start with two batches and an order allocated to one of them orderid, sku = random_orderid(), random_sku() earlier_batch, later_batch = random_batchref('old'), random_batchref('newer') api_client.post_to_add_batch(earlier_batch, sku, qty=10, eta='2011-01-02')\napi_client.post_to_add_batch(later_batch, sku, qty=10, eta='2011-01-02') response = api_client.post_to_allocate(orderid, sku, 10) assert response.json()['batchref'] == earlier_batch\n# change quantity on allocated batch so it's less than our order redis_client.publish_message('change_batch_quantity', { 'batchref': earlier_batch, 'qty': 5 })\nYou can read the story of what’s going on in this test from the comments: we want to send an event into the system that causes an order line to be reallocated, and we see that reallocation come out as an event in Redis too.\nredis_client is another little test helper, the details of which don’t really matter; its job is to be able to send and receive messages from various Redis channels.\nWe’ll use a channel called change_batch_quantity to send in our request to change the quantity for a batch, and we’ll listen to another channel called line_allocated to look out for the expected reallocation.\nBecause of the asynchronous nature of the system under test, we need to use the tenacity library again to add a retry loop—first, because it may take some time for our new line_allocated message to arrive, but also because it won’t be the only message on that channel.\nOur Redis pub/sub listener (we call it an event consumer) is very much like Flask: it translates from the outside world to our events:\nSimple Redis message listener (src/allocation/entrypoints/redis_eventconsumer.py)\nSimple Redis message publisher (src/allocation/adapters/redis_eventpublisher.py)\nWe take a hardcoded channel here, but you could also store a mapping between event classes/names and the appropriate channel, allowing one or more message types to go to different channels.\nHere’s what the Allocated event will look like:\nNew event (src/allocation/domain/events.py)\n@dataclass class Allocated(Event): orderid: str sku: str qty: int batchref: str\nIt captures everything we need to know about an allocation: the details of the order line, and which batch it was allocated to.\nProduct.allocate() emits new event to record what happened (src/allocation/domain/model.py)\nbatch.allocate(line) self.version_number += 1 self.events.append(events.Allocated( orderid=line.orderid, sku=line.sku, qty=line.qty,\nThe handler for ChangeBatchQuantity already exists, so all we need to add is a handler that publishes the outgoing event:\nThe message bus grows (src/allocation/service_layer/messagebus.py)\nHANDLERS = { events.Allocated: [handlers.publish_allocated_event], events.OutOfStock: [handlers.send_out_of_stock_notification], } # type: Dict[Type[events.Event], List[Callable]]\nPublishing the event uses our helper function from the Redis wrapper:\nPublish to Redis (src/allocation/service_layer/handlers.py)\ndef publish_allocated_event( event: events.Allocated, uow: unit_of_work.AbstractUnitOfWork, ): redis_eventpublisher.publish('line_allocated', event)\nA nice simple one for this chapter: make it so that the main allocate() use case can also be invoked by an event on a Redis channel, as well as (or instead of) via the API.\nYou will likely want to add a new E2E test and feed through some changes into redis_eventconsumer.py.\nEvents can come from the outside, but they can also be published externally—our publish handler converts an event to a message on a Redis channel.\nWe use events to talk to the outside world.",
      "keywords": [
        "Redis",
        "event",
        "system",
        "message",
        "order",
        "batch",
        "Simple Redis message",
        "allocated",
        "Channel",
        "CONNASCENCE",
        "Redis pub",
        "Redis message",
        "API",
        "warehouse system"
      ],
      "concepts": [
        "event",
        "messages",
        "messaging",
        "allocated",
        "allocation",
        "allocate",
        "services",
        "batch",
        "batches",
        "publish"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 9,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 12,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 10,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 7,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 8,
          "title": "",
          "score": 0.448,
          "base_score": 0.448,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "redis",
          "event",
          "events",
          "channel",
          "message"
        ],
        "semantic": [],
        "merged": [
          "redis",
          "event",
          "events",
          "channel",
          "message"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44523570764650244,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533968+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Command-Query Responsibility Segregation (CQRS)",
      "start_page": 310,
      "end_page": 334,
      "summary": "In this chapter, we’re going to start with a fairly uncontroversial insight: reads (queries) and writes (commands) are different, so they should be treated differently (or have their responsibilities segregated,\ndef test_allocating_to_a_batch_reduces_the_available_quantity(): batch = Batch(\"batch-001\", \"SMALL-TABLE\", qty=20, eta=date.today()) line = OrderLine('order-ref', \"SMALL-TABLE\", 2)\nTo communicate changes between those small chunks, we introduced the Domain Events pattern so we can write rules like “When stock is damaged or lost, adjust the available quantity on the batch, and reallocate orders if necessary.”\nFor example, our customers won’t notice if the query is a few seconds out of date, but if our allocate service is inconsistent, we’ll make a mess of their orders.\nThis insight is key to understanding why reads can be safely inconsistent: we’ll always need to check the current state of our system when we come to allocate, because all distributed systems are inconsistent.\nIt’s OK to trade performance for consistency on the read side, because stale data is essentially unavoidable.\nFor the write side, our fancy domain architectural patterns help us to evolve our system over time, but the complexity we’ve built so far doesn’t buy anything for reading data.\nchange it to return a simple OK message and instead provide a new read-only endpoint to retrieve allocation state:\ndef test_happy_path_returns_202_and_batch_is_allocated(): orderid = random_orderid() sku, othersku = random_sku(), random_sku('other') earlybatch = random_batchref(1) laterbatch = random_batchref(2) otherbatch = random_batchref(3) api_client.post_to_add_batch(laterbatch, sku, 100, '2011-01-02') api_client.post_to_add_batch(earlybatch, sku, 100, '2011-01-01') api_client.post_to_add_batch(otherbatch, othersku, 100, None)\n@pytest.mark.usefixtures('postgres_db') @pytest.mark.usefixtures('restart_api') def test_unhappy_path_returns_400_and_error_message(): unknown_sku, orderid = random_sku(), random_orderid() r = api_client.post_to_allocate( orderid, unknown_sku, qty=20, expect_success=False, ) assert r.status_code == 400 assert r.json()['message'] == f'Invalid sku {unknown_sku}'\n@app.route(\"/allocations/<orderid>\", methods=['GET']) def allocations_view_endpoint(orderid): uow = unit_of_work.SqlAlchemyUnitOfWork() result = views.allocations(orderid, uow) if not result: return 'not found', 404 return jsonify(result), 200\n(src/allocation/views.py)\ndef allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork): with uow: results = list(uow.session.execute( 'SELECT ol.sku, b.reference' ' FROM allocations AS a' ' JOIN batches AS b ON a.batch_id = b.id' ' JOIN order_lines AS ol ON a.orderline_id = ol.id' ' WHERE ol.orderid = :orderid', dict(orderid=orderid) )) return [{'sku': sku, 'batchref': batchref} for sku, batchref in results]\nwhich code modifies state (the event handlers) and which code just retrieves read-only state (the views).\nSplitting out your read-only views from your state-modifying command and event handlers is probably a good idea, even if you don’t want to go to full-blown CQRS.\ndef test_allocations_view(sqlite_session_factory): uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory) messagebus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None), uow)\nassert views.allocations('order1', uow) == [ {'sku': 'sku1', 'batchref': 'sku1batch'}, {'sku': 'sku2', 'batchref': 'sku2batch'}, ]\nA simple view that uses the repository (src/allocation/views.py)\ndef allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork): with uow: products = uow.products.for_order(orderid=orderid) batches = [b for p in products for b in p.batches]\n(src/allocation/domain/model.py)\nYour Domain Model Is Not Optimized for Read Operations\nsaid before, a domain model is not a data model—we’re trying to capture the way the business works: workflow, rules around state\nIf you’re building a simple CRUD app, reads and writes are going to be closely related, so you don’t need a domain model or CQRS.\nmaking more and more choices about how to structure that model, which make it more and more awkward to use for read operations.\nA simple view that uses the ORM (src/allocation/views.py)\nfrom allocation import unit_of_work, model\ndef allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork): with uow: batches = uow.session.query(model.Batch).join( model.OrderLine, model.Batch._allocations ).filter( model.OrderLine.orderid == orderid ) return [ {'sku': b.sku, 'batchref': b.batchref} for b in batches ]\nIt’s common in such cases to add some denormalized views, build read replicas, or even add caching layers.\nA much nicer query (src/allocation/views.py)\ndef allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork): with uow: results = list(uow.session.execute( 'SELECT sku, batchref FROM allocations_view WHERE orderid = :orderid', dict(orderid=orderid) )) ...\n…by keeping a totally separate, denormalized data store for our view model?\nwriting data to a relational database, we need to make sure that we get a lock over the rows we’re changing so we don’t run into consistency problems.\nIf you’re struggling to scale a system with a complex data store, ask whether you could build a simpler read model.\nUpdating a Read Model Table Using an Event Handler\nEVENT_HANDLERS = { events.Allocated: [ handlers.publish_allocated_event, handlers.add_allocation_to_read_model ],\nHere’s what our update-view-model code looks like:\ndef add_allocation_to_read_model( event: events.Allocated, uow: unit_of_work.SqlAlchemyUnitOfWork, ): with uow: uow.session.execute( 'INSERT INTO allocations_view (orderid, sku, batchref)' ' VALUES (:orderid, :sku, :batchref)', dict(orderid=event.orderid, sku=event.sku, batchref=event.batchref) ) uow.commit()\nA second listener for read model updates\nevents.Deallocated: [ handlers.remove_allocation_from_read_model, handlers.reallocate ],\ndef remove_allocation_from_read_model( event: events.Deallocated, uow: unit_of_work.SqlAlchemyUnitOfWork, ): with uow: uow.session.execute( 'DELETE FROM allocations_view ' ' WHERE orderid = :orderid AND sku = :sku',\noperation, one to update the write model and one to update the read model, which the GET/read operation can use.\nIf we never updated the view model, and the ASYMMETRICAL-DRESSER was forever in stock, that would be annoying for customers, but the allocate service would still fail, and we’d take action to fix the problem.\nSince we’re using a service layer to update our view model, we can write a tool that does the following:\nQueries the current state of the write side to work out what’s currently allocated\nCalls the add_allocate_to_read_model handler for each allocated item\nWe can use this technique to create entirely new read models from historical data.\nChanging Our Read Model Implementation Is Easy\nread model by using a totally separate storage engine, Redis.\nHandlers update a Redis read model (src/allocation/service_layer/handlers.py)\ndef add_allocation_to_read_model(event: events.Allocated, _): redis_eventpublisher.update_readmodel(event.orderid, event.sku, event.batchref)\ndef remove_allocation_from_read_model(event: events.Deallocated, _): redis_eventpublisher.update_readmodel(event.orderid, event.sku, None)\nRedis read model read and update (src/allocation/adapters/redis_eventpublisher.py)\nView adapted to Redis (src/allocation/views.py)\ndef allocations(orderid): batches = redis_eventpublisher.get_readmodel(orderid) return [ {'batchref': b.decode(), 'sku': s.decode()} for s, b in batches.items() ]\nEvent handlers are a great way to manage updates to a read model, if you decide you need one.\nThey also make it easy to change the implementation of that read model at a later date.\nImplement another view, this time to show the allocation for a single order line.\nAs it happens, the allocation service at MADE.com does use “full- blown” CQRS, with a read model stored in Redis, and even a second\nFor the kind of allocation service we’re building, it seems unlikely that you’d need to use a separate read model and event handlers for updating it.\nBut as your domain model becomes richer and more complex, a simplified read model become ever more compelling.\nOften, your read operations will be acting on the same conceptual objects as your write model, so using the ORM, adding some read methods to your repositories, and using domain model classes for your read operations is just fine.\nIn our book example, the read operations act on quite different conceptual entities to our domain model.\nThe allocation service thinks in terms of Batches for a single SKU, but users care about allocations for a whole order, with multiple SKUs, so using the ORM ends up being a little awkward.",
      "keywords": [
        "read model",
        "model",
        "READ",
        "domain model",
        "allocation",
        "orderid",
        "sku",
        "Redis read model",
        "view",
        "uow",
        "view model",
        "Domain",
        "’re",
        "data",
        "batch"
      ],
      "concepts": [
        "allocate",
        "allocated",
        "allocation",
        "models",
        "queries",
        "query",
        "reads",
        "batch",
        "batches",
        "views"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 7,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 9,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 11,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 10,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 8,
          "title": "",
          "score": 0.452,
          "base_score": 0.452,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "read",
          "orderid",
          "model",
          "read model",
          "uow"
        ],
        "semantic": [],
        "merged": [
          "read",
          "orderid",
          "model",
          "read model",
          "uow"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45606498135119344,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.533986+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Dependency Injection (and Bootstrapping)",
      "start_page": 335,
      "end_page": 497,
      "summary": "We’ll also add a new component to our architecture called bootstrap.py; it will be in charge of dependency injection, as well as some other initialization stuff that we often need.\nIf you haven’t already, it’s worth reading Chapter 3 before continuing with this chapter, particularly the discussion of functional versus object-oriented dependency management.\nWe’ve shown you two ways of managing dependencies and testing them.\nFor our database dependency, we’ve built a careful framework of explicit dependencies and easy options for overriding them in tests.\nOur main handler functions declare an explicit dependency on the UoW:\nOur handlers have an explicit dependency on the UoW (src/allocation/service_layer/handlers.py)\ndef allocate( cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork ):\nAnd that makes it easy to swap in a fake UoW in our service-layer tests:\nService-layer tests against a fake UoW: (tests/unit/test_services.py)\nThe UoW depends on a session factory (src/allocation/service_layer/unit_of_work.py)\nWe take advantage of it in our integration tests to be able to sometimes use SQLite instead of Postgres:\nIntegration tests against a different DB (tests/integration/test_uow.py)\ndef test_rolls_back_uncommitted_work_by_default(sqlite_session_factory): uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)\nThe standard way to do things is to declare our dependency implicitly by simply importing it, and then if we ever need to change it for tests, we can monkeypatch, as is Right and True in dynamic languages:\nEmail sending as a normal import-based dependency (src/allocation/service_layer/handlers.py)\ndef send_out_of_stock_notification( event: events.OutOfStock, uow: unit_of_work.AbstractUnitOfWork, ): email.send( 'stock@made.com', f'Out of stock for {event.sku}', )\nmock dot patch, thank you Michael Foord (tests/unit/test_handlers.py)\nThe trouble is that we’ve made it look easy because our toy example doesn’t send real email (email.send_mail just does a print), but in real life, you’d end up having to call mock.patch for every single test that might cause an out-of-stock notification.\n(src/allocation/service_layer/handlers.py)\nSo far, we’ve really been dealing with only passing the UoW around: our tests use FakeUnitOfWork, while Flask and Redis eventconsumer entrypoints use the real UoW, and the\nMoreover, putting all the responsibility for passing dependencies to the right handler onto the message bus feels like a violation of the SRP.\nbe called later with those dependencies already injected is to use closures or partial functions to compose the function with its\n# existing allocate function, with abstract uow dependency def allocate( cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork ): line = OrderLine(cmd.orderid, cmd.sku, cmd.qty) with uow: ...\ndef bootstrap(..): uow = unit_of_work.SqlAlchemyUnitOfWork()\n# prepare a version of the allocate fn with UoW dependency captured in a closure allocate_composed = lambda cmd: allocate(cmd, uow)\nHere’s the same pattern again for the send_out_of_stock_notification() handler, which has different dependencies:\n# prepare a version of the send_out_of_stock_notification with dependencies sosn_composed = lambda event: send_out_of_stock_notification(event, email.send_mail)\n# we replace the old `def allocate(cmd, uow)` with:\ndef __init__(self, uow: unit_of_work.AbstractUnitOfWork): self.uow = uow\ndef __call__(self, cmd: commands.Allocate): line = OrderLine(cmd.orderid, cmd.sku, cmd.qty) with self.uow: # rest of handler method as before ...\n# bootstrap script prepares actual UoW uow = unit_of_work.SqlAlchemyUnitOfWork()\n# then prepares a version of the allocate fn with dependencies already injected allocate = AllocateHandler(uow)\n# later at runtime, we can call the handler instance, and it will have # the UoW already injected allocate(cmd)\nA bootstrap function (src/allocation/bootstrap.py)\ndef bootstrap( start_orm: bool = True, uow: unit_of_work.AbstractUnitOfWork = unit_of_work.SqlAlchemyUnitOfWork(),\ndependencies = {'uow': uow, 'send_mail': send_mail, 'publish': publish} injected_event_handlers = { event_type: [ inject_dependencies(handler, dependencies) for handler in event_handlers ] for event_type, event_handlers in handlers.EVENT_HANDLERS.items() } injected_command_handlers = { command_type: inject_dependencies(handler, dependencies) for command_type, handler in handlers.COMMAND_HANDLERS.items() }\norm.start_mappers() is our example of initialization work that needs to be done once at the beginning of an app.\nIt’s nice to have them in a single place, but sometimes dependencies have some side effects at construction time, in which case you might prefer to default them to None instead.\nWe build up our injected versions of the handler mappings by using a function called inject_dependencies(), which we’ll show next.\nWe return a configured message bus ready for use.\nHere’s how we inject dependencies into a handler function by inspecting it:\nDI by inspecting function signatures (src/allocation/bootstrap.py)\ndef inject_dependencies(handler, dependencies): params = inspect.signature(handler).parameters deps = { name: dependency for name, dependency in dependencies.items() if name in params } return lambda message: handler(message, **deps)\nManually creating partial functions inline (src/allocation/bootstrap.py)\ninjected_event_handlers = { events.Allocated: [ lambda e: handlers.publish_allocated_event(e, publish), lambda e: handlers.add_allocation_to_read_model(e, uow), ], events.Deallocated: [ lambda e: handlers.remove_allocation_from_read_model(e, uow), lambda e: handlers.reallocate(e, uow), ], events.OutOfStock: [ lambda e: handlers.send_out_of_stock_notification(e, send_mail) ] } injected_command_handlers = { commands.Allocate: lambda c: handlers.allocate(c, uow), commands.CreateBatch: \\ lambda c: handlers.add_batch(c, uow), commands.ChangeBatchQuantity: \\ lambda c: handlers.change_batch_quantity(c, uow), }\nOur app is structured in such a way that we always want to do dependency injection in only one place, the handler functions, so this super-manual solution and Harry’s inspect()-based one will both work fine.\nMessageBus as a class (src/allocation/service_layer/messagebus.py)\ndef __init__( self, uow: unit_of_work.AbstractUnitOfWork, event_handlers: Dict[Type[events.Event], List[Callable]], command_handlers: Dict[Type[commands.Command], Callable], ): self.uow = uow self.event_handlers = event_handlers self.command_handlers = command_handlers\nEvent and command handler logic stays the same (src/allocation/service_layer/messagebus.py)\ndef handle_event(self, event: events.Event): for handler in self.event_handlers[type(event)]: try: logger.debug('handling event %s with handler %s', event, handler) handler(event) self.queue.extend(self.uow.collect_new_events()) except Exception: logger.exception('Exception handling event %s', event) continue\ndef handle_command(self, command: commands.Command): logger.debug('handling command %s', command) try: handler = self.command_handlers[type(command)] handler(command) self.queue.extend(self.uow.collect_new_events()) except Exception: logger.exception('Exception handling command %s', command) raise\nCOMMAND_HANDLERS dict, they use the versions on self.\nInstead of passing a UoW into the handler, we expect the handlers to already have all their dependencies, so all they need is a single argument, the specific event or command.\nIn our application’s entrypoints, we now just call bootstrap.bootstrap() and get a message bus that’s ready to go, rather than configuring a UoW and the rest of it:\nFlask calls bootstrap (src/allocation/entrypoints/flask_app.py)\n@app.route(\"/add_batch\", methods=['POST']) @@ -19,8 +16,7 @@ def add_batch(): cmd = commands.CreateBatch( request.json['ref'], request.json['sku'], request.json['qty'], eta, ) - uow = unit_of_work.SqlAlchemyUnitOfWork() - messagebus.handle(cmd, uow) + bus.handle(cmd) return 'OK', 201\nWe no longer need to explicitly build a particular type of UoW; the bootstrap script defaults take care of it.\nIn tests, we can use bootstrap.bootstrap() with overridden defaults to get a custom message bus.\nOverriding bootstrap defaults (tests/integration/test_views.py)\n@pytest.fixture def sqlite_bus(sqlite_session_factory): bus = bootstrap.bootstrap( start_orm=True, uow=unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory), send_mail=lambda *args: None, publish=lambda *args: None, ) yield bus clear_mappers()\ndef test_allocations_view(sqlite_bus): sqlite_bus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None)) sqlite_bus.handle(commands.CreateBatch('sku2batch', 'sku2', 50, date.today())) ...\nassert views.allocations('order1', sqlite_bus.uow) == [ {'sku': 'sku1', 'batchref': 'sku1batch'}, {'sku': 'sku2', 'batchref': 'sku2batch'}, ]\nBootstrap in unit test (tests/unit/test_handlers.py)\ndef bootstrap_test_app(): return bootstrap.bootstrap( start_orm=False, uow=FakeUnitOfWork(), send_mail=lambda *args: None, publish=lambda *args: None, )\nChange all the handlers to being classes as per the DI using classes example, and amend the bootstrapper’s DI code as appropriate.\nTwo types of dependencies (src/allocation/service_layer/messagebus.py)\nWe’d use this for the case when the dependency is relatively complex.\nEven though it’s simple, let’s use send_mail as an example to talk through how you might define a more complex dependency.\n(src/allocation/adapters/notifications.py)\ndef send(self, destination, message): msg = f'Subject: allocation service notification\\n{message}' self.server.sendmail( from_addr='allocations@example.com', to_addrs=[destination], msg=msg )\nNotifications in message bus (src/allocation/bootstrap.py)\ndef bootstrap( start_orm: bool = True, uow: unit_of_work.AbstractUnitOfWork = unit_of_work.SqlAlchemyUnitOfWork(), - send_mail: Callable = email.send,\nWe work through and define a fake version for unit testing:\nFake notifications (tests/unit/test_handlers.py)\nAnd we use it in our tests:\nTests change slightly (tests/unit/test_handlers.py)\ndef test_sends_email_on_out_of_stock_error(self): fake_notifs = FakeNotifications() bus = bootstrap.bootstrap( start_orm=False, uow=FakeUnitOfWork(), notifications=fake_notifs, publish=lambda *args: None, ) bus.handle(commands.CreateBatch(\"b1\", \"POPULAR-CURTAINS\", 9, None)) bus.handle(commands.Allocate(\"o1\", \"POPULAR-CURTAINS\", 10)) assert fake_notifs.sent['stock@made.com'] == [ f\"Out of stock for POPULAR-CURTAINS\", ]\nIn our integration tests, we use the real EmailNotifications class, talking to the MailHog server in the Docker cluster:\n@pytest.fixture def bus(sqlite_session_factory): bus = bootstrap.bootstrap( start_orm=True, uow=unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory), notifications=notifications.EmailNotifications(), publish=lambda *args: None, ) yield bus clear_mappers()\ndef get_email_from_mailhog(sku): host, port = map(config.get_email_host_and_port().get, ['host', 'http_port']) all_emails = requests.get(f'http://{host}:{port}/api/v2/messages').json() return next(m for m in all_emails['items'] if sku in str(m))\ndef test_out_of_stock_email(bus): sku = random_sku() bus.handle(commands.CreateBatch('batch1', sku, 9, None)) bus.handle(commands.Allocate('order1', sku, 10)) email = get_email_from_mailhog(sku) assert email['Raw']['From'] == 'allocations@example.com' assert email['Raw']['To'] == ['stock@made.com'] assert f'Out of stock for {sku}' in email['Raw']['Data']\nWe use our bootstrapper to build a message bus that talks to the real notifications class.\nWe use the bus to do our test setup.\nOnce you have more than one adapter, you’ll start to feel a lot of pain from passing dependencies around manually, unless you do some kind of dependency injection.\nThe bootstrap script is also good as a place to provide sensible default configuration for your adapters, and as a single place to override those adapters with fakes for your tests.\nA dependency injection framework can be useful if you find yourself needing to do DI at multiple levels—if you have chained dependencies\nThis chapter also presented a worked example of changing an implicit/simple dependency into a “proper” adapter, factoring out an ABC, defining its real and fake implementations, and thinking through integration testing.\n3. Build a fake and use it for unit/service-layer/handler tests.\n1 Because Python is not a “pure” OO language, Python developers aren’t necessarily used to the concept of needing to compose a set of objects into a working application.\nThis may cause problems if you ever find yourself wanting to test your Flask app in-process by\nOne of the first things we can do is to start building a service layer (Figure E-1).\nStart by working out the use cases of your system.\nEach of your use cases needs to have an imperative name: Apply Billing Charges, Clean Abandoned Accounts, or Raise Purchase Order, for example.\nIn our case, most of our use cases were part of the manager classes and had names like Create Workspace or Delete Document Version.\nYou might need to call one use case from another.\nMost of the complexity of the application was in the permissions model because each document was contained in a folder, and folders allowed read, write, and edit permissions, much like a Linux filesystem.\nAs a result, every read or write operation against a document had to load an enormous number of objects from the database in order to test permissions and quotas.\nSome of the code for operations was in web handlers that ran when a user clicked a button or submitted a form; some of it was in manager objects that held code for orchestrating work; and some of it was in the domain model.\nModel objects would make database calls or copy files on disk, and the test coverage was abysmal.\nTo fix the problem, we first introduced a service layer so that all of the code for creating a document or workspace was in one place and could be understood.\nThis involved pulling data access code out of the domain model and into command handlers.\nIt’s fine if you have duplication in the use-case functions.\nIt’s better to duplicate some code in a few places than to have use-case functions calling one another in a long chain.\ncode out of the domain model and into the use cases.\nWe should also try to pull I/O concerns (e.g., sending email, writing files) out of the domain model and up into the use-case functions.\nIf you need data from another part of the system, it’s totally fine to use a read model, but avoid updating multiple aggregates in a single transaction.\nWe talked about the infamous SELECT N+1 problem in Chapter 12, and how we might choose to use different techniques when reading data for queries versus reading data for commands.\nWhen we needed to write data, we changed a single aggregate at a time, and we introduced a message bus to handle events.\n2. Build a second system that consumes those events and uses them to build its own domain model.\nOur first step was to write a domain model that could represent batches, shipments, and products.\nOur Flask API queried these view models instead of running the complex domain model.\nOnce we had a working domain model, we switched to building out some infrastructural pieces.\nYears of adding hacks and work-arounds to a broken model caught up with us, and we had to rewrite the entire user management function as a brand-new system.\nWe like to use interactive techniques like event storming and CRC modeling, because humans are good at collaborating through play.\nFor example, the first code we wrote for the availability service was the batch and order line model.\nOnce you have that, it’s much easier to push logic into the model and push edge concerns like validation or error handling to the entrypoints.\nIt’s worth having a service layer even if you still have a big, messy Django ORM because it’s a way to start understanding the boundaries of operations.\nExtracting use cases will break a lot of my existing code; it’s too tangled\nIn our first case-study system, we had a lot of View Builder objects that used repositories to fetch data and then performed some transformations to return dumb read models.\nWhen your use case has finished, it can raise an event, and a handler elsewhere can run.\nIs it a code smell for a use case to use multiple repositories/aggregates, and if so, why?\nAn aggregate is a consistency boundary, so if your use case needs to update two aggregates atomically (within the same transaction), then your consistency boundary is wrong, strictly speaking.\nIdeally you should think about moving to a new aggregate that wraps up all the things you want to change at the same time.\nIf you’re actually updating only one aggregate and using the other(s) for read-only access, then that’s fine, although you could consider building a read/view model to get you that data instead— it makes things cleaner if each use case has only one aggregate.\nIf you do need to modify two aggregates, but the two operations don’t have to be in the same transaction/UoW, then consider splitting the work out into two different handlers and using a\nWe’ve written systems in which the view models needed extensive unit tests.\n+ This makes it easy to test the view builder by giving it mocked data (e.g., a list of dicts).\n“Fancy CQRS” with event handlers is really a way of running our complex view logic whenever we write so that we\nAggregates, domain events, and dependency inversion are ways to control complexity in large systems.\nIt just so happens that when you’ve built a set of use cases and a model for a business process,\nIn practice you will want to make handlers idempotent so that calling them repeatedly with the same message will not make repeated changes to state.\nReads events from the external message bus and translates them into commands, passing them to the internal message bus.\nA piece of infrastructure that different services use to intercommunicate, via events.\n│ │ │ ├── orm.py │ │ │ └── repository.py │ │ ├── config.py │ │ ├── domain │ │ │ ├── __init__.py │ │ │ └── model.py │ │ ├── entrypoints │ │ │ ├── __init__.py │ │ │ └── flask_app.py │ │ └── service_layer │ │ ├── __init__.py │ │ └── services.py │ └── setup.py └── tests ├── conftest.py ├── e2e │ └── test_api.py ├── integration │ ├── test_orm.py │ └── test_repository.py ├── pytest.ini └── unit ├── test_allocate.py ├── test_batches.py └── test_services.py\nOur docker-compose.yml and our Dockerfile are the main bits of configuration for the containers that run our app, and they can also run the tests (for CI).\nA Makefile provides the entrypoint for all the typical commands a developer (or a CI server) might want to run during their normal workflow: make build, make test, and so on.\nYou could just use docker-compose and pytest directly, but if nothing else, it’s nice to have all the “common commands” in a list somewhere, and unlike documentation, a Makefile is code so it has less tendency to become out of date.\nAll the source code for our app, including the domain model, the Flask app, and infrastructure code, lives in a Python package\nCurrently, the structure within this module is totally flat, but for a more complex project, you’d expect to grow a folder hierarchy that includes domain_model/, infrastructure/, services/, and api/.\nRunning code or tests directly from your own dev machine, perhaps talking to mapped ports from Docker containers\nWhenever our application code needs access to some config, it’s going to get it from a file called config.py.\nSample config functions (src/allocation/config.py)\nWe use functions for getting the current config, rather than constants available at import time, because that allows client code to modify os.environ if it needs to.\nconfig.py also defines some default settings, designed to work when running the code from the developer’s local machine.4\nIf you decide to use a bootstrap script, you can make it the only place (other than tests) that config is imported to.\nIn the docker-compose file, we define the different services (containers) that we need for our app.\nUsually one main image contains all our code, and we can use it to run our API, our tests, or any other service that needs access to the domain model.\nIn production you might not use containers for this; you might have a cloud provider instead, but docker-compose gives us a way of producing a similar service for dev or CI.\nMounting our source and test code as volumes means we don’t need to rebuild our containers every time we make a code change.\nWORKDIR /src ENV FLASK_APP=allocation/entrypoints/flask_app.py FLASK_DEBUG=1 PYTHONUNBUFFERED=1 CMD flask run --host=0.0.0.0 --port=80\n└── tests ├── conftest.py ├── e2e │ └── test_api.py ├── integration │ ├── test_orm.py │ └── test_repository.py ├── pytest.ini └── unit ├── test_allocate.py ├── test_batches.py └── test_services.py\nNothing particularly clever here, just some separation of different test types that you’re likely to want to run separately, and some files for\nThere’s no src folder or setup.py in the test folders because we usually haven’t needed to make tests pip-installable, but if you have\nSource code in an src folder, pip-installable using setup.py\nConfiguration via environment variables, centralized in a Python file called config.py, with defaults allowing things to run outside containers\ntended to find that going further and trying to split out different images for different types of application code (e.g., Web API versus pub/sub client) usually ends up being more trouble than it’s worth; the cost in terms of complexity and longer rebuild/CI times is too high.\nThis appendix is intended as a little illustration of the benefits of the Repository, Unit of Work, and Service Layer patterns.\nready to use our API and asking if we could build a thing that reads just batches and orders from a couple of CSVs and outputs a third CSV\ndomain model and service layer.\ndef test_cli_app_reads_csvs_with_batches_and_orders_and_outputs_allocations( make_csv ): sku1, sku2 = random_ref('s1'), random_ref('s2') batch1, batch2, batch3 = random_ref('b1'), random_ref('b2'), random_ref('b3') order_ref = random_ref('o') make_csv('batches.csv', [ ['ref', 'sku', 'qty', 'eta'], [batch1, sku1, 100, ''], [batch2, sku2, 100, '2011-01-01'], [batch3, sku2, 100, '2011-01-02'], ]) orders_csv = make_csv('orders.csv', [ ['orderid', 'sku', 'qty'], [order_ref, sku1, 3], [order_ref, sku2, 12], ])\nexpected_output_csv = orders_csv.parent / 'allocations.csv' with open(expected_output_csv) as f: rows = list(csv.reader(f)) assert rows == [ ['orderid', 'sku', 'qty', 'batchref'], [order_ref, sku1, '3', batch1], [order_ref, sku2, '12', batch2], ]\nfrom allocation import model\ndef load_batches(batches_path): batches = [] with batches_path.open() as inf: reader = csv.DictReader(inf) for row in reader: if row['eta']: eta = datetime.strptime(row['eta'], '%Y-%m-%d').date() else: eta = None batches.append(model.Batch( ref=row['ref'], sku=row['sku'], qty=int(row['qty']), eta=eta )) return batches\ndef main(folder): batches_path = Path(folder) / 'batches.csv' orders_path = Path(folder) / 'orders.csv' allocations_path = Path(folder) / 'allocations.csv'\nwith orders_path.open() as inf, allocations_path.open('w') as outf: reader = csv.DictReader(inf) writer = csv.writer(outf) writer.writerow(['orderid', 'sku', 'batchref']) for row in reader: orderid, sku = row['orderid'], row['sku'] qty = int(row['qty']) line = model.OrderLine(orderid, sku, qty) batchref = model.allocate(line, batches) writer.writerow([line.orderid, line.sku, batchref])\nAnd we’re reusing our domain model objects and our domain service.\nAnd another one, with existing allocations (tests/e2e/test_csv.py)\ndef test_cli_app_also_reads_existing_allocations_and_can_append_to_them( make_csv ): sku = random_ref('s') batch1, batch2 = random_ref('b1'), random_ref('b2') old_order, new_order = random_ref('o1'), random_ref('o2') make_csv('batches.csv', [ ['ref', 'sku', 'qty', 'eta'], [batch1, sku, 10, '2011-01-01'], [batch2, sku, 10, '2011-01-02'], ]) make_csv('allocations.csv', [ ['orderid', 'sku', 'qty', 'batchref'], [old_order, sku, 10, batch1], ]) orders_csv = make_csv('orders.csv', [ ['orderid', 'sku', 'qty'], [new_order, sku, 7], ])\nAnd we could keep hacking about and adding extra lines to that load_batches function, and some sort of way of tracking and saving new allocations—but we already have a model for doing that!\nIt’s called our Repository and Unit of Work patterns.\nIt abstracts away all the logic for reading CSVs from disk, including the fact that it has to read two different CSVs (one for batches and one for allocations), and it gives us just the familiar .list() API, which provides the illusion of an in-memory collection of domain objects:\nA repository that uses CSV as its storage mechanism (src/allocation/service_layer/csv_uow.py)\nself._allocations_path = Path(folder) / 'allocations.csv' self._batches = {} # type: Dict[str, model.Batch] self._load()\ndef _load(self): with self._batches_path.open() as f: reader = csv.DictReader(f) for row in reader: ref, sku = row['ref'], row['sku'] qty = int(row['qty']) if row['eta']: eta = datetime.strptime(row['eta'], '%Y-%m-%d').date() else: eta = None self._batches[ref] = model.Batch( ref=ref, sku=sku, qty=qty, eta=eta ) if self._allocations_path.exists() is False: return with self._allocations_path.open() as f: reader = csv.DictReader(f) for row in reader: batchref, orderid, sku = row['batchref'], row['orderid'], row['sku'] qty = int(row['qty']) line = model.OrderLine(orderid, sku, qty) batch = self._batches[batchref] batch._allocations.add(line)\n(src/allocation/service_layer/csv_uow.py)\ndef commit(self): with self.batches._allocations_path.open('w') as f: writer = csv.writer(f) writer.writerow(['orderid', 'sku', 'qty', 'batchref']) for batch in self.batches.list(): for line in batch._allocations: writer.writerow( [line.orderid, line.sku, line.qty, batch.reference] )\nand allocations to CSV is pared down to what it should be—a bit of code for reading order lines, and a bit of code that invokes our\ndef main(folder): orders_path = Path(folder) / 'orders.csv' uow = csv_uow.CsvUnitOfWork(folder) with orders_path.open() as f: reader = csv.DictReader(f) for row in reader: orderid, sku = row['orderid'], row['sku'] qty = int(row['qty']) services.allocate(orderid, sku, qty, uow)\nRepository and Unit of Work Patterns with Django\n│ ├── djangoproject │ │ ├── alloc │ │ │ ├── __init__.py │ │ │ ├── apps.py │ │ │ ├── migrations │ │ │ │ ├── 0001_initial.py │ │ │ │ └── __init__.py │ │ │ ├── models.py │ │ │ └── views.py │ │ ├── django_project │ │ │ ├── __init__.py │ │ │ ├── settings.py │ │ │ ├── urls.py │ │ │ └── wsgi.py │ │ └── manage.py │ └── setup.py └── tests ├── conftest.py ├── e2e │ └── test_api.py ├── integration\nWe used a plug-in called pytest-django to help with test database management.\nfrom djangoproject.alloc import models as django_models\n@pytest.mark.django_db def test_repository_can_save_a_batch(): batch = model.Batch(\"batch1\", \"RUSTY-SOAPDISH\", 100, eta=date(2011, 12, 25))\n[saved_batch] = django_models.Batch.objects.all() assert saved_batch.reference == batch.reference assert saved_batch.sku == batch.sku\nThe second test is a bit more involved since it has allocations, but it is still made up of familiar-looking Django code:\n@pytest.mark.django_db def test_repository_can_retrieve_a_batch_with_allocations(): sku = \"PONY-STATUE\" d_line = django_models.OrderLine.objects.create(orderid=\"order1\", sku=sku, qty=12) d_b1 = django_models.Batch.objects.create( reference=\"batch1\", sku=sku, qty=100, eta=None ) d_b2 = django_models.Batch.objects.create( reference=\"batch2\", sku=sku, qty=100, eta=None ) django_models.Allocation.objects.create(line=d_line, batch=d_batch1)\nexpected = model.Batch(\"batch1\", sku, 100, eta=None) assert retrieved == expected # Batch.__eq__ only compares reference assert retrieved.sku == expected.sku assert retrieved._purchased_quantity == expected._purchased_quantity assert retrieved._allocations == { model.OrderLine(\"order1\", sku, 12), }\nA Django repository (src/allocation/adapters/repository.py)\ndef update(self, batch): django_models.Batch.update_from_domain(batch)\ndef _get(self, reference): return django_models.Batch.objects.filter( reference=reference ).first().to_domain()\ndef list(self): return [b.to_domain() for b in django_models.Batch.objects.all()]\nCustom Methods on Django ORM Classes to Translate to/from Our Domain Model\nDjango ORM with custom methods for domain model conversion (src/djangoproject/alloc/models.py)\nfrom django.db import models from allocation.domain import model as domain_model\n@staticmethod def update_from_domain(batch: domain_model.Batch):\ntry: b = Batch.objects.get(reference=batch.reference) except Batch.DoesNotExist: b = Batch(reference=batch.reference) b.sku = batch.sku b.qty = batch._purchased_quantity b.eta = batch.eta b.save() b.allocation_set.set( Allocation.from_domain(l, b) for l in batch._allocations )\ndef to_domain(self) -> domain_model.Batch: b = domain_model.Batch( ref=self.reference, sku=self.sku, qty=self.qty, eta=self.eta ) b._allocations = set( a.line.to_domain() for a in self.allocation_set.all() ) return b\nFor value objects, objects.get_or_create can work, but for entities, you probably need an explicit try-get/except to handle the upsert.\nThe ORM (Django) depends on the model and not the other way around.\nUnit of Work Pattern with Django\nAdapted UoW tests (tests/integration/test_uow.py)\ndef insert_batch(ref, sku, qty, eta): django_models.Batch.objects.create(reference=ref, sku=sku, qty=qty, eta=eta)\ndef get_allocated_batch_ref(orderid, sku): return django_models.Allocation.objects.get( line__orderid=orderid, line__sku=sku ).batch.reference\n@pytest.mark.django_db(transaction=True) def test_uow_can_retrieve_a_batch_and_allocate_to_it(): insert_batch('batch1', 'HIPSTER-WORKBENCH', 100, None)\nuow = unit_of_work.DjangoUnitOfWork() with uow: batch = uow.batches.get(reference='batch1') line = model.OrderLine('o1', 'HIPSTER-WORKBENCH', 10) batch.allocate(line) uow.commit()\ndef test_rolls_back_uncommitted_work_by_default(): ...\n@pytest.mark.django_db(transaction=True) def test_rolls_back_on_error(): ...\n(src/allocation/service_layer/unit_of_work.py)\nOne difficulty: because, unlike with SQLAlchemy, we’re not instrumenting the domain model instances themselves, the commit() command needs to explicitly go through all the objects that have been touched by every repository and manually update them back to the ORM.\nflask_app.py, because our architecture means it’s a very thin wrapper around our service layer (which didn’t change at all, by the way):\nFlask app → Django views (src/djangoproject/alloc/views.py)\n@csrf_exempt def add_batch(request): data = json.loads(request.body) eta = data['eta'] if eta is not None: eta = datetime.fromisoformat(eta).date() services.add_batch( data['ref'], data['sku'], data['qty'], eta, unit_of_work.DjangoUnitOfWork(), ) return HttpResponse('OK', status=201)\ndef allocate(request): data = json.loads(request.body) try: batchref = services.allocate( data['orderid'], data['sku'], data['qty'], unit_of_work.DjangoUnitOfWork(), ) except (model.OutOfStock, services.InvalidSku) as e: return JsonResponse({'message': str(e)}, status=400)\nBecause Django is so tightly coupled to the database, you have to use helpers like pytest-django and think carefully about test databases, right from the very first line of code, in a way that we didn’t have to when we started out with our pure domain model.\nactively dangerous if the whole point of your app is to build a complex set of rules and modeling around the workflow of state changes.\nThe main thing they will buy you in the short term is faster unit tests, so evaluate whether that benefit feels worth it in your case.\nIn the longer term, they decouple your app from Django and the database, so if you anticipate wanting to migrate away from either of those, Repository and UoW are a good idea.\nThe Service Layer pattern might be of interest if you’re seeing a lot of duplication in your views.py.\nas long as your app is not too complex and your tests not too slow, you may be able to get something out of the fat models approach: push as much logic down to your models as possible, and apply patterns like Entity, Value Object, and Aggregate.\nIt can become a stepping-stone for moving to a fully decoupled domain model and/or service layer later.\nA business-logic layer might start out working with Django model objects and only later become fully decoupled from the framework and work on plain Python data structures.\nWhen people use the word validation, they usually mean a process whereby they test the inputs of an operation to make sure that they\nAn Allocate command without a SKU or an order ID isn’t a valid message.\nValidation on the message class (src/allocation/commands.py)\nA command factory with schema (src/allocation/commands.py)\ndef command(name, **fields): schema = Schema(And(Use(json.loads), fields), ignore_extra_keys=True) cls = make_dataclass(name, fields.keys()) cls.from_json = lambda s: cls(**schema.validate(s)) return cls\nWe use the make_dataclass function from the dataclass module to dynamically create our message type.\nDevelopers love to validate this kind of thing in their messages, and reject anything that looks like an invalid SKU.\nLet’s imagine, for example, that the procurement system adds new fields to the ChangeBatchQuantity message that record the reason for the change and the email of the user responsible for the change.\nResist the temptation to share message definitions between systems: instead, make it easy to define the data you depend on.\nIn particular, we don’t want to code defensively inside our domain model.\nInstead, we want to make sure that requests are known to be valid before our domain model or use-case handlers see them.\ndef handle_message(self, name: str, body: str): try: message_type = next(mt for mt in EVENT_HANDLERS if mt.__name__ == name) message = message_type.from_json(body) self.handle([message]) except StopIteration: raise KeyError(f\"Unknown message name {name}\") except ValidationError as e: logging.error( f'invalid message of type {name}\\n' f'{body}\\n' f'{e}'\nAPI bubbles up validation errors (src/allocation/flask_app.py)\n@app.route(\"/change_quantity\", methods=['POST']) def change_batch_quantity(): try: bus.handle_message('ChangeBatchQuantity', request.body) except ValidationError as e: return bad_request(e) except exceptions.InvalidSku as e: return jsonify({'message': str(e)}), 400\ndef handle_change_batch_quantity(m, bus: messagebus.Messag",
      "keywords": [
        "Django",
        "UoW",
        "message",
        "domain model",
        "sku",
        "message bus",
        "code",
        "system",
        "def test",
        "handlers",
        "event",
        "n’t",
        "Domain",
        "model",
        "unit"
      ],
      "concepts": [
        "allocate",
        "allocation",
        "allocations",
        "messaging",
        "messages",
        "batches",
        "batch",
        "tested",
        "event",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 5,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 3,
          "title": "",
          "score": 0.55,
          "base_score": 0.4,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 6,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "uow",
          "bootstrap",
          "sku",
          "csv",
          "row"
        ],
        "semantic": [],
        "merged": [
          "uow",
          "bootstrap",
          "sku",
          "csv",
          "row"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4120016329477404,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:57.534008+00:00"
      }
    }
  ],
  "total_chapters": 13,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "test_book_metadata.json",
    "enrichment_date": "2025-12-17T23:08:57.538597+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 2993.8687509984447,
    "total_similar_chapters": 65
  }
}