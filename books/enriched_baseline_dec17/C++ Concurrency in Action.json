{
  "metadata": {
    "title": "C++ Concurrency in Action",
    "source_file": "C++ Concurrency in Action_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "“It’s not just the best current treatment of C++11’s threading facilities ...\n“Anthony shows how to put concurrency into practice.”\n“A thoughtful, in-depth guide to the new concurrency standard for C++ straight from\n“Any serious C++ developers should understand the contents of this important book.”\nC++ Concurrency\nFor online information and ordering of this and other Manning books, please visit\nManning Publications Co. 20 Baldwin Road\nWhere those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nManning Publications Co. Development editors: Cynthia Kane, Jennifer Stout\nHello, world of concurrency in C++!\nDesigning lock-based concurrent data structures\nDesigning lock-free concurrent data structures\nDesigning concurrent code\nHello, world of concurrency in C++!\nWhat is concurrency?\nConcurrency in computer systems\nconcurrency\nWhy use concurrency?\nconcurrency for performance: task and data parallelism\nWhen not to use concurrency\nConcurrency and multithreading in C++\n■More support for concurrency and ",
      "keywords": [
        "concurrency",
        "Manning Publications",
        "SHELTER ISLAND",
        "Publications",
        "Anthony Williams",
        "Baldwin Road",
        "EDITION",
        "Manning books",
        "concurrent",
        "printed",
        "book",
        "Anthony",
        "ISLAND",
        "SHELTER",
        "data"
      ],
      "concepts": [
        "concurrency",
        "concurrent",
        "editors",
        "designations",
        "designer",
        "threads",
        "book",
        "anthony",
        "developers",
        "hello"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 4,
          "title": "",
          "score": 0.844,
          "base_score": 0.694,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "concurrency",
          "manning",
          "concurrency concurrency",
          "publications",
          "manning publications"
        ],
        "semantic": [],
        "merged": [
          "concurrency",
          "manning",
          "concurrency concurrency",
          "publications",
          "manning publications"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.47271539465395135,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.853936+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "summary": "Managing threads\nBasic thread management\nLaunching a thread\n■Waiting for a thread to complete\n■Running threads in \nIdentifying threads\nSharing data between threads\nProblems with sharing data between threads\ndata\nSynchronizing concurrent operations\nBuilding a thread-safe queue with condition variables\n■Waiting from multiple threads\nThe C++ memory model and operations on atomic types\nand concurrency\nAtomic operations and types in C++\nOperations on std::atomic<T*>: pointer arithmetic\nOperations on standard atomic integral types\nDesigning lock-based concurrent data structures\nGuidelines for designing data structures for concurrency\nLock-based concurrent data structures\nA thread-safe stack using locks\n■A thread-safe queue using \n■A thread-safe queue using \nDesigning more complex lock-based data structures\nWriting a thread-safe lookup table using locks\nthread-safe list using locks\nDesigning lock-free concurrent data structures\nTypes of nonblocking data structures\n■Wait-free data structures\ncons of lock-free data structures\nExamples of lock-free data structures\nWriting a thread-safe stack without locks\npesky leaks: managing memory in lock-free data structures\n■Writing a thread-safe \nGuidelines for writing lock-free data structures\nidentify busy-wait loops and help the other thread\nDesigning concurrent code\nTechniques for dividing work between threads\nDividing data between threads before processing begins\nyour data?\nDesigning data structures for multithreaded \nconcurrency\nImproving responsiveness with concurrency\nDesigning concurrent code in practice\nThread pools\nInterrupting threads\nLaunching and interrupting another thread\nthat a thread has been interrupted\nstd::condition_variable_any\nstd::execution::parallel_policy\nThe parallel algorithms from the C++ Standard \nStructuring multithreaded test code\nappendix C\nC++ Thread Library reference\nran the code in multiple threads, each thread processing its own set of incoming\nWe wrote the code in C++, using POSIX threads, and made a fair number of\ninvolved with the concurrency group as we worked on the changes for C++17, the\nConcurrency TS, and proposals for the future.\nto teach other C++ developers how to use the C++17 Thread Library and Concurrency\nI would also like to thank the other members of the C++ Standards Committee\nmultithreading and concurrency support in C++11, C++14, C++17, and the Concur-",
      "keywords": [
        "data structures",
        "data",
        "Lock-free data structures",
        "std",
        "thread",
        "operations",
        "designing data structures",
        "concurrent data structures",
        "Protecting shared data",
        "structures",
        "concurrency",
        "Lock-free data",
        "Waiting",
        "Thread Library",
        "Boost Thread Library"
      ],
      "concepts": [
        "data",
        "std",
        "concurrency",
        "locking",
        "structures",
        "waiting",
        "multithread",
        "contents",
        "contention",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.79,
          "base_score": 0.79,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.779,
          "base_score": 0.779,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.772,
          "base_score": 0.772,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.762,
          "base_score": 0.762,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 54,
          "title": "",
          "score": 0.75,
          "base_score": 0.75,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "structures",
          "data structures",
          "data",
          "free data",
          "thread"
        ],
        "semantic": [],
        "merged": [
          "structures",
          "data structures",
          "data",
          "free data",
          "thread"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.56879065343621,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.854061+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 17-25)",
      "start_page": 17,
      "end_page": 25,
      "summary": "Robert C.\nThis book is an in-depth guide to the concurrency and multithreading facilities from\nthe new C++ Standard, from the basic usage of std::thread, std::mutex, and std::\nChapter 10 covers the new parallelism support from C++17, which comes in the\nreference to the C++17 Thread Library.\nIf you're writing multithreaded code in C++, you should read this book.\nthe new multithreading facilities from the C++ Standard Library, this book is an essen-\nIf you haven’t used the new C++11 language facilities before, it might be worth\nthe facilities you know map onto the new standard C++ ones.\nthreaded C++.\nOnce you’re up to speed on the use of the C++ Thread Library, appendix D should\nthe publisher’s website at www.manning.com/books/c-plus-plus-concurrency-in-action-\nTo use the code from this book unchanged, you’ll need a recent C++ compiler that\nsupports the C++17 language features used in the examples (see appendix A), and\nyou’ll need a copy of the C++ Standard Thread Library.\ndio all ship with implementations of the C++17 Standard Thread Library.\nC++11 Standard Thread Library for several older compilers, along with an implemen-\nThe Boost Thread Library2 provides an API that’s based on the C++11 Standard\nPurchase of C++ Concurrency in Action, Second Edition includes free access to a pri-\nTo access the forum, go to www.manning.com/books/c-plus-plus-concurrency-\n1 The just::thread implementation of the C++ Standard Thread Library, http://www.stdthread.co.uk.\n2 The Boost C++ library collection, http://www.boost.org.\nis the author or coauthor of many of the C++ Standards Com-\nin the C++11 Standard.\nto enhance the C++ concurrency toolkit, both with standards\njust::thread Pro extensions to the C++ thread library from Just\nThe illustration on the cover of C++ Concurrency in Action is captioned “Habit of a Lady\nconcurrency in C++!\nThese are exciting times for C++ users.\ndard was published in 1998, the C++ Standards Committee gave the language and\nThe new C++ Standard (referred to as\nted to a new “train model” of releases, with a new C++ Standard to be published\nSo far, we've had two of these publications: the C++14 Standard in\n2014, and the C++17 Standard in 2017, as well as several Technical Specifications\ndescribing extensions to the C++ Standard.\nconcurrency in C++\nWhat a simple multithreaded C++ program \nHello, world of concurrency in C++!\nOne of the most significant new features in the C++11 Standard was the support of\nFor the first time, the C++ Standard acknowledged the exis-\nthreaded C++ programs without relying on platform-specific extensions and enabled\nC++14 and C++17 Standards have built upon this baseline to provide further support\nfor writing multithreaded programs in C++, as have the Technical Specifications.\nThis book is about writing programs in C++ using multiple threads for concur-\nrency and the C++ language features and library facilities that make it possible.\ncurrency support in C++, and we’ll round off this chapter with a simple example of\nC++ concurrency in action.\nwill finish with an in-depth reference to all the C++ Standard Library facilities for mul-",
      "keywords": [
        "Standard Thread Library",
        "Thread Library",
        "Frédéric Flayol",
        "Boost Thread Library",
        "ACKNOWLEDGMENTS xvi reviews",
        "Standard Thread",
        "library",
        "Standard",
        "Standard Library",
        "Standard Library facilities",
        "book",
        "concurrency",
        "thread",
        "library facilities",
        "ACKNOWLEDGMENTS xvi"
      ],
      "concepts": [
        "chapters",
        "threaded",
        "library",
        "libraries",
        "book",
        "standard",
        "code",
        "concurrency",
        "forum",
        "level"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.705,
          "base_score": 0.705,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.682,
          "base_score": 0.682,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.639,
          "base_score": 0.639,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.624,
          "base_score": 0.624,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.618,
          "base_score": 0.618,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "standard",
          "library",
          "thread library",
          "concurrency",
          "facilities"
        ],
        "semantic": [],
        "merged": [
          "standard",
          "library",
          "thread library",
          "concurrency",
          "facilities"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4871612745397802,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.854136+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 26-36)",
      "start_page": 26,
      "end_page": 36,
      "summary": "What is concurrency?\nmachine can only perform one task at a time, but it can switch between tasks many times\nthat the tasks are happening concurrently.\nconcurrency with such systems; because the task switches are so fast, you can’t tell at\nswitching provides the illusion of concurrency to both the user and the applications\nrunning more than one task in parallel.\nTwo approaches to concurrency: parallel execution on a dual-core \nHello, world of concurrency in C++!\ncessor or multicore systems, some processors can execute multiple threads on a single\nmore tasks than the hardware can run in parallel, so task switching is still used in these\naffecting the performance of concurrent code.\nyour application is running on a machine with one single-core processor or with many\nthrough task switching or by genuine hardware concurrency.\nhow you make use of concurrency in your application may well depend on the\ncuss the issues involved in designing concurrent code in C++.\nWhat is concurrency?\nto concurrency.\nThe first approach is to have multiple single-threaded processes, which is similar\ntiple threads in a single process, which is like having two developers in the same office.\nwhich are multithreaded and some of which are single-threaded, but the principles\nCONCURRENCY WITH MULTIPLE PROCESSES\nThe first way to make use of concurrency within an appli-\nsingle-threaded processes that are run at the same time,\ncan be easier to write safe concurrent code with processes rather than threads.\nUsing separate processes for concurrency also has an additional advantage—you\nThread\nThread\nrunning concurrently\nHello, world of concurrency in C++!\nCONCURRENCY WITH MULTIPLE THREADS\nThe alternative approach to concurrency is to run multiple\nthreads in a single process.\nweight processes: each thread runs independently of the\nBut all threads in a process share the same address\nthreads.\nure 1.4 shows two threads within a process communicating\nThe shared address space and lack of protection of data between threads makes\ntiple threads, the application programmer must ensure that the view of data seen by\nple threads within a process compared to launching and communicating between\nmultiple single-threaded processes means that this is the favored approach to concur-\nsupport for communication between processes, so applications that use multiple pro-\nThread 1\nThread 2\nrunning concurrently in a \nWhy use concurrency?\nprocessing, whereas people talk about concurrency when their primary concern is sepa-\nyou would use concurrency in your applications.\nWhy use concurrency?\nThere are two main reasons to use concurrency in an application: separation of con-\ncan use concurrency to separate distinct areas of functionality, even when the opera-\nof concurrency, you either have to write a task-switching framework or actively make\nthreading to separate these concerns, the user interface code and DVD playback code\nSimilarly, separate threads are often used to run tasks that must run continu-\nHello, world of concurrency in C++!\nUsing concurrency for performance: task and data parallelism\nrunning a single task faster but from running multiple tasks in parallel.\ncomputing power, it must be designed to run multiple tasks concurrently.\nThere are two ways to use concurrency for performance.\nThe divisions may be either in terms of processing—one thread performs one\npart of the algorithm while another thread performs a different part—or in terms of\ndata—each thread performs the same operation on different parts of the data.\nTechniques for dividing tasks between threads are covered in chapters 8 and 10.\nThe second way to use concurrency for performance is to use the available paral-\nWhy use concurrency?\nforming the same operation on multiple sets of data concurrently, there’s a different\nWhen not to use concurrency\nated with maintaining multithreaded code, don’t use concurrency.\nIf the task being run on the thread is completed quickly, the\ntime taken by the task may be dwarfed by the overhead of launching the thread, possi-\nbly making the overall performance of the application worse than if the task had been\nIf you have too many threads run-\naddress space for a process, because each thread requires a separate stack space.\nrun too many threads, this will eventually cause problems.\nIf the server side of a client/server application launches a separate thread for each\nful use of thread pools can provide optimal performance (see chapter 9).\nHello, world of concurrency in C++!\nFinally, the more threads you have running, the more context switching the oper-\nbest possible performance of the system, it’s necessary to adjust the number of threads\nThe use of concurrency for performance is like any other optimization strategy: it\nConcurrency and multithreading in C++\nStandardized support for concurrency through multithreading is a relatively new\nthreaded code without resorting to platform-specific extensions.\nstand the rationale behind lots of the decisions in the Standard C++ Thread Library,\nThe 1998 C++ Standard doesn’t acknowledge the existence of threads, and the opera-\nC++ Runtime Library (such as the code for the exception-handling mechanism) works\nNot content with using the platform-specific C APIs for handling multithreading,\nC++ programmers have looked to their class libraries to provide object-oriented\nConcurrency and multithreading in C++\nthreading that simplify tasks.\nmany C++ class libraries, and that provides considerable benefit to the programmer, is\nConcurrency support in the C++11 standard\nNot only is there a thread-\naware memory model, but the C++ Standard Library was extended to include classes\nfor managing threads (see chapter 2), protecting shared data (see chapter 3), syn-\nchronizing operations between threads (see chapter 4), and low-level atomic opera-\nThe C++11 Thread Library is heavily based on the prior experience accumulated\nBoost Thread Library was used as the primary model on which the new library is\nand the Boost Thread Library has itself changed to match the C++ Standard in many\nConcurrency support is one of the changes with the C++11 Standard—as men-\nthis book, some of those changes have had a direct impact on the Thread Library and\nHello, world of concurrency in C++!\nMore support for concurrency and parallelism in C++14 \nThe only specific support for concurrency and parallelism added in C++14 was a new\nwhich describes extensions to the functions and classes provided by the C++ Standard,\nespecially around synchronizing operations between threads (see chapter 4).\nEfficiency in the C++ Thread Library\nthose in the new Standard C++ Thread Library specifically—is that of efficiency.\nLibrary in general and the Standard C++ Thread Library in particular; one of the\nThe C++ Standard Library also provides higher-level abstractions and facilities that\ntoo many threads are competing for a mutex, it will impact the performance signifi-\nIn those rare cases where the C++ Standard Library doesn’t provide the perfor-\nAlthough the C++ Thread Library provides reasonably comprehensive facilities for\nmultithreading and concurrency, on any given platform there will be platform-specific\nwithout giving up the benefits of using the Standard C++ Thread Library, the types in\nthe C++ Thread Library may offer a native_handle() member function that allows\nsome functions might be running concurrently, so you need to ensure that shared\ntions concurrently, specific functions and objects must be used to manage the differ-",
      "keywords": [
        "Thread Library",
        "concurrency",
        "threads",
        "Standard Library",
        "Standard",
        "Boost Thread Library",
        "task",
        "Library",
        "multiple threads",
        "code",
        "performance",
        "task switching",
        "data",
        "application",
        "processes"
      ],
      "concepts": [
        "concurrency",
        "concurrently",
        "threads",
        "perform",
        "processing",
        "processes",
        "task",
        "code",
        "parallel",
        "parallelism"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.844,
          "base_score": 0.694,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 28,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "concurrency",
          "use concurrency",
          "library",
          "thread library",
          "processes"
        ],
        "semantic": [],
        "merged": [
          "concurrency",
          "use concurrency",
          "library",
          "thread library",
          "processes"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4214142114923226,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854194+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 37-49)",
      "start_page": 37,
      "end_page": 49,
      "summary": "we move to multiple threads:\n#include <thread>           \nstd::thread t(hello);   3\nthreading support in the Standard C++ Library are in new headers: the functions and\nThis is because every thread has to have an initial function, where the new thread of\nevery other thread it’s specified in the constructor of a std::thread object—in this\ncase, the std::thread object named t has the new hello() function as its initial\nthread (in main()) to wait for the thread associated with the std::thread object, in\nIn chapter 2, we’ll look at the classes and functions available for managing threads.\nManaging threads\nthing managed through the std::thread object associated with a given thread, as\nparameters to the thread function when it’s launched and how to transfer owner-\nship of a thread from one std::thread object to another.\ncode to run on a new thread\nthread running main().\nThese threads then run concurrently with each\nother and with the initial thread.\nsee, if you have a std::thread object for a thread, you can wait for it to finish; but first\nLaunching a thread\nAs you saw in chapter 1, threads are started by constructing a std::thread object that\ndown to constructing a std::thread object:\nstd::thread my_thread(do_some_work);\nheader is included so the compiler can see the definition of the std::thread class.\nwith much of the C++ Standard Library, std::thread works with any callable type, so\nyou can pass an instance of a class with a function call operator to the std::thread\nstd::thread my_thread(f);\nManaging threads\nOne thing to consider when passing a function object to the thread constructor is\nstd::thread my_thread(background_task());\ndeclares a my_thread function that takes a single parameter (of type pointer-to-a-\nreturns a std::thread object, rather than launching a new thread.\nstd::thread my_thread((background_task()));       \nstd::thread my_thread{background_task()};         \nration, allowing my_thread to be declared as a variable of type std::thread.\nstd::thread my_thread([]{\nIf you don’t decide before the std::thread object\nis destroyed, then your program is terminated (the std::thread destructor calls\nbefore the std::thread object is destroyed—the thread itself may well have finished\nstd::thread object is destroyed; it will only stop running when it finally returns from\nthe thread function.\nproblem—even in single-threaded code it’s undefined behavior to access an object\nstd::thread my_thread(my_func);\nmy_thread.detach();             \nA function that returns while a thread still has access to local variables\nAccessing a local variable with a detached thread after it has been destroyed\nMain thread\nNew thread\nDetaches my_thread\nthread to finish\nNew thread might \nManaging threads\nobject for your thread function, that object is copied into the thread, so the original\na thread within a function that has access to the local variables in that function, unless\nthe thread is guaranteed to finish before the function exits.\nfunction exits by joining with the thread.\nIf you need to wait for a thread to complete, you can do this by calling join() on\nthe associated std::thread instance.\nated with the thread, so the std::thread object is no longer associated with the now-\nonly once for a given thread; once you’ve called join(), the std::thread object is no\ndetach() before a std::thread object is destroyed.\nMain thread\nNew thread\nstd::thread t(my_func);\nstd::thread& t;\nexplicit thread_guard(std::thread& t_):\nManaging threads\n~thread_guard()\nstd::thread t(my_func);\nWhen the execution of the current thread reaches the end of f, the local objects are\nConsequently, the thread_guard object,\npens if the function exits because do_something_in_current_thread throws an\nthe thread had already been joined.\nBy declaring them as deleted, any attempt to copy a thread_guard object will\nThis breaks the association of the thread with the std::thread\nobject and ensures that std::terminate() won’t be called when the std::thread\nobject is destroyed, even though the thread is still running in the background.\nRunning threads in the background\nCalling detach() on a std::thread object leaves the thread to run in the back-\na std::thread object that references it, so it can no longer be joined.\nber function of the std::thread object.\nAfter the call completes, the std::thread\nobject is no longer associated with the actual thread of execution and is therefore no\nstd::thread t(do_background_work);\nIn order to detach the thread from a std::thread object, there must be a thread to\ndetach: you can’t call detach() on a std::thread object with no associated thread of\nexactly the same way—you can only call t.detach() for a std::thread object t when\nManaging threads\nstd::thread t(edit_document,new_name);                \nused to start a thread: rather than just passing the name of the function to the\nstd::thread constructor, you also pass in the filename parameter.\nPassing arguments to a thread function\nmentally as simple as passing additional arguments to the std::thread constructor.\nstd::thread t(f,3,”hello”);\nthe new thread.\nPassing arguments to a thread function\nstd::thread t(f,3,buffer);         \nnew thread and there’s a significant chance that the oops function will exit before\nthe buffer has been converted to a std::string on the new thread, thus leading to\nto the std::thread constructor:\nstd::thread t(f,3,std::string(buffer));     \nbut this conversion happens too late because the std::thread constructor copies the\nYou might try this if the thread is\nstd::thread t(update_data_for_widget,w,data);       \nerence, the std::thread constructor doesn’t know that; it’s oblivious to the types of\nstd::thread t(update_data_for_widget,w,std::ref(data));\nManaging threads\nprising, because both the operation of the std::thread constructor and the opera-\nstd::thread t(&X::do_lengthy_work,&my_x);\nmember function call: the third argument to the std::thread constructor will be the\nstd::move to transfer ownership of a dynamic object into a thread:\nstd::thread t(process_big_object,std::move(p));\nsemantics as std::unique_ptr, and std::thread is one of them.\nThough std::thread",
      "keywords": [
        "thread",
        "thread object",
        "std",
        "object",
        "function",
        "thread function",
        "thread constructor",
        "thread function std",
        "managing threads",
        "Concurrent World",
        "data",
        "local",
        "n’t",
        "standard",
        "initial thread"
      ],
      "concepts": [
        "thread",
        "std",
        "functions",
        "function",
        "object",
        "data",
        "void",
        "hello",
        "listing",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 6,
          "title": "",
          "score": 0.747,
          "base_score": 0.747,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "std thread",
          "thread object",
          "std",
          "object"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "std thread",
          "thread object",
          "std",
          "object"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5114485345695756,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854253+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 50-58)",
      "start_page": 50,
      "end_page": 58,
      "summary": "Transferring ownership of a thread\nownership can be transferred between instances, because instances of std::thread are\nTransferring ownership of a thread\nThis is where the move support of std::thread comes in.\nbe moved between std::thread instances, as in the following example.\nthreads among three std::thread instances, t1, t2, and t3:\nstd::thread t1(some_function);         \nstd::thread t2=std::move(t1);                \nt1=std::thread(some_other_function);   \nstd::thread t3;                              \nThen, a new thread is started and associated with a temporary std::thread object.\nThe final move transfers ownership of the thread running some_function back to\nManaging threads\nThis is done for consistency with the std::thread destructor.\nvalue to the std::thread object that manages it.\nThe move support in std::thread means that ownership can readily be trans-\nstd::thread f()\nreturn std::thread(some_function);\nstd::thread g()\nstd::thread t(some_other_function,42);\nof std::thread by value as one of the parameters, as shown here:\nvoid f(std::thread t);\nf(std::thread(some_function));\nstd::thread t(some_function);\nOne benefit of the move support of std::thread is that you can build on the\nstd::thread t;\nexplicit scoped_thread(std::thread t_):        \nReturning a std::thread from a function\nTransferring ownership of a thread\n~scoped_thread()\nscoped_thread t{std::thread(func(some_local_state))};   \ndo_something_in_current_thread();\nlar to std::thread, except that it would automatically join in the destructor much like\nscoped_thread does.\nstd::thread t;\nexplicit joining_thread(std::thread t_) noexcept:\nManaging threads\njoining_thread& operator=(std::thread other) noexcept\n~joining_thread() noexcept\nvoid swap(joining_thread& other) noexcept\nstd::thread::id get_id() const noexcept{\nstd::thread& as_thread() noexcept\nconst std::thread& as_thread() const noexcept\nThe move support in std::thread also allows for containers of std::thread objects,\nstd::vector<std::thread> threads;\ntransferring the results of operations between threads are discussed in chapter 4.\nOne feature of the C++ Standard Library that helps here is std::thread::hardware_\nThis function returns an indication of the number of threads that can\ntions are possible; the std::thread constructor will throw if it can’t start a new thread\nSpawns threads\nManaging threads\nstd::thread::hardware_concurrency();\nstd::vector<T> results(num_threads);\nthreads[i]=std::thread(                             \nblock_start,last,results[num_threads-1]);    \nIf the call to std::thread::\nNow that you know how many threads you have, you can create a std::vector<T>\nfor the intermediate results and a std::vector<std::thread> for the threads.\nthreads you spawned with std::for_each, as in listing 2.8, and then add up the results\nAlternative ways of returning results from threads are\nManaging threads\nIdentifying threads\nThread identifiers are of type std::thread::id and can be retrieved in two ways.\nFirst, the identifier for a thread can be obtained from its associated std::thread object\nIf the std::thread object doesn’t have an\nstd::thread::id object, which indicates “not any thread.” Alternatively, the identifier\nfor the current thread can be obtained by calling std::this_thread:: get_id(),\nObjects of type std::thread::id can be freely copied and compared; they wouldn’t\nIf two objects of type std::thread::id are\nare the same or not; objects of type std::thread::id offer the complete set of com-\norder for all non-equal values of std::thread::id, so they behave as you’d intuitively\nstd::hash<std::thread::id> so that values of type std::thread::id can be used as\nInstances of std::thread::id are often used to check whether a thread needs to\nFor example, if threads are used to divide work, as in listing\nstd::this_thread::get_id() before launching the other threads, and then the core\nstd::thread::id master_thread;\nif(std::this_thread::get_id()==master_thread)\nAlternatively, the std::thread::id of the current thread could be stored in a data\nThe idea is that std::thread::id will suffice as a generic identifier for a thread in\neven write out an instance of std::thread::id to an output stream such as std::cout:\nstd::cout<<std::this_thread::get_id();",
      "keywords": [
        "thread",
        "std",
        "function",
        "ownership",
        "move",
        "n’t",
        "listing",
        "block",
        "thread object",
        "number",
        "void",
        "Standard Library",
        "type std",
        "Standard",
        "results"
      ],
      "concepts": [
        "thread",
        "std",
        "void",
        "listing",
        "operator",
        "operations",
        "operation",
        "operate",
        "object",
        "value"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.747,
          "base_score": 0.747,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.708,
          "base_score": 0.708,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.627,
          "base_score": 0.627,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.569,
          "base_score": 0.569,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std thread",
          "thread",
          "std",
          "thread id",
          "id"
        ],
        "semantic": [],
        "merged": [
          "std thread",
          "thread",
          "std",
          "thread id",
          "id"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46902933151708504,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.854313+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "summary": "Sharing data\naging threads, let’s look at the issues surrounding shared data.\nIf you’re sharing data between threads, you need to\nhave rules for which thread can access which bit of data when, and how any updates\nProblems with sharing data between threads\nProtecting data with mutexes\nAlternative facilities for protecting shared data\nProblems with sharing data between threads\nare communicated to the other threads that care about that data.\ndata can be shared between multiple threads in a single process is not only a benefit—\nThis chapter is about sharing data safely between threads in C++, avoiding the\nProblems with sharing data between threads\nWhen it comes down to it, the problems with sharing data between threads are all due\nIf all shared data is read-only, there’s no problem,\nsame data.\nBut if data is shared between threads, and one or more threads start modify-\nThe simplest potential problem with modifying data that’s shared between threads\none thread is reading the doubly linked list while another is removing a node, it’s\nSharing data between threads\nProblems with sharing data between threads\nThe C++ Standard also defines the term data race to mean the specific\naccess the data structure when only one of them has been completed.\nis to wrap your data structure with a protection mechanism to ensure that only the\nFrom the point of view of other threads accessing that data structure, such\nSharing data between threads\nAnother option is to modify the design of your data structure and its invariants so\nAnother way of dealing with race conditions is to handle the updates to the data\nThe most basic mechanism for protecting shared data provided by the C++ Stan-\nProtecting shared data with mutexes\nSo, you have a shared data structure such as the linked list from the previous section,\nand you want to protect it from race conditions and the potential broken invariants\nthe data structure as mutually exclusive, so that if any thread was running one of them,\nany other thread that tried to access that data structure had to wait until the first\nBefore accessing a shared data struc-\nture, you lock the mutex associated with that data, and when you’ve finished accessing\nthe data structure, you unlock the mutex.\nensures that all threads see a self-consistent view of the shared data, without any bro-\nMutexes are the most general of the data-protection mechanisms available in C++,\nProtecting shared data with mutexes\ndata (see section 3.2.2) and avoid race conditions inherent in your interfaces (see sec-\nsection 3.2.4) and protecting either too much or too little data (see section 3.2.8).\ning shows how to protect a list that can be accessed by multiple threads using\nProtecting a list with a mutex\nSharing data between threads\nthe majority of cases it’s common to group the mutex and the protected data together\ntions of the class, and the mutex and protected data would both become private\ndata and thus which code needs to lock the mutex.\nthe class lock the mutex before accessing any other data members and unlock it when\ndone, the data is nicely protected from all comers.\nmember functions returns a pointer or reference to the protected data, then it doesn’t\nerence can now access (and potentially modify) the protected data without locking the mutex.\nProtecting data with a mutex therefore requires careful interface design to ensure\nthat the mutex is locked before there’s any access to the protected data and that there\nStructuring code for protecting shared data\nAs you’ve seen, protecting data with a mutex is not quite as easy as slapping an\nProtecting shared data with mutexes\nclass some_data\nvoid process_data(Function func)\nsome_data* unprotected;\nvoid malicious_function(some_data& protected_data)\nunprotected=&protected_data;\nIn this example, the code in process_data looks harmless enough, nicely protected\nto do: mark all the pieces of code that access the data structure as mutually exclusive.\nhelp you with; it’s up to you as programmers to lock the right mutex to protect your\ndata.\nDon’t pass pointers and references to protected data outside the scope of the lock, whether by\npossible to have race conditions, even when data is protected with a mutex.\nAccidentally passing out a reference to protected data\nto protected data",
      "keywords": [
        "data",
        "data structure",
        "shared data",
        "mutex",
        "threads",
        "protecting shared data",
        "protected data",
        "Race conditions",
        "list",
        "std",
        "race",
        "lock",
        "node",
        "data between threads",
        "n’t"
      ],
      "concepts": [
        "list",
        "threads",
        "function",
        "functions",
        "functionality",
        "std",
        "race",
        "protecting",
        "protection",
        "condition"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "sharing data",
          "shared data",
          "protecting",
          "protected"
        ],
        "semantic": [],
        "merged": [
          "data",
          "sharing data",
          "shared data",
          "protecting",
          "protected"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4743725050036459,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854370+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 67-78)",
      "start_page": 67,
      "end_page": 78,
      "summary": "data structure like the std::stack container adapter shown in listing 3.3.\nthe constructors and swap(), there are only five things you can do to a std::stack:\ndata with a mutex, this interface is still inherently subject to race conditions.\ntemplate <class Alloc> stack(Container&&, const Alloc&);\nThe interface to the std::stack container adapter\nProtecting shared data with mutexes\nfree to access the stack and might push() new elements onto or pop() the existing\nones off of the stack before the thread that called empty() or size() could use that\nIn particular, if the stack instance is not shared, it’s safe to check for empty() and\nclassic race condition, and the use of a mutex internally to protect the stack contents\ntop() will throw an exception if there aren’t any elements in the stack when it’s called.\nIf the stack is protected by a mutex internally, only one thread can be running a\ntwo calls to top() to modify the stack, so both threads will see the same value.\nwas defined to return the value popped, as well as remove it from the stack, you have a\npotential problem: the value being popped is returned to the caller only after the stack\nA possible ordering of operations on a stack from two threads\nProtecting shared data with mutexes\nThe designers of the std::stack interface\nfrom the stack (pop()), so that if you can’t safely copy the data, it stays on the stack.\ncalling code to construct an instance of the stack’s value type prior to the call, in order\nthe use of your thread-safe stack to those types that can safely be returned by value\nsuch types couldn’t be stored in your thread-safe stack.\nEXAMPLE DEFINITION OF A THREAD-SAFE STACK\nListing 3.4 shows the class definition for a stack with no race conditions in the\nstruct empty_stack: std::exception\nAn outline class definition for a thread-safe stack\nProtecting shared data with mutexes\ndescription of option 3, the use of std::shared_ptr allows the stack to take care of\nYour five stack operations have now become three: push(), pop(), and empty().\nover the data; you can ensure that the mutex is locked for the entirety of an operation.\nstd::stack<>.\nstruct empty_stack: std::exception\nstd::stack<T> data;\nstd::lock_guard<std::mutex> lock(other.m);\nstd::lock_guard<std::mutex> lock(m);\nstd::lock_guard<std::mutex> lock(m);\nif(data.empty()) throw empty_stack();    \nstd::lock_guard<std::mutex> lock(m);\nif(data.empty()) throw empty_stack();\nA fleshed-out class definition for a thread-safe stack\nstd::lock_guard<std::mutex> lock(m);\nThis stack implementation is copyable—the copy constructor locks the mutex in the\nbody B rather than the member initializer list in order to ensure that the mutex is\nProblems with mutexes can also arise from lock-\nthan one mutex locked in order to protect all the data in an operation.\nered by the mutexes, so that only one mutex needs to be locked.\nIf you end up having to lock two or more mutexes for a given operation, there’s\nProtecting shared data with mutexes\nover locks on mutexes: each of a pair of threads needs to lock both of a pair of\nmutexes to perform some operation, and each thread has one mutex and is waiting\nhaving to lock two or more mutexes in order to perform an operation.\nThe common advice for avoiding deadlock is to always lock the two mutexes in the\nsame order: if you always lock mutex A before mutex B, then you’ll never deadlock.\nmutexes on both instances must be locked.\nstd::lock—a function that can lock two or more mutexes at once without risk of\nUsing std::lock() and std::lock_guard in a swap operation\nstd::lock(lhs.m,rhs.m);    \nstd::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock);  \nstd::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock);  \nattempting to acquire a lock on std::mutex when you already hold it is undefined\n(A mutex that does permit multiple locks by the same thread is provided in\nstd::lock() B locks the two mutexes, and two std::lock_guard instances are con-\nThe std::adopt_lock parameter is supplied\nin addition to the mutex to indicate to the std::lock_guard objects that the mutexes\nmutex rather than attempt to lock the mutex in the constructor.\nto std::lock can throw an exception; in this case, the exception is propagated out of\nstd::lock.\nIf std::lock has successfully acquired a lock on one mutex and an excep-\ntion is thrown when it tries to acquire a lock on the other mutex, this first lock is\nlocking the supplied mutexes.\nplate, std::scoped_lock<>.\nThis is exactly equivalent to std::lock_guard<>, except\nare locked using the same algorithm as std::lock, so that when the constructor com-\nstd::scoped_lock guard(lhs.m,rhs.m);    \nstd::scoped_lock, because that is a C++17 library facility), the C++17 implicit class\nProtecting shared data with mutexes\nstd::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);\nThe existence of std::scoped_lock means that most of the cases where you would\nAlthough std::lock (and std::scoped_lock<>) can help you avoid deadlock in\nthose cases where you need to acquire two or more locks together, it doesn’t help if\ncan create deadlock with two threads and no locks by having each thread call join()\nto this guideline, it’s impossible to get a deadlock from the lock usage alone because\neach thread only ever holds a single lock.\nthings (like the threads waiting for each other), but mutex locks are probably the\ngle action with std::lock in order to acquire them without deadlock.\nAVOID CALLING USER-SUPPLIED CODE WHILE HOLDING A LOCK\na lock.\nIf you call user-supplied code while holding a lock, and that code acquires a\ngle operation with std::lock, the next best thing is to acquire them in the same order\nfrom section 3.2.3—the mutex is internal to each stack instance, but the operations\non the data items stored in a stack require calling user-supplied code.\nuser of the stack, but it’s rather uncommon for the data stored in a container to access\nAt least in that case you could lock the mutexes simultaneously,\nThen, in order to access the list, threads must acquire a lock on every node\nFor a thread to delete an item, it must then acquire the lock on\nOnce the lock on the next node has been acquired, the lock on the first can be\nThis hand-over-hand locking style allows multiple threads to access the list, pro-\nnodes must always be locked in the same order: if two threads tried to traverse the\nlist in opposite orders using hand-over-hand locking, they could deadlock with\nthread going one way will try to hold the lock on node A and try to acquire the lock\nA thread going the other way would be holding the lock on node B and\ntrying to acquire the lock on node A—a classic scenario for deadlock, as shown in\nacquires the lock on B before the locks on A and C, it has the potential to deadlock\nSuch a thread would try to lock either A or C first\nlock on B because the thread doing the deleting was holding the lock on B and trying\nto acquire the locks on A and C.\nProtecting shared data with mutexes\nalways lock A before B and B before C.\nyou divide your application into layers and identify all the mutexes that may be locked\nWhen code tries to lock a mutex, it isn’t permitted to lock that mutex\nlayer numbers to each mutex and keeping a record of which mutexes are locked by\nLock master entry mutex\nLock head node mutex\nLock master entry mutex\nLock tail node mutex\nLock next node mutex\nLock node A mutex\nLock node C mutex\nLock node B mutex\nBlock trying to lock node B mutex\nBlock trying to lock node A mutex\nDeadlock with threads traversing a list in opposite orders",
      "keywords": [
        "lock",
        "stack",
        "std",
        "mutex",
        "thread",
        "data",
        "n’t",
        "mutexes",
        "empty",
        "node",
        "const",
        "deadlock",
        "Lock node",
        "call",
        "exception"
      ],
      "concepts": [
        "lock",
        "stack",
        "threads",
        "data",
        "exception",
        "exceptional",
        "exceptions",
        "operation",
        "operations",
        "operator"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.642,
          "base_score": 0.642,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.616,
          "base_score": 0.616,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.612,
          "base_score": 0.612,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lock",
          "stack",
          "mutex",
          "mutexes",
          "std"
        ],
        "semantic": [],
        "merged": [
          "lock",
          "stack",
          "mutex",
          "mutexes",
          "std"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.47058678533877824,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.854453+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 79-87)",
      "start_page": 79,
      "end_page": 87,
      "summary": "std::lock_guard<hierarchical_mutex> lk(low_level_mutex);   \nstd::lock_guard<hierarchical_mutex> lk(high_level_mutex);  \nstd::lock_guard<hierarchical_mutex> lk(other_mutex);    \ndefined so that if you hold a lock on a hierarchical_mutex, then you can only acquire\na lock on another hierarchical_mutex with a lower hierarchy number, this imposes\nAssuming do_low_level_stuff doesn’t lock any mutexes, low_level_func is the\nbottom of your hierarchy, and locks the low_level_mutex e.\nlow_level_func f, while holding a lock on high_level_mutex g, but that’s OK,\nUsing a lock hierarchy to prevent deadlock\nFirst off, it locks other_mutex j, which has a hierarchy value of only 6000 d.\ntime if they’re the same level in the hierarchy, so hand-over-hand locking schemes\nthe three member functions required to satisfy the mutex concept: lock(), unlock(),\nthe lock on the mutex is held by another thread, it returns false rather than waiting\nuntil the calling thread can acquire the lock on the mutex.\nthread is allowed to lock that mutex.\nvoid lock()\ninternal_mutex.lock();          \nif(!internal_mutex.try_lock())     \nhierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);    \nmaximum value i, so initially any mutex can be locked.\nSo, the first time a thread locks an instance of hierarchical_mutex, the value of\ncheck out of the way, lock()delegates to the internal mutex for the locking e.\nthis lock has succeeded, you can update the hierarchy value f.\nIf you now lock another hierarchical_mutex while holding the lock on this first\nThe hierarchy value of this second mutex must now be less than that of\nlock a mutex with a higher hierarchy value again, even if the thread didn’t hold any\nlocks.\nlock on the internal mutex.\ninternal_mutex fails h, then you don’t own the lock, so you don’t update the hierar-\nwait for a thread while holding a lock, because that thread might need to acquire the\nFor those cases, the Standard Library provides the std::unique_lock\nLike std::lock_guard, this is a class template parameterized on the mutex\nFlexible locking with std::unique_lock\nthe invariants; an std::unique_lock instance doesn’t always own the mutex that it’s\nconstructor to have the lock object manage the lock on a mutex, you can also pass\nstd::defer_lock as the second argument to indicate that the mutex should remain\nstd::unique_lock object (not the mutex) or by passing the std:: unique_lock object\nto std::lock().\nstd::adopt_lock.\nthan std::lock_guard.\nThe flexibility of allowing an std::unique_lock instance not\nstd::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock);  \nstd::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock);  \nIn listing 3.9, the std::unique_lock objects could be passed to std::lock() c,\nbecause std::unique_lock provides lock(), try_lock(), and unlock() member\ning mutex to do the work and update a flag inside the std::unique_lock instance to\ndoing something else that requires std::unique_lock, you’re still better off using the\nC++17 variadic std::scoped_lock if it’s available to you (see section 3.2.4).\nstd:: lock_guard because the flag has to be updated or checked, as appropriate.\nThat said, there are cases where std::unique_lock is a better fit for the\nUsing std::lock() and std::unique_lock in a swap operation\nstd::defer_lock\nlocked here.\nBecause std::unique_lock instances don’t have to own their associated mutexes, the\nstd::unique_lock is\nOne possible use is to allow a function to lock a mutex and transfer ownership of\nprotection of the same lock.\nget_lock() function locks the mutex and then prepares the data before returning\nstd::unique_lock<std::mutex> get_lock()\nstd::unique_lock<std::mutex> lk(some_mutex);\nstd::unique_lock<std::mutex> lk(get_lock());    \nits own std::unique_lock instance c, and the call to do_something() can rely on\nTypically this sort of pattern would be used where the mutex to be locked is depen-\nthat returns the std::unique_lock object.\nlocked access to some protected data.\nlock and allows other threads to access the protected data.\nwell be moveable (so that it can be returned from a function), in which case the lock\nThe flexibility of std::unique_lock also allows instances to relinquish their locks\nstd::unique_lock supports the same basic set of member functions for lock-\nas std::lock.\nThe ability to release a lock before the std::unique_lock instance is\nperformance, because other threads waiting for the lock are prevented from proceed-\nlock.\nlock granularity to ensure the required data is protected, but it’s also important to\n(the cashier at the checkout), then if any thread holds the lock for longer than neces-\nWhere possible, lock a mutex only\nstd::unique_lock works well in this situation, because you can call unlock()\nwhen the code no longer needs access to the shared data and then call lock() again if\nstd::unique_lock<std::mutex> my_lock(the_mutex);\nmy_lock.unlock();                                 \nYou don’t need the mutex locked across the call to process(), so you manually\nwill require a lock on the same mutex, so the lock must be held longer.\namount of data locked; it’s also about how long the lock is held and what operations\nIn listings 3.6 and 3.9, the operation that required locking the two mutexes was a\nmutex for the minimum amount of time and also that you weren’t holding one lock\nwhile locking another.\nstd::lock_guard<std::mutex> lock_a(m);    \nLocking one mutex at a time in a comparison operator\nlocked across call \nIn either case, locking a mutex after the data has been initialized, purely in",
      "keywords": [
        "lock",
        "mutex",
        "std",
        "hierarchy",
        "data",
        "thread",
        "unique",
        "level",
        "n’t",
        "hierarchical",
        "lock object",
        "lock instance",
        "shared data",
        "object",
        "high"
      ],
      "concepts": [
        "lock",
        "std",
        "threads",
        "void",
        "value",
        "data",
        "hierarchy",
        "require",
        "protecting",
        "protection"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.869,
          "base_score": 0.719,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.803,
          "base_score": 0.653,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.711,
          "base_score": 0.711,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lock",
          "std unique_lock",
          "unique_lock",
          "std",
          "mutex"
        ],
        "semantic": [],
        "merged": [
          "lock",
          "std unique_lock",
          "unique_lock",
          "std",
          "mutex"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45788509508450453,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854511+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "summary": "Protecting shared data during initialization\nThis is because each thread must wait on the mutex in\nstd::mutex resource_mutex;\nstd::unique_lock<std::mutex> lk(resource_mutex);   \ndouble-checked part) in case another thread has done the initialization between the first\ncheck and this thread acquiring the lock:\nThread-safe lazy initialization using a mutex\nAll threads are \nSharing data between threads\nstd::lock_guard<std::mutex> lk(resource_mutex);\nRather than locking a mutex and explicitly checking the pointer,\nevery thread can use std::call_once, safe in the knowledge that the pointer will\nIn this example, both the std::once_flag B and data being initialized are namespace-\nIt’s worth noting that like std::mutex, std::once_flag instances can’t be copied\nmay try to use it after initialization has started on another thread but before it’s fin-\nThread-safe lazy initialization of a class member using std::call_once\nSharing data between threads\nplete, so the race condition is over which thread gets to do the initialization rather\nMultiple threads can then call get_my_class_instance() safely B, without having to\nfrom multiple threads, it will need to be appropriately protected during updates to\nensure that none of the threads reading the cache see a broken data structure.\ncomplete, the data structure is again safe for multiple threads to access concurrently.\nUsing std::mutex to protect the data structure is therefore overly pessimistic, because\nThe C++17 Standard Library provides two such mutexes out of the box, std::\nshared_mutex and std::shared_timed_mutex.\nC++14 only features std::shared_\nThe difference between std::shared_mutex and\nstd::shared_timed_mutex is that std::shared_timed_mutex supports additional\noperations (as described in section 4.3), so std::shared_mutex might offer a perfor-\nRather than using an instance of std::mutex for the synchronization, you use an\ninstance of std::shared_mutex.\nFor the update operations, std::lock_guard\n<std::shared_mutex> and std::unique_lock<std::shared_mutex> can be used for\nthe locking, in place of the corresponding std::mutex specializations.\nexclusive access, as with std::mutex.\ndata structure can instead use std::shared_lock<std::shared_mutex> to obtain\nstd::unique_lock, except that multiple threads may have a shared lock on the same\nstd::shared_mutex at the same time.\nshared lock, a thread that tries to acquire an exclusive lock will block until all other\nno other thread may acquire a shared or exclusive lock until the first thread has relin-\nstd::map to hold the cached data, protected using std::shared_mutex.\nmutable std::shared_mutex entry_mutex;\nstd::shared_lock<std::shared_mutex> lk(entry_mutex);      \nvoid update_or_add_entry(std::string const& domain,\nProtecting a data structure with std::shared_mutex\nSharing data between threads\nstd::lock_guard<std::shared_mutex> lk(entry_mutex);  \nIn listing 3.13, find_entry() uses an instance of std::shared_lock<> to protect it for\nshared, read-only access B; multiple threads can therefore call find_entry() simulta-\ninstance of std::lock_guard<> to provide exclusive access while the table is updated\nWith std::mutex, it’s an error for a thread to try to lock a mutex it already owns, and\nwould be desirable for a thread to reacquire the same mutex several times without\nstd::recursive_mutex.\nIt works like std::mutex, except that you can acquire multi-\nple locks on a single instance from the same thread.\nbefore the mutex can be locked by another thread, so if you call lock() three times,\nThe correct use of std::lock_guard\n<std::recursive_mutex> and std::unique_lock<std::recursive_mutex> will han-\ndesigned to be accessible from multiple threads concurrently, so it has a mutex pro-\nEach public member function locks the mutex, does the\nfunction will also try to lock the mutex, leading to undefined behavior.\nfunction that’s called from both member functions, which does not lock the mutex\nsharing data between threads and how to use std::mutex and careful interface design\navoid that in the form of std::lock().\nthe alternative data-protection facilities provided for specific scenarios, such as std::\ncall_once() and std::shared_mutex.\nthreads.\nfor synchronizing operations between threads in C++; chapter 6 shows how these can\nthreads.",
      "keywords": [
        "std",
        "mutex",
        "data",
        "resource",
        "shared",
        "thread",
        "lock",
        "call",
        "Standard Library",
        "initialization",
        "data structure",
        "shared data",
        "protecting shared data",
        "multiple threads",
        "ptr"
      ],
      "concepts": [
        "std",
        "threads",
        "locking",
        "entries",
        "entry",
        "protection",
        "protected",
        "initialization",
        "initialized",
        "initializes"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.866,
          "base_score": 0.716,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 11,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 9,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std",
          "std shared_mutex",
          "shared_mutex",
          "mutex",
          "std mutex"
        ],
        "semantic": [],
        "merged": [
          "std",
          "std shared_mutex",
          "shared_mutex",
          "mutex",
          "std mutex"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.49013944854654873,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854571+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "summary": "Waiting for an event or other condition\nWaiting for an event or other condition\ndata (protected by a mutex) and have the second thread set the flag when it com-\ning time repeatedly checking the flag, and when the mutex is locked by the waiting\nthe mutex protecting the flag in order to check it, the thread being waited for is\nSimilarly, the waiting thread is\nA second option is to have the waiting thread sleep for short periods between the\nchecks using the std::this_thread::sleep_for() function (see section 4.3):\nthread will keep on sleeping even when the task it’s waiting for is complete, introduc-\nable is associated with an event or other condition, and one or more threads can wait\nsatisfied, it can then notify one or more of the threads waiting on the condition vari-\nWaiting for a condition with condition variables\nHow do you let the thread that’s waiting for work sleep until there’s\nstd::queue<data_chunk> data_queue;      \nstd::condition_variable data_cond;\nvoid data_preparation_thread()\ndata_queue.push(data);              \nWaiting for data to process with std::condition_variable\nWaiting for an event or other condition\nvoid data_processing_thread()\nlk,[]{return !data_queue.empty();});     \ndata_queue.pop();\nFirst off, you have a queue B that’s used to pass the data between the two threads.\nWhen the data is ready, the thread preparing the data locks the mutex protecting the\nqueue using a std::lock_guard and pushes the data onto the queue c.\nthe notify_one() member function on the std::condition_variable instance to\nnotify the waiting thread (if there is one) d.\ndata onto the queue in a smaller scope, so you notify the condition variable after\nunlocking the mutex — this is so that, if the waiting thread wakes immediately, it\nThe thread then calls wait() on the std::\nIn this case, the simple []{return !data_queue.empty();} lambda function checks to\nthe condition isn’t satisfied (the lambda function returned false), wait() unlocks\nthe mutex and puts the thread in a blocked or waiting state.\nthe condition again, returning from wait() with the mutex still locked if the condi-\nthe std::lock_guard—the waiting thread must unlock the mutex while it’s waiting\nthe mutex remained locked while the thread was sleeping, the data-preparation\nthread wouldn’t be able to lock the mutex to add an item to the queue, and the wait-\nDuring a call to wait(), a condition variable may\nWhen the waiting thread reacquires the mutex and\nFundamentally, std::condition_variable::wait is an optimization over a busy-wait.\nvoid minimal_wait(std::unique_lock<std::mutex>& lk,Predicate pred){\nThe flexibility to unlock a std::unique_lock isn’t just used for the call to wait();\nUsing a queue to transfer data between threads, as in listing 4.1, is a common sce-\nBuilding a thread-safe queue with condition variables\nWaiting for an event or other condition\na queue to pass data between threads, the receiving thread often needs to wait for the\nstd::queue<T> data_queue;\nstd::condition_variable data_cond;\ndata_queue.push(new_value);\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nvalue=data_queue.front();\ndata_queue.pop();\nvoid data_preparation_thread()\nWaiting for an event or other condition\ndata_queue.push(data);        \nvoid data_processing_thread()\ndata_queue.wait_and_pop(data);    \nThe mutex and condition variable are now contained within the threadsafe_queue\ncondition variable wait d.\nstd::queue<T> data_queue;\nstd::condition_variable data_cond;\ndata_queue.push(new_value);\nFull class definition of a thread-safe queue using condition variables\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nvalue=data_queue.front();\ndata_queue.pop();\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nstd::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));\ndata_queue.pop();\nif(data_queue.empty())\nvalue=data_queue.front();\ndata_queue.pop();\nif(data_queue.empty())\ndata_queue.pop();\nreturn data_queue.empty();\nCondition variables are also useful where there’s more than one thread waiting for\nlisting 4.1 can be used; just run multiple instances of the data-processing thread.",
      "keywords": [
        "std",
        "data",
        "queue",
        "condition",
        "mutex",
        "thread",
        "Waiting",
        "lock",
        "waiting thread",
        "variable",
        "pop",
        "const",
        "condition variable",
        "function",
        "void"
      ],
      "concepts": [
        "threads",
        "waiting",
        "void",
        "data",
        "queue",
        "listing",
        "condition",
        "conditions",
        "returns",
        "times"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 44,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data_queue",
          "condition",
          "waiting",
          "queue",
          "event condition"
        ],
        "semantic": [],
        "merged": [
          "data_queue",
          "condition",
          "waiting",
          "queue",
          "event condition"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4739359792944274,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854628+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 104-111)",
      "start_page": 104,
      "end_page": 111,
      "summary": "the processing threads can all use the same data but need to wait for it to be initialized\ncases, the thread preparing the data can call the notify_all() member function on\nIf a thread needs to wait for a specific one-off event, it somehow obtains a future\nThe thread can then periodically wait on the future for short\nclass templates declared in the <future> library header: unique futures (std::future<>)\nand shared futures (std::shared_future<>).\nAn instance of std::future is the one and only instance that\nrefers to its associated event, whereas multiple instances of std::shared_future may\nThe std:future<void> and\nstd::shared_future<void> template specializations should be used where there’s no\nAlthough futures are used to communicate between threads, the\nmultiple threads may each access their own copy of std::shared_future<> without\nBack in chapter 2 you saw that std::thread doesn’t provide an easy\nthat means you have to take care of transferring the result back, because std::thread\nYou use std::async to start an asynchronous task for which you don’t need the\nRather than giving you a std::thread object to wait on, std::async\nreturns a std::future object, which will eventually hold the return value of the func-\nWhen you need the value, you just call get() on the future, and the thread\nstd::future<int> the_answer=std::async(find_the_answer_to_ltuae);\nstd::async allows you to pass additional arguments to the function by adding extra\narguments to the call, in the same way that std::thread does.\nJust as with std::thread,\nUsing std::future to get the return value of an asynchronous task\nPassing arguments to a function with std::async\nBy default, it’s up to the implementation whether std::async starts a new thread, or\nwhether the task runs synchronously when the future is waited for.\nstd::async before the function to call.\ndeferred until either wait() or get() is called on the future, std::launch::async to\nindicate that the function must be run on its own thread, or std::launch::deferred\nonly way to associate a std::future with a task; you can also do it by wrapping the task\nin an instance of the std::packaged_task<> class template or by writing code to\nexplicitly set the values using the std::promise<> class template.\nstd::packaged_\ntask is a higher-level abstraction than std::promise, so I’ll start with that.\nAssociating a task with a future\nstd::packaged_task<> ties a future to a function or callable object.\nWhen the std::\npackaged_task<> object is invoked, it calls the associated function or callable object\nand makes the future ready, with the return value stored as the associated data.\ninto self-contained sub-tasks, each of these can be wrapped in a std::packaged_\nThis abstracts out the details of the tasks; the scheduler just deals with std::packaged\nThe template parameter for the std::packaged_task<> class template is a func-\nfrom std::move(move_only())\ninstance of std::packaged_task, you must pass in a function or callable object that\nstd::packaged_task<double(double)> from a function that takes an int and returns\nstd::future<> returned from the get_future() member function, whereas the argu-\n_task <std::string(std::vector<char>*,int)> would be as shown in the follow-\nclass packaged_task<std::string(std::vector<char>*,int)>\nstd::future<std::string> get_future();\nThe std::packaged_task object is a callable object, and it can be wrapped in a\nstd::function object, passed to a std::thread as the thread function, passed to\nstd::packaged_task is invoked as a function object, the arguments supplied to the\nstored as the asynchronous result in the std::future obtained from get_future().\nYou can thus wrap a task in a std::packaged_task and retrieve the future before pass-\ning the std::packaged_task object elsewhere to be invoked in due course.\nPASSING TASKS BETWEEN THREADS\nstd:packaged_task provides one way of doing this without\nPartial class definition for a specialization of std::packaged_task< >\nRunning code on a GUI thread using std::packaged_task\nstd::deque<std::packaged_task<void()> > tasks;\nstd::packaged_task<void()> task;\ntask=std::move(tasks.front());   \nstd::thread gui_bg_thread(gui_thread);\nstd::future<void> post_task_for_gui_thread(Func f)\nstd::packaged_task<void()> task(f);       \nstd::future<void> res=task.get_future();     \ntasks.push_back(std::move(task));     \nThe future associated with the task will then be\nthe supplied function h, the future is obtained from that task i by calling the get_-\nfuture() member function, and the task is put on the list j before the future is\nthen wait for the future if it needs to know that the task has been completed, or it can\nThis example uses std::packaged_task<void()> for the tasks, which wraps a\nbut as you saw earlier, std::packaged_task can also be used in more complex situa-\nments and return a value in the std::future rather than just a completion indicator.\nby the third way of creating a future: using std::promise to set the value explicitly.\nstd::promise<T> provides a means of setting a value (of type T) that can later be\nread through an associated std::future<T> object.\nA std::promise/std::future\nblock on the future, while the thread providing the data could use the promise half of\nYou can obtain the std::future object associated with a given std::promise by\ncalling the get_future() member function, just like with std::packaged_task.\nIn this example, you use a std::promise<bool>/std::future<bool>\ngoes well just because you wanted to use std::packaged_task or std::promise.",
      "keywords": [
        "std",
        "future",
        "task",
        "thread",
        "function",
        "data",
        "packaged",
        "GUI",
        "async",
        "object",
        "GUI thread",
        "wait",
        "connection",
        "void",
        "promise"
      ],
      "concepts": [
        "std",
        "thread",
        "futures",
        "task",
        "function",
        "functions",
        "connections",
        "connection",
        "data",
        "void"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 13,
          "title": "",
          "score": 0.908,
          "base_score": 0.758,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 46,
          "title": "",
          "score": 0.841,
          "base_score": 0.691,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 15,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.687,
          "base_score": 0.687,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std",
          "future",
          "packaged_task",
          "std packaged_task",
          "std future"
        ],
        "semantic": [],
        "merged": [
          "std",
          "future",
          "packaged_task",
          "std packaged_task",
          "std future"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5149699262936428,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854687+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 112-122)",
      "start_page": 112,
      "end_page": 122,
      "summary": "std::future<double> f=std::async(square_root,-1);\nAnother way to store an exception in a future is to destroy the std::promise or\nstd::packaged_task associated with the future without calling either of the set func-\nstd::promise or std::packaged_task will store a std::future_error exception with\nan error code of std::future_errc::broken_promise in the associated state if the\nUp until now, all the examples have used std::future.\nHowever, std::future has\nstd::shared_future instead.\nAlthough std::future handles all the synchronization necessary to transfer data from\none thread to another, calls to the member functions of a particular std::future\nIf you access a single std::future\ncan wait for the same event, don’t despair just yet; std::shared_future allows exactly\nWhereas std::future is only moveable (so ownership can be transferred between\nstd::shared_future instances are copyable (so you can have multiple objects referring\nNow, with std::shared_future, member functions on an individual object are still\neach thread accesses that state through its own std::shared_future object.\nOne potential use of std::shared_future is for implementing parallel execution\nresults of the dependent cells can then use std::shared_future to reference the first\nstd::shared_future<int>\nstd::shared_future<int>\nstd::shared_future<int>\nstd::shared_future<int>\nUsing multiple std::shared_future objects to avoid data races\nInstances of std::shared_future that reference some asynchronous state are con-\nstructed from instances of std::future that reference that state.\nSince std::future\nownership must be transferred into the std::shared_future using std::move, leav-\ning std::future in an empty state, as if it were a default constructor:\nstd::future<int> f(p.get_future());\nstd::shared_future<int> sf(std::move(f));\nues, so you can construct a std::shared_future directly from the return value of the\nget_future() member function of a std::promise object, for example:\nstd::shared_future<std::string> sf(p.get_future());    \nHere, the transfer of ownership is implicit; std::shared_future<> is constructed\nstd::future also has an additional feature to facilitate the use of std::shared_\nstd::future has a share() member func-\ntion that creates a new std::shared_future and transfers ownership to it directly.\nIn this case, the type of sf is deduced to be std::shared_future< std::map< Some-\nabsolute timeout, where you wait until a specific point in time (for example,\nSo, for example, std::condition_variable has two overloads of the wait_for()\nthe way that times are specified in C++, starting with clocks.\nThe type of the value used to represent the times obtained from the clock\nThe current time of a clock can be obtained by calling the now() static member function\nfor that clock class; for example, std::chrono::system_clock::now() will return the\nThe type of the time points for a particular clock is spec-\nified by the time_point member typedef, so the return type of some_clock::now() is\nsome_clock::time_point.\nsecond has a period of std::ratio<1,25>, whereas a clock that ticks every 2.5 sec-\nstd::chrono::system_clock will not be steady, because the clock can be adjusted,\nStandard Library provides one in the form of std::chrono::steady_clock.\nother clocks provided by the C++ Standard Library are std::chrono::system_clock\nvides functions for converting its time points to and from time_t values, and\nstd::chrono::high_resolution_clock, which provides the smallest possible tick\nDurations are the simplest part of the time support; they’re handled by the std::\nThread Library are in the std::chrono namespace).\na number of minutes stored in a short is std::chrono::duration<short,std::\nof milliseconds stored in a double is std::chrono::duration<double,std::ratio\ninteger types) for use when specifying custom durations such as std::duration<d-\ndurations in the std::chrono_literals namespace, introduced with C++14.\nduration typedefs, so 15ns and std::chrono::nanoseconds(15) are identical values.\nwill be std::chrono::duration<some-floating-point-type,std::ratio<60,1>>.\nExplicit conversions can be done with std::chrono::duration_cast<>:\nstd::chrono::seconds s=\nstd::chrono::duration_cast<std::chrono::seconds>(ms);\nDuration-based waits are done with instances of std::chrono::duration<>.\nstd::future<int> f=std::async(some_task);\nif(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready)\nThe wait functions all return a status to indicate whether the wait timed out or the\nIn this case, you’re waiting for a future, so the function\nreturns std::future_status::timeout if the wait times out, std::future_status::\nThe time for a duration-based wait is measured using a steady clock internal\nWith durations under our belt, we can now move on to time points.\nThe time point for a clock is represented by an instance of the std::chrono::time_\ntime (in multiples of the specified duration) since a specific point in time called the\nshare an epoch, the time_point typedef in one class may specify the other as the clock\nreturns a duration value specifying the length of time since the clock epoch to that\nFor example, you might specify a time point as std::chrono::time_point<std::\nchrono::system_clock, std::chrono::minutes>.\nYou can add durations and subtract durations from instances of std::chrono::\ntime_point<> to produce new time points, so std::chrono::high_resolution_clock::\nnow() + std::chrono::nanoseconds(500) will give you a time 500 nanoseconds in the\nYou can also subtract one time point from another that shares the same clock.\nresult is a duration specifying the length of time between the two time points.\nauto start=std::chrono::high_resolution_clock::now();\nauto stop=std::chrono::high_resolution_clock::now();\n<<std::chrono::duration<double,std::chrono::seconds>(stop-start).count()\nThe clock parameter of a std::chrono::time_point<> instance does more than just\nWhen you pass the time point to a wait function that takes\nan absolute timeout, the clock parameter of the time point is used to measure the\nprogram, although time points associated with the system clock can be obtained by\nconverting from time_t using the std::chrono::system_clock::to_time_point()\nauto const timeout= std::chrono::steady_clock::now()+\nif(cv.wait_until(lk,timeout)==std::cv_status::timeout)\nThe two functions that handle this are std::this_thread::sleep_\nfor() and std::this_thread::sleep_until().\nstd::recursive_mutex don’t support timeouts on locking, but std::timed_mutex\ndoes, as does std::recursive_timed_mutex.\nthose listed as time_point must be an instance of std::time_point<>.\nstd::this_thread namespace\nwait_until(lock,time_\nwait_until(lock,time_point,\nstd::timed_mutex, \nstd::recursive_timed_mutex \nor std::shared_timed_\nstd::shared_timed_mutex\nstd::shared_lock<Shared-\nstd::future<ValueType> or \nstd::shared_future<Value-\nwait_until(time_point)\nstd::future_status::timeout \nif the wait timed out, \nstd::future_status::ready if \nstd::future_status::deferred ",
      "keywords": [
        "std",
        "future",
        "time",
        "clock",
        "chrono",
        "point",
        "shared",
        "wait",
        "time points",
        "duration",
        "exception",
        "thread",
        "lock",
        "function",
        "Standard Library"
      ],
      "concepts": [
        "std",
        "time",
        "timed",
        "timing",
        "waiting",
        "duration",
        "durations",
        "clocks",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.908,
          "base_score": 0.758,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 46,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 15,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.592,
          "base_score": 0.592,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.504,
          "base_score": 0.504,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std",
          "chrono",
          "std chrono",
          "std shared_future",
          "shared_future"
        ],
        "semantic": [],
        "merged": [
          "std",
          "chrono",
          "std chrono",
          "std shared_future",
          "shared_future"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4075992391326966,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854745+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 123-131)",
      "start_page": 123,
      "end_page": 131,
      "summary": "makes FP-style concurrency viable in C++; a future can be passed around between\nTo illustrate the use of futures for FP-style concurrency, let’s look at a simple imple-\nlowing listing; it takes and returns a list by value rather than sorting in place like\nstd::sort() does.\nstd::list<T> sequential_quick_sort(std::list<T> input)\nstd::list<T> result;\nstd::list<T> lower_part;\nsequential_quick_sort(std::move(lower_part)));    \nsequential_quick_sort(std::move(input)));     \nlambda function; you use a reference capture to avoid copying the pivot value (see\nstd::partition() rearranges the list in place and returns an iterator marking the\nagain to move the values from input up to the divide_point into a new list: lower_\nBy using std::move() to pass the lists in, you can\nBecause this uses a functional style already, it’s now easy to convert this to a parallel\nstd::list<T> parallel_quick_sort(std::list<T> input)\nstd::list<T> result;\nstd::list<T> lower_part;\nstd::future<std::list<T> > new_lower(                  \nstd::async(&parallel_quick_sort<T>,std::move(lower_part)));\nparallel_quick_sort(std::move(input)));       \nthread, you sort it on another thread using std::async() B.\nthe list is sorted with direct recursion as before c.\nstd::async() starts a new thread every time, then if you recurse down three times,\nyou’ll have eight threads running; if you recurse down 10 times (for ~1000 ele-\nments), you’ll have 1,024 threads running if the hardware can handle it.\nThey will run in the thread that calls get() rather than on a new\nimplementation of std::async to start a new thread for each task (even in the face of\nrun all tasks synchronously unless std::launch::async is explicitly specified.\ntion as a simple wrapper around std::packaged_task and std::thread, as shown in\nlisting 4.14; you’d create a std::packaged_task for the result of the function call, get\nthe future from it, run it on a thread, and return the future.\nstd::future<std::list<T>> rather than a list, so you need to call get() to retrieve\nstd::future<std::result_of<F(A&&)>::type>\ntypedef std::result_of<F(A&&)>::type result_type;\nstd::packaged_task<result_type(A&&)>\ntask(std::move(f)));\nstd::future<result_type> res(task.get_future());\nstd::thread t(std::move(task),std::move(a));\nSynchronizing operations with message passing\nThe idea of CSP is simple: if there’s no shared data, each thread can be reasoned\nEach thread is therefore effectively a state machine: when it\nreceives a message, it updates its state in some manner and maybe sends one or more\nmessages to other threads, with the processing performed depending on the initial\ncation passed through the message queues, but because C++ threads share an address\nshare data between the threads.\nson’s card, display appropriate messages, handle key presses, issue money, and return\nthreads: one to handle the physical machinery, one to handle the ATM logic, and one\nmessages rather than sharing any data.\nwould send a message to the logic thread when the person at the machine entered\ntheir card or pressed a button, and the logic thread would send a message to the\nOne way to model the ATM logic would be as a state machine.\nthread waits for an acceptable message, which it then processes.\nthe cash and returning the card or displaying an “insufficient funds” message and\nHaving designed a state machine for your ATM logic, you can implement it with a\nthen wait for specific sets of incoming messages and handle them when they arrive,\nListing 4.15 shows part of a simple implementation of the ATM\nhidden inside the message-passing library (a basic implementation of which is given in\nstd::string pin;\nstate=&atm::getting_pin;\nstate=&atm::waiting_for_card;     \nmessage-passing style of programming.\ntion and concurrency issues, just which messages may be received at any given point\nThe state machine for this ATM logic runs on a single\nthread), which send messages to each other to perform the task at hand, and there’s\nno shared state except that which is directly passed via messages.\nExecution starts with the run() member function f, which sets the initial state to\nThe state functions are simple member func-\nThe waiting_for_card state function B is also simple: it sends\na message to the interface to display a “waiting for card” message c, and then waits\nfor a message to handle d.\nThe only type of message that can be handled here is a\ncard_inserted message, which you handle with a lambda function e.\nwait() function; if a message is received that doesn’t match the specified type, it’s dis-\ncarded, and the thread continues to wait until a matching message is received.\nvariable, clears the current PIN, sends a message to the interface hardware to display\nOnce the message handler has completed, the state function returns, and the main\nloop then calls the new state function h.\nThe getting_pin state function is a bit more complex in that it can handle three\nstate=&atm::verifying_pin;\nThe getting_pin state function for the simple ATM implementation\nThis time, there are three message types you can process, so the wait() function has\nfies the message type as the template parameter and then passes in a lambda function\ntogether in this way, the wait() implementation knows that it’s waiting for a digit_\nThis time, you don’t necessarily change state when you get a message.\nple, if you get a digit_pressed message, you add it to the pin unless it’s the final\nThe main loop h in listing 4.15 will then call getting_pin() again to wait for\nmented by a distinct member function, which waits for the relevant messages and\nThe Concurrency TS provides new versions of std::promise and std::packaged_task",
      "keywords": [
        "std",
        "state",
        "PIN",
        "message",
        "list",
        "thread",
        "ATM",
        "function",
        "card",
        "pin state function",
        "result",
        "pivot",
        "state function",
        "handle",
        "pressed"
      ],
      "concepts": [
        "std",
        "messages",
        "messaging",
        "state",
        "threads",
        "pin",
        "function",
        "functions",
        "functional",
        "concurrent"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 36,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.547,
          "base_score": 0.547,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.523,
          "base_score": 0.523,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "message",
          "state",
          "std",
          "atm",
          "std list"
        ],
        "semantic": [],
        "merged": [
          "message",
          "state",
          "std",
          "atm",
          "std list"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4054438751603489,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854803+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 132-140)",
      "start_page": 132,
      "end_page": 140,
      "summary": "Suppose you have a task running that will produce a result, and a future that will\nWith std::future you would have to wait for the\nfuture to become ready, either with the fully-blocking wait() member function or\nGiven a future fut, a continuation is added with the call\nJust like std::future, std::experimental::future only allows the stored value to\nreturns a new future to hold the result of the continuation call.\nstd::experimental::future<int> find_the_answer;\ncontinuation is passed a ready future that holds the result that triggered the continua-\nfunction referenced in the previous example must take a std::experimental::\nstd::string find_the_question(std::experimental::future<int> the_answer);\nThe reason for this is that the future on which the continuation was chained may end\nhandle the exception, whereas by passing the future to the continuation, the continua-\nJust as for functions passed to std::async, exceptions that escape a continuation are\nstored in the future that holds the continuation result.\nfuture, and then spawn a new thread running a lambda that sets the promise’s value to\nstd::experimental::future<decltype(std::declval<Func>()())>\nThis stores the result of the function in the future, or catches the exception thrown\nfrom the function and stores that in the future, just as std::async does.\nThe value returned from a then() call is a fully-fledged future itself.\nA simple equivalent to std::async for Concurrency TS futures\nstd::future<void> process_login(\nstd::experimental::future<void> process_login(\n}).then([](std::experimental::future<user_id> id){\n}).then([](std::experimental::future<user_data> info_to_display){\nNote how each continuation takes a std::experimental::future as the sole parame-\nto return futures that become ready when the data is ready, without blocking any\nnow return a std::experimental::future<user_id> rather than a plain user_id.\nYou might think this would complicate the code, because returning a future from a\ncontinuation would give you future<future<some_value>>, or else you’d have to put\ncontinuation function you pass to a .then() call returns a future<some_type>, then the\nstd::experimental::future<void> process_login(\n[](std::experimental::future<user_id> id){\n}).then([](std::experimental::future<user_data> info_to_display){\nSo far, we’ve focused on the continuation support in std::experimental::future.\nAs you might expect, std::experimental::shared_future also supports continuations.\nThe difference here is that std::experimental::shared_future objects can have more\nthan one continuation, and the continuation parameter is a std::experimental::\nshared_future rather than a std::experimental::future.\ntheir own  std::experimental::shared_future objects.\nshared_future instance, rather than only allowing one continuation per object.\nmust also be a std::experimental::shared_future:\nauto fut2=fut.then([](std::experimental::shared_future<some_data> data){\nauto fut3=fut.then([](std::experimental::shared_future<some_data> data){\nfut is a std::experimental::shared_future due to the share() call, so the continu-\nation function must take a std::experimental::shared_future as its parameter.\nHowever, the return value from the continuation is a plain std::experimental::\nfut2 and fut3 are std::experimental::futures.\nWaiting for more than one future\nnient—you have to wait for each future in turn, and then gather the results.\ning the futures and spawn the new task when all the futures are ready.\nstd::future<FinalResult> process_data(std::vector<MyData>& vec)\nstd::vector<std::future<ChunkResult>> results;\nreturn std::async([all_results=std::move(results)](){\nThis code spawns a new asynchronous task to wait for the results, and then processes\nYou pass the set of futures to be waited on to when_all, and it returns a new future\nThis future can then be\nused with continuations to schedule additional work when the all the futures are\nGathering results from futures using std::async\nstd::experimental::future<FinalResult> process_data(\nstd::vector<std::experimental::future<ChunkResult>> results;\nreturn std::experimental::when_all(\n[](std::future<std::vector<\nstd::experimental::future<ChunkResult>>> ready_results)\nstd::vector<std::experimental::future<ChunkResult>>\nIn this case, you use when_all to wait for all the futures to become ready, and then\nGathering results from futures using std::experimental::when_all\nHere, you can use std::experimental::when_any to gather the futures together,\nthe std::experimental::when_any_result class template.\nstd::experimental::future<FinalResult>\nstd::vector<std::experimental::future<MyData *>> results;\nstd::shared_ptr<std::experimental::promise<FinalResult>> final_result =\nstd::experimental::future<std::experimental::when_any_result<\nstd::vector<std::experimental::future<MyData *>>>>\nUsing std::experimental::when_any to process the first value found\nresults.futures[results.index].get();     \nresults.futures.erase(\nresults.futures.begin() + results.index);    \nif (!results.futures.empty()) {\nresults.futures.begin(), results.futures.end())\nstd::experimental::when_any(results.begin(), results.end())\nreturn final_result->get_future();     \nwhen_any i, that will trigger its continuation when the next future is ready.\nj. The return value of the function is the future for the final result 1).\nfutures to wait for.",
      "keywords": [
        "std",
        "future",
        "experimental",
        "continuation",
        "ready",
        "result",
        "function",
        "user",
        "const",
        "shared",
        "thread",
        "tasks",
        "size",
        "process",
        "chunk"
      ],
      "concepts": [
        "std",
        "future",
        "thread",
        "result",
        "continuations",
        "continuation",
        "returns",
        "function",
        "functions",
        "task"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 13,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 46,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.561,
          "base_score": 0.561,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 31,
          "title": "",
          "score": 0.508,
          "base_score": 0.508,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "experimental",
          "std experimental",
          "future",
          "experimental future",
          "continuation"
        ],
        "semantic": [],
        "merged": [
          "experimental",
          "std experimental",
          "future",
          "experimental future",
          "continuation"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3656433625295251,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.854881+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 141-154)",
      "start_page": 141,
      "end_page": 154,
      "summary": "When threads arrive at the barrier, they block until all of the threads involved\nbe reused—the threads can then arrive at the barrier again to wait for all the threads\nA basic latch type: std::experimental::latch\nlatch done(thread_count);         \nmy_data data[thread_count];\nstd::vector<std::future<void> > threads;\nfor(unsigned i=0;i<thread_count;++i)\nprocess_data(data,thread_count);   \nspawns the appropriate number of threads using std::async c.\nEach thread then\nThe main thread can wait for all the data to\ntasks running in other threads, because the latch is a synchronization object, so\nthread that returns from a call to wait on the same latch object.\nobjects for synchronizing a group of threads.\nSuppose you have a group of threads that are operating on some data.\nEach thread\nYou construct a barrier with a count specifying the number of threads involved in the\nonly synchronize within a group of threads—a thread cannot wait for a barrier to be\nready unless it is one of the threads in the synchronization group.\nthat thread cannot wait for the barrier to be ready anymore, and the count of threads\ndivide_into_chunks(data_block data, unsigned num_threads);\nunsigned const concurrency = std::thread::hardware_concurrency();\nstd::experimental::barrier sync(num_threads);\nListing 4.26 shows an example of using a barrier to synchronize a group of threads.\nblock of data into num_threads chunks.\nAll threads then wait on\nthreads will synchronize together at the first use of the barrier c.\nthe first synchronization point, all the threads are waiting for thread 0 to arrive, but\nThe Concurrency TS doesn’t just give you one barrier type; as well as std::experi-\nrun when all threads have arrived at the barrier, before they are all released again.\na completion function, as well as a thread count.\nthis facility to ensure that the correct number of threads will arrive at the barrier the\nunsigned const concurrency = std::thread::hardware_concurrency();\nstd::experimental::flex_barrier sync(num_threads, [&] {    \nc, and encapsulates the code that was run on thread 0 at the start of each iteration.\n_barrier, and you are passing a completion function as well as a thread count d.\nSynchronizing operations between threads is an important part of writing an applica-\ntion that uses concurrency: if there’s no synchronization, the threads are essentially\nlook at the low-level facilities that make it all work: the C++ memory model and atomic\natomic types\nThe atomic types provided by the C++ \nsynchronization between threads\nThe atomic types and operations allow\nto the atomic types and operations, and finally cover the various types of synchroniza-\ntion available with the operations on atomic types.\nplanning on writing code that uses the atomic operations for synchronization (such as\nlevel atomic operations, so I’ll start with those.\nThe C++ memory model and operations on atomic types\nIf two threads access separate memory locations,\nIf either thread is modifying the data, there’s a potential for a race condi-\nmutex is locked prior to both accesses, only one thread can access the memory location\nthe two threads.\nThe use of atomic operations to enforce an ordering is described\nIf more than two threads access the same memory location, each pair of\nfrom separate threads, one or both of those accesses is not atomic, and if one or both\nfined behavior by using atomic operations to access the memory location involved\nBefore we look at atomic operations, there’s one more concept that’s important to\ndo use atomic operations, the compiler is responsible for ensuring that the necessary\nthat thread to that object must occur later in the modification order.\nThe C++ memory model and operations on atomic types\nbetween threads.\nSo, what constitutes an atomic operation, and how can these be used to enforce\nAtomic operations and types in C++\nation that reads the value of an object is atomic, and all modifications to that object are\nanother thread.\nexample, assignment to a struct with atomic members), then other threads may\nIn C++, you need to use an atomic type to get an atomic operation in most cases, so\nThe standard atomic types\ntypes are atomic, and only operations on these types are atomic in the sense of the lan-\natomic.\nIn fact, the standard atomic types themselves might use such emulation: they\nmine whether operations on a given type are done directly with atomic instructions\nThis is important to know in many cases—the key use case for atomic operations is\ntion; if the atomic operations themselves use an internal mutex then the hoped-for\ncompile time whether the atomic types for the various integral types are lock-free.\nAtomic operations and types in C++\nSince C++17, all atomic types have a static constexpr member variable, X::is_\nalways_lock_free, which is true if and only if the atomic type X is lock-free for all\nexample, for a given target platform, std::atomic<int> might always be lock-free, so\nstd::atomic<int>::is_always_lock_free will be true, but std::atomic<uintmax_t>\nnecessary instructions, so this is a run-time property, and std::atomic<uintmax_t>\nATOMIC_CHAR32_T_LOCK_FREE, \nThey evaluate to the value 0 if the atomic type is never lock-free, to the value 2 if the\natomic type is always lock-free, and to the value 1 if the lock-free status of the corre-\nstd::atomic_flag.\nto implement a simple lock and implement all the other atomic types using that as a\nWhen I said simple, I meant it: objects of the std::atomic_flag type are initial-\nThe remaining atomic types are all accessed through specializations of the\nstd::atomic<> class template and are a bit more full-featured but may not be lock-\natomic variants of all the built-in types (such as std::atomic<int> and std::atomic\nIn addition to using the std::atomic<> class template directly, you can use the set\nof names shown in table 5.1 to refer to the implementation-supplied atomic types.\nBecause of the history of how atomic types were added to the C++ Standard, if you\nsponding std::atomic<> specialization or to a base class of that specialization,\nsponding std::atomic<> specializations.\ndirect naming of std::atomic<> specializations in the same program can therefore\nThe C++ memory model and operations on atomic types\nAs well as the basic atomic types, the C++ Standard Library also provides a set of\ntypedefs for the atomic types corresponding to the various non-atomic Standard\nThe alternative names for the standard atomic types and their corresponding \nstd::atomic<> specializations\nAtomic type\nstd::atomic<bool>\nstd::atomic<char>\nstd::atomic<signed char>\nstd::atomic<unsigned char>\natomic_int\nstd::atomic<int>\natomic_uint\nstd::atomic<unsigned>\nstd::atomic<short>\nstd::atomic<unsigned short>\nstd::atomic<long>\nstd::atomic<unsigned long>\nstd::atomic<long long>\nstd::atomic<unsigned long long>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nstd::atomic<wchar_t>\natomic_int_least8_t\natomic_uint_least8_t\natomic_int_least16_t\natomic_uint_least16_t\natomic_int_least32_t\natomic_uint_least32_t\natomic_int_least64_t\natomic_uint_least64_t\nAtomic operations and types in C++\nIt’s generally simpler to say std::atomic<T> for whichever\nThe standard atomic types are not copyable or assignable in the conventional\nthe integral types and std::atomic<> specializations for ++ and -- pointers support.\natomic_intptr_t\natomic_uintptr_t\natomic_ptrdiff_t\natomic_intmax_t",
      "keywords": [
        "atomic",
        "atomic types",
        "threads",
        "std",
        "atomic operations",
        "standard atomic types",
        "data",
        "standard atomic",
        "barrier",
        "types",
        "memory",
        "operations",
        "memory location",
        "experimental",
        "object"
      ],
      "concepts": [
        "std",
        "atomic",
        "threads",
        "synchronization",
        "synchronizes",
        "data",
        "object",
        "type",
        "operating",
        "operation"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.65,
          "base_score": 0.65,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.636,
          "base_score": 0.636,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.623,
          "base_score": 0.623,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 35,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.585,
          "base_score": 0.585,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "atomic",
          "types",
          "std atomic",
          "atomic types",
          "std"
        ],
        "semantic": [],
        "merged": [
          "atomic",
          "types",
          "std atomic",
          "atomic types",
          "std"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.49205258493561616,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.854952+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 155-167)",
      "start_page": 155,
      "end_page": 167,
      "summary": "The C++ memory model and operations on atomic types\nThe std::atomic<> class template isn’t only a set of specializations, though.\nEach of the operations on the atomic types has an optional memory-ordering argu-\nment which is one of the values of the std::memory_order enumeration.\n_order enumeration has six possible values: std::memory_order_relaxed, std::\nThe permitted values for the memory ordering depend on the operation category.\nLet’s now look at the operations you can perform on each of the standard atomic\ntypes, starting with std::atomic_flag.\nOperations on std::atomic_flag\nstd::atomic_flag is the simplest standard atomic type, which represents a Boolean\nthe atomic types.\nObjects of the std::atomic_flag type must be initialized with ATOMIC_FLAG_INIT.\nIf the std::atomic_flag object has static storage\nAtomic operations and types in C++\nis a read-modify-write operation and so can have any of the memory-ordering tags\nAs with every atomic operation, the default for both is memory_order_seq_cst.\nYou can’t copy-construct another std::atomic_flag object from the first, and\nyou can’t assign one std::atomic_flag to another.\nstd::atomic_flag but something common with all the atomic types.\nThe limited feature set makes std::atomic_flag ideally suited to use as a spin-\nstd::atomic_flag flag;\nwhile(flag.test_and_set(std::memory_order_acquire));\nImplementation of a spinlock mutex using std::atomic_flag\nThe C++ memory model and operations on atomic types\nflag.clear(std::memory_order_release);\noff using std::atomic<bool>, so I’ll cover that next.\nOperations on std::atomic<bool>\nThe most basic of the atomic integral types is std::atomic<bool>.\nfeatured Boolean flag than std::atomic_flag, as you might expect.\nstd::atomic<bool> b(true);\nOne other thing to note about the assignment operator from a non-atomic bool is\npattern with the atomic types: the assignment operators they support return values (of\nRather than using the restrictive clear() function of std::atomic_flag, writes (of\nstd::atomic<bool> also supports a plain nonmodifying query of the value with an\nAtomic operations and types in C++\nstd::atomic<bool> b;\nbool x=b.load(std::memory_order_acquire);\nx=b.exchange(false,std::memory_order_acq_rel);\nexchange() isn’t the only read-modify-write operation supported by std::atomic<bool>;\ncompare-exchange operation is the cornerstone of programming with atomic types;\nit compares the value of the atomic variable with a supplied expected value and\nThe C++ memory model and operations on atomic types\nFor std::atomic<bool> this isn’t so\nstd::atomic<bool> b;\nb.compare_exchange_weak(expected,true,memory_order_acq_rel);\nOne further difference between std::atomic<bool> and std::atomic_flag is\nthat std::atomic<bool> may not be lock-free; the implementation may have to\nacquire a mutex internally in order to ensure the atomicity of the operations.\ncheck whether operations on std::atomic<bool> are lock-free.\nture common to all atomic types other than std::atomic_flag.\nstd::atomic<T*>, so we’ll look at those next.\nAtomic operations and types in C++\nOperations on std::atomic<T*>: pointer arithmetic\nThe atomic form of a pointer to some type T  is std::atomic<T*>, just as the atomic\nform of bool is std::atomic<bool>.\nstd::atomic<bool>, it’s neither copy-constructible nor copy-assignable, although it\nobligatory is_lock_free() member function, std::atomic<T*> also has load(),\nmember functions, with similar semantics to those of std::atomic<bool>, again tak-\nThe new operations provided by std::atomic<T*> are the pointer arithmetic\nfrom the built-in types: if x is std::atomic<Foo*> to the first entry of an array of Foo\nknown as exchange-and-add, and it’s an atomic read-modify-write operation, like\nstd::atomic<Foo*> p(some_array);\np.fetch_add(3,std::memory_order_release);\nThe C++ memory model and operations on atomic types\nOperations on standard atomic integral types\nAs well as the usual set of operations (load(), store(), exchange(), compare_\nexchange_weak(), and compare_exchange_strong()), the atomic integral types such\nstd::atomic<T*>; the named functions atomically perform their operation and\nstd::atomic<>, though; the type has to fulfill certain criteria.\nstd::atomic<UDT> for some user-defined type UDT,, this type must have a trivial copy-\nAtomic operations and types in C++\ncompiler isn’t going to be able to generate lock-free code for std::atomic<UDT>, so it\nthere are no atomic arithmetic operations on floating-point values.\nbehavior with compare_exchange_strong if you use std::atomic<> with a user-\nThe C++ memory model and operations on atomic types\noperator), but you can instantiate std::atomic<> with classes containing counters or\nof std::atomic<T> is limited to the set of operations available for std::atomic<bool>:\nTable 5.3 shows the operations available on each atomic type.\nFree functions for atomic operations\noperations on the atomic types.\nfor all the operations on the various atomic types.\nprefix (for example, std::atomic_load()).\nThe operations available on atomic types\natomic_\natomic\natomic\natomic\natomic\nAtomic operations and types in C++\neach of the atomic types.\nexample, std::atomic_store(&atomic_var,new_value) versus std::atomic_store_\nexplicit(&atomic_var,new_value,std::memory_order_release).\nWhereas the atomic\nFor example, std::atomic_is_lock_free() comes in one variety (though over-\nloaded for each type), and std::atomic_is_lock_free(&a) returns the same value as\na.is_lock_free() for an object of atomic type a.\nLikewise, std::atomic_load(&a) is\nstd::atomic_load_explicit(&a, std::memory_order_acquire).\nvalue) is a reference, whereas the second parameter of std::atomic_compare_\nstd::atomic_compare\nThe operations on std::atomic_flag buck the trend in that they spell out the\n_explicit suffix: std::atomic_flag_test_and_set_explicit() and std::atomic_\nthe atomic types support atomic operations, because std::shared_ptr<> is quite defi-\nnitely not an atomic type (accessing the same std::shared_ptr<T> object from multi-\nThe atomic operations available are load, store, exchange, and compare-\natomic types, taking an std::shared_ptr<>* as the first argument:\nstd::shared_ptr<my_data> local=std::atomic_load(&p);\nThe C++ memory model and operations on atomic types\nstd::atomic_store(&p,local);\nAs with the atomic operations on other types, the _explicit variants are also pro-\nvided to allow you to specify the desired memory ordering, and the std::atomic_\nwhich is an atomic type.\nIt provides the same set of operations as std::atomic<UDT>: load, store,\nBut as with the std::atomic template, you still need to\nEven if it is not lock-free, std::experimental::atomic_\nAs with all uses of atomic types and operations, if you are using them for a\nordering of operations between threads.\ndetails of the concurrency aspects of the memory model and how atomic operations\nstd::atomic<bool> data_ready(false);\nThe required enforced ordering comes from the operations on the std::\natomic<bool> variable, data_ready;, they provide the necessary ordering by virtue of\nWith the default atomic operations, that’s indeed\nprovide this relationship if the data structure contains atomic types and the opera-\ntions on that data structure perform the appropriate atomic operations internally, but\nfundamentally it comes only from operations on atomic types.\nThe C++ memory model and operations on atomic types\nchronizes with a suitably-tagged atomic read operation on x that reads the value stored\nby either that write, W, or a subsequent atomic write operation on x by the same thread\nEnforcing an ordering between non-atomic operations using atomic operations",
      "keywords": [
        "atomic",
        "atomic types",
        "std",
        "Atomic operations",
        "operations",
        "memory",
        "order",
        "types",
        "flag",
        "operation",
        "exchange",
        "data",
        "standard atomic types",
        "compare",
        "bool"
      ],
      "concepts": [
        "values",
        "operations",
        "operation",
        "operator",
        "operating",
        "operates",
        "ordering",
        "order",
        "data",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.837,
          "base_score": 0.687,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 18,
          "title": "",
          "score": 0.631,
          "base_score": 0.631,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.534,
          "base_score": 0.534,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "atomic",
          "std",
          "std atomic",
          "types",
          "atomic types"
        ],
        "semantic": [],
        "merged": [
          "atomic",
          "std",
          "std atomic",
          "types",
          "atomic types"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3815919691996804,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855007+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 168-175)",
      "start_page": 168,
      "end_page": 175,
      "summary": "Synchronizing operations and enforcing ordering\nof operation ordering in a program; it specifies which operations see the effects of\nthread inter-thread happens before operation B on another thread, then A happens\nthread synchronizes with operation B in another thread, then A inter-thread happens\noperation A is sequenced before operation B, and operation B inter-thread happens\nbefore operation C, then A inter-thread happens before C.\ndifference is that operations tagged with memory_order_consume (see section 5.3.3)\nto cover the memory-ordering tags used for atomic operations and how they relate to\nMemory ordering for atomic operations\nThere are six memory ordering options that can be applied to operations on atomic\nspecify otherwise for a particular operation, the memory-ordering option for all oper-\nels: sequentially consistent ordering (memory_order_seq_cst), acquire-release ordering\nSynchronizing operations and enforcing ordering\nensuring atomicity, and even sequentially-consistent ordering doesn’t require any spe-\nchoice for operation ordering and synchronizes-with.\nSEQUENTIALLY CONSISTENT ORDERING\nwhich is why it’s the default: all threads must see the same order of operations.\nbefore another in one thread, that ordering must be seen by all other threads.\nThis provides one ordering constraint on the operation of two (or more)\nthreads in the system using sequentially consistent atomic operations.\ncarry forward to threads that use atomic operations with relaxed memory orderings;\nthey can still see the operations in a different order, so you must use sequentially con-\nsistent operations on all your threads in order to get the benefit.\nto x and y are explicitly tagged with memory_order_seq_cst, although this tag could\nx.store(true,std::memory_order_seq_cst);  \ny.store(true,std::memory_order_seq_cst);  \nwhile(!x.load(std::memory_order_seq_cst));\nif(y.load(std::memory_order_seq_cst))     \nwhile(!y.load(std::memory_order_seq_cst));\nif(x.load(std::memory_order_seq_cst))     \nstd::thread a(write_x);\nstd::thread b(write_y);\nstd::thread c(read_x_then_y);\nstd::thread d(read_y_then_x);\nSynchronizing operations and enforcing ordering\nreturns false, the store to x must occur before the store to y, in which case the load of\nBecause the semantics of memory_order_seq_cst require a single\ntotal ordering over all operations tagged memory_order_seq_cst, there’s an implied\nordering relationship between a load of y that returns false d and the store to y B.\nFor there to be a single total order, if one thread sees x==true and then subse-\nThe operations and happens-before relationships for the case that read_x_then_y\ny in read_x_then_y to the store to y in write_y shows the implied ordering relation-\nship required in order to maintain sequential consistency: the load must occur before\nthe store in the global order of memory_order_seq_cst operations in order to achieve\nthe most expensive memory ordering because it requires global synchronization\nsequential consistency and consider using other memory orderings.\nNON-SEQUENTIALLY CONSISTENT MEMORY ORDERINGS\non the order of events because of operations in other threads in the absence of\nthreads don’t have to agree on the order of events.\nrequirement is that all threads agree on the modification order of each individual variable.\nations on distinct variables can appear in different orders on different threads, pro-\ntent world and using memory_order_relaxed for all operations.\nselectively introduce ordering relationships between operations and claw back some\nRELAXED ORDERING\nOperations on atomic types performed with relaxed ordering don’t participate in\nOperations on the same variable within a single thread\nstill obey happens-before relationships, but there’s almost no requirement on order-\ncation order of each variable is the only thing shared between threads that are using\nmemory_order_relaxed.\nRelaxed operations have few ordering requirements\nSynchronizing operations and enforcing ordering\nx.store(true,std::memory_order_relaxed);   \ny.store(true,std::memory_order_relaxed);  \nwhile(!y.load(std::memory_order_relaxed));   \nif(x.load(std::memory_order_relaxed))     \nstd::thread a(write_x_then_y);\nstd::thread b(read_y_then_x);\nthe load of y d reads true and the store of x B happens before the store of y c.\nloads can see the stores out of order.\nstd::atomic<int> x(0),y(0),z(0);   \nvalues[i].x=x.load(std::memory_order_relaxed);\nvalues[i].y=y.load(std::memory_order_relaxed);\nvalues[i].z=z.load(std::memory_order_relaxed);\nvar_to_inc->store(i+1,std::memory_order_relaxed);  \nvalues[i].x=x.load(std::memory_order_relaxed);\nvalues[i].y=y.load(std::memory_order_relaxed);\nvalues[i].z=z.load(std::memory_order_relaxed);\nRelaxed operations on multiple threads",
      "keywords": [
        "order",
        "operations",
        "ordering",
        "memory",
        "std",
        "thread",
        "relaxed",
        "operation",
        "atomic",
        "read",
        "Memory ordering",
        "atomic operations",
        "sequentially consistent ordering",
        "relaxed ordering",
        "store"
      ],
      "concepts": [
        "ordering",
        "order",
        "thread",
        "sequential",
        "operation",
        "operator",
        "std",
        "listing",
        "void",
        "atomic"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 20,
          "title": "",
          "score": 0.87,
          "base_score": 0.72,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.636,
          "base_score": 0.636,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "",
          "score": 0.631,
          "base_score": 0.631,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.555,
          "base_score": 0.555,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.553,
          "base_score": 0.553,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ordering",
          "load",
          "operations",
          "memory_order_relaxed",
          "memory_order_seq_cst"
        ],
        "semantic": [],
        "merged": [
          "ordering",
          "load",
          "operations",
          "memory_order_relaxed",
          "memory_order_seq_cst"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4175185917182442,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855063+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 176-183)",
      "start_page": 176,
      "end_page": 183,
      "summary": "std::thread t1(increment,&x,values1);\nstd::thread t2(increment,&y,values2);\nstd::thread t3(increment,&z,values3);\nstd::thread t4(read_vals,values4);\nstd::thread t5(read_vals,values5);\nthreads.\nEach thread loops 10 times, reading the values of the three atomic variables\nThree of the threads each\nOnce all the threads have been joined, you print the values from the\nbetween all threads.\nin turn, and that has the thread incrementing a given variable printing the values 0 to\nthread, then this is what you get when every operation uses memory_order_relaxed.\nIf you think about the program logic from listing 5.5, then write_x_then_y is\nlike some guy calling up the man in cubicle x and telling him to write true, then\ncalling up the man in cubicle y and telling him to write true.\nread_y_then_x repeatedly calls up the man in cubicle y asking for a value until he says\ntrue and then calls the man in cubicle x to ask for a value.\natomic loads are acquire operations (memory_order_acquire), atomic stores are release\noperations (memory_order_release), and atomic read-modify-write operations (such\nthe thread that does the acquire.\nx.store(true,std::memory_order_release);\ny.store(true,std::memory_order_release);\nwhile(!x.load(std::memory_order_acquire));\nif(y.load(std::memory_order_acquire))     \nwhile(!y.load(std::memory_order_acquire));\nif(x.load(std::memory_order_acquire))     \nstd::thread a(write_x);\nstd::thread b(write_y);\nstd::thread c(read_x_then_y);\nstd::thread d(read_y_then_x);\nble for both the load of x c and the load of y B to read false.\ndifferent threads, so the ordering from the release to the acquire in each case has no\nstores from the same thread, like in listing 5.5.\nory_order_release and the load from y to use memory_order_acquire like in the fol-\nvoid write_x_then_y()\nx.store(true,std::memory_order_relaxed);   \ny.store(true,std::memory_order_release);   \nwhile(!y.load(std::memory_order_acquire));   \nif(x.load(std::memory_order_relaxed))     \nstd::thread a(write_x_then_y);\nstd::thread b(read_y_then_x);\nEventually, the load from y, d will see true as written by the store c.\nstore uses memory_order_release and the load uses memory_order_acquire, the store\nThe store to x B happens before the store to y c because\nthe store to x also happens before the load from y and by extension happens before\nload from y might read false, in which case there’d be no requirement on the value\nThe value stored by a release operation must be seen by an acquire\nFirst off, thread a is running write_x_then_y and says to the man in cubicle x,\n“Please write true as part of batch 1 from thread a,” which he duly writes down.\nThread a then says to the man in cubicle y, “Please write true as the last write of\nbatch 1 from thread a,” which he duly writes down.\nThread b keeps asking the man in box y for a value with\nNow, thread b goes on to ask the man in box x for a value, but this time it says,\n“Please can I have a value, and by the way I know about batch 1 from thread a.” Now\nthread a.\nacquire-release ordering can be used to synchronize data across several threads, even\nthread modifies some shared variables and does a store-release to one of them.\nond thread then reads the variable subject to the store-release with a load-acquire and\ntions see the values written by the store-release operations to ensure the synchronizes-\nwith relationships, this third thread can read the values of the other variables stored\ndata[0].store(42,std::memory_order_relaxed);\ndata[1].store(97,std::memory_order_relaxed);\ndata[2].store(17,std::memory_order_relaxed);\ndata[3].store(-141,std::memory_order_relaxed);\ndata[4].store(2003,std::memory_order_relaxed);\nsync1.store(true,std::memory_order_release);    \nwhile(!sync1.load(std::memory_order_acquire));  \nsync2.store(true,std::memory_order_release);  \nwhile(!sync2.load(std::memory_order_acquire));      \nassert(data[0].load(std::memory_order_relaxed)==42);\nassert(data[1].load(std::memory_order_relaxed)==97);\nassert(data[2].load(std::memory_order_relaxed)==17);\nassert(data[3].load(std::memory_order_relaxed)==-141);\nassert(data[4].load(std::memory_order_relaxed)==2003);\nFirst off, the stores to data from thread_1 happens before the store to sync1\nsync1 B is in a while loop, it will eventually see the value stored from thread_1 and\nthus happens before) the store to sync2 d, which forms a release-acquire pair with the\nThus the stores to data in thread_1 happen\nread-modify-write operation with memory_order_acq_rel in thread_2.\nsync.store(1,std::memory_order_release);\nwhile(sync.load(std::memory_order_acquire)<2);",
      "keywords": [
        "thread",
        "order",
        "memory",
        "write",
        "std",
        "load",
        "store",
        "ordering",
        "man",
        "operations",
        "atomic",
        "acquire",
        "relaxed",
        "number",
        "list"
      ],
      "concepts": [
        "threads",
        "ordering",
        "order",
        "std",
        "values",
        "sync",
        "list",
        "acquire",
        "storing",
        "stores"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 18,
          "title": "",
          "score": 0.636,
          "base_score": 0.636,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "",
          "score": 0.601,
          "base_score": 0.601,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "load",
          "store",
          "load std",
          "std",
          "memory_order_relaxed"
        ],
        "semantic": [],
        "merged": [
          "load",
          "store",
          "load std",
          "std",
          "memory_order_relaxed"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.504848354358243,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.855122+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "summary": "Synchronizing operations and enforcing ordering\nIn this case, you want both acquire and release semantics, so memory_order\nwith memory_order_acquire semantics doesn’t synchronize with anything, even though\nwith a fetch_or with memory_order_release semantics, because the read part of the\nRead-modify-write operations with memory_order\ndeal with the same ordering issues: locking a mutex is an acquire operation, and\nif you use acquire and release orderings on atomic variables to build a simple lock,\natomic operations, the pairwise synchronization of acquire-release ordering has the\nDATA DEPENDENCY WITH ACQUIRE-RELEASE ORDERING AND MEMORY_ORDER_CONSUME\nThis is because memory_order_consume is special: it’s all about data\nIt’s introduced by using atomic load operations tagged with memory\nchronized data to direct dependencies; a store operation (A) tagged with memory_\norder_release, memory_order_acq_rel, or memory_order_seq_cst is dependency-\nordered-before a load operation (B) tagged with memory_order_consume if the consume\nif the load uses memory_order_acquire.\ndency to some operation (C), then A is also dependency-ordered-before C.\ninter-thread happens-before relation, but it does: if A is dependency-ordered-before\nOne important use for this kind of memory ordering is where the atomic opera-\nBy using memory_order_consume on the load and\nmemory_order_release on the prior store, you ensure that the pointed-to data is cor-\na.store(99,std::memory_order_relaxed);   \np.store(x,std::memory_order_release);    \nUsing std::memory_order_consume to synchronize data\nSynchronizing operations and enforcing ordering\nwhile(!(x=p.load(std::memory_order_consume)))       \nassert(a.load(std::memory_order_relaxed)==99);   \nis tagged memory_order_release, the load of p d is tagged memory_order_consume.\na global read-only array, and you use std::memory_order_consume when retrieving an\nint i=index.load(std::memory_order_consume);\nIn real code, you should always use memory_order_acquire where you might be\ntempted to use memory_order_consume, and std::kill_dependency is unnecessary.\nthe load is tagged with memory_order_consume, memory_order_acquire, or memory\n_order_seq_cst, and each operation in the chain loads the value written by the pre-\ntial store synchronizes with (for memory_order_acquire or memory_order_seq_cst)\nor is dependency-ordered-before (for memory_order_consume) the final load.\natomic read-modify-write operations in the chain can have any memory ordering\ncount.store(number_of_items,std::memory_order_release);   \nif((item_index=count.fetch_sub(1,std::memory_order_acquire))<=0)  \nReading values from a queue with atomic operations\nSynchronizing operations and enforcing ordering\norder_release) B to let the other threads know that data is available.\nconsuming the queue items might then do count.fetch_sub(1,memory_order_\n_order_acquire semantics, and the store had memory_order_release semantics, so\nthe store synchronizes with the load and the thread can read the item from the buffer.\nsub() also had memory_order_release semantics, which would introduce unnecessary\nrule or memory_order_release on the fetch_sub operations, there would be nothing\neach one that’s tagged memory_order_acquire.\nordering semantics applied to operations on atomic variables, it’s also possible to\noperations that enforce memory-ordering constraints without modifying any data and\nare typically combined with atomic operations that use the memory_order_relaxed\nFences are global operations and affect the ordering of other\natomic operations in the thread that executed the fence.\nRelaxed operations can be ordered with fences\nSynchronizing operations and enforcing ordering\nx.store(true,std::memory_order_relaxed);          \nstd::atomic_thread_fence(std::memory_order_release);    \ny.store(true,std::memory_order_relaxed);          \nwhile(!y.load(std::memory_order_relaxed));     \nstd::atomic_thread_fence(std::memory_order_acquire);    \nif(x.load(std::memory_order_relaxed))     \nstd::thread b(read_y_then_x);\nThe release fence c synchronizes with the acquire fence f because the load from y\ntagged with memory_order_release rather than memory_order_relaxed.\nacquire fence f makes it as if the load from y e was tagged with memory_order_\nrelease operation, the release operation synchronizes with the acquire fence.\nstd::atomic_thread_fence(std::memory_order_release);\nx.store(true,std::memory_order_relaxed);                    \ny.store(true,std::memory_order_relaxed);                    \ntions to enforce an ordering is that they can enforce an ordering on non-atomic oper-\nOrdering non-atomic operations with atomics\nstd::atomic_thread_fence(std::memory_order_release);\ny.store(true,std::memory_order_relaxed);            \nwhile(!y.load(std::memory_order_relaxed));         \nstd::atomic_thread_fence(std::memory_order_acquire);\nstd::thread b(read_y_then_x);\nEnforcing ordering on non-atomic operations\nSynchronizing operations and enforcing ordering\nThe fences still provide an enforced ordering of the store to x B and the store to y\ndata race on y, but the fences enforce an ordering on the operations on x, once the\nIt’s not only fences that can order non-atomic operations.\neffects back in listing 5.10 with a memory_order_release/memory_order_consume pair\nexamples in this chapter could be rewritten with some of the memory_order_relaxed\nOrdering non-atomic operations\nOrdering of non-atomic operations through the use of atomic operations is where the\nThe lock() operation is a loop on flag.test_and_set() using std::memory_\norder_acquire ordering, and the unlock() is a call to flag.clear() with std::memory\nunlock(), which calls flag.clear() with std::memory_order_release semantics.\nstd::memory_order_acquire semantics.",
      "keywords": [
        "memory",
        "operations",
        "order",
        "std",
        "release",
        "atomic",
        "operation",
        "thread",
        "acquire",
        "ordering",
        "store",
        "data",
        "atomic operations",
        "fence",
        "load"
      ],
      "concepts": [
        "operates",
        "std",
        "ordering",
        "order",
        "synchronize",
        "synchronization",
        "synchronized",
        "stores",
        "stored",
        "load"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 18,
          "title": "",
          "score": 0.87,
          "base_score": 0.72,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.587,
          "base_score": 0.587,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.566,
          "base_score": 0.566,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.552,
          "base_score": 0.552,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ordering",
          "memory_order_consume",
          "memory_order_release",
          "load",
          "operations"
        ],
        "semantic": [],
        "merged": [
          "ordering",
          "memory_order_consume",
          "memory_order_release",
          "load",
          "operations"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4195659305530075,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855178+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 193-216)",
      "start_page": 193,
      "end_page": 216,
      "summary": "and the lock()) and happens before any accesses to that data from this second thread\nconcurrent data structures\nIf a data structure is to be accessed from multiple threads, either it must be com-\nanother is to design the data structure itself for concurrent access.\nDesigning lock-based concurrent data structures\nWhen designing a data structure for concurrency, you can use the basic building\nAt the basic level, designing a data structure for concurrency means that multiple\nthreads can access the data structure concurrently, either performing the same or dis-\ntinct operations, and each thread will see a self-consistent view of the data structure.\nThis data structure is said to be thread-safe.\nto have multiple threads performing one type of operation on the data structure con-\nAlternatively, it may be safe for multiple threads to access a data structure concur-\ning the opportunity for concurrency to threads accessing the data structure.\nA mutex protects a data structure by explicitly preventing true concurrent\nThis is called serialization: threads take turns accessing the data protected by the\nGuidelines for designing data structures for concurrency\nI covered the basics of how to make the data structure thread-safe back in\nEnsure that no thread can see a state where the invariants of the data structure\nconstraints you want to put on the users of the data structure; if one thread is access-\nrequire exclusive access to the data structure, but it’s up to the user to ensure that\ndesigner of the data structure, you need to decide whether these operations are safe\nCan different parts of the data structure be protected with different mutexes?\nIt’s not uncommon for data structures to allow concurrent access from multiple\nthreads that merely read the data structure, whereas a thread that can modify the\nDesigning lock-based concurrent data structures\nThe simplest thread-safe data structures typically use mutexes and locks to protect\nthe data.\nto ensure that only one thread is accessing the data structure at a time.\ninto the design of thread-safe data structures, we’ll stick to looking at such lock-based\ndata structures in this chapter and leave the design of concurrent data structures with-\nLock-based concurrent data structures\nThe design of lock-based concurrent data structures is all about ensuring that the\nright mutex is locked when accessing the data and that the lock is held for the min-\na data structure.\nwith multiple mutexes even more carefully than the design of a data structure with a\neral simple data structures, using mutexes and locks to protect the data.\ndata structure remains thread-safe.\nsimplest data structures around, and it uses only a single mutex.\nA thread-safe stack using locks\nis to write a thread-safe data structure akin to std::stack<>, which supports pushing\ndata items onto the stack and popping them off again.\nLock-based concurrent data structures\nstd::stack<T> data;\ndata.push(std::move(new_value));    \nstd::shared_ptr<T> pop()\nstd::make_shared<T>(std::move(data.top())));    \ndata.pop();       \nvalue=std::move(data.top());        \ndata.pop();                \nreturn data.empty();\nDesigning lock-based concurrent data structures\nmutex can’t fail, so that’s always safe, and the use of std::lock_guard<> ensures that\nBecause all the member functions use std::lock_guard<> to protect the data, it’s\nLock-based concurrent data structures\nbecause of the use of locks, only one thread is ever doing any work in the stack data\nThe queue from chapter 4 shows a way of incorporating this waiting into the data struc-\nA thread-safe queue using locks and condition variables\nstraints of writing a data structure that’s safe for concurrent access from multiple\nstd::queue<T> data_queue;\nstd::condition_variable data_cond;\ndata_queue.push(std::move(new_value));\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nvalue=std::move(data_queue.front());\ndata_queue.pop();\nDesigning lock-based concurrent data structures\nstd::shared_ptr<T> wait_and_pop()    \ndata_cond.wait(lk,[this]{return !data_queue.empty();});    \nstd::make_shared<T>(std::move(data_queue.front())));\ndata_queue.pop();\nif(data_queue.empty())\nvalue=std::move(data_queue.front());\ndata_queue.pop();\nstd::shared_ptr<T> try_pop()\nif(data_queue.empty())\nstd::make_shared<T>(std::move(data_queue.front())));\ndata_queue.pop();\nreturn data_queue.empty();\nwaiting thread can call wait_and_pop() and the data structure will handle the waiting\nLock-based concurrent data structures\nexception in wait_and_pop(), such as when the new std::shared_ptr<> is con-\npush() call and store std::shared_ptr<> instances rather than direct data values.\nstd::queue<std::shared_ptr<T> > data_queue;\nstd::condition_variable data_cond;\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nvalue=std::move(*data_queue.front());     \ndata_queue.pop();\nif(data_queue.empty())\nvalue=std::move(*data_queue.front());     \ndata_queue.pop();\nstd::shared_ptr<T> wait_and_pop()\ndata_cond.wait(lk,[this]{return !data_queue.empty();});\nstd::shared_ptr<T> res=data_queue.front();            \nA thread-safe queue holding std::shared_ptr<> instances\nDesigning lock-based concurrent data structures\ndata_queue.pop();\nstd::shared_ptr<T> try_pop()\nif(data_queue.empty())\nstd::shared_ptr<T> res=data_queue.front();    \ndata_queue.pop();\nstd::shared_ptr<T> data(\nreturn data_queue.empty();\nThe basic consequences of holding the data by std::shared_ptr<> are straightfor-\nreturn an std::shared_ptr<> instance can retrieve it from the queue, d and e,\nIf the data is held by std::shared_ptr<>, there’s an additional benefit: the alloca-\nJust like in the stack example, the use of a mutex to protect the entire data struc-\ndata structure, you can provide more fine-grained locking and allow a higher level of\nLock-based concurrent data structures\nA thread-safe queue using fine-grained locks and \nIn listings 6.2 and 6.3 you have one protected data item (data_queue) and therefore\nThe simplest data structure for a queue is a singly linked list, as shown in figure 6.1.\ning the head pointer with the pointer to the next item and then returning the data\nfunction and no wait_and_pop() because this queue only supports single-threaded use.\nT data;\nnode(T data_):\ndata(std::move(data_))\nstd::unique_ptr<node> head;    \nDesigning lock-based concurrent data structures\nstd::shared_ptr<T> try_pop()\nstd::make_shared<T>(std::move(head->data)));\nstd::unique_ptr<node> const old_head=std::move(head);\nGiven that you have two data items (head B and tail c), you\nLock-based concurrent data structures\nout reading both head and tail, you now have to lock the same mutex in both push()\ntry_pop() doesn’t access head->next if the queue is empty.\nstd::shared_ptr<T> data;     \nstd::unique_ptr<node> head;\nstd::shared_ptr<T> try_pop()\nstd::shared_ptr<T> const res(head->data);     \nstd::shared_ptr<T> new_data(\nDesigning lock-based concurrent data structures\nBecause head is a std::unique_ptr<node>, you need to call head.get()\ntry_pop() accesses both head and tail, but tail is needed\nafter the new node is allocated i, and before you assign the data to the current tail\nFirst off, you need to lock the mutex on head and hold it\naccess to tail needing a lock on the tail mutex.\nLock-based concurrent data structures\nstd::shared_ptr<T> data;\nstd::unique_ptr<node> head;\nstd::lock_guard<std::mutex> tail_lock(tail_mutex);\nstd::unique_ptr<node> pop_head()\nstd::lock_guard<std::mutex> head_lock(head_mutex);\nstd::shared_ptr<T> try_pop()\nstd::unique_ptr<node> old_head=pop_head();\nreturn old_head?old_head->data:std::shared_ptr<T>();\nstd::shared_ptr<T> new_data(\nstd::lock_guard<std::mutex> tail_lock(tail_mutex);\nA thread-safe queue with fine-grained locking\nDesigning lock-based concurrent data structures\nFor each node x in the list, where x!=tail, x->data points to an instance of T\nan empty node and data and next are correctly set for the old tail node, which is now\nent mutexes, and they potentially access the same data; all data in the queue originates\nBecause the call to get_tail() locks the same mutex as the call to push(),\nthe call to push(), in which case it sees the new value of tail and the new data\nget_tail() and the lock on the head_mutex, because other threads called try_pop()\n(and thus pop_head()) and acquired the lock first, preventing your initial thread\nstd::unique_ptr<node> pop_head()    \nstd::lock_guard<std::mutex> head_lock(head_mutex);\nLock-based concurrent data structures\ninitial thread can acquire the lock on head_mutex, and not only is the returned tail\nbeyond tail and off the end of the list, destroying the data structure.\nmutex is unlocked, and try_pop() can extract the data and delete the node if there\ntry_pop() that can throw exceptions are the mutex locks, and the data isn’t modified\nlocks are acquired is in pop_head(), which always acquires the head_mutex, and then\npush(), the new node and new data item are allocated with no locks held.\nthat multiple threads can be allocating new nodes and data items concurrently with-\nDesigning lock-based concurrent data structures\nat all compared to the std::queue<>-based implementation where the lock is held\nconcurrently; only one thread can call pop_head() at  a time, but multiple threads can\nthen delete their old nodes and return the data safely.\nOK, so listing 6.6 provides a thread-safe queue with fine-grained locking, but it sup-\nwould require both head_mutex and tail_mutex to be locked, but you’ve already\nlock on that for the call to data_cond.wait().\nthe queue and the mutex unlocked; all that remains is to return the data to the caller.\nLock-based concurrent data structures\nIn comparison, empty() is trivial: lock head_mutex and check for head== get_tail()\nstd::shared_ptr<T> data;\nstd::unique_ptr<node> head;\nstd::condition_variable data_cond;\nstd::shared_ptr<T> try_pop();\nstd::shared_ptr<T> wait_and_pop();\nstd::shared_ptr<T> new_data(\nstd::lock_guard<std::mutex> tail_lock(tail_mutex);\nA thread-safe queue with locking and waiting: internals and interface\nA thread-safe queue with locking and waiting: pushing new values\nDesigning lock-based concurrent data structures\nstd::lock_guard<std::mutex> tail_lock(tail_mutex);\nstd::unique_ptr<node> pop_head()    \nstd::unique_lock<std::mutex> wait_for_data()    \nstd::unique_lock<std::mutex> head_lock(head_mutex);\ndata_cond.wait(head_lock,[&]{return head.get()!=get_tail();});\nreturn std::move(head_lock);         \nstd::unique_ptr<node> wait_pop_head()\nstd::unique_lock<std::mutex> head_lock(wait_for_data());    \nstd::unique_ptr<node> wait_pop_head(T& value)\nstd::unique_lock<std::mutex> head_lock(wait_for_data());    \nvalue=std::move(*head->data);\nstd::shared_ptr<T> wait_and_pop()\nstd::unique_ptr<node> const old_head=wait_pop_head();\nreturn old_head->data;\nstd::unique_ptr<node> const old_head=wait_pop_head(value);\nA thread-safe queue with locking and waiting: wait_and_pop()\nLock-based concurrent data structures\nmodifies the list to remove the head item, and wait_for_data() c, which waits for\nthe queue to have some data to pop.\nthat the same lock is held while the data is modified by the relevant wait_pop_head()\nstd::unique_ptr<node> try_pop_head()\nstd::lock_guard<std::mutex> head_lock(head_mutex);\nstd::unique_ptr<node> try_pop_head(T& value)\nstd::lock_guard<std::mutex> head_lock(head_mutex);\nvalue=std::move(*head->data);\nstd::shared_ptr<T> try_pop()\nstd::unique_ptr<node> old_head=try_pop_head();\nreturn old_head?old_head->data:std::shared_ptr<T>();\nstd::unique_ptr<node> const old_head=try_pop_head(value);\nstd::lock_guard<std::mutex> head_lock(head_mutex);\nA thread-safe queue with locking and waiting: try_pop() and empty()",
      "keywords": [
        "std",
        "data",
        "data structure",
        "concurrent data structures",
        "lock",
        "head",
        "mutex",
        "data structures std",
        "tail",
        "pop",
        "node",
        "queue",
        "call",
        "concurrent data",
        "lock-based concurrent data"
      ],
      "concepts": [
        "std",
        "data",
        "thread",
        "tail",
        "lock",
        "operations",
        "operation",
        "operator",
        "operating",
        "head"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.938,
          "base_score": 0.788,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.909,
          "base_score": 0.759,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "data_queue",
          "std",
          "data structures",
          "structures"
        ],
        "semantic": [],
        "merged": [
          "data",
          "data_queue",
          "std",
          "data structures",
          "structures"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.48546704709418614,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855235+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 217-226)",
      "start_page": 217,
      "end_page": 226,
      "summary": "Designing lock-based concurrent data structures\nIt’s an unbounded queue; threads can continue to push new values onto\nDesigning more complex lock-based data structures\ning these data structures for concurrent access.\nWriting a thread-safe lookup table using locks\nA lookup table or dictionary associates values of one type (the key type) with values of\nStandard Library, this facility is provided by the associative containers: std::map<>,\nDesigning more complex lock-based data structures\nFor the first cut at a thread-safe lookup table interface, you’ll\nand put a simple mutex lock around the entirety of each member function, all of\nmapped_type get_value(key_type const& key, mapped_type default_value);\nmutex and a simple lock around every member function to protect the underlying\nDesigning lock-based concurrent data structures\nstd::shared_mutex used in listing 3.13.\nsibilities for concurrent access, only one thread could modify the data structure at a\nDESIGNING A MAP DATA STRUCTURE FOR FINE-GRAINED LOCKING\nmoves down the tree, this isn’t much better than a single lock across the whole data\ngiven data value is going to be, so you need a single lock for the whole array.\ncan safely have a separate lock per bucket.\neter, the user can choose whether to specialize std::hash<> for their key type or pro-\ntemplate<typename Key,typename Value,typename Hash=std::hash<Key> >\ntypedef std::pair<Key,Value> bucket_value;\ntypedef std::list<bucket_value> bucket_data;\nA thread-safe lookup table\nDesigning more complex lock-based data structures\ntypedef typename bucket_data::iterator bucket_iterator;\nbucket_iterator find_entry_for(Key const& key) const    \nreturn std::find_if(data.begin(),data.end(),\n[&](bucket_value const& item)\nstd::shared_lock<std::shared_mutex> lock(mutex);     \nbucket_iterator const found_entry=find_entry_for(key);\nvoid add_or_update_mapping(Key const& key,Value const& value)\nstd::unique_lock<std::shared_mutex> lock(mutex);      \nbucket_iterator const found_entry=find_entry_for(key);\ndata.push_back(bucket_value(key,value));\nstd::unique_lock<std::shared_mutex> lock(mutex);      \nbucket_iterator const found_entry=find_entry_for(key);\nstd::vector<std::unique_ptr<bucket_type> > buckets;    \nstd::size_t const bucket_index=hasher(key)%buckets.size();\nDesigning lock-based concurrent data structures\nreturn get_bucket(key).value_for(key,default_value);    \nvoid add_or_update_mapping(Key const& key,Value const& value)\nget_bucket(key).add_or_update_mapping(key,value);    \nget_bucket(key).remove_mapping(key);    \nThis implementation uses a std::vector<std::unique_ptr<bucket_type>> g to\ncalled without any locking (i, j, and 1)), and then the bucket mutex can be locked\nstd::list<> of key/value pairs, so adding and removing entries is easy.\nDesigning more complex lock-based data structures\nBecause the “normal” operations on the lookup table require a lock on only\none bucket at a time, this would be the only operation that requires a lock on all the\nstd::map<Key,Value> threadsafe_lookup_table::get_map() const\nstd::vector<std::unique_lock<std::shared_mutex> > locks;\nstd::unique_lock<std::shared_mutex>(buckets[i].mutex));\nstd::map<Key,Value> res;\nfor(bucket_iterator it=buckets[i].data.begin();\nit!=buckets[i].data.end();\ncurrency of the lookup table as a whole by locking each bucket separately and by\nusing a std::shared_mutex to allow reader concurrency on each bucket.\nIn the next section, you’ll do exactly that by using a thread-safe list container\nWriting a thread-safe list using locks\nA list is one of the most basic data structures, so it should be straightforward to write a\nObtaining contents of a threadsafe_lookup_table as std::map<>\nDesigning lock-based concurrent data structures\nby acquiring locks in the user-supplied operations and don’t cause data races by stor-\nThe basic idea with fine-grained locking for a linked list is to have one mutex per\nseparate parts of the list are truly concurrent: each operation holds only the locks on\nstd::shared_ptr<T> data;\nstd::unique_ptr<node> next;\nA thread-safe list with iteration support\nDesigning more complex lock-based data structures\nnode(T const& value):              \ndata(std::make_shared<T>(value))\nstd::unique_ptr<node> new_node(new node(value));   \nstd::lock_guard<std::mutex> lk(head.m);\nnew_node->next=std::move(head.next);    \nhead.next=std::move(new_node);     \nstd::unique_lock<std::mutex> lk(head.m);   \nstd::unique_lock<std::mutex> next_lk(next->m);    \nstd::unique_lock<std::mutex> lk(head.m);\nstd::unique_lock<std::mutex> next_lk(next->m);\nDesigning lock-based concurrent data structures\nstd::unique_lock<std::mutex> lk(head.m);\nstd::unique_lock<std::mutex> next_lk(next->m);\nmutex for the head node in order to get the appropriate next value f and insert the\nso good: you only need to lock one mutex in order to add a new item to the list, so\nlock, so the lock is only protecting the update of a couple of pointer values that can’t\nTo start with, you lock the mutex on the head node i.\nIf that pointer isn’t NULL j, you lock the mutex on that node 1) in order to\ntion is well behaved, because the mutex for the node holding the data item is held\nnode from the list by updating current->next 1(.\nrelease the lock held on the mutex for the next node.\nhold the mutex on the previous node (current), so no new thread can try to acquire\nthe lock on the node you’re deleting.\nlocking was to improve the possibilities for concurrency over a single mutex, so have\nthe mutex for each node must be locked in turn, the threads can’t pass each other.\neral common data structures (stack, queue, hash map, and linked list), looking at\naccess, using locks to protect the data and prevent data races.",
      "keywords": [
        "std",
        "key",
        "data",
        "data structures",
        "node",
        "lock",
        "bucket",
        "mutex",
        "list",
        "const",
        "Key const",
        "lookup table",
        "queue",
        "lookup",
        "function"
      ],
      "concepts": [
        "std",
        "value",
        "locks",
        "node",
        "key",
        "keys",
        "function",
        "functions",
        "listing",
        "hash"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.909,
          "base_score": 0.759,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.79,
          "base_score": 0.79,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "key",
          "std",
          "lock",
          "mutex",
          "data"
        ],
        "semantic": [],
        "merged": [
          "key",
          "std",
          "lock",
          "mutex",
          "data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5233830761441438,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855294+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 227-245)",
      "start_page": 227,
      "end_page": 245,
      "summary": "Designing lock-based concurrent data structures\nDesigning lock-free concurrent data structures\nIf you can write data structures that are safe for concurrent access without locks,\nThis data structure is called a lock-free\noperations introduced in chapter 5 can be used to build lock-free data structures.\nWe’ll start by looking at what it means for data structures to be lock-\nNot all these data structures are lock-free, though, so let’s look at the\nand can still be locked by only one thread at a time.\nLock-Free—If multiple threads are operating on a data structure, then after a\nWait-Free—Every thread operating on a data structure will complete its opera-\ntions, starting with lock-free so you can see what kinds of data structures are covered.\nLock-free data structures\nFor a data structure to qualify as lock-free, more than one thread must be able to\noperations; a lock-free queue might allow one thread to push and one to pop but\nthe threads accessing the data structure is suspended by the scheduler midway\nAlgorithms that use compare/exchange operations on the data structure often\nanother thread might have modified the data in the meantime, in which case the code\nLock-free algorithms with these loops can result in one thread being subject to star-\nData structures that avoid this problem are wait-free as well as lock-free.\nDesigning lock-free concurrent data structures\nA wait-free data structure is a lock-free data structure with the additional property that\nevery thread accessing the data structure can complete its operation within a bounded\nGiven how hard it is to get a lock-free or wait-free data structure right, you need\nThe pros and cons of lock-free data structures\nWhen it comes down to it, the primary reason for using lock-free data structures is to\nWith a lock-free data structure, some thread\nWith a wait-free data structure, every thread can make\nA second reason to use lock-free data structures is robustness.\nthrough an operation on a lock-free data structure, nothing is lost except that thread’s\ndata; other threads can proceed normally.\nThe flip side here is that if you can’t exclude threads from accessing the data struc-\nAll this means that writing thread-safe data structures without using locks is\nExamples of lock-free data structures\nthreads each try to change the data structure, but for each thread, the changes made\nthread is accessing the data structure.\natomic operations, and there’ll likely be more of them in a lock-free data structure than\nin the mutex locking code for a lock-based data structure.\nware must synchronize data between threads that access the same atomic variables.\nbased data structure and a lock-free one before committing either way.\nExamples of lock-free data structures\nIn order to demonstrate some of the techniques used in designing lock-free data\nstructures, we’ll look at the lock-free implementation of a series of simple data struc-\nstructure, but I’ll use the examples to highlight particular aspects of lock-free data\nAs already mentioned, lock-free data structures rely on the use of atomic opera-\nDesigning lock-free concurrent data structures\nWriting a thread-safe stack without locks\nSet its next pointer to the current head node.\nCrucially, if two threads are adding nodes, there’s a race\ncondition between steps 2 and 3: a second thread could modify the value of head\nnote that once head has been updated to point to your new node, another thread\ning shows how you can implement a thread-safe push() without locks.\nnode* next;\nExamples of lock-free data structures\nnode(T const& data_):    \nstd::atomic<node*> head;\nwhile(!head.compare_exchange_weak(new_node->next,new_node));   \nthe node’s next pointer to the current head d, and set the head pointer to the new\nBy populating the data in the node structure itself from the node construc-\nthat the head pointer still has the same value as you stored in new_node->next d, and\nReturn the data from the retrieved node.\nDesigning lock-free concurrent data structures\nIf there are two threads\nstep 2, the second thread will be dereferencing a dangling pointer.\nthe nodes.\nthreads read the same value of head, they’ll return the same node.\nthread popped the node you were trying to pop.\nthread-safe stack back in chapter 3, you saw how returning the object by value left you\ncopy the data once you know you’re the only thread returning the node, which means\nfrom chapter 3: return a (smart) pointer to the data value.\nExamples of lock-free data structures\ndata onto the stack—you have to allocate memory for the node anyway.\nnode* next;\nnode(T const& data_):\nstd::atomic<node*> head;\nwhile(!head.compare_exchange_weak(new_node->next,new_node));\nthe data associated with your node, if there is one, or a null pointer if not e.\nA lock-free stack that leaks nodes\nDesigning lock-free concurrent data structures\ncondition where one thread deletes a node while another thread still holds a pointer\nsure there are no other threads that still hold pointers to it.\nIf only one thread ever\nthreads that can access a given node are the thread that added that node to the stack,\nand any threads that call pop().\nthen the thread that called pop() must be the only thread that can touch the node,\nsame stack instance, you need some way to track when it’s safe to delete a node.\nnot worried about nodes in push(), because they’re only accessible from one thread\nuntil they’re on the stack, whereas multiple threads might be accessing the same node\nIf there are no threads calling pop(), it’s perfectly safe to delete all the nodes cur-\nyou’ve extracted the data, then you can delete them all when there are no threads call-\nstd::atomic<unsigned> threads_in_pop;    \nvoid try_reclaim(node* old_head);\nReclaiming nodes when no threads are in pop()\nExamples of lock-free data structures\n++threads_in_pop;            \nThe atomic variable threads_in_pop B is used to count the threads currently trying\nswap() to remove the data from the node d rather than copying the pointer, so that\nstd::atomic<node*> to_be_deleted;\nwhile(nodes)\ndelete nodes;\nnodes=next;\nvoid try_reclaim(node* old_head)\nif(threads_in_pop==1)    \nif(!--threads_in_pop)                \nelse if(nodes_to_delete)    \nnodes if you can\nExtract data from node \nnodes\nthread in pop()?\nDesigning lock-free concurrent data structures\n--threads_in_pop;\nIf the count of threads_in_pop is 1 when you’re trying to reclaim the node B,\nyou’re the only thread currently in pop(), which means it’s safe to delete the node\nment, you know that no other thread can be accessing this list of pending nodes.\ng. This can happen if there are multiple threads accessing the data structure concur-\nC adds node Y to the to_be_deleted list, even though thread B is still referencing it as\nExamples of lock-free data structures\nthreads_in_pop == 2\nThread C calls pop() and runs until pop() returns\nthreads_in_pop == 0\nthreads_in_pop == 2\nThread B calls pop() and is preempted after the first read of head\nthreads_in_pop == 1\n(Thread A)\nthreads_in_pop\nthreads_in_pop == 2\nnodes_to_delete\nthreads_in_pop\nthreads_in_pop == 2\nmust check threads_in_pop after claiming the nodes to be deleted \nDesigning lock-free concurrent data structures\nThread A can’t therefore delete the\nnodes without potentially causing undefined behavior for thread B.\nnext pointer from the nodes to link them together.\npointer from the last node with the current to_be_deleted pointer 1), and store the\nfirst node in the chain as the new to_be_deleted pointer 1!.\nnodes that have been added by another thread.\nthe nodes from being deleted.\nmore threads are accessing a particular node so that it can be reclaimed.\nDetecting nodes that can’t be reclaimed using hazard pointers\nthreads is hazardous.\nIf other threads do indeed hold references to that node and\nthe other thread that deleting the object would indeed be hazardous.\nExamples of lock-free data structures\nWhen a thread wants to delete an object, it must first check the hazard pointers\nyou need one of these for each thread that might access the data structure.\nassume you have a function get_hazard_pointer_for_current_thread() that returns\nstd::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\nbetween the reading of the old head pointer B and the setting of the hazard pointer\nc. During this window no other thread knows you’re accessing this particular node.\nFortunately, if the old head node is going to be deleted, head itself must have changed,\nsafe in the knowledge that no other thread will delete the nodes from under you.\nDesigning lock-free concurrent data structures\nWell, almost: every time you reload old_head, you need to update the hazard pointer\nreferencing your node, you can safely delete it; otherwise, you have to add it to a list of\nnodes to be deleted later.\nstd::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\nif(outstanding_hazard_pointers_for(old_head))  \ndelete_nodes_with_no_hazards();    \nOnce you’ve claimed the node as yours, you can clear your hazard pointer\nc. If you did get a node, you need to check the hazard pointers belonging to other\nhazard pointer to head.\na node before you \nExamples of lock-free data structures\nthere are no longer any hazard pointers referencing those nodes, you can safely\nAny nodes for which there are still outstanding hazard pointers will be\nfor_current_thread(), reclaim_later(), outstanding_hazard_pointers_for(), and\nThe exact scheme for allocating hazard pointer instances to threads used by\nget_hazard_pointer_for_current_thread() doesn’t matter for the program logic\nple structure: a fixed-size array of pairs of thread IDs and pointers.\npointer_for_current_thread() then searches through the array to find the first\n_thread()\nDesigning lock-free concurrent data structures\nstd::atomic<void*>& get_hazard_pointer_for_current_thread()    \nThe implementation of get_hazard_pointer_for_current_thread() itself is decep-\nhazard pointer for the current thread.\nWith this implementation of get_hazard_pointer_for_current_thread(), the\nEach thread has its ",
      "keywords": [
        "data structures",
        "lock-free data structures",
        "data",
        "thread",
        "node",
        "lock-free data",
        "pointer",
        "head",
        "concurrent data structures",
        "hazard pointer",
        "structures",
        "pop",
        "hazard",
        "lock-free",
        "lock-free concurrent data"
      ],
      "concepts": [
        "threads",
        "nodes",
        "pointers",
        "locks",
        "std",
        "delete",
        "deletion",
        "deleted",
        "structure",
        "pop"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.938,
          "base_score": 0.788,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.923,
          "base_score": 0.773,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.772,
          "base_score": 0.772,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "free",
          "node",
          "lock free",
          "data",
          "structures"
        ],
        "semantic": [],
        "merged": [
          "free",
          "node",
          "lock free",
          "data",
          "structures"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4611248910461374,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855350+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 246-258)",
      "start_page": 246,
      "end_page": 258,
      "summary": "reclaim_later() and delete_nodes_with_no_hazards() can then work on a sim-\nple linked list; reclaim_later() adds nodes to the list, and delete_nodes_with_no_\nstd::atomic<data_to_reclaim*> nodes_to_reclaim;\nvoid add_to_reclaim_list(data_to_reclaim* node)    \nwhile(!nodes_to_reclaim.compare_exchange_weak(node->next,node));\ndata_to_reclaim* current=nodes_to_reclaim.exchange(nullptr);   \nates a new instance of data_to_reclaim for your pointer and adds it to the reclaim list\nThe destructor of data_to_reclaim isn’t called when you’re adding nodes to the\nlist; it’s called when there are no more hazard pointers to that node.\nare now free to add further nodes to the list or even try to reclaim them without\nNot only do you scan the hazard pointer list for the node you’re about\nThere may well be max_hazard_pointers nodes in the list, and you’re checking\nmore than max_hazard_pointers nodes on the list.\nnodes on the list, you’re not much better off.\nBut if you wait until there are 2*max_hazard_pointers nodes on the list,\nbe able to reclaim at least max_hazard_pointers nodes, and it will then be at least\nmax_hazard_pointers calls to pop() before you try to reclaim any nodes again.\nRather than checking around max_hazard_pointers nodes every call to\nnodes every max_hazard_pointers calls to pop() and reclaiming at least max_hazard_\npointers nodes.\nhave to count the nodes on the reclamation list, which means using an atomic count,\nbefore all its nodes have been reclaimed, they can be stored in the global list as before\nDetecting nodes in use with reference counting\nnodes are still being accessed by reader threads.\ntions on std::shared_ptr<> are atomic, they aren’t guaranteed to be lock-free.\nstd::shared_ptr<node> for the list, as in listing 7.9.\npointer from the popped node in order to avoid the potential for deeply nested\ndestruction of nodes when the last std::shared_ptr referencing a given node is\nstd::shared_ptr<T> data;\nstd::shared_ptr<node> next;\nnode(T const& data_):\nstd::shared_ptr<node> head;\nstd::shared_ptr<node> const new_node=std::make_shared<node>(data);\nnew_node->next=std::atomic_load(&head);\nwhile(!std::atomic_compare_exchange_weak(&head,\nstd::shared_ptr<node> old_head=std::atomic_load(&head);\nwhile(old_head && !std::atomic_compare_exchange_weak(&head,\nstd::atomic_store(&old_head->next,std::shared_ptr<node>());\nstd::shared_ptr<>, but remembering to use the atomic operations consistently is hard.\nstd::experimental::atomic_shared_ptr<T> handles the reference count-\nA lock-free stack using a lock-free std::shared_ptr<> implementation\nstd::shared_ptr<T> data;\nstd::experimental::atomic_shared_ptr<node> next;\nnode(T const& data_):\nstd::experimental::atomic_shared_ptr<node> head;\nstd::shared_ptr<node> const new_node=std::make_shared<node>(data);\nnew_node->next=head.load();\nwhile(!head.compare_exchange_weak(new_node->next,new_node));\nstd::shared_ptr<node> old_head=head.load();\nold_head->next=std::shared_ptr<node>();\nIn the probable case that your std::shared_ptr<> implementation isn’t lock-free,\neach node: an internal count and an external count.\nto the node and is increased every time the pointer is read.\nWhen the external count/pointer pairing is no longer required (the node is no\nstack that uses this technique to ensure that the nodes are reclaimed only when it’s\nstruct counted_node_ptr   \nnode* ptr;\nstd::shared_ptr<T> data;\ncounted_node_ptr next;           \nnode(T const& data_):\nstd::atomic<counted_node_ptr> head;   \ncounted_node_ptr new_node;\nnew_node.ptr=new node(data);\nnew_node.external_count=1;\nnew_node.ptr->next=head.load();\nwhile(!head.compare_exchange_weak(new_node.ptr->next,new_node));\nFirst, the external count is wrapped together with the node pointer in the counted_\nThis can then be used for the next pointer in the node struc-\nBecause counted_node_ptr is a simple\nstruct, you can use it with the std::atomic<> template for the head of the list e.\nPushing a node on a lock-free stack using split reference counts\nstructure will be small enough for std::atomic<counted_node_ptr> to be lock-free.\nYou construct a counted_node_ptr that refers to a\nexternal reference to the node (the head pointer itself).\nvoid increase_head_count(counted_node_ptr& old_counter)\ncounted_node_ptr new_counter;\ncounted_node_ptr old_head=head.load();\nnode* const ptr=old_head.ptr;    \nPopping a node from a lock-free stack using split reference counts\nexternal references to the head node to indicate that you’re referencing it and to\nthe reference count, another thread could free the node before you access it, leaving\nthe value loaded from head in order to access the pointed-to node c.\nnull pointer, you can try to remove the node by a compare_exchange_strong() call\npointers to its node.\nthan the external count f; you’ve removed the node from the list, so you drop one\noff the count for that, and you’re no longer accessing the node from this thread, so\nIf the compare/exchange d fails, another thread removed your node before you\nyou must decrease the reference count on the node you were trying to remove.\ned_node_ptr used for transferring the data: head.\nhead refers to, and third is the data item pointed to by that node.\nThe thread doing the push() first constructs the data item and the node and then\ncounted_node_ptr new_node;\nnew_node.ptr=new node(data);\nnew_node.external_count=1;\nnew_node.ptr->next=head.load(std::memory_order_relaxed)\nwhile(!head.compare_exchange_weak(new_node.ptr->next,new_node,\nold value read by the compare_exchange_strong() in increase_head_count(), so\nvoid increase_head_count(counted_node_ptr& old_counter)\ncounted_node_ptr new_counter;\nIf the exchange succeeds, you access ptr->data, so you need to ensure\nthat the store to ptr->data in the push() thread happens before the load.\neven if this compare/exchange in pop() uses std::memory_order_relaxed.\nand no other thread can be operating on the same node; that’s the whole point of the\n_acquire in increase_head_count() was enough, so std::memory_order_relaxed is\nthread can have modified the node data.\nretrieve the data knows that another thread did modify the node data; the successful\nstruct counted_node_ptr\nnode* ptr;\nstd::shared_ptr<T> data;\ncounted_node_ptr next;\nnode(T const& data_):\nA lock-free stack with reference counting and relaxed atomic operations\nstd::atomic<counted_node_ptr> head;\nvoid increase_head_count(counted_node_ptr& old_counter)\ncounted_node_ptr new_counter;\ncounted_node_ptr new_node;\nnew_node.ptr=new node(data);\nnew_node.external_count=1;\nnew_node.ptr->next=head.load(std::memory_order_relaxed)\nwhile(!head.compare_exchange_weak(new_node.ptr->next,new_node,\ncounted_node_ptr old_head=\nhead.load(std::memory_order_relaxed);\nnode* const ptr=old_head.ptr;\nstd::memory_order_release)==-count_increase)",
      "keywords": [
        "node",
        "data",
        "std",
        "ptr",
        "count",
        "head",
        "pointers nodes",
        "atomic",
        "shared",
        "pointer",
        "data structures",
        "reclaim",
        "lock-free data structures",
        "memory",
        "exchange"
      ],
      "concepts": [
        "std",
        "list",
        "nodes",
        "pointer",
        "atomic",
        "thread",
        "head",
        "count",
        "operation",
        "operations"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 25,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.498,
          "base_score": 0.348,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "node",
          "new_node",
          "head",
          "counted_node_ptr",
          "ptr"
        ],
        "semantic": [],
        "merged": [
          "node",
          "new_node",
          "head",
          "counted_node_ptr",
          "ptr"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34041378141107553,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855401+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 259-269)",
      "start_page": 259,
      "end_page": 269,
      "summary": "ptr->internal_count.load(std::memory_order_acquire);\nIf you take listing 6.6 as a basis, you need two node pointers: one for the head of the\nstd::atomic<node*> tail;\nnode* const old_head=head.load();\nhead(new node),tail(head.load())\nwhile(node* const old_head=head.load())\nnode* old_head=pop_head();\nstd::shared_ptr<T> const res(old_head->data);   \nnode* const old_tail=tail.load();     \nwith the load from tail B; the store to the preceding node’s data pointer f is\nneed to set the data items on the dummy node before you update tail.\nthe current tail node that needs updating is the next pointer, which could therefore\nchange to pop() in order to discard nodes with a null data pointer and loop again.\nnext pointer to your new node and then update tail.\nIf the atomic operations on std::shared_ptr<> are lock-free, you’re home\ncounted_node_ptr new_next;\nnew_next.ptr=new node;\nnew_next.external_count=1;\nnode* const old_tail=tail.load();    \nif(old_tail->data.compare_exchange_strong(\nnal count in tail the same as you did for head, but each node already has an external\ncount in the next pointer of the previous node in the queue.\nstruct counted_node_ptr\nImplementing push() for a lock-free queue with a reference-counted tail\nnode* ptr;\nstd::atomic<counted_node_ptr> head;\nstd::atomic<counted_node_ptr> tail;    \nstd::atomic<node_counter> count;   \ncounted_node_ptr next;\nnode_counter new_count;\nnew_count.external_counters=2;   \ncounted_node_ptr new_next;\nnew_next.ptr=new node;\nnew_next.external_count=1;\ncounted_node_ptr old_tail=tail.load();\nincrease_external_count(tail,old_tail);    \nif(old_tail.ptr->data.compare_exchange_strong(   \nold_tail.ptr->next=new_next;\nfree_external_counter(old_tail);    \nIn listing 7.16, tail is now atomic<counted_node_ptr>, the same as head B, and the\nThe node is initialized with the internal_count set to zero and the external_\ncounters set to 2 e, because every new node starts out referenced from tail and\nber of the node g, you call a new function increase_external_count() to increase\nthe count f, and then afterward you call free_external_counter() on the old tail\ncounted_node_ptr old_head=head.load(std::memory_order_relaxed); \nincrease_external_count(head,old_head);   \nnode* const ptr=old_head.ptr;\nfree_external_counter(old_head);        \nPopping a node from a lock-free queue with a reference-counted tail\nsame as the tail node, you can release the reference d and return a null pointer\nstack in listing 7.12, this compares the external count and pointer as a single entity; if\nthat to the caller after you’ve released the external counter to the popped node f.\nnode_counter old_counter=\n--new_counter.internal_count;    \nif(!new_counter.internal_count && \nReleasing a node reference in a lock-free queue\nstd::atomic<counted_node_ptr>& counter,\ncounted_node_ptr& old_counter)\ncounted_node_ptr new_counter;\n++new_counter.external_count;\nold_counter.external_count=new_counter.external_count;\nstatic void free_external_counter(counted_node_ptr &old_node_ptr)\nint const count_increase=old_node_ptr.external_count-2;\nnode_counter old_counter=\nptr->count.load(std::memory_order_relaxed);\nObtaining a new reference to a node in a lock-free queue\nFreeing an external counter to a node in a lock-free queue\nwhile(!ptr->count.compare_exchange_strong(    \nif(!new_counter.internal_count && \nThe counterpart to increase_external_count() is free_external_counter().\nexchange_strong() on old_tail.ptr->data (f from listing 7.16), no other thread\nnode needs to be set to a new dummy node, and then the tail pointer itself must be\ndata or the dummy node from one of the threads that’s waiting to push.\nthe next pointer in a node atomic, you can then use compare_exchange_strong() to\nstd::atomic<node_counter> count;\nstd::atomic<counted_node_ptr> next;    \ncounted_node_ptr old_head=head.load(std::memory_order_relaxed);\nincrease_external_count(head,old_head);\nnode* const ptr=old_head.ptr;\ncounted_node_ptr next=ptr->next.load();        \nfree_external_counter(old_head);\nthe implicit conversion to counted_node_ptr, but putting in the explicit call reminds\nvoid set_new_tail(counted_node_ptr &old_tail,      \ncounted_node_ptr const &new_tail)\nnode* const current_tail_ptr=old_tail.ptr;\nfree_external_counter(old_tail);    \ncounted_node_ptr new_next;\nnew_next.ptr=new node;\nnew_next.external_count=1;\ncounted_node_ptr old_tail=tail.load();\nincrease_external_count(tail,old_tail);\nif(old_tail.ptr->data.compare_exchange_strong(    \ncounted_node_ptr old_next={0};\nif(!old_tail.ptr->next.compare_exchange_strong(     \ncounted_node_ptr old_next={0};\nif(old_tail.ptr->next.compare_exchange_strong(    \nnew_next.ptr=new node;     ",
      "keywords": [
        "node",
        "ptr",
        "count",
        "tail",
        "data",
        "std",
        "external",
        "push",
        "queue",
        "listing",
        "pop",
        "lock-free",
        "head",
        "pointer",
        "thread"
      ],
      "concepts": [
        "listing",
        "tail",
        "count",
        "std",
        "push",
        "pop",
        "popping",
        "free",
        "structure",
        "queue"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 24,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.588,
          "base_score": 0.438,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 20,
          "title": "",
          "score": 0.525,
          "base_score": 0.525,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "node",
          "counted_node_ptr",
          "ptr",
          "tail",
          "old_tail"
        ],
        "semantic": [],
        "merged": [
          "node",
          "counted_node_ptr",
          "ptr",
          "tail",
          "old_tail"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3383195199273908,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855452+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 270-277)",
      "start_page": 270,
      "end_page": 277,
      "summary": "Examples of lock-free data structures\nIf you do set the data pointer g, you need to handle the case where another\nHaving set the data pointer in the node g, this new version of push() updates the\nuses a compare_exchange_weak() loop c to update the tail, because if other threads\nIf the thread calling push() failed to set the data pointer this time through the\nupdate the next pointer to the new node allocated on this thread 1!.\ncation include having a separate memory allocator on each thread and using free lists\nDesigning lock-free concurrent data structures\nfor writing lock-free data structures from the examples.\nGuidelines for writing lock-free data structures\ndata structures, it helps to have some guidelines to focus on.\nregarding concurrent data structures from the beginning of chapter 6 still apply, but\nwhich you can then refer to when designing your own lock-free data structures.\nwhen you can see the full set of code that can operate on the guts of the data struc-\nto avoid deleting objects when other threads might still have references to them, but\nWaiting until no threads are accessing the data structure and deleting all objects\ndata structures.\nGuidelines for writing lock-free data structures\nThread 1 performs some operation based on this value, such as dereferencing it\nThread 1 is stalled by the operating system.\nAnother thread performs some operations on x that change its value to B.\nA thread then changes the data associated with the value A such that the value\nA thread then changes x back to A based on this new data.\nThread 1 resumes and performs a compare/exchange on x, comparing against\nthread 1 has no way of telling and will corrupt the data structure.\nGuideline: identify busy-wait loops and help the other thread\nIn the final queue example, you saw how a thread performing a push operation had\nto wait for another thread also performing a push to complete its operation before it\nmodifying the algorithm so that the waiting thread performs the incomplete steps if\nit’s scheduled to run before the original thread completes the operation, you can\nDesigning lock-free concurrent data structures\nFollowing from the lock-based data structures of chapter 6, this chapter has described\nsimple implementations of various lock-free data structures, starting with a stack and a\natomic operations to ensure that there are no data races and that each thread sees a\nmuch harder for lock-free data structures than lock-based ones and examined a cou-\nhelping the thread you’re waiting for to complete its operation.\nDesigning lock-free data structures is a difficult task, and it’s easy to make mistakes,\nguidelines, you’ll be better equipped to design your own lock-free data structure,\nWherever data is shared between threads, you need to think about the data struc-\ntures used and how the data is synchronized between threads.\ntures for concurrency, you can encapsulate that responsibility in the data structure\nitself, so the rest of the code can focus on the task it’s trying to perform with the data\non from concurrent data structures to concurrent code in general.\nuse multiple threads to improve their performance, and the choice of concurrent\ndata structure is crucial where the algorithms need their worker threads to share data.\nuse those tools to design basic data structures that are safe for concurrent access\ning concurrent code than the design and use of basic data structures.\nTechniques for dividing data between threads\ndata structures\nsynchronize accesses to that data, which threads need to wait for which other threads\nmental) considerations of how many threads to use, which code to execute on which\nstructure the shared data for optimal performance.\nLet’s start by looking at techniques for dividing work between threads.\nTechniques for dividing work between threads\nTechniques for dividing work between threads\nYou need to decide how many threads to use and what\nYou need to decide whether to have “generalist” threads\nstarting with dividing data between threads before we do any other work.\nDividing data between threads before processing begins\nthat perform an operation on each element in a data set.\nalgorithm, you can assign each element to one of the processing threads.\nNo matter how the data is divided, each thread then\n.org/) frameworks: a task is split into a set of parallel tasks, the worker threads run these\nThread 1\nThread 2\nDistributing consecutive chunks of data between threads\nnumber of items to process on a thread, for example.\nrather than spawning new threads each time.",
      "keywords": [
        "data structures",
        "lock-free data structures",
        "data",
        "thread",
        "concurrent data structures",
        "lock-free data",
        "structures",
        "code",
        "concurrent data",
        "lock-free",
        "dividing data",
        "lock-free concurrent data",
        "concurrent code",
        "data structures set",
        "memory"
      ],
      "concepts": [
        "thread",
        "algorithm",
        "uses",
        "useful",
        "memory",
        "code",
        "operations",
        "operate",
        "operating",
        "task"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.923,
          "base_score": 0.773,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.779,
          "base_score": 0.779,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "data structures",
          "structures",
          "free",
          "lock free"
        ],
        "semantic": [],
        "merged": [
          "data",
          "data structures",
          "structures",
          "free",
          "lock free"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4827260236459426,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855510+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 278-289)",
      "start_page": 278,
      "end_page": 289,
      "summary": "Techniques for dividing work between threads\nThread Library to decide when to run the task on a new thread and when to run it\nThis is important: if you’re sorting a large set of data, spawning a new thread for\nperformance, if you have too many threads, you might slow down the application.\nThere’s also a possibility of running out of threads if the data set is large.\nOne alternative is to use the std::thread::hardware_concurrency() function to\ncan push the chunk to be sorted onto a thread-safe stack, such as one of those\nIf a thread has nothing else to do, either because it has\nthread_safe_stack<chunk_to_sort> chunks;   \nmax_thread_count(std::thread::hardware_concurrency()-1),\nthreads[i].join();   \nstd::list<T> do_sort(std::list<T>& chunk_data)   \nthreads.push_back(std::thread(&sorter<T>::sort_thread,this));\nstd::list<T> new_higher(do_sort(chunk_data));\nvoid sort_thread()\nstd::this_thread::yield();    \nTechniques for dividing work between threads\nchunks c and the set of threads d.\nspawning a new thread for one chunk, it pushes it onto the stack 1!\nthread while you still have processors to spare 1@.\nhandled by another thread, you then have to wait for it to be ready 1#.\nyou try to process chunks from the stack on this thread while you’re waiting 1$.\nYour freshly spawned threads sit in a loop trying to sort chunks off the stack 1&,\nthreads 1* to give them a chance to put some more work on the stack.\nhas been sorted, do_sort will return (even though the worker threads are still run-\nThis sets the end_of_data flag f and waits for the threads to fin-\nyou have with a spawn_task that launches a new thread, and you’re no longer rely-\nInstead, you limit the number of threads to the value of std::\nthread::hardware_concurrency() in order to avoid excessive task switching.\nalthough the threads are processing separate data elements, they all access the stack\nDividing work between threads by allocating different chunks of data to each thread\nthat the threads are going to be doing the same work on each chunk of data.\nThreads may or may not work on the same data, but if they do, it’s\ncurrency; each thread has a different task, which it carries out independently of other\nthreads.\nOccasionally other threads may give it data or trigger events that it needs to\nThis is where threads come in.\nIf you run each of the tasks in a separate thread, the\nTechniques for dividing work between threads\nIf everything is independent, and the threads have no need to\nThe user interface thread\nother threads.\nLikewise, the thread running the background task still focuses on the\nare that there is a lot of data shared between the threads or the different threads end\nthreads.\nWhen dividing work across threads by task type, you don’t have to limit yourself to\noperations to be applied, you can divide the work so each thread performs one stage\nDIVIDING A SEQUENCE OF TASKS BETWEEN THREADS\nthread.\nThis allows the thread performing the first operation in the sequence to\nstart on the next data element while the second thread in the pipeline is working on\nThis is an alternative to dividing the data between threads, as described in sec-\nby dividing the tasks between threads rather than the data, you change the perfor-\nthreads, then each thread has five items to process.\nHaving looked at various techniques for dividing the work between threads, let’s\nEven if you’re using multiple threads to separate concerns, you need to\ncessors or 16 single-core processors: in each case the system can run 16 threads con-\nthreads.\nthe threads, as discussed in chapter 1.\nTo allow applications to scale the number of threads in line with the number of\nthreads the hardware can run concurrently, the C++11 Standard Thread Library pro-\nvides std::thread::hardware_concurrency().\nUsing std::thread::hardware_concurrency() directly requires care; your code\nthreads call a function that uses std::thread::hardware_concurrency() for scaling\nBut even if you take into account all threads running in your application, you’re\nber of threads.\nstd::thread::hardware_concurrency() on these platforms, although this isn’t guar-\nimpact of another problem: that of multiple processors trying to access the same data.\nIf two threads are executing concurrently on different processors and they’re both\nBut if one of the threads\nDepending on the nature of the operations on the two threads, and\nThe counter is global, so any threads that call processing_loop() are modifying the\nthread on another processor is running the same code, the data for counter must\norder to lock the mutex, another thread must transfer the data that makes up the\nunlock it, and the mutex data has to be transferred to the next thread to acquire the\nThis transfer time is in addition to any time that the second thread has to wait\nthread, then as you add more cores and processors to the system, it becomes more\nIf you’re using multiple threads to process the same data more quickly, the threads are\nrally serializes threads at the operating system level rather than at the processor level.\nIf you have enough threads ready to run, the operating system can schedule another\nthread to run while one thread is waiting for the mutex, whereas a processor stall pre-\nvents any threads from running on that processor.\nmance of those threads that are competing for the mutex; they can only run one at a\nthreads accessing the data (even reader threads) still have to modify the mutex itself.\nAs the number of processors accessing the data goes up, the contention on the mutex\nlocation is only ever accessed by one thread, you can still get cache ping-pong due to\nsmall data items in adjacent memory locations will be in the same cache line.\nthis is good: if a set of data accessed by a thread is in the same cache line, this is better\nBut if the data items in a cache line are unrelated and need to be\naccessed by different threads, this can be a major cause of performance problems.\nEvery time the thread\ntransferred to the processor running that thread, only to be transferred to the cache\nfor the processor running the thread for entry 1 when that thread needs to update its\nby the same thread are close together in memory (and thus more likely to be in the\nsame cache line), whereas those that are to be accessed by separate threads are far\nIf having multiple threads access data from the same cache line is bad, how does\nthe memory layout of data accessed by a single thread affect things?\nAlthough false sharing is caused by having data accessed by one thread too close to\ndata accessed by another thread, another pitfall associated with data layout directly\nimpacts the performance of a single thread on its own.\nthe data accessed by a single thread is spread out in memory, it’s likely that it lies on\nOn the flip side, if the data accessed by a single thread is close\ndata is spread out, more cache lines must be loaded from memory onto the processor\ncontaining data for the current thread also contains data that’s not for the current\nthread.\nthe processor will experience a cache miss and have to fetch a data item from main\nIf there are more threads than cores in the system, each\ncore is going to be running multiple threads.\ncache, as you try to ensure that different threads are accessing different cache lines in\nConsequently, when the processor switches threads, it’s\nmore likely to have to reload the cache lines if each thread uses data spread across\nmultiple cache lines than if each thread’s data is close together in the same cache line.\nIf there are more threads than cores or processors, the operating system might also\nchoose to schedule a thread on one core for one time slice and then on another core\nthread’s data from the cache for the first core to the cache for the second; the more\nTask-switching problems are particularly prevalent when lots of threads are ready to\nIn multithreaded systems, it’s typical to have more threads than processors, unless\nBut threads often spend time waiting\nform useful work rather than having processors sitting idle while the threads wait.\nIf you have too many additional threads, there will\nbe more threads ready to run than there are available processors, and the operating\nOversubscription can arise when you have a task that repeatedly spawns new threads\nnumber of threads when you separate by task type is more than the number of proces-\nIf you’re spawning too many threads because of data division, you can limit the\nIn section 8.1 we looked at various ways of dividing work between threads, and in sec-",
      "keywords": [
        "threads",
        "data",
        "cache",
        "chunk",
        "std",
        "task",
        "cache line",
        "work",
        "code",
        "processors",
        "sort",
        "multiple threads",
        "single thread",
        "performance",
        "mutex"
      ],
      "concepts": [
        "thread",
        "data",
        "std",
        "processors",
        "cache",
        "task",
        "time",
        "timings",
        "chunk",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 14,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.761,
          "base_score": 0.761,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 36,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "threads",
          "thread",
          "cache",
          "data",
          "processors"
        ],
        "semantic": [],
        "merged": [
          "threads",
          "thread",
          "cache",
          "data",
          "processors"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.480352427991367,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855568+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 290-297)",
      "start_page": 290,
      "end_page": 297,
      "summary": "you’ve seen in section 8.2, the layout of the data used by a single thread can have an\nimpact, even if that data isn’t shared with any other threads.\nthreaded performance are contention, false sharing, and data proximity.\nthe data layout or changing which data elements are assigned to which thread.\noff, let’s look at an easy win: dividing array elements between threads.\nrow of the first matrix with the corresponding element of the first column of the second\nelement in the second column of the result, and so forth.\numns, in order to make it worthwhile to use multiple threads to optimize the multipli-\noptimal performance, you need to pay careful attention to the data access patterns,\nThere are many ways you can divide the work between threads.\nmore rows/columns than available processors, you could have each thread calculate\nthe values for a number of columns in the result matrix, or have each thread calculate\nthe results for a number of rows, or even have each thread calculate the results for a\nBecause other threads will be\nthere’ll be no false sharing because threads will be working on separate cache lines.\nOn the other hand, if you have each thread compute a set of rows, then it needs to\nchoose adjacent rows, this means that the thread is now the only thread writing to\nthread.\nresults of those 10,000 elements, they need to access the entirety of the second matrix\nsquare blocks rather than have each thread compute the entirety of a small number of\ndivide between threads; look at all the aspects of the data access patterns carefully, and\nTry to adjust the data distribution between threads so that data that’s close\ntogether is worked on by the same thread.\nTry to minimize the data required by any given thread.\nTry to ensure that data accessed by separate threads is sufficiently far apart to\nIf multiple threads need to traverse the tree, then they all need to\nIf the data is being modified by the threads that need it, this can avoid the\nthreads.\na thread that acquires the mutex; the data it needs may already be in the processor\nif other threads try to lock the mutex while it’s held by the first thread, they’ll need\nread-modify-write operation may cause the data held in the cache by the thread that\nthread isn’t going to touch the mutex until it unlocks it.\ncache line with the data being used by the thread, the thread that owns the mutex can\ntake a performance hit because another thread tried to lock the mutex!\nblocks of padding between the data elements that can be concurrently accessed by dif-\nferent threads.\nSo far in this chapter we’ve looked at ways of dividing work between threads, factors\nAlthough code can work even if it isn’t scalable—a single-threaded application is\na parallel algorithm many of the operations will be running on separate threads.\nIf a function spawned on a new thread exits with an exception, the application\nresult=std::accumulate(first,last,result);    \nstd::thread::hardware_concurrency();\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::vector<T> results(num_threads);           \nstd::vector<std::thread>  threads(num_threads-1);    \nthreads[i]=std::thread(             \nblock_start,block_end,std::ref(results[i]));\nblock_start,last,results[num_threads-1]);    \nstd::for_each(threads.begin(),threads.end(),\nstd::mem_fn(&std::thread::join));\nreturn std::accumulate(results.begin(),results.end(),init);   \ning thread, it’s fine.\nIf the construction of threads throws, the\nexceptions; the destructors of your new std::thread objects will call std::terminate\nquences; your thread objects will be destroyed and call std::terminate.\nThat’s it for the main thread, but there’s more: the calls to accumulate_block on\nthe new threads might throw at B.\ntions thrown on your new threads.\nstd::thread::hardware_concurrency();\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::vector<std::future<T> > futures(num_threads-1);    \nstd::vector<std::thread> threads(num_threads-1);\nthreads[i]=std::thread(std::move(task),block_start,block_end); \nT last_result=accumulate_block<Iterator,T>()(block_start,last);   \nstd::for_each(threads.begin(),threads.end(),\nstd::mem_fn(&std::thread::join));\nusing std::packaged_task and std::future for the exception safety, so you can use it\nT in the call to std::accumulate c, rather than reusing the supplied result value,\nfutures d to store an std::future<T> for each spawned thread.\nIn the thread-\nthat task on a new thread, passing in the start and end of the block to process g.\nWhen the task runs, the result will be captured in the future, as will any exception\nIf more than one of the worker threads\nThe remaining problem is the leaking threads if an exception is thrown between\nsolution is to catch any exceptions, join with the threads that are still joinable(), and\nT last_result=accumulate_block<Iterator,T>()(block_start,last);\nstd::for_each(threads.begin(),threads.end(),\nstd::mem_fn(&std::thread::join));",
      "keywords": [
        "std",
        "thread",
        "data",
        "block",
        "elements",
        "result",
        "unsigned long",
        "rows",
        "matrix",
        "data structures",
        "exception",
        "accumulate",
        "unsigned long const",
        "mutex",
        "data access"
      ],
      "concepts": [
        "thread",
        "std",
        "data",
        "result",
        "elements",
        "element",
        "performance",
        "performs",
        "exception",
        "exceptions"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 4,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "threads",
          "std",
          "data",
          "results"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "threads",
          "std",
          "data",
          "results"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.342079382562613,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855720+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 298-305)",
      "start_page": 298,
      "end_page": 305,
      "summary": "for(unsigned long i=0;i<(num_thread-1);++i)\nif(threads[i].joinable())\nthread[i].join();\nAll the threads will be joined, no matter how the code leaves the\nclass join_threads\nstd::vector<std::thread>& threads;\nexplicit join_threads(std::vector<std::thread>& threads_):\nthreads(threads_)\n~join_threads()\nfor(unsigned long i=0;i<threads.size();++i)\nif(threads[i].joinable())\nthreads[i].join();\nwhole vector of threads.\nunsigned long const min_per_thread=25;\nunsigned long const max_threads=\nunsigned long const hardware_threads=\nstd::thread::hardware_concurrency();\nunsigned long const num_threads=\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::vector<std::future<T> > futures(num_threads-1);\nstd::vector<std::thread> threads(num_threads-1);\nfor(unsigned long i=0;i<(num_threads-1);++i)\nthreads[i]=std::thread(std::move(task),block_start,block_end);\nfor(unsigned long i=0;i<(num_threads-1);++i)\nloop, safe in the knowledge that the threads will be joined however the function exits.\nyou don’t need to have explicitly joined with the threads at this point.\nthe original from listing 8.2, where you needed to have joined with the threads to\nthe threads, let’s take a look at the same thing done with std::async().\nany threads spawned are completed when the future is ready.\ntor will wait for the thread to complete.\nof threads.\nAt one extreme you have a single-threaded\nFor any given multithreaded program, the number of threads that are performing\nEven if every thread is doing useful work for\nThreads often spend time waiting for each other or waiting for I/O operations\nEvery time one thread has to wait for something (whatever that something is),\nunless there’s another thread ready to take its place on the processor, you have a pro-\nwhere only one thread is doing any useful work and “parallel” sections where all the\nAs you’ve seen, threads may wait for many\npotential for threads to wait, you can improve the potential for performance gains on\nI mentioned at the beginning of this section that threads don’t always have useful\nSometimes they have to wait for other threads, or for I/O to complete, or\nassuming that the threads are running “flat out” and always have useful work to do\nThis is not true; in application code, threads fre-\nI/O to complete, waiting to acquire a mutex, waiting for another thread to complete\nWhatever the reason for the waits, if you have only as many threads as there are\nphysical processing units in the system, having blocked threads means you’re wasting\nThe processor that would otherwise be running a blocked thread is instead\ntime by running one or more additional threads.\nConsider a virus-scanner application, which divides the work across threads using\nby running an additional scanning thread.\nand as many scanning threads as there are physical cores or processors in the system.\nthreads.\nthe thread spends waiting.\nwithout running additional threads.\nFor example, if a thread is blocked because it’s\nI/O if that’s available, and then the thread can perform other useful work while the\nIn other cases, if a thread is waiting for another\nthread to perform an operation, then rather than blocking, the waiting thread might\nIn an extreme case, if a thread is waiting for a task to be completed and that task\nhasn’t yet been started by any thread, the waiting thread might perform the task in\nRather than adding threads to ensure that all available processors are being used,\nsometimes it pays to add threads to ensure that external events are handled in a timely\nIf you have a single-threaded application, this can make long-running tasks\nwhole new thread and leave a dedicated GUI thread to process the events.\nThe threads\nstd::thread task_thread;\nvoid gui_thread()\nSeparating GUI thread from task thread\ntask_thread=std::thread(task);\ntask_thread.join();\ntask_thread.join();\nBy providing a dedicated event-handling thread, the\nments to process on each thread.\nonly parallel task running, so you can use std::thread::hardware_concurrency() to\ndetermine the number of threads.",
      "keywords": [
        "threads",
        "std",
        "task",
        "code",
        "unsigned long",
        "parallel",
        "block",
        "unsigned long const",
        "event",
        "processors",
        "long",
        "waiting",
        "work",
        "long const",
        "unsigned"
      ],
      "concepts": [
        "thread",
        "std",
        "task",
        "code",
        "process",
        "processed",
        "events",
        "waiting",
        "time",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 31,
          "title": "",
          "score": 0.728,
          "base_score": 0.728,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.626,
          "base_score": 0.626,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 14,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "threads",
          "long",
          "thread",
          "unsigned long",
          "unsigned"
        ],
        "semantic": [],
        "merged": [
          "threads",
          "long",
          "thread",
          "unsigned long",
          "unsigned"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45807674160801115,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855796+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 306-313)",
      "start_page": 306,
      "end_page": 313,
      "summary": "exceptions to the caller, you still need to use the std::packaged_task and std::\nfuture mechanisms to transfer the exception between threads.\nstd::thread::hardware_concurrency();\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::vector<std::future<void> > futures(num_threads-1);    \nstd::vector<std::thread> threads(num_threads-1);\nstd::for_each(block_start,block_end,f);\nthreads[i]=std::thread(std::move(task));    \nstd::for_each(block_start,last,f);\nworker threads don’t return a value, the calls to futures[i].get() e provide a\nmeans of retrieving any exceptions thrown on the worker threads; if you don’t want to\nA parallel version of std::for_each\nJust as your parallel implementation of std::accumulate could be simplified using\nstd::async, so can your parallel_for_each.\nstd::future<void> first_half=                    \nstd::async(&parallel_for_each<Iterator,Func>,\nAs with your std::async-based parallel_accumulate from listing 8.5, you split the\nthe use of std::async and the get() member function of std::future e provides\nA parallel implementation of std::find\nfirst element in the range matches the search criterion, there’s no need to examine\nany other elements.\nA parallel version of std::for_each using std::async\nother worker threads to process the remaining elements.\nparallel implementation, because the serial algorithm can stop searching and return\nthreads, each thread will have to examine one quarter of the elements in the range,\ntime a single thread would take to check every element.\nIf the matching element lies\nflag is set, one of the other threads has found a match, so you can cease processing\nvalues and exceptions, and then process the results back in the main thread; or you\ncan use std::promise to set the final result directly from the worker threads.\nto stop on the first exception (even if you haven’t processed all elements), you can use\nstd::promise to set both the value and the exception.\nto allow the other workers to keep searching, you can use std::packaged_task, store\nIn this case I’ve opted to use std::promise because the behavior matches that of\nthe threads to finish before getting the result from the future.\nstd::promise<Iterator>* result,\nstd::atomic<bool>* done_flag)\nresult->set_exception(std::current_exception());   \nstd::thread::hardware_concurrency();\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::promise<Iterator> result;     \nstd::vector<std::thread> threads(num_threads-1);\nthreads[i]=std::thread(find_element(),    \nfind_element()(block_start,last,match,&result,&done_flag);   \nthrough the elements in the block it’s been given, checking the flag at each step c.\na match is found, it sets the final result value in the promise d, and then sets the\nThis means that if a thread calling find_element either finds a match or throws an\nexception, all other threads will see done_flag set and will stop.\nfind a match or throw at the same time, they’ll race to set the result in the promise.\nThe main thread also uses find_element to search the remain-\nbefore you check the result, because there might not be any matching elements.\nthe std::future<Iterator> you can get from the promise 1$.\nuse std::async and recursive data division to simplify your implementation, while\nof parallel_find using std::async is shown in the following listing.\nAn implementation of a parallel find algorithm using std::async\nstd::future<Iterator> async_result=\nstd::async(&parallel_find_impl<Iterator,MatchType>,   \nmid_point,last,match,std::ref(done));\nIf you do find a match, the done flag is set before returning f.\nsearching either because you got to the end of the list, or because another thread set\nstd::async to run the search in the second half of the range i, being careful to use\nnous search is running on another thread, the destructor of the async_result vari-\nAs before, the use of std::async provides you with exception safety and exception-\nstill be correct without it but would keep checking elements until every thread\nprocessed in the sequence that you get from std::find.\nIf the elements are independent, it doesn’t matter for things like parallel\n_for_each, but it means that your parallel_find might return an element toward\nA parallel implementation of std::partial_sum\nstd::partial_sum calculates the running totals in a range, so each element is replaced\nsum of individual chunks and then add the resulting value of the last element in the\nIf the last element of each block is\nupdated first, the remaining elements in a block can be updated by one thread while a\nadd those sums to those two elements away, then add the next set of results to the\nfinal results for the first two elements.",
      "keywords": [
        "std",
        "threads",
        "unsigned long const",
        "unsigned long",
        "Iterator",
        "elements",
        "long const",
        "find",
        "element",
        "block",
        "parallel",
        "result",
        "flag",
        "unsigned",
        "long"
      ],
      "concepts": [
        "std",
        "threads",
        "element",
        "elements",
        "returning",
        "parallel",
        "parallelize",
        "parallelized",
        "parallelism",
        "results"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 31,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 35,
          "title": "",
          "score": 0.646,
          "base_score": 0.646,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std",
          "elements",
          "std async",
          "async",
          "match"
        ],
        "semantic": [],
        "merged": [
          "std",
          "elements",
          "std async",
          "async",
          "match"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4611795787476319,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855855+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 314-321)",
      "start_page": 314,
      "end_page": 321,
      "summary": "forward propagation, where k is the number of threads.\nstd::future<value_type>* previous_end_value,\nstd::promise<value_type>* end_value)\nstd::for_each(begin,last,[addend](value_type& item) \nend_value->set_exception(std::current_exception()); \nunsigned long const min_per_thread=25;    \nunsigned long const max_threads=\nunsigned long const hardware_threads=\nstd::thread::hardware_concurrency();\nunsigned long const num_threads=\nstd::min(hardware_threads!=0?hardware_threads:2,max_threads);\nunsigned long const block_size=length/num_threads;\nstd::vector<std::thread> threads(num_threads-1);    \nend_values(num_threads-1);          \nstd::vector<std::future<value_type> > \nprevious_end_values.reserve(num_threads-1);    \nfor(unsigned long i=0;i<(num_threads-1);++i)\nthreads[i]=std::thread(process_chunk(),         \nstd::advance(final_element,std::distance(block_start,last)-1);   \nprocess_chunk()(block_start,final_element,                  \n(num_threads>1)?&previous_end_values.back():0,\nto store the value of the last element in the chunk, and a vector of futures 1%, which is\nAfter you’ve spawned the thread, you can update the block start, remembering to\nBefore you process the final chunk, you need to get an iterator for the last element\nvalue, so you don’t need to do anything once the final chunk has been processed.\noperation is complete once all the threads have finished.\nfinal element c, but then you need to know if you’re the first chunk or not d.\nyou are not the first chunk, then there was a previous_end_value from the previ-\nIf there was not a previous_end_value, you’re the first chunk, so you can update\nthe end_value for the next chunk (again, if there is one—you might be the only\nBecause of the synchronization between the threads, this code isn’t readily amena-\nthe threads at each step.\nthreads to wait until the required number of threads has reached the barrier.\nthe threads have reached the barrier, they’re all unblocked and may proceed.\nnumber of “seats,” and threads have to wait until all the “seats” are filled.\nare enough waiting threads, they can all proceed; the barrier is reset and starts waiting\nfor the next batch of threads.\nthreads come around and wait until next time.\nstd::this_thread::yield();    \nAs each thread waits, the number of spaces is decremented d.\nthreads have reached the barrier f, you yield() while waiting h, so the waiting\nthread doesn’t hog the CPU in a busy wait.\nnot ideal for cases where threads are likely to be waiting a long time, and it doesn’t\nwork if there’s more than count threads that can potentially call wait() at any one\nThis is what you need here; you have a fixed number of threads that need to run in\nWell, it’s almost a fixed number of threads.\nmeans that either you have to keep those threads looping until the entire range has\nbeen processed, or you need to allow your barrier to handle threads dropping out and\nLet’s call it done_waiting(), because a thread\nit reflects the new lower number of waiting threads.\nIf you don’t do this, the other threads will be waiting for-\nIf you’re the last thread\nstep, every thread calls wait() on the barrier to ensure the threads step through\ntogether, and once each thread is done, it calls done_waiting() on the barrier to dec-\nAt each step, the threads read from either\nthe original range or the buffer and write the new value to the corresponding element\nIf the threads read from the original range on one step, they read from\nOnce a thread has finished looping,\nstd::this_thread::yield();\nstd::vector<value_type>& buffer,\nstd::vector<value_type> buffer(length);\nstd::vector<std::thread> threads(length-1);    \nthreads[i]=std::thread(process_element(),first,last,   \nthe main thread 1).\nThe key difference this time is that the number of threads is\ndependent on the number of items in the list rather than on std::thread::hardware\nwould be possible to have fewer threads, with each thread handling several values\nthreads that this is less efficient than the forward-propagation algorithm.\nYou then wait on the barrier f before starting the next step.\nupdate the element in the original range if your final result was stored in the buffer g.\n_element on one of the worker threads, it will terminate the application.\nusing a number of threads to form a pipeline.",
      "keywords": [
        "threads",
        "std",
        "end",
        "Iterator",
        "chunk",
        "element",
        "barrier",
        "unsigned",
        "count",
        "spaces",
        "number",
        "type",
        "wait",
        "code",
        "block"
      ],
      "concepts": [
        "threads",
        "std",
        "elements",
        "element",
        "wait",
        "count",
        "barrier",
        "unsigned",
        "chunks",
        "list"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "",
          "score": 0.728,
          "base_score": 0.728,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "threads",
          "barrier",
          "value_type",
          "chunk",
          "number"
        ],
        "semantic": [],
        "merged": [
          "threads",
          "barrier",
          "value_type",
          "chunk",
          "number"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44630319762272136,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855915+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 322-329)",
      "start_page": 322,
      "end_page": 329,
      "summary": "thread pool—a preconfigured group of threads that run tasks assigned to the pool.\nIn earlier chapters, you’ve been explicitly managing threads by creating std::thread\nWork stealing for pool threads\nThread pools\nIn this chapter, we’ll look at mechanisms for managing threads and tasks, starting\nwith the automatic management of the number of threads and the division of tasks\nThread pools\nA thread pool allows you to accom-\none of the worker threads, which executes the task before looping back to take another\nthreads to use, the most efficient way to allocate tasks to threads, and whether or not\nthread pool.\nThe simplest possible thread pool\nnumber as the value returned by std::thread::hardware_concurrency()) that pro-\nEach worker thread takes work off the queue, runs the specified task,\nThe following listing shows a sample implementation of this thread pool.\nclass thread_pool\nthreadsafe_queue<std::function<void()> > work_queue;     \nstd::function<void()> task;\nstd::this_thread::yield();    \nthread_pool():\nunsigned const thread_count=std::thread::hardware_concurrency(); \nstd::thread(&thread_pool::worker_thread,this));   \n~thread_pool()\nwork_queue.push(std::function<void()>(f));    \nSimple thread pool\nThread pools\nthe tasks, and they can’t return any values, so you can use std::function<void()> to\nThe threads are started in the constructor: you use std::thread::hardware_\nthreads.\nThe worker_thread function itself is quite simple: it sits in a loop waiting until the\nIf there are no tasks on the queue, the function calls std::this_thread::\nFor many purposes this simple thread pool will suffice, especially if the tasks are\nBut there are also many circumstances where this simple thread pool may not\nWaiting for tasks submitted to a thread pool\nIn the examples in chapter 8 that explicitly spawned threads, after dividing the work\nthread pools, you’d need to wait for the tasks submitted to the thread pool to com-\nBy moving that complexity into the thread pool itself, you can wait for the tasks\nYou can have the submit() function return a task handle of some description\nuse of condition variables or futures, simplifying the code that uses the thread pool.\nthread needs a result computed by the task.\ning 9.2 shows the changes required to the simple thread pool that allow you to wait\nthread.\nA thread pool with waitable tasks\nThread pools\nclass thread_pool\nthread_safe_queue<function_wrapper> work_queue;   \nfunction_wrapper task;                    \nstd::this_thread::yield();\nstd::packaged_task<result_type()> task(std::move(f));    \nstd::future<result_type> res(task.get_future());     \nwork_queue.push(std::move(task));    \nthat you know the return type of the supplied function f, which is where std::\nYou then wrap the function f in a std::packaged_task<result_type()> d,\nstd::packaged_task<> e before pushing the task onto the queue f and returning\nThis pool allows you to wait for your tasks and have them return results.\nlisting shows what the parallel_accumulate function looks like with this thread pool.\nthread_pool pool;\nWhen there are only a few threads in the pool, each thread will process\ncurrently.” There’s an inherent overhead to submitting a task to a thread pool, having\nthe worker thread run it, and passing the return value through a std::future<>, and\nparallel_accumulate using a thread pool with waitable tasks",
      "keywords": [
        "thread pool",
        "thread",
        "std",
        "pool",
        "function",
        "task",
        "Simple thread pool",
        "queue",
        "result",
        "Work",
        "wrapper",
        "type",
        "block",
        "Simple thread",
        "thread management std"
      ],
      "concepts": [
        "thread",
        "tasks",
        "function",
        "functions",
        "pool",
        "void",
        "returned",
        "result",
        "submitted",
        "submit"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.922,
          "base_score": 0.772,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 40,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pool",
          "thread pool",
          "thread",
          "tasks",
          "simple thread"
        ],
        "semantic": [],
        "merged": [
          "pool",
          "thread pool",
          "thread",
          "tasks",
          "simple thread"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46317477126580286,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.855993+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 330-337)",
      "start_page": 330,
      "end_page": 337,
      "summary": "Thread pools\ntasks, obtaining the futures, or storing the std::thread objects so you can join with\ntasks and waits for the pool threads to finish.\nthread pool.\nThis works well, because each task is either running on its own thread or will be\nthread partitioned the data it was sorting, it added a new chunk to the stack for one of\nthe whole point of using a thread pool—you don’t have access to the task list to do\nThe simplest way to do this is to add a new function on thread_pool to run a task\nvoid thread_pool::run_pending_task()\nif(work_queue.try_pop(task))\nThis tries to take a task off the queue and run it if there is one;\nThread pools\nstd::list<T> new_higher(do_sort(chunk_data));\nWhen submitting the task to the pool, you use std::bind() to bind the this\nthat wait for other tasks, this thread pool is still far from ideal.\nsubmit() and every call to run_pending_task()accesses the same queue.\nare continually popping items off the queue in order to run the tasks.\nOne way to avoid cache ping-pong is to use a separate work queue per thread.\nEach thread then posts new items to its own queue and takes work from the global\neach thread has its own work queue, as well as the global one.\nthreadsafe_queue<function_wrapper> pool_work_queue;\nstatic thread_local std::unique_ptr<local_queue_type>\nlocal_work_queue;    \nif(local_work_queue)         \nlocal_work_queue->push(std::move(task));\npool_work_queue.push(std::move(task));   \nA thread pool with thread-local work queues\nThread pools\ntask=std::move(local_work_queue->front());\nlocal_work_queue->pop();\nelse if(pool_work_queue.try_pop(task))    \nYou’ve used a std::unique_ptr<> to hold the thread-local work queue c because\ntor of std::unique_ptr<> will ensure that the work queue is destroyed when the\nsubmit() then checks to see if the current thread has a work queue e.\nit’s a pool thread, and you can put the task on the local queue; otherwise, you need to\nput the task on the pool queue as before f.\nuneven, it can easily result in one thread having a lot of work in its queue while the\nend up on the local queue of the worker thread that processed that one.\nfull queue, the queue must be accessible to the thread doing the stealing from run_\nThis requires that each thread register its queue with the thread\nIt’s possible to write a lock-free queue that allows the owner thread to push and\nclass work_stealing_queue\nstd::deque<data_type> the_queue;    \nwork_stealing_queue()\nthe_queue.push_front(std::move(data));\nThread pools\nfront of the queue, while try_steal() e works on the back.\nThis means that this “queue” is a last-in-first-out stack for its own thread; the task\nin the cache than the data related to a task pushed on the queue previously.\nthreadsafe_queue<task_type> pool_work_queue;\nstd::vector<std::unique_ptr<work_stealing_queue> > queues;   \nstatic thread_local work_stealing_queue* local_work_queue;   \nbool pop_task_from_local_queue(task_type& task)\nreturn local_work_queue && local_work_queue->try_pop(task);\nbool pop_task_from_pool_queue(task_type& task)\nreturn pool_work_queue.try_pop(task);\nA thread pool that uses work stealing\nbool pop_task_from_other_thread_queue(task_type& task)   \nif(queues[index]->try_steal(task))\nthread_pool():\nqueues.push_back(std::unique_ptr<work_stealing_queue>(  \nnew work_stealing_queue));\nstd::thread(&thread_pool::worker_thread,this,i));\n~thread_pool()\nif(local_work_queue)\nlocal_work_queue->push(std::move(task));\npool_work_queue.push(std::move(task));",
      "keywords": [
        "Thread",
        "queue",
        "std",
        "thread pool",
        "task",
        "work",
        "pool",
        "work queue",
        "local",
        "type",
        "chunk",
        "data",
        "run",
        "stealing",
        "local queue"
      ],
      "concepts": [
        "std",
        "threads",
        "tasks",
        "queue",
        "pool",
        "works",
        "sorted",
        "waits",
        "returned",
        "list"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.922,
          "base_score": 0.772,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "",
          "score": 0.761,
          "base_score": 0.761,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 31,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "queue",
          "task",
          "local_work_queue",
          "pool",
          "thread"
        ],
        "semantic": [],
        "merged": [
          "queue",
          "task",
          "local_work_queue",
          "pool",
          "thread"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4702583254624424,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856052+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 338-346)",
      "start_page": 338,
      "end_page": 346,
      "summary": "Interrupting threads\nthreads are blocked waiting for something such as I/O or a mutex lock.\nNext on the list of “advanced” thread-management techniques is interrupting\nthreads.\nInterrupting threads\npoint of view of the interface for launching and interrupting a thread rather than that\nof the thread being interrupted.\nLaunching and interrupting another thread\nstd::thread, with an additional interrupt() function:\nclass interruptible_thread\ninterruptible_thread(FunctionType f);\nthe thread itself?\ndata structure needs to be accessible through a thread_local variable that’s set when\nthe thread is started, so that when a thread calls your interruption_point() func-\nmanage the thread; it needs to be allocated in a way that the interruptible_thread\nInterrupting threads\nthread_local interrupt_flag this_thread_interrupt_flag;    \nclass interruptible_thread\ninterruptible_thread(FunctionType f)\nstd::promise<interrupt_flag*> p;    \np.set_value(&this_thread_interrupt_flag);\nto the address of the this_thread_interrupt_flag (which is declared thread_local\nable, p, this is OK because the interruptible_thread constructor waits until p is no\nneed to ensure that the flag variable is cleared when the thread exits, or is detached,\npointer to an interrupt flag, you have a thread to interrupt, so you can set the flag g.\nBasic implementation of interruptible_thread\nIt’s then up to the interrupted thread what it does with the interruption.\nDetecting that a thread has been interrupted\nYou can now set the interruption flag, but that doesn’t do you any good if the thread\nsafe to be interrupted, and it throws a thread_interrupted exception if the flag is set:\nif(this_thread_interrupt_flag.is_set())\nthrow thread_interrupted();\nSome of the best places for interrupting a thread\nInterrupting a condition variable wait\nso let’s start there: what do you need to do in order to be able to interrupt a wait on a\nThe interrupt_flag\nInterrupting threads\nvoid interruptible_wait(std::condition_variable& cv,\nthis_thread_interrupt_flag.set_condition_variable(cv);   \nthis_thread_interrupt_flag.clear_condition_variable();   \ncondition variable with an interrupt flag, this code is nice and simple.\ninterruption, associates the condition variable with interrupt_flag for the current\nthread B, waits on the condition variable c, clears the association with the condition\nIf the thread is interrupted during the\nwait on the condition variable, the interrupting thread will broadcast the condition\nvariable and wake you from the wait, so you can check for interruption.\nof the interrupt flag with the condition variable.\nIf the thread is\nthe interrupt flag, because the thread isn’t waiting and so can’t be woken by a notify on the\ncheck for interruption and the call to wait().\nthe interrupting) for that thread to lock (in the call to interrupt()), without know-\nA broken version of interruptible_wait for std::condition\nstd::condition_variable* thread_cond;\ninterrupt_flag():\nthis_thread_interrupt_flag.clear_condition_variable();\nvoid interruptible_wait(std::condition_variable& cv,\nUsing a timeout in interruptible_wait for std::condition\nInterrupting threads\nthis_thread_interrupt_flag.set_condition_variable(cv);          \nvoid interruptible_wait(std::condition_variable& cv,\nthis_thread_interrupt_flag.set_condition_variable(cv);          \nwhile(!this_thread_interrupt_flag.is_set() && !pred())\nInterrupting a wait on std::condition_variable_any\n_mutex in your interrupt_flag and the lock supplied to the wait call, as shown here.\nstd::condition_variable* thread_cond;\nstd::condition_variable_any* thread_cond_any;\ninterrupt_flag():\ninterruptible_wait for std::condition_variable_any\nvoid wait(std::condition_variable_any& cv,Lockable& lk)\nvoid interruptible_wait(std::condition_variable_any& cv,\nInterrupting threads\nthis_thread_interrupt_flag.wait(cv,lk);\nThis allows threads that are trying to interrupt you to acquire\nthe lock on set_clear_mutex and check the thread_cond_any pointer once you’re\nduring the wait() call before clearing the thread_cond_any pointer in your cus-\nThat rounds up interrupting condition variable waits, but what about other blocking\nexample, here’s an overload of interruptible_wait() for std::future<>:\nvoid interruptible_wait(std::future<T>& uf)\nwhile(!this_thread_interrupt_flag.is_set())\nThis waits until either the interrupt flag is set or the future is ready but does a block-",
      "keywords": [
        "thread",
        "std",
        "condition",
        "variable",
        "interrupt",
        "condition variable",
        "wait",
        "flag",
        "interruption",
        "mutex",
        "lock",
        "point",
        "cond",
        "task",
        "Interrupting threads"
      ],
      "concepts": [
        "thread",
        "interrupted",
        "interruption",
        "interruptions",
        "std",
        "waiting",
        "void",
        "variable",
        "variables",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 35,
          "title": "",
          "score": 0.728,
          "base_score": 0.728,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.659,
          "base_score": 0.659,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 11,
          "title": "",
          "score": 0.652,
          "base_score": 0.652,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.65,
          "base_score": 0.65,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "interrupting",
          "this_thread_interrupt_flag",
          "interrupt",
          "interruptible_wait std",
          "interruptible_wait"
        ],
        "semantic": [],
        "merged": [
          "interrupting",
          "this_thread_interrupt_flag",
          "interrupt",
          "interruptible_wait std",
          "interruptible_wait"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4764461540087538,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856111+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 347-354)",
      "start_page": 347,
      "end_page": 354,
      "summary": "catch(thread_interrupted&)\nIf you do this, and another thread calls interrupt() again, your\nthread will be interrupted again the next time it calls an interruption point.\nwant to do this if your thread is performing a series of independent tasks; interrupting\nBecause thread_interrupted is an exception, all the usual exception-safety precau-\nthe thread in the interruptible_thread constructor now looks like this:\nInterrupting threads\ncatch(thread_interrupted const&)\nstd::vector<interruptible_thread> background_threads;\ninterruptible_thread(background_thread,disk_1));\ninterruptible_thread(background_thread,disk_2));\nbackground_threads[i].interrupt();    \nbackground threads are interrupted f, and then the main thread waits for each back-\nWhy do you interrupt all the threads before waiting for any?\nimmediately, you therefore cause the interrupting thread to wait, even though it still\nmore work to do (all the threads have been interrupted) do you wait.\nall the threads being interrupted to process their interruptions in parallel and poten-\nthread pools and interrupting threads.\nWe’ve also looked at various ways of allowing one thread to interrupt the process-\nThe execution policy of std::execution::par indicates to the standard library that\nit is allowed to perform this call as a parallel algorithm, using multiple threads.\nBefore we get onto the algorithms themselves, let’s take a look at the execution\nExecution policies\nThe standard specifies three execution policies:\nstd::execution::sequenced_policy \nstd::execution::parallel_policy \nstd::execution::parallel_unsequenced_policy \nImplementations may also define additional execution policies that have\nYou cannot define your own execution policies.\nquences of using one of the standard execution policies, starting with the general\nchanges for all algorithm overloads that take an exception policy.\nIf you pass an execution policy to one of the standard library algorithms, then the\nbehavior of that algorithm is now governed by the execution policy.\nIf an execution policy is supplied to an algorithm, then that algorithm’s complexity\nExecution policies\nexecution, many parallel algorithms will perform more of the core operations of the\nwith an execution policy may perform some multiple of the number of operations\nperformed by its counterpart without an execution policy, where that multiple will\nIf an exception is thrown during execution of an algorithm with an execution policy,\nsupplied execution policies will call std::terminate if there are any uncaught excep-\nwith one of the standard execution policies is std::bad_alloc, which is thrown if the\nexample, the following call to std::for_each, without an execution policy, will propa-\nviding an execution policy.\nWHERE AND WHEN ALGORITHM STEPS ARE EXECUTED\nfers between the standard execution policies.\nThe policy specifies which execution\nagents are used to perform the steps of the algorithm, be they “normal” threads, vec-\nThe execution policy will also specify\nThe details for each of the standard execution policies are given in sections 10.2.2,\n10.2.3, and 10.2.4, starting with the most basic policy: std::execution::sequenced\n10.2.2 std::execution::sequenced_policy\ntion to perform all operations on the thread that called the function, so there is no\nBut it is still an execution policy, and therefore has the same conse-\nNot only must all operations be performed on the same thread, but they must be\nas that of the corresponding overload without an execution policy.\n10.2.3 std::execution::parallel_policy\nThe parallel policy provides basic parallel execution across a number of threads.\nOperations may be performed either on the thread that invoked the algorithm, or on\ninvoked in parallel, and must not rely on being run on the same thread as any other\nYou can use the parallel execution policy for the vast majority of cases where you\nwould have used a standard library algorithm without an execution policy.\nlibrary were to execute the lambdas across multiple threads, this would be a data race,\nThe requirements for std::execution::parallel_\nthat would likely defeat the point of using the parallel execution policy, because that\n10.2.4 std::execution::parallel_unsequenced_policy\nAn algorithm invoked with the parallel unsequenced policy may perform the\nalgorithm steps on unspecified threads of execution, unordered and unsequenced\nthat take an execution policy.",
      "keywords": [
        "execution policy",
        "execution",
        "thread",
        "policy",
        "std",
        "algorithm",
        "Parallel algorithms",
        "parallel",
        "execution policies",
        "parallel execution policy",
        "standard execution policies",
        "background",
        "standard",
        "standard execution",
        "background threads"
      ],
      "concepts": [
        "threads",
        "std",
        "policies",
        "execute",
        "executed",
        "interruption",
        "interruptions",
        "interrupted",
        "algorithms",
        "algorithmic"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 34,
          "title": "",
          "score": 0.728,
          "base_score": 0.728,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.653,
          "base_score": 0.653,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.646,
          "base_score": 0.646,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "execution",
          "policy",
          "execution policy",
          "execution policies",
          "policies"
        ],
        "semantic": [],
        "merged": [
          "execution",
          "policy",
          "execution policy",
          "execution policies",
          "policies"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4644832689500483,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856167+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 355-362)",
      "start_page": 355,
      "end_page": 362,
      "summary": "Parallel algorithms\nThat’s quite a list; pretty much every algorithm in the C++ Standard Library that\nstd::sort has two “normal” overloads without an execution policy:\nallows Input Iterators or Output Iterators, then the overloads with an execution policy\nsingle-pass: you can only access the current element, and you cannot store iterators to\nThe parallel algorithms from the C++ Standard Library\nThus, given the “normal” signature for std::copy\nIterator categories in the C++ Standard Library\nThe C++ Standard Library defines five categories of iterators: Input Iterators, Output\nan Input Iterator invalidates any copies of that iterator.\ndates any copies of that iterator.\nThough you can't make an iterator go back to a previous element, you can store\ncopies and use them to reference earlier elements.\nParallel algorithms\nThis is important for parallelism: it means that the iterators can be freely copied\nIterator does not invalidate other copies is important, as it means that separate\nthreads can operate on their own copies of the iterators, incrementing them when\nIf the overload with an execution policy allowed use of Input Iterators, this would\nforce any threads to serialize access to the one and only iterator that was used for read-\n10.3.1 Examples of using parallel algorithms\nWith the C++ Standard Library algorithms, you can instead write\nstd::for_each(std::execution::par,v.begin(),v.end(),do_stuff);\nstd::execution::par is the policy that you’ll want to use most often, unless your\ncode is suitable for parallelization, then it should work with std::execution::par.\nsome circumstances, you may be able to use std::execution::par_unseq instead.\nThe parallel algorithms from the C++ Standard Library\nrely on the algorithm itself not accessing the same element from multiple threads,\nand use external synchronization outside the call to the parallel algorithm to prevent\nThe example from listing 10.1 shows some code that can be used with std::\nexecution::par, but not std::execution::par_unseq.\nfor synchronization means that attempting to use std::execution::par_unseq would\nvoid increment_all(std::vector<X>& v){\nstd::for_each(std::execution::par,v.begin(),v.end(),\nThe next listing shows an alternative that can be used with std::execution::par_un-\nParallel algorithms on a class with internal synchronization\nParallel algorithms on a class without internal synchronization\nParallel algorithms\nstd::mutex m;\nstd::vector<Y> v;\nstd::for_each(std::execution::par_unseq,v.begin(),v.end(),\nThe element accesses in listing 10.2 now have no synchronization, and it is safe to use\nstd::execution::par_unseq.\nLet’s now take a look at a more realistic example of how the parallel algorithms\nwant to process those logs to see aggregate data: how many visits per page, where do\nThe parallel algorithms from the C++ Standard Library\nextern log_info parse_log_line(std::string const &line);   \nusing visit_map_type= std::unordered_map<std::string, unsigned long long>;\ncount_visits_per_page(std::vector<std::string> const &log_lines) {\noperator()(visit_map_type lhs, visit_map_type rhs) const {   \nvisit_map_type operator()(log_info log,visit_map_type map) const{ \nvisit_map_type operator()(visit_map_type map,log_info log) const{ \nvisit_map_type operator()(log_info log1,log_info log2) const{ \nreturn std::transform_reduce(      \nstd::execution::par, log_lines.begin(), log_lines.end(),\nvisit_map_type(), combine_visits(), parse_log_line);\naround a call to std::transform_reduce c.\nParallel algorithms\nThe implementation of std::transform_reduce will therefore use the available\nhardware to perform this calculation in parallel (because you passed std::execution\nIn this chapter we looked at the parallel algorithms available in the C++ Standard\nyour choice of execution policy has on the behavior of the algorithm, and the restric-",
      "keywords": [
        "Iterators",
        "Standard Library",
        "std",
        "Input Iterators",
        "Forward Iterators",
        "Standard Library algorithms",
        "execution",
        "Parallel algorithms",
        "map",
        "execution policy",
        "Standard",
        "Output Iterators",
        "algorithms",
        "Parallel",
        "Library"
      ],
      "concepts": [
        "std",
        "iteration",
        "algorithm",
        "data",
        "element",
        "elements",
        "policy",
        "policies",
        "parallelized",
        "parallelism"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 14,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "",
          "score": 0.595,
          "base_score": 0.445,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 3,
          "title": "",
          "score": 0.594,
          "base_score": 0.594,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "iterators",
          "execution",
          "algorithms",
          "std execution",
          "parallel algorithms"
        ],
        "semantic": [],
        "merged": [
          "iterators",
          "execution",
          "algorithms",
          "std execution",
          "parallel algorithms"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4361016701396421,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856226+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 363-372)",
      "start_page": 363,
      "end_page": 372,
      "summary": "You can get any sort of bug in concurrent code; it’s not special in that regard.\nA thread is blocked when it’s unable to proceed\nmultithreaded code, but it’s not always desirable—hence the problem of unwanted\nDeadlock—As you saw in chapter 3, in the case of deadlock, one thread is waiting\nIf your threads deadlock, the\nIt’s therefore undesirable to block on external input from a thread that also\non the relative scheduling of operations in separate threads.\noperations to synchronize threads or through access to shared data without\nthread deleted the data being accessed), random memory corruption (due to a\nthe thread and the data it operates on aren’t tied together in some way, there’s\nthe potential for the data to be destroyed before the thread has finished and for\nally call join() in order to wait for the thread to complete, you need to ensure\nto ensure that correct synchronization is used, any thread can overwrite the data\nbeing used by any other thread in the application.\nEven after thoroughly reviewing your code, you still might have missed some bugs,\nemploy when testing multithreaded code.\n11.2.1 Reviewing code to locate potential bugs\nAs I’ve already mentioned, when reviewing multithreaded code to check for concurrency-\nQUESTIONS TO THINK ABOUT WHEN REVIEWING MULTITHREADED CODE\nor someone else) to think about specific questions relating to the code being\ncode and can help identify potential problems.\nWhere in the code could other threads be at this time?\nthread and those done in another?\nother threads?\nbetween the threads.\nmutex over its lifetime, such as with the thread-safe queue from chapter 6 where you\nthat other threads may have modified the shared data.\nlead to race conditions and bugs where the functions provided on a thread-safe data\nple threads concurrently, this is no longer the case because the lock on the internal\nyou test your code to confirm or disprove your belief in its lack of bugs?\n11.2.2 Locating concurrency-related bugs by testing\nWhen developing single-threaded applications, testing your applications is relatively\nTesting multithreaded code is an order of magnitude harder, because the precise\nthe code.\nHaving a potential race condition doesn’t mean the code will fail always, just\nYou want each test to run the smallest amount of code that\ncurrent pushes and pops work rather than testing it through a whole chunk of code\nIt can help if you think about how code should be tested when\nIt’s also worth eliminating the concurrency from the test in order to verify that the\na single thread, it’s a common, or garden-variety, bug rather than a concurrency-\nIf you’re managing threads manually, you’ll have to modify the code\nto use a single thread for the test.\nsingle thread, you can eliminate concurrency as a cause.\nThere’s more to testing concurrent code than the structure of the code being\ncontinue on with the example of testing a concurrent queue, you have to think about\nfor all threads\nThe number of threads to use relates to the particular\ncode being tested, but there are various ways of structuring tests to obtain suitable\napplication code to be easier to test.\nTesting multithreaded code is difficult, so you want to do what you can to make it eas-\nhas been written about designing single-threaded code for testability, and much of the\nIn general, code is easier to test if the following factors apply:\nYour tests can take complete control of the environment surrounding the code\nbeing tested\nThe code that performs the particular operation being tested is close together\nYou thought about how to test the code before you wrote it\nthreaded code, because it’s inherently that much harder to test.\nimportant: even if you don’t go as far as writing your tests before the code, it’s well\nworth thinking about how you can test the code before you write it—what inputs to\nuse, which conditions are likely to be problematic, how to stimulate the code in poten-\nOne of the best ways to design concurrent code for testing is to eliminate the con-\ncated data within a single thread, then you’ve greatly reduced the problem.\nthread can then be tested using the normal single-threaded techniques.\ntest concurrent code that deals with communicating between threads and ensuring\nthat only one thread at a time is accessing a particular block of data is now much\ntested independently with single-threaded techniques, with the test harness providing\nthe right thread in the right order can be tested independently, but with multiple con-\ncurrent threads and simple state logic designed specifically for the tests.\nall the usual single-threaded techniques, because this is now single-threaded code.\nwhich then becomes shared if multiple threads use the same set of library calls.\nbe a problem because it’s not immediately apparent that the code accesses shared data.\nfunction that’s safe for concurrent access from multiple threads.\nThere’s more to designing multithreaded code for testability than structuring your\nto bear in mind the same set of questions you ask yourself when reviewing the code,\nthe code, it will affect which design choices you make and will make testing easier.\nNow that we’ve looked at designing code to make testing easier, and potentially\nmodified the code to separate the “concurrent” parts (such as the thread-safe contain-\nniques for testing concurrency-aware code.\nof code that exercises the functions being tested.\nThe idea behind brute-force testing is to stress the code to see if it breaks.\ncally means running the code many times, possibly with many threads running at\nIf there’s a bug that manifests only when the threads are scheduled in a particu-\nlar fashion, then the more times the code is run, the more likely the bug is to appear.\nIf you run the test once and it passes, you might feel a bit of confidence that the code\nously for a thread-safe queue, this brute-force testing can give you a high degree of\nconfidence in your code.\nOn the other hand, if the code being tested is considerably\ncan run the test as many times as you like and it won’t fail, even if it would fail every\nyou’re testing on happens to run.\nof unit for the code being tested but also with respect to the design of the test harness\nthe code paths and the possible thread interactions as feasible.\ncode, it’s not guaranteed to find all the problems.\nanteed to find the problems, if you have the time to apply it to your code and the\nthe code.\ndata accesses, locks, and atomic operations from each thread.\nby each thread.\nin your code.\nSo, you have a technique that involves running your test many times under normal\nning your test many times under special conditions but that’s more likely to find any\nof the tests.\nby the calling thread when the data was accessed and report a failure if this was not\none mutex is held by a particular thread at once.\nIf another thread locks the same\ntest didn’t deadlock while running.\ncode is one where the implementations of the threading primitives such as mutexes\nand condition variables give the test writer control over which thread gets the lock",
      "keywords": [
        "code",
        "thread",
        "multiple threads",
        "Multiple threads calling",
        "multithreaded code",
        "testing multithreaded code",
        "Data",
        "Race conditions",
        "bugs",
        "threads calling",
        "n’t",
        "Race",
        "multiple",
        "concurrency-related bugs",
        "multithreaded"
      ],
      "concepts": [
        "thread",
        "tested",
        "code",
        "coding",
        "data",
        "concurrent",
        "particular",
        "problem",
        "application",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 7,
          "title": "",
          "score": 0.546,
          "base_score": 0.546,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "",
          "score": 0.542,
          "base_score": 0.542,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "",
          "score": 0.512,
          "base_score": 0.512,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "test",
          "testing",
          "tested",
          "thread"
        ],
        "semantic": [],
        "merged": [
          "code",
          "test",
          "testing",
          "tested",
          "thread"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38754117445676606,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856281+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 373-381)",
      "start_page": 373,
      "end_page": 381,
      "summary": "Having looked at various ways of executing test code, let’s now look at ways of\n11.2.5 Structuring multithreaded test code\nThe thread-specific setup code that must run on each thread\nThe code for each thread that you want to run concurrently\none thread calling push() on an empty queue while another thread calls pop().\nThe general setup code is simple: you must create the queue.\npop() has no thread-specific setup code.\nThe thread-specific setup code for the thread\nyou want to do this as part of the thread-specific setup, so that it doesn’t affect the test.\nby constructing an int in the setup code.\nThe code being tested is relatively straight-\nforward—a call to push() from one thread and a call to pop() from another—but\neither the pop() returned the data item supplied to the push() and the queue is\nThe final code is therefore an assertion that the popped value is the pushed\nEach thread sets a promise to\nindicate that it’s ready and then waits on a (copy of a) std::shared_future obtained\nfrom a third std::promise; the main thread waits for all the promises from all the\nvoid test_concurrent_push_and_pop_on_empty_queue()\nstd::promise<void> go,push_ready,pop_ready;    \nstd::future<void> push_done;     \nstd::future<int> pop_done;\nAn example test for concurrent push() and pop() calls on a queue\nthe futures you’ll use to indicate that the threads have finished e.\nfor the test threads to complete (which would deadlock—a deadlock in the test code\nInside the try block you can then start the threads, f and g—you use std::\nNote that the use of std::async makes your exception-safety task easier than it would\nbe with plain std::thread because the destructor for the future will join with the\nthread.\nrelevant promise for signaling readiness, while taking a copy of the ready future you\ngeneral ready signal before running the test code.\nFinally, the main thread calls get() on the futures from the async calls to wait for\ndidn’t make the threads wait for the go signal, then the push thread may have com-\nUsing the futures in this way ensures that both threads are running and\nUnblocking the future then allows both threads to run.\nFor tests that require more than two threads, this\n11.2.6 Testing the performance of multithreaded code\nIt’s therefore important to test your code to\nwant code that runs approximately 24 times faster or processes 24 times as much data\nat the overall design of the code before you start testing, so you know whether you’re\nConsequently, when testing for the performance of multithreaded code, it’s best to\nture tests for concurrent code.\n(such as rvalue references) or serve to make the code simpler or easier to under-\nWithout further ado, let’s start by looking at rvalue references, which are used\nRvalue references\nences; C++ references allow you to create a new name for an existing object.\nRvalue references\nto bind an rvalue to a const lvalue reference:\nhad rvalue references in order to allow you to pass temporaries to functions taking ref-\nThe C++11 Standard introduced rvalue references, which bind only to rvalues, not to lval-\nues or rvalues by having one overload take an lvalue reference and another take an\nrvalue reference.\nfunction parameter is an rvalue, you can use it as temporary storage, or “steal” its con-\nConsider a function that takes an std::vector<int> as a parameter and needs\nof doing this would be to take the parameter as a const lvalue reference and make the\nvoid process_copy(std::vector<int> const& vec_)\nThis allows the function to take both lvalues and rvalues but forces the copy in every\nIf you overload the function with a version that takes an rvalue reference, you\ncan avoid the copy in the rvalue case, because you know you can freely modify the\nvoid process_copy(std::vector<int> && vec)\nstd::copy(other.data,other.data+1000000,data);\nand copy the data across.\nby rvalue reference c.\nRvalue references\nFor class X the move constructor is an optimization, but in some cases it makes\ncopy constructor.\nnon-null instance is the one and only pointer to its object, so a copy constructor\nmember variable without copying, because although an rvalue reference parameter\nMove semantics are used extensively in the Thread Library, both where copies make\nlooked at transferring the ownership of threads between std::thread instances.\nstd::thread, std::unique_lock<>, std::future<>, std::promise<>, and std::\nstd::string and std::vector<> both can be copied as always, but\nAn std::thread instance that has been used as\nthe source of a move is equivalent to a default-constructed std::thread instance, for\nCopies\nrvalue reference\nto rvalue reference\nRvalue references and function templates\nThere’s a final nuance when you use rvalue references for parameters to a function\ntemplate: if the function parameter is an rvalue reference to a template parameter,\nThis allows a single function template to accept both lvalue and rvalue parameters\nand is used by the std::thread constructor (sections 2.1 and 2.2) so that the supplied\nSometimes it doesn’t make sense to allow a class to be copied.\ncopying.",
      "keywords": [
        "std",
        "thread",
        "code",
        "rvalue",
        "rvalue references",
        "reference",
        "int",
        "pop",
        "move",
        "copy",
        "queue",
        "setup code",
        "test code",
        "future",
        "data"
      ],
      "concepts": [
        "std",
        "code",
        "threads",
        "tested",
        "pop",
        "popped",
        "reference",
        "refer",
        "void",
        "copy"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.553,
          "base_score": 0.553,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 3,
          "title": "",
          "score": 0.547,
          "base_score": 0.547,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.545,
          "base_score": 0.545,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rvalue",
          "rvalue references",
          "code",
          "test",
          "references"
        ],
        "semantic": [],
        "merged": [
          "rvalue",
          "rvalue references",
          "code",
          "test",
          "references"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43766181139208493,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856336+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 382-396)",
      "start_page": 382,
      "end_page": 396,
      "summary": "Deleted functions\nany of the class’s member functions or friends tried to copy an instance: \ndelete to the function declaration.\nfunction of your class.\nalso explicitly write a move constructor and move-assignment operator, your class\nMove-only objects can be passed as function parameters and returned from functions,\nYou can apply the = delete specifier to any function, not just copy constructors\nDefaulted functions\nWhereas deleted functions allow you to explicitly declare that a function isn’t imple-\nmented, defaulted functions are the opposite extreme: they allow you to specify that\nthe compiler should write the function for you, with its “default” implementation.\ncan only do this for functions that the compiler can autogenerate anyway: default con-\ncompiler to write the function and change the access level.\nDefaulted functions\ndefine a custom copy constructor (for example), you can still get a compiler-\nJust as deleted functions are declared by following the declaration with = delete,\ndefaulted functions are declared by following the declaration by = default; for example:\nLiteral types used for constexpr functions (see section A.4) must have a trivial\nClasses with a trivial default constructor, copy constructor, copy assignment\nClasses with trivial copy assignment operators can be used with the std::atomic<>\nclass template (see section 5.2.6) in order to provide a value of that type with\nJust declaring the function as = default doesn’t make it trivial—it will only be trivial if\nThe second difference between classes with compiler-generated functions and\nIf you create an instance of class X without an initializer, the contained int (a) is\ndefault constructor, then a is initialized to zero:\ncompiler-generated default constructor and any of your data members and base\nclasses also have a compiler-generated default constructor, data members of those\nvalue or initialized to zero, depending on whether or not the outer class has its default\nmembers like a are always initialized (because you specify a value or explicitly default\nIf you omit the initialization of a from the constructor of X as in the third example B,\nconstexpr functions\ncompiler will no longer generate the default constructor for you, so if you want one\nexplicitly declaring the constructor as defaulted, you can force the compiler to gener-\nyou explicitly invoke the default constructor to request zero initialization, or (c) you\ninitialization with a value is declared constexpr (see section A.4) in order to allow\nconstexpr functions\nProvide an initializer for a static const class data member of integral type in\nThe constexpr keyword is primarily a function modifier.\nreturn type of a function meet certain requirements and the body is sufficiently sim-\nple, a function can be declared constexpr, in which case it can be used in constant\nconstexpr int square(int x)\nbecause the function can be used in a constant expression doesn’t mean that all uses\nconstexpr functions\nAll non-static data members and base classes must be trivial types.\nIt must have either a trivial default constructor or a constexpr constructor\nial default constructor, such as class CX in the following listing.\nNote that we’ve explicitly declared the default constructor B as defaulted (see section\nYou can, for example, provide a constexpr function that cre-\nA class with a trivial default constructor\nYou can also create a simple constexpr function that copies its parameter:\nBut that’s about all you can do in C++11—a constexpr function can only call other\nconstexpr functions.\nthing in a constexpr function, provided it doesn’t modify any objects with non-local\nWhat you can do, even in C++11, is apply constexpr to the member functions\nconstexpr CX(int a_, int b_):\nconstexpr int get_a() const    \nconstexpr int get_b()    \nconstexpr int foo()\nof constexpr functions), so get_b() is no longer implicitly const.\nmore complex constexpr functions such as the following:\nconstexpr CX make_cx(int a)\nconstexpr int foo_squared(CX val)\nconstexpr functions\nsions and constexpr functions involving user-defined types is that objects of a literal\ntype initialized with a constant expression are statically initialized, and so their initial-\nIf the constructor is declared constexpr and the con-\nthe default constructor of std::mutex is declared constexpr to ensure that mutex ini-\nSo far, we’ve looked at constexpr as applied to functions.\nis initialized with a constant expression, constexpr constructor, or aggregate initial-\nconstexpr int i=45;              \nconstexpr int j=foo();   \nconstexpr function requirements\nIn order to declare a function as constexpr it must meet a few requirements; if it\nC++11, the requirements for a constexpr function were as follows:\nThe function body must consist of a single return statement.\nAny constructor or conversion operator used to construct the return value from\nconstexpr functions are pure functions with no side effects.\nFor constexpr class member functions there are additional requirements:\nconstexpr member functions can’t be virtual.\nThe class for which the function is a member must be a literal type.\ncompiler it must satisfy the requirements for a constexpr function.\nThe constructors chosen for the initialization of the data members and base\nclasses must be constexpr constructors.\nThis is the same set of rules as for functions, except that there’s no return value, so no\nInstead, the constructor initializes all the bases and data members\nTrivial copy constructors are implicitly constexpr.\nWhen constexpr is applied to a function template, or to a member function of a class\nThis allows you to write function templates that are\nfunctions otherwise, for example:\nLambda functions\nconstexpr int i=sum(3,42);    \nThe function must satisfy all the other requirements for a constexpr function.\ncan’t declare a function with multiple statements constexpr just because it’s a func-\nLambda functions\nLambda functions are one of the most exciting features of the C++11 Standard,\nThe C++11 lambda function syntax\nallows a function to be defined at the point where it’s needed in another expression.\nThis works well for things like predicates provided to the wait functions of std::\nthe necessary state in the member variables of a class with a function call operator.\nAt its simplest, a lambda expression defines a self-contained function that takes no\nparameters and relies only on global variables and functions.\nIf your lambda function body consists of a single\nreturn statement, the return type of the lambda is the type of the expression being\nThe return type of the lambda passed to cond.wait() B is deduced from the type of\nreturn type is specified by following the lambda parameter list with an arrow (->) and\nA simple lambda with a deduced return type\nLambda functions\nLambda functions that reference local variables\nLambda functions with a lambda introducer of [] can’t reference any local variables\naccess copies of the local variables at the time the lambda was created.\nstd::function<int(int)> make_offseter(int offset)\nEvery call to make_offseter returns a new lambda function object through the\nstd::function<> function wrapper.\nThis returned function adds the supplied offset\nstd::function<int(int)> offset_42=make_offseter(42);\nstd::function<int(int)> offset_123=make_offseter(123);\nwill write out 54,135 twice because the function returned from the first call to make_\noffseter always adds 42 to the supplied argument, whereas the function returned\nreturn the lambda and call it outside the scope of the original function.\nA lambda function that captures all the local variables by reference is introduced\nstd::function<int(int)> offset_a=[&](int j){return offset+j;};    \nstd::function<int(int)> offset_b=[&](int j){return offset+j;};    \nWhereas in the make_offseter function from the previous example you used the [=]\nlambda introducer to capture a copy of the offset, the offset_a function in this exam-\nple uses the [&] lambda introducer to capture offset by reference c.\nstd::function<int()> f=[=,&j,&k]{return i+j+k;};\nstd::function<int()> f=[&,j,k]{return i+j+k;};\nLambda functions\nstd::function<int()> f=[&i,j,&k]{return i+j+k;};\nmembers if the function containing the lambda is a member function.\ncan’t be captured directly; if you want to access class members from your lambda, you\nexample, the lambda captures this to allow access to the some_data class member:\nconstructor as a thread function (section 2.1.1) and as the function when using paral-\nIn this case, the function call operator is",
      "keywords": [
        "function",
        "int",
        "constexpr",
        "std",
        "functions",
        "constructor",
        "Lambda",
        "default constructor",
        "constexpr functions",
        "default",
        "type",
        "constexpr int",
        "Lambda functions",
        "offset",
        "constant"
      ],
      "concepts": [
        "function",
        "std",
        "returned",
        "initialized",
        "initialize",
        "initialization",
        "defaulted",
        "classes",
        "lambda",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 46,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 13,
          "title": "",
          "score": 0.462,
          "base_score": 0.312,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.453,
          "base_score": 0.303,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 6,
          "title": "",
          "score": 0.422,
          "base_score": 0.422,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.38,
          "base_score": 0.38,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "constexpr",
          "functions",
          "lambda",
          "int",
          "function"
        ],
        "semantic": [],
        "merged": [
          "constexpr",
          "functions",
          "lambda",
          "int",
          "function"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2471667211578706,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856378+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "summary": "Variadic templates are templates with a variable number of parameters.\nber of parameters, you can now have variadic templates that have a variable number of\ntemplate parameters.\nVariadic templates are used throughout the C++ Thread Library.\nFor example, the std::thread constructor for starting a thread (section 2.1.1) is a\nvariadic function template, and std::packaged_task<> (section 4.2.2) is a variadic\neter list, variadic templates are declared with an ellipsis in the template parameter list:\nFor example, the primary template for std::packaged\nchapter 4 that you can write std::packaged_task<int(std::string,double)> to\ndeclare a task that takes an std::string and a double as parameters when you call it\nization to show that the types that make up Args when the template is instantiated\nThe variadic parameter Args is called a parameter pack, and the use of Args...\nparameter is my_class, and the Args parameter pack is empty, whereas with\nstd::packaged_task<void(int,double,my_class&,std::string*)> the ReturnType\nis void, and Args is the list int, double, my_class&, std::string*.\nIn this case the single member variable data is an instantiation of std::tuple<>\ncontaining all the types specified, so dummy<int,double,char> has a member of\ntype std::tuple<int,double,char>.\nThis time, the tuple has an additional (first) member of type std::string.\nThe type expression can be as complex as you like, provided the parameter pack\nIf your parameter pack Params contains the types int,int,char, then\ntemplate parameters required:\nTypes>\nstd::pair<Types...> data;\nThis creates a new parameter pack, args, which is a list of the function parameters\na pattern with the pack expansion for declaring the function parameters, just as you\nthe std::thread constructor to take all the function arguments by rvalue reference\nAutomatically deducing the type of a variable\nNote that in this case, the pack expansion contains both the type pack ArgTypes and\nthe function parameter pack args, and the ellipsis follows the whole expression.\nvoid bar<int&,double,std::string>(\nfoo(std::forward<int&>(args_1),\nIt doesn’t matter whether this is a type parameter pack or a function argu-\nAutomatically deducing the type of a variable\nC++ is a statically typed language: the type of every variable is known at compile time.\nNot only that, but as a programmer you have to specify the type of each variable.\nauto and the corresponding type of the variable:\nother place in the language where types are deduced: parameters of function tem-\nthe type of var is the same as the type deduced for the parameter of a function tem-\na template type parameter:\ntype expression explicitly declares the variable as a reference; for example:\nThis can greatly simplify the declaration of variables, particularly where the full type\nThread-local variables\nThread-local variables\nYou mark a variable as being thread-local by declaring it with the\nand local variables can be declared thread-local, and are said to have thread storage\nthread_local int x;    \nstatic thread_local std::string s;       \nstatic thread_local std::string X::s;     \nthread_local std::vector<int> v;    \nThread-local variables at namespace scope and thread-local static class data members\nare constructed before the first use of a thread-local variable from the same transla-\nthread-local variables when the thread is started; others may construct them immedi-\ncontaining thread-local variables—these variables can be constructed on a given\nthread the first time that thread references a thread-local variable from the dynami-\nThread-local variables declared inside a function are initialized the first time the\nnot called by a given thread, any thread-local variables declared in that function are\nconstruction of a thread-local variable throws an exception, std::terminate() is called\nThe destructors for all thread-local variables that have been constructed on a given\nof a thread-local variable exits with an exception, std::terminate() is called, as for\nA thread-local variable \nThread-local variables are also destroyed for a thread if that thread calls\ntion exits, the destructors of thread-local variables on those threads are not called.\nso if you pass a pointer to a thread-local variable to another thread, you need to\nC++17 extends the idea of automatically deducing types to template parameters: if you\nare declaring an object of a templated type, then in many cases the type of the tem-\ntype deduction rules for function templates.\nFor example, std::lock_guard takes a single template parameter, which is the\nto that type.\nIf you declare an object to be of type std::lock_guard, then the type\nparameter can be deduced from the type of the supplied mutex:\nThe same applies to std::scoped_lock, except that it has multiple template parame-",
      "keywords": [
        "std",
        "template",
        "type",
        "Args",
        "parameter pack",
        "thread",
        "Thread-local variables",
        "pack",
        "Variadic templates",
        "variable",
        "parameter",
        "variables",
        "Thread-local",
        "int",
        "Variadic"
      ],
      "concepts": [
        "std",
        "types",
        "template",
        "thread",
        "variable",
        "variables",
        "parameters",
        "initialized",
        "initialization",
        "reference"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 51,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.574,
          "base_score": 0.424,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.535,
          "base_score": 0.385,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread local",
          "local",
          "pack",
          "local variables",
          "parameter"
        ],
        "semantic": [],
        "merged": [
          "thread local",
          "local",
          "pack",
          "local variables",
          "parameter"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.333000218617637,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856429+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 405-412)",
      "start_page": 405,
      "end_page": 412,
      "summary": "Erlang provides support for message-passing concurrency.\nThere are even C++ class\nclass\nstd::thread class and\nstd::mutex class and\nstd::atomic<> class\nclass\nclass\nA message-passing\nBack in chapter 4, I presented an example of sending messages between threads\nListing C.1 shows the message queue.\nbase class; the specific message type is handled with a template class derived from\nmessage_base class doesn’t have any member functions, the popping thread will\nA simple message queue\nmessage_base\nexplicit wrapped_message(Msg const& contents_):\nclass queue         \nstd::queue<std::shared_ptr<message_base> > q;   \nq.push(std::make_shared<wrapped_message<T> >(msg));   \nstd::shared_ptr<message_base> wait_and_pop()\nc.wait(lk,[&]{return !q.empty();});    \nSending messages is handled through an instance of the sender class shown in list-\nThis is a thin wrapper around a message queue that only allows messages to\nthe queue itself.\nqueue*q;    \ntemplate<typename Message>\nvoid send(Message const& msg)\nEach message type \nYour message \nqueue\npointers to message_base\nmessage and \nmessage types being waited on and call the appropriate handler function.\nqueue q;     \ndispatcher wait()        \nreturn dispatcher(&q);\nWhereas a sender references a message queue, a receiver owns it.\ndoing the message dispatch starts with a call to wait().\nThe dispatcher class is shown in\nwork consists of waiting for a message and dispatching it.\nclass close_queue      \nclass dispatcher\nqueue* q;\nThe dispatcher class\nmessage on the queue\nthe queue.\nWaiting for a queue \nThe message for \nstd::shared_ptr<message_base> const& msg)\nif(dynamic_cast<wrapped_message<close_queue>*>(msg.get()))\nexplicit dispatcher(queue* q_):          \ntemplate<typename Message,typename Func>\nTemplateDispatcher<dispatcher,Message,Func> \nreturn TemplateDispatcher<dispatcher,Message,Func>(\nwait_and_dispatch();\ntor calls wait_and_dispatch(), which is a loop B that waits for a message and passes it\nto dispatch().\ndispatch() itself c is rather simple, it checks whether the message\nis a close_queue message and throws an exception if it is; otherwise, it returns false\ndispatching messages\nmessage, and throws.\nwait for messages.\nof message with a \nIt’s a template, and the message type isn’t deducible, so you must specify\nwhich message type to handle and pass in a function (or callable object) to handle it.\nhandle() itself passes the queue, the current dispatcher object, and the handler\nfunction to a new instance of the TemplateDispatcher class template, to handle mes-\nin the destructor before waiting for messages; not only does it prevent moved-from\nclass TemplateDispatcher\nqueue* q;\nbool dispatch(std::shared_ptr<message_base> const& msg)\nif(wrapped_message<Msg>* wrapper=\nThe TemplateDispatcher class template\nIf you handle the message, \nCheck the message type\nTemplateDispatcher(queue* q_,PreviousDispatcher* prev_,Func&& f_):\nwait_and_dispatch();\nThe TemplateDispatcher<> class template is modeled on the dispatcher class and is\nfor a message.\nBecause you don’t throw exceptions if you handle the message, you now need to\nYour message\nif you chain calls to handle() e to allow multiple types of messages to be handled, this\nthrow an exception (including the dispatcher’s default handler for close_queue mes-\nThis simple framework allows you to push any type of message on the queue and\nallows you to pass around a reference to the queue for pushing messages on, while",
      "keywords": [
        "message",
        "queue",
        "std",
        "dispatcher",
        "wait",
        "TemplateDispatcher",
        "Msg",
        "message queue",
        "Boost",
        "message type",
        "template",
        "type",
        "handle",
        "sender",
        "Func"
      ],
      "concepts": [
        "classes",
        "message",
        "messaging",
        "std",
        "dispatcher",
        "queue",
        "threading",
        "functions",
        "function",
        "msg"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.582,
          "base_score": 0.582,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.571,
          "base_score": 0.571,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.564,
          "base_score": 0.564,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.561,
          "base_score": 0.561,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 34,
          "title": "",
          "score": 0.554,
          "base_score": 0.554,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "message",
          "queue",
          "dispatcher",
          "message queue",
          "templatedispatcher"
        ],
        "semantic": [],
        "merged": [
          "message",
          "queue",
          "dispatcher",
          "message queue",
          "templatedispatcher"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4460232327615124,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856503+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 413-421)",
      "start_page": 413,
      "end_page": 421,
      "summary": "struct withdraw\nwithdraw(std::string const& account_,\ncancel_withdrawal(std::string const& account_,\nwithdrawal_processed(std::string const& account_,\nstruct withdraw_pressed\nverify_pin(std::string const& account_,std::string const& pin_,\nstruct display_enter_pin\nstruct display_withdrawal_cancelled\nstruct display_pin_incorrect_message\nstruct display_withdrawal_options\nget_balance(std::string const& account_,messaging::sender atm_queue_):\n[&](withdraw_ok const& msg)\n[&](withdraw_denied const& msg)\n[&](cancel_pressed const& msg)\ndisplay_withdrawal_cancelled());\n[&](balance const& msg)\ninterface_hardware.send(display_balance(msg.amount));\n[&](cancel_pressed const& msg)\ninterface_hardware.send(display_withdrawal_options());\n.handle<withdraw_pressed>(\n[&](withdraw_pressed const& msg)\nwithdrawal_amount=msg.amount;\nbank.send(withdraw(account,msg.amount,incoming));\nstate=&atm::process_withdrawal;\n[&](balance_pressed const& msg)\n[&](cancel_pressed const& msg)\n[&](pin_verified const& msg)\n[&](pin_incorrect const& msg)\ndisplay_pin_incorrect_message());\n[&](cancel_pressed const& msg)\n[&](digit_pressed const& msg)\nstate=&atm::verifying_pin;\n[&](cancel_pressed const& msg)\ninterface_hardware.send(display_enter_pin());\n[&](verify_pin const& msg)\nif(msg.pin==\"1937\")\nmsg.atm_queue.send(pin_verified());\nmsg.atm_queue.send(pin_incorrect());\n[&](withdraw const& msg)\nmsg.atm_queue.send(withdraw_ok());\nmsg.atm_queue.send(withdraw_denied());\n[&](get_balance const& msg)\nmsg.atm_queue.send(::balance(balance));\n[&](withdrawal_processed const& msg)\n[&](cancel_withdrawal const& msg)\n[&](display_insufficient_funds const& msg)\n.handle<display_enter_pin>(\n[&](display_enter_pin const& msg)\n[&](display_enter_card const& msg)\n[&](display_balance const& msg)\n.handle<display_withdrawal_options>(\n[&](display_withdrawal_options const& msg)\n.handle<display_withdrawal_cancelled>(\n[&](display_withdrawal_cancelled const& msg)\n.handle<display_pin_incorrect_message>(\n[&](display_pin_incorrect_message const& msg)",
      "keywords": [
        "std",
        "ATM",
        "amount",
        "handle",
        "unsigned amount",
        "msg",
        "pin",
        "pressed",
        "const",
        "display",
        "balance",
        "account",
        "messaging",
        "struct",
        "withdrawal"
      ],
      "concepts": [
        "handle",
        "std",
        "atm",
        "message",
        "messaging",
        "pin",
        "incoming",
        "state",
        "balance",
        "msg"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.443,
          "base_score": 0.443,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.439,
          "base_score": 0.439,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 54,
          "title": "",
          "score": 0.423,
          "base_score": 0.423,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 19,
          "title": "",
          "score": 0.418,
          "base_score": 0.418,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "msg",
          "const msg",
          "const",
          "send",
          "struct"
        ],
        "semantic": [],
        "merged": [
          "msg",
          "const msg",
          "const",
          "send",
          "struct"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33022364604109034,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.856555+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 422-446)",
      "start_page": 422,
      "end_page": 446,
      "summary": "The <chrono> header provides classes for representing points in time, durations,\nand clock classes, which act as a source of time_points.\nThe std::chrono::steady_clock class is the\nstd::chrono::duration class template \nThe std::chrono::duration class template provides a facility for representing\ndurations.\nduration value and an instantiation of the std::ratio class template indicating the\nstd::chrono::duration<int, std::milli> is a count of milliseconds stored in a value\nof type int, whereas std::chrono::duration<short, std::ratio<1,50>> is a count of\nfiftieths of a second stored in a value of type short, and std::chrono:: d-uration\nconstexpr duration(const duration<Rep2, Period2>& d);\nduration& operator*=(const rep& rhs);\nduration& operator/=(const rep& rhs);\nduration& operator%=(const rep& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nSTD::CHRONO::DURATION::REP TYPEDEF \nrep is the type of value used to hold the internal representation of the duration\nSTD::CHRONO::DURATION::PERIOD TYPEDEF \nstd::ratio<1,50>, a duration value with a count() of N represents N fiftieths of\nSTD::CHRONO::DURATION DEFAULT CONSTRUCTOR\nConstructs an std::chrono::duration instance with a default value.\nThe internal value of the duration (of type rep) is default initialized.\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM A COUNT VALUE\nConstructs an std::chrono::duration instance with a specified count.\nThe internal value of the duration object is initialized with static_cast<rep>(r).\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM ANOTHER STD::CHRONO::DURATION VALUE\nConstructs an std::chrono::duration instance by scaling the count value of another\nstd::chrono::duration object.\nconstexpr duration(const duration<Rep2,Period2>& d);\nthis->count()==duration_cast<duration<Rep,Period>>(d).count()\nSTD::CHRONO::DURATION::COUNT MEMBER FUNCTION\nThe internal value of the duration object, as a value of type rep.\nSTD::CHRONO::DURATION::OPERATOR+ UNARY PLUS OPERATOR\nSTD::CHRONO::DURATION::OPERATOR- UNARY MINUS OPERATOR\nReturns a duration such that the count() value is the negative value of this->\nduration(-this->count());\nSTD::CHRONO::DURATION::OPERATOR++ PRE-INCREMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR++ POST-INCREMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR-- PRE-DECREMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR-- POST-DECREMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSTD::CHRONO::DURATION::OPERATOR*= COMPOUND ASSIGNMENT OPERATOR\nduration& operator*=(rep const& rhs);\nSTD::CHRONO::DURATION::OPERATOR/= COMPOUND ASSIGNMENT OPERATOR\nduration& operator/=(rep const& rhs);\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nduration& operator%=(rep const& rhs);\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nSTD::CHRONO::DURATION::ZERO STATIC MEMBER FUNCTION\nReturns a duration object representing a value of zero.\nSTD::CHRONO::DURATION::MIN STATIC MEMBER FUNCTION\nReturns a duration object holding the minimum possible value for the specified\nSTD::CHRONO::DURATION::MAX STATIC MEMBER FUNCTION\nReturns a duration object holding the maximum possible value for the specified\nSTD::CHRONO::DURATION EQUALITY COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs==rhs returns CommonDuration(lhs)\nSTD::CHRONO::DURATION INEQUALITY COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nSTD::CHRONO::DURATION LESS-THAN COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs<rhs returns CommonDuration(lhs)\nSTD::CHRONO::DURATION GREATER-THAN COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nSTD::CHRONO::DURATION LESS-THAN-OR-EQUALS COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nSTD::CHRONO::DURATION GREATER-THAN-OR-EQUALS COMPARISON OPERATOR\nconst duration<Rep1, Period1>& lhs, \nconst duration<Rep2, Period2>& rhs);\nSTD::CHRONO::DURATION_CAST NONMEMBER FUNCTION\nExplicitly converts an std::chrono::duration object to a specific std::chrono::\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nToDuration must be an instantiation of std::chrono::duration.\nstd::chrono::time_point class template \nThe std::chrono::time_point class template represents a point in time, as measured by\nIt’s specified as a duration since the epoch of that particular clock.\nduration since the epoch and must be an instantiation of the std::chrono::duration\ntemplate <class Clock,class Duration = typename Clock::duration>\nexplicit time_point(const duration& d);\ntime_point(const time_point<clock, Duration2>& t);\nduration time_since_epoch() const;\ntime_point& operator+=(const duration& d);\ntime_point& operator-=(const duration& d);\nSTD::CHRONO::TIME_POINT DEFAULT CONSTRUCTOR\nConstructs a time_point representing the epoch of the associated Clock; the internal\nSTD::CHRONO::TIME_POINT DURATION CONSTRUCTOR\nConstructs a time_point representing the specified duration since the epoch of the\nexplicit time_point(const duration& d);\nFor a time_point object, tp, constucted with tp(d) for some duration, d, tp.time_\nSTD::CHRONO::TIME_POINT CONVERSION CONSTRUCTOR\ntime_point(const time_point<clock, Duration2>& t);\nobject of the Duration type, and that value is stored in the newly constructed time_\nSTD::CHRONO::TIME_POINT::TIME_SINCE_EPOCH MEMBER FUNCTION\nRetrieves the duration since the clock epoch for a particular time_point object.\nduration time_since_epoch() const;\nSTD::CHRONO::TIME_POINT::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nAdds the specified duration to the value stored in the specified time_point object.\ntime_point& operator+=(const duration& d);\nSTD::CHRONO::TIME_POINT::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSubtracts the specified duration from the value stored in the specified time_point\ntime_point& operator-=(const duration& d);\nSTD::CHRONO::TIME_POINT::MIN STATIC MEMBER FUNCTION\ntime_point(time_point::duration::min()) (see 11.1.1.15)\nSTD::CHRONO::TIME_POINT::MAX STATIC MEMBER FUNCTION\ntime_point(time_point::duration::max()) (see 11.1.1.16)\nstd::chrono::system_clock class \nThe std::chrono::system_clock class provides a means of obtaining the current\nby calling std::chrono::system_clock::now().\nclock::time_point can be converted to and from time_t with the std::chrono::\nsystem_clock::to_time_t() and std::chrono::system_clock::to_time_point()\nThe system clock isn’t steady, so a subsequent call to std::chrono::system_\ntypedef std::chrono::duration<rep,period> duration;\ntypedef std::chrono::time_point<system_clock> time_point;\nSTD::CHRONO::SYSTEM_CLOCK::REP TYPEDEF \nSTD::CHRONO::SYSTEM_CLOCK::PERIOD TYPEDEF \nduration or time_point.\nSTD::CHRONO::SYSTEM_CLOCK::DURATION TYPEDEF \nAn instantiation of the std::chrono::duration class template that can hold the dif-\nference between any two time points returned by the system-wide real-time clock.\ntypedef std::chrono::duration<\nstd::chrono::system_clock::rep,\nstd::chrono::system_clock::period> duration;\nSTD::CHRONO::SYSTEM_CLOCK::TIME_POINT TYPEDEF \nAn instantiation of the std::chrono::time_point class template that can hold time\npoints returned by the system-wide real-time clock.\ntypedef std::chrono::time_point<std::chrono::system_clock> time_point;\nSTD::CHRONO::SYSTEM_CLOCK::NOW STATIC MEMBER FUNCTION \nSTD::CHRONO::SYSTEM_CLOCK::TO_TIME_T STATIC MEMBER FUNCTION \nSTD::CHRONO::SYSTEM_CLOCK::FROM_TIME_T STATIC MEMBER FUNCTION \nstd::chrono::steady_clock class \nThe std::chrono::steady_clock class provides access to the system-wide steady clock.\nThe current time can be obtained by calling std::chrono::steady_clock::now().\nThere is no fixed relationship between values returned by std::chrono::steady_\nstd::chrono::steady_clock::now() happens-before another call to std::chrono\n::steady_clock::now(), the second call must return a time point equal to or later\ntypedef std::chrono::duration<rep,period> duration;\ntypedef std::chrono::time_point<steady_clock>\nSTD::CHRONO::STEADY_CLOCK::REP TYPEDEF \nSTD::CHRONO::STEADY_CLOCK::PERIOD TYPEDEF \nduration or time_point.\nSTD::CHRONO::STEADY_CLOCK::DURATION TYPEDEF \nThis is an instantiation of the std::chrono::duration class template that can hold\nthe difference between any two time points returned by the system-wide steady clock.\ntypedef std::chrono::duration<\nstd::chrono::steady_clock::rep,\nstd::chrono::steady_clock::period> duration;\nSTD::CHRONO::STEADY_CLOCK::TIME_POINT TYPEDEF \nThis instantiation of the std::chrono::time_point class template can hold time\ntypedef std::chrono::time_point<std::chrono::steady_clock> time_point;\nSTD::CHRONO::STEADY_CLOCK::NOW STATIC MEMBER FUNCTION \nA time_point representing the current time of the system-wide steady clock.\nIf one call to std::chrono::steady_clock::now() happens-before another, the\nstd::chrono::high_resolution_clock typedef \nThe std::chrono::high_resolution_clock class provides access to the system-wide\nby calling std::chrono::high_resolution_clock::now().\nresolution_clock may be a typedef for the std::chrono::system_clock class or the\nstd::chrono::steady_clock class, or it may be a separate type.\nAlthough std::chrono::high_resolution_clock has the highest resolution of all\nthe library-supplied clocks, std::chrono::high_resolution_clock::now() still takes\nstd::chrono::high_resolution_clock::now() when timing short operations.\ntypedef std::chrono::duration<rep,period> duration;\ntypedef std::chrono::time_point<\nThe std::condition_variable class allows a thread to wait for a condition to become\ntemplate <typename Clock, typename Duration>\nconst std::chrono::time_point<Clock, Duration>& absolute_time);\nconst std::chrono::time_point<Clock, Duration>& absolute_time,\nconst std::chrono::duration<Rep, Period>& relative_time);\nconst std::chrono::duration<Rep, Period>& relative_time,\nWakes one of the threads currently waiting on a std::condition_variable.\nWake all of the threads currently waiting on a std::condition_variable.\nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION \nWaits until std::condition_variable is woken by a call to notify_one(), a call to\nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nstd::chrono::duration<Rep,Period> const& relative_time);\nSTD::CONDITION_VARIABLE::WAIT_FOR MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nstd::chrono::duration<Rep,Period> const& relative_time,\nstd::chrono::duration<Rep,Period> remaining_time=\nif(wait_for(lock,remaining_time)==std::cv_status::timeout)\ntemplate<typename Clock,typename Duration>\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nSTD::CONDITION_VARIABLE::WAIT_UNTIL MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nstd::chrono::time_point<Clock,Duration> const& absolute_time,\nif(wait_until(lock,absolute_time)==std::cv_status::timeout)\nuation of (bool)pred() returns true or Clock::now() returns a time equal to",
      "keywords": [
        "std",
        "duration",
        "chrono",
        "time",
        "operator",
        "const duration",
        "typedef std",
        "clock",
        "COMPOUND ASSIGNMENT OPERATOR",
        "STATIC MEMBER FUNCTION",
        "point",
        "MEMBER FUNCTION",
        "duration operator",
        "Declaration typedef std",
        "constexpr duration operator"
      ],
      "concepts": [
        "std",
        "durations",
        "duration",
        "clock",
        "operator",
        "operating",
        "operations",
        "declaration",
        "period",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.538,
          "base_score": 0.388,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "",
          "score": 0.498,
          "base_score": 0.348,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 50,
          "title": "",
          "score": 0.488,
          "base_score": 0.488,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "duration",
          "chrono",
          "std chrono",
          "time_point",
          "chrono duration"
        ],
        "semantic": [],
        "merged": [
          "duration",
          "chrono",
          "std chrono",
          "time_point",
          "chrono duration"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3361696242631227,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856603+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 447-454)",
      "start_page": 447,
      "end_page": 454,
      "summary": "APPENDIX D\nC++ Thread Library reference\nSTD::NOTIFY_ALL_AT_THREAD_EXIT NONMEMBER FUNCTION \ncurrent thread exits.\nstd::system_error if the effects can’t be achieved.\nThe lock is held until the thread exits, so care must be taken to avoid\nthread has exited when they are woken, particularly with the potential for spurious\nThis can be achieved by testing a predicate on the waiting thread that’s\nonly made true by the notifying thread under the protection of the mutex and\nwithout releasing the lock on the mutex prior to the call of notify_all_at_thread\n_exit.std::condition_variable_any class.\nD.2.2\nmeets the Lockable requirements.\nvoid notify_one() noexcept;\nvoid notify_all() noexcept;\nstd::cv_status wait_until(\ntypename Duration, typename Predicate>\nbool wait_until(\nstd::cv_status wait_for(\ntypename Period, typename Predicate>\nbool wait_for(\nAPPENDIX D\nC++ Thread Library reference\nvoid notify_one() noexcept;\nstd::system_error if the effects can’t be achieved.\nA call to notify_\nvoid notify_all() noexcept;\nA call to notify_\na call to notify_one() or notify_all() by another thread, or the thread is woken\nIf the lock object is unlocked\nIt’s there-\nA call to notify_\nwhile(!pred())\nAPPENDIX D\nC++ Thread Library reference\nA call to notify_\nstd::cv_status wait_for(\nified by relative_time has elapsed or the thread is woken spuriously.\nThe lock\nIf the lock object is unlocked\nIt’s\ncate is used in preference where possible.\nOtherwise, it’s recommended that\nwait_for() be called in a loop that tests the predicate associated with the\ntimeout is still valid; wait_until() may be more appropriate in many circum-\nThe thread may be blocked for longer than the specified duration.\nA call to notify_\ntypename Period, typename Predicate>\nbool wait_for(\nwhile(!pred())\nend-internal_clock::now();\nThe thread may be blocked for longer than the\nA call to notify_\nAPPENDIX D\nC++ Thread Library reference\nnotify_all(), until a specified time has been reached, or the thread is woken\nstd::cv_status wait_until(\nIf the lock object is unlocked\nIt’s\nicate is used in preference where possible.\nOtherwise, it’s recommended that\nwait_until() be called in a loop that tests the predicate associated with the\nthread became unblocked.\nA call to notify_\ntypename Duration, typename Predicate>\n<atomic> header\nbool wait_until(\nwhile(!pred())\nor later than absolute_time.\nwhich the thread became unblocked.\nA call to notify_\nD.3\n<atomic> header \nThe <atomic> header provides the set of basic atomic types and operations on those",
      "keywords": [
        "STD",
        "NOTIFY",
        "wait",
        "condition",
        "variable",
        "Lockable",
        "call",
        "Thread",
        "time",
        "call to notify",
        "MEMBER FUNCTION",
        "predicate",
        "MEMBER FUNCTION Waits",
        "pred",
        "typename Lockable"
      ],
      "concepts": [
        "std",
        "wait",
        "lock",
        "define",
        "effects",
        "thread",
        "atomic",
        "predicate",
        "period",
        "header"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.854,
          "base_score": 0.704,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 11,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "notify_",
          "predicate",
          "typename",
          "thread",
          "wait_until"
        ],
        "semantic": [],
        "merged": [
          "notify_",
          "predicate",
          "typename",
          "thread",
          "wait_until"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43911791212439855,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856659+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 455-489)",
      "start_page": 455,
      "end_page": 489,
      "summary": "extern \"C\" void atomic_thread_fence(memory_order order);\nextern \"C\" void atomic_signal_fence(memory_order order);\nstd::atomic<> specialization\nstd::atomic_char \nstd::atomic<char> \nstd::atomic_schar \nstd::atomic_uchar \nstd::atomic_short \nstd::atomic<short> \nstd::atomic_ushort \nstd::atomic_int \nstd::atomic<int> \nstd::atomic_uint \nstd::atomic_long \nstd::atomic<long> \nstd::atomic_ulong \nstd::atomic_llong \nstd::atomic_ullong \nstd::atomic_wchar_t \nstd::atomic<wchar_t> \nstd::atomic_char16_t \nstd::atomic<char16_t> \nstd::atomic_char32_t \nstd::atomic<char32_t> \nLOCK_FREE is 2, operations on instances of std::atomic<int> and std::atomic\natomic operations.\nstd::atomic_thread_fence function \nextern \"C\" void atomic_thread_fence(std::memory_order order);\nThe std::atomic_signal_fence() function inserts a memory barrier or fence in the\nextern \"C\" void atomic_signal_fence(std::memory_order order);\nstd::atomic_thread_fence(order) except that the constraints apply only between\nstd::atomic_flag class \natomic_flag() noexcept = default;\nvolatile atomic_flag*, memory_order) noexcept;\natomic_flag*, memory_order) noexcept;\nvolatile atomic_flag*, memory_order) noexcept;\natomic_flag*, memory_order) noexcept;\nstd::atomic_flag() noexcept = default;\nConstructs a new std::atomic_flag object in the clear state.\nSTD::ATOMIC_FLAG_TEST_AND_SET_EXPLICIT NONMEMBER FUNCTION \nvolatile atomic_flag* flag, memory_order order) noexcept;\natomic_flag* flag, memory_order order) noexcept;\nThis is an atomic store operation for the memory location comprising\nSTD::ATOMIC_FLAG_CLEAR NONMEMBER FUNCTION \nSTD::ATOMIC_FLAG_CLEAR_EXPLICIT NONMEMBER FUNCTION \nvolatile atomic_flag* flag, memory_order order) noexcept;\natomic_flag* flag, memory_order order) noexcept;\nstd::atomic class template \nThe std::atomic class provides a wrapper with atomic operations for any type that\natomic() noexcept = default;\nconstexpr atomic(BaseType) noexcept;\nbool atomic_is_lock_free(volatile const atomic<BaseType>*) noexcept;\nbool atomic_is_lock_free(const atomic<BaseType>*) noexcept;\nvoid atomic_init(volatile atomic<BaseType>*, void*) noexcept;\nBaseType atomic_exchange(volatile atomic<BaseType>*, memory_order)\nBaseType atomic_exchange(atomic<BaseType>*, memory_order) noexcept;\nvolatile atomic<BaseType>*, memory_order) noexcept;\natomic<BaseType>*, memory_order) noexcept;\nvoid atomic_store(volatile atomic<BaseType>*, BaseType) noexcept;\nvoid atomic_store(atomic<BaseType>*, BaseType) noexcept;\nvolatile atomic<BaseType>*, BaseType, memory_order) noexcept;\natomic<BaseType>*, BaseType, memory_order) noexcept;\nBaseType atomic_load(volatile const atomic<BaseType>*) noexcept;\nvolatile const atomic<BaseType>*, memory_order) noexcept;\nconst atomic<BaseType>*, memory_order) noexcept;\nvolatile atomic<BaseType>*,BaseType * old_value,\nvolatile atomic<BaseType>*,BaseType * old_value,\nvolatile atomic<BaseType>*,BaseType * old_value,BaseType new_value)\natomic<BaseType>*,BaseType * old_value,BaseType new_value) noexcept;\nvolatile atomic<BaseType>*,BaseType * old_value,\nConstructs an instance of std::atomic with a default-initialized value.\natomic() noexcept;\nConstructs a new std::atomic object with a default-initialized value.\nNonatomically stores the supplied value in an instance of std::atomic<BaseType>.\nvoid atomic_init(atomic<BaseType> volatile* p, BaseType v) noexcept;\nConstructs an instance of std::atomic with the supplied BaseType value.\nConstructs a new std::atomic object with a value of b.\nSTD::ATOMIC CONVERSION ASSIGNMENT OPERATOR \nSTD::ATOMIC::IS_LOCK_FREE MEMBER FUNCTION \nSTD::ATOMIC_IS_LOCK_FREE NONMEMBER FUNCTION \nbool atomic_is_lock_free(volatile const atomic<BaseType>* p) noexcept;\nbool atomic_is_lock_free(const atomic<BaseType>* p) noexcept;\nSTD::ATOMIC::IS_ALWAYS_LOCK_FREE STATIC DATA MEMBER \nAtomically loads the current value of the std::atomic instance.\nAtomically loads the value stored in *this.\nThis is an atomic load operation for the memory location comprising\nSTD::ATOMIC_LOAD NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance.\nBaseType atomic_load(volatile const atomic<BaseType>* p) noexcept;\nBaseType atomic_load(const atomic<BaseType>* p) noexcept;\nSTD::ATOMIC_LOAD_EXPLICIT NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance.\nvolatile const atomic<BaseType>* p, memory_order order) noexcept;\nconst atomic<BaseType>* p, memory_order order) noexcept;\nSTD::ATOMIC::OPERATOR BASETYPE CONVERSION OPERATOR \nSTD::ATOMIC::STORE MEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nAtomically stores new_value in *this.\nThis is an atomic store operation for the memory location comprising\nSTD::ATOMIC_STORE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nvoid atomic_store(volatile atomic<BaseType>* p, BaseType new_value)\nvoid atomic_store(atomic<BaseType>* p, BaseType new_value) noexcept;\nSTD::ATOMIC_STORE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nvolatile atomic<BaseType>* p, BaseType new_value, memory_order order)\natomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nSTD::ATOMIC::EXCHANGE MEMBER FUNCTION \nAtomically stores a new value and reads the old one.\nSTD::ATOMIC_EXCHANGE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nBaseType atomic_exchange(volatile atomic<BaseType>* p, BaseType new_value)\nBaseType atomic_exchange(atomic<BaseType>* p, BaseType new_value) noexcept;\nSTD::ATOMIC_EXCHANGE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nvolatile atomic<BaseType>* p, BaseType new_value, memory_order order)\natomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nmemory_order order = std::memory_order_seq_cst) volatile noexcept;\notherwise, it’s an atomic load operation for the memory location comprising\nvolatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\natomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nvolatile atomic<BaseType>* p,BaseType * old_value,\natomic<BaseType>* p,BaseType * old_value,\nmemory_order order = std::memory_order_seq_cst) volatile noexcept;\notherwise, it’s an atomic load operation for the memory location comprising\nvolatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\natomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nvolatile atomic<BaseType>* p,BaseType * old_value,\natomic<BaseType>* p,BaseType * old_value,\nSpecializations of the std::atomic template \nSpecializations of the std::atomic class template are provided for the integral types\nstd::atomic<bool>\nstd::atomic<char>\nstd::atomic<short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nand std::atomic<T*> for all types T.\nD.3.10 std::atomic<integral-type> specializations \nThe std::atomic<integral-type> specializations of the std::atomic class template\nstd::atomic<char>\nstd::atomic<short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\natomic() noexcept = default;\nconstexpr atomic(integral-type) noexcept;\nvoid store(integral-type,memory_order = memory_order_seq_cst) noexcept;\nintegral-type load(memory_order = memory_order_seq_cst) const noexcept;\nbool atomic_is_lock_free(volatile const atomic<integral-type>*) noexcept;\nbool atomic_is_lock_free(const atomic<integral-type>*) noexcept;\nvoid atomic_init(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_init(atomic<integral-type>*,integral-type) noexcept;\nvolatile atomic<integral-type>*,integral-type) noexcept;\natomic<integral-type>*,integral-type) noexcept;\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\natomic<integral-type>*,integral-type, memory_order) noexcept;\nvoid atomic_store(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_store(atomic<integral-type>*,integral-type) noexcept;\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\natomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_load(volatile const atomic<integral-type>*) noexcept;\nintegral-type atomic_load(const atomic<integral-type>*) noexcept;\nvolatile const atomic<integral-type>*,memory_order) noexcept;\nconst atomic<integral-type>*,memory_order) noexcept;\nvolatile atomic<integral-type>*,\natomic<integral-type>*,\nvolatile atomic<integral-type>*,\natomic<integral-type>*,\nvolatile atomic<integral-type>*,\natomic<integral-type>*,\nvolatile atomic<integral-type>*,\natomic<integral-type>*,\nintegral-type atomic_fetch_add(\nvolatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_add(\natomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_add_explicit(\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_add_explicit(\natomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub(\nvolatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub(\natomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub_explicit(\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub_explicit(\natomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and(\nvolatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and(\natomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and_explicit(\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and_explicit(\natomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or(\nvolatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or(\natomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or_explicit(\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or_explicit(\natomic<integral-type>*,integral-type, memory_order) noexcept;\nvolatile atomic<integral-type>*,integral-type) noexcept;\natomic<integral-type>*,integral-type) noexcept;\nvolatile atomic<integral-type>*,integral-type, memory_order) noexcept;\natomic<integral-type>*,integral-type, memory_order) noexcept;\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_ADD MEMBER FUNCTION \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_add(\nvolatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_add(\natomic<integral-type>* p, integral-type i) noexcept;\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\natomic<integral-type>* p, integral-type i, memory_order order)\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_SUB MEMBER FUNCTION \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_sub(\nvolatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_sub(\natomic<integral-type>* p, integral-type i) noexcept;\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_sub_explicit(\nintegral-type atomic_fetch_sub_explicit(\natomic<integral-type>* p, integral-type i, memory_order order)\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_AND MEMBER FUNCTION \nSTD::ATOMIC_FETCH_AND NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_and(\nvolatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_and(\natomic<integral-type>* p, integral-type i) noexcept;\nSTD::ATOMIC_FETCH_AND_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_and_explicit(\nintegral-type atomic_fetch_and_explicit(\natomic<integral-type>* p, integral-type i, memory_order order)\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_OR MEMBER FUNCTION \nSTD::ATOMIC_FETCH_OR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_or(\nvolatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_or(\natomic<integral-type>* p, integral-type i) noexcept;\nSTD::ATOMIC_FETCH_OR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nintegral-type atomic_fetch_or_explicit(\nintegral-type atomic_fetch_or_explicit(\natomic<integral-type>* p, integral-type i, memory_order order)\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_XOR MEMBER FUNCTION \nSTD::ATOMIC_FETCH_XOR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nvolatile atomic<integral-type>* p, integral-type i) noexcept;\natomic<integral-type>* p, integral-type i) noexcept;\nSTD::ATOMIC_FETCH_XOR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\natomic<integral-type>* p, integral-type i, memory_order order)\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ PREINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the new value.\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ POSTINCREMENT OPERATOR \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- PREDECREMENT OPERATOR \nAtomically decrements the value stored in *this and returns the new value.\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- POSTDECREMENT OPERATOR \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR \nAtomically adds the supplied value to the value stored in *this and returns the new\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR&= COMPOUND ASSIGNMENT OPERATOR \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR|= COMPOUND ASSIGNMENT OPERATOR \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR^= COMPOUND ASSIGNMENT OPERATOR \natomic() noexcept = default;\nbool atomic_is_lock_free(volatile const atomic<T*>*) noexcept;\nT* atomic_exchange(volatile atomic<T*>*, T*) noexcept;\nT* atomic_exchange_explicit(volatile atomic<T*>*, T*, memory_order)\nT* atomic_exchange_explicit(atomic<T*>*, T*, memory_order) noexcept;\nvoid atomic_store(volatile atomic<T*>*, T*) noexcept;\nvoid atomic_store(atomic<T*>*, T*) noexcept;\nvoid atomic_store_explicit(volatile atomic<T*>*, T*, memory_order)\nvoid atomic_store_explicit(atomic<T*>*, T*, memory_order) noexcept;\nT* atomic_load(volatile const atomic<T*>*) noexcept;\nT* atomic_load_explicit(volatile const atomic<T*>*, memory_order) noexcept;\nT* atomic_load_explicit(const atomic<T*>*, memory_order) noexcept;\nvolatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nvolatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nvolatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\natomic<T*>*,T* * old_value,T* new_value) noexcept;\nvolatile atomic<T*>*,T* * old_value, T* new_value,\nT* atomic_fetch_add(volatile atomic<T*>*, ptrdiff_t) noexcept;\nvolatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\natomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_sub(volatile atomic<T*>*, ptrdiff_t) noexcept;\nvolatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\natomic<T*>*, ptrdiff_t, memory_order) noexcept;\nSTD::ATOMIC<T*>::FETCH_ADD MEMBER FUNCTION \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nT* atomic_fetch_add(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nvolatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\natomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nSTD::ATOMIC<T*>::FETCH_SUB MEMBER FUNCTION \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nT* atomic_fetch_sub(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nvolatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\natomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nSTD::ATOMIC<T*>::OPERATOR++ PREINCREMENT OPERATOR \nSTD::ATOMIC<T*>::OPERATOR++ POSTINCREMENT OPERATOR \nSTD::ATOMIC<T*>::OPERATOR-- PREDECREMENT OPERATOR \nSTD::ATOMIC<T*>::OPERATOR-- POSTDECREMENT OPERATOR ",
      "keywords": [
        "NONMEMBER FUNCTION Atomically",
        "ATOMIC",
        "MEMBER FUNCTION Atomically",
        "order",
        "FUNCTION Atomically",
        "std",
        "volatile atomic",
        "noexcept",
        "integral-type atomic",
        "EXPLICIT NONMEMBER FUNCTION",
        "memory",
        "NONMEMBER FUNCTION",
        "integral-type",
        "bool atomic",
        "FUNCTION Atomically loads"
      ],
      "concepts": [
        "std",
        "operations",
        "operation",
        "operator",
        "type",
        "value",
        "description",
        "descriptions",
        "volatile",
        "define"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.882,
          "base_score": 0.732,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.619,
          "base_score": 0.619,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.595,
          "base_score": 0.595,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "integral type",
          "integral",
          "atomic",
          "basetype",
          "noexcept"
        ],
        "semantic": [],
        "merged": [
          "integral type",
          "integral",
          "atomic",
          "basetype",
          "noexcept"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.440075876089445,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856718+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 490-512)",
      "start_page": 490,
      "end_page": 512,
      "summary": "std::future class template \nThe std::future class template provides a means of waiting for an asynchronous\nOnly one std::future instance references any\nInstances of std::future are MoveConstructible and MoveAssignable but not\nSTD::FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::future object without an associated asynchronous result.\nConstructs a new std::future instance.\nSTD::FUTURE MOVE CONSTRUCTOR \nConstructs one std::future object from another, transferring ownership of the asyn-\nchronous result associated with the other std::future object to the newly con-\nMove-constructs a new std::future instance from other.\nstructor is associated with the newly constructed std::future object.\nno associated asynchronous result.\nSTD::FUTURE MOVE ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with the one std::future\nTransfers ownership of an asynchronous state between std::future instances.\nother has no associated asynchronous result.\nSTD::FUTURE DESTRUCTOR \nDestroys an std::future object.\nSTD::FUTURE::SHARE MEMBER FUNCTION \nConstructs a new std::shared_future instance and transfers ownership of the asyn-\nchronous result associated with *this to this newly constructed std::shared_future\nAs-if shared_future<ResultType>(std::move(*this)).\nThe asynchronous result associated with *this prior to the invocation of share()\n(if any) is associated with the newly constructed std::shared_future instance.\nSTD::FUTURE::VALID MEMBER FUNCTION \nChecks if an std::future instance is associated with an asynchronous result.\nSTD::FUTURE::WAIT MEMBER FUNCTION \nOtherwise, waits until the asynchronous result associated with an instance of\nstd::future is ready.\nand stores the returned value or thrown exception as the asynchronous result.\nSTD::FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nstd::future_status::deferred if the asynchronous result associated with *this\nstarted execution, std::future_status::ready if the asynchronous result associ-\nSTD::FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nstd::future_status::deferred if the asynchronous result associated with *this\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\nblocked, only that if the function returns std::future_status::timeout,\nSTD::FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nassociated with an instance of std::future is ready, and then returns the stored value\nstd::shared_future class template \nThe std::shared_future class template provides a means of waiting for an asynchro-\nnous result from another thread, in conjunction with the std::promise and std::\npackaged_task class templates and the std::async function template, which can be\nMultiple std::shared_future instances can\nInstances of std::shared_future are CopyConstructible and CopyAssignable.\nYou can also move-construct a std::shared_future from a std::future with the\nAccesses to a given instance of std::shared_future aren’t synchronized.\ntherefore not safe for multiple threads to access the same std::shared_future\nSTD::SHARED_FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::shared_future object without an associated asynchronous result.\nConstructs a new std::shared_future instance.\nSTD::SHARED_FUTURE MOVE CONSTRUCTOR \nConstructs one std::shared_future object from another, transferring ownership of\nthe asynchronous result associated with the other std::shared_future object to the\nConstructs a new std::shared_future instance.\nstructor is associated with the newly constructed std::shared_future object.\nhas no associated asynchronous result.\nSTD::SHARED_FUTURE MOVE-FROM-STD::FUTURE CONSTRUCTOR \nConstructs an std::shared_future object from astd::future, transferring owner-\nship of the asynchronous result associated with the std::future object to the newly\nconstructed std::shared_future.\nshared_future(std::future<ResultType>&& other) noexcept;\nConstructs a new std::shared_future instance.\nstructor is associated with the newly constructed std::shared_future object.\nhas no associated asynchronous result.\nSTD::SHARED_FUTURE COPY CONSTRUCTOR \nConstructs one std::shared_future object from another, so that both the source and\nthe copy refer to the asynchronous result associated with the source std::shared_\nConstructs a new std::shared_future instance.\nstructor is associated with the newly constructed std::shared_future object and\nSTD::SHARED_FUTURE DESTRUCTOR \nDestroys an std::shared_future object.\nthe last std::shared_future instance associated with that asynchronous result,\nSTD::SHARED_FUTURE::VALID MEMBER FUNCTION \nChecks if an std::shared_future instance is associated with an asynchronous result.\nSTD::SHARED_FUTURE::WAIT MEMBER FUNCTION \nOtherwise, waits until the asynchronous result associated with an instance of\nstd::shared_future is ready.\nCalls to get() and wait() from multiple threads on std::shared_future instances\nand stores the returned value or thrown exception as the asynchronous result.\nSTD::SHARED_FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nstd::future_status::deferred if the asynchronous result associated with *this\nstarted execution, std::future_status::ready if the asynchronous result associ-\nSTD::SHARED_FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nstd::future_status::deferred if the asynchronous result associated with *this\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\nblocked, only that if the function returns std::future_status::timeout,\nSTD::SHARED_FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nassociated with an instance of std::shared_future is ready, and then returns the\nCalls to get() and wait() from multiple threads on std::shared_future instances\nand stores the returned value or thrown exception as the asynchronous result.\nstd::packaged_task class template \nThe std::packaged_task class template packages a function or other callable object\nso that when the function is invoked through the std::packaged_task instance, the\nstd::future.\nstd::future<ResultType> get_future();\nConstructs an std::packaged_task instance with no associated task or asynchro-\nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT \nConstructs an std::packaged_task object with an associated task and asynchronous\nConstructs an std::packaged_task instance with an associated asynchronous\nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT WITH AN ALLOCATOR\nConstructs an std::packaged_task object with an associated task and asynchronous\nConstructs an std::packaged_task instance with an associated asynchronous\nthe asynchronous result and task associated with the other std::packaged_task\nThe asynchronous result and task associated with other prior to the invocation of\nthe constructor is associated with the newly constructed std::packaged_task\nother has no associated asynchronous result.\nTransfers ownership of the asynchronous result associated with one std::packaged_\nTransfers ownership of the asynchronous result and task associated with other to\n*this, and discards any prior asynchronous result, as-if std::packaged_task(other)\nThe asynchronous result and task associated with other prior to the invocation of\nExchanges ownership of the asynchronous results associated with two std::packaged\nExchanges ownership of the asynchronous results and tasks associated with other\nThe asynchronous result and task associated with other prior to the invocation of\nThe asynchronous result and task associated\nSTD::PACKAGED_TASK::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this.\nstd::future<ResultType> get_future();\n*this has an associated asynchronous result.\nAn std::future instance for the asynchronous result associated with *this.\nthis asynchronous result through a prior call to get_future().\nAssociates an std::packaged_task instance with a new asynchronous result for the\nAs-if *this=packaged_task(std::move(f)), where f is the stored task associated\nSTD::PACKAGED_TASK::VALID MEMBER FUNCTION \nChecks whether *this has an associated task and asynchronous result.\ntrue if *this has an associated task and asynchronous result, false otherwise.\nInvokes the task associated with an std::packaged_task instance, and stores the return\nvalue or exception in the associated asynchronous result.\nreturns normally, stores the return value in the asynchronous result associated with\nThe asynchronous result associated with *this is ready with a stored value or excep-\nfuture<ResultType>::get() or std::shared_future<ResultType>::get(), which\nSTD::PACKAGED_TASK::MAKE_READY_AT_THREAD_EXIT MEMBER FUNCTION \nInvokes the task associated with an std::packaged_task instance, and stores the\nreturn value or exception in the associated asynchronous result without making the\nassociated asynchronous result ready until thread exit.\nreturns normally, stores the return value in the asynchronous result associated with\nThe asynchronous result associated with *this has a stored value or exception but\nAn exception of type std::future_error with an error code of\nstd::future_errc::no_state if *this has no associated asynchronous state.\nexit() synchronizes-with a call to std::future<ResultType>::get() or std::shared\nThe std::promise class template provides a means of setting an asynchronous result,\nwhich may be retrieved from another thread through an instance of std::future.\nA std::future associated with the asynchronous result of a particular std::promise\nstd::future<ResultType> get_future();\nConstructs an std::promise instance with an associated asynchronous result of\nfor the associated asynchronous result.\nConstructs an std::promise instance with an associated asynchronous result of\nasynchronous result associated with the other std::promise object to the newly con-\nstructor is associated with the newly constructed std::promise object.\nno associated asynchronous result.\nTransfers ownership of the asynchronous result associated with one std::promise\nmade ready with an exception of type std::future_error and an error code of\nstd::future_errc::broken_promise.\nExchanges ownership of the asynchronous results associated with two std::promise\nhave a stored value or exception, that result becomes ready with an std::future_\nerror exception with an error code of std::future_errc::broken_promise.\nSTD::PROMISE::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this.\nstd::future<ResultType> get_future();\n*this has an associated asynchronous result.\nAn std::future instance for the asynchronous result associated with *this.\nthis asynchronous result through a prior call to get_future().\nSTD::PROMISE::SET_VALUE MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this.\nStores r in the asynchronous result associated with *this if ResultType isn’t void.\nThe asynchronous result associated with *this is ready with a stored value.\ncall to set_value() happens-before a call to std::future<ResultType>::get() or\nstd::shared_future<ResultType>::get(), which retrieves the value stored.\nSTD::PROMISE::SET_VALUE_AT_THREAD_EXIT MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this without making that\nStores r in the asynchronous result associated with *this if ResultType isn’t void.\nThe asynchronous result associated with *this has a stored value but isn’t ready\npens-before a call to std::future<ResultType>::get() or std::shared_future\nSTD::PROMISE::SET_EXCEPTION MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this.\nThe asynchronous result associated with *this is ready with a stored exception.\nsuccessful call to set_exception() happens-before a call to std::future<Result-\nType>::get() or std::shared_future<ResultType>::get(), which retrieves the\nSTD::PROMISE::SET_EXCEPTION_AT_THREAD_EXIT MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this without making\nThe asynchronous result associated with *this has a stored exception but isn’t\nhappens-before a call to std::future<ResultType>::get() or std::shared_\nA call to std::async returns a std::future\nThe returned std::future will become ready when this thread is complete and will\nThe destructor of the last future object associated with the asynchronous state of\nthe returned std::future blocks until the future is ready.\nstd::future as a deferred function call.\nxyz...) will be returned from a call to get() on that std::future.\nstd::shared_future instance that references the same associated state as the\nstd:: future object returned from the std::async call.\nstd::future_error when the effects can’t be achieved, or any exception thrown",
      "keywords": [
        "std",
        "asynchronous result",
        "future",
        "result",
        "asynchronous",
        "MEMBER FUNCTION",
        "task",
        "function",
        "shared",
        "promise",
        "packaged",
        "thread",
        "exception",
        "deferred function",
        "type std"
      ],
      "concepts": [
        "std",
        "function",
        "functions",
        "template",
        "results",
        "promise",
        "returns",
        "void",
        "throws",
        "effects"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 13,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "",
          "score": 0.841,
          "base_score": 0.691,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 15,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 39,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 6,
          "title": "",
          "score": 0.531,
          "base_score": 0.531,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "asynchronous",
          "asynchronous result",
          "associated",
          "result",
          "std"
        ],
        "semantic": [],
        "merged": [
          "asynchronous",
          "asynchronous result",
          "associated",
          "result",
          "std"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36658881661657156,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856772+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 513-528)",
      "start_page": 513,
      "end_page": 528,
      "summary": "APPENDIX D\nC++ Thread Library reference\ntemplate<typename LockableType>\ntemplate<typename LockableType>\nclass shared_lock;\ntemplate<typename ...\ntemplate<typename LockableType1,typename...\ntemplate<typename LockableType1,typename...\ntemplate<typename Callable,typename...\nD.5.1\nThe std::mutex class provides a basic mutual exclusion and synchronization facility\nOnce a thread is\nallow other threads to acquire it.\nstd::mutex meets the Lockable requirements.\nmutex(mutex const&)=delete;\nmutex& operator=(mutex const&)=delete;\nconstexpr mutex() noexcept;\nconstexpr mutex() noexcept;\n<mutex> header\nAn exception of type std::system_error if an error occurs.\n*this is locked by the calling thread if the function returns true.\nAPPENDIX D\nC++ Thread Library reference\nThe function may fail to acquire the lock (and return false) even if\nD.5.2\nPrior to accessing the\nrecursive_mutex(recursive_mutex const&)=delete;\nrecursive_mutex& operator=(recursive_mutex const&)=delete;\nrecursive_mutex() noexcept;\n~recursive_mutex();\n<mutex> header\nrecursive_mutex() noexcept;\nAn exception of type std::system_error if unable to create a new std::recursive\n_mutex instance.\n~recursive_mutex();\n*this, the lock count is increased by one.\nAn exception of type std::system_error if an error occurs.\nAPPENDIX D\nC++ Thread Library reference\nreturns true.\nreturns true and the count of locks on *this held by the calling thread is\nthe function may fail to acquire the lock (and return false) even if no other\nD.5.3\nbasic mutual exclusion and synchronization facility provided by std::mutex.\nPrior to\nIf a lock is already held by\nacquired (whichever function was used to acquire it), it must be released, by calling\n<mutex> header\ntemplate<typename Rep,typename Period>\nstd::chrono::duration<Rep,Period> const& relative_time);\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\n_mutex instance.\nAPPENDIX D\nC++ Thread Library reference\nAn exception of type std::system_error if an error occurs.\n*this is locked by the calling thread if the function returns true.\nThe function may fail to acquire the lock (and return false) even if\ntemplate<typename Rep,typename Period>\nstd::chrono::duration<Rep,Period> const& relative_time);\nAttempts to acquire a lock on *this for the calling thread within the time specified\nIf relative_time.count() is zero or negative, the call will\neither the lock has been acquired or the time period specified by relative_time\n<mutex> header\n*this is locked by the calling thread if the function returns true.\nThe function may fail to acquire the lock (and return false) even if\nWhere possible, the elapsed time is determined\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time.\nabsolute_time.\n*this is locked by the calling thread if the function returns true.\nThe function may fail to acquire the lock (and return false) even if\nThere’s no guarantee as to how long\nAPPENDIX D\nC++ Thread Library reference\nD.5.4\ntop of the mutual exclusion and synchronization facility provided by std::recursive_\nPrior to accessing the data protected by the mutex, the mutex must be locked by\ncalling lock(), try_lock(), try_lock_for(), or try_lock_until().\nbeen acquired (whichever function was used to acquire it), it must be released by call-\nAll of these locks must be released by a corresponding call\ntemplate<typename Rep,typename Period>\nstd::chrono::duration<Rep,Period> const& relative_time);\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\n<mutex> header\nAn exception of type std::system_error if unable to create a new std::recursive\n*this, the lock count is increased by one.\nAn exception of type std::system_error if an error occurs.\nthread.\nAPPENDIX D\nC++ Thread Library reference\n*this is locked by the calling thread if the function returns true.\nreturns true and the count of locks on *this held by the calling thread is\nthe function may fail to acquire the lock (and return false) even if no other\nthread.\ntemplate<typename Rep,typename Period>\nstd::chrono::duration<Rep,Period> const& relative_time);\nAttempts to acquire a lock on *this for the calling thread within the time specified\nIf relative_time.count() is zero or negative, the call will\neither the lock has been acquired or the time period specified by relative_time\n*this is locked by the calling thread if the function returns true.\nreturns true and the count of locks on *this held by the calling thread is\nthe function may fail to acquire the lock (and return false) even if no other\nWhere possible, the elapsed time is determined by a\n<mutex> header\nthread.\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time.\nabsolute_time.\n*this is locked by the calling thread if the function returns true.\nreturns true and the count of locks on *this held by the calling thread is\nthe function may fail to acquire the lock (and return false) even if no other\nThere’s no guarantee as to how long the calling\nthread.\nAPPENDIX D\nC++ Thread Library reference\nD.5.5\nto hold a shared lock.\nmust be locked with an exclusive lock by calling lock() or try_lock().\nOnce a thread is\nallow other threads to acquire it.\nThreads that only want to read the protected data\nshared_mutex& operator=(shared_mutex const&)=delete;\n<mutex> header\nrent thread.\nAPPENDIX D\nC++ Thread Library reference\nThe function may fail to acquire the lock (and return false) even if\nthread.\nIf any threads are\nthread.\n<mutex> header\nThe function may fail to acquire the lock (and return false) even if\nD.5.6\nOnce a thread is\nallow other threads to acquire it.\nThreads that only want to read the protected data",
      "keywords": [
        "lock",
        "mutex",
        "LOCK MEMBER FUNCTION",
        "std",
        "Thread",
        "calling thread",
        "MEMBER FUNCTION Attempts",
        "MEMBER FUNCTION",
        "shared lock",
        "mutex object",
        "shared",
        "MEMBER FUNCTION Acquires",
        "LOCK MEMBER",
        "FUNCTION",
        "MEMBER FUNCTION Releases"
      ],
      "concepts": [
        "lock",
        "std",
        "thread",
        "effects",
        "unlock",
        "preconditions",
        "template",
        "facility",
        "void",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.968,
          "base_score": 0.818,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.942,
          "base_score": 0.792,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 44,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.866,
          "base_score": 0.716,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 9,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "calling thread",
          "typename",
          "calling",
          "acquire",
          "returns true"
        ],
        "semantic": [],
        "merged": [
          "calling thread",
          "typename",
          "calling",
          "acquire",
          "returns true"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5368458004259792,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856832+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 529-537)",
      "start_page": 529,
      "end_page": 537,
      "summary": "APPENDIX D\nC++ Thread Library reference\nlock, that thread will wait.\nshared_timed_mutex(shared_timed_mutex const&)=delete;\nshared_timed_mutex& operator=(shared_timed_mutex const&)=delete;\nshared_timed_mutex() noexcept;\n~shared_timed_mutex();\nbool try_lock();\nbool try_lock_for(\nbool try_lock_until(\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nbool try_lock_shared();\nbool try_lock_shared_for(\nbool try_lock_shared_until(\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nshared_timed_mutex() noexcept;\n~shared_timed_mutex();\nthread.\nThe calling thread must not hold a lock on *this.\ncurrent thread.\nbool try_lock();\nThe calling thread must not hold a lock on *this.\nAPPENDIX D\nC++ Thread Library reference\ncurrent thread.\nbool try_lock_for(\nThe calling thread must not hold a lock on *this.\nIf relative_time.count() is zero or negative,\nThe thread may be blocked for longer\nWhere possible, the elapsed time is determined\ncurrent thread.\nbool try_lock_until(\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nThe calling thread must not hold a lock on *this.\nIf absolute_time<=Clock::now() on entry, the\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked.\nrent thread.\nIf any threads are\n*this is not locked by the calling thread.\nThe calling thread must not hold a lock on *this.\n*this is locked by the calling thread with a shared lock.\nAPPENDIX D\nC++ Thread Library reference\nrent thread.\nbool try_lock_shared();\nThe calling thread must not hold a lock on *this.\nrent thread.\nbool try_lock_for(\nThe calling thread must not hold a lock on *this.\nIf relative_time.count() is zero or negative, the call\nThe thread may be blocked for longer\nWhere possible, the elapsed time is determined\nrent thread.\nbool try_lock_until(\nstd::chrono::time_point<Clock,Duration> const& absolute_time);\nThe calling thread must not hold a lock on *this.\nIf absolute_time<=Clock::now() on entry, the call\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked.\nthread.\nAPPENDIX D\nC++ Thread Library reference\n*this is not locked by the calling thread.\nD.5.7\nreturn, or by throwing an exception.\nThe calling thread must own a lock on m.\nD.5.8\nmultiple mutexes at once.\nthe mutexes are unlocked when the block is left, whether that’s by running off the\nend, by the use of a control flow statement such as break or return, or by throwing an\nAPPENDIX D\nC++ Thread Library reference\nof the mutexes, in order to avoid deadlock, using the same algorithm as the\nthey must already be locked by the calling thread.\nThe calling thread must own a lock on the mutexes in m.\nD.5.9\nThe std::unique_lock class template provides a more general lock ownership wrap-",
      "keywords": [
        "lock",
        "shared lock",
        "std",
        "shared",
        "Thread",
        "calling thread",
        "mutex",
        "timed",
        "MEMBER FUNCTION Attempts",
        "SHARED MEMBER FUNCTION",
        "MEMBER FUNCTION",
        "calling",
        "exclusive lock",
        "FUNCTION",
        "LOCK MEMBER FUNCTION"
      ],
      "concepts": [
        "std",
        "lock",
        "thread",
        "effects",
        "declaration",
        "preconditions",
        "template",
        "returns",
        "function",
        "throws"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.968,
          "base_score": 0.818,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "",
          "score": 0.889,
          "base_score": 0.739,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 44,
          "title": "",
          "score": 0.854,
          "base_score": 0.704,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 11,
          "title": "",
          "score": 0.842,
          "base_score": 0.692,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "calling thread",
          "thread",
          "calling",
          "thread hold",
          "hold lock"
        ],
        "semantic": [],
        "merged": [
          "calling thread",
          "thread",
          "calling",
          "thread hold",
          "hold lock"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5368180596110599,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856903+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 538-555)",
      "start_page": 538,
      "end_page": 555,
      "summary": "unique_lock<std::mutex>, and all instantiations of std::unique_lock are suitable\nMutex type meets the TimedLockable requirements, then std::unique_lock<Mutex>\nexplicit unique_lock(mutex_type& m);\nunique_lock(mutex_type& m, adopt_lock_t);\nunique_lock(mutex_type& m, defer_lock_t) noexcept;\nunique_lock(mutex_type& m, try_to_lock_t);\nConstructs an std::unique_lock instance with no associated mutex.\nConstructs an std::unique_lock instance that has no associated mutex.\nthis->mutex()==NULL, this->owns_lock()==false.\nConstructs an std::unique_lock instance that locks the supplied mutex.\nexplicit unique_lock(mutex_type& m);\nConstructs an std::unique_lock instance that references the supplied mutex.\nthis->owns_lock()==true, this->mutex()==&m.\nConstructs an std::unique_lock instance that owns the lock on the supplied mutex.\nunique_lock(mutex_type& m,std::adopt_lock_t);\nConstructs an std::unique_lock instance that references the supplied mutex and\nthis->owns_lock()==true, this->mutex()==&m.\nunique_lock(mutex_type& m,std::defer_lock_t) noexcept;\nConstructs an std::unique_lock instance that references the supplied mutex.\nthis->owns_lock()==false, this->mutex()==&m.\nConstructs an std::unique_lock instance associated with the supplied mutex and\nunique_lock(mutex_type& m,std::try_to_lock_t);\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nConstructs an std::unique_lock instance that references the supplied mutex.\nthis->owns_lock() returns the result of the m.try_lock() call, this->mutex()==&m.\nConstructs an std::unique_lock instance associated with the supplied mutex and\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nConstructs an std::unique_lock instance that references the supplied mutex.\nthis->owns_lock() returns the result of the m.try_lock_for() call, this->mutex()\nConstructs an std::unique_lock instance associated with the supplied mutex and\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nConstructs an std::unique_lock instance that references the supplied mutex.\nSTD::UNIQUE_LOCK MOVE-CONSTRUCTOR \nstd::unique_lock object.\nIf other owned a lock on a mutex prior\nstd::unique_lock object.\nFor a newly constructed std::unique_lock object, x, x.mutex() is equal to the value\nother.mutex()==NULL, other.owns_lock()==false.\nDestroys an std::unique_lock instance and unlocks the corresponding mutex if it’s\nIf this->owns_lock()returns true, calls this->mutex()->unlock().\nstd::unique_lock objects.\nExchanges ownership of their associated mutex locks between two std::unique_lock\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->lock().\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->try_lock().\nSTD::UNIQUE_LOCK::UNLOCK MEMBER FUNCTION \nThe Mutex type used to instantiate std::unique_lock must meet the TimedLock-\nthis->mutex()!=NULL, this->owns_lock()==false.\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nthis->mutex()!=NULL, this->owns_lock()==false.\nSTD::UNIQUE_LOCK::OWNS_LOCK MEMBER FUNCTION \nSTD::UNIQUE_LOCK::MUTEX MEMBER FUNCTION \nthis->mutex()==NULL, this->owns_lock()==false.\nD.5.10 std::shared_lock class template \nThe std::shared_lock class template provides an equivalent to std::unique_lock,\nEvery std::shared_lock<Mutex> meets the Lockable requirements.\nstd::shared_lock<Mutex> also meets the TimedLockable requirements.\nexplicit shared_lock(mutex_type& m);\nshared_lock(mutex_type& m, adopt_lock_t);\nshared_lock(mutex_type& m, defer_lock_t) noexcept;\nshared_lock(mutex_type& m, try_to_lock_t);\nshared_lock(\nshared_lock(\n~shared_lock();\nSTD::SHARED_LOCK DEFAULT CONSTRUCTOR \nConstructs an std::shared_lock instance with no associated mutex.\nConstructs an std::shared_lock instance that has no associated mutex.\nthis->mutex()==NULL, this->owns_lock()==false.\nSTD::SHARED_LOCK LOCKING CONSTRUCTOR \nexplicit shared_lock(mutex_type& m);\nConstructs an std::shared_lock instance that references the supplied mutex.\nthis->owns_lock()==true, this->mutex()==&m.\nConstructs an std::shared_lock instance that owns the lock on the supplied mutex.\nshared_lock(mutex_type& m,std::adopt_lock_t);\nConstructs an std::shared_lock instance that references the supplied mutex and\nthis->owns_lock()==true, this->mutex()==&m.\nshared_lock(mutex_type& m,std::defer_lock_t) noexcept;\nConstructs an std::shared_lock instance that references the supplied mutex.\nthis->owns_lock()==false, this->mutex()==&m.\nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex.\nshared_lock(mutex_type& m,std::try_to_lock_t);\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nConstructs an std::shared_lock instance that references the supplied mutex.\nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A DURATION TIMEOUT \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex.\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nConstructs an std::shared_lock instance that references the supplied mutex.\nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex.\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nConstructs an std::shared_lock instance that references the supplied mutex.\nSTD::SHARED_LOCK MOVE-CONSTRUCTOR \ncreated std::shared_lock object.\nConstructs an std::shared_lock instance.\nated std::shared_lock object.\nFor a newly-constructed std::shared_lock object, x, x.mutex() is equal to the value\nstd::shared_lock objects are not CopyConstructible, so there’s no\nSTD::SHARED_LOCK MOVE-ASSIGNMENT OPERATOR \nstd::shared_lock object.\nowned a shared lock on a mutex prior to the assignment, that lock is now owned by\nother.mutex()==NULL, other.owns_lock()==false.\nSTD::SHARED_LOCK DESTRUCTOR \nDestroys an std::shared_lock instance and unlocks the corresponding mutex if it’s\nIf this->owns_lock()returns true, calls this->mutex()->unlock_shared().\nSTD::SHARED_LOCK::SWAP MEMBER FUNCTION \nstd::shared_lock objects.\nSWAP NONMEMBER FUNCTION FOR STD::SHARED_LOCK\nExchanges ownership of their associated mutex locks between two std::shared_lock\nSTD::SHARED_LOCK::LOCK MEMBER FUNCTION \nAcquires a shared lock on the mutex associated with *this.\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->lock_shared().\nAny exceptions thrown by this->mutex()->lock_shared().\nSTD::SHARED_LOCK::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this.\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->try_lock_shared().\ntrue if the call to this->mutex()->try_lock_shared() returned true, false\nAny exceptions thrown by this->mutex()->try_lock_shared().\nSTD::SHARED_LOCK::UNLOCK MEMBER FUNCTION \nReleases a shared lock on the mutex associated with *this.\nSTD::SHARED_LOCK::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->try_lock_shared_for(relative_time).\ntrue if the call to this->mutex()->try_lock_shared_for() returned true, false\nAny exceptions thrown by this->mutex()->try_lock_shared_for().\nSTD::SHARED_LOCK::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nthis->mutex()!=NULL, this->owns_lock()==false.\nCalls this->mutex()->try_lock_shared_until(absolute_time).\ntrue if the call to this->mutex()->try_lock_shared_until() returned true, false\nAny exceptions thrown by this->mutex()->try_lock_shared_until().\nSTD::SHARED_LOCK::OPERATOR BOOL MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex.\nSTD::SHARED_LOCK::OWNS_LOCK MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex.\ntrue if *this owns a shared lock on a mutex, false otherwise.\nSTD::SHARED_LOCK::MUTEX MEMBER FUNCTION \nSTD::SHARED_LOCK::RELEASE MEMBER FUNCTION \nthis->mutex()==NULL, this->owns_lock()==false.",
      "keywords": [
        "lock",
        "std",
        "mutex",
        "LOCK MEMBER FUNCTION",
        "shared lock",
        "shared",
        "unique",
        "lock instance",
        "MEMBER FUNCTION",
        "MEMBER FUNCTION Attempts",
        "LOCK MEMBER",
        "Constructs an std",
        "NULL",
        "supplied Mutex",
        "MUTEX MEMBER FUNCTION"
      ],
      "concepts": [
        "locking",
        "throwing",
        "function",
        "declaration",
        "requirements",
        "constructors",
        "effects",
        "template",
        "returns",
        "duration"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.942,
          "base_score": 0.792,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.889,
          "base_score": 0.739,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 9,
          "title": "",
          "score": 0.869,
          "base_score": 0.719,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 10,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 44,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared_lock",
          "mutex",
          "std shared_lock",
          "unique_lock",
          "owns_lock"
        ],
        "semantic": [],
        "merged": [
          "shared_lock",
          "mutex",
          "std shared_lock",
          "unique_lock",
          "owns_lock"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4631411173980891,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.856977+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 556-563)",
      "start_page": 556,
      "end_page": 563,
      "summary": "D.5.11 std::lock function template \nD.5.12 std::try_lock function template \nThe std::try_lock function template allows you to try to lock a set of lockable\nstd::ratio class template \nThe std::ratio class template provides a mechanism for compile-time arithmetic\nratio<2,3>), or fifteen forty-thirds (std::ratio<15,43>).\nstd::ratio_add template alias\nThe std::ratio_add template alias provides a mechanism for adding two std::ratio\nusing ratio_add = std::ratio<see below>;\nR1 and R2 must be instantiations of the std::ratio class template.\nratio_add<R1,R2> is defined as an alias for an instantiation of std::ratio that\nIn the absence of arithmetic overflow, std::ratio_add<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den + R2::num *\nstd::ratio_subtract template alias\nThe std::ratio_subtract template alias provides a mechanism for subtracting two\nstd::ratio values at compile time, using rational arithmetic.\nusing ratio_subtract = std::ratio<see below>;\nR1 and R2 must be instantiations of the std::ratio class template.\nratio_subtract<R1,R2> is defined as an alias for an instantiation of std::ratio\nIn the absence of arithmetic overflow, std::ratio\n_subtract<R1,R2> shall have the same num and den values as std::ratio<R1::num\nstd::ratio_multiply template alias\nThe std::ratio_multiply template alias provides a mechanism for multiplying two\nstd::ratio values at compile time, using rational arithmetic.\nusing ratio_multiply = std::ratio<see below>;\nR1 and R2 must be instantiations of the std::ratio class template.\nratio_multiply<R1,R2> is defined as an alias for an instantiation of std::ratio\nIn the absence of arithmetic overflow, std::ratio_multiply\n<R1,R2> shall have the same num and den values as std::ratio<R1::num * R2::num,\nstd::ratio_divide template alias\nThe std::ratio_divide template alias provides a mechanism for dividing two std::\nusing ratio_divide = std::ratio<see below>;\nR1 and R2 must be instantiations of the std::ratio class template.\nratio_divide<R1,R2> is defined as an alias for an instantiation of std::ratio that\nIn the absence of arithmetic overflow, std::ratio_divide<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den, R1::den *\nstd::ratio_equal class template \nThe std::ratio_equal class template provides a mechanism for comparing two std::\nR1 and R2 must be instantiations of the std::ratio class template.\nstd::ratio_equal<std::ratio<1,3>, std::ratio<1,6> >::value == false\nstd::ratio_equal<std::ratio<1,3>, std::ratio<2,3> >::value == false\nstd::ratio_not_equal class template \nThe std::ratio_not_equal class template provides a mechanism for comparing two\nstd::ratio values for inequality at compile time, using rational arithmetic.\npublic std::integral_constant<bool,!ratio_equal<R1,R2>::value>\nR1 and R2 must be instantiations of the std::ratio class template.\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<2,6> >::value == false\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<1,3> >::value == false\nstd::ratio_less class template \nThe std::ratio_less class template provides a mechanism for comparing two std::\nR1 and R2 must be instantiations of the std::ratio class template.\nstd::ratio_less<R1,R2> derives from std::integral_constant<bool, value >,\nstd::ratio_less<\nstd::ratio<999999999,1000000000>, \nstd::ratio<1000000001,1000000000> >::value == true\nstd::ratio_less<\nstd::ratio<1000000001,1000000000>,\nstd::ratio<999999999,1000000000> >::value == false\nstd::ratio_greater class template \nThe std::ratio_greater class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic.\npublic std::integral_constant<bool,ratio_less<R2,R1>::value>\nR1 and R2 must be instantiations of the std::ratio class template.\nD.6.10 std::ratio_less_equal class template \nThe std::ratio_less_equal class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic.\npublic std::integral_constant<bool,!ratio_less<R2,R1>::value>\nR1 and R2 must be instantiations of the std::ratio class template.\nD.6.11 std::ratio_greater_equal class template \nThe std::ratio_greater_equal class template provides a mechanism for comparing\ntwo std::ratio values at compile time, using rational arithmetic.\npublic std::integral_constant<bool,!ratio_less<R1,R2>::value>",
      "keywords": [
        "std",
        "ratio",
        "ratio class template",
        "template",
        "class template",
        "ratio class",
        "typedef ratio",
        "den",
        "equal class template",
        "lock",
        "num",
        "equal",
        "call",
        "flag",
        "Class definition template"
      ],
      "concepts": [
        "ratio",
        "effects",
        "flag",
        "den",
        "locking",
        "thread",
        "arithmetic",
        "values",
        "header",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 47,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.519,
          "base_score": 0.519,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 9,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 43,
          "title": "",
          "score": 0.488,
          "base_score": 0.488,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.461,
          "base_score": 0.461,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ratio",
          "std ratio",
          "r1",
          "r2",
          "std"
        ],
        "semantic": [],
        "merged": [
          "ratio",
          "std ratio",
          "r1",
          "r2",
          "std"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35547013493881474,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.857029+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 564-572)",
      "start_page": 564,
      "end_page": 572,
      "summary": "std::thread class \nThe std::thread class is used to manage a thread of execution.\n~thread();\nSTD::THREAD::ID CLASS \nAn instance of std::thread::id identifies a particular thread of execution.\nThe std::thread::id value that identifies a particular thread of execution shall be\ndistinct from the value of a default-constructed std::thread::id instance and\nThe std::thread::id values for particular threads aren’t predictable and may\nstd::thread::id is CopyConstructible and CopyAssignable, so instances of\nstd::thread::id may be freely copied and assigned.\nSTD::THREAD::ID DEFAULT CONSTRUCTOR \nConstructs an std::thread::id object that doesn’t represent any thread of execution.\nConstructs an std::thread::id instance that has the singular not any thread value.\nAll default-constructed std::thread::id instances store the same value.\nSTD::THREAD::ID EQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent the same thread\nbool operator==(std::thread::id lhs,std::thread::id rhs) noexcept;\nSTD::THREAD::ID INEQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent different threads\nbool operator!=(std::thread::id lhs,std::thread::id rhs) noexcept;\nSTD::THREAD::ID LESS-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\nbool operator<(std::thread::id lhs,std::thread::id rhs) noexcept;\nthread ID values.\nthread::id instance compares less than any std::thread::id instance that\nIf two instances of std::thread::id are\nAny set of distinct std::thread::id val-\nSTD::THREAD::ID LESS-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\nbool operator<=(std::thread::id lhs,std::thread::id rhs) noexcept;\nSTD::THREAD::ID GREATER-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\nbool operator>(std::thread::id lhs,std::thread::id rhs) noexcept;\nSTD::THREAD::ID GREATER-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\nbool operator>=(std::thread::id lhs,std::thread::id rhs) noexcept;\nSTD::THREAD::ID STREAM INSERTION OPERATOR \nWrites a string representation of the std::thread::id value into the specified stream.\nInserts a string representation of the std::thread::id value into the specified stream.\nstd::thread::id that compare equal have the same representation, and\nSTD::THREAD::NATIVE_HANDLE_TYPE TYPEDEF \nReturns a value of type native_handle_type that represents the thread of execution\nConstructs an std::thread object without an associated thread of execution.\nConstructs an std::thread instance that has no associated thread of execution.\nFor a newly constructed std::thread object, x, x.get_id()==id().\nSTD::THREAD CONSTRUCTOR \nConstructs an std::thread object associated with a new thread of execution.\nConstructs an std::thread instance and associates it with a newly created thread of\nFor a newly constructed std::thread object, x, x.get_id()!=id().\nSTD::THREAD MOVE-CONSTRUCTOR \nTransfers ownership of a thread of execution from one std::thread object to a newly\ncreated std::thread object.\nConstructs an std::thread instance.\nIf other has an associated thread of execution\nobject has no associated thread of execution.\nFor a newly constructed std::thread object, x, x.get_id() is equal to the value of\nSTD::THREAD DESTRUCTOR \nDestroys an std::thread object.\n~thread();\nSTD::THREAD MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a thread of execution from one std::thread object to another\nstd::thread object.\nno associated thread of execution.\nExchanges ownership of their associated threads of execution between two std::\nthread of execution is now associated with other.\nExchanges ownership of their associated threads of execution between two std::\nSTD::THREAD::JOINABLE MEMBER FUNCTION \nlonger has an associated std::thread object.\nSTD::THREAD::GET_ID MEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the thread of execution asso-\nIf *this has an associated thread of execution, returns an instance of std::\nstd::thread::id.\nSTD::THIS_THREAD::GET_ID NONMEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the current thread of execution.\nAn instance of std::thread::id that identifies the current thread.",
      "keywords": [
        "thread",
        "std",
        "thread of execution",
        "execution",
        "thread object",
        "COMPARISON OPERATOR Compares",
        "operator",
        "noexcept",
        "Returns",
        "Declaration thread",
        "Thread Library",
        "MEMBER FUNCTION",
        "COMPARISON OPERATOR",
        "bool operator",
        "OPERATOR Compares"
      ],
      "concepts": [
        "threads",
        "declaration",
        "execution",
        "executions",
        "operator",
        "operate",
        "effects",
        "functions",
        "function",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 5,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 6,
          "title": "",
          "score": 0.708,
          "base_score": 0.708,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 30,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread id",
          "id",
          "std thread",
          "thread",
          "std"
        ],
        "semantic": [],
        "merged": [
          "thread id",
          "id",
          "std thread",
          "thread",
          "std"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4715647916971506,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.857087+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 573-581)",
      "start_page": 573,
      "end_page": 581,
      "summary": "of 328–329\n327–328\n24–27\natomic operations 128–142\nfree functions for 140–142\nmemory ordering for 146–164\norderings 149–150\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\n138–140\nstd::atomic class templates 439–449\nstd::atomic specializations 450–466\nstd::atomic_flag classes 436–439\natomics 168–169\nefficiency in Thread Library 12–13\nfrom 331–338\nC++ Thread Library 384, 401–550\n<thread> header 541–549\nstd::chrono::duration class templates 401–410\nclear( ) function 133–134\nclocks 93–94\noperations 99–123\npassing 104–108\nwhen_any 115–118\nconcurrency 126–127\n81–93\n282–299\nperformance 266–270\n271–277\n279–280\nconcurrency 280–282\n284–289\nthreads 252–260\ndesigning for 174–176\noverview of 2–7\n6–7\nuses for 7–10\n108–110\n318–321\nthread-safe queues using 179–182\nvariables 76–81\nconstexpr functions 363–369\ntemplates and 368–369\nreferences 226–232\nvisits 336–338\nparallelism of 8–9\nsharing between threads 36–71\nproblems with 37–40\n40–64\ndata parallelism 8–9\n266–270\nby testing 344–346\n347–350\n350–352\ncode 352–353\n259–260\n252–260\ndone_waiting( ) function 295–296\ndurations 94–96\n146–164\natomics 168–169\n164–165\nwaiting for 73–81\n81–93\ntasks 82–84\nwaiting for multiple threads 90–93\nin parallel algorithms 271–277\nwith std::async( ) 276–277\nexceptions 89–90\n328–329\nfetch_add( ) function 131, 137–138, 234\nfetch_sub( ) function 137–138, 165\nqueues using 183–194\ndata 185–190\nthread-safe queues using 183–194\ndata 185–190\n99–104\ndefaulted 360–363\ndeleted 358–360\n140–142\nstd::future class templates 467–472\nwaiting for more than one 114–115\ntasks 82–84\nwaiting for multiple threads 90–93\nget_future( ) function 85–86, 92\nhandle( ) function 107–108, 388, 405\nthread-safe stacks 48–50\ninterrupt( ) function 316–317, 319\n325–326\nthreads 315–316, 318–326\nlaunching threads 316–318\nis_lock_free( ) function 128–129\njoin( ) function 20–22, 341\nlambda functions 369–374\nload( ) function 131–132\ndesigning 173–194, 204\nfor concurrency 174–176\n199–204\nlocks 194–196\ndata 185–190\nvariables 179–182\nthread-safe stacks using locks 176–179\nlock-free concurrent data structures 205–250\n206–207\nstacks 232–236\ncounting 226–232\n218–226\nstructures 214–218\nlocks 236–248\nlocks 210–213\nhelp other threads 249–250\n248–249\n248–249\nrecursive 70–71\n53–54\nthread-safe queues using 183–194\nthread-safe queues using 179–182\nthread-safe stacks using 176–179\n194–199\n238–244\nthreads 244–248\nlocations of 125–127\nmemory models 125–128\nconcurrency 126–127\nobjects 125–127\n146–164\norderings 149–150\nframeworks for 384–401\nconcurrency and 10–13\nefficiency in C++ Thread Library 12–13\ncode 352–353\nstd::call_once function templates 534–535\nstd::lock_guard class templates 512–513\nstd::mutex classes 490–492\n498–501\nstd::scoped_lock class templates 513–514\nstd::shared_lock class templates 523–532\n505–512\nstd::unique_lock class templates 514–523\nmutexes 41–42\n59–60\n62–64\n44–50\ndata 42–43\n61–62\nmy_thread function 18–19\nnamespace this_thread 549–550\n218–226\n226–232\norderings 149–150\nnotify_one( ) function 75–76, 81, 349\noverview of 125–127\nordering 169–172\nordering with atomics 168–169\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\nsynchronizing 142–172\n164–165\n146–164\n164–165\nfunction 221–222\nof threads, transferring 27–31\nparallel algorithms 327–338\n276–277",
      "keywords": [
        "FUNCTION",
        "STD",
        "data",
        "atomic",
        "Thread",
        "data structures",
        "memory",
        "Concurrency",
        "function templates",
        "Thread Library",
        "functions",
        "lock function templates",
        "class templates",
        "thread-safe",
        "templates"
      ],
      "concepts": [
        "function",
        "functions",
        "functional",
        "thread",
        "std",
        "classes",
        "data",
        "locks",
        "operation",
        "operator"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 53,
          "title": "",
          "score": 0.865,
          "base_score": 0.715,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.852,
          "base_score": 0.702,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "",
          "score": 0.837,
          "base_score": 0.687,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 43,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.699,
          "base_score": 0.699,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "templates",
          "class templates",
          "164",
          "137",
          "194"
        ],
        "semantic": [],
        "merged": [
          "templates",
          "class templates",
          "164",
          "137",
          "194"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4728272566782239,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.857978+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 582-589)",
      "start_page": 582,
      "end_page": 589,
      "summary": "327–328\nof std::find 284–289\nof std::for_each 282\nof std::partial_sum 290, 293–299\n6–7\n344–345\nstd::condition_variable::wait member function \n238–244, 345\n244–248\n225–226\n68–70\n59–60\nstd::async function templates 488\nstd::async( ) function 82, 171, 254, 276–277\nstd::atomic 134–137\nstd::atomic_ is_lock_free( ) function 142\nstd::atomic::exchange member function 446\nstd::atomic::is_lock_free member function 443\nstd::atomic::load member function 444\nstd::atomic::store member function 445\nstd::atomic<> primary class template 138–140\nstd::atomic_exchange nonmember function 446\nstd::atomic_fetch_add nonmember function 455, \nstd::atomic_fetch_and nonmember function 457\nstd::atomic_fetch_or nonmember function 458\nstd::atomic_fetch_sub nonmember function 456, \nstd::atomic_fetch_xor nonmember function 459\noperations on 132–134\nstd::atomic_flag::clear member function 438\nstd::atomic_flag::test_and_set member \nstd::atomic_init nonmember function 442\nstd::atomic<integral-type>::operator--\nstd::atomic<integral-type>::operator&= \nstd::atomic<integral-type>::operator++\nstd::atomic_load nonmember function 444\nstd::atomic_signal_fence function 436\nstd::atomic_store nonmember function 445\nstd::atomic<T*> 137–138\nstd::atomic<t*> partial specialization 461–463\nstd::atomic<t*>::fetch_add member function 463\nstd::atomic<t*>::fetch_sub member function 464\nstd::atomic<t*>::operator--\nstd::atomic<t*>::operator++\nstd::atomic<t*>::operator+= compound assign-\nstd::atomic<t*>::operator-= compound assignment \nstd::atomic_thread_fence function 435\nstd::bind( ) function 26, 309\nstd::call_once function template 534–535\nstd::chrono::duration::count member \nstd::chrono::duration::operator--\nstd::chrono::duration::operator*= compound \nstd::chrono::duration::operator/= compound \nstd::chrono::duration::operator%= compound \nstd::chrono::duration::operator++\nstd::chrono::duration::operator+= compound \nstd::chrono::duration::operator-= compound \nstd::chrono::steady_clock class 94, 401, 414–416\nstd::chrono::system_clock class 94, 413–414\nfunction 420–421\nfunction 422–423\nfunction 426–427\nfunction 428–429\nstd::execution::par 327–328\nstd::execution::parallel_policy 328, 330–331\nstd::execution::sequenced_policy 328–330\n227–228\nstd::experimental::barrier 120–121, 172, 295\nstd::experimental::flex_barrier 120–123, 172\nstd::experimental::latch 118–119, 171\nstd::find 284–289\nstd::future::get member function 471\nstd::future::share member function 469\nstd::future::valid member function 470\nstd::future::wait member function 470\nstd::future::wait_for member function 470\nstd::future::wait_until member function 471\nstd::kill_dependency( ) function 163\nstd::launch::async function 84, 103\nstd::launch::deferred function 103\nstd::lock function template 51, 54, 533\nstd::move( ) function 26, 309, 360\nstd::mutex 170\nstd::mutex::lock member function 491\nstd::mutex::try_lock member function 491\nstd::mutex::unlock member function 492\nstd::packaged_task::operator( ) function call \nstd::packaged_task::reset member function 481\nstd::packaged_task::swap member function 480\nstd::packaged_task::valid member function 481\nstd::partial_sum 290–299\nstd::partition( ) function 101\nstd::promise 87–89\nstd::promise::get_future member function 485\nstd::promise::set_value member function 486\nstd::promise::set_value_at_thread_exit member \nstd::promise::swap member function 485\nstd::recursive_mutex::lock member function 493\nstd::recursive_mutex::try_lock member \nstd::recursive_timed_mutex::lock member \nstd::recursive_timed_mutex::try_lock member \nstd::recursive_timed_mutex::try_lock_for member \nstd::shared_future::get member function 476\nstd::shared_future::valid member function 475\nstd::shared_future::wait member function 475\nstd::shared_future::wait_for member \nstd::shared_future::wait_until member \nstd::shared_lock\nstd::shared_lock::lock member function 529\nstd::shared_lock::mutex member function 532\nstd::shared_lock::operator bool member \nstd::shared_lock::owns_lock member \nstd::shared_lock::release member function 532\nstd::shared_lock::swap member function 528\nstd::shared_lock::try_lock member function 529\nstd::shared_lock::try_lock_for member \nstd::shared_lock::try_lock_until member \nstd::shared_lock::unlock member function 530\nstd::shared_mutex\nstd::shared_mutex::lock member function 503\nstd::shared_mutex::lock_shared member \nstd::shared_mutex::try_lock member \nstd::shared_mutex::try_lock_shared member \nstd::shared_mutex::unlock member function 504\nstd::shared_mutex::unlock_shared member \nstd::shared_timed_mutex::lock member \nstd::shared_timed_mutex::lock_shared member \nstd::shared_timed_mutex::try_lock member \nstd::shared_timed_mutex::try_lock_for member \nstd::shared_timed_mutex::try_lock_until member \nstd::shared_timed_mutex::unlock member \nstd::sort( ) function 101\nstd::terminate( ) function 22, 27, 272, 379\nstd::this_thread::get_id nonmember function 549\nstd::this_thread::yield nonmember function 549\nstd::thread\nstd::thread::detach member function 548\nstd::thread::get_id member function 549\nfunction 31–32, 257, 261, 282, 301, 549\nstd::thread::join member function 548\nstd::thread::joinable member function 547\nstd::thread::native_handle member \nstd::thread::swap member function 547\nstd::timed_mutex::lock member function 495\nstd::timed_mutex::try_lock member function 496\nstd::timed_mutex::try_lock_for member \nstd::timed_mutex::try_lock_until member \nstd::timed_mutex::unlock member function 497\nstd::try_lock function template 533\nstd::unique_lock::mutex member function 523\nstd::unique_lock::operator bool member \nstd::unique_lock::release member function 523\nstd::unique_lock::swap member function 519\nstd::unique_lock::unlock member function 521\nstd::unique_ptr 26–27\nsubmit( ) function 303–305, 307, 310\nfor std::shared_lock 529\nfor std::threads 547\n164–165\noperations 142–172\n164–165\nthreads 259–260\nconstexpr functions and 368–369\ntest_and_set( ) function 129, 133–134, 207\n352–353\n350–352\n310–311\nstd::thread class 541–549\n269–270\noperations 267–269\n259–260\n325–326\n321–323\n244–248\npassing arguments to thread function 24–27",
      "keywords": [
        "member function",
        "lock member function",
        "std",
        "function",
        "member",
        "shared member function",
        "nonmember function",
        "static member function",
        "member function overloads",
        "wait member function",
        "lock member",
        "swap member function",
        "explicit nonmember function",
        "mutex member function",
        "lock"
      ],
      "concepts": [
        "std",
        "function",
        "functions",
        "operations",
        "operator",
        "member",
        "classes",
        "nonmember",
        "threads",
        "lock"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 45,
          "title": "",
          "score": 0.882,
          "base_score": 0.732,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.865,
          "base_score": 0.715,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.618,
          "base_score": 0.618,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 43,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "std",
          "member",
          "member std",
          "function",
          "member function"
        ],
        "semantic": [],
        "merged": [
          "std",
          "member",
          "member std",
          "function",
          "member function"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45213778818597056,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:30.858042+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 590-592)",
      "start_page": 590,
      "end_page": 592,
      "summary": "252–260\nwaiting to complete 20–22\nthread-safe lists 199–204\nthread-safe lookup tables 194–199\nusing condition variables 179–182\n183–194\ndata 185–190\ndata 185–190\nusing locks 179–182\n238–244\nthreads 244–248\nusing locks 176–179\ndurations 94–96\nfunctions accepting timeouts 98–99\ntimeouts 98–99\n82–84\n135–136\n377–378\nlocal 371–374\nthread-local 379–380\nwait( ) function 75, 81, 108, 295–296, \nfor conditions 73–81\nvariables 76–81\nwith condition variables 74–76\nfor events 73–81\n115–118\nfor more than one future 114–115\nfor multiple threads 90–93\n82–84\nfor tasks 307–309\nfor tasks submitted to thread pools 303–307\ninterrupting condition variable wait 318–321\ndurations 94–96\nfunctions accepting timeouts 98–99\nwhen_any function 115–118\nC++ Concurrency in Action, Second Edition is the deﬁ nitive guide \nmanning.com/books/c-plus-plus-concurrency-in-action-second-edition\nC++ Concurrency IN ACTION\nof C++ concurrency ",
      "keywords": [
        "function",
        "fine-grained condition variables",
        "dividing data recursively",
        "condition variables",
        "functions accepting timeouts",
        "condition variable wait",
        "time points",
        "separating data",
        "trivial copy-assignment operator",
        "thread-safe lookup tables",
        "making queues lock-free",
        "dividing work",
        "thread-specific setup code",
        "point timeouts std",
        "enabling concurrency"
      ],
      "concepts": [
        "functions",
        "function",
        "functional",
        "threads",
        "concurrency",
        "concurrent",
        "tasks",
        "waiting",
        "locks",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "",
          "score": 0.75,
          "base_score": 0.75,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "",
          "score": 0.65,
          "base_score": 0.65,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 11,
          "title": "",
          "score": 0.612,
          "base_score": 0.612,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 22,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 52,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "timeouts",
          "81",
          "condition",
          "accepting",
          "98 99"
        ],
        "semantic": [],
        "merged": [
          "timeouts",
          "81",
          "condition",
          "accepting",
          "98 99"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4805149616136652,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:30.858100+00:00"
      }
    }
  ],
  "total_chapters": 54,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "C++ Concurrency in Action_metadata.json",
    "enrichment_date": "2025-12-17T23:01:30.869676+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 5382.159294000303,
    "total_similar_chapters": 270
  }
}