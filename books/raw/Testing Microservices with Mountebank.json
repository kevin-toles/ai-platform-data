{
  "metadata": {
    "title": "Testing Microservices with Mountebank",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 380,
    "conversion_date": "2025-12-25T18:19:44.497154",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Testing Microservices with Mountebank.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Testing microservices",
      "start_page": 27,
      "end_page": 54,
      "detection_method": "regex_chapter_title",
      "content": "In fact, Amazon did this in their early years, and for good reason. The company had a monolithic codebase they\n\ncalled “Obidos” that looked quite similar to figure 1.1. Configuring the database that way makes it easy to join\n\ndifferent domain entities, such as customers and orders to show a customer’s order history or orders and\n\nproducts to show product details on an order. Having everything in one database also means you can rely on\n\ntransactions to maintain consistency, which makes it easy to update a product’s inventory when you ship an\n\norder, for example.\n\nFigure 1.1. A monolithic application handles view, business, and persistence logic for multiple domains.\n\nThis setup also makes testing—the focus of this book— easier. Most of the tests can be in process, and, assuming\n\nyou are using dependency injection, you can test pieces in isolation using mocking libraries. Black-box testing\n\nthe application only requires you to coordinate the application deployment with the database schema\n\nversion. Test data management comes down to loading the database with a set of sample test data. You can\n\neasily solve all of these problems.\n\n1.1.1. The path toward microservices\n\nIt’s useful to follow the history of Amazon.com to understand what compels organizations to move away\n\nfrom monolithic applications. As the site became more\n\npopular, it also became bigger, and Amazon had to hire more engineers to develop it. The problems started when\n\nthe development organization was large enough that multiple teams had to develop different parts of Obidos\n\n(figure 1.2).\n\nFigure 1.2. Scaling a monolith means multiple teams have to work in the same codebase.\n\nThe breaking point came in 2001, as the company\n\nstruggled to evolve pieces of the application because of the coupling between teams. By CEO mandate, the\n\nengineering organization split Obidos into a series of services and organized its teams around them.\n\n[2]\n\nAfter\n\nthe transformation, each team was able to change the code relevant to the domain of their service with much\n\nhigher confidence that they weren’t breaking other teams’ code—no other team shared their codebase. Amazon now has tremendous ability to develop different\n\nparts of the website experience independently, but the transformation has required a change of paradigm.\n\nWhereas Obidos used to be solely responsible for\n\nrendering the site, nowadays a single web page at Amazon.com can generate over a hundred service calls\n\n(figure 1.3).\n\n2\n\nSee https://queue.acm.org/detail.cfm?id=1142065 for details.\n\nFigure 1.3. Services use diﬀerent databases for diﬀerent domains.\n\nThe upshot is that each service can focus on doing one\n\nthing well and is much easier to understand in isolation. The downside is that such an architecture pushes the\n\ncomplexity that used to exist inside the application into the operational and runtime environment. Showing both\n\ncustomer details and order details on a single screen changes from being a simple database join to\n\norchestrating multiple service calls and combining the data in the application code. Although each service is\n\nsimple in isolation, the system as a whole is harder to understand.\n\nNetflix was one of the first companies of its size to migrate its core business to Amazon’s cloud services,\n\nwhich significantly influenced the way the company thought about services. Once again, the need to scale its\n\ndevelopment efforts is what drove this change. Adrian Cockcroft, formerly the Netflix lead cloud architect,\n\n[3]\n\nnoted two opposing tensions. increased by a factor of 1,000 in recent years as\n\nFirst, demands on IT had\n\ntechnology moved from managing payroll and a few enterprise services to becoming the core differentiator\n\nfor digitally native companies. Second, as the number of engineers increased, the communication and coordination overhead of activities like troubleshooting a\n\nbroken build became a significant slowdown to delivering software.\n\n3\n\nSee https://www.infoq.com/presentations/migration-cloud-native.\n\nNetflix experienced this slowdown once the organization grew to about 100 engineers, with multiple teams\n\ncompeting in the same codebase. Like Amazon, the company solved the problem by breaking the monolithic\n\napplication into services, and it made a conscious effort to make each service do only one thing. This\n\narchitectural approach—what we now call microservices —supported development teams working independently\n\nof each other, allowing the company to scale its development organization.\n\nAlthough scaling development efforts is the primary\n\nforce leading us toward microservices, it has a welcome side effect: it’s much easier to release a small service than\n\nit is to release a large application. If you can release services independently, you can release features to the\n\nmarket much more quickly. By removing the need for release coordination between teams, microservices\n\nprovide an architectural solution for what would normally have to be solved manually. Customers are the\n\nbeneficiaries. Both Amazon and Netflix are known for the ability to rapidly innovate in the marketplace but to\n\ndo so required rethinking how they organized to deliver software and how they tested software.\n\n1.1.2. Microservices and organizational structure\n\nBefore we get into how mountebank makes testing microservices easier, you need to understand how\n\nmicroservices require a different testing mentality. It all starts with team organization and ends with a full-frontal\n\nassault on traditional QA approaches that gate releases through coordinated end-to-end testing.\n\nMicroservices require you to rethink traditional\n\norganizational structures. Silos exist in any large organization, but some silos are anathema to the goal of\n\nmicroservices, which is to allow independent releases of small services with minimal coordination. Traditional\n\norganizations use certain silos as “gates” that validate a deployment before it’s released. Each gate acts as a point\n\nof coordination. Coordination is one way to gain confidence in a release, but it’s slow and couples teams\n\ntogether. Microservices work best when you silo the organization by business capability and allow the\n\ndevelopment team to own the release of its code autonomously.\n\nA useful metaphor to explain the concept is to imagine\n\nyour IT organization as a highway. When you need to increase throughput—the number of features released\n\nover a given timeframe—you do so by adding lanes to the highway. Having more lanes means you can support\n\nmore cars at the same time (figure 1.4). This is analogous to hiring more developers to do more work in parallel.\n\nYou also may want to be able to release a single feature more quickly. This is equivalent to raising the speed limit\n\non the highway, enabling a single car to get from point A to point B in less time. So far, so good.\n\nFigure 1.4. During normal traﬀic, the number of lanes and speed limit define throughput and velocity.\n\nOne thing will kill both throughput and velocity, no\n\nmatter how many lanes you have or what the speed limit is: congestion. Congestion also has an indirect cost that\n\nyou have almost certainly experienced if you live in a big city. Navigating through stop-and-go traffic is a soul-\n\ncrushing experience. It’s demotivating. It’s hard to get excited to get in your car and drive. Many large IT\n\norganizations that, with the best of intentions, create unintended congestion, suffer from a real motivational\n\ncost.\n\nThey create congestion in two ways. First, they overuse the highway, having too many cars given the space\n\navailable. Second, they add coordination that creates congestion. This second way is harder to eradicate.\n\nOne way to require coordination on the highway is to add\n\na toll gate (especially those old-timey ones, before an automated camera replaced the need to pay physical\n\nmoney to a real person). Another way is to have fewer\n\nupstream lanes than you have downstream, where “upstream” refers to the section of the highway closer to\n\nthe exit in highways, and closer to a production release in IT. Reducing the number of upstream lanes throttles\n\ntraffic by requiring merging multiple lanes into one (figure 1.5). Sometimes this reduction happens by design\n\nor because of road construction. Other times it happens because of an accident, which results from an\n\nunfortunate degree of coupling between two cars.\n\nFigure 1.5. Having fewer upstream lanes increases congestion.\n\nLike all models, the highway metaphor is imperfect, but\n\nit highlights some useful points. As you saw earlier in the Amazon and Netflix examples, microservices often\n\noriginate from an organization’s desire to increase feature throughput. A helpful side effect is that a smaller codebase has a higher speed limit, increasing velocity.\n\nBut both of these advantages are negated if you don’t change the organization to remove congestion.\n\nIn organizational terms, fixing overutilization is easy in principle, though it’s often politically challenging. You\n\ncan either hire more people (add more lanes to the highway) or reduce the amount of work in progress\n\n(throttle entry to the highway).\n\nThe other reasons for congestion are harder to fix. It’s common to see organizations with less upstream capacity\n\nthan downstream capacity. One toll gate example would be increasing coordination by requiring releases to go\n\nthrough a central release management team that throttles development throughput. Accidents are even\n\nmore common. Every time someone discovers a bug or the build breaks in a codebase that multiple teams share,\n\nyou have an accident requiring coordination, and both throughput and velocity suffer. Adrian Cockcroft cited\n\nthis exact reason for driving Netflix toward microservices.\n\nMicroservices provide a technical solution for reducing congestion caused by accidents. By not sharing the same\n\ncodebase, broken builds don’t affect multiple teams, effectively creating different lanes for different services. But toll gates are still a problem, and to fully unlock the\n\nthroughput and velocity advantages we hope to gain through microservices, we have to address organizational\n\ncomplexity. That comes in many forms (for example, from operations to database schema management), but\n\nthere’s one form of upstream congestion that’s particularly relevant to this book: our QA organization.\n\nMicroservices fundamentally challenge the way you test.\n\n1.2. THE PROBLEM WITH END-TO-END TESTING\n\nTraditionally, a central QA team could partner with a\n\ncentral release management team to coordinate a schedule of changes that needed to be deployed into\n\nproduction. They could arrange it such that only one change went through at a time, and that change could be\n\ntested against the production versions of its runtime dependencies before being released. Such an approach is perfectly reasonable up to a point. Beyond that—and this\n\nis often where organizations turn to microservices—it’s inappropriate.\n\nYou still need confidence that the entire system will work when you release part of it. Gaining that confidence\n\nthrough traditional end-to-end testing and coordinated releases couples all of the services together, moving the\n\ncoupling bottleneck from the development organization to the QA organization (figure 1.6).\n\nFigure 1.6. Centralized QA processes recouple releases together, causing a bottleneck.\n\nCoordinated integration testing between services recouples codebases that have been decoupled through\n\nservice decomposition, destroying the scaling and rapid delivery advantages of microservices. As soon as you\n\ncouple the releases of services together, you have reintroduced the communication and coordination\n\noverhead you were trying to avoid. It doesn’t matter how many services you have; when you have to release them at the same time, you have a monolith.\n\nThe only way to truly scale the technology organization is to decouple releases, so that it can deploy a service\n\nindependently of the service’s dependencies (figure 1.7). This requires a fundamental rethinking of the test\n\nstrategy for microservices. End-to-end testing doesn’t completely disappear, but relying on it as a gate to\n\nreleasing software becomes another sacrifice on the path toward microservices. The question remains: how do you\n\ngain the confidence you need in your changes before releasing them?\n\nFigure 1.7. Independent testing works to avoid release congestion.\n\n1.3. UNDERSTANDING SERVICE VIRTUALIZATION\n\nThe problem with dependencies is that you can’t depend on them.\n\nMichael Nygard, “Architecture Without an End State”\n\nAdditional problems exist besides coordination, as\n\nshown in figure 1.8. Running in a shared environment means tests may pass or fail for reasons that have\n\nnothing to do with either the service you are testing or the tests themselves. They could fail because of resource\n\ncontention with other teams who are touching the same data, overwhelming the server resources of a shared\n\nservice, or environmental instability. They could fail, or be nearly impossible to write to begin with, because of an\n\ninability to get consistent test data set up in all of the services.\n\nFigure 1.8. End-to-end testing introduces several problems of coordination.\n\nIt turns out that other industries have already solved this\n\nproblem. A car, for example, is made up of a multitude of components, each of which can be released to the market\n\nindependently of the car as a whole. By and large, nobody buys an alternator or a flywheel for anything\n\nother than to fix a car. All the same, it’s common for companies to manufacture and sell those parts\n\nseparately even though they have never tested them in your specific car.\n\nA car battery comes standard with negative and positive\n\nterminals. You can test the battery—outside the car—by using a voltmeter attached to those two terminals and\n\nverifying that the voltage is between 12.4 and 12.7 volts.\n\nWhen you start the car, the alternator is responsible for charging the battery, but you can verify the behavior of\n\nthe battery independently of the alternator by providing a current as input to the battery and measuring the\n\nvoltage. Such a test tells you that, if the alternator is behaving correctly, then the battery also behaves\n\ncorrectly. You can gain most of the confidence you need to verify the battery is working by using a fake alternator.\n\nService virtualization involves nothing more than using test doubles that operate over the wire and is analogous\n\nto how you test car batteries without a real alternator. You silo the runtime environment into the bits relevant\n\nto test a single service or application and fake the rest, assuming standard interfaces. In traditional mocking\n\nand stubbing libraries, you would stub out a dependency and inject that into your object’s constructor, allowing your tests to probe the object under test in isolation.\n\nWith service virtualization, you virtualize a service and configure the service under test to use the virtualized\n\nservice’s URL as a runtime dependency (figure 1.9). You can set up the virtual service with a specific set of canned\n\nresponses, allowing you to probe the service under test in isolation.\n\nFigure 1.9. Testing using service virtualization\n\nService virtualization lets you do black box testing of the\n\nservice while tightly controlling the runtime environment in which it operates. Although it falls short of the end-to-\n\nend confidence that integration tests give you, it does make testing much easier. If you need to test what your\n\nshopping cart will do if you try to submit the order when you are out of inventory, you don’t have to figure out how\n\nto change the inventory system to run your test. You can virtualize the inventory service and configure it to\n\nrespond with an out-of-inventory message. You can take full advantage of the reduced coupling that the narrow\n\nservice interface provides to dramatically reduce the amount of test setup required.\n\nDeterministic tests are tests that always pass or fail when\n\ngiven the same code to test. Nondeterminism is a tester’s worst enemy. Every time you try to “fix” a broken test by\n\nre-running it because it worked last time, the devil on your shoulder does a little dance while the angel on your\n\nother shoulder lets out a big sigh. Automated tests create a social contract for a team: when a test breaks, you fix\n\nthe code. When you allow flaky tests, you chip away at that social contract. All kinds of bad behavior might\n\noccur when teams lose confidence that their tests are giving them meaningful feedback, including completely\n\nignoring the output of a build.\n\nFor your tests to run deterministically, you need to\n\ncontrol what the virtual service returns. You can seed the response in several ways, depending on the type and\n\ncomplexity of the tests. What works for writing automated behavioral tests against your service and for\n\ntesting edge cases likely won’t work when running performance tests where you need to execute thousands\n\nof requests against the virtual service.\n\n1.3.1. Test-by-test setup using an API\n\nThe simplest approach is to mirror what mocking libraries do: directly collude with the mocked objects.\n\nThe unit testing community often speaks about the 3A pattern, which is to say that each test has three\n\ncomponents: arrange, act, and assert. First you set up the data needed for the test to run (arrange), then you\n\nexecute the system under test (act), and finally you assert that you got the expected response (figure 1.10). Service\n\nvirtualization can support this approach through an API that lets you configure the virtual service dynamically.\n\nFigure 1.10. Service virtualization supports a standard unit testing pattern.\n\nThis approach supports creating a laboratory environment for each test, in order to strictly control the\n\ninputs and dependencies of the service under test. However, it does make a couple of fundamental\n\nassumptions. First, it expects each test to start with a clean slate, which means that each test must remove the\n\ntest data it added to prevent that data from interfering with subsequent tests. Second, the approach doesn’t\n\nwork if multiple tests are run in parallel. Both of these assumptions fit nicely into automated behavioral tests, as\n\ntest runners typically ensure tests are run serially. As long as each developer runs their own virtual service, you\n\ncan avoid the resource contention that comes with concurrent test runs.\n\n1.3.2. Using a persistent data store\n\nCreating test data test-by-test doesn’t work well for manual testers, it doesn’t work if multiple testers are hitting the same virtual service, and it doesn’t work in\n\nsituations (like performance testing) where you need a large batch of data. To address these concerns, you can\n\nconfigure the virtual service to read the test data from a persistent store. With the test-by-test approach to test\n\ndata creation, all you have to do is tell the virtual service what the next response should look like. With a data\n\nstore, you will need some way of deciding which response to send based on something from the request.\n\nFor example, you might send back different responses based on identifiers in the request URL (figure 1.11).\n\nFigure 1.11. Using persistent test data from a data store\n\nThe downside of this approach is that the arrange step is\n\nremoved from the test, meaning that to understand what each test is trying to do, you need some information that\n\nit doesn’t directly specify. If you are testing what happens when you submit orders under various\n\nscenarios, for example, you’d have to know that order 123 should have appropriate inventory, whereas order\n\n234 should experience an out-of-inventory situation. The configuration that sets that up is in a data store instead\n\nof in the arrange section of your test.\n\n1.3.3. Record and replay\n\nOnce you have determined where to store the test data, the next question is how to create it. This is rarely an\n\nissue for automated behavioral tests because you would create the data specific to the testing scenario. But if you\n\nare using a persistent data store, creating the test data is often a significant challenge, especially when you want\n\nlarge quantities of realistic data. The solution is often to record interactions with the real dependency in a way\n\nthat allows you to play them back through a virtual service (figure 1.12).\n\nFigure 1.12. Capturing real traﬀic for later replay\n\nThe trick with recording responses is that you still have to specify some condition on the request that has to\n\nmatch before playing back the recorded response. You need to know that the order identifier in the URL is\n\nwhat’s used to separate the successful order submit response from the out-of-inventory response.\n\nService virtualization isn’t a silver bullet, and by itself it’s not enough to cover the confidence gap created by giving\n\nup end-to-end testing. But it’s a critical component of a modern test strategy in a distributed world, and we’ll\n\nexplore ways of closing that confidence gap in chapters 9 and 10, when we combine service virtualization with\n\nother techniques to create a continuous delivery pipeline.\n\nService virtualization isn’t just for microservices!\n\nAlthough the focus here is on microservices and the\n\nchanges in test strategy they require, service virtualization is a useful tool in many other contexts. A\n\ncommon use case is mobile development, where the mobile team needs to be able to develop independently of the team building the API. The need to compete in the\n\nmobile ecosystem has driven many organizations to change their integration approach to one based on\n\nHTTP-based APIs, and mobile developers can take advantage of that fact to virtualize the APIs as they\n\ndevelop the front-end code.\n\n1.4. INTRODUCING MOUNTEBANK\n\nMountebank means a charlatan. It comes from Italian words meaning, literally, to mount a bench, which\n\ncaptures the behavior of snake oil salesmen who duped uninformed consumers into forking over money for\n\nquack medicine. It’s a useful word for describing what mountebank the tool does, which is to conspire with\n\n[4]\n\nyour tests to dupe the system under test into believing that mountebank is a real runtime dependency.\n\n4\n\nSince its initial release, I have always preferred to lowercase the “m” in “mountebank” when writing about the tool. Largely this has to do with the way the documentation is written—in a personified voice from a snake oil salesman who claims false humility by, among other things, not capitalizing his name. Whatever the historical origins, it’s now a mildly\n\nunexpected stylistic twist. Sorry about that.\n\nMountebank is a service virtualization tool. It supports all of the service virtualization scenarios we have looked\n\nat: behavioral testing using the API test-by-test, using a persistent data store, and acting as a proxy for record\n\nand replay situations. Mountebank also supports the ability to pick a response based on certain criteria of the\n\nrequest and to select which request fields you want to differentiate the responses during the playback stage of\n\nrecord-playback. Most of the remainder of this book explores those scenarios and more to help you build a\n\nrobust test strategy for microservices, because a robust test strategy is the key to unlocking release\n\nindependence.\n\nMountebank is a standalone application that provides a REST API to create and configure virtual services, which\n\nare called imposters in the API (figure 1.13). Rather than configuring the service that you are testing to point to\n\nURLs of real services, you configure it to point to the imposters you create through mountebank.\n\nFigure 1.13. Configuring virtual services with a simple mountebank imposter\n\nEach imposter represents a socket that acts as the virtual\n\nservice and accepts connections from the real service you are testing. Spinning up and shutting down imposters is\n\na lightweight operation, so it’s common to use the arrange step of automated tests to create the imposter,\n\nthen shut it down in the cleanup stage of each test. Although we will use HTTP/S for most examples in this\n\nbook, mountebank supports other protocols, including binary messages over TCP, and more protocols are\n\nexpected soon.\n\nAs Larry Wall, the creator of the Perl scripting language, once said, the goal of a good tool is to make the easy\n\n[5]\n\nthings easy and the hard things possible. tries to accomplish this with a rich set of request-\n\nMountebank\n\nmatching and response-generation capabilities, balanced by as many defaults as reasonably possible. Figure 1.14\n\nshows how mountebank matches a request to a response.\n\n5\n\nMost developers who have had the misfortune of using many “enterprise” tools will realize\n\nthat this statement is far from the truism it may sound like.\n\nFigure 1.14. Matching a request to a response with mountebank\n\nNetwork protocols are complicated beasts. The first job\n\nof the imposter is to simplify a protocol-specific request\n\ninto a JSON structure so that you can match the request against a set of predicates. Each protocol gets its own\n\nrequest JSON structure; we will look at HTTP’s structure in the next chapter.\n\nYou configure each imposter with a list of stubs. A stub is nothing more than a collection of one or more responses\n\nand, optionally, a list of predicates. Predicates are defined in terms of request fields, and each one says\n\nsomething like “Request field X must equal 123.” No self- respecting mocking tool would leave users with a simple\n\nequals as the only comparison operator, and mountebank ups the game with special predicate\n\nextensions to make working with XML and JSON easier. Chapter 4 explores predicates in detail.\n\nMountebank passes the request to each stub in list order\n\nand picks the first one that matches all the predicates. If the request doesn’t match any of the stubs defined,\n\nmountebank returns a default response. Otherwise, it returns the first response for the stub, which brings us to\n\nhow responses are generated (figure 1.15).\n\nFigure 1.15. Response generation in mountebank using predicates and responses in stubs\n\nThe first thing that happens is that the selected response shifts to the back of the list. This allows you to cycle\n\nthrough the responses in order each time a request matches the stub’s predicates. Because mountebank\n\nmoves the request to the back instead of removing it, you never run out of responses—you can start reusing them.\n\nThis data structure is what the academics call a circular buffer, because a circle prefers to start over rather than\n\nend.\n\nThe response resolver box in figure 1.15 is a bit of a\n\nsimplification. Each response is responsible for generating a JSON structure representing the protocol-\n\nspecific response fields (like the HTTP status code), and you can generate those fields in different ways.\n\nMountebank has three different response types that take entirely different approaches to generating the JSON:\n\nAn is response type returns the provided JSON as-is, creating a\n\ncanned response. We explore canned responses in chapter 3.\n\nA proxy response type forwards the request on to a real\n\ndependency and converts its response into a JSON response\n\nstructure. You use proxies for record-playback functionality, and\n\nwe describe them in chapter 5.\n\nAn inject response type allows you to programmatically define the\n\nresponse JSON using JavaScript. Injection is how you can extend\n\nmountebank when its built-in capabilities don’t quite do what you\n\nneed, and we cover that in chapter 6.\n\nOnce the response is resolved, mountebank passes the JSON structure to behaviors for post-processing.\n\nBehaviors, which we discuss in chapter 7, include, among others:\n\nCopying values from the request into the response\n\nAdding latency to the response\n\nRepeating a response, rather than moving it to the back of the list\n\nUp to this point, mountebank has dealt only with JSON,\n\nand every operation (with the exception of forwarding a proxy request) has been protocol-agnostic. Once the\n\nresponse JSON is finalized, the imposter converts the JSON to a protocol-aware network response and sends it\n\nover the wire. Although we will spend much of our time in this book looking at HTTP requests and responses, all\n\nof the core capabilities of mountebank work with any supported network protocol (even binary ones), and in\n\nchapter 8, we will show some non-HTTP examples.\n\nTo keep simple things simple, nearly everything in\n\nmountebank is optional. That allows you to get started gently, which we will do in the next chapter.\n\n1.5. THE SERVICE VIRTUALIZATION TOOL ECOSYSTEM\n\nThis book is about two things: mountebank and how\n\nservice virtualization fits into your microservices test strategy. Although both topics are valuable, the second\n\none is much broader than mountebank.\n\nThe service virtualization ecosystem offers several quality tools, both open source and commercial.\n\nCommercial tooling is still quite popular in large enterprises. HP, CA, and Parasoft all offer commercial\n\nservice virtualization tools, and SmartBear took the (originally noncommercial) SoapUI and converted it into\n\npart of their commercial service virtualization toolkit. Many of the commercial tools are high quality and offer a\n\nricher set of capabilities than the open source tooling, such as broader protocol support, but in my experience,\n\nthey both devalue the developer experience and hinder true continuous delivery. (Chapter 9 offers a fuller\n\ncritique.) Of the open source tools, I believe that mountebank comes the closest to the full feature set of\n\ncommercial tools.\n\nThe open source tooling offers a rich set of options primarily aimed at virtualizing HTTP. WireMock is\n\nprobably the most popular alternative to mountebank. Whereas mountebank aims to be cross-platform by\n\nhaving its public API be REST over HTTP, WireMock\n\n(and many others) optimizes for a specific platform. Although this involves tradeoffs, WireMock is easier to\n\nget started with in a purely Java-based project, as you don’t have to worry about calling an HTTP API or any\n\ncomplicated wiring into the build process.\n\nMountebank has an ecosystem of language bindings and\n\nbuild plugins, but you will have to search for them, and they may not expose the full capabilities of the tool. (In\n\nthe next chapter, you will see an example using JavaScript to wrap the REST API, and chapter 8 has an\n\nexample using a pre-built C# language binding.) That said, mountebank has broader portability than\n\nWireMock.\n\nAnother popular example is Hoverfly, a newer Go-based service virtualization tool that baked in middleware as\n\npart of the toolchain, allowing a high degree of customization. Mountebank offers middleware in the form of the shellTransform behavior, which we look at in chapter 7. Moco and stubby4j are other popular\n\noptions that are Java-based, although stubby4j has been ported to multiple languages.\n\nAs you will see in part 3 of this book, service virtualization\n\nhelps in a number of scenarios, and one tool isn’t always right for every scenario. Many of the commercial tools\n\naim for centralized testing, including performance tests. Many of the open source tools aim for a friendly\n\ndeveloper experience when doing functional service tests as part of the development process. I believe\n\nmountebank is unique in the open source world in that it aspires to support the full spectrum of service\n\nvirtualization use cases, including performance testing (which we look at in chapter 10). That said, you won’t hurt\n\nmy feelings if you use another tool for certain types of testing, and I hope that this book helps you identify what\n\nyou need in the different types of tests to thrive in a microservices world.\n\nSUMMARY\n\nMicroservices represent an architectural approach that can\n\nincrease both delivery throughput and velocity.\n\nTo realize the full potential of microservices, you must release\n\nthem independently.\n\nTo gain release independence, you must also test independently.\n\nService virtualization allows independent black-box testing of services.\n\nMountebank is an open source service virtualization tool for testing microservices.\n\nChapter 2. Taking mountebank for a test drive\n\nThis chapter covers\n\nUnderstanding how mountebank virtualizes HTTP\n\nInstalling and running mountebank\n\nExploring mountebank on the command line\n\nUsing mountebank in an automated test\n\nIn trying to do for pet supplies what Amazon did for books, Pets.com became one of the most spectacular\n\nfailures of the dot-com bust that occurred around the turn of the millennium. On the surface, the company had\n\neverything it needed to be successful, including a brilliant marketing campaign that featured a famous\n\nsock puppet. Yet it flamed out from IPO to liquidation in under a year, becoming synonymous with the bursting of\n\nthe dot-com bubble in the process.\n\nBusiness-minded folk claim that Pets.com failed because no market existed for ordering pet supplies over the\n\ninternet. Or it failed because of the lack of a viable business plan...or maybe because the company sold\n\nproducts for less than it cost to buy and distribute them. But as technologists, we know better.\n\nPets.com made only two mistakes that mattered. They didn’t use microservices, and, more importantly, they didn’t use mountebank.\n\n[1]\n\nIn an era in which social\n\nmedia and meme generators have conspired to bring cat picture innovation to new heights, it’s clear that we need\n\ninternet-provided pet supplies now more than ever. The time to correct the technical mistakes of Pets.com is long\n\noverdue. In this chapter, we will get started on a microservices architecture for a modern pet supply\n\ncompany and show how you can use mountebank to maintain release independence between services.\n\n1\n\nI am, of course, joking. Neither microservices nor mountebank existed back then.\n\n2.1. SETTING UP THE EXAMPLE\n\nThough building an online pet supply site is a bit tongue- in-cheek, it will serve as a useful reference to get\n\ncomfortable with mountebank. As an e-commerce platform, it looks similar to the Amazon.com example\n\nyou saw in chapter 1.\n\nThe architecture shown in figure 2.1 is simplified, but it’s complex enough to work with. Each of the services on the\n\nright have its own set of runtime dependencies, but we will look at the architecture from the perspective of the\n\nwebsite team. One of the hallmarks of a good architecture is that, although you will need to understand\n\nsomething about your dependencies, you shouldn’t need to know anything about the other teams’ dependencies. I\n\nhave also introduced a façade layer that represents presentational APIs relevant to a specific channel. This is\n\na common pattern to aggregate and transform downstream service calls into a format optimized for the\n\nchannels (mobile, web, and so on).\n\nFigure 2.1. Your reference architecture for exploring mountebank",
      "page_number": 27
    },
    {
      "number": 2,
      "title": "Taking mountebank for a test drive",
      "start_page": 55,
      "end_page": 85,
      "detection_method": "regex_chapter_title",
      "content": "An advantage of using HTTP for integration is that,\n\nunlike libraries and frameworks, you can use an API without knowing what language the API was written in. [2]\n\nIt would be perfectly acceptable, for example, for you\n\nto write the product catalog service in Java and the\n\ninventory service in Scala. Indeed, having the ability to make new technology adoption easier is another side\n\nbenefit of microservices.\n\n2\n\nIt’s this fact that makes mountebank usable in any language.\n\n2.2. HTTP AND MOUNTEBANK: A PRIMER\n\nHTTP is a text-based request-response network protocol. An HTTP server knows how to parse that text into its\n\nconstituent parts, but it’s simple enough that you can parse it without a computer. Mountebank assumes that\n\nyou are comfortable with those constituent parts. After all, you can’t expect to provide a convincing fake of an\n\nHTTP service if you don’t first understand what a real one looks like.\n\nLet’s deep-dive into HTTP using one of the first features you need to support: listing the available products.\n\nFortunately, the product catalog service has an endpoint for retrieving the products in JSON format. All you have\n\nto do is make the right API call, which looks like figure 2.2 in HTTP-speak.\n\nFigure 2.2. Breaking down the HTTP request for products\n\nThe first line of any HTTP request contains three\n\ncomponents: the method, the path, and the protocol version. In this case, the method is GET, which denotes\n\nthat you are retrieving information rather than trying to change state on some server resource. Your path is /products, and you are using version 1.1 of the HTTP protocol. The second line starts the headers, a set of\n\nnewline-separated key-value pairs. In this example, the Host header combines with the path and protocol to give\n\nthe full URL like you would see in a browser:\n\nhttp://api.petstore.com/products. The Accept header tells the server that you are expecting JSON back.\n\nWhen the product catalog service receives that request, it returns a response that looks like figure 2.3. A real service\n\npresumably would have many more data fields and many more items per page, but I have simplified the response\n\nto keep it digestible.\n\nFigure 2.3. The response from the product catalog\n\nA high degree of symmetry exists between HTTP requests and responses. As with the first line of the\n\nrequest, the first line of the response contains metadata, although for responses the most important metadata\n\nfield is the status code. A 200 status is HTTP-speak for success, but in case you forgot, it tells you with the word OK following the code. Other codes have other words that go with them, like BAD REQUEST for a 400, but the text\n\ndoesn’t serve any purpose other than a helpful hint. The libraries that you use for integrating with HTTP services\n\nonly care about the code, not the text.\n\nThe headers once again follow the metadata, but here\n\nyou see the HTTP body. The body is always separated from the headers by an empty line, and even though your\n\nHTTP request did not have a body, you will see plenty of examples in this book that do.\n\nThis particular body includes a link to the next page of\n\nresults, which is a common pattern for implementing paging in services. If you were to craft the HTTP request\n\nthat follows the link, it would look similar to the first request, as shown in figure 2.4.\n\nFigure 2.4. Adding a query parameter to an HTTP request\n\nThe difference appears to be in the path, but every HTTP\n\nlibrary that I’m aware of would give you the same path\n\nfor both the first and second request. Everything after the question mark denotes what is called the\n\nquerystring. (Mountebank calls it the query.) Like the headers, the query is a set of key-value pairs, but they are separated by the & character and included in the URL, separated from the path with a ? character.\n\nHTTP can attribute much of its success to its simplicity. The textual format makes it almost as easy for pizza-\n\nfueled computer programmers to read as it is for electricity-fueled computers to parse. That’s good for you\n\nbecause writing virtual services requires you to understand the protocol-specific request and response\n\nformats, which are treated as simple JSON objects that mimic closely the standard data structures used by HTTP\n\nlibraries in any language. To generalize, figure 2.5 shows how mountebank translates an HTTP request.\n\nFigure 2.5. How mountebank views an HTTP request\n\nNotice in figure 2.5 that even though the body is\n\nrepresented in JSON, HTTP itself doesn’t understand JSON, which is why the JSON is represented as a simple\n\nstring value. In later chapters, we will look at how mountebank makes working with JSON easier.\n\nFigure 2.6 shows how mountebank represents an HTTP\n\nresponse.\n\nFigure 2.6. How mountebank represents an HTTP response\n\nThis type of translation happens for all the protocols mountebank supports—simplifying the application\n\nprotocol details into a JSON representation. Each protocol gets its own JSON representation for both\n\nrequests and responses. The core functionality of mountebank performs operations on those JSON\n\nobjects, blissfully unaware of the semantics of the protocol. Aside from the servers and proxies to listen to\n\nand forward network requests, the core functionality in mountebank is protocol-agnostic.\n\nNow that you’ve seen how to translate HTTP semantics to mountebank, it’s time to create your first virtual\n\nservice.\n\n2.3. VIRTUALIZING THE PRODUCT CATALOG SERVICE\n\nOnce you understand how to integrate your codebase with a service, the next step is to figure out how to\n\nvirtualize it for testing purposes. Continuing with our example, let’s virtualize the product catalog service so\n\nyou can test the web façade in isolation (figure 2.7).\n\nFigure 2.7. Virtualizing the product catalog service to test the web facade\n\nRemember, an imposter is the mountebank term for a\n\nvirtual service. Mountebank ships with a REST API that lets you create imposters and write tests against them in\n\nany language.\n\nMountebanks, imposters, and funny sounding docs\n\nMuch of the mountebank documentation is written in\n\nthe voice of a true mountebank, prone to hyperbole and false modesty, where even the word “mountebank” shifts\n\nfrom describing the tool itself to the author of the documentation (yours truly). When I originally wrote the\n\ntool, I made imposters the core domain concept, in part because it fit the theme of using synonyms for charlatans\n\nto describe fake services, and in part because it self- deprecatingly made fun of my own Impostor Syndrome, a chronic ailment of consultants like myself. And yes, as\n\nPaul Hammant (one of the original creators of the popular Selenium testing tool and one of the first users\n\nof mountebank) pointed out to me, impostor (with an “or” instead of “er” at the end) is the “proper” spelling.\n\nNow that mountebank is a popular tool used all over the world, complete with a best-selling book (the one you are\n\nholding), Paul also helpfully suggested that I change the docs to remove the hipster humor. Unfortunately, he has\n\nyet to indicate where I’m supposed to find the time for such pursuits.\n\nBefore you start, you will need to install mountebank.\n\nThe website, http://www.mbtest.org/docs/install, lists several installation options, but you’ll use npm, a package\n\nmanager that ships with node.js, by typing the following in a terminal window:\n\nnpm install -g mountebank\n\nThe -g flag tells npm to install mountebank globally, so you can run it from any directory. Let’s start it up:\n\nmb\n\nYou should see the mountebank log on the terminal:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking\n\norders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\nThe log will prove invaluable in working with mountebank in the future, so it’s a good idea to familiarize yourself with it. The first word (info, in this case) tells you the log level, which will be either debug,\n\ninfo, warn, or error. The part in brackets (mb:2525) tells you the protocol and port and is followed by the log message. The administrative port logs as the mb protocol and starts on port 2525 by default. (The mb protocol is\n\nHTTP, but mountebank logs it differently to make it easy to spot.) The imposters you create will use different ports but log to the same output stream in the terminal. The\n\nstartup log message directs you to open http://localhost:2525 in your web browser, which will\n\nprovide you the complete set of documentation for the version of mountebank you are running.\n\nTo demonstrate creating imposters, you will use a utility called curl, which lets you make HTTP calls on the\n\ncommand line. curl comes by default on most Unix-like shells, including Linux and macOS. You can install it on\n\nWindows using Cygwin, or use PowerShell, which ships with modern versions of Windows. (We will show a\n\nPowerShell example next.) Open another terminal window and run the code shown in the following listing. [3]\n\n3\n\nTo avoid carpal tunnel syndrome, you can download the source at https://github.com/bbyars/mountebank-in-action.\n\nListing 2.1. Creating an imposter on the command line\n\ncurl -X POST http://localhost:2525/imposters --\n\ndata '{ 1\n\n\"port\": 3000, 2\n\n\"protocol\": \"http\",\n\n2\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": {\n\n3\n\n\"statusCode\": 200, 3\n\n\"headers\": {\"Content-Type\":\n\n\"application/json\"}, 3\n\n\"body\": {\n\n3\n\n\"products\": [\n\n3\n\n{ 3\n\n\"id\": \"2599b7f4\",\n\n3\n\n\"name\": \"The Midas Dogbowl\",\n\n3\n\n\"description\": \"Pure gold\"\n\n3\n\n},\n\n3\n\n{ 3\n\n\"id\": \"e1977c9e\",\n\n3\n\n\"name\": \"Fishtank Amore\",\n\n3\n\n\"description\": \"Show your fish some\n\nlove\" 3\n\n} 3\n\n],\n\n3\n\n\"_links\": {\n\n3\n\n\"next\": \"/products?\n\npage=2&itemsPerPage=2\" 3\n\n} 3\n\n}\n\n3\n\n}\n\n3\n\n}]\n\n}]\n\n}'\n\n1 Creates new imposters\n\n2 Minimally defines each imposter by a port and a protocol\n\n3 Defines a canned HTTP response\n\nAn important point to note is that you are passing a JSON object as the body field. As far as HTTP is\n\nconcerned, a response body is a stream of bytes. Usually HTTP interprets that stream as a string, which is why\n\n[4]\n\nmountebank typically expects a string as well. said, most services these days use JSON as their lingua\n\nThat\n\nfranca. Mountebank, being itself a modern JSON- speaking service, can properly accept a JSON body.\n\n4\n\nMountebank supports binary response bodies, encoding them with Base64. We look at binary support in chapter 8.\n\nThe equivalent command on PowerShell in Windows\n\nexpects you to save the request body in a file and pass it in to the Invoke-RestMethod command. Save the\n\nJSON after the --data parameter from the curl command code above into a file called imposter.json,\n\nthen run the following command from the same directory:\n\nInvoke-RestMethod -Method POST -Uri\n\nhttp://localhost:2525/imposters\n\nInFile imposter.json\n\nNotice what happens in the logs:\n\ninfo: [http:3000] Open for business...\n\nThe part in brackets now shows the new imposter. As\n\nyou add more imposters, this will become increasingly important. You can disambiguate all log entries by\n\nlooking at the imposter information that prefixes the log message.\n\nYou can test your imposter on the command line as well, using the curl command we looked at previously, as shown in figure 2.8.\n\nFigure 2.8. Using curl to send a request to your virtual product catalog service\n\nThe curl command prints out the HTTP response as shown in the following listing.\n\nListing 2.2. The HTTP response from the curl command\n\nHTTP/1.1 200 OK Content-Type: application/json\n\nConnection: close\n\nDate: Thu, 19 Jan 2017 14:51:23 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\" }\n\n],\n\n\"_links\": {\n\n\"next\": \"/products?page=2&itemsPerPage=2\"\n\n}\n\n}\n\nThat HTTP response includes a couple of extra headers, and the date has changed, but other than that, it’s exactly\n\nthe same as the real one returned by the service shown in figure 2.3. You aren’t accounting for all situations though.\n\nThe imposter will return exactly the same response no matter what the HTTP request looks like. You could fix\n\nthat by adding predicates to your imposter configuration.\n\nAs a reminder, a predicate is a set of criteria that the incoming request must match before mountebank will\n\nsend the associated response. Let’s create an imposter that only has two products to serve up. We will use a\n\npredicate on the query parameter to show an empty result set on the request to the second page. For now, restart mb to free up port 3000 by pressing Ctrl-C and\n\ntyping mb again. (You will see more elegant ways of cleaning up after yourself shortly.) Then use the\n\ncommand shown in the following listing in a separate terminal.\n\nListing 2.3. An imposter with predicates\n\ncurl -X POST http://localhost:2525/imposters --\n\ndata '{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n1\n\n{\n\n\"predicates\": [{ \"equals\": {\n\n2\n\n\"query\": { \"page\": \"2\" }\n\n2\n\n}\n\n2\n\n}],\n\n\"responses\": [{ \"is\": {\n\n3\n\n\"statusCode\": 200,\n\n3\n\n\"headers\": {\"Content-Type\":\n\n\"application/json\"}, 3\n\n\"body\": { \"products\": [] }\n\n3 }\n\n3\n\n}]\n\n},\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n4 \"statusCode\": 200,\n\n4\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" }, 4\n\n\"body\": {\n\n4\n\n\"products\": [\n\n4 {\n\n4\n\n\"id\": \"2599b7f4\",\n\n4\n\n\"name\": \"The Midas Dogbowl\",\n\n4\n\n\"description\": \"Pure gold\"\n\n4 },\n\n4\n\n{\n\n4\n\n\"id\": \"e1977c9e\",\n\n4\n\n\"name\": \"Fishtank Amore\",\n\n4 \"description\": \"Show your fish\n\nsome love\" 4\n\n}\n\n4\n\n],\n\n4\n\n\"_links\": {\n\n4 \"next\": \"/products?\n\npage=2&itemsPerPage=2\" 4\n\n}\n\n4\n\n}\n\n4\n\n}\n\n4 }]\n\n}\n\n]\n\n}'\n\n1 Using two stubs allows different responses for different\n\nrequests.\n\n2 Requires that the request querystring include page=2\n\n3 Sends this response if the request matches the\n\npredicate\n\n4 Otherwise, sends this response\n\nNow, if you send a request to the imposter without a\n\nquerystring, you’ll get the same response as before. But adding page=2 to the querystring gives you an empty\n\nproduct list:\n\ncurl -i http://localhost:3000/products?page=2\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json\n\nConnection: close\n\nDate: Sun, 21 May 2017 17:19:17 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": []\n\n}\n\nExploring the mountebank API on the command line is a great way to get familiar with it and to try sample\n\nimposter configurations. If you change the configuration of your web façade to point to http://localhost:3000\n\ninstead of https://api.petstore.com, you will get the products we have defined and can manually test the\n\nwebsite. You have already taken a huge step toward decoupling yourself from the real services.\n\nPostman as an alternative to the command line\n\nAlthough using command-line tools like curl is great for lightweight experimentation and perfect for the book\n\nformat, it’s often useful to have a more graphical approach to organize different HTTP requests. Postman\n\n(https://www.getpostman.com/) has proven to be an extremely useful tool for playing with HTTP APIs. It\n\nstarted out as a Chrome plugin but now has downloads for Mac, Windows, and Linux. It lets you fill in the\n\nvarious HTTP request fields and save requests for future use.\n\nThat said, the real benefit of service virtualization is in enabling automated testing. Let’s see how you can wire\n\nup mountebank to your test suite.\n\n2.4. YOUR FIRST TEST\n\nTo properly display the products on the website, the web\n\nfaçade needs to combine the data that comes from the product catalog service with marketing copy that comes\n\nfrom a marketing content service (figure 2.9). You will add tests that verify that the data that gets to the website\n\nis valid.\n\nFigure 2.9. Combining product data with marketing copy\n\nThe data that the web façade provides to the website should show both the product catalog data and the\n\nmarketing content. The response from the web façade should look like the following listing.\n\nListing 2.4. Combining product data with marketing content\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json\n\nDate: Thu, 19 Jan 2017 15:43:21 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": [ {\n\n\"id\": \"2599b7f4\",\n\n1\n\n\"name\": \"The Midas Dogbowl\",\n\n2\n\n\"description\": \"Pure gold\",\n\n2\n\n\"copy\": \"Treat your dog like the king he is\", 3\n\n\"image\": \"/content/c5b221e2\"\n\n3\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\", \"copy\": \"Love your fish; they'll love you\n\nback\",\n\n\"image\": \"/content/a0fad9fb\"\n\n}\n\n],\n\n\"_links\": {\n\n\"next\": \"/products?page=2&itemsPerPage=2\"\n\n} }\n\n1 Comes from the product catalog service, but also will be\n\nused to look up content\n\n2 Comes from the product catalog service\n\n3 Comes from the marketing content service\n\nLet’s write a service test that validates that if the product catalog and content services return the given data, then\n\nthe web façade will combine the data as shown above. Although mountebank’s HTTP API allows you to use it in\n\nany language, you will use JavaScript for the example. The first thing you will need to do is make it easy to\n\ncreate imposters from your tests. A common approach to make building a complex configuration easier is to use what is known as a fluent interface, which allows you to\n\nchain function calls together to build a complex configuration incrementally.\n\nThe code in listing 2.5 uses a fluent interface to build up the imposter configuration in code. Each withStub call\n\ncreates a new stub on the imposter, and each matchingRequest and respondingWith call adds a\n\npredicate and response, respectively, to the stub. When you are done, you call create to use mountebank’s\n\nREST API to create the imposter.\n\nListing 2.5. Using a fluent interface to build imposters in code\n\nrequire('any-promise/register/q');\n\n1\n\nvar request = require('request-promise-any');\n\n1\n\nmodule.exports = function (options) {\n\n2\n\nvar config = options || {};\n\nconfig.stubs = [];\n\nfunction create () {\n\n3\n\nreturn request({ method: \"POST\",\n\nuri: \"http://localhost:2525/imposters\",\n\njson: true,\n\nbody: config\n\n});\n\n}\n\nfunction withStub () { 4\n\nvar stub = { responses: [], predicates: [] },\n\nbuilders = {\n\nmatchingRequest: function (predicate) {\n\n5\n\nstub.predicates.push(predicate);\n\nreturn builders;\n\n}, respondingWith: function (response) {\n\n6\n\nstub.responses.push({ is: response });\n\nreturn builders;\n\n},\n\ncreate: create,\n\nwithStub: withStub\n\n};\n\nconfig.stubs.push(stub);\n\nreturn builders;\n\n}\n\nreturn {\n\nwithStub: withStub,\n\ncreate: create\n\n};\n\n};\n\n1 node.js libraries that make calling HTTP services easier\n\n2 The node.js way of exposing a function to different files\n\n3 Calls the REST API to create an imposter\n\n4 The entry point to the fluent interface—each call\n\ncreates a new stub\n\n5 Adds a new request predicate to the stub\n\n6 Adds a new response to the stub\n\nJavaScript: ES5 vs. ES2015\n\nModern JavaScript syntax is defined in the version of the EcmaScript (ES) specification. At the time of this writing,\n\nES2015, which adds a bunch of syntactic bells and whistles, is seeing wide adoption, but ES5 still has the\n\nbroadest support. Although those syntactic bells and whistles are nice once you get used to them, they make\n\nthe code a little more opaque for non-JavaScript developers. Because this isn’t a book on JavaScript, I use\n\nES5 here to keep the focus on mountebank.\n\nYou will see how the fluent interface makes the\n\nconsuming code more elegant shortly. The key to making it work is exposing the create and withStub functions\n\nin the builder, which allows you to chain functions together to build the entire configuration and send it to\n\nmountebank.\n\nAssuming you saved the code above in a file called imposter.js, you can use it to create the product catalog\n\nservice response on port 3000. The code in listing 2.6 replicates what you did earlier on the command line and\n\nshows how the function chaining that the fluent interface\n\ngives you makes the code easier to follow. Save the following code in test.js.[5]\n\nListing 2.6. Creating the product imposter in code\n\nvar imposter = require('./imposter'), 1\n\nproductPort = 3000;\n\nfunction createProductImposter() {\n\nreturn imposter({\n\n2\n\nport: productPort,\n\nprotocol: \"http\", name: \"Product Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({equals: {path:\n\n\"/products\"}}) 3\n\n.respondingWith({\n\n4\n\nstatusCode: 200, headers: {\"Content-Type\":\n\n\"application/json\"},\n\nbody: {\n\nproducts: [\n\n{\n\nid: \"2599b7f4\",\n\nname: \"The Midas Dogbowl\",\n\ndescription: \"Pure gold\" },\n\n{\n\nid: \"e1977c9e\",\n\nname: \"Fishtank Amore\",\n\ndescription: \"Show your fish some\n\nlove\"\n\n}\n\n] }\n\n})\n\n.create();\n\n5\n\n}\n\n1 Imports your fluent interface\n\n2 Passes the root-level information into the entry\n\nfunction\n\n3 Adds the request predicate\n\n4 Adds the response\n\n5 Sends a POST to the mountebank endpoint to create the\n\nimposter\n\nIt’s worth noting a couple of points about the way you are\n\ncreating the product catalog imposter. First, you have added a name to the imposter. The name field doesn’t\n\nchange any behavior in mountebank other than the way the logs format messages. The name will be included in\n\nthe text in brackets to make it easier to understand log messages by imposter. If you look at the mountebank logs after you create this imposter, you will see the name echoed:\n\ninfo: [http:3000 Product Service] Open for business...\n\nThat’s a lot easier than having to remember the port each\n\nimposter is running on.\n\nThe second thing to note is that you are adding a predicate to match the path. This isn’t strictly necessary, as your test will correctly pass without it if the web\n\nfaçade code is doing its job. However, adding the predicate makes the test better. It not only verifies the\n\nbehavior of the façade given the response, it also verifies that the façade makes the right request to the product\n\nservice.\n\nWe haven’t looked at the marketing content service yet. It accepts a list of IDs on a querystring and returns a set\n\nof content entries for each ID provided. The code in the following listing creates an imposter using the same IDs\n\nthat the product catalog service provides. (Add this to the test.js file you created previously.)\n\nListing 2.7. Creating the content imposter\n\nvar contentPort = 4000;\n\nfunction createContentImposter() {\n\nreturn imposter({\n\nport: contentPort,\n\nprotocol: \"http\",\n\nname: \"Content Service\" })\n\n.withStub()\n\n.matchingRequest({\n\nequals: {\n\n1\n\npath: \"/content\",\n\n1\n\nquery: { ids: \"2599b7f4,e1977c9e\" } 1\n\n}\n\n1\n\n})\n\n.respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"}, body: {\n\ncontent: [\n\n2\n\n{\n\n2\n\nid: \"2599b7f4\",\n\n2\n\ncopy: \"Treat your dog like the king he is\", 2\n\nimage: \"/content/c5b221e2\"\n\n2\n\n},\n\n2\n\n{\n\n2\n\nid: \"e1977c9e\", 2\n\ncopy: \"Love your fish; they'll love\n\nyou back\", 2\n\nimage: \"/content/a0fad9fb\"\n\n2\n\n}\n\n2\n\n] 2\n\n}\n\n})\n\n.create();\n\n}\n\n1 Only respond if the path and query match as shown.\n\n2 The entries that the content service would return\n\nArmed with the createProductImposter and\n\ncreateContentImposter functions, you now can write a service test that calls the web façade over the wire\n\nand verifies that it aggregates the data from the product catalog and marketing content services appropriately\n\n(figure 2.10).\n\nFigure 2.10. The steps of the service test to verify web façadeʼs data aggregating\n\nFor this step, you will use a JavaScript test runner called Mocha, which wraps each test in an it function and\n\ncollections of tests in a describe function (similar to a test class in other languages). Finish off the test.js file\n\nyou have been creating by adding the code in the following listing.\n\nListing 2.8. Verifying the web façade\n\nrequire('any-promise/register/q');\n\nvar request = require('request-promise-any'),\n\nassert = require('assert'),\n\nwebFacadeURL = 'http://localhost:2000';\n\ndescribe('/products', function () {\n\n1\n\nit('combines product and content data',\n\nfunction (done) { 2\n\ncreateProductImposter().then(function () {\n\n3\n\nreturn createContentImposter(); 3\n\n}).then(function () {\n\nreturn request(webFacadeURL + '/products');\n\n4\n\n}).then(function (body) {\n\nvar products = JSON.parse(body).products;\n\nassert.deepEqual(products, [ 5\n\n{\n\n\"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\",\n\n\"copy\": \"Treat your dog like the king\n\nhe is\",\n\n\"image\": \"/content/c5b221e2\" },\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some\n\nlove\",\n\n\"copy\": \"Love your fish; they'll love\n\nyou back\", \"image\": \"/content/a0fad9fb\"\n\n}\n\n]);\n\nreturn imposter().destroyAll();\n\n6\n\n}).then(function () {\n\ndone();\n\n7 });\n\n});\n\n});\n\n1 Mocha groups multiple tests in a describe function.\n\n2 Each it function represents a single test.\n\n3 Arrange\n\n4 Act\n\n5 Assert\n\n6 Cleanup\n\n7 Tells mocha that the asynchronous test is finished\n\nNotice that you added one step to the test to clean up the\n\nimposters. Mountebank supports a couple ways of removing imposters. You can remove a single imposter by sending a DELETE HTTP request to the /imposters/:port URL (where :port represents the\n\nport of the imposter), or remove all imposters in a single call by issuing a DELETE request to /imposters. Add\n\nthem to your imposter fluent interface in imposter.js, as shown in the following listing.\n\nListing 2.9. Adding the ability to remove imposters\n\nfunction destroy () {\n\nreturn request({\n\nmethod: \"DELETE\",\n\nuri: \"http://localhost:2525/imposters/\" +\n\nconfig.port 1 });\n\n}\n\nfunction destroyAll () {\n\nreturn request({\n\nmethod: \"DELETE\",\n\nuri: \"http://localhost:2525/imposters\"\n\n});\n\n}\n\n1 Passes in the config object, as in listing 2.5\n\nWhew! You now have a complete service test that verifies some fairly complex aggregation logic of a service in a black-box fashion by virtualizing its runtime\n\ndependencies. (You had to create some scaffolding, but you will be able to reuse the imposters.js module in all of\n\nyour tests moving forward.) The prerequisites for running this test are that both the web façade and\n\nmountebank are running, and you have configured the web façade to use the appropriate URLs for the\n\nimposters (http://localhost:3000 for the product catalog service, and http://localhost:4000 for the marketing\n\ncontent service).[6]\n\nJavaScript promises\n\nYour test code relies on a concept called promises to\n\nmake it easier to follow. JavaScript hasn’t traditionally had any I/O, and when node.js added I/O capability, it\n\ndid so in what is known as a nonblocking manner. This means that system calls that need to read or write data to\n\nsomething other than memory are done asynchronously. The application requests the operating system to read\n\nfrom disk, or from the network, and then moves on to other activities while waiting for the operating system to\n\nreturn. For a web service like the kind you are building, “other activities” would include processing new HTTP\n\nrequests.\n\nThe traditional way of telling node.js what to do when the operating system has finished the operation is to register a callback function. In fact, the request library that you are using to make HTTP calls works this way by\n\ndefault, as shown in this callback-based HTTP request:\n\nvar request = require('request');\n\nrequest('http://localhost:4000/products',\n\nfunction (error, response, body) {\n\n// Process the response here\n\n})\n\nThe problem with this approach is that it gets unwieldy to nest multiple callbacks, and downright tricky to figure\n\nout how to loop over a sequence of multiple asynchronous calls. With promises, asynchronous operations return an object that has a then function, which serves the same purpose as the callback. But\n\npromises add all kinds of simplifications to make combining complex asynchronous operations easier. You\n\nwill use them in your tests to make the code easier to read.\n\nPart 3 of this book will show more fully worked-out\n\nautomated tests and how to include them in a continuous delivery pipeline. First, though, you need to get familiar\n\nwith the capabilities of mountebank. Part 2 breaks down the core mountebank capabilities step by step, starting in\n\nthe next chapter by exploring canned responses in depth and adding HTTPS to the mix.\n\nSUMMARY\n\nMountebank translates the fields of the HTTP application protocol\n\ninto JSON for requests and responses.\n\nMountebank virtualizes services by creating imposters, which bind\n\na protocol to a socket. You can create imposters using\n\nmountebank’s RESTful API.\n\nYou can use mountebank’s API in automated tests to create\n\nimposters returning a specific set of canned data to allow you to\n\ntest your application in isolation.\n\nPart 2. Using mountebank\n\nThe test in chapter 2 was a behavioral test, but service\n\nvirtualization can satisfy a wide spectrum of testing needs. Understanding how mountebank fits into that\n\nspectrum requires exploring the full capabilities of the tool.\n\nThe test we just looked at used basic building blocks of service virtualization—and indeed of any stubbing tool— the ability to evaluate the request to determine how to\n\nrespond. We’ll look at these capabilities over the course of the next two chapters, including additional context\n\naround HTTPS, managing configuration files, and taking advantage of mountebank’s built-in XML and JSON\n\nparsing.\n\nChapters 5 and 6 demonstrate more advanced response\n\ngeneration, allowing a more interesting set of test scenarios. By adding record and replay capability, you\n\ncan generate test data dynamically to perform large-scale tests and to build the foundation for performance testing\n\n(which we’ll examine in part 3). The ability to programmatically change responses gives you key\n\nflexibility to support hard-to-test scenarios like OAuth.\n\nBehaviors, or postprocessing steps, provide advanced functionality. From managing test data in a CSV file to\n\nadding latency to your responses, behaviors give you a robust set of tools both to simplify testing and to support\n\na wider range of testing scenarios. We explore behaviors in chapter 7.\n\nWe round out this section by looking at mountebank’s\n\nsupport for protocols, which is the glue that makes everything else possible. Although we spend much of the\n\nbook exploring HTTP use cases, mountebank supports\n\nmultiple protocols, and in chapter 8 we explore how it works with additional TCP-based protocols.\n\nChapter 3. Testing using canned responses\n\nThis chapter covers\n\nThe is response type, which is the fundamental building block for a stub\n\nUsing is responses in secure scenarios, with HTTPS servers and mutual authentication\n\nPersisting your imposter configuration using file templates\n\nDuring a famous U.S. White House scandal of the 1990s,\n\nthen-president Bill Clinton defended his prior statements by saying “It depends on what the meaning of is is.” The\n\ngrand jury and politicians ultimately failed to come to an agreement on the question, but, fortunately,\n\nmountebank has no uncertainty on the matter.\n\nIt turns out that is is quite possibly the most important, and the most foundational, concept in all of\n\nmountebank. Although an imposter, capturing the core idea of binding a protocol to a port, might beg to differ,\n\nby itself it adds little to a testing strategy. A response that looks like the real response—a response that, as far as the\n\nsystem under test is concerned, is the real response— changes everything. Is is the key to being fake. Without\n\nis, a service binding a protocol to a port is a lame beast at best. Adding the ability to respond, and to respond as if\n\nthe service is the real service, turns that service into a genuinely useful imposter.\n\nIn mountebank, the is response type is how you create canned responses, or responses that simulate a real response in some static way that you configure. Although it is one of three response types (proxy and inject being the other two), it’s the most important one. In this\n\nchapter, we will explore is responses both by using the REST API and by persisting them in configuration files.\n\nWe will also start to layer in key security concerns. Although all of our examples so far have assumed HTTP,\n\nthe reality is that any serious web-based service built today will use HTTPS, layering transport layer security\n\n(TLS) onto the HTTP protocol. Because security— especially authentication—is generally one of the first\n\naspects of any microservice implementation you run into when writing tests, we will look at using an HTTPS\n\nserver that uses certificates to validate the client.\n\nFinally, we will explore how to persist imposter configurations. As you have no doubt realized by now,\n\nstubbing out services over the wire can be significantly more verbose than stubbing out objects in-process.\n\nFiguring out how to lay out that configuration in a maintainable way is essential to using service\n\nvirtualization to shift tests earlier in the development life cycle.\n\n3.1. THE BASICS OF CANNED RESPONSES\n\nIt’s a bit rude for any book on software development to\n\nskip out on the customary “Hello, world!” example. A “Hello, world!” response looks like the following listing\n\nin HTTP.\n\n1\n\nBrian Kernighan and Dennis Ritchie showed how to print “Hello, world!” to the terminal in their venerable book The C Programming Language. It has become a common introductory\n\nexample.\n\nListing 3.1. Hello world! in an HTTP response\n\nHTTP/1.1 200 OK\n\nContent-Type: text/plain\n\nHello, world!\n\n[1]",
      "page_number": 55
    },
    {
      "number": 3,
      "title": "Testing using canned responses",
      "start_page": 86,
      "end_page": 118,
      "detection_method": "regex_chapter_title",
      "content": "As you saw in the last chapter, returning this response in mountebank is as simple as translating the response you\n\nwant into the appropriate JSON structure, as follows.\n\nListing 3.2. The HTTP response structure in JSON\n\n{\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\": \"text/plain\" },\n\n\"body\": \"Hello, world!\"\n\n}\n\nTo create an HTTP imposter, listening on port 3000, that will return this response, save the following code in a helloWorld.json file.\n\nListing 3.3. The imposter configuration to respond with Hello, world!\n\n{ \"protocol\": \"http\",\n\n1\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": {\n\n2\n\n\"statusCode\": 200, 3\n\n\"headers\": { \"Content-Type\": \"text/plain\"\n\n}, 3\n\n\"body\": \"Hello, world!\"\n\n3\n\n}\n\n}]\n\n}] }\n\n1 Protocol defines response structure\n\n2 Tells mountebank to use an is response\n\n3 Defines the canned response to be translated into HTTP\n\nYou represent the JSON response you want from listing 3.2 inside the is response and expect mountebank to\n\ntranslate that to the HTTP shown in listing 3.1 because you’ve set the protocol to http. With mb running, you\n\ncan send an HTTP POST to http://localhost:2525/imposters to create this imposter. You will use the curl command, introduced in chapter 2, [2] to send the HTTP request:\n\n2\n\nFeel free to follow along using Postman or some graphical REST client. The examples are also available at https://github.com/bbyars/mountebank-in-action.\n\ncurl -d@helloWorld.json\n\nhttp://localhost:2525/imposters\n\nThe -d@ command-line switch reads the file that follows\n\nand sends the contents of that file as an HTTP POST body. You can verify that mountebank has created the\n\nimposter correctly by sending any HTTP request you want to port 3000:\n\n[3]\n\n3\n\nIn the examples that follow, I will continue to use the -i command-line parameter for curl.\n\nThis tells curl to print the response headers to the terminal.\n\ncurl -i http://localhost:3000/any/path?\n\nquery=does-not-matter\n\nThe response is almost, but not quite, the same as the “Hello, world!” response shown in listing 3.1:\n\nHTTP/1.1 200 OK Content-Type: text/plain\n\nConnection: close\n\nDate: Wed, 08 Feb 2017 01:42:38 GMT\n\nTransfer-Encoding: chunked\n\nHello, world!\n\nThree additional HTTP headers somehow crept in. Understanding where these headers came from requires\n\nus to revisit a concept described in chapter 1 as the default response.\n\n3.1.1. The default response\n\nYou may recall the diagram shown in figure 3.1, which describes how mountebank selects which response to\n\nreturn based on the response.\n\nFigure 3.1. How mountebank selects a response\n\nThis diagram implies that if the request doesn’t match any predicate, a hidden default stub will be used. That\n\ndefault stub contains no predicates, so it always matches the request, and it contains exactly one response—the\n\ndefault response. You can see this default response if you create an imposter without any stubs:\n\ncurl http://localhost:2525/imposters --data '\n\n{ \"protocol\": \"http\",\n\n\"port\": 3000\n\n}'\n\nNote\n\nBecause you’re using port 3000 across multiple examples, you may find that you have to shut down and\n\nrestart mountebank between examples to avoid a port conflict. Alternatively, you can use the API to clean up the previous imposter(s) by sending an HTTP DELETE command to http://local host:2525/imposters (to\n\nremove all existing imposters) or to http://localhost:2525/imposters/3000 (to remove only the imposter on port 3000). If you’re using curl, the command would be curl -X DELETE http://local\n\nhost:2525/imposters.\n\nYou have not defined any responses with that lame beast\n\nof an imposter; you have only said you want an HTTP server listening on port 3000. If you send any HTTP\n\nrequest to that port, you get the default response shown in the following listing.\n\nListing 3.4. The default response in mountebank\n\nHTTP/1.1 200 OK\n\nConnection: close\n\nDate: Wed, 08 Feb 2017 02:04:17 GMT\n\nTransfer-Encoding: chunked\n\nWe looked at the first line of the response in chapter 2, and the 200 status code indicates that mountebank\n\nprocessed the request successfully. The Date header is a standard response header that any responsible HTTP\n\nserver sends, providing the server’s understanding of the current date and time. The other two headers require a\n\nbit more explanation.\n\nHTTP connections: to reuse or not to reuse?\n\nHTTP is an application protocol built on top of the hard work of a few lower level network protocols, the most\n\nimportant of which (for our purposes) is TCP. TCP is responsible for establishing the connection between the\n\nclient and the server through a series of messages often referred to as the TCP handshake (figure 3.2).\n\nFigure 3.2. TCP making the connection for HTTP messages\n\nAlthough the TCP messages to establish the connection (represented by the dashed lines) are necessary, they\n\naren’t necessary for every request. Once the connection is established, the client and server can reuse it for\n\nmultiple HTTP messages. That ability is important, particularly for websites that need to serve HTML,\n\nJavaScript, CSS, and a set of images, each of which requires a round trip between the client and the server.\n\nHTTP supports keep-alive connections as a performance\n\noptimization. A server tells the client to keep the connection open by setting the Connection header to\n\nKeep-Alive. Mountebank defaults it to close, which tells the client to negotiate the TCP handshake for every\n\nrequest. If you are writing service tests, performance likely doesn’t matter, and you may prefer the\n\ndeterminism that comes with a fresh connection for each request. If you are writing performance tests, where the\n\nservices you are virtualizing should be tuned with keep- alive connections, or if your purpose is to ensure your\n\napplication behaves well with keep-alive connections, you should change the default.\n\nKnowing where an HTTP body ends\n\nNotice in figure 3.2 that a single HTTP request may\n\nconsist of multiple packets. (The operating system breaks up data into a series of packets to optimize sending them\n\nover the network.) The same is true of a server response: what looks to be a single response may get transmitted in\n\nmultiple packets. A consequence of this is that clients and servers need some way of knowing when an HTTP\n\nmessage is complete. With headers, it’s easy: the headers end when you get a blank header line. But there’s no way\n\nto predict where blank lines will occur in HTTP bodies, so you need a different strategy. HTTP provides two\n\nstrategies, as shown in figure 3.3.\n\nFigure 3.3. Using chunked encodings or content length to calculate where the body ends\n\nThe default imposter behavior sets the Transfer-\n\nEncoding: chunked header, which breaks the body into a series of chunks and prefixes each one with the\n\nnumber of bytes it contains. Special formatting delineates each chunk, making parsing relatively easy.\n\nThe advantage of sending the body a chunk at a time is that the server can start streaming data to the client\n\nbefore the server has all of the data. The alternative strategy is to calculate the length of the entire HTTP\n\nbody before sending it and provide that information in the header. To select that strategy, the server sets a Content-Length header to the number of bytes in the body.\n\nWhen I created mountebank, I had to choose one default\n\nstrategy. In truth, the web framework mountebank is written in chose it for me, which is the only reason\n\nmountebank imposters default to chunked encoding. The two strategies are mutually exclusive, so if you need to\n\nset the Content-Length header, the Transfer- Encoding header won’t be set.\n\n3.1.2. Understanding how the default response works\n\nNow that you have seen what the default response looks like, it is probably a good time to admit that there is no\n\nsuch thing as a default stub in mountebank. That’s a bald-faced lie. I’m sorry—I did feel a little guilty writing\n\nit—but it’s a useful simplification for situations where no stub matches the request. And in case you haven’t\n\nnoticed yet, lying is exactly what mountebank does.\n\nThe reality is that mountebank merges the default response into any response you provide. Not providing a\n\nresponse is the same as providing an empty response, which is why you see the purest form of the default\n\nresponse in listing 3.4. But you also could provide a partial response; for example, the following response\n\nstructure doesn’t provide all of the response fields:\n\n{\n\n\"is\": {\n\n\"body\": \"Hello, world!\"\n\n}\n\n}\n\nNot to worry. Mountebank will still return a full response, helpfully filling in the blanks for you:\n\nHTTP/1.1 200 OK\n\nConnection: close Date: Sun, 12 Feb 2017 17:38:39 GMT\n\nTransfer-Encoding: chunked\n\nHello, world!\n\n3.1.3. Changing the default response\n\nMountebank’s ability to merge in defaults for the response is a pleasant convenience. As I suggested, it\n\nmeans you only need to specify the fields that are different from the defaults, which simplifies the response\n\nconfiguration. But that’s only useful if the defaults represent what you typically want. Fortunately,\n\nmountebank allows you to change the default response to better suit your needs.\n\nImagine a test suite that only wants to test error paths. You can default the statusCode to a 400 Bad\n\nRequest to avoid having to specify it in each response. Although you can’t get rid of the Date header (it’s\n\nrequired for valid responses), you’ll go ahead and change the other default headers to use keep-alive connections and set the Content-Length header, as in the following listing.\n\nListing 3.5. Changing the default response\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"defaultResponse\": { 1\n\n\"statusCode\": 400, 2 \"headers\": { 3\n\n\"Connection\": \"Keep-Alive\", 4\n\n\"Content-Length\": 0 5\n\n}\n\n},\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": { \"body\": \"BOOM!!!\" } 6 }]\n\n}]\n\n}\n\n1 Changes the built-in default response for this imposter\n\nonly\n\n2 Defaults to a Bad Request\n\n3 Adds or changes default headers\n\n4 Uses keep-alive connections\n\n5 Sets the Content-Length header\n\n6 The response details will be merged into the default\n\nresponse.\n\nIf you now send a test request to the imposter, it merges the new default fields into the response, as follows.\n\nListing 3.6. A response using the new defaults\n\nHTTP/1.1 400 Bad Request 1 Connection: Keep-Alive 2\n\nContent-Length: 7 3\n\nDate: Fri, 17 Feb 2017 16:29:00 GMT 4\n\nBOOM!!! 5\n\n1 400 comes from your default status code.\n\n2 You’re now using keep-alive connections.\n\n3 The Content-Length value was corrected from 0 to 7,\n\nand the Transfer-Encoding header is gone.\n\n4 The Date header remains from the original default\n\nresponse.\n\n5 The body from your is response is merged in.\n\nNotice in particular that mountebank set the Content-\n\nLength header to the correct value. Mountebank imposters won’t send out invalid HTTP responses.\n\n3.1.4. Cycling through responses\n\nLet’s imagine one more test scenario: this time, you will test what happens when you submit an order through an\n\nHTTP POST to an order service. Part of the order submission process involves checking to make sure inventory is sufficient. The tricky part, from a testing\n\nperspective, is that inventory is sold and restocked—it doesn’t stay static for the same product. This means that\n\nthe exact same request to the inventory service can respond with a different result each time (figure 3.4).\n\nFigure 3.4. Inventory checks return volatile results for the same request.\n\nIn chapter 2, you saw a similar example with requests to\n\nthe product catalog service, which returned different responses for the same path. In that example, you were\n\nable to use different predicates to determine which response to send based on the page query parameter,\n\nbut in the inventory example, nothing about the request allows you to select one response over the other.\n\nWhat you need is a way to cycle through a set of\n\nresponses to simulate the volatility of the on-hand inventory for a fast-selling product. The solution is to use\n\nthe fact that each stub contains a list of responses, as shown in the following listing. Mountebank returns those\n\n[4]\n\nresponses in the order provided.\n\n4\n\nNote that in the following example and several others throughout the book, I will use an overly simplified response to save space and remove some of the noise. No self-respecting inventory service would ever return only a single number, but it makes the intent of the\n\nexample stand out more clearly, allowing you to focus on the fact that some data is different for each response.\n\nListing 3.7. Returning a list of responses for the same stub\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } }, 1\n\n{ \"is\": { \"body\": \"21\" } }, 1\n\n{ \"is\": { \"body\": \"0\" } } 1\n\n]\n\n}\n\n] }\n\n1 Responses that are returned in order\n\nThe first call returns 54, the second call returns 21, and\n\nthe third call returns 0. If your tests need to trigger a fourth call, it will once again return 54, then 21, and 0.\n\nMountebank treats the list of responses as an infinite list, with the first and last entries connected like a circle, the\n\ncircular buffer data structure discussed in chapter 1.\n\nAs shown in figure 3.5, the illusion of an infinite list is maintained by shifting each response to the end of the\n\nlist when it is returned. You can cycle through them as many times as you need to.\n\nFigure 3.5. Each stub cycles through the responses forever.\n\nIn chapter 7, we’ll look at all kinds of interesting post- processing actions you can take on a response, but for\n\nnow, you don’t need to know anything more about canned responses. Let’s switch gears and see how to layer\n\nin security.\n\n3.2. HTTPS IMPOSTERS\n\nTo keep things simple, we’ve focused on HTTP services\n\nso far. The reality is that real services require security, and that means using HTTPS. The S stands for SSL/TLS,\n\nwhich adds encryption, and, optionally, identity verification to each request. Figure 3.6 shows the basic\n\nstructure for SSL.\n\nFigure 3.6. The basic structure of SSL/TLS\n\nThe infrastructure handles the SSL layer of HTTPS so that, as far as the application is concerned, each request\n\nand response is standard HTTP. The details of how the\n\nSSL layer works are a bit complex, but the key concepts that make it work are the server’s certificate and the\n\nserver’s keys. The HTTPS server presents the client an SSL certificate during the handshake process, which\n\ndescribes the server’s identity, including information such as the owner, the domain it’s attached to, and the\n\nvalidity dates.\n\nIt’s entirely possible that a malicious server may try to\n\npass itself off as, say, Google, in the hopes of you passing it confidential information that you would only intend to\n\npass to Google. That’s why Certificate Authorities (CAs) exist. Trust has to start somewhere, and CAs are the\n\nfoundation of trust in the SSL world. By sending a certificate, which contains a digital signature, to a CA\n\nthat your organization trusts, you can confirm that the certificate is in fact from Google.\n\nThe certificate also includes the server’s public key. The\n\neasiest approach to encryption is to use a single key for both encryption and decryption. Because of its efficiency,\n\nmost of the communication relies on single-key encryption, but first the client and server have to agree\n\non the key used without anyone else knowing it. The type of encryption SSL relies on during this handshake uses a\n\nneat trick that requires different keys for those two operations: the public key is used for encryption, and a\n\nseparate private key is used for decryption (figure 3.7). This allows the server to share its public key and the\n\nclient to use that key for encryption, knowing that only the server will be able to decrypt the resulting payload\n\nbecause only it has the private key.\n\nFigure 3.7. Using two keys prevents attackers from reading messages in transit even when the encryption key is shared.\n\nThe good news is that creating an HTTPS imposter can\n\nlook exactly like creating an HTTP imposter. The only required difference is that you set the protocol to https:\n\n{\n\n\"protocol\": \"https\", \"port\": 3000\n\n}\n\nThis is great for quickly setting up an HTTPS server, but\n\nit uses a default certificate (and key pair) that ships with mountebank. That certificate is both insecure and\n\nuntrusted. Although that may be OK for some types of testing, any respectable service call should validate that\n\nthe certificate is trusted, which, as shown in figure 3.6, involves a call to a trusted CA. By default, the HTTPS\n\nlibrary that your system under test is using should reject mountebank’s built-in certificate, meaning that it won’t\n\nbe able to connect to your virtual HTTPS service.\n\nThat leaves three options. The first is that you could configure the service you’re testing not to validate that\n\nthe certificate is trusted. Don’t do this. You don’t want to risk leaving code like that in during production, and you\n\ndon’t want to test one behavior for your service (that doesn’t do a certificate validation) and deploy a\n\ncompletely different behavior to production (that does validation). The whole point of testing, after all, is to gain\n\nconfidence in what you’re sending to production, which requires that you actually test what’s going to\n\nproduction.\n\nThe second option is to avoid testing with HTTPS.\n\nInstead, create an HTTP imposter and configure your system under test to point to it. The networking libraries\n\nthat your system under test uses should support that change without any code changes, and you usually can trust them to work with HTTPS. This is a reasonable\n\noption to use when you are testing on your local machine.\n\nThe third option, shown in figure 3.8, is to use a certificate that is trusted, at least in the test environment.\n\nOrganizations can run their own CA, making trust part of the environment rather than part of the application. That\n\nallows you to set up the test instances of your virtual services with appropriately trusted certificates.\n\nFigure 3.8. Setting up a test environment with HTTPS\n\nWith this approach, the test creates the imposter with\n\nboth the certificate and the private key, as shown in the following listing. You can pass them in what is known as\n\nPEM format; we will look at how to create them shortly.\n\nListing 3.8. Creating an HTTPS imposter\n\n{\n\n\"protocol\": \"https\", \"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\\n...\",\n\n1\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\\n...\"\n\n1\n\n}\n\n1 Much abbreviated from the actual text\n\nThis setup is still insecure. The test needs to know the\n\nprivate key to create the imposter, so the imposter will know how to decrypt communication from the system\n\nunder test. The certificate is tied to the domain name in the URL, so as long as you segment that domain name to\n\nyour test environment, you aren’t risking leaking any production secrets. With appropriate environment\n\nseparation, this approach allows you to test the system under test without changing its behavior to allow untrusted certificates.\n\n3.2.1. Setting up a trusted HTTPS imposter\n\nHistorically, getting certificates trusted by a public CA has been a painful and confusing process, and it cost\n\nenough money to discourage their use in exactly the kind of lightweight testing scenarios that mountebank\n\nsupports. Using SSL is such a cornerstone of internet security that major players are pushing to change that\n\nprocess to the point where it’s easy for even hobbyists without a corporate purse to create genuine certificates\n\nfor the domains they register.\n\nLet’s Encrypt (https://letsencrypt.org/) is a free option that supports creating certificates for domains with minimal\n\nfuss, backed by a public CA. Every CA will require validation from the domain owner to ensure that no one\n\nis able to grab a certificate for a domain they don’t own. Let’s Encrypt allows you to completely automate the\n\nprocess by round-tripping a request based on a DNS lookup of the domain listed in the certificate (figure 3.9).\n\nFigure 3.9. How Letʼs Encrypt validates the domain\n\nLet’s Encrypt uses a command-line tool called Certbot\n\n(https://certbot.eff.org) to automate the creation of certificates. Certbot expects you to install a client on the\n\nmachine receiving the SSL request. The client stands up a web server and sends a request to a Let’s Encrypt\n\nserver. Let’s Encrypt in turn looks up the domain for which you are requesting a certificate in DNS and sends\n\na request to that IP address. If that request reaches the certbot server that created the first request, Let’s Encrypt\n\nhas validated that you own the domain.\n\nThe certbot command depends on the web server you are using, and because it is constantly evolving, you\n\nshould check the documentation for details. In the general case, you might run:\n\ncertbot certonly --webroot -w /var/test/petstore\n\nd test.petstore.com\n\nThat would create a certificate for the test.petstore.com domain that is served out of a web server running in\n\n/var/test/petstore. Simplifications are available if you’re using a common web server like Apache or Nginx. See\n\nhttps://certbot.eff.org/docs/using.html#getting-certificates-and- choosing-plugins for details.\n\nBy default, the directory that certbot stores the SSL information in is /etc/lets encrypt/live/$domain, where\n\n$domain is the domain name of your service. If you look in that directory, you will find a few files, but two are\n\nrelevant for our purposes: privkey.pem contains the private key, and cert.pem contains the certificate. The\n\ncontents of those two files are what you would put in the key and cert fields when creating the HTTPS imposter.\n\nA PEM file has newlines. An example certificate might\n\nlook like the following:\n\n-----BEGIN CERTIFICATE-----\n\nMIIDejCCAmICCQDlIe97PDjXJDANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJV\n\nUzEOMAwGA1UECBMFVGV4YXMxFTATBgNVBAoTDFRob3VnaHRXb3JrczEMMAoGA1UE\n\nCxMDT1NTMRMwEQYDVQQDEwptYnRlc3Qub3JnMSYwJAYJKoZIhvcNAQkBFhdicmFu\n\nZG9uLmJ5YXJzQGdtYWlsLmNvbTAeFw0xNTA1MDMyMDE3NTRaFw0xNTA2MDIyMDE3\n\nNTRaMH8xCzAJBgNVBAYTAlVTMQ4wDAYDVQQIEwVUZXhhczEVMBMGA1UEChMMVGhv\n\ndWdodFdvcmtzMQwwCgYDVQQLEwNPU1MxEzARBgNVBAMTCm1idGVzdC5vcmcxJjAk BgkqhkiG9w0BCQEWF2JyYW5kb24uYnlhcnNAZ21haWwuY29tMIIBIjANBgkqhkiG\n\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5V88ZyZ5hkPF7MzaDMvhGtGSBKIhQia2a0vW\n\n6VfEtf/Dk80qKaalrwiBZlXheT/zwCoO7WBeqh5agOs0CSwzzEEie5/J6yVfgEJb\n\nVROpnMbrLSgnUJXRfGNf0LCnTymGMhufz2utzcHRtgLm3nf5zQbBJ8XkOaPXokuE\n\nUWwmTHrqeTN6munoxtt99olzusraxpgiGCil2ppFctsQHle49Vjs88KuyVjC5AOb\n\n+P7Gqwru+R/1vBLyD8NVNl1WhLqaaeaopb9CcPgFZClchuMaAD4cecndrt5w4iuL\n\nq91g71AjdXSG6V3R0DC2Yp/ud0Z8wXsMMC6X6VUxFrbeajo8CQIDAQABMA0GCSqG\n\nSIb3DQEBBQUAA4IBAQCobQRpj0LjEcIViG8sXauwhRhgmmEyCDh57psWaZ2vdLmM ED3D6y3HUzz08yZkRRr32VEtYhLldc7CHItscD+pZGJWlpgGKXEHdz/EqwR8yVhi\n\nakBMhHxSX9s8N8ejLyIOJ9ToJQOPgelI019pvU4cmiDLihK5tezCrZfWNHXKw1hw\n\nSh/nGJ1UddEHCtC78dz6uIVIJQC0PkrLeGLKyAFrFJp4Bim8W8fbYSAffsWNATC+\n\ndVKUlunVLd4RX/73nY5EM3ErcDDOCdUEQ2fUT59FhQF89DihFG4xW4OLq42/pgmW\n\nKQBvwwfJxIFqg4fdnJUkHoLX3+glQWWrz80cauVH\n\n-----END CERTIFICATE-----\n\nYou’ll want to keep the newlines, escaped in typical JSON fashion with '\\n', in the strings you send\n\nmountebank. In this example, shortening the field for clarity, the resulting imposter configuration might look\n\nlike this:\n\n{\n\n\"protocol\": \"https\",\n\n\"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\nMIIEpAIBAAKC...\",\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\n\n\\nMIIDejCCAmICCQD...\" }\n\nNote that, although it’s awkward to show in book format,\n\nthe string would include all the way up to the end of the file (from the example, “...\\nWWrz80cauVH\\n-----END\n\nCERTIFICATE-----”) for the certificate.\n\nAnd...that’s it. Everything else about your imposter\n\nremains the same. Once you have set the certificate and private key, the SSL layer is able to convert encrypted\n\nmessages into HTTP requests and responses, which means the is responses you have already created continue to work. It may seem like a lot of work to set up\n\nthe certificates, but that’s the nature of SSL. Fortunately, tools like Let’s Encrypt and shortcuts like using wildcard\n\ncertificates simplify the process considerably.\n\nUsing wildcard certificates to simplify testing\n\nA typical certificate is associated with a single domain name, such as mypet store.com. By adding a wildcard in front of the domain, the certificate becomes valid for\n\nall subdomains. You could, for example, create a *.test.mypetstore.com certificate, and that\n\ncertificate would be valid for products.test.mypet store.com as well as\n\ninventory.test.mypetstore.com. It wouldn’t be valid for production domains that don’t include test as\n\npart of their domain name.\n\nA wildcard certificate is ideal for testing scenarios. You may find it easy to manually add a wildcard certificate to\n\nthe CA, tied exclusively to a testing subdomain, and reuse the certificate and private key for all imposters.\n\n3.2.2. Using mutual authentication\n\nIt turns out that certificates aren’t only valid for HTTPS servers; they’re also a common way to validate the\n\nidentity of clients (figure 3.10). You don’t see this when browsing the internet, because public websites have to\n\nassume the validity of the browsers that access them, but in a microservices architecture, it’s important to validate\n\nthat only authenticated clients can make a request to a server.\n\nFigure 3.10. Setting up a test environment with HTTPS to validate clients as well as servers\n\nIf the service you are testing expects to validate its\n\nidentity with your imposter using a client certificate, you need to be able to configure your imposter in a way that\n\nexpects that certificate. This is as simple as adding a mutualAuth field to the configuration set to true, as\n\nshown in the following listing.\n\nListing 3.9. Adding mutual authentication to an imposter\n\n{\n\n\"protocol\": \"https\", \"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\nMIIEpAIBAAKC...\",\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\n\n\\nMIIDejCCAmICCQD...\",\n\n\"mutualAuth\": true\n\n1\n\n}\n\n1 The server will expect a client certificate.\n\nNow the server will challenge the client with a certificate request. Using certificates, both for HTTPS and for\n\nmutual authentication, allows you to virtualize servers in secure environments. But the fact that you have to\n\nescape the PEM files in JSON gets quite clunky. Let’s look at how to make maintaining that data a bit easier\n\nusing configuration files.\n\n3.3. SAVING THE RESPONSES IN A CONFIGURATION FILE\n\nBy now, you’re probably realizing that, as the complexity of the responses and security configuration increases, the\n\nJSON that you send mountebank can be quite complex.\n\nThis is true even for a single field, like the multiline PEM files that need to be encoded as a JSON string.\n\nFortunately, mountebank has robust support for persisting the configuration in a friendly format.\n\nNow that you’ve added an inventory service and seen how to convert it to HTTPS, let’s see how you would\n\nformat the imposter configuration in files to make it easier to manage. The first bit of ugliness you’ll want to\n\nsolve is storing the certificate and private key in separate PEM files so you can avoid a long JSON string. If you store those as cert.pem and key.pem in the ssl\n\ndirectory, then you can create a file for the inventory imposter as inventory.ejs (figure 3.11).\n\nFigure 3.11. The tree structure for the secure inventory imposter configuration\n\nMountebank uses a templating language called EJS\n\n(http://www.embeddedjs.com/) to interpret the config file, which uses a fairly standard set of templating primitives. The content between <%- and %>, as shown in the following listing, is dynamically evaluated and\n\ninterpolated into the surrounding quotes. Save the following in inventory.ejs.\n\nListing 3.10. Storing the inventory service in a configuration file\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"https\",\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem') %>\", 1\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem') %>\", 1\n\n\"stubs\": [\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } },\n\n{ \"is\": { \"body\": \"21\" } },\n\n{ \"is\": { \"body\": \"0\" } }\n\n] }\n\n]\n\n}\n\n1 Converts the multiline file content into a JSON string\n\nThe inventory service will be available at startup, if you\n\nstart mountebank with the appropriate command-line flag:\n\nmb --configfile inventory.ejs\n\nMountebank adds the stringify function to the templating language, which does the equivalent of a JavaScript JSON.stringify call on the contents of the given file. In this case, the stringify call escapes the\n\nnewlines. The benefit to you is that the configuration is much easier to read. (The filename variable is passed\n\nin by mountebank. It’s a bit of a hack needed to make relative paths work.)\n\nWith those two templating primitives—the angle brackets that will be replaced by dynamic data and the stringify function to turn that data into presentable JSON—you can build robust templates. Storing the SSL\n\ninformation separately is useful, but I intentionally oversimplified the inventory imposter to focus on the behavior of the responses array. Let’s add in the product catalog and marketing content services you saw in chapter 2.\n\n3.3.1. Saving multiple imposters in the config file\n\nAs you saw, templating allows you to break up your configuration into multiple files. You’ll take advantage of\n\nthat to revisit the product catalog and marketing content imposter configurations you saw in chapter 2, putting\n\neach imposter in one or more files. The first thing you need to do is define the root configuration, which now\n\nneeds to take a list of imposters. The tree structure will look like figure 3.12.\n\nFigure 3.12. The tree structure for multiple services\n\nSave the following listing as imposters.ejs.\n\nListing 3.11. The root configuration file, referencing other imposters\n\n{\n\n\"imposters\": [\n\n<% include inventory.ejs %>, 1\n\n<% include product.ejs %>, 1\n\n<% include content.ejs %> 1\n\n]\n\n}\n\n1 Interpolates content from other files as is\n\nThe include function comes from EJS. Like the\n\nstringify function, it loads in content from another file. Unlike stringify, the include function doesn’t\n\nchange the data; it brings the data as is from the referenced file. You can use the include EJS function\n\nand the stringify mountebank function to lay out your content any way you like. For complex\n\nconfigurations, you can store the response bodies— JSON, XML, or any other complex representation—in\n\ndifferent files with newlines and load them in as needed. To keep it simple, you’ll save each imposter in its own\n\nfile, loading in the same wildcard certificate and private key. Save the product catalog imposter configuration that you saw in listing 2.1 in product.ejs, with some modifications as shown in the following listing.\n\nListing 3.12. The updated version of the product catalog imposter configuration\n\n{\n\n\"protocol\": \"https\",\n\n1 \"port\": 3001,\n\n2\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem'); %>\",\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem');\n\n%>\",\n\n\"stubs\": [{\n\n3 \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" },\n\n\"body\": {\n\n\"products\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\"\n\n}\n\n]\n\n}\n\n}\n\n}],\n\n\"predicates\": [{\n\n\"equals\": { \"path\": \"/products\" } }]\n\n}]\n\n}\n\n1 Converts to HTTPS\n\n2 Uses a different port to avoid a port conflict\n\n3 Same stub configuration as in chapter 2\n\nFinally, save the marketing content imposter configuration you saw in listing 2.7 in a file called\n\ncontent.ejs, with the modifications shown in the following listing.\n\nListing 3.13. The updated version of the marketing content imposter configuration\n\n{ \"protocol\": \"https\",\n\n\"port\": 3002,\n\n1\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem'); %>\",\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem');\n\n%>\",\n\n\"stubs\": [{ \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" },\n\n\"body\": {\n\n\"content\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"copy\": \"Treat your dog like the\n\nking he is\",\n\n\"image\": \"/content/c5b221e2\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"copy\": \"Love your fish; they'll love you back\",\n\n\"image\": \"/content/a0fad9fb\"\n\n}\n\n]\n\n}\n\n}\n\n}],\n\n\"predicates\": [{ \"equals\": {\n\n\"path\": \"/content\",\n\n\"query\": { \"ids\": \"2599b7f4,e1977c9e\" }\n\n}\n\n}]\n\n}]\n\n}\n\n1 Uses a different port\n\nNow you can start mountebank by pointing to the root configuration file:\n\nmb --configfile imposters.ejs\n\nNotice what happens in the logs:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking\n\norders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\ninfo: [mb:2525] PUT /imposters\n\ninfo: [https:3000] Open for business...\n\ninfo: [https:3001] Open for business...\n\ninfo: [https:3002] Open for business...\n\nAll three imposters are up and running. Of interest is the log entry pointing out that an HTTP PUT command was\n\nsent the mountebank URL of http://localhost:2525/imposters. After running the contents of the configuration file through EJS, the mb\n\ncommand sends the results as the request body of the PUT command, which creates (or replaces) all the\n\nimposters in one shot. Nearly every feature in mountebank is exposed via an API first, so anything you\n\ncan do on the command line, you can implement using the API. If you had more advanced persistence\n\nrequirements, you could construct the JSON and send it to mountebank using curl, as in the following listing.\n\nListing 3.14. Using curl to send the JSON to mountebank\n\ncurl -X PUT http://localhost:2525/imposters --\n\ndata '{\n\n\"imposters\": [\n\n{\n\n\"protocol\": \"https\",\n\n\"port\": 3000\n\n},\n\n{ \"protocol\": \"https\",\n\n\"port\": 3001\n\n}\n\n]\n\n}'\n\nFor clarity, I’ve left out all the important bits of the imposter configuration. You may find the PUT command\n\na convenience in automated test suites where a setup step overwrites the entire set of imposters with one API\n\ncall, rather than relying on all of the individual tests to send the DELETE calls to clean up their imposters.\n\nIf you do load the imposters through a configuration file,\n\nthe imposter setup is part of starting mountebank, which you’re expected to do before running your tests. That\n\narrangement allows you to remove some of the setup steps from the test itself—specifically, those related to\n\nconfiguring and deleting the imposters.\n\nSUMMARY\n\nThe is response type allows you to create a canned response. The fields you specify in the response object merge in with the default\n\nresponse. You can change the default response if you need to.\n\nOne stub can return multiple responses. The list of responses acts\n\nlike a circular buffer, so once the last response is returned,\n\nmountebank cycles back to the first response.\n\nHTTPS imposters are possible, but you have to create the key pair\n\nand certificate. Let’s Encrypt is a free service that lets you\n\nautomate the process.\n\nSetting the mutualAuth flag on an imposter means that it will accept client certificates used for authentication.\n\nMountebank uses EJS templating for persisting the configuration\n\nof your imposters. You load them at startup by passing the root template as the parameter to the --configfile command-line option.\n\nChapter 4. Using predicates to send diﬀerent responses\n\nThis chapter covers\n\nUsing predicates to send different responses for different requests\n\nSimplifying predicates on JSON request bodies\n\nUsing XPath to simplify predicates on XML request bodies\n\nDuring his younger years, Frank William Abagnale Jr. forged a pilot’s license and traveled the world by\n\n[1]\n\ndeadheading. successful impersonation of a pilot meant that his food\n\nDespite not being able to fly, his\n\nand lodging were fully paid for by the airline. When that well dried up, he impersonated a physician in New Orleans for nearly a year without any medical\n\nbackground, supervising resident interns. When he didn’t know how to respond after a nurse said a baby had\n\n“gone blue,” he realized the life and death implications of that false identity and decided to make yet another\n\nchange. After forging a law transcript from Harvard University, he kept taking the Louisiana bar exam until\n\nhe passed it, then he posed as an attorney with the attorney general’s office. His story was memorialized in\n\nthe 2002 movie, Catch Me If You Can.\n\n1\n\nThe term refers to a pilot riding as a passenger on a flight to get to work. For example, a pilot who took up an assignment to fly from New York to London would need to first ride as a\n\npassenger to New York if he lived in Denver.\n\nFrank Abagnale was one of the most successful imposters\n\nof all time.\n\nAlthough mountebank cannot guarantee you Abagnale’s\n\nindefatigable confidence, it does give you the ability to mimic one of the other key factors of his success:\n\ntailoring your response to your audience. Had Abagnale\n\nacted like a pilot when facing a room full of medical interns, he would have had a much shorter medical\n\ncareer. Mountebank uses predicates to determine which response to use based on the incoming request, giving\n\nyour imposter the ability pretend to be a virtual pilot for one request and a virtual doctor for the next one.\n\n4.1. THE BASICS OF PREDICATES\n\nTesting a service that depends on a fictional Abagnale service (figure 4.1) is hard work. It involves doing\n\nsomething that has almost certainly never been done before in the history of mocking frameworks: you have to\n\ncreate a virtual imposter that pretends to be a real imposter.\n\nFigure 4.1. The Abagnale service adapts its response to the questions you ask it.\n\nFortunately, mountebank makes this easy. If you assume\n\nyour system under test embeds the question it asks the Abagnale service inside the HTTP body, then your",
      "page_number": 86
    },
    {
      "number": 4,
      "title": "Using predicates to send diﬀerent responses",
      "start_page": 119,
      "end_page": 150,
      "detection_method": "regex_chapter_title",
      "content": "imposter configuration can look something like the following listing.\n\n[2]\n\n2\n\nTo keep the examples as simple as possible, we’ll use HTTP.\n\nListing 4.1. Creating an Abagnale imposter\n\n{\n\n\"protocol\": \"http\", \"port\": 3000,\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"contains\": { \"body\":\n\n\"Which route are you flying?\" }\n\n1\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"Miami to Rio\" }\n\n2\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"startsWith\": { \"body\":\n\n\"Where did you get your degree?\" }\n\n3\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"Harvard Medical School\"\n\n} 4\n\n}]\n\n}, {\n\n\"responses\": [{\n\n\"is\": { \"body\":\n\n\"I'll have to get back to you\" }\n\n5\n\n}]\n\n}\n\n] }\n\n1 When asked a pilot question...\n\n2 ...respond like a pilot.\n\n3 When asked a doctor question...\n\n4 ...respond like a doctor.\n\n5 If you don’t know what type of question it is, stall!\n\nEach predicate matches on a request field. The examples in listing 4.1 all match the body, but any of the other HTTP request fields you saw in the previous chapter are fair game: method, path, query, and headers.\n\nIn chapter 2, we showed an example using equals predicates. Our simple Abagnale imposter shows off a couple more possibilities, using contains and startsWith. We’ll look at the range of predicates\n\nshortly, but most of them are pretty self-explanatory. If the body contains the text “Which route are you flying?”\n\nthen the imposter responds with “Miami to Rio,” and if the body starts with the text “Where did you get your\n\ndegree?” the imposter responds with “Harvard Medical School.” This allows you to test the doctor and pilot\n\nservices without depending on the full talents of Mr. Abagnale.\n\nNote in particular the last stub, containing the “I’ll have to get back to you” bit of misdirection. It contains no\n\npredicates, which means that all requests will match it, including those that match the predicates in other stubs. Because a request can match multiple stubs,\n\nmountebank always picks the first match, based on array order. This allows you to represent a fallback default response by putting it at the end of the stubs array without any predicates.\n\nWe haven’t paid too much attention to stubs as a standalone concept because they only make sense in the\n\npresence of predicates. As you saw in the last chapter, it’s possible to send different responses to the exact same request, which is why the responses field is a JSON array. This simple fact, combined with the need to tailor\n\nthe response to the request, is the raison d’etre of stubs.\n\nEach imposter contains a list of stubs. Each stub contains a circular buffer for the responses. Mountebank selects\n\nwhich stub to use based on the stub’s predicates (figure 4.2).\n\nFigure 4.2. Mountebank matches the request against each stubʼs predicates.\n\nBecause mountebank uses a “first-match” policy on the stubs, having multiple stubs that could respond to the\n\nsame request isn’t a problem.\n\n4.1.1. Types of predicates\n\nLet’s take a closer look at the simplest predicate operators (figure 4.3). These all behave much as you\n\nwould expect. But a few more interesting types of predicates are available, starting with the incredibly useful matches predicate.\n\nFigure 4.3. Mountebank matches the request against each stubʼs predicates.\n\nThe matches predicate\n\nYour Abagnale service needs to respond intelligently to questions in both the present tense and the past tense.\n\n“Which route are you flying?” and “Which route did you fly?” should both trigger a response of “Miami to Rio.” You could write multiple predicates, but the matches predicate lets you simplify your configuration with the\n\nuse of a regular expression (or regex, as used in figure 4.4).\n\nFigure 4.4. How simple predicates match against the full request field\n\nRegular expressions include a wonderfully rich set of\n\nmetacharacters to simplify pattern matching. This example uses three:\n\n. — matches any character except a newline\n\n— matches the previous character zero or more times\n\n\\ — escapes the following character, matching it literally\n\nIt may look like we had to double-escape the question mark, but that’s only because \\ is a JSON string escape\n\ncharacter as well as a regular expression escape character. The first \\ JSON-escapes the second \\, which\n\nregex escapes the question mark, because it turns out that a question mark is also a special metacharacter. Like\n\nthe asterisk, it sets an expectation of the previous character or expression, but unlike the * metacharacter,\n\n? matches it only zero times or one time. If you have never seen regular expressions before, that’s a bit\n\nconfusing, so let’s break down your pattern against the\n\nrequest field value of “Which route did you fly?” (See figure 4.5.)\n\nFigure 4.5. How a regular expression matches against a string value\n\nMost characters match one-for-one against the request field. As soon as you reach the first metacharacters (.*),\n\nthe pattern matches as much as it can until the next literal character (a blank space). The wildcard pattern .*\n\nis a simple way of saying “I don’t care what they entered.” As long as the rest of the pattern matches, then\n\nthe entire regular expression matches.\n\nThe second .* matches because the * allows a zero- character match, which is another way of saying it can be\n\nsatisfied even if it doesn’t match anything. Conveniently, it also would have matched the “ing” if you had entered\n\n“flying” instead of “fly,” which is why regular expressions are so flexible. Finally, the pattern \\? matches the\n\nending question mark.\n\nThe matches predicate is one of the most versatile\n\npredicates in mountebank. With a couple of additional metacharacters, it can completely replace the other\n\npredicates we’ve looked at so far. We will demonstrate that with another example.\n\nReplacing other predicates with regular expressions\n\nA key to Abagnale’s success is that he knew when it was\n\ntime to run. After narrowly escaping arrest in New Orleans, he switched from being a pilot to a doctor with a new identity. When he realized the gravity of his medical\n\nignorance, he switched again to being a lawyer. Recognizing when the jig is up requires seeing suspicious\n\nquestions for what they are.\n\nThe Abagnale service responds with desperation when\n\nasked to see his driver’s license, saying “Catch me if you can!” It gives the same response if the questioner asks for\n\nhis “state’s driver’s license,” or his “current driver’s license,” so you’ll have to be a little loose with your\n\nmatching.\n\nYou will need to match if the question contains the phrase “driver’s license.” You want to confirm that those\n\ncharacters represent a question by ensuring not only that they contain a question mark, but that they end with a\n\nquestion mark, and, to make absolutely sure that you are catching the right question, you will also ensure that the\n\nphrase starts with “Can I see your.” Note that you can\n\nachieve all of this without regular expressions, but it would require combining multiple predicates, as follows:\n\n{\n\n\"predicates\": [\n\n{ \"startsWith\": { \"body\": \"Can I see your\" }\n\n}, { \"contains\": { \"body\": \"driver's license\" }\n\n},\n\n{ \"endsWith\": { \"body\": \"?\" } }\n\n]\n\n}\n\nYou can match exactly the same bodies with one matches predicate, as shown in the following listing.\n\nListing 4.2. Using the matches predicate to do the job of a set of startsWith, contains, and endsWith predicates\n\n{\n\n\"predicates\": [{\n\n\"matches\": { \"body\": \"^Can I see\n\nyour.*driver's license.*\\\\?\" } }],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"Catch me if you can!\" }\n\n}]\n\n}\n\nYou’ve already seen how the .* metacharacters match any characters, or no characters at all. Wrapping text\n\nwith those metacharacters on either side is equivalent to using a contains predicate (figure 4.6).\n\nFigure 4.6. Emulating the contains predicate with a regular expression\n\nThe ^ metacharacter matches only if the following character occurs at the beginning of the string, which allows you to recreate the startsWith predicate (figure 4.7).\n\nFigure 4.7. Emulating the startsWith predicate with a regular expression\n\nFinally, the $ metacharacter matches only if the preceding character occurs at the end of the string, which mimics the endsWith predicate (figure 4.8).\n\nFigure 4.8. Emulating the endsWith predicate with a regular expression\n\nThe beauty of regular expressions is that you can\n\ncombine all of those criteria into a single pattern (figure 4.9).\n\nFigure 4.9. Emulating the startsWith, contains, and endsWith predicates with one regular expression\n\nIf you remove the .* metacharacter but leave in the ^ and $ metacharacters that anchor a match at the\n\nbeginning and end of a string of text, you create what equates to the equals predicate, although in this case\n\nthe equals predicate is more readable (figure 4.10).\n\nFigure 4.10. Emulating the equals predicate with a regular expression\n\nRegular expression patterns can greatly simplify your use of predicates. Let’s look at a more common use case of the matches predicate next.\n\nMatching any identifier on the path\n\nAlthough our predicates so far have focused on the http body field, predicates can work on any request field. A\n\ncommon pattern is to match the path field. Frank Abagnale took on a number of names, and in typical\n\nRESTful fashion, your Abagnale service allows you to query them by sending a GET request to /identities,\n\nor see the details about a single persona by looking at /identities/{id}, where {id} is the identifier for\n\nthat particular identity. Let’s start by matching the /identities/{id} path.\n\nIf you had a test scenario that involved hitting this endpoint, you could use the matches predicate to match\n\nany numeric identifier passed in, as shown in the following listing.\n\nListing 4.3. Using the matches predicate to match any identity resource\n\n{\n\n\"predicates\": [{\n\n\"matches\": { \"path\": \"/identities/\\\\d+\" }\n\n}],\n\n\"responses\": [{ \"is\": {\n\n\"body\": {\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n}\n\n}\n\n}] }\n\nThe metacharacters used here, \\d+, represent one or\n\nmore digits, so the pattern will match /identities/123 and /identities/2 but not\n\nidentities/frank-williams. Several other useful metacharacters are available, including (but not limited\n\nto!) the ones listed in table 4.1.\n\nTable 4.1. Regular expression metacharacters\n\nMetacharacter\n\nDescription\n\nExample\n\n\\\n\n^\n\n$\n\n. *\n\n?\n\n+\n\n\\d \\D\n\n\\w\n\n\\W\n\n\\s\n\n\\S\n\nUnless it’s part of a metacharacter like those described below, it escapes the next character, forcing a literal match. Matches the beginning of the string\n\nMatches the end of the string\n\nMatches any non-newline character Matches the previous character 0 or more times Matches the previous character 0 or 1 times Matches the previous character 1 or more times Matches a digit Inverts \\d, matching nondigit characters Matches an alphanumeric “word” character Inverts \\w, matching nonalphanumeric symbols Matches a whitespace character (mainly spaces, tabs, and newlines) Inverts \\s, matching any non-space character\n\n4 \\* 2\\? matches “What is 4 * 2?”\n\n^Hello matches “Hello, World!” but not “Goodbye. Hello.” World!$ matches “Hello, World!” but not “World! Hello.” ..... matches “Hello” but not “Hi” a*b matches “b” and “ab” and “aaaaaab” a?b matches “b” and “ab” but not “aab”\n\na+b matches “ab” and “aaaab” but not “b” \\d\\d\\d matches “123” but not “12a” \\D\\D\\D matches “ab!” but not “123”\n\n\\w\\w\\w matches “123” and “abc” but not “ab!” \\W\\W\\W matches “!?.” but not “ab.”\n\nHello\\sworld matches “Hello world” and “Hello world” Hello\\Sworld matches “Hello-world” and “Hello----world”\n\nRegular expressions allow you to define robust patterns to match characters and are a rich subject in their own\n\nright. Several excellent books are available on the subject, and a number of internet sites provide tutorials.\n\nIf you are looking for a quick start, I recommend the tutorials on http://www.regular-expressions.info/. We look at\n\nmore examples in chapter 7.\n\n4.1.2. Matching object request fields\n\nGoogle supports full-text searching using the q querystring parameter, so, for example,\n\nhttps://www.google.com/?q=mountebank will show web pages that are somehow relevant to the search text\n\n“mountebank.” Other web services, like the Twitter API, have adopted the q parameter as a search option even\n\nwhen searching more JSON-structured data like your Abagnale service. Having a single search parameter\n\nallows a Google-like user experience with a single text box, where the user doesn’t have to specify the fields they\n\nare matching. They don’t even have to match a field completely. Implementing a full-text search can be a\n\nlittle tricky, but you don’t need to worry about that; you need to pretend to be a service that implements full-text\n\nsearching.\n\nThe /identities path for your Abagnale service\n\nsupports searching using the q querystring parameter. For example, /identities?q=Frank will search for all\n\nof Abagnale’s identities that are somehow relevant to “Frank,” which you can use as a shortcut to find those\n\nidentities where he used his real first name. The predicate for querystring parameters looks a little\n\ndifferent, but only because the querystring is an object field instead of a string field, as shown in the following\n\nlisting.\n\nListing 4.4. Adding a predicate for a query parameter\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n\"query\": { \"q\": \"Frank\" } 1\n\n}\n\n}],\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": { \"identities\": [ 2\n\n{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n},\n\n{\n\n\"name\": \"Frank Adams\", \"career\": \"Brigham Young Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\n}\n\n}]\n\n}\n\n1 Because query is an object field, the predicate value is\n\nalso an object.\n\n2 Returns an array\n\nFor HTTP requests, both query and headers are object\n\nfields. To get to the right query parameter (or header), you have to add an extra level to your predicate.\n\n4.1.3. The deepequals predicate\n\nIn some situations, you want to match only if no query\n\nparameters were passed; for example, sending a GET to /identities without any searching or paging\n\nparameters should return all identities. None of the predicates shown so far supports this scenario, as they all\n\nwork on a single request field. For more complex key- value pair structures, like HTTP queries and headers, the\n\nother predicates expect you to navigate down to the primitive field inside, like the q parameter within the\n\nquerystring we just looked at.\n\nThe deepEquals predicate matches an entire object structure, allowing you to specify an empty querystring:\n\n{\n\n\"deepEquals\": { \"query\": {} }\n\n}\n\nShortly you will see how it’s possible to combine multiple predicates, which allows you to require two query parameters. But the deepEquals predicate is the only way to guarantee that those two query parameters and\n\nnothing else are passed:\n\n{\n\n\"deepEquals\": {\n\n\"query\": {\n\n\"q\": \"Frank\", \"page\": 1\n\n}\n\n}\n\n}\n\nWith this predicate, a querystring of ?q=Frank&page=1 would match, but a querystring of ?\n\nq=Frank&page=1&sort=desc wouldn’t.\n\n4.1.4. Matching multivalued fields\n\nAnother interesting characteristic of HTTP query\n\nparameters and headers is that you can pass the same key multiple times. Your Abagnale service supports multiple q parameters and returns only those matches that satisfy all of the provided queries. For example, GET\n\n/identities?q=Frank&q=Georgia would return only Frank Williams, because Frank Adams worked in\n\nUtah.\n\n{\n\n\"identities\": [{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\" }]\n\n}\n\nAll of the predicates we looked at so far support multivalued fields, but, once again, deepEquals is significantly different from the others. If you use an equals predicate, the predicate will pass if any of the values equals the predicate value:\n\n{\n\n\"equals\": { \"query\": {\n\n\"q\": \"Frank\"\n\n}\n\n}\n\n}\n\nThe deepEquals predicate requires all of the values to match. Mountebank represents such multivalue fields as\n\n[3]\n\n[3]\n\narrays in the request. look something like this in mountebank:\n\nThis particular request would\n\n3\n\nYou also can use this trick when creating responses with multivalue fields. This is most commonly seen with the Set-Cookie response header.\n\n{\n\n\"method\": \"GET\",\n\n\"path\": \"/identities\",\n\n\"query\": {\n\n\"q\": [\"Frank\", \"Georgia\"]\n\n} }\n\nThe trick is to pass the array as the predicate value:\n\n{\n\n\"deepEquals\": {\n\n\"query\": {\n\n\"q\": [\"Georgia\", \"Frank\"] }\n\n}\n\n}\n\nNote that the order of the values doesn’t matter. You can\n\nuse the array syntax for any predicate, not just deepEquals, but deepEquals is the only one that\n\nrequires an exact match. The example in the following listing demonstrates the difference.\n\nListing 4.5. Using predicate arrays\n\n{\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": {\n\n1 \"query\": { \"q\": [\"Frank\", \"Georgia\"] }\n\n1\n\n}\n\n1\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"deepEquals matched\" }\n\n}]\n\n},\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n2\n\n\"query\": { \"q\": [\"Frank\", \"Georgia\"] }\n\n2\n\n}\n\n2\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"equals matched\" }\n\n}]\n\n}\n\n]\n\n}\n\n1 Requires exact match\n\n2 Requires these elements to be present\n\nIf you send a request to /identities?q=Georgia&q=Frank, the response body will show that the deepEquals\n\npredicate matched because all of the array elements matched and no additional array elements were present\n\nin the request. But if you send a request to /identities? q=Georgia&q=Frank&q=Doctor, the deepEquals\n\npredicate will no longer match because the predicate definition isn’t expecting “Doctor” as an array element. The equals predicate will match, because it allows additional elements in the request array that aren’t\n\nspecified in the predicate definition.\n\n4.1.5. The exists predicate\n\nThere’s one more primitive predicate to look at. The exists predicate tests for either the existence or\n\nnonexistence of a request field. If you have a test that depends on the q parameter to be passed and the page\n\nparameter not being passed, then exists is what you want:\n\n{\n\n\"exists\": {\n\n\"query\": {\n\n\"q\": true,\n\n\"page\": false\n\n}\n\n} }\n\nThe exists predicate also comes in quite handy when\n\nyou want to check for the presence of a header. For example, you may decide that for testing purposes, you\n\nwant to verify that the service handles an HTTP challenge correctly (represented by a 401 status code) when the Authorization request header is missing, without worrying about whether the credentials stored in the Authorization header are correct, as shown here:\n\n{\n\n\"predicates\": [{\n\n\"exists\": {\n\n\"headers\": { \"Authorization\": false }\n\n}\n\n}], \"responses\": [{\n\n\"is\": { \"statusCode\": 401 }\n\n}]\n\n}\n\nThe headers field in this snippet specifies the condition\n\nthat there’s no Authorization header, and the is response returns a 401 status code.\n\nThe exists predicate works on string fields like the\n\nbody, which is considered not to exist if it’s an empty string. It’s usually more useful in conjunction with the\n\nJSON or XML support described later in this chapter.\n\n4.1.6. Conjunction junction\n\nThe predicates field is an array. Every predicate in the array must match for mountebank to use that stub. You can reduce the array to a single element by using the and predicate, so the following two sets of predicates will\n\nmatch the exact same requests (for example, one with a body of “Frank Abagnale”):\n\n{\n\n\"predicates\": [\n\n{ \"startsWith\": { \"body\": \"Frank\" } },\n\n{ \"endsWith\": { \"body\": \"Abagnale\" } } ]\n\n}\n\nand\n\n{\n\n\"predicates\": [{ \"and\": [\n\n{ \"startsWith\": { \"body\": \"Frank\" } },\n\n{ \"endsWith\": { \"body\": \"Abagnale\" } }\n\n]\n\n}]\n\n}\n\nBy itself, the and predicate isn’t very useful. But combined with its conjunction cousin, the or predicate,\n\nand its distant disjunction relative, the not predicate, you can create predicates of dizzying complexity in a\n\nfestival of Booleanism. For example, the predicate in listing 4.6 matches requests that return Frank Williams,\n\nregardless of whether the system under test directly calls that persona URL (assumed to be /identities /123)\n\nor they search for him at /identities? q=Frank+Williams, but only if no page query\n\nparameter is added.\n\nListing 4.6. Combining multiple predicates using and, or, and not\n\n{\n\n\"or\": [\n\n{ \"equals\": { \"path\": \"/identities/123\" } },\n\n1\n\n{\n\n\"and\": [\n\n{ \"equals\": { \"path\": \"/identities\" } }, 2\n\n{\n\n\"and\": [\n\n{\n\n\"contains\": { \"query\": { \"q\":\n\n\"Frank\" } } 3\n\n},\n\n{ \"contains\": { \"query\": { \"q\":\n\n\"Williams\" } } 4\n\n},\n\n{\n\n\"not\": {\n\n5\n\n\"exists\": { \"query\": { \"page\":\n\ntrue } } 5 }\n\n5\n\n}\n\n]\n\n}\n\n]\n\n}\n\n] }\n\n1 Matches if the request goes directly to the URL...\n\n2 ...or if it searches\n\n3 ...with a query containing Frank\n\n4 ...and a query containing Williams\n\n5 ...with no paging (you can get rid of the not predicate by\n\nchanging the page value to false).\n\nSometimes it’s necessary to create complex conditions, and the rich set of predicates mountebank supports\n\nenables you to specify such conditions. But to make your configuration readable and maintainable, it’s good\n\npractice to make the predicates as simple as possible for your use case. The conjunctions are there when you need\n\nthem, but you’ll probably be happier if you can avoid using them too much.\n\n4.1.7. A complete list of predicate types\n\nThere’s one other predicate you haven’t seen yet—the inject predicate—but you’ll have to wait until chapter 6 to take a look at it. Before we proceed, let’s review the\n\npredicates you have at your disposal. For your reference, table 4.2 provides the complete list of predicate operators\n\nthat mountebank supports.\n\nTable 4.2. All predicates that mountebank supports\n\nOperator\n\nDescription\n\nequals deepEquals contains startsWith endsWith Matches\n\nexists\n\nnot or and inject\n\nRequires the request field to equal the predicate value Performs nested set equality on object request fields Requires the request field to contain the predicate value Requires the request field to start with the predicate value Requires the request field to end with the predicate value Requires the request field to match the regular expression provided as the predicate value Requires the request field to exist as a nonempty value (if true) or not (if false) Inverts the subpredicate Requires any of the subpredicates to be satisfied Requires all of the subpredicates to be satisfied Requires a user-provided function to return true (see chapter 6)\n\n4.2. PARAMETERIZING PREDICATES\n\nEach predicate consists of an operator and zero or more parameters that alter the behavior of the predicate in certain ways. Two parameters, xpath and jsonpath, change the scope of the predicate to a value embedded in\n\nthe HTTP body; we look at those shortly. The other parameter affects the way the predicate evaluates the\n\nrequest field.\n\n4.2.1. Making case-sensitive predicates\n\nAll predicates are case-insensitive by default. The following predicate, for example, will be satisfied regardless of whether the q parameter is “Frank” or “frank” or “FRANK”:\n\n{\n\n\"equals\": {\n\n\"query\": { \"q\": \"frank\" }\n\n}\n\n}\n\nThis is even true for the matches predicate. Regular expressions are case-sensitive by default, but\n\nmountebank changes their default to match the behavior of the other predicates. If you need case sensitivity, you can set the caseSensitive parameter to true, as follows.\n\nListing 4.7. Using a case-sensitive predicate\n\n{\n\n\"equals\": {\n\n\"query\": { \"q\": \"Frank\" }\n\n},\n\n\"caseSensitive\": true\n\n}\n\n“FRANK” and “frank” will no longer satisfy the predicate.\n\nMountebank’s default behavior also treats the keys in a case-insensitive manner, so that without the caseSensitive parameter, the predicate above also would match a querystring of ?Q=FRANK. This is often\n\nappropriate, especially for HTTP headers where the case of the headers shouldn’t matter. Adding the caseSensitive parameter forces case sensitivity on both the keys and the values.\n\n4.3. USING PREDICATES ON JSON VALUES\n\nJSON is the lingua franca of most RESTful APIs these\n\ndays. As you have seen previously, it’s possible to create mountebank responses that use JSON objects instead of strings for the body field. Mountebank also provides ample support for creating predicates against JSON\n\nbodies.\n\n4.3.1. Using direct JSON predicates\n\nDespite Frank Abagnale’s cleverness, there’s nothing\n\nmagical about what he does. When he needs to add a new identity, for example, he POSTs the JSON representation\n\nof the identity to the /identities URL, like the rest of us.\n\nBecause the HTTP body is a string as far as mountebank is concerned, you can use a contains predicate to capture a particular JSON field. But doing so is\n\ninconvenient, as the white space has to match between the key and the value. The matches predicate gives you\n\na lot more flexibility at the cost of readability. Fortunately for you, mountebank’s willingness to treat\n\nHTTP bodies as JSON as well as strings allows you to navigate the JSON object structure like you have navigated the query object structure previously, as shown in the following listing.\n\nListing 4.8. Using a direct JSON predicate on an HTTP body\n\n{\n\n\"predicates\": [{\n\n\"equals\": {\n\n\"body\": { 1\n\n\"career\": \"Doctor\" 1\n\n} 1\n\n} }],\n\n\"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 201,\n\n\"headers\": { \"Location\": \"/identities/123\"\n\n},\n\n\"body\": \"Welcome, Frank Williams\"\n\n} }]\n\n}\n\n1 The “career” field at the root of the JSON body has to\n\nequal “Doctor.”\n\nYou can navigate as many levels deep in the JSON object as you need to. Mountebank treats arrays no differently\n\nfrom how it treats the multivalued fields described in section 4.1.4, which used the example of a repeating key\n\non the querystring. For complex queries, you are probably better off using JSONPath.\n\n4.3.2. Selecting a JSON value with JSONPath\n\nYou can initialize the set of identities that the Abagnale service provides by sending a PUT command to the\n\n/identities path, passing in an array of identities, as shown here:\n\n{ \"identities\": [\n\n{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n},\n\n{\n\n\"name\": \"Frank Adams\", \"career\": \"Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\nThis test scenario requires you to send a 400 if the PUT command includes a career of “Teacher” as the last\n\nmember of the array and a 200 otherwise. That is obviously a bit of a stretch, but it enables you to show off\n\nthe power of JSONPath. JSONPath is a query language that simplifies the task of selecting values from a JSON\n\ndocument and excels with large and complex documents. Stefan Goessner came up with the idea and documented\n\nits syntax at http://goessner.net/articles/JsonPath/.\n\nLet’s look at the entire imposter configuration in the\n\nfollowing listing.\n\nListing 4.9. Using JSONPath to match only the last element of an array\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000, \"stubs\": [{\n\n\"predicates\": [\n\n{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } },\n\n1\n\n{\n\n\"jsonpath\": { 2\n\n\"selector\": \"$.identities[(@.length-\n\n1)].career\" 2\n\n},\n\n2\n\n\"equals\": { \"body\": \"Teacher\" }\n\n3\n\n} ],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 }\n\n}] 4\n\n}]\n\n5\n\n}\n\n1 Only matches a PUT request to /identities\n\n2 Limits the scope of the predicate to the JSONPath\n\nquery\n\n3 The JSON value selected within the body must equal\n\n“Teacher.”\n\n4 Returns a 400 status code\n\n5 Returns the built-in default 200 response if the\n\npredicate doesn’t match\n\nYou set the predicate operator to have the body equal\n\nTeacher, even though the body contains an entire JSON document. The jsonpath parameter modifies its\n\nattached predicate and limits its scope to the result of the query. Let’s take a look at the query as annotated in figure\n\n4.11.\n\nFigure 4.11. Breaking down a JSONPath query\n\nJSONPath provides tremendous flexibility for selecting\n\nthe value you need in a large JSON document. Its older cousin, XPath, does the same thing for XML documents.\n\n4.4. SELECTING XML VALUES\n\nAlthough XML isn’t as common in services created in\n\nrecent years, it’s still a prevalent service format and is universally used for SOAP services. If you allowed sending XML as well as JSON with the PUT call to /identities that we just looked at—perhaps because\n\nAbagnale needs to impersonate an enterprise architect— you could expect a body that looks like this:\n\n<identities>\n\n<identity career=\"Doctor\">\n\n<name>Frank Williams</name> <location>Georgia</location>\n\n</identity>\n\n<identity career=\"Teacher\">\n\n<name>Frank Adams</name>\n\n<location>Utah</location>\n\n</identity>\n\n</identities>\n\nBrigham Young University disputes Abagnale’s account of teaching there. Our test scenario could detect\n\nAbagnale trying to claim he was a teacher in Utah and send a 400 status code. This is a complex enough query\n\nthat the existing predicate operators are not up to the task.\n\n[4]\n\nIt also involves using XML attributes in the\n\nquery.\n\n4\n\nNot even the matches predicate, because you cannot parse XML (or HTML) with regular\n\nexpressions. Trying to parse XML with regular expressions causes the unholy child to weep the blood of virgins: http://stackoverflow.com/questions/1732348/regex-match-open-tags-except- xhtml-self-contained-tags/1732454#1732454.\n\nIn the simple case shown in the following listing, the xpath parameter mirrors the jsonpath parameter,\n\nlimiting the scope of what the predicate operator examines.\n\nListing 4.10. Using XPath to prevent Abagnale from claiming he taught in Utah\n\n{\n\n\"predicates\": [\n\n{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } }, 1\n\n{\n\n\"xpath\": {\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\" 2\n\n},\n\n\"equals\": { \"body\": \"Utah\" } 3\n\n}\n\n],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 } }]\n\n4\n\n}\n\n1 Verifies this is a PUT to /identities\n\n2 Limits the predicate to the given value\n\n3 The value must equal “Utah.”\n\n4 Returns a Bad Request\n\nXPath predates JSONPath, and unsurprisingly, their\n\nsyntax is similar. Figure 4.12 breaks down the XPath expression.\n\nFigure 4.12. Breaking down an XPath query\n\nOne of the most unfortunate design decisions in XML is\n\nthe support for namespaces, which has caused needless harm to walls the world over as programmers\n\neverywhere banged their heads on them when confronted with namespaced documents. The idea is\n\nsensible enough: as you combine multiple XML documents, you need a way of resolving naming\n\nconflicts.\n\n[5]\n\n5\n\nAt least we thought we did. Oddly, the same problem should exist with JSON documents, which lack namespaces, but...\n\nLet’s future-proof your XML document by adding\n\nnamespaces:\n\n<identities xmlns:id=\"https://www.abagnale-\n\nspec.com/identity\"\n\nxmlns:n=\"https://www.abagnale-\n\nspec.com/name\">\n\n<id:identity career=\"Doctor\">\n\n<n:name>Frank Williams</n:name>\n\n<location>Georgia</location> </id:identity>\n\n<id:identity career=\"Teacher\">\n\n<n:name>Frank Adams</n:name>\n\n<location>Utah</location>\n\n</id:identity>\n\n</identities>\n\nDespite your best attempts at future-proofing, you still require a breaking change in version 2 of the Abagnale\n\nservice that moves the name value to an attribute instead of an XML tag:\n\n<identities xmlns:id=\"https://www.abagnale-\n\nspec.com/identity\" xmlns:n=\"https://www.abagnale-\n\nspec.com/name\">\n\n<id:identity career=\"Doctor\" n:name=\"Frank\n\nWilliams\">\n\n<location>Georgia</location>\n\n</id:identity>\n\n<id:identity career=\"Teacher\" n:name=\"Frank\n\nAdams\"> <location>Utah</location>\n\n</id:identity>\n\n</identities>\n\nYou need to write a test scenario that verifies that the\n\nAbagnale service responds with a 400 if you pass the name field in the wrong spot, as in the following listing.\n\nThis is a great opportunity to use the exists predicate operator. You also have to add namespaces to your query, as name isn’t the same as n:name in XML.\n\nListing 4.11. Using XPath to assert that the name attribute exists and the name tag doesnʼt\n\n{\n\n\"predicates\": [\n\n{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } },\n\n1\n\n{ \"or\": [\n\n2\n\n{\n\n\"xpath\": {\n\n\"ns\": {\n\n\"i\":\n\n\"https://www.abagnale-\n\nspec.com/identity\", 3 \"n\": \"https://www.abagnale-\n\nspec.com/name\" 3\n\n},\n\n\"selector\": \"//i:identity/n:name\"\n\n4\n\n},\n\n\"exists\": { \"body\": true }\n\n4 },\n\n{\n\n\"xpath\": {\n\n\"selector\": \"//i:identity[@n:name]\",\n\n5\n\n\"ns\": {\n\n\"i\": \"https://www.abagnale-\n\nspec.com/identity\", \"n\": \"https://www.abagnale-\n\nspec.com/name\"\n\n}\n\n},\n\n\"exists\": { \"body\": false }\n\n5\n\n}\n\n] }\n\n],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 } }]\n\n6\n\n}\n\n1 Verifies this is a PUT to /identities\n\n2 There are two possible scenarios.\n\n3 Adds the namespace map\n\n4 Matches if the n:name tag exists...\n\n5 ...or if the n:name attribute doesn’t exist\n\n6 Sends a Bad Request if the predicates match.\n\nThe xpath parameter allows you to pass in a namespace\n\nmap in the ns field, which takes a prefix and a URL. The URL has to match the one defined in the XML document,\n\nbut the prefix can be whatever you want. XPath namespace queries use the prefix in front of each\n\nelement.\n\nAnd with that beast of a stub, it’s time to take a step back and review what you learned.\n\nSUMMARY\n\nPredicates allow mountebank to respond differently to different\n\nrequests. Mountebank ships with a full range of predicate operators, including the versatile matches operator, which matches request fields based on a regular expression.\n\nThe deepEquals predicate operator is used to match an entire object structure, such as the query object. You also can match a\n\nsingle field within the object (for example, a single query parameter) with one of the standard predicate operators by\n\nnavigating into the object structure.\n\nPredicates are case-insensitive by default. You can change that by setting the caseSensitive predicate parameter to true.\n\njsonpath and xpath predicate parameters limit the scope on the request field to the part that matches the JSONPath or XPath\n\nselector.\n\nChapter 5. Adding record/replay behavior\n\nThis chapter covers\n\nUsing proxy responses to capture real responses automatically\n\nReplaying the saved responses with the correct predicates\n\nCustomizing proxies by changing the headers or using mutual\n\nauthentication\n\nThe best imposters don’t simply pretend to be someone\n\nelse; they actively copy the person they impersonate. This mimicry requires both observation and memory:\n\nobservation to study the behaviors of the person being impersonated and memory to be able to replay those\n\nbehaviors at a later time. Satirists on comedy shows like Saturday Night Live, where actors and actresses often\n\npretend to be famous U.S. political figures, base their performances on those skills.\n\nMountebank lacks the comedic flair of Saturday Night Live impersonators, but it does support a high-fidelity\n\nform of mimicry. Rather than creating a canned response for each request, an imposter can go directly to the source. It’s as if the imposter is wearing an earpiece, and\n\nevery time your system under test asks it a question, the real service whispers the answer in your imposter’s ear.\n\nBetter yet, mountebank imposters have a great memory, so once the imposter has heard a response, it can replay\n\nthe response in the future even without the earpiece. Thanks to the magic of proxy responses, a mountebank\n\nimposter can be almost indistinguishable from the real thing.\n\n5.1. SETTING UP A PROXY\n\nThe proxy response type lets you put a mountebank imposter between the system under test and a real\n\nservice that it depends on, saving a real response in the process (figure 5.1).\n\nFigure 5.1. An imposter acting as a proxy\n\nThis arrangement allows capturing real data that you can replay in tests, rather than hand-creating it using canned\n\nresponses. To illustrate, let’s revisit the imaginary pet store architecture you first saw in chapter 3. The pet store,\n\nlike all modern e-commerce shops, needs a service to keep track of inventory, and, to keep it simple, ours takes\n\na product ID on the URL and sends back the on-hand stock for that product. In chapter 3, you virtualized it with hand-created is response types. Let’s reimagine it using a proxy, which requires the real service to be available to\n\ncapture the responses, as shown in figure 5.2.\n\nFigure 5.2. Using a proxy to query the downstream inventory\n\nThe simplest imposter configuration looks like the following listing.\n\n[1]\n\n1\n\nYou can follow the examples from the GitHub repository at https://github.com/bbyars/mountebank-in-action.\n\nListing 5.1. Imposter configuration for a basic proxy",
      "page_number": 119
    },
    {
      "number": 5,
      "title": "Adding record/replay behavior",
      "start_page": 151,
      "end_page": 180,
      "detection_method": "regex_chapter_title",
      "content": "{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"http://api.petstore.com\"\n\n} 1 }]\n\n}]\n\n}\n\n1 New response type\n\nThe difference is in the new response type. Whereas an is response tells the imposter to return the given response, a proxy response tells the imposter to fetch\n\nthe response from the downstream service. The basic form of the proxy response shown in the listing passes\n\nthe request unchanged to the downstream service and sends the response unchanged back to the system under\n\ntest. By itself, that isn’t a very useful thing, but the proxy remembers the response and will replay it the next time it sees the same request, rather than fetching a new\n\nresponse (figure 5.3).\n\nFigure 5.3. By default, the proxy returns the first result as the response to all subsequent calls.\n\nMountebank exposes the current state of each imposter through the API. If you send a GET request to http://localhost:2525/imposters/3000, you will see the\n\n[2]\n\nsaved response. imposter configuration in some detail after the first call\n\nIt’s worth looking at the changed\n\nto the proxy, as shown in the following listing.\n\n2\n\nThe 3000 at the end of the URL is the imposter’s port.\n\nListing 5.2. Saved proxy responses change imposter state\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"numberOfRequests\": 1,\n\n\"requests\": [], \"stubs\": [{\n\n\"predicates\": [],\n\n1\n\n\"responses\": [{\n\n\"is\": {\n\n2\n\n\"statusCode\": 200,\n\n\"headers\": { \"Connection\": \"close\",\n\n\"Date\": \"Sat, 15 Apr 2017 17:04:02\n\nGMT\",\n\n\"Transfer-Encoding\": \"chunked\"\n\n},\n\n\"body\": \"54\",\n\n\"_mode\": \"text\",\n\n\"_proxyResponseTime\": 10 3\n\n}\n\n}]\n\n},\n\n{\n\n4\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyOnce\"\n\n5\n\n}\n\n}]\n\n}],\n\n\"_links\": {\n\n\"self\": { \"href\":\n\n\"http://localhost:2525/imposters/3000\" 6\n\n}\n\n}\n\n}\n\n1 No predicates? We’ll come back to that....?\n\n2 Saves the response as an is response\n\n3 Saves the time to call the downstream service\n\n4 Your original stub is still there.\n\n5 Only calls downstream once\n\n6 The URL you called to get the configuration\n\nThere’s a lot in there, and we aren’t quite ready to get to all of it yet. Mountebank recorded the time it took to call the downstream service in the _proxyResponseTime\n\nfield; you can use this to add simulated latency during performance testing. We explore how to do that in\n\nchapters 7 and 10. The most important observations for now are:\n\nMountebank proxies the first call to the base URL given in the to field of the proxy configuration. It appends the request path and query parameters and passes through the request headers and\n\nbody unchanged.\n\nMountebank captures the response as a new stub with an is response. It saves it in front of the stub with the proxy response. (This is what the proxyOnce mode means; we will look at the alternative shortly.)\n\nThe newly created stub has no predicates. Because mountebank\n\nalways uses the first match when iterating over stubs, it will never\n\nagain call the proxy response, because a stub with no predicates\n\nalways matches.\n\nProxies change the state of the imposter. By default, they create a new stub (figure 5.4).\n\nFigure 5.4. The proxy saves the downstream response in a new stub.\n\nThe default behavior of a proxy (defined by the proxyOnce mode) is to call the downstream service the\n\nfirst time it sees a request it doesn’t recognize, and from that point forward to send the saved response back for\n\nfuture requests that look similar. Unfortunately, the example we’ve been considering isn’t discriminatory in\n\nhow it recognizes requests; all requests match the generated stub. Let’s fix that.\n\n5.2. GENERATING THE CORRECT PREDICATES\n\nProxies will create new responses formed from the downstream service response, but you need to give them\n\nhints on how to create the request predicates that determine when mountebank will replay those\n\nresponses. We will start by looking at how you can replay different responses for different request paths.\n\n5.2.1. Creating predicates with predicateGenerators\n\nYour inventory service includes the product id on the path, so sending a GET to /inventory/2599b7f4 returns the inventory for product 2599b7f4, and a GET to\n\n/inventory/e1977c9e returns inventory for product e1977c9e. Let’s augment the proxy definition you set up\n\nin listing 5.1 to save the response for each product separately. Because it’s the path that varies between\n\nthose requests, you need to tell mountebank to create a new is response with a path predicate. You do so using\n\na proxy parameter called predicateGenerators. As the name indicates, the predicateGenerators are\n\nresponsible for creating the predicates on the saved responses. You add an object for each predicate you want to generate underneath a matches key, as in the following listing.\n\nListing 5.3. Imposter response that saves a diﬀerent response for each path\n\n{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\", 1\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true 2\n\n} }]\n\n}\n\n}\n\n1 Proxy new paths to the given base URL...\n\n2 ...but generate a predicate for each new path\n\nYou can test it out with a couple calls using different\n\npaths:\n\ncurl http://localhost:3000/inventory/2599b7f4\n\ncurl http://localhost:3000/inventory/e1977c9e\n\nLet’s look at the state of the imposter again after the\n\nchange:\n\ncurl http://localhost:2525/imposters/3000\n\nThe stubs field contains the newly created responses\n\nwith their predicates, as in the following listing.\n\nListing 5.4. Saved proxy responses with predicates\n\n{ \"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/2599b7f4\" } 1\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"54\" } 2\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/e1977c9e\" } 3\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"100\" }\n\n4\n\n}]\n\n}, {\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true\n\n} }],\n\n\"mode\": \"proxyOnce\"\n\n}\n\n}]\n\n}\n\n]\n\n}\n\n1 Saves the first call as the first stub\n\n2 To save space, most of the response isn’t shown.\n\n3 Creates a new stub with a different predicate\n\n4 The response will be different.\n\nThe generated predicates use deepEquals for most\n\ncases. Recall that the deepEquals predicate requires that all fields be present for object fields like query and\n\nheaders, so if you include either of those using the simple syntax shown in listing 5.4, the complete set of\n\nquerystring parameters or request headers would have to be present in a subsequent request for mountebank to\n\nserve the saved response:\n\n{\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true,\n\n\"query\": true 1 }\n\n}]\n\n}\n\n1 All query parameters will need to match.\n\nAs you saw in the last chapter, when defining predicates\n\nfor object fields, you can be more specific if you need to be. If, for example, you want to save a different response for each different path and page query parameter, regardless of what else is on the querystring, you navigate into the query object:\n\n{\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true,\n\n\"query\": { \"page\": true 1\n\n}\n\n}\n\n}]\n\n}\n\n1 Only the page parameter needs to match\n\n5.2.2. Adding predicate parameters\n\nThe predicateGenerators field closely mirrors the\n\nstandard predicates field and accepts all the same parameters. Each object in the predicateGenerators\n\narray generates a corresponding object in the newly created stub’s predicates array. If, for example, you\n\nwanted to generate a case-sensitive match of the path and a case-insensitive match of the body, you could add\n\ntwo predicateGenerators, as shown in the following listing.\n\nListing 5.5. Generating case-sensitive predicates\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [\n\n{ 1\n\n\"matches\": { \"path\": true }, 1 \"caseSensitive\": true 1\n\n}, 1\n\n{ 2\n\n\"matches\": { \"body\": true } 2\n\n} 2\n\n] }\n\n}]\n\n}\n\n1 Generates a case-sensitive predicate\n\n2 Generates a default case-insensitive predicate\n\nThe newly created stub has both predicates:\n\n{\n\n\"predicates\": [\n\n{ \"caseSensitive\": true,\n\n\"deepEquals\": { \"path\": \"...\" }\n\n},\n\n{\n\n\"deepEquals\": { \"body\": \"...\" }\n\n}\n\n],\n\n\"responses\": [{ \"is\": { ... }\n\n}]\n\n}\n\nAs you saw in the last chapter, more parameters are available beyond caseSensitive. The jsonpath and xpath predicate parameters allow you to limit the scope\n\nof the predicate, and you can generate those too.\n\nGenerating JSONPath predicates\n\nIn chapter 4, we demonstrated JSONPath predicates in the context of virtualizing the inimitable Frank Abagnale\n\nservice, which showed a list of fake identities the famous (real) imposter had assumed. A partial list of identities might look like this:\n\n{\n\n\"identities\": [\n\n{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n}, {\n\n\"name\": \"Frank Adams\",\n\n\"career\": \"Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\nIf you needed a predicate that matched the career field of the last element of the identities array, then you\n\ncould use the same JSONPath selector you saw in the previous chapter. Because you now want mountebank to\n\ngenerate the predicate, you specify the selector in the predicateGenerators object and rely on the proxy to\n\nfill in the value, as in the following listing.\n\nListing 5.6. Specifying a jsonpath predicateGenerators\n\n{ \"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [{\n\n\"jsonpath\": {\n\n1\n\n\"selector\": \"$.identities[(@.length-\n\n1)].career\" 1\n\n}, 1\n\n\"matches\": { \"body\": true }\n\n2\n\n}]\n\n}\n\n}\n\n1 Saves the value defined by the selector...\n\n2 ...from the body field.\n\nRemember, predicateGenerators work on the incoming request, so the JSONPath selector saves off the\n\nvalue in the request body. If you sent the Abagnale JSON\n\nin listing 5.6 to your proxy, the generated stub would look something like this:\n\n{\n\n\"predicates\": [{\n\n\"jsonpath\": {\n\n\"selector\": \"$.identities[(@.length- 1)].career\"\n\n},\n\n\"deepEquals\": { \"body\": \"Teacher\" }\n\n1\n\n}],\n\n\"responses\": [{\n\n\"is\": { ... }\n\n}] }\n\n1 The value captured from the selector in the incoming\n\nrequest body\n\nFuture requests that match the given selector will use the\n\nsaved response.\n\nGenerating XPath predicates\n\nThe same technique works for XPath. If you translate Abagnale’s list of identities to XML, it might look like:\n\n<identities>\n\n<identity career=\"Doctor\"> <name>Frank Williams</name>\n\n<location>Georgia</location>\n\n</identity>\n\n<identity career=\"Teacher\">\n\n<name>Frank Adams</name>\n\n<location>Utah</location>\n\n</identity>\n\n</identities>\n\nThe predicateGenerators mirrors the xpath predicate you saw in chapter 4, so if you had a need for a\n\npredicate to match on the location where Abagnale pretended to be a teacher, the following listing would do\n\nthe trick.\n\nListing 5.7. Specifying an xpath predicateGenerators\n\n{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\", \"predicateGenerators\": [{\n\n\"xpath\": {\n\n1\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\" 1\n\n},\n\n1\n\n\"matches\": { \"body\": true } 2\n\n}]\n\n}\n\n}\n\n1 Saves the value defined by the selector...\n\n2 ...from the body field.\n\nThe predicates that the proxy creates show the correct\n\nlocation:\n\n{\n\n\"predicates\": [{\n\n\"xpath\": {\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\"\n\n}, \"deepEquals\": { \"body\": \"Utah\" } 1\n\n}],\n\n\"responses\": [{\n\n\"is\": { ... }\n\n}]\n\n}\n\n1 The value captured from the selector in the incoming\n\nrequest body\n\nCapturing multiple JSONPath or XPath values\n\nJSONPath and XPath selectors both can capture multiple\n\nvalues. To take a simple example, look at the following XML:\n\n<doc>\n\n<number>1</number>\n\n<number>2</number>\n\n<number>3</number>\n\n</doc>\n\nIf you use the XPath selector of //number on this XML document, you get three values: 1, 2, and 3. The predicateGenerators field is smart enough to capture multiple values and save them using a standard\n\npredicate array, which requires all results to be present in subsequent requests to match but allows them to be\n\npresent in any order.\n\n5.3. CAPTURING MULTIPLE RESPONSES FOR THE SAME REQUEST\n\nThe examples we looked at so far have been great for minimizing traffic to a downstream service while still\n\ncollecting real responses. For each type of request, defined by the predicateGenerators, you only pass\n\nthe request to the real service once. This is the promise of the default mode, appropriately called proxyOnce. The\n\nguarantee is satisfied by ensuring that mountebank creates new stubs before the stub with the proxy\n\nresponse (figure 5.5). Mountebank’s first match policy will ensure that subsequent requests matching the\n\ngenerated predicates don’t reach the proxy response.\n\nFigure 5.5. In proxyOnce mode, mountebank creates new stubs before the stub with the proxy.\n\nA significant downside to proxyOnce is that each\n\ngenerated stub can have only one response. This is a problem for your inventory service, which returns\n\ndifferent responses over time for the same request, reflecting volatility in stock for an item (figure 5.6).\n\nFigure 5.6. Volatile responses for the same request\n\nIn proxyOnce mode, mountebank captures only the\n\nfirst response (54). If your test cases relied on the volatility of inventory over time, you’d need a proxy that\n\nwould let you capture a richer data set to replay. The proxyAlways mode ensures that all requests reach the\n\ndownstream service, allowing you to capture multiple responses for a single request type (figure 5.7).\n\nFigure 5.7. In proxyAlways mode, new stubs are created a\u0000er the stub with the proxy\n\nCreating this type of proxy is as simple as passing in the\n\nmode, as in the following listing.\n\nListing 5.8. Creating a proxyAlways proxy response\n\n{\n\n\"proxy\": { \"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\", 1\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true }\n\n}]\n\n}\n\n}\n\n1 Ensures that all responses are captured\n\nThe key difference in the mechanics between proxyOnce and proxyAlways, as shown in figures 5.5\n\nand 5.7, is that proxyOnce generates new stubs before\n\nthe stub containing the proxy response, whereas proxyAlways generates stubs after the proxy stub.\n\nBoth approaches rely on mountebank’s first-match policy when matching a request to a stub. In the proxyOnce\n\ncase, a subsequent request matching the generated predicates is guaranteed to match before the proxy stub, and in the proxyAlways case, the proxy stub is guaranteed to match before the generated stubs.\n\nBut proxyAlways does more than create new stubs. It first looks to see if a stub whose predicates already exists\n\nmatch the generated predicates, and, if so, it appends the saved response to that stub. This behavior allows\n\nmultiple responses to be saved for the same request. You can see this by calling the imposter in listing 5.8 a few times and querying its state (by sending a GET request to http://localhost:2525/imposters/3000, assuming it was started on port 3000). To save space and make the\n\nsalient bits stand out, I’ve omitted the full response inside each generated is response in the following\n\nlisting.\n\nListing 5.9. The imposter state a\u0000er a few calls to a proxyAlways response\n\n{\n\n\"stubs\": [\n\n{\n\n\"responses\": [{ \"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n1\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true }\n\n}]\n\n} }]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/2599b7f4\" } 2\n\n}],\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } },\n\n3\n\n{ \"is\": { \"body\": \"21\" } },\n\n3\n\n{ \"is\": { \"body\": \"0\" } } 3\n\n]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/e1977c9e\" } 4\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"100\" }\n\n5\n\n}]\n\n}\n\n]\n\n}\n\n1 Ensures that all requests are proxied\n\n2 First request type\n\n3 All responses saved\n\n4 Second request type\n\n5 Only one response\n\nA proxyAlways proxy allows you to capture a full set of\n\ntest data that is as rich as your downstream service (at least for the requests sent to it). Although this is great for\n\nsupporting complex test cases, it comes with a significant problem. As you can see in listing 5.9, none of the saved responses will ever get called. With proxyOnce, you don’t need to worry about switching from recording to\n\nreplaying; it happens automatically. Not so with proxyAlways, so it’s time to look at how you can tell\n\nmountebank to replay all the data it has captured.\n\n5.4. WAYS TO REPLAY A PROXY\n\nConceptually, the switch from recording to replaying is as simple as removing the proxy response (figure 5.8).\n\nFigure 5.8. Replaying involves removing the proxy\n\nAll it takes to switch into replay mode is a single\n\ncommand, as follows:\n\nmb replay\n\nIf you watch the logs after that command, you’ll see\n\nsomething like the following:\n\ninfo: [mb:2525] GET /imposters?\n\nreplayable=true&removeProxies=true\n\ninfo: [mb:2525] PUT /imposters\n\ninfo: [http:3000] Ciao for now\n\ninfo: [http:3000] Open for business...\n\nThe switch involves resetting all imposters, removing their proxies. You can see in the third line that you’re shutting down the existing imposter (Ciao for now) and restarting it on the fourth line (Open for\n\nbusiness...). The line before shows the API call to send the altered configuration; this is the same line you’d\n\nsee if you started mountebank with the --configfile command-line option.\n\nThe first line shows a different part of the mountebank REST API. Just as you can query the state of a single imposter by sending a GET request to http://localhost:2525/imposters/3000 (assuming the\n\nimposter is on port 3000), you can query all imposters at http://localhost:2525/imposters. The replay command\n\nadds two query parameters to that call:\n\nBecause the configuration for all imposters is quite possibly a\n\nconsiderable amount of data, it’s trimmed by default. The replayable query parameter ensures that all data essential (and no more) for replay is returned.\n\nThe removeProxies parameter removes the proxy responses, leaving only the captured is responses.\n\nThe mb replay command replays the responses as is. If you need to tweak the captured responses for any reason,\n\nyou can always use the API call to get the data yourself and process it as appropriate. Even better, you can let\n\nmountebank’s command line do the job for you. The following command saves the current state of all\n\nimposters, with proxies removed:\n\nmb save --savefile imposters.json --removeProxies\n\nThe mb save command saves all imposter configuration\n\nto the given file. The –savefile argument specifies where to save the configuration, and the -- removeProxies flag strips the proxy responses from\n\nthe configuration. Functionally, the mb replay command is nothing but an mb save followed by a\n\nrestart. The following command reimplements the replay command:\n\nmb save --savefile imposters.json --removeProxies\n\nmb restart --configfile imposters.json\n\nThe ability to save all responses to a downstream service in proxyAlways mode and replay them with a single\n\ncommand dramatically simplifies capturing data for rich test suites.\n\n5.5. CONFIGURING THE PROXY\n\nProxies are configurable, both on the request they send to the downstream service and the generated responses\n\nthey send back to the system under test (figure 5.9).\n\nFigure 5.9. You can configure both the proxy request and the generated response.\n\nWe look at how to alter the responses in chapter 7 when we examine behaviors. You can apply two basic types of\n\nconfiguration to the proxied request: using certificate- based mutual authentication and adding custom\n\nheaders.\n\n5.5.1. Using mutual authentication\n\nRecall from chapter 3 that you can configure imposters to expect a client certificate by setting the mutualAuth\n\nfield to true. In that case, configuring the certificate and private key is the responsibility of the system under test.\n\nIf the downstream service you are proxying to requires\n\nmutual authentication, then you have to configure the certificate on the proxy itself (figure 5.10).\n\nFigure 5.10. Configuring the proxy to pass a client certificate\n\nIn this case, setting up the proxy is similar to how you set HTTPS imposters. You set the certificate and private key\n\nin PEM format directly on the proxy, as shown in the following listing.\n\nListing 5.10. A proxy configured to pass a client certificate\n\n{\n\n\"proxy\": {\n\n\"to\": \"https://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true } }],\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\n...\", 1\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\\n...\"\n\n1\n\n}\n\n}\n\n1 The actual text is much longer.\n\nRefer to chapter 3 for the full PEM format and how to create certificates.\n\n5.5.2. Adding custom headers\n\nOccasionally, it’s useful to add another header that gets passed to the downstream service. For example, many\n\nservices return compressed responses for efficiency. Although the original data may be human-readable\n\nJSON, after compression it turns into unreadable binary. By default, any proxies you set up will respond with the\n\ncompressed data unchanged. Because negotiating gzipped compression through headers is a standard\n\noperation in HTTP, the HTTP libraries that the system under test uses would decompress the data, allowing the\n\ncode that you’re testing to see the plain text response (figure 5.11).\n\nFigure 5.11. Proxying compressed responses\n\nThe standard proxy configurations that you’ve seen so far\n\nhave no issues returning the compressed data to the system under test. The problem occurs when you want to\n\nactually look at the data, such as after saving it in a configuration file using the mb save command. You\n\nmay want to examine the JSON bodies of the generated\n\nis responses and perhaps tweak them to better fit your test cases, but you won’t be able to. All you will be able to\n\n[3]\n\nsee are encoded binary strings.\n\n3\n\nWe’ll look at how mountebank handles binary data in chapter 8.\n\nHTTP provides a way for clients to tell servers not to send back compressed data by setting the Accept- Encoding header to \"identity\". The original request\n\nfrom the system under test likely doesn’t include this header, because it can handle the compressed data just\n\nfine (and using compressed data in production is a good idea anyway for efficiency reasons). Fortunately, you can\n\ninject headers into the proxied request, as shown in the following listing.\n\nListing 5.11. Injecting a header into the request to prevent response compression\n\n{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true } }],\n\n\"injectHeaders\": {\n\n\"Accept-Encoding\": \"identity\" 1\n\n}\n\n}\n\n}\n\n1 Prevents response compression\n\nIf you need to inject multiple headers, add multiple key/value pairs to the injectHeaders object. Each\n\nheader will be added to the original request headers.\n\n5.6. PROXY USE CASES\n\nThe examples so far in this chapter have focused on using proxies to record and replay. That is the most\n\ncommon use case for proxies, as it allows you to capture\n\na rich set of test data for your test suite by recording real traffic. In addition, proxies have at least two other use\n\ncases: as a fallback response and as a way of presenting an HTTP face to an HTTPS service.\n\n5.6.1. Using a proxy as a fallback\n\nThough it isn’t a common scenario, sometimes it’s convenient to test against a real dependency when that\n\ndependency is stable, reliable, and highly available. One project I was on involved testing against a software-as-a-\n\nservice (SaaS) credit card processor, and the SaaS provider supported a reliable preproduction\n\nenvironment for testing. In fact, it was perhaps too reliable. “Happy path” testing (testing the expected flow\n\nthrough the service) was easy, but the service was so reliable that testing error conditions was difficult.\n\nYou can get the best of both worlds by using a partial\n\nproxy. Most calls flow through to the credit card processing service, but a few special requests trigger\n\ncanned error responses. Mountebank supports this scenario by relying on its first-match policy, putting the\n\nerror conditions first and the proxy last (figure 5.12).\n\nFigure 5.12. Mixing canned responses with a fallback proxy\n\nNotice that proxy has no predicates, which means all\n\nrequests that don’t match the predicates on the previous stubs will flow through to the proxy. Ensuring that all\n\nrequests flow through to the credit card processor involves putting the proxy in proxyAlways mode. In\n\nthe following listing, the code to do this relies on putting the proxy stub last.\n\nListing 5.12. Using a partial proxy\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{ 1\n\n\"contains\": { \"body\": \"5555555555555555\"\n\n} 1\n\n}],\n\n1\n\n\"responses\": [{\n\n2\n\n\"is\": { \"body\": \"FRAUD ALERT... \" } 2\n\n}]\n\n2\n\n},\n\n{\n\n\"predicates\": [{\n\n3\n\n\"contains\": { \"body\": \"4444444444444444\"\n\n} 3 }],\n\n3\n\n\"responses\": [{\n\n4\n\n\"is\": { \"body\": \"INSUFFICIENT FUNDS...\" }\n\n4\n\n}]\n\n4 },\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n5\n\n\"to\": \"http://localhost:8080\",\n\n5\n\n\"mode\": \"proxyAlways\" 5\n\n}\n\n5\n\n}]\n\n}\n\n]\n\n}\n\n1 If the body contains this credit card #...\n\n2 ...send a fraud-alert response.\n\n3 If it contains this credit card #...\n\n4 ...send an over-balance response.\n\n5 All other calls go to the real service.\n\nIn this scenario, you would not use mb replay, because\n\nyou aren’t trying to virtualize the downstream service. Mountebank still creates new stubs and responses, so for\n\nany long-lived partial proxy, you’ll run into memory leaks. A future version of mountebank will support\n\nconfiguring proxies not to remember each response.\n\n5.6.2. Converting HTTPS to HTTP\n\nAnother less common scenario is to make an HTTPS service easier to test against. When I’ve seen this done, it\n\nhas been as a workaround in an enterprise test environment that hasn’t configured SSL correctly,\n\nleading to certificates that don’t validate against the Certificate Authority. As I mentioned in chapter 3, I\n\nstrongly advise against changing the system under test to accept invalid certificates, because you risk releasing that\n\nconfiguration into production. Although the best solution is to fix the test environment certificates, the division of\n\nlabor in some enterprises makes that difficult to do. Assuming that you’re confident in the ability of the\n\nsystem under test to negotiate HTTPS with valid certificates (behavior that’s nearly always provided by\n\nthe core language libraries), you can rely on mountebank to bridge the misconfigured HTTPS to HTTP, as shown\n\nin the following listing.\n\nListing 5.13. Using a proxy to bridge HTTPS to HTTP\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\", 1 \"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://localhost:8080\", 2\n\n\"mode\": \"proxyAlways\"\n\n}\n\n}]\n\n}] }\n\n1 The imposter itself is an HTTP server...\n\n2 ...but forwards requests to an HTTPS server.\n\nBecause mountebank is designed to help test in environments that have yet to be fully configured, the\n\nimposter itself doesn’t validate the certificate during the proxy call. This doesn’t require any similar change in\n\nconfiguration in the system under test.\n\nSUMMARY\n\nProxy responses capture real downstream responses and save them for future replay. The default proxyOnce mode saves the response in front of the proxy stub, meaning you don’t need to do anything to replay the response.\n\nThe proxyAlways mode allows you to capture a full set of test data by capturing multiple responses for the same logical request.\n\nYou have to explicitly switch from record mode to replay mode by using mb replay.\n\nThe predicateGenerators field tells mountebank how to create the predicates based on the incoming request. All fields used to discriminate requests are listed under the matches object. You configure parameters no differently than you would for\n\nnormal predicates.\n\nProxies support mutual authentication. You are responsible for setting the key and cert fields.\n\nYou can alter the request headers that your proxied request sends to the downstream service with the injectHeaders field. This is useful, for example, to disable compression so you can save a text\n\nresponse.\n\nChapter 6. Programming mountebank\n\nThis chapter covers\n\nMatching requests even when none of the standard predicates do the trick\n\nAdding dynamic data to mountebank responses\n\nMaking network calls to create a stub response\n\nSecuring mountebank against unwanted remote execution\n\nDebugging your scripts within mountebank\n\nMost developers have the experience of a tool or\n\nframework that makes solving a problem harder than it should be or outright impossible. A primary goal in\n\nmountebank is to keep things that should be easy actually easy, and to make hard things possible.\n\nComprehensive default settings, predicates, and simple is responses enable you to solve easy problems with\n\nsimple solutions. Support for certificates, JSON, XML, and proxies help you to create solutions for a harder set\n\nof problems. But sometimes, that’s not enough.\n\nAt times, the built-in features of mountebank may not directly support your use case. Matching predicates\n\nbased on JSON and XML is nice, but what if your requests use CSV format? Or say you have a service that\n\nneeds to replay information from an earlier request in a subsequent response, requiring stateful memory. Maybe\n\nyour use case extends beyond what a proxy is capable of doing. Or perhaps you need to grab some data from\n\noutside mountebank itself and insert it into the stub response.\n\nYou need inject.\n\nMountebank ships with several scriptable interfaces, which we’ll examine in chapters 7 and 8. The main\n\nscriptable interface comes in the form of a new predicate type and a new response type, both called inject, that\n\nallows you to match requests and create responses in ways mountebank wasn’t designed to support.\n\n6.1. CREATING YOUR OWN PREDICATE\n\nLike most technologists, I like to while away the idle hours pretending to be a rock star. Scaling that desire\n\ninto a set of microservices requires both a manager service (to manage my itinerary) and a roadie service (to\n\nmanage my instruments). The two services collaborate to protect my guitars against excess humidity (figure 6.1).\n\nFigure 6.1. Service collaboration used to protect my guitars\n\nLet’s assume that the manager service (the system under\n\ntest) is responsible for sending updated weather reports in CSV format to the roadie service, which uses that\n\ninformation to protect my guitars against dangerous humidity levels. Nice acoustic guitars are made of wood\n\nthat generally wants to stay in the 40–60% humidity",
      "page_number": 151
    },
    {
      "number": 6,
      "title": "Programming mountebank",
      "start_page": 181,
      "end_page": 214,
      "detection_method": "regex_chapter_title",
      "content": "range, and the roadie service is responsible for alerting the manager service when a plan is needed to protect the\n\nguitars against excessive humidity.\n\nAlthough virtualizing a roadie service is a bit fantastical,\n\nyou’ll run into services that use something other than JSON or XML as their lingua franca from time to time.\n\nCSV is still a relatively popular format in certain types of service integrations, particularly those that involve\n\npassing some bulk-style information between teams or organizations, such as:\n\nRetrieving an augmented set of customer information from a\n\nmarketing research partner who has added (for example)\n\ndemographic information to your customer information\n\nRetrieving an updated list of tax rates by ZIP code in the United\n\nStates from an outside partner\n\nRetrieving bulk reporting information from an internal team\n\nFor this example, you’ll expect the manager service to\n\npass weather data showing the next 10 days of weather information in a format similar to what you might get on\n\na site like weather.com, as shown in the following listing.\n\nListing 6.1. A weather CSV service payload\n\nDay,Description,High,Low,Precip,Wind,Humidity\n\n4-Jun,PM Thunderstorms,83,71,50%,E 5 mph,65%\n\n5-Jun,PM Thunderstorms,82,69,60%,NNE 8mph,73%\n\n6-Jun,Sunny,90,67,10%,NNE 11mph,55% 7-Jun,Mostly Sunny,86,65,10%,NE 7 mph,53%\n\n8-Jun,Partly Cloudy,84,68,10%,ESE 4 mph,53%\n\n9-Jun,Partly Cloudy,88,68,0%,SSE 11mph,56%\n\n10-Jun,Partly Cloudy,89,70,10%,S 15 mph,57%\n\n11-Jun,Sunny,90,73,10%,S 16 mph,61%\n\n12-Jun,Partly Cloudy,91,74,10%,S 13 mph,63%\n\n13-Jun,Partly Cloudy,90,74,10%,S 17 mph,64%\n\nMountebank doesn’t natively support CSV, and it doesn’t have a greaterThan predicate to look for humidity\n\nlevels greater than 60%, so you are already outside of its built-in capabilities, but let’s raise the stakes even higher.\n\nYour expectations of the roadie service are that it only\n\npresses the panic button if you are out of range for at least three consecutive days, or if you are more than 10%\n\nout of range for a single day (figure 6.2).\n\nFigure 6.2. A test scenario requiring advanced predicate logic\n\nThat makes for a complex predicate, but no one said\n\nbecoming a rock star would be easy. You can create your own predicate using JavaScript and the inject\n\npredicate, but first you have to start mountebank with the –allowInjection command line flag:\n\nmb --allowInjection\n\nThe logs show a warning at startup that hints at why injection is disabled by default:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking orders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\nwarn: [mb:2525] Running with --allowInjection\n\nset.\n\nSee http://localhost:2525/docs/security for\n\nsecurity info\n\nYou can go ahead and view the docs provided at the given URL now, or wait until you get to the topic of\n\nsecurity later in this chapter. One thing you should not do is ignore the warning. It’s great that you can do\n\nwonderfully complex feats of logic with JavaScript injection and mountebank. Unfortunately, so can\n\nmalicious attackers on your network who can access your machine. They now have a remote execution engine\n\nlistening on a socket. You can protect yourself, and we look at ways to do that at the end of this chapter. For now, the safest option is to add the --localOnly flag, which dis-allows remote connections:\n\nmb restart --allowInjection --localOnly\n\nAll of the predicates we looked at in chapter 4 operate on a single request field. Not so with inject, which gives\n\nyou complete control by passing the entire request object into a JavaScript function you write. That JavaScript function returns true if the predicate matches the request and false otherwise, as shown in the following\n\n[1]\n\nlisting.\n\n1\n\nJavaScript as a language has a lot of warts, and experts will tell you that you can return truthy or falsy. I never cared to learn what that meant, so I stick with true or false and recommend you do too.\n\nListing 6.2. The structure of an inject predicate\n\nfunction (request) {\n\nif (...) { 1\n\nreturn true; 2\n\n}\n\nelse { return false; 3\n\n}\n\n}\n\n1 Condition can use entire request\n\n2 Predicate matched\n\n3 Predicate didn’t match\n\nPlugging the function into the predicates array of a\n\nstub involves JSON-escaping the function, which replaces newlines with '\\n', and escaping double\n\nquotes:\n\n{\n\n\"predicates\": [{\n\n\"inject\": \"function (request) {\\n if (...)\n\n{\\n\n\nreturn true;\\n }\\n else {\\n return\n\nfalse;\\n }\\n}\" }],\n\n\"responses: [{\n\n\"is\": { ... }\n\n}]\n\n}\n\nI wouldn’t recommend doing the JSON-escaping by hand. In the examples that follow, you will use EJS templating and the stringify function mountebank adds to EJS. (Refer to chapter 3 for details on how to lay\n\n[2]\n\nout configuration files.) imposter configuration in code, your JSON libraries\n\nIf you’re building up the\n\nshould manage the escaping for you.\n\n2\n\nBrowse the GitHub repo at https://github.com/bbyars/mountebank-in-action to see fully worked- out examples.\n\nAll you have to do now is write the JavaScript function. I\n\nhave intentionally created a complex example to show that injection is up to nearly any task. Let’s look at the\n\nJavaScript bit by bit, starting with a function to parse CSV. You need a function that will take raw text, like that\n\nshown in listing 6.1, and convert it into an array of JavaScript objects:\n\n[\n\n{\n\n\"Day\": \"4-Jun\",\n\n\"Description\": \"PM Thunderstorms\",\n\n\"High\": 83,\n\n\"Low\": 71\n\n\"Precip\": \"50%\", \"Wind\": \"E 5 mph\",\n\n\"Humidity\": \"65%\"\n\n},\n\n...\n\n]\n\nYou will call the function shown in the following listing csvToObjects.\n\nListing 6.3. A JavaScript function to parse CSV data—csvToObjects\n\nfunction csvToObjects (csvData) {\n\n1\n\nvar lines = csvData.split('\\n'),\n\n2\n\nheaders = lines[0].split(','),\n\nresult = [];\n\n// Remove the headers\n\nlines.shift();\n\n3\n\nlines.forEach(function (line) {\n\nvar fields = line.split(','),\n\n4\n\nrow = {};\n\nfor (var i = 0; i < headers.length; i++) {\n\nvar header = headers[i],\n\ndata = fields[i];\n\nrow[header] = data;\n\n5\n\n}\n\nresult.push(row);\n\n6\n\n});\n\nreturn result;\n\n}\n\n1 csvData is the raw text.\n\n2 Splits input by line endings\n\n3 Removes first line (headers)\n\n4 Splits line by commas\n\n5 Adds data keyed by header\n\n6 Adds to array\n\nAs far as CSV parsing functions go, this is as simple as it\n\ngets. It works for your data, but not for more complex data involving escaped commas inside quotes and other\n\nedge scenarios.\n\nThe next function you will need is one to look for three consecutive days of humidity over 60%, as shown in the\n\nfollowing listing.\n\nListing 6.4. Looking for three consecutive days out of range\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n1 var daysOutOfRange = 0,\n\nresult = false;\n\nhumidities.forEach(function (humidity) {\n\nif (humidity < 60) {\n\ndaysOutOfRange = 0;\n\n2\n\nreturn; }\n\ndaysOutOfRange += 1;\n\nif (daysOutOfRange >= 3) {\n\n3\n\nresult = true;\n\n}\n\n});\n\nreturn result;\n\n}\n\n1 Accepts an array of integers representing humidity\n\nlevel\n\n2 Resets counter if humidity is in range\n\n3 Sets result if humidity is out of range for three days\n\nDetecting three consecutive days of out-of-range humidity is complicated enough to extract into a\n\nseparate function, but it isn’t too difficult to code. The last check—looking for a single day more than 10% out of\n\nrange—is simple enough that you can do it inline using the JavaScript some function of arrays, which returns\n\ntrue if the supplied function is true for any element of the array. The predicate function looks like the following\n\nlisting.\n\nListing 6.5. A predicate to test for excess humidity levels\n\nfunction (request) {\n\n1\n\nfunction csvToObjects (csvData) { ... }\n\n2\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n... } 3\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\n4\n\nreturn parseInt(row.Humidity.replace('%', '')); 4\n\n}),\n\n4\n\nhasDayTenPercentOutOfRange =\n\nhumidities.some( 5\n\nfunction (humidity) { return humidity >=\n\n70; } 5\n\n); 5\n\nreturn hasDayTenPercentOutOfRange ||\n\n6\n\nhasThreeDaysOutOfRange(humidities);\n\n6\n\n}\n\n1 Passes the request object in\n\n2 See listing 6.3.\n\n3 See listing 6.4.\n\n4 Converts the CSV rows to a list of humidity integers\n\n5 Converts the CSV rows to a list of humidity integers\n\n6 Matches if either condition is true\n\nWhen you include an inject predicate in a stub,\n\nmountebank passes the entire request object to the provided function. You have included your csvToObjects and hasThreeDaysOutOfRange functions as subfunctions inside the parent predicate\n\nfunction. You can use this approach to include code of considerable complexity.\n\nAdding this predicate allows you to mimic the roadie\n\nservice effectively, virtualizing its behavior with high fidelity. Although that highlights the power of JavaScript\n\ninjection, it also raises an important concern about service virtualization.\n\nVirtualizing the roadie service has been a wonderful example to demonstrate the power of inject. However, it does come with two pretty serious drawbacks: it hasn’t\n\nhelped one bit in terms of making me an actual rock star, and it’s likely the kind of thing you’d want to avoid\n\nvirtualizing in a real application stack. Remember, service virtualization is a testing strategy that gives you\n\ndeterminism when testing a service that has runtime dependencies. It’s not a way of reimplementing runtime\n\ndependencies in a different platform. Although mountebank provides advanced functionality to make\n\nyour stubs smarter when you need them to be, your best bet is to not need them to be so smart. The dumber your\n\nvirtual services can be, the more maintainable your test architecture will be.\n\n6.2. CREATING YOUR OWN DYNAMIC RESPONSE\n\nYou also can create your own response in mountebank. The inject response joins is and proxy to round out the core response types, and it represents a dynamic\n\nresponse that JavaScript generates. In its simplest form, the response injection function mirrors that for\n\npredicates, accepting the entire request as a parameter. It’s responsible for returning a response object that\n\nmountebank will merge with the default response. Think of it as creating an is response using a JavaScript\n\nfunction, as follows.\n\nListing 6.6. The basic structure of response injection\n\n{\n\n\"responses\": [{\n\n\"inject\": \"function (request) { return {\n\nstatusCode: 400 }; }\" }]\n\n}\n\nIn this basic form, it’s quite easy to replace your\n\npredicate injection you used to virtualize the roadie service with response injection. Because you’re giving the\n\nresponse injection function the same request as the predicate injection function, you could remove the\n\npredicate injection and move the conditions to the function that generates the response, as shown in the\n\nfollowing listing.\n\nListing 6.7. A response injection function to virtualize the roadie service humidity checks\n\nfunction (request) {\n\n1\n\nfunction csvToObjects (csvData) { ... } 2\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n... } 3\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%', ''));\n\n}),\n\nhasDayTenPercentOutOfRange = humidities.some(\n\nfunction (humidity) { return humidity >=\n\n70; }\n\n),\n\nisTooHumid = hasDayTenPercentOutOfRange ||\n\n4\n\nhasThreeDaysOutOfRange(humidities);\n\nif (isTooHumid) {\n\nreturn {\n\n5 statusCode: 400,\n\n5\n\nbody: 'Humidity levels dangerous, action\n\nrequired' 5\n\n};\n\n5\n\n}\n\nelse { return {\n\n6\n\nbody: 'Humidity levels OK for the next 10\n\ndays' 6\n\n};\n\n6\n\n}\n\n}\n\n1 Mountebank passes request in\n\n2 See listing 6.3.\n\n3 See listing 6.4.\n\n4 Capture condition\n\n5 Return failure response\n\n6 Return happy path response\n\nInstead of returning true or false to determine\n\nwhether a predicate matches, a response injection returns the response object, or at least the portion of it\n\nthat isn’t the default response. In this case, it returns a 400 status code and a body indicating action is required\n\nif the humidity check requires you to take action, or a default 200 code with a body letting you know that the\n\nhumidity levels are OK.\n\n6.2.1. Adding state\n\nAt first glance, there isn’t much difference between using\n\na predicate injection and using a response injection in this example. But for complex workflows, response injections have a key advantage: they can keep state. To\n\nsee how that can be useful, imagine having to virtualize a scenario where your manager service sends multiple\n\nweather reports, and the roadie service needs to detect three consecutive days out of range even if they span two\n\nreports that the manager sent (figure 6.3).\n\nFigure 6.3. Two reports need to be spanned to detect dangerous humidity.\n\nMountebank passes a state parameter into your\n\nresponse injection functions that you can use to remember information across multiple requests. It’s\n\ninitially an empty object, but you can add whatever information you want to it each time the injection\n\nfunction executes. In this example, you will have to save the humidity results by day so you can detect dangerous\n\nhumidity levels even if three consecutive days over 60% humidity span two requests from the manager service.\n\nYou start by adding the parameter to your function and\n\ninitializing it with the variables you want to remember. In this case, state will remember the days, and if the\n\nroadie service sees the weather report for a day it hasn’t seen yet, your function will add the humidity to a list:\n\nfunction (request, state) { if (!state.humidities) {\n\nstate.days = [];\n\nstate.humidities = [];\n\n}\n\n... }\n\nNow the rest of the function is nearly identical to listing 6.7. You only have to add to the state.humidities array at the appropriate time and do your checks on that\n\narray instead of a local variable, as in the following listing.\n\nListing 6.8. Remembering state between responses\n\nfunction (request, state) {\n\nfunction csvToObjects (csvData) {...}\n\nfunction hasThreeDaysOutOfRange (humidities)\n\n{...}\n\n// Initialize state arrays if (!state.humidities) {\n\nstate.days = [];\n\nstate.humidities = [];\n\n}\n\nvar rows = csvToObjects(request.body);\n\nrows.forEach(function (row) {\n\nif (state.days.indexOf(row.Day) < 0) { 1\n\nstate.days.push(row.Day);\n\nstate.humidities.push(row.Humidity.replace('%',\n\n'')); 2\n\n}\n\n});\n\nvar hasDayTenPercentOutOfRange =\n\nstate.humidities.some(function (humidity) {\n\n3\n\nreturn humidity >= 70;\n\n});\n\nif (hasDayTenPercentOutOfRange ||\n\nhasThreeDaysOutOfRange(state.humidities)) { return {\n\nstatusCode: 400,\n\nbody: 'Humidity levels dangerous, action\n\nrequired'\n\n};\n\n}\n\nelse {\n\nreturn { body: 'Humidity levels OK'\n\n};\n\n}\n\n}\n\n1 Only adds to the list if it hasn’t seen this day before\n\n2 Adds new humidity\n\n3 Switches these functions to use state variable\n\nVoilà! Now the virtual roadie service can keep track of\n\nhumidity levels across multiple requests. There is only one feature of injection left to look at, but it is a big topic:\n\nasynchronous operations.\n\n6.2.2. Adding async\n\nAsynchronicity is baked into JavaScript, to the extent\n\nthat it is generally required to access any file or network resource used to craft a dynamic response. Understanding why requires a quick tour of how\n\nprogramming languages manage I/O, as JavaScript is fairly unusual in this regard. Until Microsoft introduced the XMLHttpRequest object that powers AJAX requests, JavaScript lacked any form of I/O found in the\n\nbase class libraries of other languages. It took Node.js to add a full complement of I/O functions to JavaScript, but\n\nit did so following the AJAX pattern familiar to a generation of web developers: using callbacks.\n\nTake a look at the following code to sort the lines in a file. This is in Ruby, but the code would be similar in Python,\n\nJava, C#, and most traditional languages.\n\nListing 6.9. Using traditional I/O to sort lines in a file\n\nlines = File.readlines('input.txt') puts lines.sort\n\nputs \"The end...\"\n\nFirst you read all the lines in the input.txt file into an array, then you sort the array, printing the output to the\n\nconsole. Finally, you print “The end...” to the console. At first blush, nothing could be simpler, but the File.readlines function is hiding considerable complexity.\n\nAs shown in figure 6.4, under the hood, Ruby has to make a system call into the operating system (OS), because\n\nonly the OS has the appropriate privileges to interact with the hardware, including the disk storing input.txt.\n\nTo buy time while it waits on the results, the OS scheduler switches to another process to execute for a\n\nperiod of time. When the data from the disk is available, the OS feeds it back into the original process. Computers\n\nmove fast enough that this is largely transparent to the user; for most I/O operations, the application will still feel quite responsive. It’s also transparent to the\n\ndeveloper, as the linear nature of the code matches a mental model, which is why blocking I/O—having the\n\nprocess block until the operation completes—is the most common form of I/O.\n\nFigure 6.4. What happens with traditional blocking I/O\n\nJavaScript was born of the web, a programming\n\nenvironment rich with events, such as responding when a user presses a button or types in a text field. AJAX,\n\nwhich made web pages more responsive by allowing the user to fetch data from the server without refreshing the\n\nentire page (a form of I/O involving the network), maintained that event model, treating getting a response\n\nfrom the server as an event. When Ryan Dahl wrote Node.js to add more I/O capability to JavaScript, he\n\nintentionally maintained that event model because he wanted to explore nonblocking I/O in a mainstream\n\nlanguage. The fact that developers were already used to AJAX events made JavaScript a natural fit (figure 6.5).\n\nFigure 6.5. Nonblocking I/O doesnʼt block the process.\n\nEach I/O operation registers a callback function that\n\nexecutes when the OS has data, and program execution continues immediately to the next line of code. Let’s\n\nrewrite the Ruby file sort operation in JavaScript, using nonblocking I/O, as in the following listing.\n\nListing 6.10. File sort using nonblocking I/O\n\nvar fs = require('fs');\n\n1\n\nfs.readFile('input.txt', function (err, data) {\n\n2\n\nvar lines = data.toString().split('\\n'); console.log(lines.sort());\n\n});\n\nconsole.log('The end...');\n\n3\n\n1 Built-in node library for filesystem operations\n\n2 Ignores error handling\n\n3 Execution continues before the file is actually read\n\nThe callback function is passed as a parameter to fs.readFile, and execution immediately continues to\n\nthe next line of code. At some later point, when the OS has the contents of input.txt, the callback function will\n\nexecute, providing the sorted lines. That flow means that, in this example, you will print “The end...” to the console\n\nbefore you print the sorted lines of input.txt.\n\nAs of this writing, predicate injection doesn’t support\n\nasynchronous operations. But async support is important when it comes to response injection, as I/O operations\n\nare often valuable in scripting dynamic responses. Let’s show an example by virtualizing a simple OAuth flow in\n\nmountebank.\n\nOAuth\n\nOAuth is a delegated authorization framework. It allows one party (the resource owner) to allow another party\n\n(the client) to act on a resource held by a third party (the resource server), where the identity of the resource\n\nowner is guaranteed by a fourth party (the authorization server). You can often collapse these four roles in various\n\nways, creating a number of alternative flows.\n\nA standard use case is where a person (the resource\n\nowner) allows a web app (the client) to act on a resource held by a third party like GitHub (resource server) after\n\npresenting credentials to that third party (authorization server). This flow allows the web app to perform secure\n\noperations in GitHub on behalf of the user, even though the user never provided their GitHub credentials to that\n\nwebsite.\n\nThis OAuth flow is common, and its mechanics are difficult to stub out. In the section “Virtualizing an\n\nOAuth-backed GitHub client,” I use that as an opportunity to show how to manage async in an inject\n\nresponse by building a small GitHub web app and virtualizing the GitHub API for testing purposes.\n\nVirtualizing an OAuth-backed GitHub client\n\nGitHub has a robust marketplace of client applications available at https://github.com/marketplace. Unfortunately,\n\nnone of them solve an immediate problem of the readers of this book: adding a star to the mountebank repo.\n\n[3]\n\nBut GitHub has a public RESTful API that allows you to build an app. You will treat that app as your system\n\nunder test, requiring you to virtualize the GitHub API itself.\n\n3\n\nFor readers willing to star the old-fashioned way, you can do so by visiting the repo directly at https://github.com/bbyars/mountebank.\n\nGitHub uses OAuth, which requires a complex set of interactions before GitHub will accept an API call to star\n\n[4]\n\na repo. new application, which you can do at\n\nThe first thing you need to do is to register your\n\nhttps://github.com/settings/applications/new (figure 6.6).\n\n4\n\nWe will use a basic OAuth web flow described at https://developer.github.com/v3/guides/basics-of- authentication/.\n\nFigure 6.6. Registering a GitHub application\n\nThe OAuth flow expects to call back to complete the\n\nauthentication and uses the URL you provided during registration to call back into. The general flow is shown\n\nin figure 6.7.\n\nFigure 6.7. Understanding the GitHub OAuth flow\n\nThe application calls the /login/oauth/authorize endpoint, passing a client_id. GitHub calls the callback URL you provided during registration, passing a random code. The application is then expected to call a /login/oauth/access_token URL, sending the client_id, the code and a client_secret. If all of that is done correctly, GitHub sends back a token that\n\nthe application can use to authorize subsequent calls. Although the fictional app is designed to star the\n\nmountebank repo if you haven’t already done so, I will only show how to test whether you have already starred\n\nor not. (On the advice of my legal staff, I have decided to leave starring mountebank as an exercise for the reader.)\n\nThe client_id and client_secret are provided\n\nduring registration on GitHub (figure 6.8). Like a private key, you should keep the client_secret super secret.\n\nYou shouldn’t store it in source control, and you most\n\ncertainly shouldn’t publish it in a book for millions to read.\n\n[5]\n\n5\n\nDon’t worry—I removed this toy app before you had a chance to read these words. You’ll have to register your own app to fully follow the source code in question, although you can use the same code located at https://github.com/bbyars/mountebank-in-action\n\nFigure 6.8. Viewing the client secret to communicate to GitHub\n\nYour test case should validate this entire flow, requiring\n\nyou to virtualize three GitHub endpoints. Structurally, the GitHub imposter needs to have three stubs representing those endpoints (figure 6.9). You can\n\nvirtualize the last two calls in order to get the token and to check if you have starred the mountebank repo, with simple is responses. You can’t virtualize the first call, to actually authorize, with an is response. in order to\n\nauthorize. The fact that it has to call back into the system under test requires you to move beyond simple stubbing approaches. You’ll use an inject response to do the callback with an easily identifiable test code.\n\nFigure 6.9. The three stubs needed to virtualize this GitHub workflow\n\nUsing easily identifiable names (like TEST-CODE and\n\nTEST-ACCESS-TOKEN) for the test data that the imposter creates is a useful tip to make it easier to spot in\n\na complex workflow.\n\nStarting the OAuth handshake\n\nThe first endpoint (/login/oauth/authorize) starts the OAuth handshake by sending the random code (TEST-\n\nCODE) to your web app. It’s also the most complicated response, involving calling back to the system under test, which you cannot solve using an is or a proxy response. Conceptually, the stub looks like the following\n\nlisting.\n\nListing 6.11. Stub using response injection to make OAuth callback\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n\"method\": \"GET\",\n\n\"path\": \"/login/oauth/authorize\",\n\n\"query\": {\n\n\"client_id\": \"<%=\n\nprocess.env.GH_CLIENT_ID %>\" 1\n\n} }\n\n}],\n\n\"responses\": [{\n\n\"comment\": \"This sends back a code of TEST-\n\nCODE\", 2\n\n\"inject\": \"<%- stringify(filename, 'auth.js') %>\" 3\n\n}]\n\n}\n\n1 Adds environment variable\n\n2 Mountebank ignores this line.\n\n3 Brings in injection function\n\nBoth the tests and the example web app use environment variables for the client_id and the client_secret, and you will use EJS templating to interpolate them into your configuration file. Notice also the added comment field in the response. Mountebank ignores any fields it\n\ndoesn’t recognize, so you can always add more metadata. For complicated workflows like this one, such comments\n\ncan help you follow along more easily.\n\nThe following listing shows the injection function in auth.js.\n\nListing 6.12. Injection function to make OAuth callback\n\nfunction (request, state, logger, callback) { var http = require('http'),\n\noptions = {\n\nmethod: 'GET',\n\nhostname: 'localhost',\n\nport: 3000,\n\npath: '/callback?code=TEST-CODE'\n\n1\n\n}, httpRequest = http.request(options, function\n\n(response) {\n\nvar body = '';\n\n2\n\nresponse.setEncoding('utf8');\n\n2\n\nresponse.on('data', function (chunk) {\n\n2 body += chunk;\n\n2\n\n});\n\n2\n\nresponse.on('end', function () {\n\ncallback({ body: body });\n\n3 });\n\n});\n\nhttpRequest.end();\n\n4\n\n}\n\n1 Callback with TEST-CODE\n\n2 Node.js code to collect the response body\n\n3 Asynchronously returns the response\n\n4 Sends the request and returns from the function\n\nMuch of the code is manipulating the Node.js http\n\nmodule to make the call to http://localhost:3000/callback?code=TEST-CODE.\n\nBecause the HTTP call involves network I/O, the function returns immediately after the call to httpRequest.end(). When the network call returns, Node.js invokes the function passed as a parameter to the http.request() call. Node’s http library streams the HTTP response back, so you may receive multiple data events and have to collect the response body as you go. When you have received the entire response, Node triggers the end event, at which point you can create the response you want. In your case, you’ll return the same\n\nbody the callback URL provided. Passing that to the callback function parameter ends your response\n\ninjection, returning the parameter as the response to mountebank.\n\nFor example, if your call to\n\nhttp://localhost:3000/callback?code=TEST-CODE returns a body of “You have already starred the\n\nmountebank repo,” then the end result of the injection\n\nfunction would be equivalent to the following is response:\n\n{\n\n\"is\": {\n\n\"body\": \"You have already starred the\n\nmountebank repo\" }\n\n}\n\nRemember, a key goal of mountebank is to make simple\n\nthings easy to do and to make hard things possible. Virtualizing an OAuth flow is hard. This is about as\n\ncomplicated a workflow as you will see in most tests involving service virtualization. It’s certainly a lot to walk\n\nthrough, but you were able to do the hard bits of it with about 20 lines of JavaScript, and when you run into\n\nsimilar problems, you will be thankful that solving them is at least possible with mountebank.\n\nValidating the OAuth authorization\n\nLet’s look at the next stub, for the\n\n/login/oauth/access_token endpoint. This one should only match if the app reflected back the TEST-CODE in\n\nits request body and correctly sent the preconfigured client_id and client_secret. You can use\n\npredicates and a simple is response to send back a test access token, as shown in the following listing.\n\nListing 6.13. The stub to get an access token\n\n{\n\n\"predicates\": [\n\n{\n\n\"equals\": {\n\n\"method\": \"POST\", \"path\": \"/login/oauth/access_token\"\n\n}\n\n},\n\n{\n\n\"contains\": {\n\n\"body\":\n\n\"client_id=<%= process.env.GH_CLIENT_ID %>\"\n\n1\n\n}\n\n},\n\n{\n\n\"contains\": { \"body\":\n\n\"client_secret=<%=\n\nprocess.env.GH_CLIENT_SECRET %>\" 2\n\n}\n\n},\n\n{\n\n\"contains\": {\n\n\"body\": \"code=TEST-CODE\" 3\n\n}\n\n}\n\n],\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": {\n\n\"access_token\": \"TEST-ACCESS-TOKEN\", 4\n\n\"token_type\": \"bearer\",\n\n\"scope\": \"user:email\"\n\n}\n\n}\n\n}]\n\n}\n\n1 Requires client_id from environment\n\n2 Requires client_secret from environment\n\n3 TEST-CODE comes from injection in previous stub\n\n4 Returns a test access token\n\nOnce your web app retrieves the access_token, it has\n\nsuccessfully navigated the OAuth flow.\n\nChecking if youʼve starred mountebank\n\nAt this point, the app should be armed with an access token and should make the GitHub call to check if you’ve\n\nstarred the mountebank repo or not. Your predicate needs to validate the token, and, once again, you can use a simple is response, as shown in the following listing.\n\nListing 6.14. Stub to check if youʼve starred the mountebank repo\n\n{\n\n\"predicates\": [{\n\n\"equals\": {\n\n\"method\": \"GET\",\n\n\"path\": \"/user/starred/bbyars/mountebank\",\n\n\"headers\": {\n\n\"Authorization\": \"token TEST-ACCESS- TOKEN\" 1\n\n}\n\n}\n\n}],\n\n\"responses\": [{\n\n\"comment\": \"204=yes, 404=no\",\n\n2\n\n\"is\": { \"statusCode\": 404 } 3\n\n}]\n\n}\n\n1 Validates token\n\n2 Mountebank ignores this line.\n\n3 Indicates that user hasn’t starred repo\n\nWith OAuth now virtualized, any other API endpoints\n\nshould be quite easy to stub out. All you have to do is check for the Authorization header as shown in listing\n\n6.14.\n\n6.2.3. Deciding between response vs. predicate injection\n\nMountebank passes the request object to both\n\npredicate and response injection functions, so you could put conditional logic based on the request in either\n\nlocation. Compared to response injection, predicate injection is relatively easy to use. If you need to send a\n\nstatic response back based on a dynamic condition, programming your own predicate and using an is\n\nresponse stays true to the intention of predicates in mountebank. But response injection is considerably\n\nmore capable, and you won’t hurt my feelings if you move your conditional logic to a response function so you can take advantage of state or async support.\n\nA predicate injection function takes only two parameters:\n\nrequest— The protocol-specific request object (HTTP in all of the examples)\n\nlogger— Used to write debugging information to the mountebank logs\n\nThe response injection function includes those\n\nparameters and adds two more:\n\nstate— An initially empty object that you can add state to; will be passed into subsequent calls for the same imposter\n\ncallback— A callback function to support asynchronous operations\n\nResponse functions always have the option of\n\nsynchronously returning a response object. You only need the callback if you use nonblocking I/O and need\n\nto return the response object asynchronously.\n\n6.3. A WORD OF CAUTION: SECURITY MATTERS\n\nJavaScript injection is disabled by default when you start mountebank, and for good reason. With injection\n\nenabled, mountebank becomes a potential remote execution engine accessible to anyone on the network.\n\nWhen mountebank is run naively, an attacker can take advantage of that fact to do evil things on the network\n\nwhile spoofing your identity (figure 6.10).\n\nFigure 6.10. Using mountebank to spoof your identity during a network attack\n\nInjection is an enormously useful feature, but you have\n\nto use it with an understanding of the security implications. This is why mountebank shows a warning message in the logs every time you start it with the -- allowInjection flag. You can take some precautions\n\nto protect yourself.\n\nThe first precaution is to not run mb under your user account. Starting mountebank as an unprivileged user,\n\nideally one without domain credentials on your network, goes a long way toward protecting yourself. You should\n\nalways use the least privileged user you can get away with, adding in network access only when your tests\n\nrequire it.\n\nThe next layer of security is to restrict which machines can access the mountebank web server. We’ve used the -\n\nlocalOnly flag throughout this chapter, which restricts access to processes running on the same machine. This option is perfect when your tests run on the same machine as mountebank, and it should be the\n\ndefault choice most of the time. When you do require remote tests (during extensive load testing, for example),\n\nyou can still restrict which machines can access mountebank’s web server with the --ipWhitelist flag,\n\nwhich captures a pipe-delimited set of IP addresses. For example:\n\nmb --allowInjection --ipWhitelist\n\n\"10.22.57.137|10.22.57.138\"\n\nIn this example, the only remote IP addresses allowed access to mountebank are 10.22.57.137 and 10.22.57.138.\n\n6.4. DEBUGGING TIPS\n\nWriting injection functions has all the same complexity as writing any code, except that it’s much harder to debug them through an IDE because they run in a\n\nremote process. I often resort to what, back in my college days when we coded in C, we called printf debugging.\n\nIn JavaScript, it looks something like this:\n\nfunction (request) {\n\n// Function definition...\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%',\n\n''));\n\n});\n\nconsole.log(JSON.stringify(humidities)); 1\n\nreturn {}; 2\n\n}\n\n1 Shows full object structure\n\n2 I’ll figure this out later....\n\nThe console.log function in JavaScript prints the\n\nparameter to the standard output of the running process, which in this case is mb. The JSON.stringify function\n\nconverts an object to a JSON string, allowing you to inspect the full object graph. This code is quite ugly—I outdented the console.log function to not lose sight\n\nof it, and I returned an empty object for the response, relying on the standard default response. If you’ve\n\nwritten code for longer than a few seconds, you’ll likely recognize the pattern. Most code starts out ugly before\n\nyou figure out how to communicate your intent to a ruthlessly precise computer.\n\nTo make the output a little easier to spot in the logs, mountebank passes another parameter to both predicate\n\nand response injection: the logger itself. In typical logging fashion, the logger has four functions: debug,\n\ninfo, warn, and error. The debug messages are usually not shown to the console (unless you start mb\n\nwith the --loglevel debug flag). To make your debugging messages stand out, use the warn or error\n\nfunctions, which will print your debugging output to the console in a different color:\n\nfunction (request, state, logger) {\n\n// Function definition...\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%',\n\n''));\n\n});\n\nlogger.warn(JSON.stringify(humidities));\n\n1\n\nreturn {};\n\n}\n\n1 Prints to the console as yellow text\n\nThe code above shows logging for response injection. Predicate injections pass in the logger as the second\n\nparameter as well.\n\nSUMMARY\n\nYou can create your own predicates with a JavaScript function that accepts the request object and returns a Boolean representing whether the predicate matched or not.\n\nYou also can create your own response with a JavaScript function that accepts the request object and returns an object representing the response.\n\nIf you need to remember state between requests, mountebank passes an initially empty state object to the response function. Any fields you set on the object will persist between calls.\n\nBecause JavaScript and node.js use nonblocking I/O, most\n\nresponse functions that need to access data outside the process\n\nwill have to return asynchronously. Instead of returning an object, you can pass the response object to the callback parameter.\n\nInjection is powerful, but it also creates a remote execution engine\n\nrunning on your machine. Anytime you run mountebank with injection enabled, you should limit remote connections and use an\n\nunprivileged identity.\n\nChapter 7. Adding behaviors\n\nThis chapter covers\n\nProgrammatically postprocessing a response\n\nAdding latency to a response\n\nRepeating a response multiple times\n\nCopying input from the request into the response\n\nLooking up data from a CSV file to plug into a response\n\nThe basic is response is easy to understand but limited\n\nin functionality. Proxy responses provide high-fidelity mimicry, but each saved response represents a snapshot\n\nin time. Response injection provides significant flexibility but comes with high complexity. Sometimes, you want the simplicity of is and proxy responses with a touch of dynamism, all without the complexity of inject.\n\nSoftware engineers who hail from the object-oriented\n\nschool of thought use the term decorate to mean intercepting a plain message and augmenting it in some\n\nway before forwarding it on to the recipient. It’s like what the postal service does when it applies a postmark to your letter after sorting it. The original letter you sent is\n\nstill intact, but the postal workers have decorated it to postprocess it with a bit of dynamic information. In\n\nmountebank, behaviors represent a way of decorating responses before the imposter sends them over the wire.\n\nBecause of their flexibility and utility, behaviors also represent one of the most rapidly evolving parts of\n\nmountebank itself. We’ll look at all of the behaviors available as of this writing (representing v1.13), but\n\nexpect more to come in the future.\n\n7.1. UNDERSTANDING BEHAVIORS\n\nIf you ignore the complexity of interacting with the network and different protocols, mountebank has only\n\nthree core concepts:\n\nPredicates help route requests on the way in.\n\nResponses generate the responses on the way out.\n\nBehaviors postprocess the responses before shipping them over the wire (figure 7.1).\n\nFigure 7.1. Behaviors can transform a response from a stub before it goes out via the imposter.\n\nAlthough nothing prevents you from using behaviors with inject responses, most behaviors exist to allow you to reduce the amount of complexity inherent in using\n\nJavaScript to craft the response. Behaviors are a way of avoiding the complexity of inject responses in favor of\n\nsimpler is responses, while still being able to provide appropriate dynamism to your response.\n\nBehaviors sit alongside the type of response in the stub definition, as shown in listing 7.1. You can combine multiple behaviors together, but only one of each type.\n\nNo behavior should depend on the order of execution of other behaviors. That is an implementation detail subject\n\nto change without notice.\n\nListing 7.1. Adding behaviors to a stub definition\n\n{\n\n\"responses\": [{\n\n\"is\": { \"statusCode\": 500 },\n\n\"_behaviors\": { 1 \"decorate\": ..., 2\n\n\"wait\": ... 3",
      "page_number": 181
    },
    {
      "number": 7,
      "title": "Adding behaviors",
      "start_page": 215,
      "end_page": 249,
      "detection_method": "regex_chapter_title",
      "content": "} 1\n\n}]\n\n}\n\n1 All postprocessing steps on the is response\n\n2 See section 7.2.\n\n3 See section 7.3.\n\nIn this example, the is response will first merge the 500\n\nstatus code into the default response, then it will pass the generated response object to both the decorate and\n\nwait behaviors. Each behavior will postprocess the response in a specific way, which we’ll look at shortly.\n\nSome behaviors still rely on programmatic control of the postprocessing, which requires the --allowInjection flag to be set when starting mb. This carries with it all the\n\nsame security considerations we examined in the last chapter. We look at those behaviors next.\n\n7.2. DECORATING A RESPONSE\n\nThe bluntest instruments in the behavior toolbox are the decorate and shellTransform behaviors, which accept the response object as input and transform it in\n\nsome way, sending a new response object as output (figure 7.2).\n\nFigure 7.2. Decoration allows you to postprocess the response.\n\nThey’re quite similar to response injection, except they\n\nprovide more focused injection (in the case of decorate), or more flexibility (shellTransform).\n\n7.2.1. Using the decorate function\n\nWithout behaviors, you’d be forced to use an inject\n\nresponse if only one field of the response were dynamic. For example, assume you want to send the following\n\nresponse body back:\n\n{\n\n\"timestamp\": \"2017-07-22T14:49:21.485Z\",\n\n\"givenName\": \"Stubby\",\n\n\"surname\": \"McStubble\",\n\n\"birthDate\": \"1980-01-01\"\n\n}\n\nYou could capture this body in an is response, setting the body field to the JSON, but that would assume that\n\nan outdated timestamp is irrelevant to the test case at hand. That isn’t always a valid assumption. Unfortunately, translating that to an inject response hides the intent, as shown in the following listing.\n\nListing 7.2. Using an inject response to send a dynamic timestamp\n\n{\n\n\"responses\": [{\n\n\"inject\": \"function () { return { body: {\n\ntimestamp: new Date(),\n\ngivenName: 'Stubby', surname: 'McStubble',\n\nbirthDate:\n\n'1980-01-01' } } }\"\n\n}]\n\n}\n\nTo make sense of what the response is doing, you have to\n\nextract the JavaScript function and stare at it. Compare that to combining an is response with a decorate\n\nresponse, which sends the same JSON over the wire without awkward translation, as you can see in the\n\n[1]\n\nfollowing listing.\n\n1\n\nAs always, you can follow along with the book’s source code at https://github.com/bbyars/mountebank-in-action.\n\nListing 7.3. Combining an is response with a decorate behavior\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n1 \"body\": {\n\n1\n\n\"givenName\": \"Stubby\",\n\n1\n\n\"surname\": \"McStubble\",\n\n1\n\n\"birthDate\": \"1980-01-01\"\n\n1 }\n\n1\n\n},\n\n\"_behaviors\": {\n\n\"decorate\": \"function (request, response,\n\nlogger) { 2\n\nresponse.body.timestamp = new Date(); }\"\n\n2 }\n\n}]\n\n}\n\n1 Return this...\n\n2 ...but add a current timestamp.\n\nAs noted, this is like the post office adding a postmark to your envelope. You provide the core content with an is response, and the decorate behavior adds the current\n\ntimestamp to the message. The end response is the same as with the inject approach, but separating the static\n\npart of the response from the dynamic part often makes the code easier to maintain.\n\nThe decorate function isn’t as capable as a full inject\n\nresponse. Behaviors don’t have access to any user- controlled state like response injection. They don’t allow\n\nasynchronous responses, which eliminates a large class of JavaScript I/O operations, as discussed in chapter 6. That said, the decorate behavior does allow the majority of the response message to be visible in a static is response, which simplifies maintenance of your test data.\n\n7.2.2. Adding decoration to saved proxy responses\n\nBehaviors are agnostic to the type of response they are applied to, which means you can decorate a proxy response as well. But by default, the decoration applies only to the proxy response itself, not to the is response it saves. (See figure 7.3.)\n\nFigure 7.3. Behaviors applied to proxies donʼt transfer to the saved responses.\n\nYou can add a couple of behaviors to the saved is\n\nresponses, including decorate. You have to configure the proxy with the decorate behavior. We’ll work with\n\na more complicated example than updating a timestamp to show how proxying and decoration work hand in\n\nglove.\n\nMost industrial APIs include some sort of rate limiting to prevent denial of service. The Twitter API represents a standard approach, where Twitter sends back an x- rate-limit-remaining header in the response to let\n\nthe user know how many requests the API user has left for a certain time frame. Once those requests are spent,\n\nTwitter will send a 429 HTTP status code (Too Many Requests) until the time period is up.\n\n[2]\n\nAt times you may\n\nwant to test the consumer’s response when it triggers rate limit errors midstream through a workflow. One\n\noption is to proxy all requests to the downstream rate- limited service (using the proxyAlways mode described\n\nin chapter 5). But it may be difficult to capture a rate limit\n\nscenario through proxying real traffic. Another option is to capture the first response and use decoration to\n\ntrigger a rate limit error after a few requests (figure 7.4).\n\n2\n\nYou can read the full details at https://dev.twitter.com/rest/public/rate-limiting.\n\nFigure 7.4. Manufacturing a rate limit exception on a recorded response\n\nSetting up this scenario requires you to proxy to the\n\ndownstream server to save the response but add a decorate behavior on the saved response, as in the\n\nfollowing listing. The original proxy response will be undecorated, returning the response captured from the\n\ndownstream service.\n\nListing 7.4. Adding a decorate behavior to recorded responses\n\n{\n\n\"responses\": [{ \"proxy\": {\n\n\"to\": \"http://downstream-service.com\", 1\n\n\"mode\": \"proxyOnce\", 2\n\n\"addDecorateBehavior\": \"...\" 3\n\n}\n\n}]\n\n}\n\n1 The base URL of the downstream service\n\n2 Only captures the first response\n\n3 Adds a decorator to the recorded response (see listing 7.5\n\nfor further content)\n\nThe decorator function will have access to the saved response and can change the x-rate-limit-\n\nremaining header or return an error as desired by the test case. In the code below, you will decrement the\n\nheader 25 requests at a time to accelerate reaching a rate limit error, but you can tweak that value according to your test scenario. Because decorate functions don’t have access to the same ability to save state that inject\n\nresponses do, you have to use a file to store the last value sent for the x-rate-limit-remaining header, as\n\nshown in the following listing.\n\nListing 7.5. Decorator function to accelerate a rate limit exception\n\nfunction (request, response) {\n\nvar fs = require('fs'),\n\n1 currentValue = parseInt(\n\n2\n\nresponse.headers['x-rate-limit-\n\nremaining']), 2\n\ndecrement = 25;\n\nif (fs.existsSync('rate-limit.txt')) {\n\n3 currentValue = parseInt(\n\n3\n\nfs.readFileSync('rate-limit.txt'));\n\n3\n\n}\n\n3\n\nif (currentValue <= 0) { response.statusCode = 429;\n\n4\n\nresponse.body = {\n\n4\n\nerrors: [{\n\n4\n\ncode: 88,\n\n4 message: 'Rate limit exceeded'\n\n4\n\n}]\n\n4\n\n};\n\n4\n\nresponse.headers['x-rate-limit-remaining'] =\n\n0; 4 }\n\nelse {\n\nfs.writeFileSync('rate-limit.txt',\n\n5\n\ncurrentValue - decrement);\n\n5\n\nresponse.headers['x-rate-limit-remaining'] =\n\n6 currentValue - decrement;\n\n6\n\n}\n\n}\n\n1 Node’s module for filesystem access\n\n2 Gets the value in the recorded response\n\n3 Gets the saved value\n\n4 Sends the error response\n\n5 Saves the next value\n\n6 Updates the header\n\nWhat happened to nonblocking I/O?\n\nIn the last chapter, I described how JavaScript and\n\nNode.js use nonblocking I/O, which required adding asynchronous support to response injection. That is still\n\ntrue, but Node.js has added a small number of blocking, synchronous calls for common filesystem operations.\n\nNotice how the function names used in listing 7.5 end in Sync (fs.existsSync, fs.readFileSync, and\n\nfs.writeFileSync). These are special convenience functions that break out of the standard nonblocking I/O\n\nmodel. To the best of my knowledge, no such convenience functions exist for I/O that has to traverse\n\nthe network.\n\nYou were forced to use the filesystem to save state\n\nbecause decorators cannot save state directly, and you were forced to use the Sync functions because\n\ndecorators do not support asynchronous operations. Both of these are supported in response injection, as\n\ndescribed in chapter 6. Future versions of mountebank may support them in decorators as well. The next behavior, shellTransform, suffers from neither of these limitations.\n\n7.2.3. Adding middleware through shellTransform\n\nThe next behavior is both the most general purpose and\n\nthe most powerful, and both of those aspects come with added complexity. Like decorate, shellTransform\n\nallows you programmatic postprocessing of the response. But it doesn’t require the use of JavaScript, and it allows\n\nyou to chain together a series of postprocessing transformations (figure 7.5).\n\nFigure 7.5. The shellTransform behavior allows you to combine multiple transformations piped through the shell.\n\nTo see how it works, let’s take the two transformations\n\nyou’ve already seen (adding a timestamp and triggering a rate limit exception) and convert them to shellTransform behaviors. You implement each transformation as a command line application that\n\naccepts the JSON-encoded request and JSON-encoded response as parameters passed in on standard input and\n\nreturns the transformed JSON-encoded response on standard output. You’ll start by wiring up the imposter\n\nconfiguration, as shown in the following listing.\n\nListing 7.6. Imposter configuration for shellTransform\n\n{ \"responses\": [{\n\n\"is\": {\n\n\"headers\": { 1\n\n\"x-rate-limit-remaining\": 3 1\n\n}, 1\n\n\"body\": { 2\n\n\"givenName\": \"Stubby\", 2\n\n\"surname\": \"McStubble\", 2 \"birthDate\": \"1980-01-01\" 2\n\n} 2\n\n},\n\n\"_behaviors\": {\n\n\"shellTransform\": [\n\n\"ruby scripts/applyRateLimit.rb\", 3 \"ruby scripts/addTimestamp.rb\" 4\n\n]\n\n}\n\n}]\n\n}\n\n1 Transformed with applyRateLimit.rb\n\n2 Transformed with addTimestamp.rb\n\n3 This will execute first.\n\n4 Then this will execute on the transformed response.\n\nIn this example, you have chosen to pipe the\n\ntransformations through Ruby scripts, but it could’ve been any language. The code in applyRateLimit.rb is a\n\nsimple Ruby conversion of the code in listing 7.5. Mountebank passes two parameters on standard input—\n\nthe request and the response. Here you only need the response, as shown in the following listing.\n\nListing 7.7. Ruby script to transform the response to trigger a rate limit error\n\nrequire 'json'\n\n1\n\nresponse = JSON.parse(ARGV[1])\n\n2\n\nheaders = response['headers']\n\ncurrent_value = headers['x-rate-limit- remaining'].to_i\n\nif File.exists?('rate-limit.txt')\n\ncurrent_value = File.read('rate-\n\nlimit.txt').to_i\n\nend\n\nif current_value <= 0 response['statusCode'] = 429\n\nresponse['body'] = {\n\n'errors' => [{\n\n'code' => 88, 'message' => 'Rate limit\n\nexceeded'\n\n}]\n\n}\n\nresponse['headers']['x-rate-limit-remaining'] =\n\n0\n\nelse\n\nFile.write('rate-limit.txt', current_value - 25)\n\nheaders['x-rate-limit-remaining'] =\n\ncurrent_value - 25\n\nend\n\nputs response.to_json\n\n3\n\n1 Ruby module imported for JSON handling\n\n2 The second command-line parameter is the current\n\nJSON response.\n\n3 Prints the transformed response to stdout\n\nSome syntax changes are obvious from the JavaScript code to the Ruby code (primarily a different hash syntax) and they use some different functions (to_i instead of parseInt), but most of the code looks like a Ruby\n\ndecorator. The key differences are the input (parsing the response from the command line) and the output\n\n(printing the transformed response to stdout). In the following listing, you do the same thing with\n\naddTimestamp.rb.\n\nListing 7.8. Ruby script to add a timestamp to the response JSON\n\nrequire 'json'\n\nresponse = JSON.parse(ARGV[1]) response['body']['timestamp'] = Time.now.getutc\n\nputs response.to_json\n\nBy chaining together transformations, shellTransform acts as a way of adding a transformation pipeline to your response handling,\n\nallowing as much complexity as you require. My standard advice still applies: it’s good to have such power\n\nfor when you absolutely need it, but try not to need it.\n\n7.3. ADDING LATENCY TO A RESPONSE\n\nThe next behavior we’ll look at is a whole lot simpler and doesn’t require you to set the --allowInjection command line flag. Sometimes you need to simulate latency in responses, and the wait behavior tells mountebank to take a nap before returning the response.\n\nYou pass it the number of milliseconds to sleep as follows.\n\nListing 7.9. Using a wait behavior to add latency\n\n{\n\n\"is\": {\n\n\"body\": {\n\n\"name\": \"Sleepy\"\n\n}\n\n}, \"_behaviors\": {\n\n\"wait\": 3000 1\n\n}\n\n}\n\n1 Adds 3 seconds of latency\n\nLike the decorate behavior, you can add the wait\n\nbehavior to saved responses that proxies generate. When you set the addWaitBehavior to true on a proxy, mountebank will automatically fill in the generated wait\n\nbehavior based on how long the real downstream call took. I show how to use that to create robust\n\nperformance tests in chapter 10.\n\n7.4. REPEATING A RESPONSE MULTIPLE TIMES\n\nSometimes you need to send the same response multiple\n\ntimes before moving on to the next response. You can copy the same response multiple times in the responses array, but that practice is generally frowned on by the software community, as it hurts\n\nmaintainability. It’s an important enough concept that it even has its own acronym: DRY (Don’t Repeat Yourself).\n\nThe repeat behavior lets the computer do the repeating for you (figure 7.6). It accepts the number of times you\n\nwant to repeat the response, and mountebank helps you avoid those snooty software engineers looking down\n\ntheir noses at you for not being DRY enough.\n\nFigure 7.6. Repeating a response multiple times\n\nA common use case involves triggering an error response\n\nafter a set number of happy path responses. An example I already used a couple of times in this book involves\n\nquerying an inventory service. In chapters 3 and 4, I showed how you can use a list of responses to show the\n\n[3]\n\ninventory for a product running out over time:\n\n3\n\nAs we did earlier, we are returning an overly simplistic body to focus on only the point of the example.\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } },\n\n{ \"is\": { \"body\": \"21\" } },\n\n{ \"is\": { \"body\": \"0\" } }\n\n]\n\n}\n\nMost test cases don’t require this level of specificity. In slightly oversimplified terms, a consumer of an inventory service only cares about two scenarios:\n\nThe inventory is greater than zero (or greater than or equal to the\n\nquantity being ordered).\n\nThe inventory is zero (or less than the quantity being ordered).\n\nSimplifying even further, the only two scenarios that matter for this test case are\n\nA happy path\n\nAn out-of-inventory error\n\nThe only slightly complicated factor is that you might\n\nwant to return a few happy path responses before returning an out-of-inventory error. You can do that with only two responses and a repeat behavior, as shown in the following listing.\n\nListing 7.10. Using a repeat behavior to return an error a\u0000er a small number of successes\n\n{\n\n\"responses\": [\n\n{\n\n\"is\": { \"body\": \"9999\" }, 1\n\n\"_behaviors\": { \"repeat\": 3 } 2 },\n\n{\n\n\"is\": { \"body\": \"0\" } 3\n\n}\n\n]\n\n}\n\n1 Return the happy path...\n\n2 ...three times.\n\n3 Then return the error path.\n\nTest case construction\n\nWe have looked at some advanced mountebank\n\ncapabilities over the course of this book. Sometimes those capabilities are indispensable for solving\n\ncomplicated test problems. But you can simplify most test cases to a small essential core of what you’re trying\n\nto test, and removing the noise in the test data setup helps keep the focus on that core. The repeat example\n\nwe just looked at shows how the thought process of simplifying your test case has knock-on benefits to your\n\ntest data management.\n\n7.5. REPLACING CONTENT IN THE RESPONSE\n\nYou can always add dynamic data to a response through an inject response, or through the decorate and\n\nshellTransform behaviors. But two additional behaviors support inserting certain types of dynamic\n\ndata into the response without the overhead of programmatic control.\n\n7.5.1. Copying request data to the response\n\nThe copy behavior allows you to capture some part of the request and insert it into the response. Imagine that\n\nthe system under test depended on a service reflecting the account ID from the request URL back in the\n\nresponse body, so that (for example) when you send a GET request to /accounts/8731, you get a response that\n\nreflects that ID and otherwise resembles my account profile in various online forums I participate in:\n\n{\n\n\"id\": \"8731\", 1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon request\"\n\n}\n\n1 This has to match the ID in the path.\n\nThis response has two core aspects:\n\nThe id, which has to match the one provided on the request URL\n\nThe test data you need for your scenario\n\nA standard is response supports managing scenario-\n\nspecific test data, and the copy behavior allows you to insert the id from the request. Copying the id from the\n\nrequest to the response requires you to reserve a slot in the response that you can replace and to be able to select\n\nonly the data you want from the request. The first part is easier—you add any token you choose to the response, as\n\nshown in the following listing.\n\nListing 7.11. Specifying a token in the response to replace with a value from the request\n\n{\n\n\"is\": {\n\n\"body\": { \"id\": \"$ID\", 1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon\n\nrequest\"\n\n}\n\n} }\n\n1 A placeholder that the copy behavior will replace with\n\nthe value\n\nThe first section of the copy behavior requires that you\n\nspecify the request field you’re copying from and the response token you need to replace:\n\n{\n\n\"from\": \"path\", \"into\": \"$ID\",\n\n...\n\n}\n\nThe only part you need to fill in is the part that selects the id. The copy behavior (and the lookup behavior, which we look at next) uses some of the same predicate\n\nmatching capabilities we looked at in chapter 4, specifically regular expressions, XPath, and JSONPath.\n\nRecall that each predicate applies a matching operation against a request field. Whereas a predicate tells you whether or not the match was successful, the copy and lookup behaviors are able to grab the specific text in the\n\nrequest field that matched.\n\nFor this example, a regular expression will do the trick. You need to capture a string of digits at the end of the request path. You can use some of the regex primitives we looked at in chapter 4 to make that selection:\n\n\\d — A digit, 0–9 (you have to double-escape the backslash in JSON)\n\n+ — One or more times\n\n$ — The end of the string\n\nPutting it all together, the stub would look like the\n\nfollowing listing.\n\nListing 7.12. Using a copy behavior to insert the ID from the URL into the response body\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": { \"id\": \"$ID\",\n\n1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon\n\nrequest\"\n\n} },\n\n\"_behaviors\": {\n\n\"copy\": [{\n\n2\n\n\"from\": \"path\",\n\n3\n\n\"into\": \"$ID\",\n\n4 \"using\": {\n\n5\n\n\"method\": \"regex\",\n\n5\n\n\"selector\": \"\\\\d+$\"\n\n5\n\n}\n\n5 }]\n\n}\n\n}]\n\n}\n\n1 The token to replace\n\n2 An array—multiple replacements are allowed\n\n3 The request field to copy from\n\n4 The token to replace\n\n5 The selection criteria to act on the request path\n\nThere’s a lot more to this behavior, but before we get too\n\nfar, I should point out a couple of aspects that you may have already noticed. First, the copy behavior accepts an\n\narray, which means you can make multiple replacements in the response. Each replacement should use a different\n\ntoken, and each one can select from a different part of the request.\n\nThe other thing to notice is that you never specify where\n\nthe token is in the response. That’s by design. You could have put the token in the headers or even the\n\nstatusCode, and mountebank would replace it. If the token is listed multiple times, mountebank will replace\n\neach instance, regardless of where it’s located in the response.\n\nUsing a grouped match\n\nThe previous example made an assumption that you\n\ncould define a regular expression that entirely matched the value you needed to grab and nothing else. That’s a\n\npretty weak assumption.\n\nMany services use some form of a globally unique identifier (GUID) as an id, and the path often extends\n\nbeyond the part containing the id. For example, the path might be /accounts/5ea4d2b5/profile, where\n\n“5ea4d2b5” is the id you need to copy. You can no longer rely on \\\\d+ as a selector because the id contains\n\nmore than digits. You certainly can rely on other mechanisms to match—for instance, by recognizing that the id follows the word “accounts” in the path:\n\naccounts/\\\\w+\n\nThat selector uses the “\\w” regular expression\n\nmetacharacter to capture a word character (letters and\n\ndigits) and adds the “+” to ensure that you capture one or more of them. Then it prefixes the “accounts/” to ensure\n\nyou’re grabbing the right portion of the path. With this expression, you do successfully grab the id.\n\nUnfortunately, you grab the “accounts/” literal string as well, and the replaced body would look like:\n\n{\n\n\"id\": \"accounts/5ea4d2b5\",\n\n...\n\n}\n\nRegular expressions support grouped matches to allow you to grab only the data you need from a match. Every regular expression has a default first group that’s the\n\nentire match. Every time you surround a portion of the selector with parentheses, you describe another group.\n\nYou will adjust your selector to add a group around the id portion of the path, while leaving the literal string\n\n“accounts/” to make sure you are grabbing the right portion of the path:\n\naccounts/(\\\\w+)\n\nWhen you match this regular expression against the string “/accounts/5ea4d2b5/profile”, you get an array of\n\nmatch groups that looks like\n\n[\n\n\"accounts/5ea4d2b5\",\n\n\"5ea4d2b5\"\n\n]\n\nThe first group is the entire match, and the second is the first parenthetical group. If you place an unadorned\n\ntoken in the response, as you did in the previous section, mountebank will replace it with the first index of the\n\narray, which corresponds to the entire match. But you\n\ncan add an index to the token corresponding to the index of the match group array, as shown in the following listing, which allows you to pinpoint the part of the path you want to copy with laser precision.\n\nListing 7.13. Using a grouped match to copy a portion of the request path\n\n{\n\n\"is\": {\n\n\"body\": {\n\n\"id\": \"$ID[1]\", 1\n\n...\n\n}\n\n},\n\n\"_behaviors\": { \"copy\": [{\n\n\"from\": \"path\",\n\n\"into\": \"$ID\", 2\n\n\"using\": {\n\n\"method\": \"regex\",\n\n\"selector\": \"accounts/(\\\\w+)\" 3\n\n}\n\n}] }\n\n}\n\n1 Specifies the index in the response token\n\n2 Specifies the base token in the behavior\n\n3 Uses a grouped match for more precision\n\nYou can always use indexed tokens in the response. Assuming you specify a token of $ID like you did in\n\nlisting 7.13, then putting $ID in the response is equivalent to putting $ID[0]. I doubt that matters much\n\nfor regular expressions, as I suspect most real-world use cases will have to use groups to grab the exact value they\n\nwant. That isn’t necessarily true for the other selection approaches that the copy behavior supports: xpath and\n\njsonpath.\n\nUsing an XPath selector\n\nAlthough a regular expression elegantly supports grabbing a value out of any request field, xpath and\n\njsonpath selectors can be useful to match values inside an incoming request body. They work similarly to how xpath and jsonpath predicates work, as described in chapter 4. The key difference is that predicate XPath and\n\nJSONPath selectors are used with a matching operator (like equals) to test whether the request matches,\n\nwhereas using those selectors with behaviors helps grab the matching text in the request to change the response.\n\nTake the following request body from the system under test, representing a list of accounts:\n\n<accounts xmlns=\"https://www.example.com/accounts\">\n\n<account id=\"d0a7b1b8\" />\n\n<account id=\"5ea4d2b5\" />\n\n<account id=\"774d4feb\" />\n\n</accounts>\n\nYour virtual response needs to reflect back details of the second account in the request. Those details are specific\n\nto your test scenario, but the ID has to match what was sent in the request body. You can use an XPath selector to grab the id attribute of the second account attribute, as shown in the following listing.\n\nListing 7.14. Using an XPath selector to copy a value from the request to the response\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": \"<account><id>$ID</id>...\n\n</account>\" 1\n\n},\n\n\"_behaviors\": {\n\n\"copy\": [{ \"from\": \"body\",\n\n\"into\": \"$ID\",\n\n2\n\n\"using\": {\n\n\"method\": \"xpath\",\n\n3\n\n\"selector\": \"//a:account[2]/@id\",\n\n3\n\n\"ns\": {\n\n3\n\n\"a\":\n\n\"https://www.example.com/accounts\" 3\n\n} 3\n\n}\n\n}]\n\n}\n\n}]\n\n}\n\n1 Tokenizes the response body\n\n2 Defines a token in the copy behavior\n\n3 Selects the value from the XML request body\n\nThe xpath selector and namespace support work\n\nidentically to xpath predicates (chapter 4) and predicate generators (chapter 5). As you saw with predicates,\n\nmountebank also supports JSONPath. We will look at an example with the lookup behavior shortly.\n\nVirtualizing a CORS preflight response\n\nCross-origin resource sharing, or CORS, is a standard that allows browsers to make cross-domain requests. In\n\nthe olden days, a browser would only execute JavaScript calls to the same domain as the one that served up the\n\nhosting web page. This same-origin policy is the bedrock of browser security, as it helps prevent a host of\n\nmalicious JavaScript injection attacks. But as websites became more dynamic and needed to pull behavior from\n\ndisparate resources spread across multiple domains, it also proved to be too restrictive. Creative developers\n\nfound creative hacks to bypass the same-origin policy, like JSONP, which manipulates the script element in\n\nan HTML document to pass JavaScript from a different domain to a callback function already defined. JSONP is\n\nconfusing and hard to understand because it works around the browser’s built-in security mechanism.\n\nThe CORS standard evolves the browser’s security model to allow the browser and server to both weigh in on\n\nwhether a cross-domain request is valid or not. The standard requires a preflight request for certain types of\n\ncross-domain requests to determine whether the request is valid. A preflight request is an HTTP OPTIONS call\n\nwith a few special headers that commonly trip up testers when creating virtual services\n\n[4]\n\n(figure 7.7).\n\n4\n\nFuture versions of mountebank likely will make virtualizing CORS preflight requests easier.\n\nFigure 7.7. A CORS preflight request to establish trust\n\nThe browser is set to automatically send these preflight requests for some types of cross-origin requests. If you\n\nwant to virtualize the cross-origin service so you can test the browser application, your virtual service needs to\n\nknow how to respond to a preflight request in a way that enables the browser to make the actual cross-origin request (for example, the call to POST /resource in figure 7.7). Copying the Origin header value from the\n\nrequest header into the response header, as shown in the following listing, is a great example of using the copy\n\nbehavior and demonstrates tokenizing something other than the HTTP body.\n\nListing 7.15. Virtualizing a CORS preflight request\n\n{\n\n\"predicates\": [{\n\n1\n\n\"equals\": { \"method\": \"OPTIONS\" }\n\n1\n\n}],\n\n1\n\n\"responses\": [{ \"is\": {\n\n\"headers\": {\n\n\"Access-Control-Allow-Origin\":\n\n\"${ORIGIN}\", 2\n\n\"Access-Control-Allow-Methods\": \"PUT,\n\nDELETE\"\n\n}\n\n}, \"_behaviors\": {\n\n\"copy\": [{\n\n\"from\": { \"headers\": \"Origin\" },\n\n3\n\n\"into\": \"${ORIGIN}\",\n\n4\n\n\"using\": { \"method\": \"regex\", \"selector\":\n\n\".+\" } 5 }]\n\n}\n\n}]\n\n}\n\n1 Looks for a preflight request signature\n\n2 Tokenizes the response header\n\n3 Looks in the request Origin header\n\n4 Replaces the response header token...\n\n5 ...with the entire request header value.\n\nThe regular expression “.+” means “one or more characters” and effectively captures the entire request\n\nheader. Because you don’t need to use a grouped match, you can use the token in the response without an array\n\nindex. You can do a lot more with CORS configuration, but the copy approach satisfies the need for creating a\n\nflexible virtual service that reflects the client requests in a way that enables the client to make subsequent\n\nrequests.\n\n7.5.2. Looking up data from an external data source\n\nService virtualization is great for testing error flows,\n\nwhich are often difficult to reproduce on demand in real systems but happen enough in live systems that you still\n\nneed to test for them. The challenge is creating a set of test data that captures all the error flows in a visible and\n\nmaintainable way. Take account creation, for example. Using predicates, you could set up a virtual account\n\nservice that responds with different error conditions based on the name of the account you are trying to\n\ncreate. Let’s say the first error flow you need to test is what happens when the account already exists. The following configuration would ensure your virtual service\n\nreturns an error for a duplicate user when the name is “Kip Brady,” assuming that’s passed in as a JSON name\n\nfield in the request:\n\n{\n\n\"stubs\": [{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Kip Brady\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" }\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"duplicateEntry\",\n\n\"message\": \"User already exists\"\n\n}]\n\n}\n\n} }]\n\n}]\n\n}\n\nIf you want “Mary Reynolds” to represent a user too young to register, you can use the same JSONPath\n\nselector to look for a different value:\n\n{\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Kip Brady\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" }\n\n}], \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"duplicateEntry\",\n\n\"message\": \"User already exists\"\n\n}] }\n\n}\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Mary Reynolds\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" } }],\n\n\"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"tooYoung\",\n\n\"message\": \"You must be 18 years old to register\"\n\n}]\n\n}\n\n}\n\n}]\n\n}\n\n]\n\n}\n\n“Tom Larsen” can represent a 500 server error, and “Harry Smith” can represent an overloaded server. Both\n\nrequire new stubs.\n\nThis is obviously an unsustainable approach to managing\n\ntest data. The JSONPath selector is the same among all the stubs, as is the structure of the JSON body. You’d like\n\nto be able to centralize the test data in a CSV file, as shown in the following listing.\n\nListing 7.16. Centralizing error conditions in a CSV file\n\nname,statusCode,errorCode,errorMessage\n\nTom Larsen,500,serverError,An unexpected error\n\noccurred\n\nKip Brady,400,duplicateEntry,User already exists\n\nMary Reynolds,400,tooYoung,You must be 18 years\n\nold to register Harry Smith,503,serverBusy,Server currently\n\nunavailable\n\nThe lookup behavior, a close cousin of the copy\n\nbehavior, allows you to do this. Like the copy behavior, it replaces tokens in the response with dynamic data. The\n\nkey difference is where that dynamic data comes from. For the copy behavior, it’s the request. For the lookup\n\nbehavior, it’s an external data source. As of this writing, the only data source mountebank supports is a CSV file\n\n(figure 7.8). That likely will change by the time you are reading this.\n\nFigure 7.8. Looking up a value from a CSV file\n\nBefore we get to how the replacement happens, let’s look\n\nat the lookup operation itself. As you can see in figure 7.8, a successful lookup requires three values:\n\nA key selected from the request (Kip Brady)\n\nThe connection to the external data source (data/accounts.csv)\n\nThe key column in the external data source (name)\n\nThose three values are sufficient to capture a row of values you can use in the replacement. You represent them in a way that closely resembles the copy behavior, as shown in the following listing.\n\nListing 7.17. Using a lookup behavior to retrieve external test data\n\n{\n\n\"responses\": [{\n\n\"is\": { ... },\n\n1\n\n\"_behaviors\": {\n\n\"lookup\": [{\n\n\"key\": {\n\n2 \"from\": \"body\",\n\n2\n\n\"using\": {\n\n2\n\n\"method\": \"jsonpath\",\n\n2\n\n\"selector\": \"$..name\"\n\n2\n\n} 2\n\n},\n\n2\n\n\"fromDataSource\": {\n\n\"csv\": {\n\n3\n\n\"path\": \"examples/accounts.csv\",\n\n4 \"keyColumn\": \"name\"\n\n5\n\n}\n\n},\n\n\"into\": \"${row}\"\n\n6\n\n}]\n\n} }]\n\n}\n\n1 We’ll come back to this.\n\n2 Selects the value from the request\n\n3 Type of data source\n\n4 Path to CSV file\n\n5 Column name to match request value\n\n6 Response token name\n\nThe key part of the lookup behavior is similar to the\n\ncopy behavior we looked at earlier. It allows you to use a regex, xpath, or jsonpath selector to look in a\n\nrequest field and grab a value. You can add an index field to use a grouped regular expression match.\n\nThe into field is also the same as what you saw with\n\ncopy. Here you used ${row} as the token name. It can be anything you like. As far as mountebank is concerned,\n\nit it’s a string. The addition is what you see in the fromDataSource field. For CSV data sources, you\n\nspecify the path to the file (relative to the running mb process) and the name of the key column.\n\nIf you pass \"Kip Brady\" in as the name, your token (${row}) matches an entire row of values from the CSV\n\nfile. In JSON format, it would look like this:\n\n{\n\n\"name\": \"Kip Brady\",\n\n\"statusCode\": \"400\",\n\n\"errorCode\": \"duplicateEntry\",\n\n\"errorMessage\": \"User already exists\"\n\n}\n\nThis highlights a secondary difference between copy and lookup: with a lookup behavior, your token represents an entire row of values, meaning each individual\n\nreplacement has to be indexed with the column name. Let’s look at the is response for your example, which\n\ntokenizes the responses you previously had to copy into multiple stubs, in the following listing.\n\nListing 7.18. Using tokens to create a single response for all error conditions\n\n{\n\n\"is\": {\n\n\"statusCode\": \"${row}['statusCode']\", 1\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"${row}['errorCode']\", 1\n\n\"message\": \"${row}['errorMessage']\" 1\n\n}] }\n\n},\n\n\"_behaviors\": {\n\n\"lookup\": [{ ... }] 2\n\n}\n\n}\n\n1 Looks up the appropriate field in the row you looked up\n\n2 See listing 7.17.\n\nThe lookup behavior treats the token like a JSON object that you can key by the column name. This allows you to retrieve an entire row of data in the lookup behavior and use fields within the row to populate the response.\n\n7.6. A COMPLETE LIST OF BEHAVIORS\n\nFor reference, table 7.1 provides a complete list of behaviors that mountebank supports, including whether\n\nthey support affecting saved proxy responses and whether they require the --allowInjection\n\ncommand-line flag.\n\nTable 7.1. All behaviors that mountebank supports\n\nBehavior\n\nWorks on saved proxy responses?\n\nRequires injection support?\n\nDescription\n\ndecorate\n\nshellTransform\n\nwait\n\nrepeat\n\ncopy\n\nlookup\n\nyes\n\nno\n\nyes\n\nno\n\nno\n\nno\n\nyes\n\nyes\n\nno\n\nno\n\nno\n\nno\n\nUses a JavaScript function to postprocess the response Sends the response through a command- line pipeline for postprocessing Adds latency to a response Repeats a response multiple times Copies a value from the request into the response Replaces data in the response with data from an external data source based on a key from the request\n\nBehaviors are powerful additions to mountebank, and we\n\ncovered a lot of ground in this chapter. We will round out mountebank’s core capabilities in the next chapter when\n\nwe look at protocols.\n\nSUMMARY\n\nThe decorate and shellTransform behaviors are similar to response injection in that they allow programmatic transformation\n\nof the response. But they apply postprocessing transformation, and the shellTransform allows multiple transformations.\n\nThe wait behavior allows you to add latency to a response by passing in the number of milliseconds to delay it.\n\nThe repeat behavior supports sending the same response multiple times.\n\nThe copy behavior accepts an array of configurations, each of which selects a value from the request and replaces a response\n\ntoken with that value. You can use regular expressions, JSONPath,\n\nand XPath to select the request value.\n\nThe lookup behavior also accepts an array of configurations, each of which looks up a row of data from an external data source based\n\non the value selected from the request using regular expressions,\n\nJSONPath, and XPath. The token in the response is indexed by the field name.\n\nChapter 8. Protocols\n\nThis chapter covers\n\nHow protocols work in mountebank\n\nAn in-depth look at the TCP protocol\n\nHow to stub a bespoke text-based TCP protocol\n\nHow to stub a binary .NET Remoting service\n\nLet’s get real: faking it only gets you so far. At some\n\npoint, even the best virtual services have to lay down some real bits on the wire.\n\nThe functionality we have explored so far—responses,\n\npredicates, and behaviors—is largely the realm of stubbing and mocking tools. Those stubbing and\n\nmocking tools were created for creating test doubles in process, allowing you to perform focused tests by\n\nmethodically manipulating dependencies. Responses correspond to what stub functions return, and predicates\n\nexist to provide different results based on the way the function is called (for example, returning a different\n\nresult based on a request parameter). I’m not aware of any in-process mocking tools that have the concept of\n\nbehaviors per se, but there’s nothing preventing them. Behaviors are transformations on the result.\n\nBut there is one thing that virtual services do that\n\ntraditional stubs don’t: they respond over the network. All the responses so far have been HTTP, but\n\nmountebank supports other ways of responding. Enterprise integration is often messy, and sometimes\n\nyou’ll need to virtualize non-HTTP services. Whether you have custom remote procedure calls or a mail server as\n\npart of your stack, service virtualization can help. And now, at long last, it’s time to explore mountebank’s\n\nsupport for network protocols.\n\n8.1. HOW PROTOCOLS WORK IN MOUNTEBANK\n\nProtocols are where the rubber meets the road in\n\nmountebank. The core role of the protocol is to translate the incoming network request into a JSON\n\nrepresentation for predicates to operate on, and to translate the JSON mountebank response structure into\n\nthe wire format expected by the system under test (figure 8.1).\n\nFigure 8.1. The flow of a mountebank-generated HTTP response\n\nAll of the mountebank imposters that we’ve seen so far\n\nare full-featured HTTP servers, and the bits they put on the wire conform to the HTTP protocol. That’s the secret\n\nsauce that allows you to repoint the system under test to mountebank with no changes: it sends out an HTTP\n\nrequest and gets an HTTP response back. It has no need to know that an imposter formed the response by\n\nemploying a secret cabal of stubs to match the request",
      "page_number": 215
    },
    {
      "number": 8,
      "title": "Protocols",
      "start_page": 250,
      "end_page": 287,
      "detection_method": "regex_chapter_title",
      "content": "with predicates, form a JSON response, and postprocess it with behaviors. All the system under test cares about is\n\nthat mountebank accepts an HTTP request and returns a bunch of bits over the wire that look like an HTTP\n\nresponse.\n\nMountebank, unique in the open source service\n\nvirtualization world, is multiprotocol. The clean separation of concepts allows the stubbing functionality\n\nto work regardless of what protocol your system under test expects. We’ll explore that in the context of older\n\nremote procedure call (RPC) protocols, but before we get there, let’s start with a primer on a foundational building\n\nblock of network communication.\n\n8.2. A TCP PRIMER\n\nMountebank supports the TCP protocol, but TCP isn’t on equal footing with HTTP/S. It’s more accurate to say that\n\nmountebank supports a range of custom application protocols built on top of TCP. Most conceptual\n\nnetworking models show protocols in layers, and it takes a whole suite of protocols to make something as complex\n\nas the internet work. (See figure 8.2.)\n\nFigure 8.2. A client application talking to a server application over the internet\n\nThe genius of the TCP/IP stack is that clients and server\n\nprocesses can act as if they’re talking directly to each other, even when they’re on remote machines. When you\n\nvisit the mountebank website (http://www.mbtest.org), your browser can act as if it’s speaking directly with the\n\nweb server on the other end. The same is true when your system under test makes an HTTP call. Regardless of\n\nwhether the service it’s accessing is real or virtualized, the client code can operate as if there’s a direct\n\nconnection to the server.\n\nThe reality is more complicated. HTTP is an application\n\nprotocol and depends on TCP as the transport protocol to deliver to the remote host. TCP in turn relies on\n\ndownstream protocols to route between networks (the IP protocol) and to interface with the routers on the same\n\nnetwork (the network is often referred to as the “link,” which is why the lowest layer is called the link layer in\n\nfigure 8.2).\n\nWhat your web browser is doing is forming an HTTP request and passing it to the local operating system,\n\nwhich hands off the request to the TCP protocol implementation. TCP lovingly wraps the HTTP message\n\nin an envelope that adds a bunch of delivery guarantees and performance optimizations. Then it hands control\n\nover to the IP protocol, which once again wraps the whole message and adds addressing information that the\n\ncore infrastructure of the internet knows how to use to route to the correct remote machine. Finally, the doubly\n\nwrapped HTTP message is handed off to the device driver for the network interface on your computer, which\n\nyet again wraps the message with some Ethernet or Wi- Fi information needed to transmit the whole package to\n\nyour router, which happily forwards it to the next network. (See figure 8.3.) The process works in reverse once it reaches the right server machine.\n\n[1]\n\n1\n\nFigure 8.3 is inspired by a similar image on the Wikipedia page describing the internet protocol suite, which has a more comprehensive explanation of how layering works: https://en.wikipedia.org/wiki/Internet_protocol_suite.\n\nFigure 8.3. Transforming an HTTP request to route across the network\n\nTCP enables host-to-host communication, but it’s the\n\napplication protocols on top that allow a client process to talk to a server process. HTTP is the most famous\n\napplication protocol but far from the only one. Mountebank aspires to treat well-known application\n\nprotocols like HTTP as first class citizens, but a host of niche or custom application protocols exist in the archaeological substratum of enterprise integration, and\n\nmountebank’s support of the TCP protocol also provides a way to virtualize them.\n\n8.3. STUBBING TEXT-BASED TCP-BASED RPC\n\nFor those of you who grew up in a world where distributed programming was commonplace, it can be a\n\nbit bewildering to look at some of the ways applications used to integrate. Imagine you are a C++ programmer in\n\na bygone era being paid to integrate two applications over the network. The challenges of distributed\n\nprogramming aren’t commonly understood yet. Early attempts at formalizing RPC using standards like CORBA\n\nmay have happened, but they seem overly complicated. It seems much simpler to pass a function name and a few\n\nparameters to a remote socket and expect it to return a status code indicating whether the function succeeded\n\nand a return value. If you take out the networking, it looks similar to how in-process function calls work.\n\nNow, 20 or 30 years later, younger generations are still adding capabilities to your custom RPC code because it’s\n\nso central to keeping the lights on that it’s cheaper to keep it than to rip it out. They may not like it, but none of\n\nthem have ever written code that’s stayed in production for decades. Like it or not, this is a common scenario in\n\nmany long-standing enterprises.\n\nLet’s imagine that the remote server manages core inventory. The payload of a TCP packet making an RPC\n\nrequest to change the inventory may look like this, for example:\n\nupdateInventory 1\n\n5131 2 -5 3\n\n1 Function name\n\n2 First parameter (for example, productId)\n\n3 Second parameter (for example, inventoryDelta)\n\nAnd the response may look like this:\n\n0 1\n\n1343 2\n\n1 Status code\n\n2 Total inventory\n\nIn this example, new lines separate parameters, and the\n\nschema is implicit rather than defined by something like JSON or XML. Mountebank won’t be able to understand\n\nthe semantics of the RPC, so it encapsulates the payload in a single data field.\n\nThe flow of data, as shown in figure 8.4, looks similar to the flow of data in a virtual HTTP service (figure 8.1). The\n\nonly difference is the format for the request and response.\n\nFigure 8.4. Virtualizing a custom TCP protocol\n\n8.3.1. Creating a basic TCP imposter\n\nCreating the TCP imposter is as simple as changing the protocol field to tcp:\n\n{ \"protocol\": \"tcp\",\n\n\"port\": 3000\n\n}\n\nThat configuration is sufficient for mountebank to spin\n\nup a TCP server listening on port 3000. It’ll accept the request, but the response will be blank. If you want to\n\nvirtualize the call to updateInventory, you do so with the same predicates and response capability that you’ve\n\nseen for HTTP, as shown in the following listing. The only difference is the JSON structure for requests and responses. For tcp, both the request and response contain a single data field.\n\nListing 8.1. Virtualizing a TCP updateInventory call\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"predicates\": [{\n\n\"startsWith\": { \"data\": \"updateInventory\" }\n\n1\n\n}], \"responses\": [{\n\n\"is\": { \"data\": \"0\\n1343\" }\n\n2\n\n}]\n\n}]\n\n}\n\n1 Looks for the appropriate function\n\n2 Returns the protocol-specific response format\n\nYou can test out your imposter using an application like telnet. Telnet opens an interactive TCP connection to a\n\nserver, which makes it tricky to script. Netcat (http://nc110.sourceforge.net/) is like a noninteractive\n\ntelnet, which makes it ideally suited for testing TCP- based services. You can trigger your inventory RPC call\n\nresponse and test out your TCP imposter with netcat using the following command:\n\necho \"updateInventory\\na32fbd\\n-5\" | nc localhost\n\n3000\n\nYou wrap the request message in a string and pipe it to netcat (nc), sending it to the correct socket. It’ll send\n\nback the virtual response you configured to the terminal:\n\n0\n\n1343\n\n8.3.2. Creating a TCP proxy\n\nTCP imposters work with the other response types as well. You can record and replay using a proxy like you\n\ncan with HTTP. For example, if the real service is listening on port 3333 of remoteservice.com, you could set up a record/replay imposter by pointing the to field on the proxy configuration to the remote socket, as\n\nshown in the following listing.\n\nListing 8.2. Using a TCP record/replay proxy\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{ \"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"tcp://remoteservice.com:3333\" 1\n\n}\n\n}]\n\n}]\n\n}\n\n1 Destination of remote service\n\nThe proxy behavior is identical to what you saw in chapter 5. In the default mode (proxyOnce),\n\nmountebank will save the response and serve it up on the next request without making the downstream call again.\n\nYou can test this out with netcat:\n\necho \"updateInventory\\na32fbd\\n-5\" | nc -q 1\n\nlocalhost 3000\n\nNotice the added “-q 1” parameter. By default, when netcat makes a TCP request, it closes the client end of the\n\nconnection immediately, which is appropriate for one- way fire-and-forget-style communication. It isn’t\n\nappropriate for two-way request-response-style communication common in RPC. Because it takes a\n\nsmall amount of time for mountebank to make the downstream call to get the response, by the time\n\nmountebank tries to respond, it may discover that nobody is listening. The “-q 1” parameter tells netcat to\n\nwait one second before closing the connection, so you’ll see the response on the terminal.\n\nUnfortunately, the “-q” parameter isn’t present on all versions of netcat, including the default version on Mac\n\nand Windows computers. If you leave it off, you won’t get a response on the terminal and you’ll see the error in the\n\nmountebank logs when it cannot send the response. But subsequent calls will still work, as mountebank now has\n\na saved version of the response and can respond immediately.\n\nYou can use predicateGenerators with the TCP\n\nprotocol as well, but it isn’t very discriminatory because there’s only one field. For example, the following\n\nconfiguration makes a new downstream call anytime anything in the RPC call is different.\n\nListing 8.3. A TCP proxy with predicateGenerators\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://remoteservice.com:3333\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"data\": true } 1\n\n}]\n\n}\n\n}]\n\n}]\n\n}\n\n1 Generates a new stub for each new payload\n\nAlthough it would be nice to generate predicates only on the function name, you can’t. Mountebank has no way of\n\nknowing what the function name is, so any parameter that changes will force a new downstream call.\n\nIf you’re lucky, the custom RPC protocol uses a payload format that mountebank understands: JSON or XML. If\n\nso, then you can get a bit more specific. We look at an example next.\n\n8.3.3. Matching and manipulating an XML payload\n\nI doubt you’ll see many of these custom RPC protocols\n\nthat use JSON, for the simple reason that by the time JSON was created, HTTP was already a predominant\n\napplication integration approach. If you do see it, all of the JSONPath capability you’ve seen so far will work.\n\nXML has been around a bit longer, and in fact one of the\n\nfirst attempts at using HTTP for integration was called POX over HTTP, where POX stood for Plain Ol’ XML. Let’s translate your updateInventory RPC payload to XML:\n\n<functionCall>\n\n<functionName>updateInventory</functionName>\n\n<parameters>\n\n<parameter name=\"productId\" value=\"5131\" /> <parameter name=\"amount\" value=\"-5\" />\n\n</parameters>\n\n</functionCall>\n\nYou can easily imagine other function calls, for example,\n\nthis one might be a call to get the inventory for product 5131:\n\n<functionCall> <functionName>getInventory</functionName>\n\n<parameters>\n\n<parameter name=\"productId\" value=\"5131\" />\n\n</parameters>\n\n</functionCall>\n\nNow it becomes easier to build a more robust set of predicateGenerators for your proxy. If you want to\n\nsave different responses for different combinations of function names and product IDs, you can do so, as\n\nshown in the following listing.\n\nListing 8.4. Using XPath predicateGenerators with a TCP proxy\n\n{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\",\n\n\"predicateGenerators\": [\n\n{\n\n\"matches\": { \"data\": true },\n\n1\n\n\"xpath\": {\n\n1\n\n\"selector\": \"//functionName\" 1\n\n}\n\n1\n\n},\n\n{\n\n\"matches\": { \"data\": true },\n\n2\n\n\"xpath\": { 2\n\n\"selector\":\n\n2\n\n\"//parameter[@name='productId']/@value\"\n\n2\n\n}\n\n2\n\n} ]\n\n}\n\n}]\n\n}\n\n1 The XML functionName must match.\n\n2 The productId must match.\n\nAll of the behaviors work with the TCP protocol as well, including those like the copy behavior that can use\n\nXPath to select values from the request.\n\n8.4. BINARY SUPPORT\n\nNot all application protocols speak in plain text. Many of\n\nthem instead pass a binary request/response stream, which makes them challenging to virtualize. Challenging,\n\nbut not impossible—remember mountebank’s mission statement: to keep the easy things easy while making the\n\nhard things possible.\n\nMountebank makes virtualizing binary protocols possible in two ways. First, it supports serializing request\n\nand response binary streams using Base64 encoding. Second, nearly all of the predicates work against a binary\n\nstream with exactly the same semantics they use against text.\n\n8.4.1. Using binary mode with Base64 encoding\n\nJSON—the lingua franca of mountebank—doesn’t\n\ndirectly support binary data. The workaround is to encode a binary stream into a string field. Base64\n\nreserves 64 characters—upper- and lowercase letters, digits, and two punctuation marks—and maps them to\n\nthe binary equivalents. Having 64 options allows you to 6 encode six bits at a time (2 = 64).\n\nAny modern language library will support Base64 encoding. Here’s an example in JavaScript (node.js):\n\nvar buffer = new Buffer(\"Hello, world!\"); console.log(buffer.toString(\"base64\"));\n\n1\n\n1 Prints out SGVsbG8sIHdvcmxkIQ==\n\nSimilarly, decoding from Base64 is done using the Buffer type as well, as in this JavaScript example:\n\nvar buffer = new Buffer(\"SGVsbG8sIHdvcmxkIQ==\");\n\n1\n\nconsole.log(buffer.toString(\"utf8\"));\n\n2\n\n1 Base64 encoded value\n\n2 Prints out Hello, world!\n\nHaving a TCP imposter return binary data requires letting mountebank know that you want a binary\n\nimposter and Base64 encoding the response, as shown in the following listing.\n\nListing 8.5. Setting up a binary response from an imposter\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n1\n\n\"stubs\": [{ \"responses\": [{\n\n\"is\": { \"data\": \"SGVsbG8sIHdvcmxkIQ==\" }\n\n2\n\n}]\n\n}]\n\n}\n\n1 Switches to binary mode\n\n2 Returns binary equivalent of “Hello, world!”\n\nOnce you set the mode to binary, mountebank knows to\n\ndo the following:\n\nInterpret all response data as Base64-encoded binary streams,\n\nwhich it will decode when it responds over the wire\n\nSave all proxy responses as Base-64-encoded versions of the wire\n\nresponse\n\nInterpret all predicates as Base-64-encoded binary streams, which\n\nit will decode to match against the raw wire request\n\nThat last bullet point requires more explanation.\n\n8.4.2. Using predicates in binary mode\n\nBinary data is a stream of bits. It might be, for example,\n\n01001001 00010001. I have spaced the bits into two eight-bit combinations (octets) because, although a long\n\nstring of zeros and ones can be poetry to a computer, it gets a little tricky for us human folk to read. Using two\n\noctets also allows you to encode them as two numbers from 0–255 (2^8). In this case, that would be 73 17, or,\n\nin hexadecimal, 0x49 0x11. Hexadecimal is nice because it lacks any ambiguity—each two-digit hexadecimal\n\nnumber has 256 possibilities (16^2), the same number of possibilities encoded in an eight-bit octet.\n\nLet’s say you wanted to create a predicate that contains\n\nthat binary stream. To do so, you’d first need to encode it:\n\nvar buffer = new Buffer([0x49, 0x11]);\n\nconsole.log(buffer.toString('base64')); 1\n\n1 Prints out “SRE=”\n\nNow the predicate definition is conceptually the same as it is for text, as the following listing demonstrates.\n\nListing 8.6. Using a binary contains predicate\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n1 \"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"contains\": { \"data\": \"SRE=\" }\n\n2\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"data\": \"TWF0Y2hlZAo=\" } 3\n\n}]\n\n},\n\n{\n\n\"responses\": [{\n\n\"is\": { \"data\": \"RGlkIG5vdCBtYXRjaAo=\" } 4\n\n}]\n\n}\n\n]\n\n}\n\n1 Puts the imposter in binary mode\n\n2 0x49 0x11\n\n3 “Matched”\n\n4 “Didn’t match”\n\nIf we add an octet—say 0x10—to our binary stream, the contains predicate still matches. The binary stream 0x10 0x49 0x11 encodes to “EEkR,” which clearly doesn’t\n\ncontain the text “SRE=”. Had you not configured the imposter to be in binary mode, mountebank would’ve\n\nperformed a simple string operation, and the predicate wouldn’t match. By switching to binary mode, you’re\n\ntelling mountebank to instead decode the predicate (SRE=) to a binary array ([0x49, 0x11]) and to see if the\n\nincoming binary stream ([0x10, 0x49, 0x11]) contains those octets. It does, so the predicate matches. You can test on the command line by using the base64 utility, which ships by default with most POSIX shells (for\n\nexample, Mac and Linux):\n\necho EEkR | base64 --decode | nc localhost 3000\n\nYou get a response of “Matched,” which corresponds to\n\nthe first response.\n\nNearly all of the predicates work this way: by matching against a binary array. The one exception is matches.\n\nRegular expressions don’t make sense in a binary world; the metacharacters don’t translate. In practice,\n\ncontains is probably the most useful binary predicate. It turns out that many binary RPC protocols encode a\n\nfunction name inside the request. The parameters may be serialized objects that are hard to match, but the\n\nfunction name is encoded text. We look at a real-world example next.\n\n8.5. VIRTUALIZING A .NET REMOTING SERVICE\n\nBack in the days of yore, a town crier often made public announcements. A crier would ring a handbell and shout\n\n“Hear ye! Hear ye!” to get everyone’s attention before making the announcement. This neatly solved the\n\nproblem of delivering a message to a public that was still largely illiterate.\n\n[2]\n\nProviding a town crier RPC service may come off as a\n\nlittle antiquated, but it helps get your mindset into those halcyon days of yesteryear when we thought making\n\nremote function calls look like in-process function calls was a good thing.\n\n[3]\n\n.NET Remoting wasn’t the first\n\nattempt at creating a largely transparently distributed RPC mechanism, but it did have a brief period of\n\npopularity and is representative of the broader class of RPC protocols you are likely to run across in the\n\nenterprise.\n\n2\n\nOK, the scenario may not be quite as real-world as advertised, but the protocol is....\n\n3\n\nPeter Deutsch wrote that nearly everyone who first builds a distributed application makes a key set of assumptions, all of which are false in the long run and inevitably cause big trouble. See https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing.\n\n8.5.1. Creating a simple .NET Remoting client\n\n.NET Remoting allows you to call a remote method like\n\nyou would a local method using the .NET framework. For\n\nexample, let’s assume that to make a pronouncement, you have to fill in an AnnouncementTemplate:\n\n[Serializable]\n\n1\n\npublic class AnnouncementTemplate\n\n{ public AnnouncementTemplate(string greeting,\n\nstring topic)\n\n{\n\nGreeting = greeting;\n\n2\n\nTopic = topic;\n\n3\n\n}\n\npublic string Greeting { get; }\n\n4\n\npublic string Topic { get; }\n\n4\n\n}\n\n1 Ensures that it can be passed over the wire\n\n2 So you can change “Hear ye!” to “Oyez!”\n\n3 The topic to be announced\n\n4 Public getters\n\nDon’t worry if you’re not a C# expert. This code is as\n\nsimple as it gets in most enterprise applications, which often are written in Java or C#. It creates a basic class\n\nthat accepts the greeting and topic for the pronouncement and exposes them as read-only\n\nproperties. The only interesting nuance is the [Serializable] attribute at the top. That’s a bit of C#\n\nmagic that allows the object to be passed between processes.\n\nOnce you create an AnnouncementTemplate, you’ll\n\npass it to the Announce method of the Crier class, as defined in the following listing.\n\n[4]\n\n4\n\nThe source code for this example is considerably more complicated than most other examples in this book. As always, you can download it at https://github.com/bbyars/mountebank-in-action. In addition, many of the command line examples in the book have shown a bit of a bias toward macOS and Linux. This example is geared toward Windows and will require\n\nadditional effort to run on other operating systems.\n\nListing 8.7. The Crier class definition\n\npublic class Crier : MarshalByRefObject 1\n\n{\n\npublic AnnouncementLog Announce(\n\nAnnouncementTemplate template)\n\n{\n\nreturn new AnnouncementLog(\n\n2\n\n$\"{template.Greeting}! {template.Topic}\");\n\n}\n\n}\n\n1 Allows serialization over the wire\n\n2 Returns a log capturing the announcement\n\nThe Crier class inherits from MarshalByRefObject.\n\nAgain, that is largely irrelevant to the example, except for the fact that it’s the bit of magic that allows an instance of the Crier class to be called from a remote process. The Announce method formats the greeting and topic\n\ninto a single string (that strange $\" {template.Greeting}! {template.Topic}\" line\n\nis C#’s string interpolation) and returns it wrapped inside an AnnouncementLog object, which looks like\n\nthis:\n\n[Serializable]\n\n1\n\npublic class AnnouncementLog\n\n{\n\npublic AnnouncementLog(string announcement)\n\n{ When = DateTime.Now;\n\n2\n\nAnnouncement = announcement;\n\n}\n\npublic DateTime When { get; }\n\npublic string Announcement { get; }\n\npublic override string ToString()\n\n{\n\nreturn $\"({When}): {Announcement}\"; 3\n\n}\n\n}\n\n1 Makes it remotely accessible\n\n2 Captures the time of the announcement\n\n3 Formats a log entry for the announcement\n\nFor our purposes, the AnnouncementTemplate,\n\nCrier, and AnnouncementLog form the entirety of the domain model. You could’ve simplified it to the Crier\n\nclass, but we used to think that passing entire object graphs over the wire was a good idea, and adding a couple of simple classes that the Crier uses—one as input, one as output—helps make the example ever so\n\nslightly more realistic.\n\nYou can call Crier locally, but that’s kind of boring, and\n\nmore in the realm of traditional mocking tools should you decide to test it. Instead, you will call a remote Crier instance. The source repo for this book shows how to code a simple server that listens on a TCP socket and acts as a remote Crier. We’ll focus on the client, testing it by virtualizing the server. To do that, your virtual service needs to respond like a .NET TCP\n\nRemoting service would.\n\nThe following listing shows the client you’ll test. It represents a gateway to a remote Crier instance.\n\nListing 8.8. A gateway to a remote Crier\n\npublic class TownCrierGateway\n\n{ private readonly string url;\n\npublic TownCrierGateway(int port)\n\n{\n\nurl = $\"tcp://localhost:\n\n{port}/TownCrierService\"; 1\n\n}\n\npublic string AnnounceToServer(\n\nstring greeting, string topic)\n\n{\n\nvar template = new AnnouncementTemplate(\n\ngreeting, topic);\n\nvar crier = (Crier)Activator.GetObject(\n\n2\n\ntypeof(Crier), url); var response = crier.Announce(template);\n\n3\n\nreturn $\"Call Success!\\n{response}\";\n\n4\n\n}\n\n}\n\n1 URL to the remote service\n\n2 Gets a remote object reference\n\n3 Makes the (remote) method call\n\n4 Adds metadata to the response\n\nNotice that the call to crier.Announce looks like a\n\nlocal method call. It’s not. This is the magic of .NET Remoting. The line above retrieves a remote reference to\n\nthe object based on the URL in the constructor. That desire to make remote function calls look like local\n\nfunction calls is highly representative of this era of distributed computing.\n\n8.5.2. Virtualizing the .NET Remoting server\n\nAll the TownCrierGateway class does is add a success\n\nmessage to the response of the remote call. That’s enough for you to write a test without getting lost in too\n\nmuch unnecessary complexity. You could write the test two ways, assuming you aim to virtualize the remote\n\nservice.\n\nThe first way is to create a mountebank stub that proxies the remote service and captures the response. You could\n\nreplay the response in your test.\n\nThe second way is much cooler. You could create the response (as in an instance of the AnnouncementLog class) in the test itself, as you would with traditional\n\nmocking tools, and have mountebank return it when the client calls the Announce method. Much cooler.\n\nFortunately, Matthew Herman has written an easy to use\n\n[5]\n\nmountebank library for C# called MbDotNet. it to create a test fixture. I like to write tests using the\n\nLet’s use\n\nLaw of Wishful Thinking, by which I mean I write the code I want to see and figure out how to implement it\n\nlater. This allows my test code to clearly specify the intent without getting lost in the details. In this case, I\n\nwant to create the object graph that mountebank will return inside the test itself and pass it off to a function\n\nthat creates the imposter on port 3000 using a contains predicate for the remote method name. That’s\n\na lot to hope for, but I’ve wrapped it up in a function called CreateImposter, as shown in the following\n\nlisting.\n\n5\n\nAn entire ecosystem of these client bindings exists for mountebank. I do my best to maintain a list at http://www.mbtest.org/docs/clientLibraries, but you can always search GitHub for others. Feel free to add a pull request to add your own library to the mountebank website.\n\nListing 8.9. Basic test fixture using MbDotNet\n\n[TestFixture]\n\npublic class TownCrierGatewayTest\n\n{\n\nprivate readonly MountebankClient mb =\n\n1 new MountebankClient();\n\n1\n\n[TearDown]\n\n2\n\npublic void TearDown()\n\n2\n\n{\n\n2\n\nmb.DeleteAllImposters(); 2\n\n}\n\n2\n\n[Test]\n\npublic void ClientShouldAddSuccessMessage()\n\n{\n\nvar stubResult = new\n\nAnnouncementLog(\"TEST\"); 3 CreateImposter(3000, \"Announce\",\n\nstubResult); 3\n\nvar gateway = new TownCrierGateway(3000);\n\n3\n\nvar result = gateway.AnnounceToServer(\n\n4\n\n\"ignore\", \"ignore\"); 4\n\nAssert.That(result, Is.EqualTo(\n\n5\n\n$\"Call Success!\\n{stubResult}\"));\n\n5\n\n}\n\n}\n\n1 MbDotNet’s gateway to the mountebank REST API\n\n2 Deletes all imposters after every test\n\n3 Arrange\n\n4 Act\n\n5 Assert\n\n[6]\n\nThis fixture uses NUnit annotations NUnit ensures that the TearDown method will be called after every test, which allows you to elegantly clean up\n\nto define a test.\n\nafter yourself. When you create your test fixture, you create an instance of the mountebank client (which assumes mb is already running on port 2525) and remove all imposters after every test. This is the typical pattern\n\nwhen you use mountebank’s API for functional testing.\n\n6\n\n6\n\nA popular C# testing framework; see http://nunit.org/.\n\nThe test itself uses the standard Arrange-Act-Assert\n\npattern of writing tests introduced back in chapter 1. Conceptually, the Arrange stage sets up the system under test, creating the TownCrierGateway and ensuring that when it connects to a virtual service (on\n\nport 3000), the virtual service responds with the wire format for the object graph represented by stubResult.\n\nThe Act stage calls the system under test, and the Assert stage verifies the results. This is nearly identical to what you would do with traditional mocking tools.\n\nWishful thinking only gets you so far. MbDotNet simplifies the process of wiring up your imposter using\n\nC#. You’ll delay only the serialization format for the response under a wishfully-thought-of method I have named Serialize:\n\nprivate void CreateImposter(int port,\n\nstring methodName, AnnouncementLog result)\n\n{\n\nvar imposter = mb.CreateTcpImposter(\n\nport, \"TownCrierService\", TcpMode.Binary);\n\nimposter.AddStub()\n\n.On(ContainsMethodName(methodName))\n\n1\n\n.ReturnsData(Serialize(result));\n\n2\n\nmb.Submit(imposter);\n\n3 }\n\nprivate ContainsPredicate<TcpPredicateFields>\n\nContainsMethodName(\n\nstring methodName)\n\n{\n\nvar predicateFields = new TcpPredicateFields\n\n{ Data = ToBase64(methodName)\n\n};\n\nreturn new\n\nContainsPredicate<TcpPredicateFields>(\n\npredicateFields);\n\n}\n\nprivate string ToBase64(string plaintext)\n\n{\n\nreturn Convert.ToBase64String(\n\nEncoding.UTF8.GetBytes(plaintext));\n\n}\n\n1 Adds predicate\n\n2 Adds response\n\n3 Calls the REST API (We’ll get to the Serialize method\n\nsoon.)\n\nThe CreateImposter and ContainsMethodName\n\nmethods uses the MbDotNet API, which is a simple wrapper around the mountebank REST API. The REST call is made when you call mb.Submit. The ToBase64 method uses the standard .NET library calls to encode a\n\nstring in Base64 format.\n\nAll that’s left is to fill in the Serialize method. This is the method that has to accept the object graph you want\n\nyour virtual service to return and transform it into the stream of bytes that looks like a .NET Remoting\n\nresponse. That means understanding the wire format of .NET Remoting.\n\nThat’s hard.\n\nThe good news is that, with many popular RPC protocols, someone else has usually done the hard work for you.\n\nFor .NET Remoting, that someone else is Xu Huang, who has created .NET Remoting parsers for .NET, Java, and\n\n[7]\n\nJavaScript. create the Serialize function.\n\nYou’ll use the .NET implementation to\n\n7\n\nSee https://github.com/wsky/RemotingProtocolParser.\n\nThe code appears in listing 8.10. Don’t try too hard to\n\nunderstand it all. The point isn’t to teach you the wire format for .NET Remoting. Instead, it’s to show that,\n\nwith a little bit of work, you can usually create a generalized mechanism for serializing a stub response\n\ninto the wire format for real-world RPC protocols. Once you have done the hard work, you can reuse it\n\nthroughout your test suite to make writing tests as easy as creating the object graph you want the virtual service\n\nto respond with and letting your serialization function do the work of converting it to an RPC-specific format.\n\nListing 8.10. Serializing a stub response for .NET Remoting\n\npublic string Serialize(Object obj)\n\n{\n\nvar messageRequest = new MethodCall(new[] {\n\n1\n\nnew Header(MessageHeader.Uri, \"tcp://localhost:3000/TownCrier\"),\n\nnew Header(MessageHeader.MethodName,\n\n\"Announce\"),\n\nnew Header(MessageHeader.MethodSignature,\n\nSignatureFor(\"Announce\")),\n\nnew Header(MessageHeader.TypeName,\n\ntypeof(Crier).AssemblyQualifiedName),\n\nnew Header(MessageHeader.Args, ArgsFor(\"Announce\"))\n\n});\n\nvar responseMessage = new\n\nMethodResponse(new[] c\n\n{\n\nnew Header(MessageHeader.Return, obj)\n\n}, messageRequest);\n\nvar responseStream =\n\nBinaryFormatterHelper.SerializeObject(\n\nresponseMessage);\n\nusing (var stream = new MemoryStream())\n\n{\n\nvar handle = new\n\nTcpProtocolHandle(stream);\n\nhandle.WritePreamble(); 3\n\nhandle.WriteMajorVersion();\n\n3\n\nhandle.WriteMinorVersion();\n\n3\n\nhandle.WriteOperation(TcpOperations.Reply); 3\n\nhandle.WriteContentDelimiter(\n\n3 TcpContentDelimiter.ContentLength);\n\n3\n\nhandle.WriteContentLength(\n\n3\n\nresponseStream.Length);\n\n3\n\nhandle.WriteTransportHeaders(null);\n\n3 handle.WriteContent(responseStream);\n\n4\n\nreturn Convert.ToBase64String(\n\n5\n\nstream.ToArray());\n\n}\n\n}\n\nprivate Type[] SignatureFor(string methodName)\n\n6\n\n{\n\nreturn typeof(Crier)\n\n.GetMethod(methodName)\n\n.GetParameters()\n\n.Select(p => p.ParameterType)\n\n.ToArray(); }\n\nprivate Object[] ArgsFor(string methodName)\n\n6\n\n{\n\nvar length = SignatureFor(methodName).Length;\n\nreturn Enumerable.Repeat(new Object(),\n\nlength).ToArray(); }\n\n1 Request metadata\n\n2 Wraps response\n\n3 Writes response metadata\n\n4 Writes response (with request metadata)\n\n5 Converts to Base64\n\n6 Supports RPC methods other than Announce\n\nThe SignatureFor and ArgsFor methods are simple helper methods that use .NET reflection (which lets you inspect types at runtime) to make the Serialize method general purpose. The request metadata expects\n\nsome information about the remote function signature, and those two methods allow you to dynamically define\n\nenough information to satisfy the format. The rest of the Serialize method uses Xu Huang’s library to wrap\n\nyour stub response object with the appropriate metadata, so when mountebank returns it over the wire, your .NET\n\nRemoting client will see it as a legitimate RPC response.\n\nRemember the key goal of mountebank: to make easy\n\nthings easy and hard things possible. The fact that, with a little bit of underlying serialization code, you can\n\nelegantly stub out binary .NET Remoting (and some of its cousins) over the wire is a killer feature.\n\nIn case you have forgotten how cool that is, I suggest you\n\nlook back at listing 8.9 and see how simple the test is.\n\n8.5.3. How to tell mountebank where the message ends\n\nThere’s one other bit of complexity you have to deal with to fully virtualize an application protocol using\n\nmountebank’s TCP protocol. We hinted at it back in chapter 3, when we looked at how an HTTP server knows\n\nwhen an HTTP request is complete. You may recall a figure that looked like figure 8.5.\n\nFigure 8.5. Using Content-Length to wrap multiple packets into one HTTP request\n\nAs a transport protocol, TCP opens and closes a new\n\nconnection using a handshake. That handshake is transparent to application protocols. TCP then takes the\n\napplication request and chunks it into a series of packets, sending each packet over the wire. A packet will range\n\nbetween 1,500 and around 64,000 bytes, though smaller sizes are possible. You’ll get the larger packet size when\n\nyou test on your local machine (using what’s called the loopback network interface), whereas lower level\n\nprotocols like Ethernet use smaller packet sizes when passing data over the network.\n\nBecause a logical application request may span multiple\n\npackets, the application protocol needs to know when the logical request ends. HTTP often uses the Content-\n\nLength header to provide that information. Because this header occurs early in the HTTP request, the server can\n\nwait until it receives enough bytes to satisfy the given length, regardless of how many packets it takes to deliver\n\nthe full request.\n\nEvery application protocol must have a strategy for determining when the logical request ends. Mountebank\n\nuses two strategies:\n\nThe default strategy, which assumes a one-to-one relationship\n\nbetween a packet and a request\n\nReceiving enough information to know when the request ends\n\nThe examples have worked so far because you’ve only\n\ntested with short requests. You will change that with a simple proxy, saved as remoteCrierProxy.json, as\n\nshown in the following listing.\n\nListing 8.11. Creating a TCP proxy to a .NET Remoting server\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000, \"mode\": \"binary\",\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\" }\n\n}]\n\n}]\n\n}\n\nThe source code for this book includes the executable for the .NET Remoting server. You give it the port to listen\n\nto when you start it up:\n\nServer.exe 3333\n\nYou start the mountebank server in the usual way:\n\nmb --configfile remoteCrierProxy.json\n\nFinally, if you start the .NET Remoting client on port 3000, it’s configured by default to send a request\n\ngreeting and topic that’ll exceed the size of a single packet:\n\nClient.exe 3000\n\nYou can see in the mountebank logs that it tried to proxy,\n\nbut the server didn’t respond, and the client threw an error. By default, mountebank grabs the first packet and\n\nassumes it’s the entire request. It passes it to the server— a real, bona fide .NET Remoting server—which looks\n\ninside the packet and sees that it should expect more packets to come for the request, so it continues to wait.\n\nMountebank, thinking it has seen the entire request, tries to respond. The whole process blows up (figure 8.6).\n\nFigure 8.6. Mismatched expectations around when the request ends\n\nOnce your request reaches a certain size, you have to opt\n\nfor the second strategy: telling mountebank when the request ends. The imposter will keep an internal buffer.\n\nEvery time it receives a new packet, it adds the packet data to the buffer and passes the entire buffer to a JavaScript function that you define, which returns true if the request is complete or false otherwise.\n\nYou pass in the function as the endOfRequestResolver. For this example, you’ll add\n\nit using a template to include the function in a separate file called resolver.js, as shown in the following listing.\n\nListing 8.12. Adding an endOfRequestResolver\n\n{ \"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n\"endOfRequestResolver\": {\n\n\"inject\": \"<%- stringify(filename,\n\n'resolver.js') %>\"\n\n},\n\n\"stubs\": [{ \"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\" }\n\n}]\n\n}]\n\n}\n\n.NET Remoting embeds a content length in the metadata for the request. You can use that in your function to\n\ndetermine if you’ve collected all the packets for the request or not. You’ll once again rely on Xu Huang’s\n\nparsing library, which includes a Node.js implementation, to do the heavy lifting. As before, the\n\nintention isn’t to learn everything about .NET Remoting; it’s to show how you would virtualize a real-world\n\napplication protocol. Don’t worry too much about the details of the message format. The essential part is that\n\nyou grab the content length from the message and test it against the length of the buffer mountebank passes you\n\nto see whether you’ve received the entire request, as shown in the following listing.\n\nListing 8.13. The function to determine if you have seen the entire request yet\n\nfunction (requestData, logger) {\n\n1\n\nvar path = require('path'),\n\nparserPath = path.join(process.cwd(),\n\n2\n\n'/../RemotingProtocolParser/nodejs/lib/remotingProtocolParser'),\n\nr =\n\nrequire(parserPath).tcpReader(requestData);\n\nlogger.debug('Preamble: %s', r.readPreamble());\n\nlogger.debug('Major: %s', r.readMajorVersion());\n\nlogger.debug('Minor: %s',\n\nr.readMinorVersion());\n\nlogger.debug('Operation: %s',\n\nr.readOperation());\n\nlogger.debug('Delimiter: %s',\n\nr.readContentDelimiter());\n\nlogger.debug('ContentLength: %s', 3\n\nr.readContentLength());\n\n3\n\nlogger.debug('Headers: %s',\n\nJSON.stringify(r.readHeaders()));\n\nvar expectedLength = r.offset + r.contentLength\n\n+ 1; 4 logger.info('Expected length: %s, actual\n\nlength: %s',\n\nexpectedLength, requestData.length);\n\nreturn requestData.length >= expectedLength;\n\n5\n\n}\n\n1 requestData is a Node.js Buffer object.\n\n2 Includes Xu Huang’s library\n\n3 Refers to the length of the content section, not the\n\nentire message\n\n4 Calculates the expected length\n\n5 Tests buffer length against expected\n\nThe parsing library isn’t published as an npm module. If\n\nit was, you could install it locally and include it without referencing a specific file path. In your case, you cloned\n\nHuang’s repository according to the path expected in the [8] second line of the function.\n\n8\n\nI’ve done this in the source code repo for this book, including a complete copy of the library.\n\nThe parsing library doesn’t support random access, so you can’t ask it the content length and compare that\n\nagainst your request buffer. Instead, it maintains a stateful offset and expects that you read all the\n\nmetadata fields in order. To help you debug, I wrapped those metadata fields in a logger.debug function.\n\nYou’ll be able to see them in the mountebank logs if you run with the --loglevel debug command-line flag.\n\nNow that you’ve written your function, you can try the proxy again. This time, because you’re using a JavaScript function, you have to pass the --allowInjection flag:\n\nmb --configfile imposter.json --allowInjection\n\nRestart the server on port 3333 and run the client again,\n\npointing to the mountebank proxy:\n\nClient.exe 3000\n\nThis time, everything works. You now have a fully functional virtual server for .NET Remoting.\n\nCongratulations! You’ve completed the hardest example in the entire book.\n\nHard, but possible.\n\nAnd with that, you’ve now completed your tour of mountebank. But knowing how to use a tool isn’t the\n\nsame as knowing when you should use it. That’s what the next chapter is about.\n\nAnother example: Java serialization over a Mule ESB\n\nIn late 2013, I was working at a major airline company. Several years earlier, the website had been rewritten to\n\ncommunicate with a service tier over a Mule enterprise\n\nservice bus (ESB). The ESB connector communicated over TCP and returned a serialized Java object graph.\n\nUnfortunately, passing raw objects over the wire created a tight coupling between the web tier and the service tier.\n\nIt also ran a multibillion-dollar website for most of a decade. Production-hardened enterprise software rarely\n\nlooks like the beautiful architectures you read about.\n\nI was on a team creating REST APIs for a new mobile\n\napp that needed to go out before the website could be replaced, so our APIs had to integrate with the service\n\ntier. Although we had a top-notch team comfortable with automated testing, the friction of testing without also\n\nbreaking the web tier was so painful that we gave up. We wrote automated tests when we could, but usually it was\n\ntoo hard, and bugs started creeping in.\n\nMost of this book has described mountebank for HTTP, but HTTP wasn’t the first protocol mountebank\n\nsupported. I created mountebank to test a binary Mule ESB TCP connector, serving up Java objects in our tests\n\nin much the same way we looked at for .NET Remoting. At the time, a number of quality open source\n\nvirtualization tools were available for HTTP, but none could stub out a binary TCP protocol. That’s largely still\n\ntrue today.\n\nSUMMARY\n\nIn mountebank, protocols are responsible for transforming a\n\nnetwork request into a JSON request for predicate matching, as\n\nwell as taking a mountebank JSON response and transforming it\n\ninto a network response.\n\nMountebank supports adding an application protocol on top of its\n\nTCP protocol. All predicates, response types, and behaviors\n\ncontinue to work with the TCP protocol; only the JSON structure\n\nfor requests and responses differs.\n\nMountebank supports binary payloads by Base64 encoding the data. You have to switch the imposter mode to binary for\n\nmountebank to correctly handle the encoding.\n\nOnce you figure out how to serialize an object graph into the wire\n\nformat expected by an RPC protocol, you can write tests that look\n\nsimilar to ones that use traditional stubbing tools.\n\nBy default, mountebank assumes each incoming packet represents\n\nan entire request when using the TCP protocol. To let mountebank\n\nknow when the request ends, you can pass in an endOfRequestResolver JavaScript function.\n\nPart 3. Closing the Loop\n\nNow that you have the full breadth of mountebank\n\nfunctionality under your belt, part 3 puts its usage in context.\n\nService virtualization is a powerful tool, but, like any tool, it has its limits. In chapter 9, we explore it in the\n\ncontext of continuous delivery. We’ll build a test pipeline from start to finish for some of the microservices we’ve looked at previously in this book and discuss where\n\nservice virtualization fits and where it doesn’t. We’ll also look at how to gain additional confidence in your test\n\nsuite with contract tests that give you lightweight validation that play together well with your services\n\nwithout going down the path toward full end-to-end testing.\n\nWe close out the book by looking at performance testing, always a difficult subject and one made even more so by\n\nmicroservices. The need to understand your service’s performance characteristics in a networked environment\n\nis challenged by the cost and complexity of securing an end-to-end environment for testing. Service\n\nvirtualization is a natural fit for performance testing and combines many of the features we’ve looked at previously, including proxies and behaviors.\n\nChapter 9. Mountebank and continuous delivery\n\nThis chapter covers\n\nA basic refresher on continuous delivery\n\nTesting strategy for continuous delivery and microservices\n\nWhere service virtualization applies inside a broader testing\n\nstrategy\n\nA sysadmin, a DBA, and a developer walk into a bar. The\n\nsysadmin orders a light lager to maximize uptime, the DBA orders a 30-year-aged single malt to avoid undue\n\nadulteration, and the developer orders a Pan Galactic Gargle Blaster because it hasn’t been invented yet. An\n\nhour later, the DBA has gone home already, the developer has moved on to a more modern bar, and the\n\nslightly wobbly and heavily overutilized sysadmin is holding down the fort, while also holding a lager in one\n\nhand, a single malt in another hand, and a Pan Galactic in another hand.\n\n[1]\n\n1\n\nIn The Hitchhikers Guide to the Galaxy, Douglas Adams describes the Pan Galactic Gargle Blaster as the alcoholic equivalent of a mugging—expensive and bad for the head.\n\nTraditional siloed organizational structure forces a complicated dance to get anything done. It’s no surprise\n\nthat, in large enterprises, IT and the business rarely have a healthy relationship. Historically, the common\n\napproach to improving the situation was to add more process discipline, which further complicated the dance,\n\nmaking it harder to release code into production (and, by consequence, reduced value to customers). Having\n\nincreasingly well-defined handoffs between a developer, a DBA, and a sysadmin exemplifies process discipline.\n\nEvery time you fill out a database schema change request\n\nform or an operational handoff document, you have seen process discipline in action.\n\nContinuous delivery changes the equation by emphasizing engineering discipline over process\n\ndiscipline. It’s about automating the steps required to build confidence so that the business can release new\n\ncode on demand. Although engineering discipline encompasses a wide spectrum of practices, testing plays\n\na central role. In this chapter, we look at a sample testing strategy for a microservices world and show where\n\nservice virtualization does and doesn’t fit.\n\n9.1. A CONTINUOUS DELIVERY REFRESHER\n\nJez Humble and Dave Farley wrote Continuous Delivery to capture the key practices they saw enabling the rapid\n\ndelivery of software. In chapter 1, I showed you how the traditional process discipline of centralized release\n\nmanagement and toll gates increases congestion and slows delivery. The emphasis is on safety, providing\n\nadditional checks to increase confidence that the software being delivered will work.\n\nIn contrast, continuous delivery (CD) focuses on\n\nautomation, emphasizing safety, speed, and sustainability of delivering software. It requires the code\n\nto be in a deployable state at all times, forcing you to abandon the ideas of dev complete, feature complete,\n\nand hardening iterations. Those concepts are holdovers from the world of yesteryear, in which we papered over a\n\nlack of engineering discipline by adding more layers of process.\n\nA glossary of terms surrounding continuous delivery\n\nI introduce several important terms in this chapter:",
      "page_number": 250
    },
    {
      "number": 9,
      "title": "Mountebank and continuous delivery",
      "start_page": 288,
      "end_page": 322,
      "detection_method": "regex_chapter_title",
      "content": "Continuous integration— Although continuous integration\n\n(CI) is often confused with running an automated build after every\n\ncommit through a tool like Jenkins, it’s actually the practice of\n\nensuring that your code is merged with and works with everyone\n\nelse’s on a continual basis (at least once a day).\n\nContinuous delivery— The set of software development\n\npractices that ensures code is always releasable. The full spectrum of CD practices ranges from developer-facing techniques like\n\nfeature toggles, which provide a way of hiding code that’s still a\n\nwork in progress, to production-facing approaches like monitoring\n\nand canary testing, which scales up a release to a customer base\n\nover time. In between comes testing, the focus of this book.\n\nDeployment pipeline— The path code takes from the time it’s\n\ncommitted to the time it reaches production.\n\nContinuous deployment— An advanced type of continuous\n\ndelivery that removes all manual interventions from the\n\ndeployment pipeline.\n\nIn CD, every commit of the code either fails the build or\n\ncan be released to production. There’s no need to decide up front which commit represents the release version.\n\nAlthough still common, that approach encourages sloppy engineering practices. It enables you to commit code that cannot be released to production, with the expectation\n\nthat you’ll fix it later. That attitude requires IT to own the timing of software delivery, taking control out of the\n\nhands of the business and the product manager.\n\nThe core organizing concept that makes CD possible is\n\nthe deployment pipeline. It represents the value stream of the code’s journey from commit to production and is\n\noften directly represented in continuous integration (CI) tools (figure 9.1).\n\nFigure 9.1. A deployment pipeline defines the path from commit to production.\n\nEvery code commit automatically triggers a build, which\n\nusually includes compilation, running unit tests, and static analysis. A successful build saves off a package in\n\nan artifact repository—usually a binary artifact, even if it’s just a tarball of source code for interpreted languages\n\nlike Ruby and JavaScript. Every set of verifications downstream runs against a deployed instance of that\n\npackage, until it ultimately reaches production.\n\nThe path that code takes on its way to providing value to real users varies from organization to organization and\n\neven between teams within the same organization. Much of it is defined by how you decide to test your\n\napplication.\n\n9.1.1. Testing strategy for CD with microservices\n\nTesting in a very large-scale distributed setting is a major challenge.\n\nWerner Vogels, Amazon CTO\n\nA common approach to visualizing testing strategy comes in the form of a pyramid. The visual works\n\nbecause it acknowledges that confidence comes from testing at multiple layers. It also shows that there’s value\n\nin pushing as much of the testing as possible into the lower levels, where tests are both easier to maintain and\n\nfaster to run. As you move to higher levels, the tests become harder to write, to maintain, and to troubleshoot\n\nwhen they break. They’re also more comprehensive and often better at catching difficult bugs. Each team will\n\nneed to customize a test pyramid to its needs, but you can think of a template for microservices that looks like\n\n[2]\n\nfigure 9.2.\n\n2\n\nYou also may be interested in Toby Clemson’s description of the types of testing for microservices at http://martinfowler.com/articles/microservice-testing/.\n\nFigure 9.2. Simplified test pyramid for microservices\n\nPeople have argued endlessly over what makes a unit test\n\ndifferent from higher level tests, but for the purposes of this diagram, the key difference is that you should be\n\nable to run a unit test without deploying your service into a runtime. That makes unit tests in-process and\n\nindependent of anything from the environment (see figure 9.3).\n\nFigure 9.3. The basic structure for unit and service tests\n\nThough there’s some different terminology out there, I’ve\n\nused the term service test to describe a black-box test that validates your service’s behavior over the wire. Such\n\ntests do require a deployment, but you use service virtualization to maintain isolation from your runtime\n\ndependencies. This layer allows you to do out-of-process, black-box testing while maintaining determinism.\n\nService virtualization enables you to remove nondeterminism from your tests by allowing each test to\n\ncontrol the environment it runs in, as shown in figure 9.3.\n\nYou should be able to test the bulk of the behavior of your service through a combination of unit tests and\n\nservice tests. They let you know that your service behaves correctly, assuming certain responses from its\n\ndependencies, but they don’t guarantee that those stubbed responses are appropriate. Contract tests give\n\nyou validation that breaking-contract-level changes haven’t occurred (see figure 9.4). Service tests say, in\n\neffect, that if the service gets these responses from its dependencies, then it behaves correctly. Contract tests\n\nvalidate that the service does, in fact, get those responses. Good contract tests avoid deep behavioral\n\ntesting of the dependencies—you should test them independently—but give you confidence in your stubs.\n\nFigure 9.4. The basic structure for contract and exploratory tests\n\nI’ve included exploratory testing as part of the test pyramid because most organizations find some value in\n\nmanual testing. Good exploratory testers follow their nose to find gaps in an automated test suite. Such tests\n\ncan be integrated or rely on service virtualization to test certain edge cases. Figure 9.4 shows exploratory testing using service virtualization.\n\nOther types of testing exist that don’t fit as well in the test pyramid metaphor. Cross-functional requirements\n\nlike security, performance, and failover for availability often require specialized testing and are less about the\n\nbehavior of the system than about its resiliency. Performance testing is an area where service\n\nvirtualization shines, as it allows you to replicate the performance of your dependencies without requiring a\n\nfully integrated, production-like environment to run in. In chapter 10, we explore how service virtualization\n\nenables performance and load testing.\n\nFinally, you should never forget that error prevention is\n\nonly one piece of testing strategy. The rapid release cycles of microservices encourage you to invest heavily in\n\nerror detection and remediation as well as prevention, as they contribute to your overall confidence in releasing software. Although error detection and remediation are\n\nnot the focus of this book, companies that have used microservices effectively generally stage their releases,\n\nsuch that only a small percentage of users can see the new release at first. Robust monitoring detects whether\n\nthe users experience any problem, and rolling back is as easy as switching those users to the code everyone else is\n\nusing. If no problems are detected, the release system will switch more and more users to the new code over\n\ntime until 100% of users are using the release, at which Advanced time you can remove the previous release.\n\n[3]\n\nmonitoring allows you to detect errors before your users do. Although your testing strategy is a key component of\n\ncontinuous delivery, engineering discipline significantly increases the scope of automation.\n\n3\n\nThis is called canary testing. You can read more about it at https://martinfowler.com/bliki/CanaryRelease.html.\n\n9.1.2. Mapping your testing strategy to a deployment pipeline\n\nWhatever your particular test pyramid looks like, mapping it to your deployment pipeline is generally\n\npretty straightforward (figure 9.5).\n\nFigure 9.5. Mapping your test pyramid to a deployment pipeline\n\nI like to think of boundary conditions moving from one\n\nstage to the next. In figure 9.5, I’ve shown the following boundaries:\n\nThe boundary of deployment represents the first time you’ve\n\ndeployed the application (or service). All tests to the left are run\n\nin-process; all tests to the right are run out-of-process and\n\nimplicitly test the deployment process itself, as well as the\n\napplication.\n\nThe boundary of determinism represents the first time you’ve\n\nintegrated your application into other applications. Tests to the\n\nright of this boundary may fail because of environmental\n\nconditions. Tests to the left of this boundary should fail only for\n\nreasons entirely under the application team’s control.\n\nThe boundary of automation represents where you switch to\n\nmanual, exploratory testing. (Note that the deployment itself is\n\nstill automated, but the trigger to deploy requires a human\n\npressing a button.) Some companies, for some products, have\n\nmanaged to eliminate this boundary altogether, automatically releasing code to production without any manual verifications.\n\nThis is an advanced form of continuous delivery called continuous\n\ndeployment and is clearly not appropriate in all environments.\n\nThe software that helps keep an airplane in the air requires a much\n\nhigher degree of confidence than your favorite social media\n\nplatform.\n\nThe boundary of value is the point at which real users have access\n\nto the new software.\n\nBecause this book is about testing, we only tackle the test pipeline, those early stages of the pipeline that give you\n\nconfidence that you can ship to production. The full deployment pipeline would include the production\n\ndeployment as well.\n\n9.2. CREATING A TEST PIPELINE\n\nIt has been a while since we looked at the example pet\n\nstore website in chapter 2. We used it as a small-scale simulacrum of a microservices-backed e-commerce\n\napplication and focused on the web façade service, which aggregated results from two other services (figure 9.6):\n\nThe product catalog service, responsible for sending product\n\ninformation\n\nThe content service, responsible for sending marketing copy about\n\na product\n\nFigure 9.6. An example set of microservices\n\nFor demonstration purposes, we’ll take the perspective of\n\nthe team writing the web façade code that aggregates product and marketing data to present to the website.\n\nThe example is simple enough to digest in short order and complicated enough to require meaningful testing.\n\nThe code is a simple Express app (a popular node.js web application framework). The following listing shows\n\nwhat you need to initialize the service.\n\n[4]\n\n4\n\nSee the full source code at https://github.com/bbyars/mountebank-in-action.\n\nListing 9.1. Web façade initialization code\n\nvar express = require('express'),\n\nproductServiceURL =\n\nprocess.env['PRODUCT_SERVICE_URL'], 1 contentServiceURL =\n\nprocess.env['CONTENT_SERVICE_URL'], 1\n\nproductsGateway =\n\nrequire('./models/productsGateway') 2\n\n.create(productServiceURL),\n\n2\n\ncontentGateway =\n\nrequire('./models/contentGateway') 2 .create(contentServiceURL),\n\n2\n\nproductCatalog =\n\nrequire('./models/productCatalog') 3\n\n.create(productsGateway, contentGateway);\n\n3\n\nvar app = express();\n\n4\n\n1 Configures external services\n\n2 Gateway code to external services\n\n3 Module to do the aggregation\n\n4 Creates the Express app\n\nUsing environment variables for configuration is a\n\ncommon approach and will allow you to use different URLs in different environments (and to use service\n\nvirtualization for service tests). The two gateway objects are simple wrappers over the HTTP calls, allowing you to\n\ncentralize error handling and logging around external service calls.\n\nThe code that responds to an HTTP request and returns\n\nthe aggregated results of the product and content services is quite straightforward, delegating the complex logic to the productCatalog object, as shown in the following listing.\n\nListing 9.2. Web façade code to aggregate product and content data\n\napp.get('/products', function (request, response)\n\n{ 1\n\nproductCatalog.retrieve().then(function\n\n(results) { 2\n\nresponse.json({ products: results });\n\n3\n\n}, function (err) { 4\n\nresponse.statusCode = 500;\n\n4\n\nresponse.send(err);\n\n4\n\n});\n\n});\n\n1 Responds to GET /products\n\n2 Delegates aggregation\n\n3 Returns results as JSON\n\n4 Error handling\n\nYou could’ve left the aggregation logic directly in the\n\nfunction that handles the HTTP request. You didn’t, in part, because that would’ve made it harder to unit test.\n\n9.2.1. Creating unit tests\n\nTest-driven development (TDD) is often also called test-\n\ndriven design because the act of writing unit tests helps enforce loose coupling and high cohesion in your\n\n[5]\n\nproduction code. Bundling all the aggregation logic into your HTTP handling code would have required a\n\nsignificant amount of setup to test it—exactly the type of friction that reduces your motivation to write tests to\n\nbegin with. It also would’ve been a worse design, coupling HTTP handling logic with aggregation logic.\n\nThe drive to make unit testing your code as easy as possible is one of the best influences for keeping your\n\ncodebase modular. Unit tests are as much about helping you design your application as they are about finding\n\nbugs.\n\n5\n\nTDD is more than writing unit tests. It’s a practice that involves writing a small test before the code exists and building just enough code to make the test pass. Those small iterations with\n\nrefactoring in between help grow the design of the codebase organically.\n\nUnit tests should be in-process to the application under\n\ntest, and each unit test should focus on a small piece of code. Consequently, you should never use service\n\nvirtualization in your unit tests. This is the realm of traditional mocks and stubs.\n\nLet’s look at the productCatalog code. We’ll start with the wrapper logic needed to create an instance, as shown\n\nin the following listing, and export it to another JavaScript file.\n\nListing 9.3. The shell of the productCatalog module\n\nfunction create(productsGateway, contentGateway)\n\n{ 1\n\nfunction retrieve () { ... }\n\n2\n\nreturn {\n\n3 retrieve: retrieve\n\n3\n\n};\n\n3\n\n}\n\nmodule.exports = {\n\n4 create: create\n\n4\n\n};\n\n4\n\n1 Creates an instance using dependency injection\n\n2 See listing 9.4.\n\n3 Returns the instance with one function\n\n4 Exports the creation method to other files\n\nMuch of this is JavaScript and node.js plumbing. The creation function accepts the two gateway objects as\n\n[6]\n\nparameters. This pattern—dependency injection—is another area where good unit testing practices intersect with good design. If you created the gateways at the exact\n\nspot you needed them, you wouldn’t be able to swap out the gateway instances with another object for testing.\n\nThat would mean you would be forced to test the full end-to-end flow each time because the gateways are\n\nresponsible for making the HTTP calls to the external services. It also would’ve created a tight coupling,\n\npreventing higher order code from adding decorators around the gateways for added functionality.\n\n6\n\nYou could’ve also used JavaScript constructors, but they add a bit more magic than a simple creation function.\n\nThe retrieve function uses those gateways to retrieve and aggregate the data from the product and content\n\nservices, as shown in the following listing.\n\nListing 9.4. The code that retrieves and aggregates downstream services\n\nfunction retrieve () {\n\nvar products;\n\nreturn productsGateway.getProducts()\n\n1\n\n.then(function (response) {\n\nproducts = response.products;\n\nvar productIds = products.map(function\n\n(product) {\n\nreturn product.id;\n\n2\n\n}); 2\n\nreturn contentGateway.getContent(productIds);\n\n3\n\n}).then(function (response) {\n\nvar contentEntries = response.content;\n\nproducts.forEach(function (product) {\n\nvar contentEntry = contentEntries.find( 4\n\nfunction (entry) {\n\n4\n\nreturn entry.id === product.id;\n\n4\n\n});\n\n4\n\nproduct.copy = contentEntry.copy; 5\n\nproduct.image = contentEntry.image;\n\n5\n\n});\n\nreturn products;\n\n});\n\n}\n\n1 Gets products from the product catalog service\n\n2 Maps to the IDs only\n\n3 Gets content for those products\n\n4 Matches the content entry to the product by ID\n\n5 Adds marketing content data\n\nClearly, the bulk of the web façade complexity lies in this\n\nfunction, making it a great place to focus your unit testing efforts. To keep the unit test in-process, you will\n\n[7]\n\nhave to stub out the two gateways. common JavaScript mocking library called Sinon to help. [8]\n\nYou’ll use a\n\nSinon allows you to tell the gateways what to return, which supports a very readable test setup (the Arrange\n\nstep of the standard test pattern of Arrange-Act-Assert), as shown in the following listing.\n\n7\n\nI haven’t shown the gateway code because it’s not relevant to the example. See the GitHub\n\nrepo for details.\n\n8\n\nSee http://sinonjs.org/, although it’s not hard to write your own stubs if you’d rather avoid using an external library.\n\nListing 9.5. The test setup, using dependency injection and stubs\n\nit('should merge results', function (done) { var productsResult = {\n\n1\n\nproducts: [\n\n1\n\n{ id: 1, name: 'PRODUCT-1' },\n\n1\n\n{ id: 2, name: 'PRODUCT-2' }\n\n1 ]\n\n1\n\n},\n\n1\n\nproductsGateway = {\n\n2\n\ngetProducts: sinon\n\n2 .stub()\n\n2\n\n.returns(Q(productsResult)) 2\n\n},\n\n2\n\ncontentResults = {\n\n3\n\ncontent: [\n\n3\n\n{ id: 1, copy: 'COPY-1', image: 'IMAGE-1'\n\n}, 3 { id: 2, copy: 'COPY-2', image: 'IMAGE-2'\n\n} 3\n\n]\n\n3\n\n},\n\n3\n\ncontentGateway = { 4\n\ngetContent: sinon\n\n4\n\n.stub()\n\n4\n\n.withArgs([1, 2])\n\n4\n\n.returns(Q(contentResults)) 4\n\n},\n\n4\n\ncatalog = productCatalog.create(\n\n5\n\nproductsGateway, contentGateway);\n\n5\n\n// ACT\n\n6\n\n// ASSERT\n\n6\n\n});\n\n1 Stages the product catalog service results\n\n2 Sets up the product stub\n\n3 Stages the content service results\n\n4 Sets up the content stub\n\n5 Passes the stubs into the catalog\n\n6 See listing 9.6.\n\nMost of the code is setting up the JSON responses that the gateways are responsible for returning. As you have\n\nseen in previous examples, I recommend using test data that’s easy to spot to make the assertions easy to read,\n\nwhich is why I have opted for strings like “COPY-1.” You stub out the two gateway functions—getProducts on\n\nthe productsGateway and getContent on the contentGateway—using Sinon’s stub() function and\n\nchain on the result you want with the returns function. Notice that when you create the stub for the contentGateway, you add a withArgs ([1, 2]) function call. This is like using predicates in\n\nmountebank. You’re telling Sinon to return the given result if the arguments match what you specify.\n\nThe only other nuance to the test code is the mysterious use of the Q function, which is used in the stub\n\nresponses. Q is a promise library that helps contain the complexity of using asynchronous code in JavaScript.\n\nThe real gateways have to reach across the network to retrieve results, and because JavaScript uses nonblocking I/O for network calls, using promises helps\n\nmake the asynchronous code easier to understand. If you look back to the retrieve function in listing 9.4, you’ll\n\nsee that you call a then function after each gateway call and pass in the code to execute when the I/O is finished. Wrapping the objects inside the Q function adds the then function to your stub results, so the production\n\ncode works on the stubs as expected.\n\nLet’s close off the example by looking at the Act and\n\nAssert stages of the test in the following listing.\n\nListing 9.6. The unit test assertion\n\nit('should merge results', function (done) { // ARRANGE 1\n\ncatalog.retrieve().done(function (result) { 2\n\nassert.deepEqual(result, [ 3\n\n{ id: 1, name: 'PRODUCT-1', 3\n\ncopy: 'COPY-1', image: 'IMAGE-1' }, 3\n\n{ id: 2, name: 'PRODUCT-2', 3\n\ncopy: 'COPY-2', image: 'IMAGE-2' }, 3 ]); 3\n\ndone(); 4\n\n});\n\n});\n\n1 See listing 9.5.\n\n2 Act\n\n3 Assert\n\n4 Tells the test runner that the test has finished\n\nAnytime you’re using asynchronous code, you have to tell\n\nthe test runner that the test is complete. The assertion verifies that you merged the results of the two gateways\n\ncorrectly. The done test parameter is a function that you call after your assertion to signify the end of the test\n\nexecution.\n\nYou could, and should, write many more unit tests on the retrieve function. For example, you could write unit\n\ntests to specify what happens in each of these scenarios:\n\nThere’s no marketing copy for a product.\n\nA downstream service times out (resulting in a gateway error).\n\nYou get missing JSON fields from the marketing content service.\n\nThe marketing content service returns products in a different order than the product catalog service.\n\nIt’s much easier to write the code to support these scenarios in a suite of unit tests than it is to validate\n\nthem with higher level tests. Unit tests should be numerous and run quickly, which is why they form the\n\nbase of the testing pyramid.\n\nYou can create a build script that runs the unit tests and wire that into the first stage of your continuous\n\nintegration tool. Once you have done so, you have automated the first stage of your test pipeline.\n\n9.2.2. Creating service tests\n\nService tests should exercise your application over the wire, which rules out in-process stubs. This is where\n\nservice virtualization shines, as service virtualization is the out-of-process equivalent of stubbing.\n\nAlthough you can always set up your imposters using a config file, I recommend using mountebank’s API when\n\npossible for service tests. The API allows you to create the test data for each test separately rather than having\n\nto depend on an implicit linkage between a magic key in your test setup and the scenario you’re testing. You used\n\nmountebank’s API in chapter 2.\n\nI’ve modified the example slightly to highlight the point about keeping the test data as simple as possible. Let’s\n\ntake a fresh look at the test first in the following listing; we look at the helper functions again shortly.\n\nListing 9.7. A service test that validates web façade aggregation\n\nit('aggregates data', function (done) {\n\ncreateProductImposter(['1', '2']).then(function\n\n() { 1\n\nreturn createContentImposter(['1', '2']);\n\n1\n\n}).then(function () {\n\nreturn request(webFacadeURL + '/products'); 2\n\n}).then(function (body) {\n\nvar products = JSON.parse(body).products;\n\nassert.deepEqual(products, [\n\n3\n\n{\n\n\"id\": \"ID-1\", \"name\": \"NAME-1\",\n\n\"description\": \"DESCRIPTION-1\",\n\n\"copy\": \"COPY-1\",\n\n\"image\": \"IMAGE-1\"\n\n},\n\n{\n\n\"id\": \"ID-2\",\n\n\"name\": \"NAME-2\", \"description\": \"DESCRIPTION-2\",\n\n\"copy\": \"COPY-2\",\n\n\"image\": \"IMAGE-2\"\n\n}\n\n]);\n\nreturn imposter().destroyAll();\n\n4\n\n}).done(function () { done();\n\n5\n\n});\n\n});\n\n1 Arrange\n\n2 Act\n\n3 Assert\n\n4 Cleanup\n\n5 Tells the test runner you’re done\n\nThe createProductImposter and createContentImposter functions are similar. They\n\nuse mountebank’s API to create the virtual services. Both functions accept an array of suffixes, which they use to\n\nappend to the test data within each field name. You can see what that results in by looking at the assertion in\n\nlisting 9.7. The code to do that does a simple string append to each field name:\n\nfunction addSuffixToObjects (suffixes, fields) { return suffixes.map(function (suffix) {\n\nvar result = {};\n\nfields.forEach(function (field) {\n\nresult[field] = field.toUpperCase() + '-' +\n\nsuffix;\n\n});\n\nreturn result;\n\n}); }\n\nWith that helper function, the imposter creation uses the\n\nsame fluent API you built in chapter 2 that wraps mountebank’s RESTful API, as shown in the following\n\nlisting.\n\nListing 9.8. The imposter creation functions\n\nvar imposter = require('./imposter'), 1\n\nproductPort = 3000;\n\nfunction createProductImposter (suffixes) {\n\nvar products = addSuffixToObjects(suffixes,\n\n['id', 'name', 'description']);\n\nreturn imposter({\n\nport: productPort,\n\nprotocol: \"http\",\n\nname: \"Product Catalog Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({equals: {path:\n\n\"/products\"}}) .respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"},\n\nbody: { products: products }\n\n})\n\n.create();\n\n}\n\nvar contentPort = 4000;\n\nfunction createContentImposter(suffixes) {\n\nvar contentEntries =\n\naddSuffixToObjects(suffixes,\n\n['id', 'copy', 'image']);\n\nreturn imposter({\n\nport: contentPort,\n\nprotocol: \"http\",\n\nname: \"Marketing Content Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({\n\nequals: { path: \"/content\",\n\nquery: {ids: \"ID-1,ID-2\"}\n\n}\n\n})\n\n.respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"}, body: { content: contentEntries }\n\n})\n\n.create();\n\n}\n\n1 See listing 2.5.\n\nArguably, the hardest part of managing a suite of service tests is maintaining the test data. Test data management\n\nis a complicated subject, and many vendors are willing to sell you solutions that promise to ease the pain. Although\n\nsuch tools may help in complex integrated test scenarios, I believe you should use them sparingly. Too often,\n\nthey’re used as a way of avoiding shifting the tests to the left, where left refers to the left side of the deployment\n\npipeline (close to development).\n\nCreating test cases with appropriate isolation via service virtualization is key. The example in listing 9.7 virtualizes\n\na couple of simple service schemas. Real world schemas are often much more complex. In such cases, you will\n\nwant to save off the responses in separate files and use string interpolation to add in any dynamic data needed.\n\nYou can reference the specific scenario directly in your test using a key that identifies the scenario. For example,\n\nif you wanted to test what happens when the product catalog service returns a product but there’s no marketing content for it, use a product ID of NO- CONTENT. Leaving breadcrumb trails in your test data\n\nwill make maintaining it much easier.\n\nThe team writing tests needs to own the configuration for the virtualized service\n\nGenerally, two types of service virtualization tools are\n\navailable.\n\nThe first type comes from the open source community. They almost always support HTTP/S. Although most of\n\nthem support record-playback through proxying, they\n\noften expect the team writing the tests—the client team— to define the data that the virtual service returns.\n\nThe second type represents the commercial virtualization tools. They usually are more feature-rich and support a\n\nmore complete set of protocols. But because of the licensing model, they typically expect a central team to\n\nown the virtual service definitions. For automated service tests, that’s exactly backwards from how it should\n\nbe.\n\nAutomated testing requires fine-grained control over the testing scenarios. Those scenarios will require a different\n\nset of test data than another team’s automated tests, even if both test suites have a shared dependency that\n\nrequires virtualizing. Having to go through a central team to set up your test data adds unnecessary friction,\n\nwhich has the unfortunate side effect of discouraging your developers from writing the tests. The complexity of\n\nthe configuration will also become unwieldy to understand and maintain if your test data is comingled\n\nwith that of other teams, adding more friction.\n\nAt this stage of the deployment pipeline, your team needs to be in complete control of its test data. That means\n\nyour team needs to write the configuration for the virtual services. Relying on a central team or the team\n\nproducing the service that you depend on to write the configuration for you will always result in a deficient test\n\nsuite.\n\nThroughout much of this book, I have described\n\nmountebank’s mission statement as keeping things that should be easy actually easy while making hard things possible. Rephrasing that as a competitive product\n\nstrategy, my goal with mountebank is to provide the power of commercial service virtualization tools with the\n\nCD-friendliness of the open source tools.\n\n9.2.3. Balancing service virtualization with contract tests\n\nUnit tests help you design your application and catch simple bugs created while refactoring. Service tests treat\n\nyour application as a black box and help catch bugs from a consumer’s point of view. Adding service virtualization\n\nkeeps service tests deterministic, sealing your test scenarios in a pristine laboratory environment.\n\nProduction is more like a war zone than a lab. Although\n\nservice tests give you confidence that your application works with certain assumptions about your runtime\n\ndependencies, they do nothing to validate those assumptions. Like it or not, your runtime dependencies\n\nchange over time, as they have their own release cycle, so you need some dynamic way of detecting breaking\n\nchanges in services that you depend on (for example, checking that the marketing content service returns the marketing copy in a top-level JSON field called copy). That’s the responsibility of the next stage of the test\n\npipeline: where you run your contract tests.\n\nContract tests move you into the realm of integration, and any time you integrate to the outside world (code\n\nwritten by another team or another company), you’re no longer in a friendly, deterministic environment. Your\n\ntests may fail because your code has a bug, because their code has a bug, because of configuration bugs in the\n\nenvironment itself, or because the network hiccupped. Consequently, you test as much as you can in the friendly\n\nconfines of unit and service tests. Contract tests shouldn’t be deep behavioral tests; they should be\n\nlightweight validations of your assumptions about your runtime dependencies. In effect, they validate that your\n\nstub definitions are compatible with the real service (figure 9.7).\n\nFigure 9.7. Contract tests validate the assumptions made in service tests.\n\nYou generally want to avoid behavioral testing of your dependencies—that’s the job of the team building those\n\ndependencies. Except in dysfunctional situations, you are better off treating them no differently than you would a\n\nsoftware as a service (SaaS) application that you’d pay to use, or an API from a third party like Google. Aside from\n\nthe obvious cost of behavioral testing of your dependencies, it also increases the fragility of the\n\nenvironment configuration, as behaviorally testing your dependency requires its dependencies to also be\n\nfunctional. This leads you back down the path of end-to- end integration testing, creating exactly the kind of\n\ntraffic jams on the path to production that you’re trying to avoid by using service virtualization.\n\nA contract test example\n\nThe example we’ve been looking at is a bit too simplistic to show the value of contract testing, in large part\n\nbecause we asserted only that the marketing content gets correctly merged into the product data. The tests haven’t\n\nput too much emphasis on what that product data is, but the website would depend on a certain set of fields to\n\ndisplay the data appropriately. That means that the web façade service must expect a certain set of fields coming\n\nout of the product catalog service. Contract testing helps verify that those expectations remain true as the product\n\nservice changes.\n\nLet’s say you expect a name, description, and availability\n\ndates for a product. You expect the dates to be in ISO format (YYYY-MM-DD) to ensure that you’re parsing it\n\ncorrectly. A contract test validates that those fields exist in the place and format where you expect them. Let’s start by showing the helpers to validate the type and\n\nformat of data you get back, which use a regular expression to validate the date format:\n\nfunction assertString (obj) {\n\nassert.ok(obj !== null && typeof obj ===\n\n'string');\n\n}\n\nfunction assertISODate (obj) {\n\nassertString(obj);\n\nassert.ok(/201\\d-[01]\\d-[0123]\\d/.test(obj),\n\n'not ISO date');\n\n}\n\nThe regular expression uses the same \\d metacharacter you have seen previously, which represents a digit. The\n\nrest of the regular expression matches either literal numbers (for example, to ensure that the year starts with\n\nthe decade this book was written in—201x), or a limited set of numbers represented in brackets (for example, the\n\nmonth must start with 0 or 1, and the day with 0 through 3). This isn’t a perfect test—it allows 2019-19-39, for\n\nexample—but it’s probably good enough. If you need more confidence, you can add more advanced date\n\nparsing to the test code.\n\nYou can use the assertString and assertISODate\n\nhelpers to write the test, which validates that the fields exist in the location and format you expect them, as\n\nshown in the following listing.\n\nListing 9.9. A contract test validating the placing and format of fields from a real dependency\n\nit('should return correct product fields',\n\nfunction () { return request(productServiceURL + '/products')\n\n1\n\n.then(function (body) {\n\n1\n\nvar products = JSON.parse(body).products;\n\nassert.ok(products.length > 0, 'catalog\n\nempty'); 2\n\nproducts.forEach(function (product) {\n\n3\n\nassertString(product.name);\n\n3\n\nassertString(product.description);\n\n3\n\nassertISODate(product.availabilityDates.start); 3\n\nassertISODate(product.availabilityDates.end);\n\n3\n\n});\n\n});\n\n});\n\n1 Calls the real product catalog service\n\n2 Performs a sanity check\n\n3 Validates field formats\n\nBefore we get into what this function is testing, let’s take\n\na step back to think about what it’s not testing. It’s not testing the full breadth of the product catalog service. The product catalog service may return dozens of fields\n\nthat aren’t relevant to your service (the web façade), so you don’t test them. Remember, you are testing your\n\nassumptions about the product catalog service, not the product catalog service itself.\n\nWhat you are testing is that the format of the data you get back corresponds to what you expect. You’re doing\n\nthat for every product returned, but you also could’ve done it for only the first product in the array. It’s a time\n\nversus comprehensiveness tradeoff, and you’ll have to face that tradeoff on a case-by-case basis. It’s generally a\n\nsafe assumption that a well-behaved service will return the same schema for all elements in an array.\n\nManaging test data\n\nThe hardest part—by far—of contract testing is managing\n\ntest data. Our example more or less avoids the problem by testing a read-only endpoint, although we still added a sanity check in the test to ensure that at least one\n\nproduct was returned. Contract testing services that allow you to change state are possible but require some\n\nsupport from the service.\n\nThe cleanest method is to have your test create the data\n\nbefore reading it. For example, you may submit an order by sending a POST to /orders and retrieve it by\n\nsending a GET to /orders/123, where 123 is the order ID in the response to the first call. With this approach,\n\nevery test execution creates new data, which ensures that the data is isolated from every other test execution.\n\nHowever, it does require the ability to create test data. That’s a reasonable enough assumption for the orders\n\nservice, but the product catalog service is unlikely to provide APIs to create new products, as that is generally a back-office process.\n\nAn alternative is to coordinate with the provider team on a set of data you can rely on for testing purposes. The\n\nprovider team is then responsible for maintaining a set of golden test data and ensuring that it’s available with each\n\nrelease of the software into the test environment. Any such golden data should be nonmutable by your tests, so\n\nthey can run repeatedly on the same data.\n\nWhere service virtualization fits\n\nIt’s possible to use contract testing without any virtual services. This assumes that the dependencies are\n\ndeployed in a shared environment with all of their dependencies also available, on down the stack to the\n\nsystems of record. Some organizations are either small enough or have invested heavily enough in shared\n\ninfrastructure to make this possible.\n\nAn alternative strategy is for the team that manages the\n\ndependency to deploy a test instance available for contract testing and stub out its dependency through service virtualization. This is likely to increase the\n\navailability and determinism of the test instance, as it’s now less subject to the whims of environmental hiccups.\n\nAs a client of a service that another team provides, you have the right to set some expectations of that service.\n\nAn expectation that a test instance is available is both reasonable and common. You’re better off if you can\n\ntreat that test instance as a black box and have the provider team decide whether they run their test\n\ninstance integrated or with virtual dependencies.\n\n9.2.4. Exploratory testing\n\nHistorically, tests were divided into scripted and unscripted, where the script referred to a documented set\n\nof steps and expectations a manual tester executed. The heyday of meticulously cataloging test cases in\n\ncommercial tools so that unskilled QA testers (remote from the application team) could execute them without\n\nany system context is gone. You still do scripted tests,\n\nbut you automate them nowadays. Test design occurs as you write the test, and test execution occurs every time\n\nyou run the test.\n\nExploratory testing combines test design and test\n\nexecution into one activity, bringing discipline to unscripted testing. The ability to follow their nose is one\n\nof the defining characteristics of great QA testers. Exploratory testing allows them to investigate the\n\nsoftware with an attitude of curiosity, unearthing its sharp edges through creativity rather than through\n\npredefined scripts.\n\nThe pendulum has swung quite far from the days of yesteryear, when all scripted tests were executed\n\nmanually, to the point where I occasionally perceive a stigma associated with manual testing altogether. This is\n\nunfortunate. Exploratory testing is a fine art, worthy of Whereas exhaustive automated testing its own study.\n\n[9]\n\nmechanizes the toil of scripted test execution, exploratory testing puts the ghost back in the machine. It\n\nrelies on human ingenuity to find gaps in your automation. Despite the widely held perception that\n\nmicroservices are too technical to manually test, there’s a great deal of similarity between how you manually test\n\n[10]\n\nan API\n\nand how you test a traditional GUI.\n\n9\n\nJames Bach gives a good introductory overview at http://www.satisfice.com/articles/what_is_et.shtml.\n\n10\n\nServices and APIs are often used interchangeably. Here I use “service” (or “microservice”) as the implementation and “API” as the interface. The users of a service only see the API. In fact,\n\nthey have no way of knowing if the API is implemented with one service or multiple, because you could use a reverse proxy to route requests to different services under the hood.\n\nI’ve seen people fall into two traps, both of which hurt their ability to gain confidence in your service. The first\n\nis treating the service as an implementation detail, a cog\n\nin a larger value chain, such that the only meaningful test is of the overall end user delivery (where “end users”\n\nmight be customers or business users). The second trap is believing that the service is too technical to test on its\n\nown.\n\nManually testing APIs\n\nOvercoming the first trap requires a mindset shift. The more your organization thinks of the API exposed by\n\nyour microservice as a product, the more likely you are to gain the scaling benefits of microservices. Amazon\n\nprovides an easy to spot example: Amazon Web Services (AWS). AWS started off as a simple object store (S3),\n\nwith an API to store and retrieve files. In short order, Amazon released EC2, which allowed programmatic\n\naccess to managing virtual machines. Both S3 and EC2 are products, as are the hundreds of other products in\n\nthe AWS suite. They have teams that manage them, they have customers, they provide self-service capabilities,\n\nand they hide the underlying complexity of those capabilities.\n\nAWS represents a collection of public APIs, but the same\n\nprinciple applies for APIs you build for your enterprise. The trick is realizing that your internal development\n\nteams are customers. They have needs and use your service to fulfill those needs, saving them time and\n\nreducing the complexity of their overall solution. Understanding their needs helps focus exploratory\n\ntesting.\n\nOnce you recognize that your API is a black box to your\n\ncustomers, you are free to test whether the black box behaves correctly. A good exploratory testing session would start by trying to solve an end-to-end customer\n\nproblem with your APIs and adjusting the path from one API call to the next based on what you learn as you go.\n\nDo the errors make sense? Does the response provide\n\nhints as to what happens next? Sometimes you might discover that, although your API is functionally stable, it\n\nhas significant usability gaps.\n\nAnother way to look at the second trap is thinking that,\n\nbecause there’s no UI, manual testing of APIs doesn’t make sense. Once you treat your API as a product, this\n\nargument disappears. Anytime you have customers (developers), you have a user interface. For APIs, that UI\n\nhappens to be JSON over HTTP (or equivalent).\n\nIn fact, you have been manually testing an API throughout this entire book. Every time you use curl (or\n\nPostman, a graphical equivalent) to send an HTTP request to mountebank, you’re using mountebank’s\n\ndeveloper-facing UI to test it.\n\nWhere service virtualization fits\n\nYou certainly can do exploratory testing without service virtualization. Indeed, at least some of the time, you\n\nshould. It helps gain full system context and understand the types of data that downstream systems emit.\n\nBut exploratory testing requires QA testers to get\n\ncreative. A large part of the exploration is finding out what they should test, which requires playing with some\n\nunusual setups. Virtualizing the dependencies can help provide additional knobs to tune during the exploration.\n\nA real-world scenario may help make that advice concrete. In chapter 8, I described how we used\n\nmountebank to help test APIs that powered the consumer-facing mobile application for a large airline. Our team was blessed with a couple of superbly capable\n\nQA testers who, through exploratory testing, unearthed several problems with our APIs before we released them\n\nto the public.\n\nAlthough some of their testing was manual, they used mountebank to test flows under certain scenarios. Going\n\nto the downstream integration points for those scenarios was quite onerous, so when they wanted to follow a flow\n\ninvolving a canceled flight (or a rerouted flight, overbooked flight, delayed flight, and so on), they used a\n\nset of mountebank imposters to facilitate the testing experience. The first time they tested a flow within a\n\ngiven scenario, like a canceled flight, they did so fully integrated so they could see real data. Once they had the\n\ndata, they used mountebank imposters on subsequent test explorations.\n\nExploratory testing completes our whirlwind tour of the role of testing in a continuous delivery world, including\n\nwhere service virtualization fits and where it doesn’t. We’ll round out the next chapter by using mountebank to help us with performance testing.\n\nSUMMARY\n\nA CD deployment pipeline includes automation that extends\n\nbeyond the realm of testing, but testing is central to providing the\n\nconfidence needed to release software frequently. The testing\n\nportion of the pipeline requires validations at multiple layers.\n\nUnit testing is as much a design activity as it’s a bug-catching\n\nactivity. Unit testing is in-process and, as such, uses traditional\n\nstubbing approaches instead of service virtualization.\n\nService tests are postdeployment black-box tests of your\n\napplication. Service virtualization ensures appropriate\n\ndeterminism.\n\nContract tests help validate the assumptions that your application\n\nand your service tests make. They should focus on testing your\n\nassumptions rather than behaviorally testing the dependent\n\nservice.\n\nExploratory testing unleashes human creativity to find flaws in\n\nyour software. Service virtualization may play a role in validating testers’ hunches, or you may avoid it in favor of deeper integration\n\ntesting.\n\nChapter 10. Performance testing with mountebank\n\nThis chapter covers\n\nHow service virtualization enables performance testing\n\nHow to capture appropriate test data with real latencies for load\n\ntesting\n\nHow to scale mountebank for load purposes\n\nThe final type of testing we’ll look at in this book is\n\nperformance testing, which covers a range of use cases. The simplest type of performance test is a load test,\n\nwhich uncovers the system behavior under a certain expected load. Other types of performance tests include\n\nstress tests, which show system behavior when the load exceeds available capacity, and soak tests, which show\n\nwhat happens to the system as it endures load over an extended amount of time.\n\nAll of the tests we’ve looked at up to this point have attempted to prove system correctness, but with\n\nperformance tests, the goal is to understand system behavior more than to prove its correctness. The understanding gained from performance testing does\n\nhelp to improve correctness through unearthing bugs (such as memory leaks) in the application, and it helps to\n\nensure that the operational environment of the application is capable of supporting expected loads. But\n\nno application can support infinite load, and stress testing in particular is designed to break the application\n\nby finding the upper limits of capacity. While that’s happening, a certain degree of errors is expected in many\n\nkinds of performance testing. The goal is to verify system behavior in aggregate rather than verify each service call\n\nindependently. Performance tests often help define service level objectives—for instance, that a service\n\nresponds within 500 milliseconds 99% of the time under expected load.\n\nPerformance testing can be difficult. Fortunately, mountebank is there to help.\n\n10.1. WHY SERVICE VIRTUALIZATION ENABLES PERFORMANCE TESTING\n\nOne of the first difficulties organizations run into when\n\nputting together a performance test plan is finding an environment in which to run it. Sometimes, that\n\nenvironment is production.\n\nBelieve it or not, production can be a natural place to\n\nperformance test under certain conditions. The first time you deploy a new application to production, it’s usually\n\nbefore users are able to use it. That gives you an opportunity to validate the capacity of the system, as long as you are careful with the data. In more advanced\n\nscenarios, with new features in existing applications, you may even want to validate performance by synthetically\n\nmanufacturing load in production before users are aware of the new feature. Facebook calls this dark launching,\n\nand did it for two weeks prior to allowing customers to set their own username. The functionality existed in\n\nproduction but was hidden, and a subset of user queries was routed to the new feature to verify it held up under\n\n[1]\n\nFacebook’s scale may be unique—imagine load. generating load from 1.5 billion people—but approaches\n\nlike dark launching can be valuable anytime you want to have additional confidence that a feature scales before\n\nreleasing it to the public.\n\n1\n\nSee https://www.facebook.com/notes/facebook-engineering/hammering-usernames/96390263919/ for more information.",
      "page_number": 288
    },
    {
      "number": 10,
      "title": "Performance testing with mountebank",
      "start_page": 323,
      "end_page": 380,
      "detection_method": "regex_chapter_title",
      "content": "Most performance testing happens prior to production, with the unfortunate corollary that it rarely happens with\n\nintegrations scaled to support production load. An all- too-common scenario is that, when performance testing\n\noutside of production, the application dependencies crash well before the application, making it impossible to\n\nverify service-level objectives (figure 10.1). When performance testing the application, you are implicitly\n\nmaking the assumption that the application is the weak link in the system. If it isn’t, then you aren’t really testing\n\nthe application and will be unable to discover the load it can support with the hardware it’s using.\n\nFigure 10.1. When runtime dependencies are unstable, you canʼt verify the performance of your application.\n\nAlthough those runtime dependencies are stable enough\n\nto handle production load in production, creating that level of stability isn’t always economically feasible in\n\nlower environments. Supporting additional load requires\n\nadditional hardware, and doubling the cost of production hardware for testing purposes is generally a hard sell.\n\nMany other reasons exist, some reasonable and some unfortunate, preventing nonproduction runtime\n\ndependencies from supporting the load you need to make your application the weak link. For example, it’s often\n\ndifficult and expensive to scale COTS (custom off-the- shelf software), especially when that COTS package runs\n\non a mainframe.\n\nFigure 10.1 looks like many other diagrams you have seen\n\nalready, and for good reason. Performance testing sits within a class of problems that requires a more\n\ndeterministic approach to testing an application with nondeterministic runtime dependencies. I hope by now\n\nyou can spot exactly the class of problems that service virtualization aims to help. It’s a problem that says “if the rest of the runtime ecosystem is more stable than my\n\napplication, then I can determine the performance characteristics of my application.” Service virtualization\n\nhelps ensure that the application is the weakest link in the runtime ecosystem.\n\nAt some scale, you run into another problem: the virtualization tool itself becomes a weaker link than the\n\napplication (figure 10.2). It’s in effect a hidden dependency that exposes itself when the application’s\n\ncapacity exceeds that of the virtualization tool.\n\nFigure 10.2. At a certain scale, the virtualization tool itself becomes the problem.\n\nThis problem exists with mountebank no differently\n\nfrom any other tool. The solution involves horizontally scaling the virtualization tool—running multiple\n\ninstances with shared test data and using a load balancer to spread the load over the multiple instances. This is\n\nwhere mountebank stands apart from the crowd. Scaling commercial tooling is expensive, requiring additional\n\nlicensing. Although a single instance of mountebank won’t perform as well as a single instance of most of the\n\ncommercial tools in the space, mountebank scales for free.\n\nCapital One went through performance testing pain as it\n\nmoved its mobile servicing platform to the cloud. Jason Valentino wrote about the cloud migration and\n\nacknowledged that they never anticipated the difficulty of challenges like performance testing.\n\n[2]\n\n2\n\nSee https://medium.com/capital-one-developers/moving-one-of-capital-ones-largest-customer-facing- apps-to-aws-668d797af6fc.\n\nIn fact, halfway through we discovered our corporate mocking software couldn’t handle the sheer amount of performance testing we were\n\nrunning as part of this effort (we completely crushed some pretty industrial enterprise\n\nsoftware in the process). As a result, we made the call to move the entire program over to a\n\nMountebank OSS-based solution with a custom provision to give us the ability to expand/shrink our\n\nmocking needs on demand.\n\nJason Valentino (emphasis his)\n\nIn the remainder of this chapter, we will stress test a\n\nsample service, finding its capacity. In doing so, we will [3] follow a four-step process:\n\n3\n\nThe steps are largely the same for other types of performance tests.\n\nDefine your scenarios.\n\nCapture the test data for each scenario.\n\nCreate the tests for a scenario.\n\nScale mountebank as needed.\n\nLet’s look at each of these steps in turn.\n\n10.2. DEFINING YOUR SCENARIOS\n\nPerformance tests are all about figuring out common paths users will likely take, then calling those paths a lot.\n\nFor example, let’s return to our favorite online pet store,\n\nbut add a new component to make a more realistic performance testing scenario. You’ll add a new adoption\n\nservice that provides the pet adoption information, helping connect potential owners with rescue pets. (See\n\nfigure 10.3.)\n\nFigure 10.3. Adding the adoption service to your pet store microservices\n\nThe service integrates with a public API from RescueGroups.org,\n\n[4]\n\nwhich makes it a perfect place to\n\nuse service virtualization. Although you’d like to test the adoption service to make sure it can handle load,\n\nslamming a public API providing animal adoption information free of charge seems rude, especially when\n\nit’s only for testing purposes. Every time you run your performance tests connected to a free public pet\n\nadoption service, your unintentional denial of service attack kills a kitten.\n\n4\n\nSee https://userguide.rescuegroups.org/display/APIDG/HTTP+API for API details.\n\nA scenario is a multistep flow that captures user intent. The trick is to put yourself in the mind of a user and\n\nimagine a common sequence of activities the user would want to complete. In this case, because the adoption\n\nservice is an API, the direct users will be other developers, but it will be in support of users on a website\n\nor mobile device, and their intent will be reflected in the sequence of API calls. You’d expect the end customers to\n\nsearch for nearby pets, maybe change the search parameters a few times, then click on a few pets. Let’s\n\nformalize that into a performance test scenario:\n\nUser searches for pets within a 20-mile radius of zip code 75228.\n\nUser searches for pets within a 50-mile radius of zip code 75228.\n\nUser gets details on the first three pets returned.\n\nThat scenario requires two APIs and five API calls in the process of completing two searches and providing three\n\nsets of details. The sequence of API calls with the adoption service would look like this:\n\n[5]\n\n5\n\nThe GitHub repo for this book has the source code: https://github.com/bbyars/mountebank-in- action.\n\nGET /nearbyAnimals?postalCode=75228&maxDistance=20\n\nGET /nearbyAnimals?postalCode=75228&maxDistance=50\n\nGET /animals/10677691\n\nGET /animals/10837552\n\nGET /animals/11618347\n\nThe animal IDs may vary from run to run, as the data\n\nthat the searches return changes over time. A robust test scenario would support dynamically pulling the IDs from\n\nthe search, but you’ll keep it simple to focus on the essentials.\n\nNow that you have a multistep scenario defined, it’s time to capture the test data.\n\n10.3. CAPTURING THE TEST DATA\n\nAccurately simulating a runtime environment for load\n\ntests requires that virtual services both respond similarly to how real services respond and perform like real\n\nservices perform in production. A proxy can capture both bits of information, and for performance testing you’ll almost always want to use proxyAlways mode. The\n\ndefault proxyOnce mode is convenient in situations when you want the saved responses to respond after the\n\nfirst call to the downstream service, but it’s natural to separate the test data capture from the test execution in\n\nperformance testing. Also, the richer set of data you are able to capture with proxyAlways often comes in\n\nhandy. Recall from chapter 5 that proxyAlways mode means that every call will be proxied to the downstream\n\nsystem, allowing you to record multiple responses for the same request (where the request is defined by the predicateGenerators) (figure 10.4).\n\nFigure 10.4. A proxyAlways proxy allows capturing complex test data.\n\nNotice that the leftmost box in figure 10.4 isn’t the\n\nperformance tests themselves. You aren’t ready for those\n\nyet; they come after you shut down the connection to the real RescueGroups API. At this stage, you want just\n\nenough load to capture meaningful test data. Anything more than that is an unnecessary load on downstream\n\nservices.\n\n10.3.1. Capturing the responses\n\nThis scenario is simple enough that you can capture data\n\nfor the five API calls and replay it over and over again during the performance test run. Technically, this means you don’t need proxyAlways mode for your proxy, but it’s generally a good idea to use it anyway when you are\n\ndoing anything mildly complicated with test data capture. The proxy stub looks like the following listing.\n\nListing 10.1. Basic proxy response to capture test data\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://api.rescuegroups.org/\", 1\n\n\"predicateGenerators\": [ 2 { \"matches\": { \"body\": true } } 2\n\n] 2\n\n}\n\n}]\n\n}\n\n1 Proxy to real service\n\n2 Saves new response for every unique request body\n\nThe adoption service uses different URLs and query\n\nparameters for each of the five API calls, but behind the curtain, they route to the same URL in the\n\nRescueGroups.org API. RescueGroups.org uses a super- generic API in which every call is an HTTP POST to the\n\nsame path (/http/v2.json). The JSON in the request body defines the intention of the call, any filters used, and so\n\non. Recall from chapter 5 that you use a proxy’s predicateGenerators to define the predicates for the saved response. Because each of your five API calls will\n\nsend a unique request body to the RescueGroups.org API, differentiating requests by body makes sense. If you\n\nwanted to be more specific, you could use a JSONPath predicate generator to split on the exact fields within the body that are different, but that’s overkill for this example. Once you have the proxy configured, you have\n\nto run your test scenario and save the test data (figure 10.5).\n\nFigure 10.5. Using a proxy to capture test data\n\nAnytime you use service virtualization, you have to be able to swap out the URL of the downstream dependency\n\nin the system under test. The adoption service in the GitHub repo for this chapter supports using an\n\nenvironment variable to change the URL of the downstream service. Assuming you run your proxy imposter on port 3000, you could configure the adoption\n\nservice like this:\n\nexport RESCUE_URL=http://localhost:3000/\n\nWith the proxy running and the adoption service pointed\n\nto it instead of the real service, you can capture the data for your five API calls with whatever HTTP engine you\n\nwant, including curl. Assuming the adoption service is running on port 5000, that might look like\n\ncurl http://localhost:5000/nearbyAnimals?\n\npostalCode=75228&maxDistance=20\n\ncurl http://localhost:5000/nearbyAnimals?\n\npostalCode=75228&maxDistance=50 curl http://localhost:5000/animals/10677691\n\ncurl http://localhost:5000/animals/10837552\n\ncurl http://localhost:5000/animals/11618347\n\nOnce you have done that, you can save the test data:\n\nmb save --removeProxies --savefile mb.json\n\nWith that, you have your responses. However, you need one more thing.\n\n10.3.2. Capturing the actual latencies\n\nTo get an accurate performance test, you’ll want to\n\nsimulate the actual latency from the downstream service. In chapter 7, we looked at the wait behavior, which\n\nallows you to tack on latency to each response. You can capture it from the downstream system by setting the addWaitBehavior attribute of the proxy to true, as shown in the following listing.\n\nListing 10.2. Capturing latency from the downstream system\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://api.rescuegroups.org/\",\n\n\"predicateGenerators\": [\n\n{ \"matches\": { \"body\": true } }\n\n], \"addWaitBehavior\": true 1\n\n}\n\n}]\n\n}\n\n1 Captures actual latency\n\nIf you capture your test data again by making the five API calls to the adoption service and saving the data with mb save, the proxy will have automatically added the wait behavior to each of the saved responses. For\n\nexample, in my test run, here’s a trimmed down version of a saved response:\n\n{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { ... }, 1 \"body\": \"...\", 1\n\n\"_mode\": \"text\"\n\n},\n\n\"_behaviors\": {\n\n\"wait\": 777 2\n\n}\n\n}\n\n1 Omitted for clarity\n\n2 Wait 777 ms before responding\n\nThe two searches in the test run used to prepare this sample took 777 and 667 milliseconds, respectively, and\n\nthe three requests for animal details took 292, 322, and 290 milliseconds. Those wait times were saved with each\n\nresponse to be replayed during the performance test run. The more data you capture during proxying, the more\n\nvariability you’ll have with your latencies.\n\n10.3.3. Simulating wild latency swings\n\nOur example assumes that the downstream system behaves correctly. That’s a perfectly valid assumption to\n\nmake when you want to see what happens when the system under test is the weakest link in the chain, but\n\nsometimes you also want to expose cascading errors that happen when downstream systems become overloaded,\n\nreturning a higher percentage of errors and (even worse) responding increasingly slowly. If the environment\n\nsupports recording proxy data under load, you may be\n\nable to capture the data from a downstream test system. If not, you’ll have to simulate it.\n\nThe wait behavior supports an advanced configuration for this use case. Instead of passing the number of\n\nmilliseconds to wait before returning a response, you can pass it a JavaScript function. Assuming you have started mb with the --allowInjection command-line flag, you can simulate a wild latency swing with the following\n\nfunction. It usually responds within a second, but roughly every 10 times it takes an order of magnitude\n\nlonger.\n\nListing 10.3. Adding random latency swings with JavaScript injection\n\nfunction () { var slowdown = Math.random() > 0.9,\n\nmultiplier = slowdown ? 10000 : 1000;\n\nreturn Math.floor(Math.random() * multiplier);\n\n}\n\nInstead of passing an integer representing a number of\n\nmilliseconds, you’d pass the entire JavaScript function to the wait behavior. Assuming you saved the previous\n\nfunction in a file called randomLatency.js, you could use EJS templating:\n\n{ \"is\": { ... },\n\n\"_behaviors\": {\n\n\"wait\": \"<%- stringify(filename,\n\n'randomLatency.js') %>\"\n\n}\n\n}\n\nThe downside is that this doesn’t work naturally with proxying, which captures actual latency.\n\n10.4. RUNNING THE PERFORMANCE TESTS\n\nYou can write all of the tests up to this point in the book\n\nwith traditional unit testing tools from the JUnit family.\n\nPerformance testing requires more specialized tools that provide a few key features the JUnit-style tools don’t:\n\nScenario recording, usually by configuring the tool as a proxy\n\nbetween your HTTP executor (generally a browser) and the application you are testing\n\nDomain-specific languages (DSLs) for adding pauses, simulating\n\nthe think time for users in between actions, and ramping up users\n\nThe ability to use multiple threads to simulate multiple users\n\nsending concurrent requests\n\nReporting capability to give you the performance characteristics of\n\nyour application after a test run\n\nAlthough many commercial options exist, some excellent open source performance testing tools are available that\n\ndon’t require you to open your checkbook. JMeter (http://jmeter.apache.org/) and a newer offshoot called\n\nGatling (https://gatling.io/) are popular choices. You’ll use a Gatling script simple enough to allow you to keep the\n\nfocus on service virtualization without having to learn a whole new tool.\n\nThe Gatling download is a simple zip file, which you can\n\nunpack in any directory you desire. Set the GATLING_HOME environment variable to that directory\n\nto make the example easier to follow. For example, if you have unpacked it on your home directory in Linux or\n\nmacOS, type this in your terminal (assuming you downloaded the same version used for this example):\n\nexport GATLING_HOME=~/gatling-charts-highcharts-\n\nbundle-2.3.0\n\nThe next step is to create a Gatling script that represents your scenario using its Scala-based DSL. I copied the\n\nsample scenario that ships with Gatling and adjusted it as shown in listing 10.4. As someone who has never\n\nprogrammed in Scala before, I found myself able to read and write most of the script quite fluently thanks to the\n\nexpressive DSL. It executes your five API calls with\n\npauses in between, representing the think time in seconds that the end users likely will take to process the\n\nresults.\n\nListing 10.4. A Gatling script for your test scenario\n\nclass SearchForPetSimulation extends Simulation {\n\nval httpProtocol = http\n\n.baseURL(\"http://localhost:5000\")\n\n1\n\nval searchScenario =\n\nscenario(\"SearchForPetSimulation\")\n\n.exec(http(\"first search\")\n\n2\n\n.get(\"/nearbyAnimals?\n\npostalCode=75228&maxDistance=20\")) 2\n\n.pause(10)\n\n2 .exec(http(\"second search\")\n\n2\n\n.get(\"/nearbyAnimals?\n\npostalCode=75228&maxDistance=50\")) 2\n\n.pause(15)\n\n2\n\n.exec(http(\"first animal\")\n\n3 .get(\"/animals/10677691\"))\n\n3\n\n.pause(5)\n\n3\n\n.exec(http(\"second animal\")\n\n3\n\n.get(\"/animals/10837552\"))\n\n3 .pause(5)\n\n3\n\n.exec(http(\"third animal\")\n\n3\n\n.get(\"/animals/11618347\"))\n\n3\n\nsetUp( searchScenario.inject(rampUsers(100) over (10\n\nseconds)) 4\n\n).protocols(httpProtocol)\n\n}\n\n1 Base URL of adoption service\n\n2 Searches, with think time\n\n3 Animal details with think time\n\n4 Simulates 100 users\n\nThe most interesting bit is near the bottom, which describes how many concurrent users you want to simulate and how long to ramp them up. It’s fairly\n\nunrealistic to expect that, at max load, all users start at the same time, so most performance test scenarios\n\naccount for a ramp-up period. Although 100 users isn’t much, it helps you test out your scenario.\n\nTo run Gatling, navigate to the code for chapter 10 in the GitHub repo for this book and enter the following into\n\nyour terminal.\n\nListing 10.5. Testing your performance script\n\n$GATLING_HOME/bin/gatling.sh \\\n\nsf gatling/simulations 1 -s adoptionservice.SearchForPetSimulation 2\n\nrf gatling/reports 3\n\n1 Points to your simulations directory\n\n2 Runs the correct scenario\n\n3 Saves the output here\n\nOn my machine, running this scenario for 100 users takes a little under a minute, which is hardly enough to\n\nstress either the software or hardware but enough to validate the script. Once you’re satisfied it’s working,\n\nbump the users up an order of magnitude or two and rerun to see what happens:\n\nsetUp( searchScenario.inject(rampUsers(1000) over (10\n\nseconds))\n\n).protocols(httpProtocol)\n\nOn the MacBook Pro I’m using to create this example,\n\nthe adoption service can handle 1,000 users with no\n\nproblem but croaks pretty hard at 10,000 users. When I tried running with that many users, Google Chrome\n\ncrashed, my editor froze, and I may have cried a little because of a failure to save work in progress, but,\n\nfortunately, no kittens died.\n\nThat’s useful information—not just the kittens, but the\n\nnumber of users: the adoption service, when run on my laptop, has the capacity to support somewhere between\n\n1,000 and 10,000 users concurrently. In addition to highlighting a horrific lack of error handling in the\n\nadoption service (subsequently improved), that information would help you determine the appropriate\n\nhardware to run in production based on the expected number of concurrent users trying to save a rescue\n\nanimal from the pound.\n\nI experimented a bit until I found a reasonable number of users that stressed the adoption service on my laptop\n\nwithout completely breaking it, which turned out to be 3,125 concurrent users. Understanding your\n\napplication’s behavior under stress is a useful activity for determining what happens at expected peak load and\n\nhelps to validate your service-level objectives.\n\nThe test reports are saved in the “gatling/reports” directory you passed into the -rf parameter when you\n\nstarted Gatling. The HTML page gives you all kinds of information that helps you understand the performance\n\ncharacteristics of your application. The table shown in figure 10.6 comes from a report on one of my runs and\n\nshows the % KO (errors, with KO being both a common boxing abbreviation for knockout and a clever anagram\n\nof OK) and statistical information around response times for each request.\n\nFigure 10.6. Gatling saves error rates and response time data for each step of the scenario.\n\nService performance is often quoted as something to the\n\neffect of “99% of the time we promise to return in under 500 milliseconds at or below peak load.” Clearly, you\n\nhave some work in front of you before you can make that kind of guarantee.\n\n10.5. SCALING MOUNTEBANK\n\nGiven that the adoption service is a simple example, I\n\nwas unable to get mountebank to crash under load before the service did. For production-quality services built by\n\nenterprises and deployed in a high-availability environment, that won’t always be the case. When\n\nmountebank itself becomes the weakest link in your chain, you have some options.\n\nThe first and most obvious is to run multiple\n\nmountebank instances behind a load balancer (figure 10.7). This allows different requests to route to different\n\nmountebank instances, each configured with the same test data.\n\nFigure 10.7. Load balancing using multiple mountebank instances\n\nOne situation requires additional thought. If the test data\n\nsupports sending different responses for the same logical request, then you’ll no longer be able to rely on a\n\ndeterministic ordering of those responses. The diagram in figure 10.8 shows two calls to your virtualized instance\n\nof an inventory service, which should return 54 on the first call and 21 on the second. Instead, it returns 54\n\ntwice in a row.\n\nFigure 10.8. Responses for the same request under load balancing yield unexpected results.\n\nThere’s no good way around this. Even if you use server affinity, a load balancer configuration that binds a client\n\nto the same mountebank instance for each request, you will likely still have the problem, because it’s the system\n\nunder test, not the test user, that’s making the request to mountebank. In addition to load balancing, you should\n\ndo a few things on each instance of mountebank to guarantee optimal performance.\n\nFirst, avoid using the --debug and --mock command line options when running mb. These options capture\n\nadditional information about the requests that the system under test makes to mountebank, which is useful\n\nboth for debugging the imposter configuration and for verifying that the system under test made the correct\n\nrequests. Although capturing that information during a behavioral test can be useful, performance tests require long-lived imposters. Computer programmers have a\n\ncommon phrase they use to describe the process by which a long-lived system remembers information\n\nwithout any mechanism of forgetting it: a memory leak.\n\nSecond, you will want to decrease the log output of\n\nmountebank. Mountebank uses a standard set of logging configuration levels—debug, info, warn, and error—\n\nand defaults to info. That sends some log output to the terminal and logging file on every request, which is\n\nunnecessary and unhelpful when you intend to send thousands of requests its way. I would recommend running with a warn level when you are writing and debugging performance scripts and with error during\n\nthe test run. You do that by passing --loglevel warn or --loglevel error as a flag to the mb command.\n\nFinally, you’ll generally want to configure the responses\n\nmountebank sends back to use keep-alive connections, which avoid the TCP handshake on every new\n\nconnection. Keep-alive connections are a huge\n\nperformance increase, and the proxy will generally capture them because most HTTP servers use keep-alive\n\nconnections by default. Unexpectedly, RescueGroups.org doesn’t, at least in my test run, so the example avoids the\n\nuse of keep-alive connections. This is probably appropriate because you’re trying to accurately simulate\n\nthe downstream system behavior. But it wouldn’t be hard to write a simple script to postprocess your test data file and change the Connection headers to keep-alive in all of your saved responses, should you choose to do so.\n\nAlso remember that, if you’re adding in simulated is or inject responses that you didn’t capture through\n\nproxying, you’ll have to manually set the Connection header to keep-alive, as mountebank defaults to\n\nclose for historical reasons. The easiest way is to change the default response, as you saw in chapter 3:\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"defaultResponse\": {\n\n\"headers\": {\n\n\"connection\": \"keep-alive\"\n\n}\n\n}, \"stubs\": [...]\n\n}\n\nAnd at long last, you have a complete performance\n\ntesting environment that doesn’t depend on any downstream system. And, best of all, no kittens were\n\nkilled in the process.\n\nPerformance testing wraps up our tour of service\n\nvirtualization. Although mountebank is clearly not the only tool you’ll need in your tool belt, it makes many important contributions to allowing you to validate your\n\napplication through a continuous delivery pipeline, even in a microservices world with a great deal of runtime\n\ncomplexity. Mountebank is always changing, and there’s an active mailing list on the website, which I encourage\n\nyou to use anytime you get stuck. Don’t be a stranger!\n\nSUMMARY\n\nService virtualization enables performance testing by protecting\n\ndownstream systems from load. It enables you to test your application as if it’s the weakest link in the chain.\n\nOnce you’ve determined your scenarios, you’ll generally want to use a proxy in proxyAlways mode to capture test data. Set the addWaitBehavior to true to capture actual latencies.\n\nTools like Gatling and JMeter support converting your test scenarios into robust performance scripts. Ensure you are running\n\nwith virtual services if your goal is to find the capacity of your\n\nservice without impacting downstream services.\n\nIf mountebank itself becomes the constraint, scale through load\n\nbalancing. Improve performance of each instance by decreasing\n\nlogging and using keep-alive connections.\n\nIndex\n\n[SYMBOL][A][B][C][D][E][F][G][H][I][J][K][L][M][N][O][P][Q][R][S][T]\n\n[U][V][W][X][Z]\n\nSYMBOL\n\n& character\n\n$ID token\n\n3A pattern\n\n400 Bad Request message\n\n429 HTTP status code nd\n\n500 status code, 2\n\nA\n\nAbagnale service\n\nAccept header\n\nnd\n\naddWaitBehavior attribute, 2 rd\n\nnd allowInjection flag, 2 , 3 , 4 , 5 , 6\n\nth\n\nth\n\nth\n\nAmazon Web Services (AWS)\n\nAnnounce method\n\nAnnouncementLog class\n\nAnnouncementTemplate\n\nAPIs (application program interfaces)\n\nmanually testing\n\ntest-by-test setup with\n\napplication protocol\n\nArgsFor method\n\nnd\n\nArrange-Act-Assert pattern, 2\n\nassertISODate helper\n\nassertString helper\n\nasterisk character\n\nasynchronous operations, adding\n\nstarring mountebank repo\n\nstarting OAuth handshakes\n\nvalidating OAuth authorizations\n\nvirtualizing OAuth-backed GitHub clients\n\nautomated testing\n\nautomation, boundary of\n\nAWS (Amazon Web Services)\n\nB\n\nBase64 encoding\n\nbehaviors\n\nadding latency to responses\n\ncomplete list of\n\ndecorating responses\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\noverview of\n\nrepeating responses\n\nreplacing content in responses\n\ncopying request data to responses\n\nlooking up data from external data sources\n\nbinary mode\n\npredicates in\n\nwith Base64 encoding\n\nblocking I/O\n\nboundary of automation\n\nboundary of deployment\n\nboundary of determinism\n\nboundary of value\n\nC\n\ncallback function\n\ncanned responses\n\ncycling through\n\ndefault responses\n\nchanging\n\nidentifying complete HTTP messages\n\nreusing HTTP connections\n\nnd\n\noverview of, 2\n\ntesting with\n\nHTTPS imposters\n\nsaving responses in configuration files\n\ncapturing\n\nlatencies\n\nresponses\n\ntest data\n\nnd\n\nCAs (Certificate Authorities), 2\n\nnd\n\ncaseSensitive parameter, 2\n\nCD (continuous delivery)\n\ncreating test pipelines\n\nbalancing service virtualization with contract tests\n\ncreating service tests\n\ncreating unit tests\n\nexploratory testing\n\noverview of\n\nwith microservices\n\nCertbot\n\nnd\n\nCertificate Authorities (CAs), 2\n\nCI (continuous integration)\n\nnd\n\ncircular buffer, 2\n\ncommercial virtualization tools\n\nconfigfile command-line option\n\nconfiguration files\n\nsaving multiple imposters in\n\nsaving responses in\n\nconfiguring proxies\n\nadding custom headers\n\nmutual authentication\n\ncongestion\n\nconjunctions with predicates\n\nconsole.log function\n\nnd\n\ncontains predicate, 2\n\nContainsMethodName method\n\ncontent in responses, replacing\n\ncopying request data to responses\n\nlooking up data from external data sources\n\nnd\n\nContent-Length header, 2\n\ncontinuous deployment\n\ncontinuous integration (CI)\n\ncontract tests\n\nexamples of\n\nmanaging test data\n\noverview of\n\nnd\n\nservice virtualization and, 2\n\nconvenience functions\n\ncopy behavior\n\ncopy field\n\nCORS (Cross-Origin Resource Sharing)\n\nCOTS (custom off-the-shelf software)\n\ncreate function\n\nnd\n\ncreateContentImposter function, 2\n\nCreateImposter function\n\nnd\n\ncreateProductImposter function, 2\n\ncreation function\n\nCrier class\n\nnd\n\ncsvToObjects function, 2\n\nnd\n\ncurl command, 2\n\ncustom off-the-shelf software (COTS)\n\nD\n\nd@ command-line switch\n\ndata\n\ncapturing test data\n\ncapturing latencies\n\ncapturing responses\n\nsimulating wild latency swings\n\nfrom external data sources\n\nmanaging test data\n\nnd\n\ndebugging, 2\n\ndecorate function\n\ndecorating responses\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\ndecryption\n\nnd\n\ndeepEquals predicate, 2\n\ndefault responses\n\nchanging\n\nidentifying complete HTTP messages\n\noverview of\n\nreusing HTTP connections\n\ndefault stub\n\nnd\n\nDELETE command, 2\n\nnd\n\ndeployment pipelines, 2\n\ndescribe function\n\nnd\n\ndeterministic tests, 2\n\ndev complete\n\ndomain-specific languages (DSLs)\n\ndownstream capacity\n\nDRY (Don’t Repeat Yourself)\n\nDSLs (domain-specific languages)\n\nE\n\nEcmaScript (ES)\n\nEJS language\n\nend-to-end testing\n\nendOfRequestResolver function\n\nendsWith predicate\n\nengineering discipline\n\nenterprise service bus (ESB)\n\nequals operator\n\nnd\n\nequals predicates, 2\n\nES (EcmaScript)\n\nESB (enterprise service bus)\n\nescape characters\n\nnd\n\nexists predicate, 2\n\nnd\n\nexploratory testing, 2\n\nmanually testing APIs\n\nservice virtualization and\n\nexternal data sources\n\nF\n\nfallback proxy\n\nfeature complete\n\nFile.readlines function\n\nfilename variable\n\nfromDataSource field\n\nG\n\nGATLING_HOME variable\n\nnd\n\nGET method, 2\n\nGitHub clients\n\ngreaterThan predicate\n\ngrouped matches\n\nGUID (globally unique identifier)\n\nH\n\nnd\n\nnd\n\nhandshakes, 2\n\nhardening iterations\n\nhasThreeDaysOutOfRange function\n\nheaders\n\nHTTP\n\nconverting HTTPS to\n\nmessages, identifying\n\nreusing connections\n\nhttpRequest.call() function\n\nhttpRequest.end() function\n\nHTTPS\n\nconverting to HTTP\n\nimposters\n\ntrusted, setting up\n\nusing mutual authentication\n\nI\n\nI/O operations\n\nidentifiers, matching on paths\n\nimposters\n\nHTTPS\n\ntrusted, setting up\n\nusing mutual authentication\n\nmultiple, saving in configuration files nd\n\nrd\n\noverview of, 2 , 3\n\nTCP, creating\n\ninclude function\n\nnd\n\nrd\n\ninject response type, 2 , 3\n\ninventory checks\n\nInvoke-RestMethod command\n\nipWhitelist flag\n\nnd\n\nrd\n\nnd\n\nrd\n\nis response, 2 , 3\n\nISO format\n\nJ\n\nJSON format\n\ndirect predicates\n\nvalues\n\npredicates with\n\nselecting with JSONPath\n\nnd\n\njsonpath parameter, 2\n\njsonpath predicateGenerator\n\nJSONPath tool\n\ncapturing multiple values\n\ngenerating predicates\n\nselecting JSON values with\n\nK\n\nkeep-alive connections\n\nkeys\n\nL\n\nlatencies\n\nadding to responses\n\ncapturing\n\nsimulating swings\n\nLet’s Encrypt\n\nlink layer\n\nlocalOnly flag\n\nloglevel debug flag\n\nlookup behavior\n\nM\n\nMarshalByRefObject\n\nmatches predicate\n\nmatching\n\ngrouped matches\n\nidentifiers on paths\n\nmultivalued fields\n\nobject request fields\n\nXML payloads\n\nmatchingRequest function\n\nmb protocol\n\nmb replay command\n\nnd\n\nmb save command, 2\n\nnd\n\nmetacharacters, 2\n\nmicroservices\n\nCD with, testing strategy for\n\nevolution of\n\norganizational structure and\n\noverview of\n\ntesting\n\nend-to-end testing, challenges of\n\nmountebank, overview of\n\nservice virtualization\n\nservice virtualization tool ecosystem\n\nmiddleware, adding\n\nmock option\n\nmonoliths\n\nmountebank\n\nHTTP and\n\noverview of\n\nstarring repo\n\nMule ESB\n\nmultivalued fields, matching nd\n\nmutual authentication, 2\n\nnd\n\nmutualAuth field, 2\n\nN\n\nnamespaces, support for\n\n.NET Remoting framework\n\ncreating clients\n\nvirtualizing servers\n\nvirtualizing services\n\nindicating where messages end\n\noverview of\n\nNetcat\n\nnd\n\nnonblocking I/O, 2\n\nO\n\nOAuth protocol\n\nOAuth-backed GitHub clients, virtualizing\n\nstarting handshakes\n\nvalidating authorizations\n\nobject request fields, matching\n\nOS (operating system)\n\nP\n\nparameterizing predicates\n\nparseInt function\n\npaths, matching identifiers on\n\npayloads, XML\n\nmanipulating\n\nmatching\n\nPEM format\n\nperformance testing\n\ncapturing test data\n\ncapturing latencies\n\ncapturing responses\n\nsimulating wild latency swings\n\ndefining scenarios\n\nrunning tests\n\nscaling mountebank\n\nservice virtualization enabling\n\npersistent data stores\n\npipelines\n\ndeployment pipelines\n\ntest pipelines\n\nbalancing service virtualization with contract tests\n\ncreating service tests\n\ncreating unit tests\n\nexploratory testing\n\nPostman\n\npredicate injection function\n\nnd\n\npredicateGenerators, 2\n\npredicates\n\nadding parameters\n\ncase-sensitive\n\nconjunctions with\n\nnd\n\ncreating, 2\n\ncapturing multiple JSONPath values\n\ncapturing multiple XPath values\n\nfor JSONPath\n\nfor XPath\n\nwith predicateGenerators\n\ndeepequals predicate\n\nexists predicate\n\nin binary mode\n\nmatches predicate\n\nmatching multivalued fields\n\nmatching object request fields\n\nnd\n\nrd\n\noverview of, 2 , 3\n\nparameterizing\n\npredicate injections vs responses\n\nreplacing with regular expressions\n\nselecting XML values\n\ntypes of\n\ncomplete list\n\nmatching any identifier on path\n\nwith JSON values\n\ndirect JSON predicates\n\nselecting JSON values with JSONPath\n\npreflight responses\n\nprintf debugging\n\nprivate key\n\nprocess discipline\n\nproduct catalog services\n\nproductCatalog object\n\npromises\n\nprotocols\n\nbinary support\n\nbinary mode with Base64 encoding\n\npredicates in binary mode\n\noverview of\n\nvirtualizing .NET Remoting service\n\ncreating clients\n\nindicating where messages end\n\nvirtualizing .NET Remoting servers\n\nproxies\n\nadding decoration to saved proxy responses\n\nas fallbacks\n\nconfiguring\n\nadding custom headers\n\nmutual authentication\n\nreplaying\n\nsetting up\n\nTCP, creating\n\nuse cases\n\nproxy response type\n\nnd\n\nproxyAlways mode, 2 nd\n\nproxyOnce mode, 2\n\nproxyResponseTime field\n\npublic key\n\nPUT command\n\nQ\n\nquery parameter\n\nquerystring parameter\n\nquestion mark character\n\nR\n\nrate limit exception\n\nreal imposter\n\nnd\n\nrecord/replay behavior, 2\n\ncapturing multiple responses for same request\n\nconfiguring proxies\n\nadding custom headers\n\nmutual authentication\n\ngenerating correct predicates\n\nadding predicate parameters\n\ncapturing multiple JSONPath values\n\ncapturing multiple XPath values\n\ncreating predicates with predicateGenerators\n\nproxy use cases\n\nconverting HTTPS to HTTP\n\nusing proxies as fallback\n\nreplaying proxies\n\nsetting up proxies\n\nnd\n\nnd\n\nregex (regular expressions), 2\n\nremoveProxies flag\n\nrepeat behavior\n\nrepeating responses\n\nreplayable parameter\n\nreplaying proxies\n\nrequest data, copying to responses\n\nusing grouped matches\n\nusing XPath selectors\n\nvirtualizing CORS preflight responses\n\nrequest library\n\nrequests, capturing responses for\n\nrespondingWith function\n\nnd\n\nresponse injection function, 2\n\nresponses\n\nadding latency to\n\ncapturing\n\nmultiple for same request\n\noverview of\n\ncopying request data to\n\nusing grouped matches\n\nusing XPath selectors\n\nvirtualizing CORS preflight responses\n\ncycling through\n\ndecorating\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\ndynamic, creating\n\nadding async\n\nadding state\n\npredicate injections vs\n\nrepeating\n\nreplacing content in\n\nlooking up data from external data sources\n\noverview of\n\nnd\n\nsaving in configuration files, 2\n\nretrieve function\n\nRPCs (Remote Procedure Calls), stubbing\n\ncreating TCP imposters\n\ncreating TCP proxies\n\nmanipulating XML payloads\n\nmatching XML payloads\n\nruntime dependencies\n\nS\n\nnd\n\nSaaS (software-as-a-service), 2\n\nsame-origin policy\n\nsavefile argument\n\nnd\n\nscaling, 2\n\nscenarios, defining\n\nsecurity\n\nSerialize method\n\nservers, .Net Remoting\n\nservice test\n\nservice virtualization\n\nnd\n\ncontract tests and, 2\n\nexamples of\n\nmanaging test data\n\nenabling performance testing\n\nexploratory testing and\n\nnd\n\noverview of, 2\n\npersistent data stores\n\nrecord and replay\n\ntest-by-test setup using APIs\n\ntool ecosystem of\n\nnd\n\nrd\n\nshellTransform behaviors, 2 , 3\n\nSignatureFor method\n\nsimulating latency swings\n\nsoak tests\n\nnd\n\nsoftware-as-a-service (SaaS), 2\n\nspanned reports\n\nstarring mountebank repo\n\nstartsWith predicate\n\nstate parameter\n\nstate.humidities array\n\nstates, adding\n\nstress tests\n\nstring escape character nd\n\nrd\n\nstringify function, 2 , 3\n\nstub() function\n\nstubs\n\nT\n\nTCP (Transmission Control Protocol)\n\ncreating imposters\n\ncreating proxies\n\noverview of\n\nstubbing text-based TCP-based RPCs\n\nmanipulating XML payloads\n\nmatching XML payloads\n\nTDD (test-driven development)\n\nTearDown method\n\ntest case construction\n\ntest-driven development (TDD)\n\ntesting\n\ncontract tests\n\nexamples of\n\nmanaging test data\n\nservice virtualization and, 2nd\n\ncreating test pipelines\n\nend-to-end\n\nexploratory testing\n\nmanually testing APIs\n\nservice virtualization and\n\nmapping strategy to deployment pipeline\n\nmicroservices\n\nmountebank, overview of\n\nservice virtualization\n\nservice virtualization tool ecosystem\n\nperformance testing\n\ncapturing test data\n\ndefining scenarios\n\nrunning tests\n\nscaling mountebank\n\nservice virtualization enabling\n\nservice tests, creating\n\nstrategy for CD with microservices\n\ntest-by-test setup using APIs\n\nunit tests, creating\n\nwith canned responses\n\nHTTPS imposters\n\nsaving responses in configuration files\n\nwriting tests\n\nthen function\n\nTLS (transport layer security)\n\nTownCrierGateway function\n\nTransfer-Encoding header\n\ntransport layer security (TLS)\n\ntransport protocol\n\nU\n\nnd\n\nunit testing pattern, 2\n\nnd\n\nupdateInventory function, 2\n\nupstream capacity\n\nV\n\nvalidating OAuth authorizations\n\nvalue, boundary of\n\nvirtual imposter\n\nvirtualization tools\n\nvirtualizing\n\n.NET Remoting servers\n\n.NET Remoting services\n\ncreating .NET Remoting clients\n\nindicating where messages end\n\nCORS preflight responses\n\nexploratory testing and service virtualization\n\nOAuth-backed GitHub clients\n\nproduct catalog services\n\nW\n\nnd\n\nrd\n\nwait behavior, 2 , 3\n\nwildcard certificates\n\nwithStub function\n\nX\n\nnd\n\nx-rate-limit-remaining header, 2\n\nXML language\n\npayloads\n\nmanipulating\n\nmatching\n\nselecting values\n\nXMLHttpRequest object\n\nXPath language\n\ncapturing multiple values\n\ngenerating predicates\n\nselectors\n\nnd\n\nxpath parameter, 2\n\nxpath predicate\n\nZ\n\nzero-character match\n\nList of Figures\n\nChapter 1. Testing microservices\n\nFigure 1.1. A monolithic application handles view, business, and persistence logic for multiple domains.\n\nFigure 1.2. Scaling a monolith means multiple teams have\n\nto work in the same codebase.\n\nFigure 1.3. Services use different databases for different domains.\n\nFigure 1.4. During normal traffic, the number of lanes and speed limit define throughput and velocity.\n\nFigure 1.5. Having fewer upstream lanes increases\n\ncongestion.\n\nFigure 1.6. Centralized QA processes recouple releases\n\ntogether, causing a bottleneck.\n\nFigure 1.7. Independent testing works to avoid release congestion.\n\nFigure 1.8. End-to-end testing introduces several problems of coordination.\n\nFigure 1.9. Testing using service virtualization\n\nFigure 1.10. Service virtualization supports a standard unit\n\ntesting pattern.\n\nFigure 1.11. Using persistent test data from a data store\n\nFigure 1.12. Capturing real traffic for later replay\n\nFigure 1.13. Configuring virtual services with a simple mountebank imposter\n\nFigure 1.14. Matching a request to a response with mountebank\n\nFigure 1.15. Response generation in mountebank using\n\npredicates and responses in stubs\n\nChapter 2. Taking mountebank for a test drive\n\nFigure 2.1. Your reference architecture for exploring mountebank\n\nFigure 2.2. Breaking down the HTTP request for products\n\nFigure 2.3. The response from the product catalog\n\nFigure 2.4. Adding a query parameter to an HTTP request\n\nFigure 2.5. How mountebank views an HTTP request\n\nFigure 2.6. How mountebank represents an HTTP\n\nresponse\n\nFigure 2.7. Virtualizing the product catalog service to test\n\nthe web facade\n\nFigure 2.8. Using curl to send a request to your virtual product catalog service\n\nFigure 2.9. Combining product data with marketing copy\n\nFigure 2.10. The steps of the service test to verify web façade’s data aggregating\n\nChapter 3. Testing using canned responses\n\nFigure 3.1. How mountebank selects a response\n\nFigure 3.2. TCP making the connection for HTTP messages\n\nFigure 3.3. Using chunked encodings or content length to calculate where the body ends\n\nFigure 3.4. Inventory checks return volatile results for the\n\nsame request.\n\nFigure 3.5. Each stub cycles through the responses\n\nforever.\n\nFigure 3.6. The basic structure of SSL/TLS\n\nFigure 3.7. Using two keys prevents attackers from\n\nreading messages in transit even when the encryption key is shared.\n\nFigure 3.8. Setting up a test environment with HTTPS\n\nFigure 3.9. How Let’s Encrypt validates the domain\n\nFigure 3.10. Setting up a test environment with HTTPS to validate clients as well as servers\n\nFigure 3.11. The tree structure for the secure inventory\n\nimposter configuration\n\nFigure 3.12. The tree structure for multiple services\n\nChapter 4. Using predicates to send different responses\n\nFigure 4.1. The Abagnale service adapts its response to the\n\nquestions you ask it.\n\nFigure 4.2. Mountebank matches the request against each stub’s predicates.\n\nFigure 4.3. Mountebank matches the request against each\n\nstub’s predicates.\n\nFigure 4.4. How simple predicates match against the full request field\n\nFigure 4.5. How a regular expression matches against a string value\n\nFigure 4.6. Emulating the contains predicate with a\n\nregular expression\n\nFigure 4.7. Emulating the startsWith predicate with a\n\nregular expression\n\nFigure 4.8. Emulating the endsWith predicate with a regular expression\n\nFigure 4.9. Emulating the startsWith, contains, and endsWith predicates with one regular expression\n\nFigure 4.10. Emulating the equals predicate with a regular\n\nexpression\n\nFigure 4.11. Breaking down a JSONPath query\n\nFigure 4.12. Breaking down an XPath query\n\nChapter 5. Adding record/replay behavior\n\nFigure 5.1. An imposter acting as a proxy\n\nFigure 5.2. Using a proxy to query the downstream\n\ninventory\n\nFigure 5.3. By default, the proxy returns the first result as\n\nthe response to all subsequent calls.\n\nFigure 5.4. The proxy saves the downstream response in a new stub.\n\nFigure 5.5. In proxyOnce mode, mountebank creates new\n\nstubs before the stub with the proxy.\n\nFigure 5.6. Volatile responses for the same request\n\nFigure 5.7. In proxyAlways mode, new stubs are created\n\nafter the stub with the proxy\n\nFigure 5.8. Replaying involves removing the proxy\n\nFigure 5.9. You can configure both the proxy request and\n\nthe generated response.\n\nFigure 5.10. Configuring the proxy to pass a client certificate\n\nFigure 5.11. Proxying compressed responses\n\nFigure 5.12. Mixing canned responses with a fallback proxy\n\nChapter 6. Programming mountebank\n\nFigure 6.1. Service collaboration used to protect my\n\nguitars\n\nFigure 6.2. A test scenario requiring advanced predicate logic\n\nFigure 6.3. Two reports need to be spanned to detect dangerous humidity.\n\nFigure 6.4. What happens with traditional blocking I/O\n\nFigure 6.5. Nonblocking I/O doesn’t block the process.\n\nFigure 6.6. Registering a GitHub application\n\nFigure 6.7. Understanding the GitHub OAuth flow\n\nFigure 6.8. Viewing the client secret to communicate to\n\nGitHub\n\nFigure 6.9. The three stubs needed to virtualize this GitHub workflow\n\nFigure 6.10. Using mountebank to spoof your identity during a network attack\n\nChapter 7. Adding behaviors\n\nFigure 7.1. Behaviors can transform a response from a\n\nstub before it goes out via the imposter.\n\nFigure 7.2. Decoration allows you to postprocess the response.\n\nFigure 7.3. Behaviors applied to proxies don’t transfer to the saved responses.\n\nFigure 7.4. Manufacturing a rate limit exception on a\n\nrecorded response\n\nFigure 7.5. The shellTransform behavior allows you to combine multiple transformations piped through the shell.\n\nFigure 7.6. Repeating a response multiple times\n\nFigure 7.7. A CORS preflight request to establish trust\n\nFigure 7.8. Looking up a value from a CSV file\n\nChapter 8. Protocols\n\nFigure 8.1. The flow of a mountebank-generated HTTP response\n\nFigure 8.2. A client application talking to a server\n\napplication over the internet\n\nFigure 8.3. Transforming an HTTP request to route across the network\n\nFigure 8.4. Virtualizing a custom TCP protocol\n\nFigure 8.5. Using Content-Length to wrap multiple packets into one HTTP request\n\nFigure 8.6. Mismatched expectations around when the request ends\n\nChapter 9. Mountebank and continuous delivery\n\nFigure 9.1. A deployment pipeline defines the path from\n\ncommit to production.\n\nFigure 9.2. Simplified test pyramid for microservices\n\nFigure 9.3. The basic structure for unit and service tests\n\nFigure 9.4. The basic structure for contract and\n\nexploratory tests\n\nFigure 9.5. Mapping your test pyramid to a deployment pipeline\n\nFigure 9.6. An example set of microservices\n\nFigure 9.7. Contract tests validate the assumptions made in service tests.\n\nChapter 10. Performance testing with mountebank\n\nFigure 10.1. When runtime dependencies are unstable, you can’t verify the performance of your application.\n\nFigure 10.2. At a certain scale, the virtualization tool itself becomes the problem.\n\nFigure 10.3. Adding the adoption service to your pet store\n\nmicroservices\n\nFigure 10.4. A proxyAlways proxy allows capturing complex test data.\n\nFigure 10.5. Using a proxy to capture test data\n\nFigure 10.6. Gatling saves error rates and response time data for each step of the scenario.\n\nFigure 10.7. Load balancing using multiple mountebank instances\n\nFigure 10.8. Responses for the same request under load\n\nbalancing yield unexpected results.\n\nList of Tables\n\nChapter 4. Using predicates to send different responses\n\nTable 4.1. Regular expression metacharacters\n\nTable 4.2. All predicates that mountebank supports\n\nChapter 7. Adding behaviors\n\nTable 7.1. All behaviors that mountebank supports\n\nList of Listings\n\nChapter 2. Taking mountebank for a test drive\n\nListing 2.1. Creating an imposter on the command line\n\nListing 2.2. The HTTP response from the curl command\n\nListing 2.3. An imposter with predicates\n\nListing 2.4. Combining product data with marketing\n\ncontent\n\nListing 2.5. Using a fluent interface to build imposters in code\n\nListing 2.6. Creating the product imposter in code\n\nListing 2.7. Creating the content imposter\n\nListing 2.8. Verifying the web façade\n\nListing 2.9. Adding the ability to remove imposters\n\nChapter 3. Testing using canned responses\n\nListing 3.1. Hello world! in an HTTP response\n\nListing 3.2. The HTTP response structure in JSON\n\nListing 3.3. The imposter configuration to respond with\n\nHello, world!\n\nListing 3.4. The default response in mountebank\n\nListing 3.5. Changing the default response\n\nListing 3.6. A response using the new defaults\n\nListing 3.7. Returning a list of responses for the same stub\n\nListing 3.8. Creating an HTTPS imposter\n\nListing 3.9. Adding mutual authentication to an imposter\n\nListing 3.10. Storing the inventory service in a configuration file\n\nListing 3.11. The root configuration file, referencing other\n\nimposters\n\nListing 3.12. The updated version of the product catalog\n\nimposter configuration\n\nListing 3.13. The updated version of the marketing content imposter configuration\n\nListing 3.14. Using curl to send the JSON to mountebank\n\nChapter 4. Using predicates to send different responses\n\nListing 4.1. Creating an Abagnale imposter\n\nListing 4.2. Using the matches predicate to do the job of a set of startsWith, contains, and endsWith predicates\n\nListing 4.3. Using the matches predicate to match any\n\nidentity resource\n\nListing 4.4. Adding a predicate for a query parameter\n\nListing 4.5. Using predicate arrays\n\nListing 4.6. Combining multiple predicates using and, or,\n\nand not\n\nListing 4.7. Using a case-sensitive predicate\n\nListing 4.8. Using a direct JSON predicate on an HTTP\n\nbody\n\nListing 4.9. Using JSONPath to match only the last element of an array\n\nListing 4.10. Using XPath to prevent Abagnale from claiming he taught in Utah\n\nListing 4.11. Using XPath to assert that the name attribute\n\nexists and the name tag doesn’t\n\nChapter 5. Adding record/replay behavior\n\nListing 5.1. Imposter configuration for a basic proxy\n\nListing 5.2. Saved proxy responses change imposter state\n\nListing 5.3. Imposter response that saves a different\n\nresponse for each path\n\nListing 5.4. Saved proxy responses with predicates\n\nListing 5.5. Generating case-sensitive predicates\n\nListing 5.6. Specifying a jsonpath predicateGenerators\n\nListing 5.7. Specifying an xpath predicateGenerators\n\nListing 5.8. Creating a proxyAlways proxy response\n\nListing 5.9. The imposter state after a few calls to a\n\nproxyAlways response\n\nListing 5.10. A proxy configured to pass a client certificate\n\nListing 5.11. Injecting a header into the request to prevent response compression\n\nListing 5.12. Using a partial proxy\n\nListing 5.13. Using a proxy to bridge HTTPS to HTTP\n\nChapter 6. Programming mountebank\n\nListing 6.1. A weather CSV service payload\n\nListing 6.2. The structure of an inject predicate\n\nListing 6.3. A JavaScript function to parse CSV data—\n\ncsvToObjects\n\nListing 6.4. Looking for three consecutive days out of\n\nrange\n\nListing 6.5. A predicate to test for excess humidity levels\n\nListing 6.6. The basic structure of response injection\n\nListing 6.7. A response injection function to virtualize the\n\nroadie service humidity checks\n\nListing 6.8. Remembering state between responses\n\nListing 6.9. Using traditional I/O to sort lines in a file\n\nListing 6.10. File sort using nonblocking I/O\n\nListing 6.11. Stub using response injection to make OAuth callback\n\nListing 6.12. Injection function to make OAuth callback\n\nListing 6.13. The stub to get an access token\n\nListing 6.14. Stub to check if you’ve starred the mountebank repo\n\nChapter 7. Adding behaviors\n\nListing 7.1. Adding behaviors to a stub definition\n\nListing 7.2. Using an inject response to send a dynamic timestamp\n\nListing 7.3. Combining an is response with a decorate behavior\n\nListing 7.4. Adding a decorate behavior to recorded responses\n\nListing 7.5. Decorator function to accelerate a rate limit\n\nexception\n\nListing 7.6. Imposter configuration for shellTransform\n\nListing 7.7. Ruby script to transform the response to\n\ntrigger a rate limit error\n\nListing 7.8. Ruby script to add a timestamp to the\n\nresponse JSON\n\nListing 7.9. Using a wait behavior to add latency\n\nListing 7.10. Using a repeat behavior to return an error after a small number of successes\n\nListing 7.11. Specifying a token in the response to replace with a value from the request\n\nListing 7.12. Using a copy behavior to insert the ID from\n\nthe URL into the response body\n\nListing 7.13. Using a grouped match to copy a portion of\n\nthe request path\n\nListing 7.14. Using an XPath selector to copy a value from the request to the response\n\nListing 7.15. Virtualizing a CORS preflight request\n\nListing 7.16. Centralizing error conditions in a CSV file\n\nListing 7.17. Using a lookup behavior to retrieve external test data\n\nListing 7.18. Using tokens to create a single response for all error conditions\n\nChapter 8. Protocols\n\nListing 8.1. Virtualizing a TCP updateInventory call\n\nListing 8.2. Using a TCP record/replay proxy\n\nListing 8.3. A TCP proxy with predicateGenerators\n\nListing 8.4. Using XPath predicateGenerators with a TCP\n\nproxy\n\nListing 8.5. Setting up a binary response from an imposter\n\nListing 8.6. Using a binary contains predicate\n\nListing 8.7. The Crier class definition\n\nListing 8.8. A gateway to a remote Crier\n\nListing 8.9. Basic test fixture using MbDotNet\n\nListing 8.10. Serializing a stub response for .NET Remoting\n\nListing 8.11. Creating a TCP proxy to a .NET Remoting server\n\nListing 8.12. Adding an endOfRequestResolver\n\nListing 8.13. The function to determine if you have seen the entire request yet\n\nChapter 9. Mountebank and continuous delivery\n\nListing 9.1. Web façade initialization code\n\nListing 9.2. Web façade code to aggregate product and content data\n\nListing 9.3. The shell of the productCatalog module\n\nListing 9.4. The code that retrieves and aggregates\n\ndownstream services\n\nListing 9.5. The test setup, using dependency injection and stubs\n\nListing 9.6. The unit test assertion\n\nListing 9.7. A service test that validates web façade aggregation\n\nListing 9.8. The imposter creation functions\n\nListing 9.9. A contract test validating the placing and format of fields from a real dependency\n\nChapter 10. Performance testing with mountebank\n\nListing 10.1. Basic proxy response to capture test data\n\nListing 10.2. Capturing latency from the downstream\n\nsystem\n\nListing 10.3. Adding random latency swings with\n\nJavaScript injection\n\nListing 10.4. A Gatling script for your test scenario\n\nListing 10.5. Testing your performance script",
      "page_number": 323
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "About the cover illustration\n\nThe figure on the cover of Testing Microservices with\n\nMountebank is captioned “A Man from Slovenia.” This illustration is taken from a recent reprint of Balthasar\n\nHacquet’s Images and Descriptions of Southwestern and Eastern Wends, Illyrians, and Slavs, published by\n\nthe Ethnographic Museum in Split, Croatia, in 2008. Hacquet (1739–1815) was an Austrian physician and\n\nscientist who spent many years studying the botany, geology, and ethnography of many parts of the Austrian\n\nEmpire, as well as the Veneto, the Julian Alps, and the western Balkans, inhabited in the past by peoples of the\n\nIllyrian tribes. Hand-drawn illustrations accompany the many scientific papers and books that Hacquet\n\npublished.\n\nThe rich diversity of the drawings in Hacquet’s publications speaks vividly of the uniqueness and\n\nindividuality of the eastern Alpine and northwestern Balkan regions just 200 years ago. This was a time when\n\nthe dress codes of two villages separated by a few miles identified people uniquely as belonging to one or the\n\nother, and when members of a social class or trade could be easily distinguished by what they were wearing. Dress\n\ncodes have changed since then and the diversity by region, so rich at the time, has faded away. It is now\n\noften hard to tell the inhabitant of one continent from another, and today the inhabitants of the picturesque\n\ntowns and villages in the Slovenian Alps or Balkan coastal towns are not readily distinguishable from the\n\nresidents of other parts of Europe.\n\nWe at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "book covers based on costumes from two centuries ago, brought back to life by illustrations such as this one.",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Testing Microservices with Mountebank\n\nBrandon Byars",
      "content_length": 52,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Copyright\n\nFor online information and ordering of this and other\n\nManning books, please visit www.manning.com. The publisher offers discounts on this book when ordered in\n\nquantity. For more information, please contact\n\nSpecial Sales Department\n\nManning Publications Co.\n\n20 Baldwin Road\n\nPO Box 761\n\nShelter Island, NY 11964\n\nEmail: orders@manning.com\n\n©2019 by Manning Publications Co. All rights reserved.\n\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by\n\nmeans electronic, mechanical, photocopying, or otherwise, without prior written permission of the\n\npublisher.\n\nMany of the designations used by manufacturers and\n\nsellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a\n\ntrademark claim, the designations have been printed in initial caps or all caps.\n\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have the books we\n\npublish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our responsibility to\n\nconserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and\n\nprocessed without the use of elemental chlorine.",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964\n\nDevelopment editor: Elesha Hyde\n\nTechnical development editor: John Guthrie\n\nReview editor: Ivan Martinović\n\nProject manager: Vincent Nordhaus\n\nCopy editor: Carl Quesnel\n\nProofreader: Keri Hales\n\nTechnical proofreader: Alessandro Campeis\n\nTypesetter and cover designer: Marija Tudor\n\nISBN 9781617294778\n\nPrinted in the United States of America\n\n1 2 3 4 5 6 7 8 9 10 – SP – 23 22 21 20 19 18",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Brief Table of Contents\n\nCopyright\n\nBrief Table of Contents\n\nTable of Contents\n\nPreface\n\nAcknowledgments\n\nAbout this book\n\nAbout the author\n\nAbout the cover illustration\n\n1. First Steps\n\nChapter 1. Testing microservices\n\nChapter 2. Taking mountebank for a test drive\n\n2. Using mountebank\n\nChapter 3. Testing using canned responses\n\nChapter 4. Using predicates to send different responses\n\nChapter 5. Adding record/replay behavior\n\nChapter 6. Programming mountebank\n\nChapter 7. Adding behaviors\n\nChapter 8. Protocols",
      "content_length": 515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "3. Closing the Loop\n\nChapter 9. Mountebank and continuous delivery\n\nChapter 10. Performance testing with mountebank\n\nIndex\n\nList of Figures\n\nList of Tables\n\nList of Listings",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\n\nCopyright\n\nBrief Table of Contents\n\nTable of Contents\n\nPreface\n\nAcknowledgments\n\nAbout this book\n\nAbout the author\n\nAbout the cover illustration\n\n1. First Steps\n\nChapter 1. Testing microservices\n\n1.1. A microservices refresher\n\n1.1.1. The path toward microservices\n\n1.1.2. Microservices and organizational structure\n\n1.2. The problem with end-to-end testing\n\n1.3. Understanding service virtualization\n\n1.3.1. Test-by-test setup using an API\n\n1.3.2. Using a persistent data store\n\n1.3.3. Record and replay",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "1.4. Introducing mountebank\n\n1.5. The service virtualization tool ecosystem\n\nSummary\n\nChapter 2. Taking mountebank for a test drive\n\n2.1. Setting up the example\n\n2.2. HTTP and mountebank: a primer\n\n2.3. Virtualizing the product catalog service\n\n2.4. Your first test\n\nSummary\n\n2. Using mountebank\n\nChapter 3. Testing using canned responses\n\n3.1. The basics of canned responses\n\n3.1.1. The default response\n\n3.1.2. Understanding how the default response works\n\n3.1.3. Changing the default response\n\n3.1.4. Cycling through responses\n\n3.2. HTTPS imposters\n\n3.2.1. Setting up a trusted HTTPS imposter\n\n3.2.2. Using mutual authentication\n\n3.3. Saving the responses in a configuration file",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "3.3.1. Saving multiple imposters in the config file\n\nSummary\n\nChapter 4. Using predicates to send different\n\nresponses\n\n4.1. The basics of predicates\n\n4.1.1. Types of predicates\n\n4.1.2. Matching object request fields\n\n4.1.3. The deepequals predicate\n\n4.1.4. Matching multivalued fields\n\n4.1.5. The exists predicate\n\n4.1.6. Conjunction junction\n\n4.1.7. A complete list of predicate types\n\n4.2. Parameterizing predicates\n\n4.2.1. Making case-sensitive predicates\n\n4.3. Using predicates on JSON values\n\n4.3.1. Using direct JSON predicates\n\n4.3.2. Selecting a JSON value with JSONPath\n\n4.4. Selecting XML values\n\nSummary\n\nChapter 5. Adding record/replay behavior",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "5.1. Setting up a proxy\n\n5.2. Generating the correct predicates\n\n5.2.1. Creating predicates with\n\npredicateGenerators\n\n5.2.2. Adding predicate parameters\n\n5.3. Capturing multiple responses for the same\n\nrequest\n\n5.4. Ways to replay a proxy\n\n5.5. Configuring the proxy\n\n5.5.1. Using mutual authentication\n\n5.5.2. Adding custom headers\n\n5.6. Proxy use cases\n\n5.6.1. Using a proxy as a fallback\n\n5.6.2. Converting HTTPS to HTTP\n\nSummary\n\nChapter 6. Programming mountebank\n\n6.1. Creating your own predicate\n\n6.2. Creating your own dynamic response\n\n6.2.1. Adding state\n\n6.2.2. Adding async\n\n6.2.3. Deciding between response vs. predicate injection",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "6.3. A word of caution: security matters\n\n6.4. Debugging tips\n\nSummary\n\nChapter 7. Adding behaviors\n\n7.1. Understanding behaviors\n\n7.2. Decorating a response\n\n7.2.1. Using the decorate function\n\n7.2.2. Adding decoration to saved proxy\n\nresponses\n\n7.2.3. Adding middleware through shellTransform\n\n7.3. Adding latency to a response\n\n7.4. Repeating a response multiple times\n\n7.5. Replacing content in the response\n\n7.5.1. Copying request data to the response\n\n7.5.2. Looking up data from an external data\n\nsource\n\n7.6. A complete list of behaviors\n\nSummary\n\nChapter 8. Protocols\n\n8.1. How protocols work in mountebank\n\n8.2. A TCP primer",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "8.3. Stubbing text-based TCP-based RPC\n\n8.3.1. Creating a basic TCP imposter\n\n8.3.2. Creating a TCP proxy\n\n8.3.3. Matching and manipulating an XML payload\n\n8.4. Binary support\n\n8.4.1. Using binary mode with Base64 encoding\n\n8.4.2. Using predicates in binary mode\n\n8.5. Virtualizing a .NET Remoting service\n\n8.5.1. Creating a simple .NET Remoting client\n\n8.5.2. Virtualizing the .NET Remoting server\n\n8.5.3. How to tell mountebank where the message ends\n\nSummary\n\n3. Closing the Loop\n\nChapter 9. Mountebank and continuous delivery\n\n9.1. A continuous delivery refresher\n\n9.1.1. Testing strategy for CD with microservices\n\n9.1.2. Mapping your testing strategy to a\n\ndeployment pipeline\n\n9.2. Creating a test pipeline",
      "content_length": 713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "9.2.1. Creating unit tests\n\n9.2.2. Creating service tests\n\n9.2.3. Balancing service virtualization with\n\ncontract tests\n\n9.2.4. Exploratory testing\n\nSummary\n\nChapter 10. Performance testing with mountebank\n\n10.1. Why service virtualization enables\n\nperformance testing\n\n10.2. Defining your scenarios\n\n10.3. Capturing the test data\n\n10.3.1. Capturing the responses\n\n10.3.2. Capturing the actual latencies\n\n10.3.3. Simulating wild latency swings\n\n10.4. Running the performance tests\n\n10.5. Scaling mountebank\n\nSummary\n\nIndex\n\nList of Figures\n\nList of Tables\n\nList of Listings",
      "content_length": 573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Preface\n\nPete Hodgson used to joke that building your own\n\nmocking framework was a rite of passage for ThoughtWorks developers. Those days are past, not\n\nbecause ThoughtWorks doesn’t care about testing anymore (we do, very much), but because the tooling\n\naround testing is a lot better, and because we have more interesting problems to focus on nowadays. In the winter\n\nof 2014, I had a testing problem, and it turns out that not being able to test prevents you from focusing on those\n\nmore interesting problems.\n\nWe had adopted a microservices architecture but were limited by some of the legacy service code we were\n\nstrangling out. The idea of service virtualization—using tests to mock up downstream network dependencies—\n\nwas not new to us, even if the term wasn’t common in the open source world. It seemed like every time a new\n\ndeveloper joined the team, they recommended using VCR (a Ruby tool) or WireMock (a Java tool) to solve the\n\nproblem. Those are great tools, and they are in great company. By that time, ThoughtWorkers had\n\ncontributed a couple more quality tools to the mix (stubby4j and Moco), and tools like Hoverfly would be\n\ncoming soon. You wouldn’t have been wrong to pick any of them if you needed to virtualize HTTP services.\n\nUnfortunately, our downstream service was not HTTP.\n\nThe way new team members kept suggesting the same type of tooling provided good evidence that the approach\n\nworked. The fact that without such tooling, we were unable to gain the confidence we needed in our tests\n\nwhich provided the need for mountebank. Over that winter break, buoyed by red wine and Jon Stewart jokes,\n\nI wrote mountebank.",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "This book is about mountebank, but it is also about testing and about continuous delivery and about\n\nmicroservices and architecture. You may be able to do your job without mountebank, but if you run into\n\ntrouble, service virtualization may just be able to help you focus on the more interesting problems.",
      "content_length": 305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Acknowledgments\n\nThe advantage of working for a company like\n\nThoughtWorks is that a lot of people I know have written technical books. The disadvantage is that every single\n\none of them told me that writing a book is, like, really hard. (Thanks Martin Fowler, Neal Ford, and Mike\n\nMason.)\n\nFortunately, Pete Hodgson wasn’t one of them. He has, however, written several articles, including one on\n\ntesting JavaScript on Martin Fowler’s bliki. I didn’t really understand it the first 10 times I read it, and, not being a\n\nJavaScript developer myself, tried to implement a synchronous promise library based on a naive\n\ninterpretation of promises. A few weeks later, when I untangled my frontal cortex from my corpus callosum\n\nbefore realizing what a Bad Idea trying to write my own promise library was, I asked Pete for help. He provided\n\nthe first contribution to mountebank, showing me how to actually test JavaScript. I felt that was a good thing to\n\nknow because I was writing a testing tool. Thanks, Pete.\n\nPaul Hammant was another one who never told me how\n\nhard writing a book is. Unfortunately, he also never bothered to tell me how hard managing a popular open source project is. A long-time open sourcer himself (he\n\nkicked off Selenium, one of the early inversions of control frameworks, and a host of other initiatives), he\n\nlikely assumed everyone suffered from the same desire to leave work every night to code and blog and manage a\n\ncommunity like he does. Nonetheless, Paul has been both an incredibly helpful promoter of mountebank and an\n\nincredibly helpful mentor, even if I don’t always do a good job of listening to him.",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Of course, none of this would have been possible without the support of that first team, named SilverHawks after\n\nsome cartoon that, despite being age-appropriate for me, I never watched. I owe my thanks to Shyam Chary, Bear\n\nClaw, Gonzo, Andrew Hadley, Sarah Hutchins, Nagesh Kumar, Stan Guillory, and many others. The\n\nmountebank community has grown from those humble beginnings, and I owe a debt of gratitude to all the many\n\nfolks who have dedicated their free time to improving the product.\n\nI was in Oklahoma City when Manning called to suggest writing a book. It was, like, really hard. Elesha Hyde, my\n\ndevelopmental editor, was amazing, even when I’d go silent for weeks at a time because work and life and kids\n\ndidn’t wait. I wrote this book in Oklahoma, Dallas, Toronto, Vancouver, San Francisco, San Antonio, Houston, Austin, São Paulo, and Playa del Carmen.\n\nThat’s right—I wrote parts of this book while drinking mojitos on the beach (and chapter 4 is better off for it).\n\nWhich brings me to Mona. You let me write on weekends and on vacations. You let me write at family events and\n\nwhen your damn Patriots were playing in another damn Super Bowl (or whatever they call that last game, I don’t\n\nfollow baseball that much anymore). You let me write at the spa and at the pool while you kept our kids from\n\ndrowning. Thank you.",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "About this book\n\nI wrote Testing Microservices with Mountebank to show\n\nhow service virtualization helps you test microservices, and how mountebank is a powerful service virtualization\n\ntool. That necessarily requires building a deep familiarity with mountebank—the middle section of this book is\n\ndedicated to the subject—but many of the lessons apply with any service virtualization tool.\n\nWHO SHOULD READ THIS BOOK\n\nMountebank is a developer-friendly tool, which makes\n\ndevelopers a primary audience of Testing Microservices with Mountebank. Some familiarity with test automation\n\nis expected, but I avoid using any advanced language features throughout this book to keep the focus on the\n\ntool and the approach. Automation-friendly QA testers also will find value in this book, as will those who\n\nspecialize in performance testing. Finally, service virtualization is increasingly an architectural concern,\n\nand within these pages I hope to arm solution architects with the arguments they need to make the right\n\ndecisions.\n\nHOW THIS BOOK IS ORGANIZED\n\nYou’ll find three parts and 10 chapters in this book.\n\nPart 1 introduces the overall testing philosophy of distributed\n\nsystems.\n\nChapter 1 provides a brief refresher of microservices, as\n\nwell as a critique of traditional end-to-end testing. It\n\nhelps explain how service virtualization fits in a\n\nmicroservices world and provides a mental model for\n\nmountebank.\n\nChapter 2 sets up an example architecture that we will\n\nrevisit a few times throughout the book and shows how",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "you can use mountebank to automate deterministic tests\n\ndespite a distributed architecture.\n\nPart 2 deep dives into mountebank, giving you a comprehensive\n\noverview of its capabilities.\n\nChapter 3 provides foundational material for\n\nunderstanding basic mountebank responses in the\n\ncontext of HTTP and HTTPS. It also describes basic ways\n\nof managing test data through configuration files.\n\nChapter 4 explores predicates—mountebank’s way of\n\nresponding differently to different types of requests. It also introduces mountebank’s capability around\n\nmatching XML and JSON.\n\nChapter 5 looks at mountebank’s record and replay capability. Mountebank uses proxies to real systems to\n\nlet you capture realistic test data.\n\nChapter 6 shows you how to program mountebank itself,\n\nby using a feature called injection to write your own\n\npredicates and responses in JavaScript. We look at how\n\ninjection helps solve some thorny problems around\n\nCORS and OAuth handshakes, including virtualizing\n\nGitHub’s public API.\n\nChapter 7 rounds out the core capabilities of the\n\nmountebank engine by looking at behaviors—\n\npostprocessing steps applied to responses. Behaviors let\n\nyou add latency, look up data from external sources, and\n\nperform a host of other transformation steps.\n\nChapter 8 shows how all of the concepts in chapters 3–7\n\nextend beyond HTTPS. The engine of mountebank is protocol-agnostic, and we show TCP-based examples,\n\nincluding an extended .NET Remoting test scenario.\n\nPart 3 takes a step back to put service virtualization in a broader context.\n\nChapter 9 explores an example test pipeline for\n\nmicroservices, from unit tests to manual exploratory\n\ntests, and shows where service virtualization does and\n\ndoesn’t fit.\n\nChapter 10 shows how service virtualization helps with\n\nperformance testing. It includes a fully worked out\n\nexample virtualizing a publicly available API.\n\nABOUT THE CODE\n\nThis book uses a number of code examples to help\n\nillustrate the concepts. Some of them are whimsical (see chapter 4), some are based on virtualizing real public APIs",
      "content_length": 2064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "(see chapters 6 and 10), and some are just downright gnarly (see chapter 8). I tried my best to keep the\n\nexamples interesting across the wide range of problems that service virtualization can help with. This was no easy\n\ntask. Some problems are easily understood. Some, like virtualizing a .NET Remoting service returning binary\n\ndata, are not. My hope is that I’ve maintained enough of a sense of humor to keep you interested with the easy\n\nproblems, and with the complex behavior, I’ve given you enough of a sense of what’s possible that you’ll be\n\ncapable of innovating on your own.\n\nSource code for the book is publicly available at\n\nhttps://github.com/bbyars/mountebank-in-action.\n\nBOOK FORUM\n\nPurchase of Testing Microservices with Mountebank includes free access to a private web forum run by\n\nManning Publications where you can make comments about the book, ask technical questions, and receive help\n\nfrom the author and other users. To access the forum go to www.manning.com/books/testing-microservices-with-\n\nmountebank. You can also learn more about Manning’s forums and the rules of conduct at\n\nhttps://forums.manning.com/forums/about.\n\nManning’s commitment to its readers is to provide a venue where a meaningful dialog between individual\n\nreaders and between readers and the author can take place. It is not a commitment to any specific amount of\n\nparticipation on the part of the author, whose contributions to the forum remain voluntary (and\n\nunpaid). We suggest you ask the author challenging questions, lest his interest stray.\n\nONLINE RESOURCES\n\nNeed additional help?",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "The mountebank website at https://www.mbtest.org provides the most up-to-date documentation for the tool.\n\nThe Google group site is https://groups.google.com/forum/#!forum/mounte bank-\n\ndiscuss. Feel free to ask any questions about the tool there.",
      "content_length": 247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "About the author\n\nBRANDON BYARS is a principal consultant at\n\nThoughtWorks and the creator and maintainer of\n\nmountebank. He has 20 years of experience in IT, with roles ranging from developer to DBA to architect to\n\naccount manager. When he is not geeking out over test automation, he has an interest in applying systems thinking to large-scale development and in finding ways\n\nto rediscover what it means to be human in a world where we have opened Pandora’s technobox.",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Part 1. First Steps\n\nWelcome, friend.\n\nThose words adorn the welcome mat near my front door, and they are the first words you will see when you visit\n\nthe mountebank website (https://www.mbtest.org). I’d very much like them to be the first words you read in this\n\nbook, as I welcome you to the wonderful world of service virtualization in general and mountebank in particular.\n\nThis first part aims to provide context behind that\n\nintroduction, setting the stage for introducing mountebank as part of your testing and continuous\n\ndelivery stack. Because one of the main drivers behind service virtualization is the increasingly distributed\n\nnature of computing, chapter 1 starts with a brief review of microservices, with a focus on how they change the\n\nway we test software. It puts service virtualization in context and provides a gentle introduction to the main\n\ncomponents of mountebank.\n\nChapter 2 demonstrates mountebank in action. You’ll get some dirt under your nails as you write your first test\n\nusing service virtualization, providing a simple launching point to explore the full capabilities of mountebank in\n\npart 2.",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 1. Testing microservices\n\nThis chapter covers\n\nA brief background on microservices\n\nThe challenges of testing microservices\n\nHow service virtualization makes testing easier\n\nAn introduction to mountebank\n\nSometimes, it pays to be fake.\n\nI started developing software in the days when the web\n\nwas starting to compete with desktop applications in corporate organizations. Browser-based applications\n\nbrought tremendous deployment advantages, but we tested them in almost the same way. We wrote a\n\nmonolithic application, connected it to a database, and tested exactly like our users would: by using the\n\napplication. We tested a real application.\n\nTest-driven development taught us that good object-\n\noriented design would allow us to test at much more granular levels. We could test classes and methods in\n\nisolation and get feedback in rapid iterations. Dependency injection—passing in the dependencies of a\n\nclass rather than instantiating them on demand—made our code both more flexible and more testable. As long as we passed in test dependencies that had the same\n\ninterface as the real ones, we could completely isolate the bits of code we wanted to test. We gained more\n\nconfidence in the code we wrote by being able to inject fake dependencies into it.\n\nBefore long, clever developers produced open-source libraries that made creating these fake dependencies\n\neasier, freeing us to argue about more important things, like what to call them. We formed cliques based on our",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "testing styles: the mockists reveled in the purity of using mocks; the classicists proudly stood by their stubborn\n\n[1]\n\nreliance on stubs. fundamental value of testing against fake dependencies.\n\nBut neither side argued about the\n\n1\n\nYou probably have better things to spend your time on than reading about the differences between classicists and mockists, but if you can’t help yourself, you can read more at http://martinfowler.com/articles/mocksArentStubs.html.\n\nIt turns out when it comes to design, what’s true in the\n\nsmall is also true in the large. After we made a few halting attempts at distributed programming, the ever-\n\nversatile web gave us a convenient application protocol— HTTP—for clients and servers to talk to each other. From proprietary RPC to SOAP to REST and back again to\n\nproprietary RPC, our architectures outgrew the single codebase, and we once again needed to find ways to test\n\nentire services without getting tangled in their web of runtime dependencies. The fact that most applications\n\nwere built to retrieve the URLs for dependent services from some sort of configuration that varied per\n\nenvironment meant dependency injection was built in. All we needed to do was configure our application with\n\nURLs of fake services and find easier ways to create those fake services.\n\nMountebank creates fake services, and it’s tailor-made for testing microservices.\n\n1.1. A MICROSERVICES REFRESHER\n\nMost applications are written as monoliths, a coarse-\n\ngrained chunk of code that you release together with a shared database. Think of an e-commerce site like\n\nAmazon.com. A common use case is to allow a customer to see a history of their orders, including the products\n\nthey have purchased. This is conceptually easy to do as long as you keep everything in the same database.",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "In fact, Amazon did this in their early years, and for good reason. The company had a monolithic codebase they\n\ncalled “Obidos” that looked quite similar to figure 1.1. Configuring the database that way makes it easy to join\n\ndifferent domain entities, such as customers and orders to show a customer’s order history or orders and\n\nproducts to show product details on an order. Having everything in one database also means you can rely on\n\ntransactions to maintain consistency, which makes it easy to update a product’s inventory when you ship an\n\norder, for example.\n\nFigure 1.1. A monolithic application handles view, business, and persistence logic for multiple domains.\n\nThis setup also makes testing—the focus of this book— easier. Most of the tests can be in process, and, assuming\n\nyou are using dependency injection, you can test pieces in isolation using mocking libraries. Black-box testing\n\nthe application only requires you to coordinate the application deployment with the database schema\n\nversion. Test data management comes down to loading the database with a set of sample test data. You can\n\neasily solve all of these problems.\n\n1.1.1. The path toward microservices\n\nIt’s useful to follow the history of Amazon.com to understand what compels organizations to move away\n\nfrom monolithic applications. As the site became more",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "popular, it also became bigger, and Amazon had to hire more engineers to develop it. The problems started when\n\nthe development organization was large enough that multiple teams had to develop different parts of Obidos\n\n(figure 1.2).\n\nFigure 1.2. Scaling a monolith means multiple teams have to work in the same codebase.\n\nThe breaking point came in 2001, as the company\n\nstruggled to evolve pieces of the application because of the coupling between teams. By CEO mandate, the\n\nengineering organization split Obidos into a series of services and organized its teams around them.\n\n[2]\n\nAfter\n\nthe transformation, each team was able to change the code relevant to the domain of their service with much\n\nhigher confidence that they weren’t breaking other teams’ code—no other team shared their codebase. Amazon now has tremendous ability to develop different\n\nparts of the website experience independently, but the transformation has required a change of paradigm.\n\nWhereas Obidos used to be solely responsible for",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "rendering the site, nowadays a single web page at Amazon.com can generate over a hundred service calls\n\n(figure 1.3).\n\n2\n\nSee https://queue.acm.org/detail.cfm?id=1142065 for details.\n\nFigure 1.3. Services use diﬀerent databases for diﬀerent domains.\n\nThe upshot is that each service can focus on doing one\n\nthing well and is much easier to understand in isolation. The downside is that such an architecture pushes the\n\ncomplexity that used to exist inside the application into the operational and runtime environment. Showing both\n\ncustomer details and order details on a single screen changes from being a simple database join to",
      "content_length": 630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "orchestrating multiple service calls and combining the data in the application code. Although each service is\n\nsimple in isolation, the system as a whole is harder to understand.\n\nNetflix was one of the first companies of its size to migrate its core business to Amazon’s cloud services,\n\nwhich significantly influenced the way the company thought about services. Once again, the need to scale its\n\ndevelopment efforts is what drove this change. Adrian Cockcroft, formerly the Netflix lead cloud architect,\n\n[3]\n\nnoted two opposing tensions. increased by a factor of 1,000 in recent years as\n\nFirst, demands on IT had\n\ntechnology moved from managing payroll and a few enterprise services to becoming the core differentiator\n\nfor digitally native companies. Second, as the number of engineers increased, the communication and coordination overhead of activities like troubleshooting a\n\nbroken build became a significant slowdown to delivering software.\n\n3\n\nSee https://www.infoq.com/presentations/migration-cloud-native.\n\nNetflix experienced this slowdown once the organization grew to about 100 engineers, with multiple teams\n\ncompeting in the same codebase. Like Amazon, the company solved the problem by breaking the monolithic\n\napplication into services, and it made a conscious effort to make each service do only one thing. This\n\narchitectural approach—what we now call microservices —supported development teams working independently\n\nof each other, allowing the company to scale its development organization.\n\nAlthough scaling development efforts is the primary\n\nforce leading us toward microservices, it has a welcome side effect: it’s much easier to release a small service than",
      "content_length": 1687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "it is to release a large application. If you can release services independently, you can release features to the\n\nmarket much more quickly. By removing the need for release coordination between teams, microservices\n\nprovide an architectural solution for what would normally have to be solved manually. Customers are the\n\nbeneficiaries. Both Amazon and Netflix are known for the ability to rapidly innovate in the marketplace but to\n\ndo so required rethinking how they organized to deliver software and how they tested software.\n\n1.1.2. Microservices and organizational structure\n\nBefore we get into how mountebank makes testing microservices easier, you need to understand how\n\nmicroservices require a different testing mentality. It all starts with team organization and ends with a full-frontal\n\nassault on traditional QA approaches that gate releases through coordinated end-to-end testing.\n\nMicroservices require you to rethink traditional\n\norganizational structures. Silos exist in any large organization, but some silos are anathema to the goal of\n\nmicroservices, which is to allow independent releases of small services with minimal coordination. Traditional\n\norganizations use certain silos as “gates” that validate a deployment before it’s released. Each gate acts as a point\n\nof coordination. Coordination is one way to gain confidence in a release, but it’s slow and couples teams\n\ntogether. Microservices work best when you silo the organization by business capability and allow the\n\ndevelopment team to own the release of its code autonomously.\n\nA useful metaphor to explain the concept is to imagine\n\nyour IT organization as a highway. When you need to increase throughput—the number of features released\n\nover a given timeframe—you do so by adding lanes to the highway. Having more lanes means you can support",
      "content_length": 1824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "more cars at the same time (figure 1.4). This is analogous to hiring more developers to do more work in parallel.\n\nYou also may want to be able to release a single feature more quickly. This is equivalent to raising the speed limit\n\non the highway, enabling a single car to get from point A to point B in less time. So far, so good.\n\nFigure 1.4. During normal traﬀic, the number of lanes and speed limit define throughput and velocity.\n\nOne thing will kill both throughput and velocity, no\n\nmatter how many lanes you have or what the speed limit is: congestion. Congestion also has an indirect cost that\n\nyou have almost certainly experienced if you live in a big city. Navigating through stop-and-go traffic is a soul-\n\ncrushing experience. It’s demotivating. It’s hard to get excited to get in your car and drive. Many large IT\n\norganizations that, with the best of intentions, create unintended congestion, suffer from a real motivational\n\ncost.\n\nThey create congestion in two ways. First, they overuse the highway, having too many cars given the space\n\navailable. Second, they add coordination that creates congestion. This second way is harder to eradicate.\n\nOne way to require coordination on the highway is to add\n\na toll gate (especially those old-timey ones, before an automated camera replaced the need to pay physical\n\nmoney to a real person). Another way is to have fewer",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "upstream lanes than you have downstream, where “upstream” refers to the section of the highway closer to\n\nthe exit in highways, and closer to a production release in IT. Reducing the number of upstream lanes throttles\n\ntraffic by requiring merging multiple lanes into one (figure 1.5). Sometimes this reduction happens by design\n\nor because of road construction. Other times it happens because of an accident, which results from an\n\nunfortunate degree of coupling between two cars.\n\nFigure 1.5. Having fewer upstream lanes increases congestion.\n\nLike all models, the highway metaphor is imperfect, but\n\nit highlights some useful points. As you saw earlier in the Amazon and Netflix examples, microservices often\n\noriginate from an organization’s desire to increase feature throughput. A helpful side effect is that a smaller codebase has a higher speed limit, increasing velocity.\n\nBut both of these advantages are negated if you don’t change the organization to remove congestion.\n\nIn organizational terms, fixing overutilization is easy in principle, though it’s often politically challenging. You\n\ncan either hire more people (add more lanes to the highway) or reduce the amount of work in progress\n\n(throttle entry to the highway).",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "The other reasons for congestion are harder to fix. It’s common to see organizations with less upstream capacity\n\nthan downstream capacity. One toll gate example would be increasing coordination by requiring releases to go\n\nthrough a central release management team that throttles development throughput. Accidents are even\n\nmore common. Every time someone discovers a bug or the build breaks in a codebase that multiple teams share,\n\nyou have an accident requiring coordination, and both throughput and velocity suffer. Adrian Cockcroft cited\n\nthis exact reason for driving Netflix toward microservices.\n\nMicroservices provide a technical solution for reducing congestion caused by accidents. By not sharing the same\n\ncodebase, broken builds don’t affect multiple teams, effectively creating different lanes for different services. But toll gates are still a problem, and to fully unlock the\n\nthroughput and velocity advantages we hope to gain through microservices, we have to address organizational\n\ncomplexity. That comes in many forms (for example, from operations to database schema management), but\n\nthere’s one form of upstream congestion that’s particularly relevant to this book: our QA organization.\n\nMicroservices fundamentally challenge the way you test.\n\n1.2. THE PROBLEM WITH END-TO-END TESTING\n\nTraditionally, a central QA team could partner with a\n\ncentral release management team to coordinate a schedule of changes that needed to be deployed into\n\nproduction. They could arrange it such that only one change went through at a time, and that change could be\n\ntested against the production versions of its runtime dependencies before being released. Such an approach is perfectly reasonable up to a point. Beyond that—and this\n\nis often where organizations turn to microservices—it’s inappropriate.",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "You still need confidence that the entire system will work when you release part of it. Gaining that confidence\n\nthrough traditional end-to-end testing and coordinated releases couples all of the services together, moving the\n\ncoupling bottleneck from the development organization to the QA organization (figure 1.6).\n\nFigure 1.6. Centralized QA processes recouple releases together, causing a bottleneck.\n\nCoordinated integration testing between services recouples codebases that have been decoupled through\n\nservice decomposition, destroying the scaling and rapid delivery advantages of microservices. As soon as you\n\ncouple the releases of services together, you have reintroduced the communication and coordination\n\noverhead you were trying to avoid. It doesn’t matter how many services you have; when you have to release them at the same time, you have a monolith.\n\nThe only way to truly scale the technology organization is to decouple releases, so that it can deploy a service\n\nindependently of the service’s dependencies (figure 1.7). This requires a fundamental rethinking of the test\n\nstrategy for microservices. End-to-end testing doesn’t completely disappear, but relying on it as a gate to\n\nreleasing software becomes another sacrifice on the path toward microservices. The question remains: how do you\n\ngain the confidence you need in your changes before releasing them?",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Figure 1.7. Independent testing works to avoid release congestion.\n\n1.3. UNDERSTANDING SERVICE VIRTUALIZATION\n\nThe problem with dependencies is that you can’t depend on them.\n\nMichael Nygard, “Architecture Without an End State”\n\nAdditional problems exist besides coordination, as\n\nshown in figure 1.8. Running in a shared environment means tests may pass or fail for reasons that have\n\nnothing to do with either the service you are testing or the tests themselves. They could fail because of resource\n\ncontention with other teams who are touching the same data, overwhelming the server resources of a shared\n\nservice, or environmental instability. They could fail, or be nearly impossible to write to begin with, because of an\n\ninability to get consistent test data set up in all of the services.\n\nFigure 1.8. End-to-end testing introduces several problems of coordination.",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "It turns out that other industries have already solved this\n\nproblem. A car, for example, is made up of a multitude of components, each of which can be released to the market\n\nindependently of the car as a whole. By and large, nobody buys an alternator or a flywheel for anything\n\nother than to fix a car. All the same, it’s common for companies to manufacture and sell those parts\n\nseparately even though they have never tested them in your specific car.\n\nA car battery comes standard with negative and positive\n\nterminals. You can test the battery—outside the car—by using a voltmeter attached to those two terminals and\n\nverifying that the voltage is between 12.4 and 12.7 volts.",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "When you start the car, the alternator is responsible for charging the battery, but you can verify the behavior of\n\nthe battery independently of the alternator by providing a current as input to the battery and measuring the\n\nvoltage. Such a test tells you that, if the alternator is behaving correctly, then the battery also behaves\n\ncorrectly. You can gain most of the confidence you need to verify the battery is working by using a fake alternator.\n\nService virtualization involves nothing more than using test doubles that operate over the wire and is analogous\n\nto how you test car batteries without a real alternator. You silo the runtime environment into the bits relevant\n\nto test a single service or application and fake the rest, assuming standard interfaces. In traditional mocking\n\nand stubbing libraries, you would stub out a dependency and inject that into your object’s constructor, allowing your tests to probe the object under test in isolation.\n\nWith service virtualization, you virtualize a service and configure the service under test to use the virtualized\n\nservice’s URL as a runtime dependency (figure 1.9). You can set up the virtual service with a specific set of canned\n\nresponses, allowing you to probe the service under test in isolation.\n\nFigure 1.9. Testing using service virtualization",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Service virtualization lets you do black box testing of the\n\nservice while tightly controlling the runtime environment in which it operates. Although it falls short of the end-to-\n\nend confidence that integration tests give you, it does make testing much easier. If you need to test what your\n\nshopping cart will do if you try to submit the order when you are out of inventory, you don’t have to figure out how\n\nto change the inventory system to run your test. You can virtualize the inventory service and configure it to\n\nrespond with an out-of-inventory message. You can take full advantage of the reduced coupling that the narrow\n\nservice interface provides to dramatically reduce the amount of test setup required.\n\nDeterministic tests are tests that always pass or fail when\n\ngiven the same code to test. Nondeterminism is a tester’s worst enemy. Every time you try to “fix” a broken test by\n\nre-running it because it worked last time, the devil on your shoulder does a little dance while the angel on your\n\nother shoulder lets out a big sigh. Automated tests create a social contract for a team: when a test breaks, you fix\n\nthe code. When you allow flaky tests, you chip away at that social contract. All kinds of bad behavior might",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "occur when teams lose confidence that their tests are giving them meaningful feedback, including completely\n\nignoring the output of a build.\n\nFor your tests to run deterministically, you need to\n\ncontrol what the virtual service returns. You can seed the response in several ways, depending on the type and\n\ncomplexity of the tests. What works for writing automated behavioral tests against your service and for\n\ntesting edge cases likely won’t work when running performance tests where you need to execute thousands\n\nof requests against the virtual service.\n\n1.3.1. Test-by-test setup using an API\n\nThe simplest approach is to mirror what mocking libraries do: directly collude with the mocked objects.\n\nThe unit testing community often speaks about the 3A pattern, which is to say that each test has three\n\ncomponents: arrange, act, and assert. First you set up the data needed for the test to run (arrange), then you\n\nexecute the system under test (act), and finally you assert that you got the expected response (figure 1.10). Service\n\nvirtualization can support this approach through an API that lets you configure the virtual service dynamically.\n\nFigure 1.10. Service virtualization supports a standard unit testing pattern.",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "This approach supports creating a laboratory environment for each test, in order to strictly control the\n\ninputs and dependencies of the service under test. However, it does make a couple of fundamental\n\nassumptions. First, it expects each test to start with a clean slate, which means that each test must remove the\n\ntest data it added to prevent that data from interfering with subsequent tests. Second, the approach doesn’t\n\nwork if multiple tests are run in parallel. Both of these assumptions fit nicely into automated behavioral tests, as\n\ntest runners typically ensure tests are run serially. As long as each developer runs their own virtual service, you\n\ncan avoid the resource contention that comes with concurrent test runs.\n\n1.3.2. Using a persistent data store\n\nCreating test data test-by-test doesn’t work well for manual testers, it doesn’t work if multiple testers are hitting the same virtual service, and it doesn’t work in\n\nsituations (like performance testing) where you need a large batch of data. To address these concerns, you can\n\nconfigure the virtual service to read the test data from a persistent store. With the test-by-test approach to test\n\ndata creation, all you have to do is tell the virtual service what the next response should look like. With a data\n\nstore, you will need some way of deciding which response to send based on something from the request.\n\nFor example, you might send back different responses based on identifiers in the request URL (figure 1.11).\n\nFigure 1.11. Using persistent test data from a data store",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "The downside of this approach is that the arrange step is\n\nremoved from the test, meaning that to understand what each test is trying to do, you need some information that\n\nit doesn’t directly specify. If you are testing what happens when you submit orders under various\n\nscenarios, for example, you’d have to know that order 123 should have appropriate inventory, whereas order\n\n234 should experience an out-of-inventory situation. The configuration that sets that up is in a data store instead\n\nof in the arrange section of your test.\n\n1.3.3. Record and replay\n\nOnce you have determined where to store the test data, the next question is how to create it. This is rarely an\n\nissue for automated behavioral tests because you would create the data specific to the testing scenario. But if you\n\nare using a persistent data store, creating the test data is often a significant challenge, especially when you want\n\nlarge quantities of realistic data. The solution is often to record interactions with the real dependency in a way",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "that allows you to play them back through a virtual service (figure 1.12).\n\nFigure 1.12. Capturing real traﬀic for later replay\n\nThe trick with recording responses is that you still have to specify some condition on the request that has to\n\nmatch before playing back the recorded response. You need to know that the order identifier in the URL is",
      "content_length": 346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "what’s used to separate the successful order submit response from the out-of-inventory response.\n\nService virtualization isn’t a silver bullet, and by itself it’s not enough to cover the confidence gap created by giving\n\nup end-to-end testing. But it’s a critical component of a modern test strategy in a distributed world, and we’ll\n\nexplore ways of closing that confidence gap in chapters 9 and 10, when we combine service virtualization with\n\nother techniques to create a continuous delivery pipeline.\n\nService virtualization isn’t just for microservices!\n\nAlthough the focus here is on microservices and the\n\nchanges in test strategy they require, service virtualization is a useful tool in many other contexts. A\n\ncommon use case is mobile development, where the mobile team needs to be able to develop independently of the team building the API. The need to compete in the\n\nmobile ecosystem has driven many organizations to change their integration approach to one based on\n\nHTTP-based APIs, and mobile developers can take advantage of that fact to virtualize the APIs as they\n\ndevelop the front-end code.\n\n1.4. INTRODUCING MOUNTEBANK\n\nMountebank means a charlatan. It comes from Italian words meaning, literally, to mount a bench, which\n\ncaptures the behavior of snake oil salesmen who duped uninformed consumers into forking over money for\n\nquack medicine. It’s a useful word for describing what mountebank the tool does, which is to conspire with\n\n[4]\n\nyour tests to dupe the system under test into believing that mountebank is a real runtime dependency.\n\n4",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Since its initial release, I have always preferred to lowercase the “m” in “mountebank” when writing about the tool. Largely this has to do with the way the documentation is written—in a personified voice from a snake oil salesman who claims false humility by, among other things, not capitalizing his name. Whatever the historical origins, it’s now a mildly\n\nunexpected stylistic twist. Sorry about that.\n\nMountebank is a service virtualization tool. It supports all of the service virtualization scenarios we have looked\n\nat: behavioral testing using the API test-by-test, using a persistent data store, and acting as a proxy for record\n\nand replay situations. Mountebank also supports the ability to pick a response based on certain criteria of the\n\nrequest and to select which request fields you want to differentiate the responses during the playback stage of\n\nrecord-playback. Most of the remainder of this book explores those scenarios and more to help you build a\n\nrobust test strategy for microservices, because a robust test strategy is the key to unlocking release\n\nindependence.\n\nMountebank is a standalone application that provides a REST API to create and configure virtual services, which\n\nare called imposters in the API (figure 1.13). Rather than configuring the service that you are testing to point to\n\nURLs of real services, you configure it to point to the imposters you create through mountebank.\n\nFigure 1.13. Configuring virtual services with a simple mountebank imposter",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Each imposter represents a socket that acts as the virtual\n\nservice and accepts connections from the real service you are testing. Spinning up and shutting down imposters is\n\na lightweight operation, so it’s common to use the arrange step of automated tests to create the imposter,\n\nthen shut it down in the cleanup stage of each test. Although we will use HTTP/S for most examples in this\n\nbook, mountebank supports other protocols, including binary messages over TCP, and more protocols are\n\nexpected soon.",
      "content_length": 508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "As Larry Wall, the creator of the Perl scripting language, once said, the goal of a good tool is to make the easy\n\n[5]\n\nthings easy and the hard things possible. tries to accomplish this with a rich set of request-\n\nMountebank\n\nmatching and response-generation capabilities, balanced by as many defaults as reasonably possible. Figure 1.14\n\nshows how mountebank matches a request to a response.\n\n5\n\nMost developers who have had the misfortune of using many “enterprise” tools will realize\n\nthat this statement is far from the truism it may sound like.\n\nFigure 1.14. Matching a request to a response with mountebank\n\nNetwork protocols are complicated beasts. The first job\n\nof the imposter is to simplify a protocol-specific request",
      "content_length": 731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "into a JSON structure so that you can match the request against a set of predicates. Each protocol gets its own\n\nrequest JSON structure; we will look at HTTP’s structure in the next chapter.\n\nYou configure each imposter with a list of stubs. A stub is nothing more than a collection of one or more responses\n\nand, optionally, a list of predicates. Predicates are defined in terms of request fields, and each one says\n\nsomething like “Request field X must equal 123.” No self- respecting mocking tool would leave users with a simple\n\nequals as the only comparison operator, and mountebank ups the game with special predicate\n\nextensions to make working with XML and JSON easier. Chapter 4 explores predicates in detail.\n\nMountebank passes the request to each stub in list order\n\nand picks the first one that matches all the predicates. If the request doesn’t match any of the stubs defined,\n\nmountebank returns a default response. Otherwise, it returns the first response for the stub, which brings us to\n\nhow responses are generated (figure 1.15).\n\nFigure 1.15. Response generation in mountebank using predicates and responses in stubs\n\nThe first thing that happens is that the selected response shifts to the back of the list. This allows you to cycle",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "through the responses in order each time a request matches the stub’s predicates. Because mountebank\n\nmoves the request to the back instead of removing it, you never run out of responses—you can start reusing them.\n\nThis data structure is what the academics call a circular buffer, because a circle prefers to start over rather than\n\nend.\n\nThe response resolver box in figure 1.15 is a bit of a\n\nsimplification. Each response is responsible for generating a JSON structure representing the protocol-\n\nspecific response fields (like the HTTP status code), and you can generate those fields in different ways.\n\nMountebank has three different response types that take entirely different approaches to generating the JSON:\n\nAn is response type returns the provided JSON as-is, creating a\n\ncanned response. We explore canned responses in chapter 3.\n\nA proxy response type forwards the request on to a real\n\ndependency and converts its response into a JSON response\n\nstructure. You use proxies for record-playback functionality, and\n\nwe describe them in chapter 5.\n\nAn inject response type allows you to programmatically define the\n\nresponse JSON using JavaScript. Injection is how you can extend\n\nmountebank when its built-in capabilities don’t quite do what you\n\nneed, and we cover that in chapter 6.\n\nOnce the response is resolved, mountebank passes the JSON structure to behaviors for post-processing.\n\nBehaviors, which we discuss in chapter 7, include, among others:\n\nCopying values from the request into the response\n\nAdding latency to the response\n\nRepeating a response, rather than moving it to the back of the list\n\nUp to this point, mountebank has dealt only with JSON,\n\nand every operation (with the exception of forwarding a proxy request) has been protocol-agnostic. Once the\n\nresponse JSON is finalized, the imposter converts the JSON to a protocol-aware network response and sends it",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "over the wire. Although we will spend much of our time in this book looking at HTTP requests and responses, all\n\nof the core capabilities of mountebank work with any supported network protocol (even binary ones), and in\n\nchapter 8, we will show some non-HTTP examples.\n\nTo keep simple things simple, nearly everything in\n\nmountebank is optional. That allows you to get started gently, which we will do in the next chapter.\n\n1.5. THE SERVICE VIRTUALIZATION TOOL ECOSYSTEM\n\nThis book is about two things: mountebank and how\n\nservice virtualization fits into your microservices test strategy. Although both topics are valuable, the second\n\none is much broader than mountebank.\n\nThe service virtualization ecosystem offers several quality tools, both open source and commercial.\n\nCommercial tooling is still quite popular in large enterprises. HP, CA, and Parasoft all offer commercial\n\nservice virtualization tools, and SmartBear took the (originally noncommercial) SoapUI and converted it into\n\npart of their commercial service virtualization toolkit. Many of the commercial tools are high quality and offer a\n\nricher set of capabilities than the open source tooling, such as broader protocol support, but in my experience,\n\nthey both devalue the developer experience and hinder true continuous delivery. (Chapter 9 offers a fuller\n\ncritique.) Of the open source tools, I believe that mountebank comes the closest to the full feature set of\n\ncommercial tools.\n\nThe open source tooling offers a rich set of options primarily aimed at virtualizing HTTP. WireMock is\n\nprobably the most popular alternative to mountebank. Whereas mountebank aims to be cross-platform by\n\nhaving its public API be REST over HTTP, WireMock",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "(and many others) optimizes for a specific platform. Although this involves tradeoffs, WireMock is easier to\n\nget started with in a purely Java-based project, as you don’t have to worry about calling an HTTP API or any\n\ncomplicated wiring into the build process.\n\nMountebank has an ecosystem of language bindings and\n\nbuild plugins, but you will have to search for them, and they may not expose the full capabilities of the tool. (In\n\nthe next chapter, you will see an example using JavaScript to wrap the REST API, and chapter 8 has an\n\nexample using a pre-built C# language binding.) That said, mountebank has broader portability than\n\nWireMock.\n\nAnother popular example is Hoverfly, a newer Go-based service virtualization tool that baked in middleware as\n\npart of the toolchain, allowing a high degree of customization. Mountebank offers middleware in the form of the shellTransform behavior, which we look at in chapter 7. Moco and stubby4j are other popular\n\noptions that are Java-based, although stubby4j has been ported to multiple languages.\n\nAs you will see in part 3 of this book, service virtualization\n\nhelps in a number of scenarios, and one tool isn’t always right for every scenario. Many of the commercial tools\n\naim for centralized testing, including performance tests. Many of the open source tools aim for a friendly\n\ndeveloper experience when doing functional service tests as part of the development process. I believe\n\nmountebank is unique in the open source world in that it aspires to support the full spectrum of service\n\nvirtualization use cases, including performance testing (which we look at in chapter 10). That said, you won’t hurt\n\nmy feelings if you use another tool for certain types of testing, and I hope that this book helps you identify what",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "you need in the different types of tests to thrive in a microservices world.\n\nSUMMARY\n\nMicroservices represent an architectural approach that can\n\nincrease both delivery throughput and velocity.\n\nTo realize the full potential of microservices, you must release\n\nthem independently.\n\nTo gain release independence, you must also test independently.\n\nService virtualization allows independent black-box testing of services.\n\nMountebank is an open source service virtualization tool for testing microservices.",
      "content_length": 505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Chapter 2. Taking mountebank for a test drive\n\nThis chapter covers\n\nUnderstanding how mountebank virtualizes HTTP\n\nInstalling and running mountebank\n\nExploring mountebank on the command line\n\nUsing mountebank in an automated test\n\nIn trying to do for pet supplies what Amazon did for books, Pets.com became one of the most spectacular\n\nfailures of the dot-com bust that occurred around the turn of the millennium. On the surface, the company had\n\neverything it needed to be successful, including a brilliant marketing campaign that featured a famous\n\nsock puppet. Yet it flamed out from IPO to liquidation in under a year, becoming synonymous with the bursting of\n\nthe dot-com bubble in the process.\n\nBusiness-minded folk claim that Pets.com failed because no market existed for ordering pet supplies over the\n\ninternet. Or it failed because of the lack of a viable business plan...or maybe because the company sold\n\nproducts for less than it cost to buy and distribute them. But as technologists, we know better.\n\nPets.com made only two mistakes that mattered. They didn’t use microservices, and, more importantly, they didn’t use mountebank.\n\n[1]\n\nIn an era in which social\n\nmedia and meme generators have conspired to bring cat picture innovation to new heights, it’s clear that we need\n\ninternet-provided pet supplies now more than ever. The time to correct the technical mistakes of Pets.com is long\n\noverdue. In this chapter, we will get started on a microservices architecture for a modern pet supply",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "company and show how you can use mountebank to maintain release independence between services.\n\n1\n\nI am, of course, joking. Neither microservices nor mountebank existed back then.\n\n2.1. SETTING UP THE EXAMPLE\n\nThough building an online pet supply site is a bit tongue- in-cheek, it will serve as a useful reference to get\n\ncomfortable with mountebank. As an e-commerce platform, it looks similar to the Amazon.com example\n\nyou saw in chapter 1.\n\nThe architecture shown in figure 2.1 is simplified, but it’s complex enough to work with. Each of the services on the\n\nright have its own set of runtime dependencies, but we will look at the architecture from the perspective of the\n\nwebsite team. One of the hallmarks of a good architecture is that, although you will need to understand\n\nsomething about your dependencies, you shouldn’t need to know anything about the other teams’ dependencies. I\n\nhave also introduced a façade layer that represents presentational APIs relevant to a specific channel. This is\n\na common pattern to aggregate and transform downstream service calls into a format optimized for the\n\nchannels (mobile, web, and so on).\n\nFigure 2.1. Your reference architecture for exploring mountebank",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "An advantage of using HTTP for integration is that,\n\nunlike libraries and frameworks, you can use an API without knowing what language the API was written in. [2]\n\nIt would be perfectly acceptable, for example, for you\n\nto write the product catalog service in Java and the\n\ninventory service in Scala. Indeed, having the ability to make new technology adoption easier is another side\n\nbenefit of microservices.\n\n2\n\nIt’s this fact that makes mountebank usable in any language.\n\n2.2. HTTP AND MOUNTEBANK: A PRIMER",
      "content_length": 511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "HTTP is a text-based request-response network protocol. An HTTP server knows how to parse that text into its\n\nconstituent parts, but it’s simple enough that you can parse it without a computer. Mountebank assumes that\n\nyou are comfortable with those constituent parts. After all, you can’t expect to provide a convincing fake of an\n\nHTTP service if you don’t first understand what a real one looks like.\n\nLet’s deep-dive into HTTP using one of the first features you need to support: listing the available products.\n\nFortunately, the product catalog service has an endpoint for retrieving the products in JSON format. All you have\n\nto do is make the right API call, which looks like figure 2.2 in HTTP-speak.\n\nFigure 2.2. Breaking down the HTTP request for products\n\nThe first line of any HTTP request contains three\n\ncomponents: the method, the path, and the protocol version. In this case, the method is GET, which denotes\n\nthat you are retrieving information rather than trying to change state on some server resource. Your path is /products, and you are using version 1.1 of the HTTP protocol. The second line starts the headers, a set of\n\nnewline-separated key-value pairs. In this example, the Host header combines with the path and protocol to give\n\nthe full URL like you would see in a browser:",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "http://api.petstore.com/products. The Accept header tells the server that you are expecting JSON back.\n\nWhen the product catalog service receives that request, it returns a response that looks like figure 2.3. A real service\n\npresumably would have many more data fields and many more items per page, but I have simplified the response\n\nto keep it digestible.\n\nFigure 2.3. The response from the product catalog",
      "content_length": 409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "A high degree of symmetry exists between HTTP requests and responses. As with the first line of the\n\nrequest, the first line of the response contains metadata, although for responses the most important metadata\n\nfield is the status code. A 200 status is HTTP-speak for success, but in case you forgot, it tells you with the word OK following the code. Other codes have other words that go with them, like BAD REQUEST for a 400, but the text\n\ndoesn’t serve any purpose other than a helpful hint. The libraries that you use for integrating with HTTP services\n\nonly care about the code, not the text.\n\nThe headers once again follow the metadata, but here\n\nyou see the HTTP body. The body is always separated from the headers by an empty line, and even though your\n\nHTTP request did not have a body, you will see plenty of examples in this book that do.\n\nThis particular body includes a link to the next page of\n\nresults, which is a common pattern for implementing paging in services. If you were to craft the HTTP request\n\nthat follows the link, it would look similar to the first request, as shown in figure 2.4.\n\nFigure 2.4. Adding a query parameter to an HTTP request\n\nThe difference appears to be in the path, but every HTTP\n\nlibrary that I’m aware of would give you the same path",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "for both the first and second request. Everything after the question mark denotes what is called the\n\nquerystring. (Mountebank calls it the query.) Like the headers, the query is a set of key-value pairs, but they are separated by the & character and included in the URL, separated from the path with a ? character.\n\nHTTP can attribute much of its success to its simplicity. The textual format makes it almost as easy for pizza-\n\nfueled computer programmers to read as it is for electricity-fueled computers to parse. That’s good for you\n\nbecause writing virtual services requires you to understand the protocol-specific request and response\n\nformats, which are treated as simple JSON objects that mimic closely the standard data structures used by HTTP\n\nlibraries in any language. To generalize, figure 2.5 shows how mountebank translates an HTTP request.\n\nFigure 2.5. How mountebank views an HTTP request\n\nNotice in figure 2.5 that even though the body is\n\nrepresented in JSON, HTTP itself doesn’t understand JSON, which is why the JSON is represented as a simple\n\nstring value. In later chapters, we will look at how mountebank makes working with JSON easier.\n\nFigure 2.6 shows how mountebank represents an HTTP\n\nresponse.",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Figure 2.6. How mountebank represents an HTTP response\n\nThis type of translation happens for all the protocols mountebank supports—simplifying the application\n\nprotocol details into a JSON representation. Each protocol gets its own JSON representation for both\n\nrequests and responses. The core functionality of mountebank performs operations on those JSON\n\nobjects, blissfully unaware of the semantics of the protocol. Aside from the servers and proxies to listen to\n\nand forward network requests, the core functionality in mountebank is protocol-agnostic.\n\nNow that you’ve seen how to translate HTTP semantics to mountebank, it’s time to create your first virtual\n\nservice.\n\n2.3. VIRTUALIZING THE PRODUCT CATALOG SERVICE\n\nOnce you understand how to integrate your codebase with a service, the next step is to figure out how to\n\nvirtualize it for testing purposes. Continuing with our example, let’s virtualize the product catalog service so\n\nyou can test the web façade in isolation (figure 2.7).\n\nFigure 2.7. Virtualizing the product catalog service to test the web facade",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Remember, an imposter is the mountebank term for a\n\nvirtual service. Mountebank ships with a REST API that lets you create imposters and write tests against them in\n\nany language.\n\nMountebanks, imposters, and funny sounding docs\n\nMuch of the mountebank documentation is written in\n\nthe voice of a true mountebank, prone to hyperbole and false modesty, where even the word “mountebank” shifts\n\nfrom describing the tool itself to the author of the documentation (yours truly). When I originally wrote the\n\ntool, I made imposters the core domain concept, in part because it fit the theme of using synonyms for charlatans\n\nto describe fake services, and in part because it self- deprecatingly made fun of my own Impostor Syndrome, a chronic ailment of consultants like myself. And yes, as\n\nPaul Hammant (one of the original creators of the popular Selenium testing tool and one of the first users\n\nof mountebank) pointed out to me, impostor (with an “or” instead of “er” at the end) is the “proper” spelling.\n\nNow that mountebank is a popular tool used all over the world, complete with a best-selling book (the one you are",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "holding), Paul also helpfully suggested that I change the docs to remove the hipster humor. Unfortunately, he has\n\nyet to indicate where I’m supposed to find the time for such pursuits.\n\nBefore you start, you will need to install mountebank.\n\nThe website, http://www.mbtest.org/docs/install, lists several installation options, but you’ll use npm, a package\n\nmanager that ships with node.js, by typing the following in a terminal window:\n\nnpm install -g mountebank\n\nThe -g flag tells npm to install mountebank globally, so you can run it from any directory. Let’s start it up:\n\nmb\n\nYou should see the mountebank log on the terminal:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking\n\norders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\nThe log will prove invaluable in working with mountebank in the future, so it’s a good idea to familiarize yourself with it. The first word (info, in this case) tells you the log level, which will be either debug,\n\ninfo, warn, or error. The part in brackets (mb:2525) tells you the protocol and port and is followed by the log message. The administrative port logs as the mb protocol and starts on port 2525 by default. (The mb protocol is\n\nHTTP, but mountebank logs it differently to make it easy to spot.) The imposters you create will use different ports but log to the same output stream in the terminal. The",
      "content_length": 1358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "startup log message directs you to open http://localhost:2525 in your web browser, which will\n\nprovide you the complete set of documentation for the version of mountebank you are running.\n\nTo demonstrate creating imposters, you will use a utility called curl, which lets you make HTTP calls on the\n\ncommand line. curl comes by default on most Unix-like shells, including Linux and macOS. You can install it on\n\nWindows using Cygwin, or use PowerShell, which ships with modern versions of Windows. (We will show a\n\nPowerShell example next.) Open another terminal window and run the code shown in the following listing. [3]\n\n3\n\nTo avoid carpal tunnel syndrome, you can download the source at https://github.com/bbyars/mountebank-in-action.\n\nListing 2.1. Creating an imposter on the command line\n\ncurl -X POST http://localhost:2525/imposters --\n\ndata '{ 1\n\n\"port\": 3000, 2\n\n\"protocol\": \"http\",\n\n2\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": {\n\n3\n\n\"statusCode\": 200, 3\n\n\"headers\": {\"Content-Type\":\n\n\"application/json\"}, 3\n\n\"body\": {\n\n3\n\n\"products\": [\n\n3\n\n{ 3\n\n\"id\": \"2599b7f4\",\n\n3\n\n\"name\": \"The Midas Dogbowl\",\n\n3",
      "content_length": 1102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "\"description\": \"Pure gold\"\n\n3\n\n},\n\n3\n\n{ 3\n\n\"id\": \"e1977c9e\",\n\n3\n\n\"name\": \"Fishtank Amore\",\n\n3\n\n\"description\": \"Show your fish some\n\nlove\" 3\n\n} 3\n\n],\n\n3\n\n\"_links\": {\n\n3\n\n\"next\": \"/products?\n\npage=2&itemsPerPage=2\" 3\n\n} 3\n\n}\n\n3\n\n}\n\n3\n\n}]\n\n}]\n\n}'\n\n1 Creates new imposters\n\n2 Minimally defines each imposter by a port and a protocol\n\n3 Defines a canned HTTP response\n\nAn important point to note is that you are passing a JSON object as the body field. As far as HTTP is\n\nconcerned, a response body is a stream of bytes. Usually HTTP interprets that stream as a string, which is why\n\n[4]\n\nmountebank typically expects a string as well. said, most services these days use JSON as their lingua\n\nThat\n\nfranca. Mountebank, being itself a modern JSON- speaking service, can properly accept a JSON body.\n\n4",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Mountebank supports binary response bodies, encoding them with Base64. We look at binary support in chapter 8.\n\nThe equivalent command on PowerShell in Windows\n\nexpects you to save the request body in a file and pass it in to the Invoke-RestMethod command. Save the\n\nJSON after the --data parameter from the curl command code above into a file called imposter.json,\n\nthen run the following command from the same directory:\n\nInvoke-RestMethod -Method POST -Uri\n\nhttp://localhost:2525/imposters\n\nInFile imposter.json\n\nNotice what happens in the logs:\n\ninfo: [http:3000] Open for business...\n\nThe part in brackets now shows the new imposter. As\n\nyou add more imposters, this will become increasingly important. You can disambiguate all log entries by\n\nlooking at the imposter information that prefixes the log message.\n\nYou can test your imposter on the command line as well, using the curl command we looked at previously, as shown in figure 2.8.\n\nFigure 2.8. Using curl to send a request to your virtual product catalog service",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "The curl command prints out the HTTP response as shown in the following listing.\n\nListing 2.2. The HTTP response from the curl command\n\nHTTP/1.1 200 OK Content-Type: application/json\n\nConnection: close\n\nDate: Thu, 19 Jan 2017 14:51:23 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\" }\n\n],\n\n\"_links\": {\n\n\"next\": \"/products?page=2&itemsPerPage=2\"\n\n}\n\n}\n\nThat HTTP response includes a couple of extra headers, and the date has changed, but other than that, it’s exactly\n\nthe same as the real one returned by the service shown in figure 2.3. You aren’t accounting for all situations though.\n\nThe imposter will return exactly the same response no matter what the HTTP request looks like. You could fix\n\nthat by adding predicates to your imposter configuration.\n\nAs a reminder, a predicate is a set of criteria that the incoming request must match before mountebank will\n\nsend the associated response. Let’s create an imposter that only has two products to serve up. We will use a\n\npredicate on the query parameter to show an empty result set on the request to the second page. For now, restart mb to free up port 3000 by pressing Ctrl-C and",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "typing mb again. (You will see more elegant ways of cleaning up after yourself shortly.) Then use the\n\ncommand shown in the following listing in a separate terminal.\n\nListing 2.3. An imposter with predicates\n\ncurl -X POST http://localhost:2525/imposters --\n\ndata '{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n1\n\n{\n\n\"predicates\": [{ \"equals\": {\n\n2\n\n\"query\": { \"page\": \"2\" }\n\n2\n\n}\n\n2\n\n}],\n\n\"responses\": [{ \"is\": {\n\n3\n\n\"statusCode\": 200,\n\n3\n\n\"headers\": {\"Content-Type\":\n\n\"application/json\"}, 3\n\n\"body\": { \"products\": [] }\n\n3 }\n\n3\n\n}]\n\n},\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n4 \"statusCode\": 200,\n\n4\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" }, 4\n\n\"body\": {\n\n4\n\n\"products\": [\n\n4 {\n\n4\n\n\"id\": \"2599b7f4\",",
      "content_length": 705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "4\n\n\"name\": \"The Midas Dogbowl\",\n\n4\n\n\"description\": \"Pure gold\"\n\n4 },\n\n4\n\n{\n\n4\n\n\"id\": \"e1977c9e\",\n\n4\n\n\"name\": \"Fishtank Amore\",\n\n4 \"description\": \"Show your fish\n\nsome love\" 4\n\n}\n\n4\n\n],\n\n4\n\n\"_links\": {\n\n4 \"next\": \"/products?\n\npage=2&itemsPerPage=2\" 4\n\n}\n\n4\n\n}\n\n4\n\n}\n\n4 }]\n\n}\n\n]\n\n}'\n\n1 Using two stubs allows different responses for different\n\nrequests.\n\n2 Requires that the request querystring include page=2\n\n3 Sends this response if the request matches the\n\npredicate\n\n4 Otherwise, sends this response\n\nNow, if you send a request to the imposter without a\n\nquerystring, you’ll get the same response as before. But adding page=2 to the querystring gives you an empty\n\nproduct list:",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "curl -i http://localhost:3000/products?page=2\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json\n\nConnection: close\n\nDate: Sun, 21 May 2017 17:19:17 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": []\n\n}\n\nExploring the mountebank API on the command line is a great way to get familiar with it and to try sample\n\nimposter configurations. If you change the configuration of your web façade to point to http://localhost:3000\n\ninstead of https://api.petstore.com, you will get the products we have defined and can manually test the\n\nwebsite. You have already taken a huge step toward decoupling yourself from the real services.\n\nPostman as an alternative to the command line\n\nAlthough using command-line tools like curl is great for lightweight experimentation and perfect for the book\n\nformat, it’s often useful to have a more graphical approach to organize different HTTP requests. Postman\n\n(https://www.getpostman.com/) has proven to be an extremely useful tool for playing with HTTP APIs. It\n\nstarted out as a Chrome plugin but now has downloads for Mac, Windows, and Linux. It lets you fill in the\n\nvarious HTTP request fields and save requests for future use.\n\nThat said, the real benefit of service virtualization is in enabling automated testing. Let’s see how you can wire\n\nup mountebank to your test suite.",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "2.4. YOUR FIRST TEST\n\nTo properly display the products on the website, the web\n\nfaçade needs to combine the data that comes from the product catalog service with marketing copy that comes\n\nfrom a marketing content service (figure 2.9). You will add tests that verify that the data that gets to the website\n\nis valid.\n\nFigure 2.9. Combining product data with marketing copy\n\nThe data that the web façade provides to the website should show both the product catalog data and the\n\nmarketing content. The response from the web façade should look like the following listing.\n\nListing 2.4. Combining product data with marketing content\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json\n\nDate: Thu, 19 Jan 2017 15:43:21 GMT\n\nTransfer-Encoding: chunked\n\n{\n\n\"products\": [ {\n\n\"id\": \"2599b7f4\",\n\n1",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "\"name\": \"The Midas Dogbowl\",\n\n2\n\n\"description\": \"Pure gold\",\n\n2\n\n\"copy\": \"Treat your dog like the king he is\", 3\n\n\"image\": \"/content/c5b221e2\"\n\n3\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\", \"copy\": \"Love your fish; they'll love you\n\nback\",\n\n\"image\": \"/content/a0fad9fb\"\n\n}\n\n],\n\n\"_links\": {\n\n\"next\": \"/products?page=2&itemsPerPage=2\"\n\n} }\n\n1 Comes from the product catalog service, but also will be\n\nused to look up content\n\n2 Comes from the product catalog service\n\n3 Comes from the marketing content service\n\nLet’s write a service test that validates that if the product catalog and content services return the given data, then\n\nthe web façade will combine the data as shown above. Although mountebank’s HTTP API allows you to use it in\n\nany language, you will use JavaScript for the example. The first thing you will need to do is make it easy to\n\ncreate imposters from your tests. A common approach to make building a complex configuration easier is to use what is known as a fluent interface, which allows you to\n\nchain function calls together to build a complex configuration incrementally.\n\nThe code in listing 2.5 uses a fluent interface to build up the imposter configuration in code. Each withStub call\n\ncreates a new stub on the imposter, and each matchingRequest and respondingWith call adds a",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "predicate and response, respectively, to the stub. When you are done, you call create to use mountebank’s\n\nREST API to create the imposter.\n\nListing 2.5. Using a fluent interface to build imposters in code\n\nrequire('any-promise/register/q');\n\n1\n\nvar request = require('request-promise-any');\n\n1\n\nmodule.exports = function (options) {\n\n2\n\nvar config = options || {};\n\nconfig.stubs = [];\n\nfunction create () {\n\n3\n\nreturn request({ method: \"POST\",\n\nuri: \"http://localhost:2525/imposters\",\n\njson: true,\n\nbody: config\n\n});\n\n}\n\nfunction withStub () { 4\n\nvar stub = { responses: [], predicates: [] },\n\nbuilders = {\n\nmatchingRequest: function (predicate) {\n\n5\n\nstub.predicates.push(predicate);\n\nreturn builders;\n\n}, respondingWith: function (response) {\n\n6\n\nstub.responses.push({ is: response });\n\nreturn builders;\n\n},\n\ncreate: create,\n\nwithStub: withStub\n\n};\n\nconfig.stubs.push(stub);\n\nreturn builders;\n\n}\n\nreturn {\n\nwithStub: withStub,",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "create: create\n\n};\n\n};\n\n1 node.js libraries that make calling HTTP services easier\n\n2 The node.js way of exposing a function to different files\n\n3 Calls the REST API to create an imposter\n\n4 The entry point to the fluent interface—each call\n\ncreates a new stub\n\n5 Adds a new request predicate to the stub\n\n6 Adds a new response to the stub\n\nJavaScript: ES5 vs. ES2015\n\nModern JavaScript syntax is defined in the version of the EcmaScript (ES) specification. At the time of this writing,\n\nES2015, which adds a bunch of syntactic bells and whistles, is seeing wide adoption, but ES5 still has the\n\nbroadest support. Although those syntactic bells and whistles are nice once you get used to them, they make\n\nthe code a little more opaque for non-JavaScript developers. Because this isn’t a book on JavaScript, I use\n\nES5 here to keep the focus on mountebank.\n\nYou will see how the fluent interface makes the\n\nconsuming code more elegant shortly. The key to making it work is exposing the create and withStub functions\n\nin the builder, which allows you to chain functions together to build the entire configuration and send it to\n\nmountebank.\n\nAssuming you saved the code above in a file called imposter.js, you can use it to create the product catalog\n\nservice response on port 3000. The code in listing 2.6 replicates what you did earlier on the command line and\n\nshows how the function chaining that the fluent interface",
      "content_length": 1419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "gives you makes the code easier to follow. Save the following code in test.js.[5]\n\nListing 2.6. Creating the product imposter in code\n\nvar imposter = require('./imposter'), 1\n\nproductPort = 3000;\n\nfunction createProductImposter() {\n\nreturn imposter({\n\n2\n\nport: productPort,\n\nprotocol: \"http\", name: \"Product Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({equals: {path:\n\n\"/products\"}}) 3\n\n.respondingWith({\n\n4\n\nstatusCode: 200, headers: {\"Content-Type\":\n\n\"application/json\"},\n\nbody: {\n\nproducts: [\n\n{\n\nid: \"2599b7f4\",\n\nname: \"The Midas Dogbowl\",\n\ndescription: \"Pure gold\" },\n\n{\n\nid: \"e1977c9e\",\n\nname: \"Fishtank Amore\",\n\ndescription: \"Show your fish some\n\nlove\"\n\n}\n\n] }\n\n})\n\n.create();\n\n5\n\n}\n\n1 Imports your fluent interface\n\n2 Passes the root-level information into the entry\n\nfunction\n\n3 Adds the request predicate",
      "content_length": 814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "4 Adds the response\n\n5 Sends a POST to the mountebank endpoint to create the\n\nimposter\n\nIt’s worth noting a couple of points about the way you are\n\ncreating the product catalog imposter. First, you have added a name to the imposter. The name field doesn’t\n\nchange any behavior in mountebank other than the way the logs format messages. The name will be included in\n\nthe text in brackets to make it easier to understand log messages by imposter. If you look at the mountebank logs after you create this imposter, you will see the name echoed:\n\ninfo: [http:3000 Product Service] Open for business...\n\nThat’s a lot easier than having to remember the port each\n\nimposter is running on.\n\nThe second thing to note is that you are adding a predicate to match the path. This isn’t strictly necessary, as your test will correctly pass without it if the web\n\nfaçade code is doing its job. However, adding the predicate makes the test better. It not only verifies the\n\nbehavior of the façade given the response, it also verifies that the façade makes the right request to the product\n\nservice.\n\nWe haven’t looked at the marketing content service yet. It accepts a list of IDs on a querystring and returns a set\n\nof content entries for each ID provided. The code in the following listing creates an imposter using the same IDs\n\nthat the product catalog service provides. (Add this to the test.js file you created previously.)\n\nListing 2.7. Creating the content imposter\n\nvar contentPort = 4000;",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "function createContentImposter() {\n\nreturn imposter({\n\nport: contentPort,\n\nprotocol: \"http\",\n\nname: \"Content Service\" })\n\n.withStub()\n\n.matchingRequest({\n\nequals: {\n\n1\n\npath: \"/content\",\n\n1\n\nquery: { ids: \"2599b7f4,e1977c9e\" } 1\n\n}\n\n1\n\n})\n\n.respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"}, body: {\n\ncontent: [\n\n2\n\n{\n\n2\n\nid: \"2599b7f4\",\n\n2\n\ncopy: \"Treat your dog like the king he is\", 2\n\nimage: \"/content/c5b221e2\"\n\n2\n\n},\n\n2\n\n{\n\n2\n\nid: \"e1977c9e\", 2\n\ncopy: \"Love your fish; they'll love\n\nyou back\", 2\n\nimage: \"/content/a0fad9fb\"\n\n2\n\n}\n\n2\n\n] 2\n\n}\n\n})\n\n.create();\n\n}",
      "content_length": 598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "1 Only respond if the path and query match as shown.\n\n2 The entries that the content service would return\n\nArmed with the createProductImposter and\n\ncreateContentImposter functions, you now can write a service test that calls the web façade over the wire\n\nand verifies that it aggregates the data from the product catalog and marketing content services appropriately\n\n(figure 2.10).\n\nFigure 2.10. The steps of the service test to verify web façadeʼs data aggregating\n\nFor this step, you will use a JavaScript test runner called Mocha, which wraps each test in an it function and\n\ncollections of tests in a describe function (similar to a test class in other languages). Finish off the test.js file\n\nyou have been creating by adding the code in the following listing.\n\nListing 2.8. Verifying the web façade\n\nrequire('any-promise/register/q');\n\nvar request = require('request-promise-any'),\n\nassert = require('assert'),\n\nwebFacadeURL = 'http://localhost:2000';\n\ndescribe('/products', function () {\n\n1",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "it('combines product and content data',\n\nfunction (done) { 2\n\ncreateProductImposter().then(function () {\n\n3\n\nreturn createContentImposter(); 3\n\n}).then(function () {\n\nreturn request(webFacadeURL + '/products');\n\n4\n\n}).then(function (body) {\n\nvar products = JSON.parse(body).products;\n\nassert.deepEqual(products, [ 5\n\n{\n\n\"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\",\n\n\"copy\": \"Treat your dog like the king\n\nhe is\",\n\n\"image\": \"/content/c5b221e2\" },\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some\n\nlove\",\n\n\"copy\": \"Love your fish; they'll love\n\nyou back\", \"image\": \"/content/a0fad9fb\"\n\n}\n\n]);\n\nreturn imposter().destroyAll();\n\n6\n\n}).then(function () {\n\ndone();\n\n7 });\n\n});\n\n});\n\n1 Mocha groups multiple tests in a describe function.\n\n2 Each it function represents a single test.\n\n3 Arrange\n\n4 Act\n\n5 Assert\n\n6 Cleanup",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "7 Tells mocha that the asynchronous test is finished\n\nNotice that you added one step to the test to clean up the\n\nimposters. Mountebank supports a couple ways of removing imposters. You can remove a single imposter by sending a DELETE HTTP request to the /imposters/:port URL (where :port represents the\n\nport of the imposter), or remove all imposters in a single call by issuing a DELETE request to /imposters. Add\n\nthem to your imposter fluent interface in imposter.js, as shown in the following listing.\n\nListing 2.9. Adding the ability to remove imposters\n\nfunction destroy () {\n\nreturn request({\n\nmethod: \"DELETE\",\n\nuri: \"http://localhost:2525/imposters/\" +\n\nconfig.port 1 });\n\n}\n\nfunction destroyAll () {\n\nreturn request({\n\nmethod: \"DELETE\",\n\nuri: \"http://localhost:2525/imposters\"\n\n});\n\n}\n\n1 Passes in the config object, as in listing 2.5\n\nWhew! You now have a complete service test that verifies some fairly complex aggregation logic of a service in a black-box fashion by virtualizing its runtime\n\ndependencies. (You had to create some scaffolding, but you will be able to reuse the imposters.js module in all of\n\nyour tests moving forward.) The prerequisites for running this test are that both the web façade and\n\nmountebank are running, and you have configured the web façade to use the appropriate URLs for the\n\nimposters (http://localhost:3000 for the product catalog service, and http://localhost:4000 for the marketing\n\ncontent service).[6]",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "JavaScript promises\n\nYour test code relies on a concept called promises to\n\nmake it easier to follow. JavaScript hasn’t traditionally had any I/O, and when node.js added I/O capability, it\n\ndid so in what is known as a nonblocking manner. This means that system calls that need to read or write data to\n\nsomething other than memory are done asynchronously. The application requests the operating system to read\n\nfrom disk, or from the network, and then moves on to other activities while waiting for the operating system to\n\nreturn. For a web service like the kind you are building, “other activities” would include processing new HTTP\n\nrequests.\n\nThe traditional way of telling node.js what to do when the operating system has finished the operation is to register a callback function. In fact, the request library that you are using to make HTTP calls works this way by\n\ndefault, as shown in this callback-based HTTP request:\n\nvar request = require('request');\n\nrequest('http://localhost:4000/products',\n\nfunction (error, response, body) {\n\n// Process the response here\n\n})\n\nThe problem with this approach is that it gets unwieldy to nest multiple callbacks, and downright tricky to figure\n\nout how to loop over a sequence of multiple asynchronous calls. With promises, asynchronous operations return an object that has a then function, which serves the same purpose as the callback. But\n\npromises add all kinds of simplifications to make combining complex asynchronous operations easier. You\n\nwill use them in your tests to make the code easier to read.",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Part 3 of this book will show more fully worked-out\n\nautomated tests and how to include them in a continuous delivery pipeline. First, though, you need to get familiar\n\nwith the capabilities of mountebank. Part 2 breaks down the core mountebank capabilities step by step, starting in\n\nthe next chapter by exploring canned responses in depth and adding HTTPS to the mix.\n\nSUMMARY\n\nMountebank translates the fields of the HTTP application protocol\n\ninto JSON for requests and responses.\n\nMountebank virtualizes services by creating imposters, which bind\n\na protocol to a socket. You can create imposters using\n\nmountebank’s RESTful API.\n\nYou can use mountebank’s API in automated tests to create\n\nimposters returning a specific set of canned data to allow you to\n\ntest your application in isolation.",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Part 2. Using mountebank\n\nThe test in chapter 2 was a behavioral test, but service\n\nvirtualization can satisfy a wide spectrum of testing needs. Understanding how mountebank fits into that\n\nspectrum requires exploring the full capabilities of the tool.\n\nThe test we just looked at used basic building blocks of service virtualization—and indeed of any stubbing tool— the ability to evaluate the request to determine how to\n\nrespond. We’ll look at these capabilities over the course of the next two chapters, including additional context\n\naround HTTPS, managing configuration files, and taking advantage of mountebank’s built-in XML and JSON\n\nparsing.\n\nChapters 5 and 6 demonstrate more advanced response\n\ngeneration, allowing a more interesting set of test scenarios. By adding record and replay capability, you\n\ncan generate test data dynamically to perform large-scale tests and to build the foundation for performance testing\n\n(which we’ll examine in part 3). The ability to programmatically change responses gives you key\n\nflexibility to support hard-to-test scenarios like OAuth.\n\nBehaviors, or postprocessing steps, provide advanced functionality. From managing test data in a CSV file to\n\nadding latency to your responses, behaviors give you a robust set of tools both to simplify testing and to support\n\na wider range of testing scenarios. We explore behaviors in chapter 7.\n\nWe round out this section by looking at mountebank’s\n\nsupport for protocols, which is the glue that makes everything else possible. Although we spend much of the\n\nbook exploring HTTP use cases, mountebank supports",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "multiple protocols, and in chapter 8 we explore how it works with additional TCP-based protocols.",
      "content_length": 97,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 3. Testing using canned responses\n\nThis chapter covers\n\nThe is response type, which is the fundamental building block for a stub\n\nUsing is responses in secure scenarios, with HTTPS servers and mutual authentication\n\nPersisting your imposter configuration using file templates\n\nDuring a famous U.S. White House scandal of the 1990s,\n\nthen-president Bill Clinton defended his prior statements by saying “It depends on what the meaning of is is.” The\n\ngrand jury and politicians ultimately failed to come to an agreement on the question, but, fortunately,\n\nmountebank has no uncertainty on the matter.\n\nIt turns out that is is quite possibly the most important, and the most foundational, concept in all of\n\nmountebank. Although an imposter, capturing the core idea of binding a protocol to a port, might beg to differ,\n\nby itself it adds little to a testing strategy. A response that looks like the real response—a response that, as far as the\n\nsystem under test is concerned, is the real response— changes everything. Is is the key to being fake. Without\n\nis, a service binding a protocol to a port is a lame beast at best. Adding the ability to respond, and to respond as if\n\nthe service is the real service, turns that service into a genuinely useful imposter.\n\nIn mountebank, the is response type is how you create canned responses, or responses that simulate a real response in some static way that you configure. Although it is one of three response types (proxy and inject being the other two), it’s the most important one. In this",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "chapter, we will explore is responses both by using the REST API and by persisting them in configuration files.\n\nWe will also start to layer in key security concerns. Although all of our examples so far have assumed HTTP,\n\nthe reality is that any serious web-based service built today will use HTTPS, layering transport layer security\n\n(TLS) onto the HTTP protocol. Because security— especially authentication—is generally one of the first\n\naspects of any microservice implementation you run into when writing tests, we will look at using an HTTPS\n\nserver that uses certificates to validate the client.\n\nFinally, we will explore how to persist imposter configurations. As you have no doubt realized by now,\n\nstubbing out services over the wire can be significantly more verbose than stubbing out objects in-process.\n\nFiguring out how to lay out that configuration in a maintainable way is essential to using service\n\nvirtualization to shift tests earlier in the development life cycle.\n\n3.1. THE BASICS OF CANNED RESPONSES\n\nIt’s a bit rude for any book on software development to\n\nskip out on the customary “Hello, world!” example. A “Hello, world!” response looks like the following listing\n\nin HTTP.\n\n1\n\nBrian Kernighan and Dennis Ritchie showed how to print “Hello, world!” to the terminal in their venerable book The C Programming Language. It has become a common introductory\n\nexample.\n\nListing 3.1. Hello world! in an HTTP response\n\nHTTP/1.1 200 OK\n\nContent-Type: text/plain\n\nHello, world!\n\n[1]",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "As you saw in the last chapter, returning this response in mountebank is as simple as translating the response you\n\nwant into the appropriate JSON structure, as follows.\n\nListing 3.2. The HTTP response structure in JSON\n\n{\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\": \"text/plain\" },\n\n\"body\": \"Hello, world!\"\n\n}\n\nTo create an HTTP imposter, listening on port 3000, that will return this response, save the following code in a helloWorld.json file.\n\nListing 3.3. The imposter configuration to respond with Hello, world!\n\n{ \"protocol\": \"http\",\n\n1\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": {\n\n2\n\n\"statusCode\": 200, 3\n\n\"headers\": { \"Content-Type\": \"text/plain\"\n\n}, 3\n\n\"body\": \"Hello, world!\"\n\n3\n\n}\n\n}]\n\n}] }\n\n1 Protocol defines response structure\n\n2 Tells mountebank to use an is response\n\n3 Defines the canned response to be translated into HTTP\n\nYou represent the JSON response you want from listing 3.2 inside the is response and expect mountebank to\n\ntranslate that to the HTTP shown in listing 3.1 because you’ve set the protocol to http. With mb running, you",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "can send an HTTP POST to http://localhost:2525/imposters to create this imposter. You will use the curl command, introduced in chapter 2, [2] to send the HTTP request:\n\n2\n\nFeel free to follow along using Postman or some graphical REST client. The examples are also available at https://github.com/bbyars/mountebank-in-action.\n\ncurl -d@helloWorld.json\n\nhttp://localhost:2525/imposters\n\nThe -d@ command-line switch reads the file that follows\n\nand sends the contents of that file as an HTTP POST body. You can verify that mountebank has created the\n\nimposter correctly by sending any HTTP request you want to port 3000:\n\n[3]\n\n3\n\nIn the examples that follow, I will continue to use the -i command-line parameter for curl.\n\nThis tells curl to print the response headers to the terminal.\n\ncurl -i http://localhost:3000/any/path?\n\nquery=does-not-matter\n\nThe response is almost, but not quite, the same as the “Hello, world!” response shown in listing 3.1:\n\nHTTP/1.1 200 OK Content-Type: text/plain\n\nConnection: close\n\nDate: Wed, 08 Feb 2017 01:42:38 GMT\n\nTransfer-Encoding: chunked\n\nHello, world!\n\nThree additional HTTP headers somehow crept in. Understanding where these headers came from requires",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "us to revisit a concept described in chapter 1 as the default response.\n\n3.1.1. The default response\n\nYou may recall the diagram shown in figure 3.1, which describes how mountebank selects which response to\n\nreturn based on the response.\n\nFigure 3.1. How mountebank selects a response\n\nThis diagram implies that if the request doesn’t match any predicate, a hidden default stub will be used. That\n\ndefault stub contains no predicates, so it always matches the request, and it contains exactly one response—the\n\ndefault response. You can see this default response if you create an imposter without any stubs:\n\ncurl http://localhost:2525/imposters --data '\n\n{ \"protocol\": \"http\",\n\n\"port\": 3000\n\n}'",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Note\n\nBecause you’re using port 3000 across multiple examples, you may find that you have to shut down and\n\nrestart mountebank between examples to avoid a port conflict. Alternatively, you can use the API to clean up the previous imposter(s) by sending an HTTP DELETE command to http://local host:2525/imposters (to\n\nremove all existing imposters) or to http://localhost:2525/imposters/3000 (to remove only the imposter on port 3000). If you’re using curl, the command would be curl -X DELETE http://local\n\nhost:2525/imposters.\n\nYou have not defined any responses with that lame beast\n\nof an imposter; you have only said you want an HTTP server listening on port 3000. If you send any HTTP\n\nrequest to that port, you get the default response shown in the following listing.\n\nListing 3.4. The default response in mountebank\n\nHTTP/1.1 200 OK\n\nConnection: close\n\nDate: Wed, 08 Feb 2017 02:04:17 GMT\n\nTransfer-Encoding: chunked\n\nWe looked at the first line of the response in chapter 2, and the 200 status code indicates that mountebank\n\nprocessed the request successfully. The Date header is a standard response header that any responsible HTTP\n\nserver sends, providing the server’s understanding of the current date and time. The other two headers require a\n\nbit more explanation.\n\nHTTP connections: to reuse or not to reuse?",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "HTTP is an application protocol built on top of the hard work of a few lower level network protocols, the most\n\nimportant of which (for our purposes) is TCP. TCP is responsible for establishing the connection between the\n\nclient and the server through a series of messages often referred to as the TCP handshake (figure 3.2).\n\nFigure 3.2. TCP making the connection for HTTP messages\n\nAlthough the TCP messages to establish the connection (represented by the dashed lines) are necessary, they\n\naren’t necessary for every request. Once the connection is established, the client and server can reuse it for\n\nmultiple HTTP messages. That ability is important, particularly for websites that need to serve HTML,\n\nJavaScript, CSS, and a set of images, each of which requires a round trip between the client and the server.\n\nHTTP supports keep-alive connections as a performance\n\noptimization. A server tells the client to keep the connection open by setting the Connection header to\n\nKeep-Alive. Mountebank defaults it to close, which tells the client to negotiate the TCP handshake for every",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "request. If you are writing service tests, performance likely doesn’t matter, and you may prefer the\n\ndeterminism that comes with a fresh connection for each request. If you are writing performance tests, where the\n\nservices you are virtualizing should be tuned with keep- alive connections, or if your purpose is to ensure your\n\napplication behaves well with keep-alive connections, you should change the default.\n\nKnowing where an HTTP body ends\n\nNotice in figure 3.2 that a single HTTP request may\n\nconsist of multiple packets. (The operating system breaks up data into a series of packets to optimize sending them\n\nover the network.) The same is true of a server response: what looks to be a single response may get transmitted in\n\nmultiple packets. A consequence of this is that clients and servers need some way of knowing when an HTTP\n\nmessage is complete. With headers, it’s easy: the headers end when you get a blank header line. But there’s no way\n\nto predict where blank lines will occur in HTTP bodies, so you need a different strategy. HTTP provides two\n\nstrategies, as shown in figure 3.3.\n\nFigure 3.3. Using chunked encodings or content length to calculate where the body ends",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "The default imposter behavior sets the Transfer-\n\nEncoding: chunked header, which breaks the body into a series of chunks and prefixes each one with the\n\nnumber of bytes it contains. Special formatting delineates each chunk, making parsing relatively easy.\n\nThe advantage of sending the body a chunk at a time is that the server can start streaming data to the client\n\nbefore the server has all of the data. The alternative strategy is to calculate the length of the entire HTTP\n\nbody before sending it and provide that information in the header. To select that strategy, the server sets a Content-Length header to the number of bytes in the body.\n\nWhen I created mountebank, I had to choose one default\n\nstrategy. In truth, the web framework mountebank is written in chose it for me, which is the only reason\n\nmountebank imposters default to chunked encoding. The two strategies are mutually exclusive, so if you need to",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "set the Content-Length header, the Transfer- Encoding header won’t be set.\n\n3.1.2. Understanding how the default response works\n\nNow that you have seen what the default response looks like, it is probably a good time to admit that there is no\n\nsuch thing as a default stub in mountebank. That’s a bald-faced lie. I’m sorry—I did feel a little guilty writing\n\nit—but it’s a useful simplification for situations where no stub matches the request. And in case you haven’t\n\nnoticed yet, lying is exactly what mountebank does.\n\nThe reality is that mountebank merges the default response into any response you provide. Not providing a\n\nresponse is the same as providing an empty response, which is why you see the purest form of the default\n\nresponse in listing 3.4. But you also could provide a partial response; for example, the following response\n\nstructure doesn’t provide all of the response fields:\n\n{\n\n\"is\": {\n\n\"body\": \"Hello, world!\"\n\n}\n\n}\n\nNot to worry. Mountebank will still return a full response, helpfully filling in the blanks for you:\n\nHTTP/1.1 200 OK\n\nConnection: close Date: Sun, 12 Feb 2017 17:38:39 GMT\n\nTransfer-Encoding: chunked\n\nHello, world!\n\n3.1.3. Changing the default response",
      "content_length": 1196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Mountebank’s ability to merge in defaults for the response is a pleasant convenience. As I suggested, it\n\nmeans you only need to specify the fields that are different from the defaults, which simplifies the response\n\nconfiguration. But that’s only useful if the defaults represent what you typically want. Fortunately,\n\nmountebank allows you to change the default response to better suit your needs.\n\nImagine a test suite that only wants to test error paths. You can default the statusCode to a 400 Bad\n\nRequest to avoid having to specify it in each response. Although you can’t get rid of the Date header (it’s\n\nrequired for valid responses), you’ll go ahead and change the other default headers to use keep-alive connections and set the Content-Length header, as in the following listing.\n\nListing 3.5. Changing the default response\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"defaultResponse\": { 1\n\n\"statusCode\": 400, 2 \"headers\": { 3\n\n\"Connection\": \"Keep-Alive\", 4\n\n\"Content-Length\": 0 5\n\n}\n\n},\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"is\": { \"body\": \"BOOM!!!\" } 6 }]\n\n}]\n\n}\n\n1 Changes the built-in default response for this imposter\n\nonly\n\n2 Defaults to a Bad Request\n\n3 Adds or changes default headers\n\n4 Uses keep-alive connections\n\n5 Sets the Content-Length header",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "6 The response details will be merged into the default\n\nresponse.\n\nIf you now send a test request to the imposter, it merges the new default fields into the response, as follows.\n\nListing 3.6. A response using the new defaults\n\nHTTP/1.1 400 Bad Request 1 Connection: Keep-Alive 2\n\nContent-Length: 7 3\n\nDate: Fri, 17 Feb 2017 16:29:00 GMT 4\n\nBOOM!!! 5\n\n1 400 comes from your default status code.\n\n2 You’re now using keep-alive connections.\n\n3 The Content-Length value was corrected from 0 to 7,\n\nand the Transfer-Encoding header is gone.\n\n4 The Date header remains from the original default\n\nresponse.\n\n5 The body from your is response is merged in.\n\nNotice in particular that mountebank set the Content-\n\nLength header to the correct value. Mountebank imposters won’t send out invalid HTTP responses.\n\n3.1.4. Cycling through responses\n\nLet’s imagine one more test scenario: this time, you will test what happens when you submit an order through an\n\nHTTP POST to an order service. Part of the order submission process involves checking to make sure inventory is sufficient. The tricky part, from a testing\n\nperspective, is that inventory is sold and restocked—it doesn’t stay static for the same product. This means that\n\nthe exact same request to the inventory service can respond with a different result each time (figure 3.4).\n\nFigure 3.4. Inventory checks return volatile results for the same request.",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "In chapter 2, you saw a similar example with requests to\n\nthe product catalog service, which returned different responses for the same path. In that example, you were\n\nable to use different predicates to determine which response to send based on the page query parameter,\n\nbut in the inventory example, nothing about the request allows you to select one response over the other.\n\nWhat you need is a way to cycle through a set of\n\nresponses to simulate the volatility of the on-hand inventory for a fast-selling product. The solution is to use\n\nthe fact that each stub contains a list of responses, as shown in the following listing. Mountebank returns those\n\n[4]\n\nresponses in the order provided.\n\n4\n\nNote that in the following example and several others throughout the book, I will use an overly simplified response to save space and remove some of the noise. No self-respecting inventory service would ever return only a single number, but it makes the intent of the\n\nexample stand out more clearly, allowing you to focus on the fact that some data is different for each response.\n\nListing 3.7. Returning a list of responses for the same stub\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } }, 1",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "{ \"is\": { \"body\": \"21\" } }, 1\n\n{ \"is\": { \"body\": \"0\" } } 1\n\n]\n\n}\n\n] }\n\n1 Responses that are returned in order\n\nThe first call returns 54, the second call returns 21, and\n\nthe third call returns 0. If your tests need to trigger a fourth call, it will once again return 54, then 21, and 0.\n\nMountebank treats the list of responses as an infinite list, with the first and last entries connected like a circle, the\n\ncircular buffer data structure discussed in chapter 1.\n\nAs shown in figure 3.5, the illusion of an infinite list is maintained by shifting each response to the end of the\n\nlist when it is returned. You can cycle through them as many times as you need to.\n\nFigure 3.5. Each stub cycles through the responses forever.",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "In chapter 7, we’ll look at all kinds of interesting post- processing actions you can take on a response, but for\n\nnow, you don’t need to know anything more about canned responses. Let’s switch gears and see how to layer\n\nin security.\n\n3.2. HTTPS IMPOSTERS\n\nTo keep things simple, we’ve focused on HTTP services\n\nso far. The reality is that real services require security, and that means using HTTPS. The S stands for SSL/TLS,\n\nwhich adds encryption, and, optionally, identity verification to each request. Figure 3.6 shows the basic\n\nstructure for SSL.\n\nFigure 3.6. The basic structure of SSL/TLS\n\nThe infrastructure handles the SSL layer of HTTPS so that, as far as the application is concerned, each request\n\nand response is standard HTTP. The details of how the",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "SSL layer works are a bit complex, but the key concepts that make it work are the server’s certificate and the\n\nserver’s keys. The HTTPS server presents the client an SSL certificate during the handshake process, which\n\ndescribes the server’s identity, including information such as the owner, the domain it’s attached to, and the\n\nvalidity dates.\n\nIt’s entirely possible that a malicious server may try to\n\npass itself off as, say, Google, in the hopes of you passing it confidential information that you would only intend to\n\npass to Google. That’s why Certificate Authorities (CAs) exist. Trust has to start somewhere, and CAs are the\n\nfoundation of trust in the SSL world. By sending a certificate, which contains a digital signature, to a CA\n\nthat your organization trusts, you can confirm that the certificate is in fact from Google.\n\nThe certificate also includes the server’s public key. The\n\neasiest approach to encryption is to use a single key for both encryption and decryption. Because of its efficiency,\n\nmost of the communication relies on single-key encryption, but first the client and server have to agree\n\non the key used without anyone else knowing it. The type of encryption SSL relies on during this handshake uses a\n\nneat trick that requires different keys for those two operations: the public key is used for encryption, and a\n\nseparate private key is used for decryption (figure 3.7). This allows the server to share its public key and the\n\nclient to use that key for encryption, knowing that only the server will be able to decrypt the resulting payload\n\nbecause only it has the private key.\n\nFigure 3.7. Using two keys prevents attackers from reading messages in transit even when the encryption key is shared.",
      "content_length": 1737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "The good news is that creating an HTTPS imposter can\n\nlook exactly like creating an HTTP imposter. The only required difference is that you set the protocol to https:\n\n{\n\n\"protocol\": \"https\", \"port\": 3000\n\n}\n\nThis is great for quickly setting up an HTTPS server, but\n\nit uses a default certificate (and key pair) that ships with mountebank. That certificate is both insecure and\n\nuntrusted. Although that may be OK for some types of testing, any respectable service call should validate that\n\nthe certificate is trusted, which, as shown in figure 3.6, involves a call to a trusted CA. By default, the HTTPS\n\nlibrary that your system under test is using should reject mountebank’s built-in certificate, meaning that it won’t\n\nbe able to connect to your virtual HTTPS service.",
      "content_length": 774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "That leaves three options. The first is that you could configure the service you’re testing not to validate that\n\nthe certificate is trusted. Don’t do this. You don’t want to risk leaving code like that in during production, and you\n\ndon’t want to test one behavior for your service (that doesn’t do a certificate validation) and deploy a\n\ncompletely different behavior to production (that does validation). The whole point of testing, after all, is to gain\n\nconfidence in what you’re sending to production, which requires that you actually test what’s going to\n\nproduction.\n\nThe second option is to avoid testing with HTTPS.\n\nInstead, create an HTTP imposter and configure your system under test to point to it. The networking libraries\n\nthat your system under test uses should support that change without any code changes, and you usually can trust them to work with HTTPS. This is a reasonable\n\noption to use when you are testing on your local machine.\n\nThe third option, shown in figure 3.8, is to use a certificate that is trusted, at least in the test environment.\n\nOrganizations can run their own CA, making trust part of the environment rather than part of the application. That\n\nallows you to set up the test instances of your virtual services with appropriately trusted certificates.\n\nFigure 3.8. Setting up a test environment with HTTPS",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "With this approach, the test creates the imposter with\n\nboth the certificate and the private key, as shown in the following listing. You can pass them in what is known as\n\nPEM format; we will look at how to create them shortly.\n\nListing 3.8. Creating an HTTPS imposter\n\n{\n\n\"protocol\": \"https\", \"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\\n...\",\n\n1\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\\n...\"\n\n1\n\n}",
      "content_length": 408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "1 Much abbreviated from the actual text\n\nThis setup is still insecure. The test needs to know the\n\nprivate key to create the imposter, so the imposter will know how to decrypt communication from the system\n\nunder test. The certificate is tied to the domain name in the URL, so as long as you segment that domain name to\n\nyour test environment, you aren’t risking leaking any production secrets. With appropriate environment\n\nseparation, this approach allows you to test the system under test without changing its behavior to allow untrusted certificates.\n\n3.2.1. Setting up a trusted HTTPS imposter\n\nHistorically, getting certificates trusted by a public CA has been a painful and confusing process, and it cost\n\nenough money to discourage their use in exactly the kind of lightweight testing scenarios that mountebank\n\nsupports. Using SSL is such a cornerstone of internet security that major players are pushing to change that\n\nprocess to the point where it’s easy for even hobbyists without a corporate purse to create genuine certificates\n\nfor the domains they register.\n\nLet’s Encrypt (https://letsencrypt.org/) is a free option that supports creating certificates for domains with minimal\n\nfuss, backed by a public CA. Every CA will require validation from the domain owner to ensure that no one\n\nis able to grab a certificate for a domain they don’t own. Let’s Encrypt allows you to completely automate the\n\nprocess by round-tripping a request based on a DNS lookup of the domain listed in the certificate (figure 3.9).\n\nFigure 3.9. How Letʼs Encrypt validates the domain",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Let’s Encrypt uses a command-line tool called Certbot\n\n(https://certbot.eff.org) to automate the creation of certificates. Certbot expects you to install a client on the\n\nmachine receiving the SSL request. The client stands up a web server and sends a request to a Let’s Encrypt\n\nserver. Let’s Encrypt in turn looks up the domain for which you are requesting a certificate in DNS and sends\n\na request to that IP address. If that request reaches the certbot server that created the first request, Let’s Encrypt\n\nhas validated that you own the domain.\n\nThe certbot command depends on the web server you are using, and because it is constantly evolving, you\n\nshould check the documentation for details. In the general case, you might run:\n\ncertbot certonly --webroot -w /var/test/petstore\n\nd test.petstore.com\n\nThat would create a certificate for the test.petstore.com domain that is served out of a web server running in\n\n/var/test/petstore. Simplifications are available if you’re using a common web server like Apache or Nginx. See",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "https://certbot.eff.org/docs/using.html#getting-certificates-and- choosing-plugins for details.\n\nBy default, the directory that certbot stores the SSL information in is /etc/lets encrypt/live/$domain, where\n\n$domain is the domain name of your service. If you look in that directory, you will find a few files, but two are\n\nrelevant for our purposes: privkey.pem contains the private key, and cert.pem contains the certificate. The\n\ncontents of those two files are what you would put in the key and cert fields when creating the HTTPS imposter.\n\nA PEM file has newlines. An example certificate might\n\nlook like the following:\n\n-----BEGIN CERTIFICATE-----\n\nMIIDejCCAmICCQDlIe97PDjXJDANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJV\n\nUzEOMAwGA1UECBMFVGV4YXMxFTATBgNVBAoTDFRob3VnaHRXb3JrczEMMAoGA1UE\n\nCxMDT1NTMRMwEQYDVQQDEwptYnRlc3Qub3JnMSYwJAYJKoZIhvcNAQkBFhdicmFu\n\nZG9uLmJ5YXJzQGdtYWlsLmNvbTAeFw0xNTA1MDMyMDE3NTRaFw0xNTA2MDIyMDE3\n\nNTRaMH8xCzAJBgNVBAYTAlVTMQ4wDAYDVQQIEwVUZXhhczEVMBMGA1UEChMMVGhv\n\ndWdodFdvcmtzMQwwCgYDVQQLEwNPU1MxEzARBgNVBAMTCm1idGVzdC5vcmcxJjAk BgkqhkiG9w0BCQEWF2JyYW5kb24uYnlhcnNAZ21haWwuY29tMIIBIjANBgkqhkiG\n\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5V88ZyZ5hkPF7MzaDMvhGtGSBKIhQia2a0vW\n\n6VfEtf/Dk80qKaalrwiBZlXheT/zwCoO7WBeqh5agOs0CSwzzEEie5/J6yVfgEJb\n\nVROpnMbrLSgnUJXRfGNf0LCnTymGMhufz2utzcHRtgLm3nf5zQbBJ8XkOaPXokuE\n\nUWwmTHrqeTN6munoxtt99olzusraxpgiGCil2ppFctsQHle49Vjs88KuyVjC5AOb\n\n+P7Gqwru+R/1vBLyD8NVNl1WhLqaaeaopb9CcPgFZClchuMaAD4cecndrt5w4iuL\n\nq91g71AjdXSG6V3R0DC2Yp/ud0Z8wXsMMC6X6VUxFrbeajo8CQIDAQABMA0GCSqG\n\nSIb3DQEBBQUAA4IBAQCobQRpj0LjEcIViG8sXauwhRhgmmEyCDh57psWaZ2vdLmM ED3D6y3HUzz08yZkRRr32VEtYhLldc7CHItscD+pZGJWlpgGKXEHdz/EqwR8yVhi\n\nakBMhHxSX9s8N8ejLyIOJ9ToJQOPgelI019pvU4cmiDLihK5tezCrZfWNHXKw1hw\n\nSh/nGJ1UddEHCtC78dz6uIVIJQC0PkrLeGLKyAFrFJp4Bim8W8fbYSAffsWNATC+\n\ndVKUlunVLd4RX/73nY5EM3ErcDDOCdUEQ2fUT59FhQF89DihFG4xW4OLq42/pgmW\n\nKQBvwwfJxIFqg4fdnJUkHoLX3+glQWWrz80cauVH\n\n-----END CERTIFICATE-----\n\nYou’ll want to keep the newlines, escaped in typical JSON fashion with '\\n', in the strings you send\n\nmountebank. In this example, shortening the field for clarity, the resulting imposter configuration might look\n\nlike this:",
      "content_length": 2134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "{\n\n\"protocol\": \"https\",\n\n\"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\nMIIEpAIBAAKC...\",\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\n\n\\nMIIDejCCAmICCQD...\" }\n\nNote that, although it’s awkward to show in book format,\n\nthe string would include all the way up to the end of the file (from the example, “...\\nWWrz80cauVH\\n-----END\n\nCERTIFICATE-----”) for the certificate.\n\nAnd...that’s it. Everything else about your imposter\n\nremains the same. Once you have set the certificate and private key, the SSL layer is able to convert encrypted\n\nmessages into HTTP requests and responses, which means the is responses you have already created continue to work. It may seem like a lot of work to set up\n\nthe certificates, but that’s the nature of SSL. Fortunately, tools like Let’s Encrypt and shortcuts like using wildcard\n\ncertificates simplify the process considerably.\n\nUsing wildcard certificates to simplify testing\n\nA typical certificate is associated with a single domain name, such as mypet store.com. By adding a wildcard in front of the domain, the certificate becomes valid for\n\nall subdomains. You could, for example, create a *.test.mypetstore.com certificate, and that\n\ncertificate would be valid for products.test.mypet store.com as well as\n\ninventory.test.mypetstore.com. It wouldn’t be valid for production domains that don’t include test as\n\npart of their domain name.\n\nA wildcard certificate is ideal for testing scenarios. You may find it easy to manually add a wildcard certificate to",
      "content_length": 1501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "the CA, tied exclusively to a testing subdomain, and reuse the certificate and private key for all imposters.\n\n3.2.2. Using mutual authentication\n\nIt turns out that certificates aren’t only valid for HTTPS servers; they’re also a common way to validate the\n\nidentity of clients (figure 3.10). You don’t see this when browsing the internet, because public websites have to\n\nassume the validity of the browsers that access them, but in a microservices architecture, it’s important to validate\n\nthat only authenticated clients can make a request to a server.\n\nFigure 3.10. Setting up a test environment with HTTPS to validate clients as well as servers",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "If the service you are testing expects to validate its\n\nidentity with your imposter using a client certificate, you need to be able to configure your imposter in a way that\n\nexpects that certificate. This is as simple as adding a mutualAuth field to the configuration set to true, as\n\nshown in the following listing.\n\nListing 3.9. Adding mutual authentication to an imposter\n\n{\n\n\"protocol\": \"https\", \"port\": 3000,\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\nMIIEpAIBAAKC...\",\n\n\"cert\": \"-----BEGIN CERTIFICATE-----",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "\\nMIIDejCCAmICCQD...\",\n\n\"mutualAuth\": true\n\n1\n\n}\n\n1 The server will expect a client certificate.\n\nNow the server will challenge the client with a certificate request. Using certificates, both for HTTPS and for\n\nmutual authentication, allows you to virtualize servers in secure environments. But the fact that you have to\n\nescape the PEM files in JSON gets quite clunky. Let’s look at how to make maintaining that data a bit easier\n\nusing configuration files.\n\n3.3. SAVING THE RESPONSES IN A CONFIGURATION FILE\n\nBy now, you’re probably realizing that, as the complexity of the responses and security configuration increases, the\n\nJSON that you send mountebank can be quite complex.\n\nThis is true even for a single field, like the multiline PEM files that need to be encoded as a JSON string.\n\nFortunately, mountebank has robust support for persisting the configuration in a friendly format.\n\nNow that you’ve added an inventory service and seen how to convert it to HTTPS, let’s see how you would\n\nformat the imposter configuration in files to make it easier to manage. The first bit of ugliness you’ll want to\n\nsolve is storing the certificate and private key in separate PEM files so you can avoid a long JSON string. If you store those as cert.pem and key.pem in the ssl\n\ndirectory, then you can create a file for the inventory imposter as inventory.ejs (figure 3.11).\n\nFigure 3.11. The tree structure for the secure inventory imposter configuration",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Mountebank uses a templating language called EJS\n\n(http://www.embeddedjs.com/) to interpret the config file, which uses a fairly standard set of templating primitives. The content between <%- and %>, as shown in the following listing, is dynamically evaluated and\n\ninterpolated into the surrounding quotes. Save the following in inventory.ejs.\n\nListing 3.10. Storing the inventory service in a configuration file\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"https\",\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem') %>\", 1\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem') %>\", 1\n\n\"stubs\": [\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } },\n\n{ \"is\": { \"body\": \"21\" } },\n\n{ \"is\": { \"body\": \"0\" } }\n\n] }\n\n]\n\n}\n\n1 Converts the multiline file content into a JSON string\n\nThe inventory service will be available at startup, if you\n\nstart mountebank with the appropriate command-line flag:\n\nmb --configfile inventory.ejs",
      "content_length": 899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Mountebank adds the stringify function to the templating language, which does the equivalent of a JavaScript JSON.stringify call on the contents of the given file. In this case, the stringify call escapes the\n\nnewlines. The benefit to you is that the configuration is much easier to read. (The filename variable is passed\n\nin by mountebank. It’s a bit of a hack needed to make relative paths work.)\n\nWith those two templating primitives—the angle brackets that will be replaced by dynamic data and the stringify function to turn that data into presentable JSON—you can build robust templates. Storing the SSL\n\ninformation separately is useful, but I intentionally oversimplified the inventory imposter to focus on the behavior of the responses array. Let’s add in the product catalog and marketing content services you saw in chapter 2.\n\n3.3.1. Saving multiple imposters in the config file\n\nAs you saw, templating allows you to break up your configuration into multiple files. You’ll take advantage of\n\nthat to revisit the product catalog and marketing content imposter configurations you saw in chapter 2, putting\n\neach imposter in one or more files. The first thing you need to do is define the root configuration, which now\n\nneeds to take a list of imposters. The tree structure will look like figure 3.12.\n\nFigure 3.12. The tree structure for multiple services",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Save the following listing as imposters.ejs.\n\nListing 3.11. The root configuration file, referencing other imposters\n\n{\n\n\"imposters\": [\n\n<% include inventory.ejs %>, 1\n\n<% include product.ejs %>, 1\n\n<% include content.ejs %> 1\n\n]\n\n}\n\n1 Interpolates content from other files as is\n\nThe include function comes from EJS. Like the\n\nstringify function, it loads in content from another file. Unlike stringify, the include function doesn’t\n\nchange the data; it brings the data as is from the referenced file. You can use the include EJS function\n\nand the stringify mountebank function to lay out your content any way you like. For complex\n\nconfigurations, you can store the response bodies— JSON, XML, or any other complex representation—in\n\ndifferent files with newlines and load them in as needed. To keep it simple, you’ll save each imposter in its own\n\nfile, loading in the same wildcard certificate and private key. Save the product catalog imposter configuration that you saw in listing 2.1 in product.ejs, with some modifications as shown in the following listing.",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Listing 3.12. The updated version of the product catalog imposter configuration\n\n{\n\n\"protocol\": \"https\",\n\n1 \"port\": 3001,\n\n2\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem'); %>\",\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem');\n\n%>\",\n\n\"stubs\": [{\n\n3 \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" },\n\n\"body\": {\n\n\"products\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"name\": \"The Midas Dogbowl\",\n\n\"description\": \"Pure gold\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"name\": \"Fishtank Amore\",\n\n\"description\": \"Show your fish some love\"\n\n}\n\n]\n\n}\n\n}\n\n}],\n\n\"predicates\": [{\n\n\"equals\": { \"path\": \"/products\" } }]\n\n}]\n\n}\n\n1 Converts to HTTPS\n\n2 Uses a different port to avoid a port conflict\n\n3 Same stub configuration as in chapter 2\n\nFinally, save the marketing content imposter configuration you saw in listing 2.7 in a file called",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "content.ejs, with the modifications shown in the following listing.\n\nListing 3.13. The updated version of the marketing content imposter configuration\n\n{ \"protocol\": \"https\",\n\n\"port\": 3002,\n\n1\n\n\"cert\": \"<%- stringify(filename,\n\n'ssl/cert.pem'); %>\",\n\n\"key\": \"<%- stringify(filename, 'ssl/key.pem');\n\n%>\",\n\n\"stubs\": [{ \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { \"Content-Type\":\n\n\"application/json\" },\n\n\"body\": {\n\n\"content\": [\n\n{ \"id\": \"2599b7f4\",\n\n\"copy\": \"Treat your dog like the\n\nking he is\",\n\n\"image\": \"/content/c5b221e2\"\n\n},\n\n{\n\n\"id\": \"e1977c9e\",\n\n\"copy\": \"Love your fish; they'll love you back\",\n\n\"image\": \"/content/a0fad9fb\"\n\n}\n\n]\n\n}\n\n}\n\n}],\n\n\"predicates\": [{ \"equals\": {\n\n\"path\": \"/content\",\n\n\"query\": { \"ids\": \"2599b7f4,e1977c9e\" }\n\n}\n\n}]\n\n}]\n\n}\n\n1 Uses a different port",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Now you can start mountebank by pointing to the root configuration file:\n\nmb --configfile imposters.ejs\n\nNotice what happens in the logs:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking\n\norders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\ninfo: [mb:2525] PUT /imposters\n\ninfo: [https:3000] Open for business...\n\ninfo: [https:3001] Open for business...\n\ninfo: [https:3002] Open for business...\n\nAll three imposters are up and running. Of interest is the log entry pointing out that an HTTP PUT command was\n\nsent the mountebank URL of http://localhost:2525/imposters. After running the contents of the configuration file through EJS, the mb\n\ncommand sends the results as the request body of the PUT command, which creates (or replaces) all the\n\nimposters in one shot. Nearly every feature in mountebank is exposed via an API first, so anything you\n\ncan do on the command line, you can implement using the API. If you had more advanced persistence\n\nrequirements, you could construct the JSON and send it to mountebank using curl, as in the following listing.\n\nListing 3.14. Using curl to send the JSON to mountebank\n\ncurl -X PUT http://localhost:2525/imposters --\n\ndata '{\n\n\"imposters\": [\n\n{\n\n\"protocol\": \"https\",\n\n\"port\": 3000\n\n},\n\n{ \"protocol\": \"https\",\n\n\"port\": 3001",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "}\n\n]\n\n}'\n\nFor clarity, I’ve left out all the important bits of the imposter configuration. You may find the PUT command\n\na convenience in automated test suites where a setup step overwrites the entire set of imposters with one API\n\ncall, rather than relying on all of the individual tests to send the DELETE calls to clean up their imposters.\n\nIf you do load the imposters through a configuration file,\n\nthe imposter setup is part of starting mountebank, which you’re expected to do before running your tests. That\n\narrangement allows you to remove some of the setup steps from the test itself—specifically, those related to\n\nconfiguring and deleting the imposters.\n\nSUMMARY\n\nThe is response type allows you to create a canned response. The fields you specify in the response object merge in with the default\n\nresponse. You can change the default response if you need to.\n\nOne stub can return multiple responses. The list of responses acts\n\nlike a circular buffer, so once the last response is returned,\n\nmountebank cycles back to the first response.\n\nHTTPS imposters are possible, but you have to create the key pair\n\nand certificate. Let’s Encrypt is a free service that lets you\n\nautomate the process.\n\nSetting the mutualAuth flag on an imposter means that it will accept client certificates used for authentication.\n\nMountebank uses EJS templating for persisting the configuration\n\nof your imposters. You load them at startup by passing the root template as the parameter to the --configfile command-line option.",
      "content_length": 1516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Chapter 4. Using predicates to send diﬀerent responses\n\nThis chapter covers\n\nUsing predicates to send different responses for different requests\n\nSimplifying predicates on JSON request bodies\n\nUsing XPath to simplify predicates on XML request bodies\n\nDuring his younger years, Frank William Abagnale Jr. forged a pilot’s license and traveled the world by\n\n[1]\n\ndeadheading. successful impersonation of a pilot meant that his food\n\nDespite not being able to fly, his\n\nand lodging were fully paid for by the airline. When that well dried up, he impersonated a physician in New Orleans for nearly a year without any medical\n\nbackground, supervising resident interns. When he didn’t know how to respond after a nurse said a baby had\n\n“gone blue,” he realized the life and death implications of that false identity and decided to make yet another\n\nchange. After forging a law transcript from Harvard University, he kept taking the Louisiana bar exam until\n\nhe passed it, then he posed as an attorney with the attorney general’s office. His story was memorialized in\n\nthe 2002 movie, Catch Me If You Can.\n\n1\n\nThe term refers to a pilot riding as a passenger on a flight to get to work. For example, a pilot who took up an assignment to fly from New York to London would need to first ride as a\n\npassenger to New York if he lived in Denver.\n\nFrank Abagnale was one of the most successful imposters\n\nof all time.\n\nAlthough mountebank cannot guarantee you Abagnale’s\n\nindefatigable confidence, it does give you the ability to mimic one of the other key factors of his success:\n\ntailoring your response to your audience. Had Abagnale",
      "content_length": 1623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "acted like a pilot when facing a room full of medical interns, he would have had a much shorter medical\n\ncareer. Mountebank uses predicates to determine which response to use based on the incoming request, giving\n\nyour imposter the ability pretend to be a virtual pilot for one request and a virtual doctor for the next one.\n\n4.1. THE BASICS OF PREDICATES\n\nTesting a service that depends on a fictional Abagnale service (figure 4.1) is hard work. It involves doing\n\nsomething that has almost certainly never been done before in the history of mocking frameworks: you have to\n\ncreate a virtual imposter that pretends to be a real imposter.\n\nFigure 4.1. The Abagnale service adapts its response to the questions you ask it.\n\nFortunately, mountebank makes this easy. If you assume\n\nyour system under test embeds the question it asks the Abagnale service inside the HTTP body, then your",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "imposter configuration can look something like the following listing.\n\n[2]\n\n2\n\nTo keep the examples as simple as possible, we’ll use HTTP.\n\nListing 4.1. Creating an Abagnale imposter\n\n{\n\n\"protocol\": \"http\", \"port\": 3000,\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"contains\": { \"body\":\n\n\"Which route are you flying?\" }\n\n1\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"Miami to Rio\" }\n\n2\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"startsWith\": { \"body\":\n\n\"Where did you get your degree?\" }\n\n3\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"Harvard Medical School\"\n\n} 4\n\n}]\n\n}, {\n\n\"responses\": [{\n\n\"is\": { \"body\":\n\n\"I'll have to get back to you\" }\n\n5\n\n}]\n\n}\n\n] }\n\n1 When asked a pilot question...\n\n2 ...respond like a pilot.\n\n3 When asked a doctor question...",
      "content_length": 722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "4 ...respond like a doctor.\n\n5 If you don’t know what type of question it is, stall!\n\nEach predicate matches on a request field. The examples in listing 4.1 all match the body, but any of the other HTTP request fields you saw in the previous chapter are fair game: method, path, query, and headers.\n\nIn chapter 2, we showed an example using equals predicates. Our simple Abagnale imposter shows off a couple more possibilities, using contains and startsWith. We’ll look at the range of predicates\n\nshortly, but most of them are pretty self-explanatory. If the body contains the text “Which route are you flying?”\n\nthen the imposter responds with “Miami to Rio,” and if the body starts with the text “Where did you get your\n\ndegree?” the imposter responds with “Harvard Medical School.” This allows you to test the doctor and pilot\n\nservices without depending on the full talents of Mr. Abagnale.\n\nNote in particular the last stub, containing the “I’ll have to get back to you” bit of misdirection. It contains no\n\npredicates, which means that all requests will match it, including those that match the predicates in other stubs. Because a request can match multiple stubs,\n\nmountebank always picks the first match, based on array order. This allows you to represent a fallback default response by putting it at the end of the stubs array without any predicates.\n\nWe haven’t paid too much attention to stubs as a standalone concept because they only make sense in the\n\npresence of predicates. As you saw in the last chapter, it’s possible to send different responses to the exact same request, which is why the responses field is a JSON array. This simple fact, combined with the need to tailor\n\nthe response to the request, is the raison d’etre of stubs.",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Each imposter contains a list of stubs. Each stub contains a circular buffer for the responses. Mountebank selects\n\nwhich stub to use based on the stub’s predicates (figure 4.2).\n\nFigure 4.2. Mountebank matches the request against each stubʼs predicates.\n\nBecause mountebank uses a “first-match” policy on the stubs, having multiple stubs that could respond to the\n\nsame request isn’t a problem.\n\n4.1.1. Types of predicates\n\nLet’s take a closer look at the simplest predicate operators (figure 4.3). These all behave much as you\n\nwould expect. But a few more interesting types of predicates are available, starting with the incredibly useful matches predicate.\n\nFigure 4.3. Mountebank matches the request against each stubʼs predicates.",
      "content_length": 736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "The matches predicate\n\nYour Abagnale service needs to respond intelligently to questions in both the present tense and the past tense.\n\n“Which route are you flying?” and “Which route did you fly?” should both trigger a response of “Miami to Rio.” You could write multiple predicates, but the matches predicate lets you simplify your configuration with the\n\nuse of a regular expression (or regex, as used in figure 4.4).\n\nFigure 4.4. How simple predicates match against the full request field",
      "content_length": 491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Regular expressions include a wonderfully rich set of\n\nmetacharacters to simplify pattern matching. This example uses three:\n\n. — matches any character except a newline\n\n— matches the previous character zero or more times\n\n\\ — escapes the following character, matching it literally\n\nIt may look like we had to double-escape the question mark, but that’s only because \\ is a JSON string escape\n\ncharacter as well as a regular expression escape character. The first \\ JSON-escapes the second \\, which\n\nregex escapes the question mark, because it turns out that a question mark is also a special metacharacter. Like\n\nthe asterisk, it sets an expectation of the previous character or expression, but unlike the * metacharacter,\n\n? matches it only zero times or one time. If you have never seen regular expressions before, that’s a bit\n\nconfusing, so let’s break down your pattern against the",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "request field value of “Which route did you fly?” (See figure 4.5.)\n\nFigure 4.5. How a regular expression matches against a string value\n\nMost characters match one-for-one against the request field. As soon as you reach the first metacharacters (.*),\n\nthe pattern matches as much as it can until the next literal character (a blank space). The wildcard pattern .*\n\nis a simple way of saying “I don’t care what they entered.” As long as the rest of the pattern matches, then\n\nthe entire regular expression matches.",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "The second .* matches because the * allows a zero- character match, which is another way of saying it can be\n\nsatisfied even if it doesn’t match anything. Conveniently, it also would have matched the “ing” if you had entered\n\n“flying” instead of “fly,” which is why regular expressions are so flexible. Finally, the pattern \\? matches the\n\nending question mark.\n\nThe matches predicate is one of the most versatile\n\npredicates in mountebank. With a couple of additional metacharacters, it can completely replace the other\n\npredicates we’ve looked at so far. We will demonstrate that with another example.\n\nReplacing other predicates with regular expressions\n\nA key to Abagnale’s success is that he knew when it was\n\ntime to run. After narrowly escaping arrest in New Orleans, he switched from being a pilot to a doctor with a new identity. When he realized the gravity of his medical\n\nignorance, he switched again to being a lawyer. Recognizing when the jig is up requires seeing suspicious\n\nquestions for what they are.\n\nThe Abagnale service responds with desperation when\n\nasked to see his driver’s license, saying “Catch me if you can!” It gives the same response if the questioner asks for\n\nhis “state’s driver’s license,” or his “current driver’s license,” so you’ll have to be a little loose with your\n\nmatching.\n\nYou will need to match if the question contains the phrase “driver’s license.” You want to confirm that those\n\ncharacters represent a question by ensuring not only that they contain a question mark, but that they end with a\n\nquestion mark, and, to make absolutely sure that you are catching the right question, you will also ensure that the\n\nphrase starts with “Can I see your.” Note that you can",
      "content_length": 1715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "achieve all of this without regular expressions, but it would require combining multiple predicates, as follows:\n\n{\n\n\"predicates\": [\n\n{ \"startsWith\": { \"body\": \"Can I see your\" }\n\n}, { \"contains\": { \"body\": \"driver's license\" }\n\n},\n\n{ \"endsWith\": { \"body\": \"?\" } }\n\n]\n\n}\n\nYou can match exactly the same bodies with one matches predicate, as shown in the following listing.\n\nListing 4.2. Using the matches predicate to do the job of a set of startsWith, contains, and endsWith predicates\n\n{\n\n\"predicates\": [{\n\n\"matches\": { \"body\": \"^Can I see\n\nyour.*driver's license.*\\\\?\" } }],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"Catch me if you can!\" }\n\n}]\n\n}\n\nYou’ve already seen how the .* metacharacters match any characters, or no characters at all. Wrapping text\n\nwith those metacharacters on either side is equivalent to using a contains predicate (figure 4.6).\n\nFigure 4.6. Emulating the contains predicate with a regular expression\n\nThe ^ metacharacter matches only if the following character occurs at the beginning of the string, which allows you to recreate the startsWith predicate (figure 4.7).",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure 4.7. Emulating the startsWith predicate with a regular expression\n\nFinally, the $ metacharacter matches only if the preceding character occurs at the end of the string, which mimics the endsWith predicate (figure 4.8).\n\nFigure 4.8. Emulating the endsWith predicate with a regular expression\n\nThe beauty of regular expressions is that you can\n\ncombine all of those criteria into a single pattern (figure 4.9).\n\nFigure 4.9. Emulating the startsWith, contains, and endsWith predicates with one regular expression\n\nIf you remove the .* metacharacter but leave in the ^ and $ metacharacters that anchor a match at the\n\nbeginning and end of a string of text, you create what equates to the equals predicate, although in this case\n\nthe equals predicate is more readable (figure 4.10).\n\nFigure 4.10. Emulating the equals predicate with a regular expression\n\nRegular expression patterns can greatly simplify your use of predicates. Let’s look at a more common use case of the matches predicate next.\n\nMatching any identifier on the path",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Although our predicates so far have focused on the http body field, predicates can work on any request field. A\n\ncommon pattern is to match the path field. Frank Abagnale took on a number of names, and in typical\n\nRESTful fashion, your Abagnale service allows you to query them by sending a GET request to /identities,\n\nor see the details about a single persona by looking at /identities/{id}, where {id} is the identifier for\n\nthat particular identity. Let’s start by matching the /identities/{id} path.\n\nIf you had a test scenario that involved hitting this endpoint, you could use the matches predicate to match\n\nany numeric identifier passed in, as shown in the following listing.\n\nListing 4.3. Using the matches predicate to match any identity resource\n\n{\n\n\"predicates\": [{\n\n\"matches\": { \"path\": \"/identities/\\\\d+\" }\n\n}],\n\n\"responses\": [{ \"is\": {\n\n\"body\": {\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n}\n\n}\n\n}] }\n\nThe metacharacters used here, \\d+, represent one or\n\nmore digits, so the pattern will match /identities/123 and /identities/2 but not\n\nidentities/frank-williams. Several other useful metacharacters are available, including (but not limited\n\nto!) the ones listed in table 4.1.\n\nTable 4.1. Regular expression metacharacters",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Metacharacter\n\nDescription\n\nExample\n\n\\\n\n^\n\n$\n\n. *\n\n?\n\n+\n\n\\d \\D\n\n\\w\n\n\\W\n\n\\s\n\n\\S\n\nUnless it’s part of a metacharacter like those described below, it escapes the next character, forcing a literal match. Matches the beginning of the string\n\nMatches the end of the string\n\nMatches any non-newline character Matches the previous character 0 or more times Matches the previous character 0 or 1 times Matches the previous character 1 or more times Matches a digit Inverts \\d, matching nondigit characters Matches an alphanumeric “word” character Inverts \\w, matching nonalphanumeric symbols Matches a whitespace character (mainly spaces, tabs, and newlines) Inverts \\s, matching any non-space character\n\n4 \\* 2\\? matches “What is 4 * 2?”\n\n^Hello matches “Hello, World!” but not “Goodbye. Hello.” World!$ matches “Hello, World!” but not “World! Hello.” ..... matches “Hello” but not “Hi” a*b matches “b” and “ab” and “aaaaaab” a?b matches “b” and “ab” but not “aab”\n\na+b matches “ab” and “aaaab” but not “b” \\d\\d\\d matches “123” but not “12a” \\D\\D\\D matches “ab!” but not “123”\n\n\\w\\w\\w matches “123” and “abc” but not “ab!” \\W\\W\\W matches “!?.” but not “ab.”\n\nHello\\sworld matches “Hello world” and “Hello world” Hello\\Sworld matches “Hello-world” and “Hello----world”\n\nRegular expressions allow you to define robust patterns to match characters and are a rich subject in their own\n\nright. Several excellent books are available on the subject, and a number of internet sites provide tutorials.\n\nIf you are looking for a quick start, I recommend the tutorials on http://www.regular-expressions.info/. We look at\n\nmore examples in chapter 7.\n\n4.1.2. Matching object request fields\n\nGoogle supports full-text searching using the q querystring parameter, so, for example,\n\nhttps://www.google.com/?q=mountebank will show web pages that are somehow relevant to the search text\n\n“mountebank.” Other web services, like the Twitter API, have adopted the q parameter as a search option even\n\nwhen searching more JSON-structured data like your Abagnale service. Having a single search parameter\n\nallows a Google-like user experience with a single text box, where the user doesn’t have to specify the fields they\n\nare matching. They don’t even have to match a field completely. Implementing a full-text search can be a",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "little tricky, but you don’t need to worry about that; you need to pretend to be a service that implements full-text\n\nsearching.\n\nThe /identities path for your Abagnale service\n\nsupports searching using the q querystring parameter. For example, /identities?q=Frank will search for all\n\nof Abagnale’s identities that are somehow relevant to “Frank,” which you can use as a shortcut to find those\n\nidentities where he used his real first name. The predicate for querystring parameters looks a little\n\ndifferent, but only because the querystring is an object field instead of a string field, as shown in the following\n\nlisting.\n\nListing 4.4. Adding a predicate for a query parameter\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n\"query\": { \"q\": \"Frank\" } 1\n\n}\n\n}],\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": { \"identities\": [ 2\n\n{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n},\n\n{\n\n\"name\": \"Frank Adams\", \"career\": \"Brigham Young Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\n}\n\n}]\n\n}\n\n1 Because query is an object field, the predicate value is\n\nalso an object.",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "2 Returns an array\n\nFor HTTP requests, both query and headers are object\n\nfields. To get to the right query parameter (or header), you have to add an extra level to your predicate.\n\n4.1.3. The deepequals predicate\n\nIn some situations, you want to match only if no query\n\nparameters were passed; for example, sending a GET to /identities without any searching or paging\n\nparameters should return all identities. None of the predicates shown so far supports this scenario, as they all\n\nwork on a single request field. For more complex key- value pair structures, like HTTP queries and headers, the\n\nother predicates expect you to navigate down to the primitive field inside, like the q parameter within the\n\nquerystring we just looked at.\n\nThe deepEquals predicate matches an entire object structure, allowing you to specify an empty querystring:\n\n{\n\n\"deepEquals\": { \"query\": {} }\n\n}\n\nShortly you will see how it’s possible to combine multiple predicates, which allows you to require two query parameters. But the deepEquals predicate is the only way to guarantee that those two query parameters and\n\nnothing else are passed:\n\n{\n\n\"deepEquals\": {\n\n\"query\": {\n\n\"q\": \"Frank\", \"page\": 1\n\n}\n\n}\n\n}",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "With this predicate, a querystring of ?q=Frank&page=1 would match, but a querystring of ?\n\nq=Frank&page=1&sort=desc wouldn’t.\n\n4.1.4. Matching multivalued fields\n\nAnother interesting characteristic of HTTP query\n\nparameters and headers is that you can pass the same key multiple times. Your Abagnale service supports multiple q parameters and returns only those matches that satisfy all of the provided queries. For example, GET\n\n/identities?q=Frank&q=Georgia would return only Frank Williams, because Frank Adams worked in\n\nUtah.\n\n{\n\n\"identities\": [{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\" }]\n\n}\n\nAll of the predicates we looked at so far support multivalued fields, but, once again, deepEquals is significantly different from the others. If you use an equals predicate, the predicate will pass if any of the values equals the predicate value:\n\n{\n\n\"equals\": { \"query\": {\n\n\"q\": \"Frank\"\n\n}\n\n}\n\n}\n\nThe deepEquals predicate requires all of the values to match. Mountebank represents such multivalue fields as\n\n[3]",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "[3]\n\narrays in the request. look something like this in mountebank:\n\nThis particular request would\n\n3\n\nYou also can use this trick when creating responses with multivalue fields. This is most commonly seen with the Set-Cookie response header.\n\n{\n\n\"method\": \"GET\",\n\n\"path\": \"/identities\",\n\n\"query\": {\n\n\"q\": [\"Frank\", \"Georgia\"]\n\n} }\n\nThe trick is to pass the array as the predicate value:\n\n{\n\n\"deepEquals\": {\n\n\"query\": {\n\n\"q\": [\"Georgia\", \"Frank\"] }\n\n}\n\n}\n\nNote that the order of the values doesn’t matter. You can\n\nuse the array syntax for any predicate, not just deepEquals, but deepEquals is the only one that\n\nrequires an exact match. The example in the following listing demonstrates the difference.\n\nListing 4.5. Using predicate arrays\n\n{\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": {\n\n1 \"query\": { \"q\": [\"Frank\", \"Georgia\"] }\n\n1\n\n}\n\n1\n\n}],",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "\"responses\": [{\n\n\"is\": { \"body\": \"deepEquals matched\" }\n\n}]\n\n},\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n2\n\n\"query\": { \"q\": [\"Frank\", \"Georgia\"] }\n\n2\n\n}\n\n2\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"equals matched\" }\n\n}]\n\n}\n\n]\n\n}\n\n1 Requires exact match\n\n2 Requires these elements to be present\n\nIf you send a request to /identities?q=Georgia&q=Frank, the response body will show that the deepEquals\n\npredicate matched because all of the array elements matched and no additional array elements were present\n\nin the request. But if you send a request to /identities? q=Georgia&q=Frank&q=Doctor, the deepEquals\n\npredicate will no longer match because the predicate definition isn’t expecting “Doctor” as an array element. The equals predicate will match, because it allows additional elements in the request array that aren’t\n\nspecified in the predicate definition.\n\n4.1.5. The exists predicate\n\nThere’s one more primitive predicate to look at. The exists predicate tests for either the existence or\n\nnonexistence of a request field. If you have a test that depends on the q parameter to be passed and the page\n\nparameter not being passed, then exists is what you want:",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "{\n\n\"exists\": {\n\n\"query\": {\n\n\"q\": true,\n\n\"page\": false\n\n}\n\n} }\n\nThe exists predicate also comes in quite handy when\n\nyou want to check for the presence of a header. For example, you may decide that for testing purposes, you\n\nwant to verify that the service handles an HTTP challenge correctly (represented by a 401 status code) when the Authorization request header is missing, without worrying about whether the credentials stored in the Authorization header are correct, as shown here:\n\n{\n\n\"predicates\": [{\n\n\"exists\": {\n\n\"headers\": { \"Authorization\": false }\n\n}\n\n}], \"responses\": [{\n\n\"is\": { \"statusCode\": 401 }\n\n}]\n\n}\n\nThe headers field in this snippet specifies the condition\n\nthat there’s no Authorization header, and the is response returns a 401 status code.\n\nThe exists predicate works on string fields like the\n\nbody, which is considered not to exist if it’s an empty string. It’s usually more useful in conjunction with the\n\nJSON or XML support described later in this chapter.\n\n4.1.6. Conjunction junction",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "The predicates field is an array. Every predicate in the array must match for mountebank to use that stub. You can reduce the array to a single element by using the and predicate, so the following two sets of predicates will\n\nmatch the exact same requests (for example, one with a body of “Frank Abagnale”):\n\n{\n\n\"predicates\": [\n\n{ \"startsWith\": { \"body\": \"Frank\" } },\n\n{ \"endsWith\": { \"body\": \"Abagnale\" } } ]\n\n}\n\nand\n\n{\n\n\"predicates\": [{ \"and\": [\n\n{ \"startsWith\": { \"body\": \"Frank\" } },\n\n{ \"endsWith\": { \"body\": \"Abagnale\" } }\n\n]\n\n}]\n\n}\n\nBy itself, the and predicate isn’t very useful. But combined with its conjunction cousin, the or predicate,\n\nand its distant disjunction relative, the not predicate, you can create predicates of dizzying complexity in a\n\nfestival of Booleanism. For example, the predicate in listing 4.6 matches requests that return Frank Williams,\n\nregardless of whether the system under test directly calls that persona URL (assumed to be /identities /123)\n\nor they search for him at /identities? q=Frank+Williams, but only if no page query\n\nparameter is added.\n\nListing 4.6. Combining multiple predicates using and, or, and not\n\n{\n\n\"or\": [",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "{ \"equals\": { \"path\": \"/identities/123\" } },\n\n1\n\n{\n\n\"and\": [\n\n{ \"equals\": { \"path\": \"/identities\" } }, 2\n\n{\n\n\"and\": [\n\n{\n\n\"contains\": { \"query\": { \"q\":\n\n\"Frank\" } } 3\n\n},\n\n{ \"contains\": { \"query\": { \"q\":\n\n\"Williams\" } } 4\n\n},\n\n{\n\n\"not\": {\n\n5\n\n\"exists\": { \"query\": { \"page\":\n\ntrue } } 5 }\n\n5\n\n}\n\n]\n\n}\n\n]\n\n}\n\n] }\n\n1 Matches if the request goes directly to the URL...\n\n2 ...or if it searches\n\n3 ...with a query containing Frank\n\n4 ...and a query containing Williams\n\n5 ...with no paging (you can get rid of the not predicate by\n\nchanging the page value to false).\n\nSometimes it’s necessary to create complex conditions, and the rich set of predicates mountebank supports\n\nenables you to specify such conditions. But to make your configuration readable and maintainable, it’s good\n\npractice to make the predicates as simple as possible for your use case. The conjunctions are there when you need\n\nthem, but you’ll probably be happier if you can avoid using them too much.",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "4.1.7. A complete list of predicate types\n\nThere’s one other predicate you haven’t seen yet—the inject predicate—but you’ll have to wait until chapter 6 to take a look at it. Before we proceed, let’s review the\n\npredicates you have at your disposal. For your reference, table 4.2 provides the complete list of predicate operators\n\nthat mountebank supports.\n\nTable 4.2. All predicates that mountebank supports\n\nOperator\n\nDescription\n\nequals deepEquals contains startsWith endsWith Matches\n\nexists\n\nnot or and inject\n\nRequires the request field to equal the predicate value Performs nested set equality on object request fields Requires the request field to contain the predicate value Requires the request field to start with the predicate value Requires the request field to end with the predicate value Requires the request field to match the regular expression provided as the predicate value Requires the request field to exist as a nonempty value (if true) or not (if false) Inverts the subpredicate Requires any of the subpredicates to be satisfied Requires all of the subpredicates to be satisfied Requires a user-provided function to return true (see chapter 6)\n\n4.2. PARAMETERIZING PREDICATES\n\nEach predicate consists of an operator and zero or more parameters that alter the behavior of the predicate in certain ways. Two parameters, xpath and jsonpath, change the scope of the predicate to a value embedded in\n\nthe HTTP body; we look at those shortly. The other parameter affects the way the predicate evaluates the\n\nrequest field.\n\n4.2.1. Making case-sensitive predicates\n\nAll predicates are case-insensitive by default. The following predicate, for example, will be satisfied regardless of whether the q parameter is “Frank” or “frank” or “FRANK”:",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "{\n\n\"equals\": {\n\n\"query\": { \"q\": \"frank\" }\n\n}\n\n}\n\nThis is even true for the matches predicate. Regular expressions are case-sensitive by default, but\n\nmountebank changes their default to match the behavior of the other predicates. If you need case sensitivity, you can set the caseSensitive parameter to true, as follows.\n\nListing 4.7. Using a case-sensitive predicate\n\n{\n\n\"equals\": {\n\n\"query\": { \"q\": \"Frank\" }\n\n},\n\n\"caseSensitive\": true\n\n}\n\n“FRANK” and “frank” will no longer satisfy the predicate.\n\nMountebank’s default behavior also treats the keys in a case-insensitive manner, so that without the caseSensitive parameter, the predicate above also would match a querystring of ?Q=FRANK. This is often\n\nappropriate, especially for HTTP headers where the case of the headers shouldn’t matter. Adding the caseSensitive parameter forces case sensitivity on both the keys and the values.\n\n4.3. USING PREDICATES ON JSON VALUES\n\nJSON is the lingua franca of most RESTful APIs these\n\ndays. As you have seen previously, it’s possible to create mountebank responses that use JSON objects instead of strings for the body field. Mountebank also provides ample support for creating predicates against JSON\n\nbodies.",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "4.3.1. Using direct JSON predicates\n\nDespite Frank Abagnale’s cleverness, there’s nothing\n\nmagical about what he does. When he needs to add a new identity, for example, he POSTs the JSON representation\n\nof the identity to the /identities URL, like the rest of us.\n\nBecause the HTTP body is a string as far as mountebank is concerned, you can use a contains predicate to capture a particular JSON field. But doing so is\n\ninconvenient, as the white space has to match between the key and the value. The matches predicate gives you\n\na lot more flexibility at the cost of readability. Fortunately for you, mountebank’s willingness to treat\n\nHTTP bodies as JSON as well as strings allows you to navigate the JSON object structure like you have navigated the query object structure previously, as shown in the following listing.\n\nListing 4.8. Using a direct JSON predicate on an HTTP body\n\n{\n\n\"predicates\": [{\n\n\"equals\": {\n\n\"body\": { 1\n\n\"career\": \"Doctor\" 1\n\n} 1\n\n} }],\n\n\"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 201,\n\n\"headers\": { \"Location\": \"/identities/123\"\n\n},\n\n\"body\": \"Welcome, Frank Williams\"\n\n} }]\n\n}\n\n1 The “career” field at the root of the JSON body has to\n\nequal “Doctor.”",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "You can navigate as many levels deep in the JSON object as you need to. Mountebank treats arrays no differently\n\nfrom how it treats the multivalued fields described in section 4.1.4, which used the example of a repeating key\n\non the querystring. For complex queries, you are probably better off using JSONPath.\n\n4.3.2. Selecting a JSON value with JSONPath\n\nYou can initialize the set of identities that the Abagnale service provides by sending a PUT command to the\n\n/identities path, passing in an array of identities, as shown here:\n\n{ \"identities\": [\n\n{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n},\n\n{\n\n\"name\": \"Frank Adams\", \"career\": \"Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\nThis test scenario requires you to send a 400 if the PUT command includes a career of “Teacher” as the last\n\nmember of the array and a 200 otherwise. That is obviously a bit of a stretch, but it enables you to show off\n\nthe power of JSONPath. JSONPath is a query language that simplifies the task of selecting values from a JSON\n\ndocument and excels with large and complex documents. Stefan Goessner came up with the idea and documented\n\nits syntax at http://goessner.net/articles/JsonPath/.\n\nLet’s look at the entire imposter configuration in the\n\nfollowing listing.",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Listing 4.9. Using JSONPath to match only the last element of an array\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000, \"stubs\": [{\n\n\"predicates\": [\n\n{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } },\n\n1\n\n{\n\n\"jsonpath\": { 2\n\n\"selector\": \"$.identities[(@.length-\n\n1)].career\" 2\n\n},\n\n2\n\n\"equals\": { \"body\": \"Teacher\" }\n\n3\n\n} ],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 }\n\n}] 4\n\n}]\n\n5\n\n}\n\n1 Only matches a PUT request to /identities\n\n2 Limits the scope of the predicate to the JSONPath\n\nquery\n\n3 The JSON value selected within the body must equal\n\n“Teacher.”\n\n4 Returns a 400 status code\n\n5 Returns the built-in default 200 response if the\n\npredicate doesn’t match\n\nYou set the predicate operator to have the body equal\n\nTeacher, even though the body contains an entire JSON document. The jsonpath parameter modifies its\n\nattached predicate and limits its scope to the result of the query. Let’s take a look at the query as annotated in figure\n\n4.11.\n\nFigure 4.11. Breaking down a JSONPath query",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "JSONPath provides tremendous flexibility for selecting\n\nthe value you need in a large JSON document. Its older cousin, XPath, does the same thing for XML documents.\n\n4.4. SELECTING XML VALUES\n\nAlthough XML isn’t as common in services created in\n\nrecent years, it’s still a prevalent service format and is universally used for SOAP services. If you allowed sending XML as well as JSON with the PUT call to /identities that we just looked at—perhaps because\n\nAbagnale needs to impersonate an enterprise architect— you could expect a body that looks like this:\n\n<identities>\n\n<identity career=\"Doctor\">\n\n<name>Frank Williams</name> <location>Georgia</location>\n\n</identity>\n\n<identity career=\"Teacher\">\n\n<name>Frank Adams</name>\n\n<location>Utah</location>\n\n</identity>\n\n</identities>",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Brigham Young University disputes Abagnale’s account of teaching there. Our test scenario could detect\n\nAbagnale trying to claim he was a teacher in Utah and send a 400 status code. This is a complex enough query\n\nthat the existing predicate operators are not up to the task.\n\n[4]\n\nIt also involves using XML attributes in the\n\nquery.\n\n4\n\nNot even the matches predicate, because you cannot parse XML (or HTML) with regular\n\nexpressions. Trying to parse XML with regular expressions causes the unholy child to weep the blood of virgins: http://stackoverflow.com/questions/1732348/regex-match-open-tags-except- xhtml-self-contained-tags/1732454#1732454.\n\nIn the simple case shown in the following listing, the xpath parameter mirrors the jsonpath parameter,\n\nlimiting the scope of what the predicate operator examines.\n\nListing 4.10. Using XPath to prevent Abagnale from claiming he taught in Utah\n\n{\n\n\"predicates\": [\n\n{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } }, 1\n\n{\n\n\"xpath\": {\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\" 2\n\n},\n\n\"equals\": { \"body\": \"Utah\" } 3\n\n}\n\n],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 } }]\n\n4\n\n}\n\n1 Verifies this is a PUT to /identities\n\n2 Limits the predicate to the given value\n\n3 The value must equal “Utah.”",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "4 Returns a Bad Request\n\nXPath predates JSONPath, and unsurprisingly, their\n\nsyntax is similar. Figure 4.12 breaks down the XPath expression.\n\nFigure 4.12. Breaking down an XPath query\n\nOne of the most unfortunate design decisions in XML is\n\nthe support for namespaces, which has caused needless harm to walls the world over as programmers\n\neverywhere banged their heads on them when confronted with namespaced documents. The idea is\n\nsensible enough: as you combine multiple XML documents, you need a way of resolving naming\n\nconflicts.\n\n[5]\n\n5\n\nAt least we thought we did. Oddly, the same problem should exist with JSON documents, which lack namespaces, but...\n\nLet’s future-proof your XML document by adding\n\nnamespaces:",
      "content_length": 723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "<identities xmlns:id=\"https://www.abagnale-\n\nspec.com/identity\"\n\nxmlns:n=\"https://www.abagnale-\n\nspec.com/name\">\n\n<id:identity career=\"Doctor\">\n\n<n:name>Frank Williams</n:name>\n\n<location>Georgia</location> </id:identity>\n\n<id:identity career=\"Teacher\">\n\n<n:name>Frank Adams</n:name>\n\n<location>Utah</location>\n\n</id:identity>\n\n</identities>\n\nDespite your best attempts at future-proofing, you still require a breaking change in version 2 of the Abagnale\n\nservice that moves the name value to an attribute instead of an XML tag:\n\n<identities xmlns:id=\"https://www.abagnale-\n\nspec.com/identity\" xmlns:n=\"https://www.abagnale-\n\nspec.com/name\">\n\n<id:identity career=\"Doctor\" n:name=\"Frank\n\nWilliams\">\n\n<location>Georgia</location>\n\n</id:identity>\n\n<id:identity career=\"Teacher\" n:name=\"Frank\n\nAdams\"> <location>Utah</location>\n\n</id:identity>\n\n</identities>\n\nYou need to write a test scenario that verifies that the\n\nAbagnale service responds with a 400 if you pass the name field in the wrong spot, as in the following listing.\n\nThis is a great opportunity to use the exists predicate operator. You also have to add namespaces to your query, as name isn’t the same as n:name in XML.\n\nListing 4.11. Using XPath to assert that the name attribute exists and the name tag doesnʼt\n\n{\n\n\"predicates\": [",
      "content_length": 1293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "{ \"equals\": { \"method\": \"PUT\" } },\n\n1\n\n{ \"equals\": { \"path\": \"/identities\" } },\n\n1\n\n{ \"or\": [\n\n2\n\n{\n\n\"xpath\": {\n\n\"ns\": {\n\n\"i\":\n\n\"https://www.abagnale-\n\nspec.com/identity\", 3 \"n\": \"https://www.abagnale-\n\nspec.com/name\" 3\n\n},\n\n\"selector\": \"//i:identity/n:name\"\n\n4\n\n},\n\n\"exists\": { \"body\": true }\n\n4 },\n\n{\n\n\"xpath\": {\n\n\"selector\": \"//i:identity[@n:name]\",\n\n5\n\n\"ns\": {\n\n\"i\": \"https://www.abagnale-\n\nspec.com/identity\", \"n\": \"https://www.abagnale-\n\nspec.com/name\"\n\n}\n\n},\n\n\"exists\": { \"body\": false }\n\n5\n\n}\n\n] }\n\n],\n\n\"responses\": [{ \"is\": { \"statusCode\": 400 } }]\n\n6\n\n}\n\n1 Verifies this is a PUT to /identities\n\n2 There are two possible scenarios.\n\n3 Adds the namespace map\n\n4 Matches if the n:name tag exists...\n\n5 ...or if the n:name attribute doesn’t exist",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "6 Sends a Bad Request if the predicates match.\n\nThe xpath parameter allows you to pass in a namespace\n\nmap in the ns field, which takes a prefix and a URL. The URL has to match the one defined in the XML document,\n\nbut the prefix can be whatever you want. XPath namespace queries use the prefix in front of each\n\nelement.\n\nAnd with that beast of a stub, it’s time to take a step back and review what you learned.\n\nSUMMARY\n\nPredicates allow mountebank to respond differently to different\n\nrequests. Mountebank ships with a full range of predicate operators, including the versatile matches operator, which matches request fields based on a regular expression.\n\nThe deepEquals predicate operator is used to match an entire object structure, such as the query object. You also can match a\n\nsingle field within the object (for example, a single query parameter) with one of the standard predicate operators by\n\nnavigating into the object structure.\n\nPredicates are case-insensitive by default. You can change that by setting the caseSensitive predicate parameter to true.\n\njsonpath and xpath predicate parameters limit the scope on the request field to the part that matches the JSONPath or XPath\n\nselector.",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 5. Adding record/replay behavior\n\nThis chapter covers\n\nUsing proxy responses to capture real responses automatically\n\nReplaying the saved responses with the correct predicates\n\nCustomizing proxies by changing the headers or using mutual\n\nauthentication\n\nThe best imposters don’t simply pretend to be someone\n\nelse; they actively copy the person they impersonate. This mimicry requires both observation and memory:\n\nobservation to study the behaviors of the person being impersonated and memory to be able to replay those\n\nbehaviors at a later time. Satirists on comedy shows like Saturday Night Live, where actors and actresses often\n\npretend to be famous U.S. political figures, base their performances on those skills.\n\nMountebank lacks the comedic flair of Saturday Night Live impersonators, but it does support a high-fidelity\n\nform of mimicry. Rather than creating a canned response for each request, an imposter can go directly to the source. It’s as if the imposter is wearing an earpiece, and\n\nevery time your system under test asks it a question, the real service whispers the answer in your imposter’s ear.\n\nBetter yet, mountebank imposters have a great memory, so once the imposter has heard a response, it can replay\n\nthe response in the future even without the earpiece. Thanks to the magic of proxy responses, a mountebank\n\nimposter can be almost indistinguishable from the real thing.\n\n5.1. SETTING UP A PROXY\n\nThe proxy response type lets you put a mountebank imposter between the system under test and a real",
      "content_length": 1533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "service that it depends on, saving a real response in the process (figure 5.1).\n\nFigure 5.1. An imposter acting as a proxy\n\nThis arrangement allows capturing real data that you can replay in tests, rather than hand-creating it using canned\n\nresponses. To illustrate, let’s revisit the imaginary pet store architecture you first saw in chapter 3. The pet store,\n\nlike all modern e-commerce shops, needs a service to keep track of inventory, and, to keep it simple, ours takes\n\na product ID on the URL and sends back the on-hand stock for that product. In chapter 3, you virtualized it with hand-created is response types. Let’s reimagine it using a proxy, which requires the real service to be available to\n\ncapture the responses, as shown in figure 5.2.\n\nFigure 5.2. Using a proxy to query the downstream inventory\n\nThe simplest imposter configuration looks like the following listing.\n\n[1]\n\n1\n\nYou can follow the examples from the GitHub repository at https://github.com/bbyars/mountebank-in-action.\n\nListing 5.1. Imposter configuration for a basic proxy",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"http://api.petstore.com\"\n\n} 1 }]\n\n}]\n\n}\n\n1 New response type\n\nThe difference is in the new response type. Whereas an is response tells the imposter to return the given response, a proxy response tells the imposter to fetch\n\nthe response from the downstream service. The basic form of the proxy response shown in the listing passes\n\nthe request unchanged to the downstream service and sends the response unchanged back to the system under\n\ntest. By itself, that isn’t a very useful thing, but the proxy remembers the response and will replay it the next time it sees the same request, rather than fetching a new\n\nresponse (figure 5.3).\n\nFigure 5.3. By default, the proxy returns the first result as the response to all subsequent calls.\n\nMountebank exposes the current state of each imposter through the API. If you send a GET request to http://localhost:2525/imposters/3000, you will see the\n\n[2]\n\nsaved response. imposter configuration in some detail after the first call\n\nIt’s worth looking at the changed\n\nto the proxy, as shown in the following listing.\n\n2",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "The 3000 at the end of the URL is the imposter’s port.\n\nListing 5.2. Saved proxy responses change imposter state\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"numberOfRequests\": 1,\n\n\"requests\": [], \"stubs\": [{\n\n\"predicates\": [],\n\n1\n\n\"responses\": [{\n\n\"is\": {\n\n2\n\n\"statusCode\": 200,\n\n\"headers\": { \"Connection\": \"close\",\n\n\"Date\": \"Sat, 15 Apr 2017 17:04:02\n\nGMT\",\n\n\"Transfer-Encoding\": \"chunked\"\n\n},\n\n\"body\": \"54\",\n\n\"_mode\": \"text\",\n\n\"_proxyResponseTime\": 10 3\n\n}\n\n}]\n\n},\n\n{\n\n4\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyOnce\"\n\n5\n\n}\n\n}]\n\n}],\n\n\"_links\": {\n\n\"self\": { \"href\":\n\n\"http://localhost:2525/imposters/3000\" 6\n\n}\n\n}\n\n}\n\n1 No predicates? We’ll come back to that....?\n\n2 Saves the response as an is response\n\n3 Saves the time to call the downstream service",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "4 Your original stub is still there.\n\n5 Only calls downstream once\n\n6 The URL you called to get the configuration\n\nThere’s a lot in there, and we aren’t quite ready to get to all of it yet. Mountebank recorded the time it took to call the downstream service in the _proxyResponseTime\n\nfield; you can use this to add simulated latency during performance testing. We explore how to do that in\n\nchapters 7 and 10. The most important observations for now are:\n\nMountebank proxies the first call to the base URL given in the to field of the proxy configuration. It appends the request path and query parameters and passes through the request headers and\n\nbody unchanged.\n\nMountebank captures the response as a new stub with an is response. It saves it in front of the stub with the proxy response. (This is what the proxyOnce mode means; we will look at the alternative shortly.)\n\nThe newly created stub has no predicates. Because mountebank\n\nalways uses the first match when iterating over stubs, it will never\n\nagain call the proxy response, because a stub with no predicates\n\nalways matches.\n\nProxies change the state of the imposter. By default, they create a new stub (figure 5.4).\n\nFigure 5.4. The proxy saves the downstream response in a new stub.",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "The default behavior of a proxy (defined by the proxyOnce mode) is to call the downstream service the\n\nfirst time it sees a request it doesn’t recognize, and from that point forward to send the saved response back for\n\nfuture requests that look similar. Unfortunately, the example we’ve been considering isn’t discriminatory in\n\nhow it recognizes requests; all requests match the generated stub. Let’s fix that.\n\n5.2. GENERATING THE CORRECT PREDICATES\n\nProxies will create new responses formed from the downstream service response, but you need to give them\n\nhints on how to create the request predicates that determine when mountebank will replay those\n\nresponses. We will start by looking at how you can replay different responses for different request paths.\n\n5.2.1. Creating predicates with predicateGenerators\n\nYour inventory service includes the product id on the path, so sending a GET to /inventory/2599b7f4 returns the inventory for product 2599b7f4, and a GET to\n\n/inventory/e1977c9e returns inventory for product e1977c9e. Let’s augment the proxy definition you set up\n\nin listing 5.1 to save the response for each product separately. Because it’s the path that varies between\n\nthose requests, you need to tell mountebank to create a new is response with a path predicate. You do so using\n\na proxy parameter called predicateGenerators. As the name indicates, the predicateGenerators are\n\nresponsible for creating the predicates on the saved responses. You add an object for each predicate you want to generate underneath a matches key, as in the following listing.\n\nListing 5.3. Imposter response that saves a diﬀerent response for each path\n\n{\n\n\"proxy\": {",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "\"to\": \"http://localhost:8080\", 1\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true 2\n\n} }]\n\n}\n\n}\n\n1 Proxy new paths to the given base URL...\n\n2 ...but generate a predicate for each new path\n\nYou can test it out with a couple calls using different\n\npaths:\n\ncurl http://localhost:3000/inventory/2599b7f4\n\ncurl http://localhost:3000/inventory/e1977c9e\n\nLet’s look at the state of the imposter again after the\n\nchange:\n\ncurl http://localhost:2525/imposters/3000\n\nThe stubs field contains the newly created responses\n\nwith their predicates, as in the following listing.\n\nListing 5.4. Saved proxy responses with predicates\n\n{ \"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/2599b7f4\" } 1\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"body\": \"54\" } 2\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/e1977c9e\" } 3\n\n}],",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "\"responses\": [{\n\n\"is\": { \"body\": \"100\" }\n\n4\n\n}]\n\n}, {\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true\n\n} }],\n\n\"mode\": \"proxyOnce\"\n\n}\n\n}]\n\n}\n\n]\n\n}\n\n1 Saves the first call as the first stub\n\n2 To save space, most of the response isn’t shown.\n\n3 Creates a new stub with a different predicate\n\n4 The response will be different.\n\nThe generated predicates use deepEquals for most\n\ncases. Recall that the deepEquals predicate requires that all fields be present for object fields like query and\n\nheaders, so if you include either of those using the simple syntax shown in listing 5.4, the complete set of\n\nquerystring parameters or request headers would have to be present in a subsequent request for mountebank to\n\nserve the saved response:\n\n{\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true,\n\n\"query\": true 1 }\n\n}]\n\n}",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "1 All query parameters will need to match.\n\nAs you saw in the last chapter, when defining predicates\n\nfor object fields, you can be more specific if you need to be. If, for example, you want to save a different response for each different path and page query parameter, regardless of what else is on the querystring, you navigate into the query object:\n\n{\n\n\"predicateGenerators\": [{\n\n\"matches\": {\n\n\"path\": true,\n\n\"query\": { \"page\": true 1\n\n}\n\n}\n\n}]\n\n}\n\n1 Only the page parameter needs to match\n\n5.2.2. Adding predicate parameters\n\nThe predicateGenerators field closely mirrors the\n\nstandard predicates field and accepts all the same parameters. Each object in the predicateGenerators\n\narray generates a corresponding object in the newly created stub’s predicates array. If, for example, you\n\nwanted to generate a case-sensitive match of the path and a case-insensitive match of the body, you could add\n\ntwo predicateGenerators, as shown in the following listing.\n\nListing 5.5. Generating case-sensitive predicates\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [\n\n{ 1\n\n\"matches\": { \"path\": true }, 1 \"caseSensitive\": true 1",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "}, 1\n\n{ 2\n\n\"matches\": { \"body\": true } 2\n\n} 2\n\n] }\n\n}]\n\n}\n\n1 Generates a case-sensitive predicate\n\n2 Generates a default case-insensitive predicate\n\nThe newly created stub has both predicates:\n\n{\n\n\"predicates\": [\n\n{ \"caseSensitive\": true,\n\n\"deepEquals\": { \"path\": \"...\" }\n\n},\n\n{\n\n\"deepEquals\": { \"body\": \"...\" }\n\n}\n\n],\n\n\"responses\": [{ \"is\": { ... }\n\n}]\n\n}\n\nAs you saw in the last chapter, more parameters are available beyond caseSensitive. The jsonpath and xpath predicate parameters allow you to limit the scope\n\nof the predicate, and you can generate those too.\n\nGenerating JSONPath predicates\n\nIn chapter 4, we demonstrated JSONPath predicates in the context of virtualizing the inimitable Frank Abagnale\n\nservice, which showed a list of fake identities the famous (real) imposter had assumed. A partial list of identities might look like this:\n\n{\n\n\"identities\": [",
      "content_length": 869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "{\n\n\"name\": \"Frank Williams\",\n\n\"career\": \"Doctor\",\n\n\"location\": \"Georgia\"\n\n}, {\n\n\"name\": \"Frank Adams\",\n\n\"career\": \"Teacher\",\n\n\"location\": \"Utah\"\n\n}\n\n]\n\n}\n\nIf you needed a predicate that matched the career field of the last element of the identities array, then you\n\ncould use the same JSONPath selector you saw in the previous chapter. Because you now want mountebank to\n\ngenerate the predicate, you specify the selector in the predicateGenerators object and rely on the proxy to\n\nfill in the value, as in the following listing.\n\nListing 5.6. Specifying a jsonpath predicateGenerators\n\n{ \"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"predicateGenerators\": [{\n\n\"jsonpath\": {\n\n1\n\n\"selector\": \"$.identities[(@.length-\n\n1)].career\" 1\n\n}, 1\n\n\"matches\": { \"body\": true }\n\n2\n\n}]\n\n}\n\n}\n\n1 Saves the value defined by the selector...\n\n2 ...from the body field.\n\nRemember, predicateGenerators work on the incoming request, so the JSONPath selector saves off the\n\nvalue in the request body. If you sent the Abagnale JSON",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "in listing 5.6 to your proxy, the generated stub would look something like this:\n\n{\n\n\"predicates\": [{\n\n\"jsonpath\": {\n\n\"selector\": \"$.identities[(@.length- 1)].career\"\n\n},\n\n\"deepEquals\": { \"body\": \"Teacher\" }\n\n1\n\n}],\n\n\"responses\": [{\n\n\"is\": { ... }\n\n}] }\n\n1 The value captured from the selector in the incoming\n\nrequest body\n\nFuture requests that match the given selector will use the\n\nsaved response.\n\nGenerating XPath predicates\n\nThe same technique works for XPath. If you translate Abagnale’s list of identities to XML, it might look like:\n\n<identities>\n\n<identity career=\"Doctor\"> <name>Frank Williams</name>\n\n<location>Georgia</location>\n\n</identity>\n\n<identity career=\"Teacher\">\n\n<name>Frank Adams</name>\n\n<location>Utah</location>\n\n</identity>\n\n</identities>\n\nThe predicateGenerators mirrors the xpath predicate you saw in chapter 4, so if you had a need for a\n\npredicate to match on the location where Abagnale pretended to be a teacher, the following listing would do\n\nthe trick.",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Listing 5.7. Specifying an xpath predicateGenerators\n\n{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\", \"predicateGenerators\": [{\n\n\"xpath\": {\n\n1\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\" 1\n\n},\n\n1\n\n\"matches\": { \"body\": true } 2\n\n}]\n\n}\n\n}\n\n1 Saves the value defined by the selector...\n\n2 ...from the body field.\n\nThe predicates that the proxy creates show the correct\n\nlocation:\n\n{\n\n\"predicates\": [{\n\n\"xpath\": {\n\n\"selector\":\n\n\"//identity[@career='Teacher']/location\"\n\n}, \"deepEquals\": { \"body\": \"Utah\" } 1\n\n}],\n\n\"responses\": [{\n\n\"is\": { ... }\n\n}]\n\n}\n\n1 The value captured from the selector in the incoming\n\nrequest body\n\nCapturing multiple JSONPath or XPath values\n\nJSONPath and XPath selectors both can capture multiple\n\nvalues. To take a simple example, look at the following XML:",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "<doc>\n\n<number>1</number>\n\n<number>2</number>\n\n<number>3</number>\n\n</doc>\n\nIf you use the XPath selector of //number on this XML document, you get three values: 1, 2, and 3. The predicateGenerators field is smart enough to capture multiple values and save them using a standard\n\npredicate array, which requires all results to be present in subsequent requests to match but allows them to be\n\npresent in any order.\n\n5.3. CAPTURING MULTIPLE RESPONSES FOR THE SAME REQUEST\n\nThe examples we looked at so far have been great for minimizing traffic to a downstream service while still\n\ncollecting real responses. For each type of request, defined by the predicateGenerators, you only pass\n\nthe request to the real service once. This is the promise of the default mode, appropriately called proxyOnce. The\n\nguarantee is satisfied by ensuring that mountebank creates new stubs before the stub with the proxy\n\nresponse (figure 5.5). Mountebank’s first match policy will ensure that subsequent requests matching the\n\ngenerated predicates don’t reach the proxy response.\n\nFigure 5.5. In proxyOnce mode, mountebank creates new stubs before the stub with the proxy.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "A significant downside to proxyOnce is that each\n\ngenerated stub can have only one response. This is a problem for your inventory service, which returns\n\ndifferent responses over time for the same request, reflecting volatility in stock for an item (figure 5.6).\n\nFigure 5.6. Volatile responses for the same request",
      "content_length": 315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "In proxyOnce mode, mountebank captures only the\n\nfirst response (54). If your test cases relied on the volatility of inventory over time, you’d need a proxy that\n\nwould let you capture a richer data set to replay. The proxyAlways mode ensures that all requests reach the\n\ndownstream service, allowing you to capture multiple responses for a single request type (figure 5.7).\n\nFigure 5.7. In proxyAlways mode, new stubs are created a\u0000er the stub with the proxy",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Creating this type of proxy is as simple as passing in the\n\nmode, as in the following listing.\n\nListing 5.8. Creating a proxyAlways proxy response\n\n{\n\n\"proxy\": { \"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\", 1\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true }\n\n}]\n\n}\n\n}\n\n1 Ensures that all responses are captured\n\nThe key difference in the mechanics between proxyOnce and proxyAlways, as shown in figures 5.5\n\nand 5.7, is that proxyOnce generates new stubs before",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "the stub containing the proxy response, whereas proxyAlways generates stubs after the proxy stub.\n\nBoth approaches rely on mountebank’s first-match policy when matching a request to a stub. In the proxyOnce\n\ncase, a subsequent request matching the generated predicates is guaranteed to match before the proxy stub, and in the proxyAlways case, the proxy stub is guaranteed to match before the generated stubs.\n\nBut proxyAlways does more than create new stubs. It first looks to see if a stub whose predicates already exists\n\nmatch the generated predicates, and, if so, it appends the saved response to that stub. This behavior allows\n\nmultiple responses to be saved for the same request. You can see this by calling the imposter in listing 5.8 a few times and querying its state (by sending a GET request to http://localhost:2525/imposters/3000, assuming it was started on port 3000). To save space and make the\n\nsalient bits stand out, I’ve omitted the full response inside each generated is response in the following\n\nlisting.\n\nListing 5.9. The imposter state a\u0000er a few calls to a proxyAlways response\n\n{\n\n\"stubs\": [\n\n{\n\n\"responses\": [{ \"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n1\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true }\n\n}]\n\n} }]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/2599b7f4\" } 2\n\n}],\n\n\"responses\": [",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "{ \"is\": { \"body\": \"54\" } },\n\n3\n\n{ \"is\": { \"body\": \"21\" } },\n\n3\n\n{ \"is\": { \"body\": \"0\" } } 3\n\n]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"deepEquals\": { \"path\":\n\n\"/inventory/e1977c9e\" } 4\n\n}], \"responses\": [{\n\n\"is\": { \"body\": \"100\" }\n\n5\n\n}]\n\n}\n\n]\n\n}\n\n1 Ensures that all requests are proxied\n\n2 First request type\n\n3 All responses saved\n\n4 Second request type\n\n5 Only one response\n\nA proxyAlways proxy allows you to capture a full set of\n\ntest data that is as rich as your downstream service (at least for the requests sent to it). Although this is great for\n\nsupporting complex test cases, it comes with a significant problem. As you can see in listing 5.9, none of the saved responses will ever get called. With proxyOnce, you don’t need to worry about switching from recording to\n\nreplaying; it happens automatically. Not so with proxyAlways, so it’s time to look at how you can tell\n\nmountebank to replay all the data it has captured.\n\n5.4. WAYS TO REPLAY A PROXY\n\nConceptually, the switch from recording to replaying is as simple as removing the proxy response (figure 5.8).\n\nFigure 5.8. Replaying involves removing the proxy",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "All it takes to switch into replay mode is a single\n\ncommand, as follows:\n\nmb replay\n\nIf you watch the logs after that command, you’ll see\n\nsomething like the following:\n\ninfo: [mb:2525] GET /imposters?\n\nreplayable=true&removeProxies=true\n\ninfo: [mb:2525] PUT /imposters\n\ninfo: [http:3000] Ciao for now\n\ninfo: [http:3000] Open for business...\n\nThe switch involves resetting all imposters, removing their proxies. You can see in the third line that you’re shutting down the existing imposter (Ciao for now) and restarting it on the fourth line (Open for\n\nbusiness...). The line before shows the API call to send the altered configuration; this is the same line you’d",
      "content_length": 665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "see if you started mountebank with the --configfile command-line option.\n\nThe first line shows a different part of the mountebank REST API. Just as you can query the state of a single imposter by sending a GET request to http://localhost:2525/imposters/3000 (assuming the\n\nimposter is on port 3000), you can query all imposters at http://localhost:2525/imposters. The replay command\n\nadds two query parameters to that call:\n\nBecause the configuration for all imposters is quite possibly a\n\nconsiderable amount of data, it’s trimmed by default. The replayable query parameter ensures that all data essential (and no more) for replay is returned.\n\nThe removeProxies parameter removes the proxy responses, leaving only the captured is responses.\n\nThe mb replay command replays the responses as is. If you need to tweak the captured responses for any reason,\n\nyou can always use the API call to get the data yourself and process it as appropriate. Even better, you can let\n\nmountebank’s command line do the job for you. The following command saves the current state of all\n\nimposters, with proxies removed:\n\nmb save --savefile imposters.json --removeProxies\n\nThe mb save command saves all imposter configuration\n\nto the given file. The –savefile argument specifies where to save the configuration, and the -- removeProxies flag strips the proxy responses from\n\nthe configuration. Functionally, the mb replay command is nothing but an mb save followed by a\n\nrestart. The following command reimplements the replay command:\n\nmb save --savefile imposters.json --removeProxies",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "mb restart --configfile imposters.json\n\nThe ability to save all responses to a downstream service in proxyAlways mode and replay them with a single\n\ncommand dramatically simplifies capturing data for rich test suites.\n\n5.5. CONFIGURING THE PROXY\n\nProxies are configurable, both on the request they send to the downstream service and the generated responses\n\nthey send back to the system under test (figure 5.9).\n\nFigure 5.9. You can configure both the proxy request and the generated response.\n\nWe look at how to alter the responses in chapter 7 when we examine behaviors. You can apply two basic types of\n\nconfiguration to the proxied request: using certificate- based mutual authentication and adding custom\n\nheaders.\n\n5.5.1. Using mutual authentication\n\nRecall from chapter 3 that you can configure imposters to expect a client certificate by setting the mutualAuth\n\nfield to true. In that case, configuring the certificate and private key is the responsibility of the system under test.\n\nIf the downstream service you are proxying to requires",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "mutual authentication, then you have to configure the certificate on the proxy itself (figure 5.10).\n\nFigure 5.10. Configuring the proxy to pass a client certificate\n\nIn this case, setting up the proxy is similar to how you set HTTPS imposters. You set the certificate and private key\n\nin PEM format directly on the proxy, as shown in the following listing.\n\nListing 5.10. A proxy configured to pass a client certificate\n\n{\n\n\"proxy\": {\n\n\"to\": \"https://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true } }],\n\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----\n\n\\n...\", 1\n\n\"cert\": \"-----BEGIN CERTIFICATE-----\\n...\"\n\n1\n\n}\n\n}\n\n1 The actual text is much longer.",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Refer to chapter 3 for the full PEM format and how to create certificates.\n\n5.5.2. Adding custom headers\n\nOccasionally, it’s useful to add another header that gets passed to the downstream service. For example, many\n\nservices return compressed responses for efficiency. Although the original data may be human-readable\n\nJSON, after compression it turns into unreadable binary. By default, any proxies you set up will respond with the\n\ncompressed data unchanged. Because negotiating gzipped compression through headers is a standard\n\noperation in HTTP, the HTTP libraries that the system under test uses would decompress the data, allowing the\n\ncode that you’re testing to see the plain text response (figure 5.11).\n\nFigure 5.11. Proxying compressed responses\n\nThe standard proxy configurations that you’ve seen so far\n\nhave no issues returning the compressed data to the system under test. The problem occurs when you want to\n\nactually look at the data, such as after saving it in a configuration file using the mb save command. You\n\nmay want to examine the JSON bodies of the generated",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "is responses and perhaps tweak them to better fit your test cases, but you won’t be able to. All you will be able to\n\n[3]\n\nsee are encoded binary strings.\n\n3\n\nWe’ll look at how mountebank handles binary data in chapter 8.\n\nHTTP provides a way for clients to tell servers not to send back compressed data by setting the Accept- Encoding header to \"identity\". The original request\n\nfrom the system under test likely doesn’t include this header, because it can handle the compressed data just\n\nfine (and using compressed data in production is a good idea anyway for efficiency reasons). Fortunately, you can\n\ninject headers into the proxied request, as shown in the following listing.\n\nListing 5.11. Injecting a header into the request to prevent response compression\n\n{\n\n\"proxy\": {\n\n\"to\": \"http://localhost:8080\",\n\n\"mode\": \"proxyAlways\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"path\": true } }],\n\n\"injectHeaders\": {\n\n\"Accept-Encoding\": \"identity\" 1\n\n}\n\n}\n\n}\n\n1 Prevents response compression\n\nIf you need to inject multiple headers, add multiple key/value pairs to the injectHeaders object. Each\n\nheader will be added to the original request headers.\n\n5.6. PROXY USE CASES\n\nThe examples so far in this chapter have focused on using proxies to record and replay. That is the most\n\ncommon use case for proxies, as it allows you to capture",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "a rich set of test data for your test suite by recording real traffic. In addition, proxies have at least two other use\n\ncases: as a fallback response and as a way of presenting an HTTP face to an HTTPS service.\n\n5.6.1. Using a proxy as a fallback\n\nThough it isn’t a common scenario, sometimes it’s convenient to test against a real dependency when that\n\ndependency is stable, reliable, and highly available. One project I was on involved testing against a software-as-a-\n\nservice (SaaS) credit card processor, and the SaaS provider supported a reliable preproduction\n\nenvironment for testing. In fact, it was perhaps too reliable. “Happy path” testing (testing the expected flow\n\nthrough the service) was easy, but the service was so reliable that testing error conditions was difficult.\n\nYou can get the best of both worlds by using a partial\n\nproxy. Most calls flow through to the credit card processing service, but a few special requests trigger\n\ncanned error responses. Mountebank supports this scenario by relying on its first-match policy, putting the\n\nerror conditions first and the proxy last (figure 5.12).\n\nFigure 5.12. Mixing canned responses with a fallback proxy",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Notice that proxy has no predicates, which means all\n\nrequests that don’t match the predicates on the previous stubs will flow through to the proxy. Ensuring that all\n\nrequests flow through to the credit card processor involves putting the proxy in proxyAlways mode. In\n\nthe following listing, the code to do this relies on putting the proxy stub last.\n\nListing 5.12. Using a partial proxy\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\",\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{ 1\n\n\"contains\": { \"body\": \"5555555555555555\"\n\n} 1\n\n}],\n\n1\n\n\"responses\": [{\n\n2\n\n\"is\": { \"body\": \"FRAUD ALERT... \" } 2\n\n}]\n\n2\n\n},",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "{\n\n\"predicates\": [{\n\n3\n\n\"contains\": { \"body\": \"4444444444444444\"\n\n} 3 }],\n\n3\n\n\"responses\": [{\n\n4\n\n\"is\": { \"body\": \"INSUFFICIENT FUNDS...\" }\n\n4\n\n}]\n\n4 },\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n5\n\n\"to\": \"http://localhost:8080\",\n\n5\n\n\"mode\": \"proxyAlways\" 5\n\n}\n\n5\n\n}]\n\n}\n\n]\n\n}\n\n1 If the body contains this credit card #...\n\n2 ...send a fraud-alert response.\n\n3 If it contains this credit card #...\n\n4 ...send an over-balance response.\n\n5 All other calls go to the real service.\n\nIn this scenario, you would not use mb replay, because\n\nyou aren’t trying to virtualize the downstream service. Mountebank still creates new stubs and responses, so for\n\nany long-lived partial proxy, you’ll run into memory leaks. A future version of mountebank will support\n\nconfiguring proxies not to remember each response.\n\n5.6.2. Converting HTTPS to HTTP\n\nAnother less common scenario is to make an HTTPS service easier to test against. When I’ve seen this done, it",
      "content_length": 938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "has been as a workaround in an enterprise test environment that hasn’t configured SSL correctly,\n\nleading to certificates that don’t validate against the Certificate Authority. As I mentioned in chapter 3, I\n\nstrongly advise against changing the system under test to accept invalid certificates, because you risk releasing that\n\nconfiguration into production. Although the best solution is to fix the test environment certificates, the division of\n\nlabor in some enterprises makes that difficult to do. Assuming that you’re confident in the ability of the\n\nsystem under test to negotiate HTTPS with valid certificates (behavior that’s nearly always provided by\n\nthe core language libraries), you can rely on mountebank to bridge the misconfigured HTTPS to HTTP, as shown\n\nin the following listing.\n\nListing 5.13. Using a proxy to bridge HTTPS to HTTP\n\n{\n\n\"port\": 3000,\n\n\"protocol\": \"http\", 1 \"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://localhost:8080\", 2\n\n\"mode\": \"proxyAlways\"\n\n}\n\n}]\n\n}] }\n\n1 The imposter itself is an HTTP server...\n\n2 ...but forwards requests to an HTTPS server.\n\nBecause mountebank is designed to help test in environments that have yet to be fully configured, the\n\nimposter itself doesn’t validate the certificate during the proxy call. This doesn’t require any similar change in\n\nconfiguration in the system under test.\n\nSUMMARY",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Proxy responses capture real downstream responses and save them for future replay. The default proxyOnce mode saves the response in front of the proxy stub, meaning you don’t need to do anything to replay the response.\n\nThe proxyAlways mode allows you to capture a full set of test data by capturing multiple responses for the same logical request.\n\nYou have to explicitly switch from record mode to replay mode by using mb replay.\n\nThe predicateGenerators field tells mountebank how to create the predicates based on the incoming request. All fields used to discriminate requests are listed under the matches object. You configure parameters no differently than you would for\n\nnormal predicates.\n\nProxies support mutual authentication. You are responsible for setting the key and cert fields.\n\nYou can alter the request headers that your proxied request sends to the downstream service with the injectHeaders field. This is useful, for example, to disable compression so you can save a text\n\nresponse.",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Chapter 6. Programming mountebank\n\nThis chapter covers\n\nMatching requests even when none of the standard predicates do the trick\n\nAdding dynamic data to mountebank responses\n\nMaking network calls to create a stub response\n\nSecuring mountebank against unwanted remote execution\n\nDebugging your scripts within mountebank\n\nMost developers have the experience of a tool or\n\nframework that makes solving a problem harder than it should be or outright impossible. A primary goal in\n\nmountebank is to keep things that should be easy actually easy, and to make hard things possible.\n\nComprehensive default settings, predicates, and simple is responses enable you to solve easy problems with\n\nsimple solutions. Support for certificates, JSON, XML, and proxies help you to create solutions for a harder set\n\nof problems. But sometimes, that’s not enough.\n\nAt times, the built-in features of mountebank may not directly support your use case. Matching predicates\n\nbased on JSON and XML is nice, but what if your requests use CSV format? Or say you have a service that\n\nneeds to replay information from an earlier request in a subsequent response, requiring stateful memory. Maybe\n\nyour use case extends beyond what a proxy is capable of doing. Or perhaps you need to grab some data from\n\noutside mountebank itself and insert it into the stub response.\n\nYou need inject.\n\nMountebank ships with several scriptable interfaces, which we’ll examine in chapters 7 and 8. The main",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "scriptable interface comes in the form of a new predicate type and a new response type, both called inject, that\n\nallows you to match requests and create responses in ways mountebank wasn’t designed to support.\n\n6.1. CREATING YOUR OWN PREDICATE\n\nLike most technologists, I like to while away the idle hours pretending to be a rock star. Scaling that desire\n\ninto a set of microservices requires both a manager service (to manage my itinerary) and a roadie service (to\n\nmanage my instruments). The two services collaborate to protect my guitars against excess humidity (figure 6.1).\n\nFigure 6.1. Service collaboration used to protect my guitars\n\nLet’s assume that the manager service (the system under\n\ntest) is responsible for sending updated weather reports in CSV format to the roadie service, which uses that\n\ninformation to protect my guitars against dangerous humidity levels. Nice acoustic guitars are made of wood\n\nthat generally wants to stay in the 40–60% humidity",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "range, and the roadie service is responsible for alerting the manager service when a plan is needed to protect the\n\nguitars against excessive humidity.\n\nAlthough virtualizing a roadie service is a bit fantastical,\n\nyou’ll run into services that use something other than JSON or XML as their lingua franca from time to time.\n\nCSV is still a relatively popular format in certain types of service integrations, particularly those that involve\n\npassing some bulk-style information between teams or organizations, such as:\n\nRetrieving an augmented set of customer information from a\n\nmarketing research partner who has added (for example)\n\ndemographic information to your customer information\n\nRetrieving an updated list of tax rates by ZIP code in the United\n\nStates from an outside partner\n\nRetrieving bulk reporting information from an internal team\n\nFor this example, you’ll expect the manager service to\n\npass weather data showing the next 10 days of weather information in a format similar to what you might get on\n\na site like weather.com, as shown in the following listing.\n\nListing 6.1. A weather CSV service payload\n\nDay,Description,High,Low,Precip,Wind,Humidity\n\n4-Jun,PM Thunderstorms,83,71,50%,E 5 mph,65%\n\n5-Jun,PM Thunderstorms,82,69,60%,NNE 8mph,73%\n\n6-Jun,Sunny,90,67,10%,NNE 11mph,55% 7-Jun,Mostly Sunny,86,65,10%,NE 7 mph,53%\n\n8-Jun,Partly Cloudy,84,68,10%,ESE 4 mph,53%\n\n9-Jun,Partly Cloudy,88,68,0%,SSE 11mph,56%\n\n10-Jun,Partly Cloudy,89,70,10%,S 15 mph,57%\n\n11-Jun,Sunny,90,73,10%,S 16 mph,61%\n\n12-Jun,Partly Cloudy,91,74,10%,S 13 mph,63%\n\n13-Jun,Partly Cloudy,90,74,10%,S 17 mph,64%\n\nMountebank doesn’t natively support CSV, and it doesn’t have a greaterThan predicate to look for humidity\n\nlevels greater than 60%, so you are already outside of its built-in capabilities, but let’s raise the stakes even higher.\n\nYour expectations of the roadie service are that it only",
      "content_length": 1888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "presses the panic button if you are out of range for at least three consecutive days, or if you are more than 10%\n\nout of range for a single day (figure 6.2).\n\nFigure 6.2. A test scenario requiring advanced predicate logic\n\nThat makes for a complex predicate, but no one said\n\nbecoming a rock star would be easy. You can create your own predicate using JavaScript and the inject\n\npredicate, but first you have to start mountebank with the –allowInjection command line flag:\n\nmb --allowInjection\n\nThe logs show a warning at startup that hints at why injection is disabled by default:\n\ninfo: [mb:2525] mountebank v1.13.0 now taking orders -\n\npoint your browser to http://localhost:2525\n\nfor help\n\nwarn: [mb:2525] Running with --allowInjection\n\nset.\n\nSee http://localhost:2525/docs/security for\n\nsecurity info",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "You can go ahead and view the docs provided at the given URL now, or wait until you get to the topic of\n\nsecurity later in this chapter. One thing you should not do is ignore the warning. It’s great that you can do\n\nwonderfully complex feats of logic with JavaScript injection and mountebank. Unfortunately, so can\n\nmalicious attackers on your network who can access your machine. They now have a remote execution engine\n\nlistening on a socket. You can protect yourself, and we look at ways to do that at the end of this chapter. For now, the safest option is to add the --localOnly flag, which dis-allows remote connections:\n\nmb restart --allowInjection --localOnly\n\nAll of the predicates we looked at in chapter 4 operate on a single request field. Not so with inject, which gives\n\nyou complete control by passing the entire request object into a JavaScript function you write. That JavaScript function returns true if the predicate matches the request and false otherwise, as shown in the following\n\n[1]\n\nlisting.\n\n1\n\nJavaScript as a language has a lot of warts, and experts will tell you that you can return truthy or falsy. I never cared to learn what that meant, so I stick with true or false and recommend you do too.\n\nListing 6.2. The structure of an inject predicate\n\nfunction (request) {\n\nif (...) { 1\n\nreturn true; 2\n\n}\n\nelse { return false; 3\n\n}\n\n}\n\n1 Condition can use entire request",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "2 Predicate matched\n\n3 Predicate didn’t match\n\nPlugging the function into the predicates array of a\n\nstub involves JSON-escaping the function, which replaces newlines with '\\n', and escaping double\n\nquotes:\n\n{\n\n\"predicates\": [{\n\n\"inject\": \"function (request) {\\n if (...)\n\n{\\n\n\nreturn true;\\n }\\n else {\\n return\n\nfalse;\\n }\\n}\" }],\n\n\"responses: [{\n\n\"is\": { ... }\n\n}]\n\n}\n\nI wouldn’t recommend doing the JSON-escaping by hand. In the examples that follow, you will use EJS templating and the stringify function mountebank adds to EJS. (Refer to chapter 3 for details on how to lay\n\n[2]\n\nout configuration files.) imposter configuration in code, your JSON libraries\n\nIf you’re building up the\n\nshould manage the escaping for you.\n\n2\n\nBrowse the GitHub repo at https://github.com/bbyars/mountebank-in-action to see fully worked- out examples.\n\nAll you have to do now is write the JavaScript function. I\n\nhave intentionally created a complex example to show that injection is up to nearly any task. Let’s look at the\n\nJavaScript bit by bit, starting with a function to parse CSV. You need a function that will take raw text, like that\n\nshown in listing 6.1, and convert it into an array of JavaScript objects:",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "[\n\n{\n\n\"Day\": \"4-Jun\",\n\n\"Description\": \"PM Thunderstorms\",\n\n\"High\": 83,\n\n\"Low\": 71\n\n\"Precip\": \"50%\", \"Wind\": \"E 5 mph\",\n\n\"Humidity\": \"65%\"\n\n},\n\n...\n\n]\n\nYou will call the function shown in the following listing csvToObjects.\n\nListing 6.3. A JavaScript function to parse CSV data—csvToObjects\n\nfunction csvToObjects (csvData) {\n\n1\n\nvar lines = csvData.split('\\n'),\n\n2\n\nheaders = lines[0].split(','),\n\nresult = [];\n\n// Remove the headers\n\nlines.shift();\n\n3\n\nlines.forEach(function (line) {\n\nvar fields = line.split(','),\n\n4\n\nrow = {};\n\nfor (var i = 0; i < headers.length; i++) {\n\nvar header = headers[i],\n\ndata = fields[i];\n\nrow[header] = data;\n\n5\n\n}\n\nresult.push(row);\n\n6\n\n});\n\nreturn result;\n\n}",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "1 csvData is the raw text.\n\n2 Splits input by line endings\n\n3 Removes first line (headers)\n\n4 Splits line by commas\n\n5 Adds data keyed by header\n\n6 Adds to array\n\nAs far as CSV parsing functions go, this is as simple as it\n\ngets. It works for your data, but not for more complex data involving escaped commas inside quotes and other\n\nedge scenarios.\n\nThe next function you will need is one to look for three consecutive days of humidity over 60%, as shown in the\n\nfollowing listing.\n\nListing 6.4. Looking for three consecutive days out of range\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n1 var daysOutOfRange = 0,\n\nresult = false;\n\nhumidities.forEach(function (humidity) {\n\nif (humidity < 60) {\n\ndaysOutOfRange = 0;\n\n2\n\nreturn; }\n\ndaysOutOfRange += 1;\n\nif (daysOutOfRange >= 3) {\n\n3\n\nresult = true;\n\n}\n\n});\n\nreturn result;\n\n}\n\n1 Accepts an array of integers representing humidity\n\nlevel\n\n2 Resets counter if humidity is in range\n\n3 Sets result if humidity is out of range for three days",
      "content_length": 992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Detecting three consecutive days of out-of-range humidity is complicated enough to extract into a\n\nseparate function, but it isn’t too difficult to code. The last check—looking for a single day more than 10% out of\n\nrange—is simple enough that you can do it inline using the JavaScript some function of arrays, which returns\n\ntrue if the supplied function is true for any element of the array. The predicate function looks like the following\n\nlisting.\n\nListing 6.5. A predicate to test for excess humidity levels\n\nfunction (request) {\n\n1\n\nfunction csvToObjects (csvData) { ... }\n\n2\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n... } 3\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\n4\n\nreturn parseInt(row.Humidity.replace('%', '')); 4\n\n}),\n\n4\n\nhasDayTenPercentOutOfRange =\n\nhumidities.some( 5\n\nfunction (humidity) { return humidity >=\n\n70; } 5\n\n); 5\n\nreturn hasDayTenPercentOutOfRange ||\n\n6\n\nhasThreeDaysOutOfRange(humidities);\n\n6\n\n}\n\n1 Passes the request object in\n\n2 See listing 6.3.\n\n3 See listing 6.4.\n\n4 Converts the CSV rows to a list of humidity integers\n\n5 Converts the CSV rows to a list of humidity integers",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "6 Matches if either condition is true\n\nWhen you include an inject predicate in a stub,\n\nmountebank passes the entire request object to the provided function. You have included your csvToObjects and hasThreeDaysOutOfRange functions as subfunctions inside the parent predicate\n\nfunction. You can use this approach to include code of considerable complexity.\n\nAdding this predicate allows you to mimic the roadie\n\nservice effectively, virtualizing its behavior with high fidelity. Although that highlights the power of JavaScript\n\ninjection, it also raises an important concern about service virtualization.\n\nVirtualizing the roadie service has been a wonderful example to demonstrate the power of inject. However, it does come with two pretty serious drawbacks: it hasn’t\n\nhelped one bit in terms of making me an actual rock star, and it’s likely the kind of thing you’d want to avoid\n\nvirtualizing in a real application stack. Remember, service virtualization is a testing strategy that gives you\n\ndeterminism when testing a service that has runtime dependencies. It’s not a way of reimplementing runtime\n\ndependencies in a different platform. Although mountebank provides advanced functionality to make\n\nyour stubs smarter when you need them to be, your best bet is to not need them to be so smart. The dumber your\n\nvirtual services can be, the more maintainable your test architecture will be.\n\n6.2. CREATING YOUR OWN DYNAMIC RESPONSE\n\nYou also can create your own response in mountebank. The inject response joins is and proxy to round out the core response types, and it represents a dynamic\n\nresponse that JavaScript generates. In its simplest form, the response injection function mirrors that for",
      "content_length": 1702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "predicates, accepting the entire request as a parameter. It’s responsible for returning a response object that\n\nmountebank will merge with the default response. Think of it as creating an is response using a JavaScript\n\nfunction, as follows.\n\nListing 6.6. The basic structure of response injection\n\n{\n\n\"responses\": [{\n\n\"inject\": \"function (request) { return {\n\nstatusCode: 400 }; }\" }]\n\n}\n\nIn this basic form, it’s quite easy to replace your\n\npredicate injection you used to virtualize the roadie service with response injection. Because you’re giving the\n\nresponse injection function the same request as the predicate injection function, you could remove the\n\npredicate injection and move the conditions to the function that generates the response, as shown in the\n\nfollowing listing.\n\nListing 6.7. A response injection function to virtualize the roadie service humidity checks\n\nfunction (request) {\n\n1\n\nfunction csvToObjects (csvData) { ... } 2\n\nfunction hasThreeDaysOutOfRange (humidities) {\n\n... } 3\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%', ''));\n\n}),\n\nhasDayTenPercentOutOfRange = humidities.some(\n\nfunction (humidity) { return humidity >=\n\n70; }\n\n),\n\nisTooHumid = hasDayTenPercentOutOfRange ||\n\n4",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "hasThreeDaysOutOfRange(humidities);\n\nif (isTooHumid) {\n\nreturn {\n\n5 statusCode: 400,\n\n5\n\nbody: 'Humidity levels dangerous, action\n\nrequired' 5\n\n};\n\n5\n\n}\n\nelse { return {\n\n6\n\nbody: 'Humidity levels OK for the next 10\n\ndays' 6\n\n};\n\n6\n\n}\n\n}\n\n1 Mountebank passes request in\n\n2 See listing 6.3.\n\n3 See listing 6.4.\n\n4 Capture condition\n\n5 Return failure response\n\n6 Return happy path response\n\nInstead of returning true or false to determine\n\nwhether a predicate matches, a response injection returns the response object, or at least the portion of it\n\nthat isn’t the default response. In this case, it returns a 400 status code and a body indicating action is required\n\nif the humidity check requires you to take action, or a default 200 code with a body letting you know that the\n\nhumidity levels are OK.\n\n6.2.1. Adding state\n\nAt first glance, there isn’t much difference between using\n\na predicate injection and using a response injection in this example. But for complex workflows, response injections have a key advantage: they can keep state. To",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "see how that can be useful, imagine having to virtualize a scenario where your manager service sends multiple\n\nweather reports, and the roadie service needs to detect three consecutive days out of range even if they span two\n\nreports that the manager sent (figure 6.3).\n\nFigure 6.3. Two reports need to be spanned to detect dangerous humidity.\n\nMountebank passes a state parameter into your\n\nresponse injection functions that you can use to remember information across multiple requests. It’s\n\ninitially an empty object, but you can add whatever information you want to it each time the injection\n\nfunction executes. In this example, you will have to save the humidity results by day so you can detect dangerous\n\nhumidity levels even if three consecutive days over 60% humidity span two requests from the manager service.\n\nYou start by adding the parameter to your function and\n\ninitializing it with the variables you want to remember. In this case, state will remember the days, and if the\n\nroadie service sees the weather report for a day it hasn’t seen yet, your function will add the humidity to a list:\n\nfunction (request, state) { if (!state.humidities) {",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "state.days = [];\n\nstate.humidities = [];\n\n}\n\n... }\n\nNow the rest of the function is nearly identical to listing 6.7. You only have to add to the state.humidities array at the appropriate time and do your checks on that\n\narray instead of a local variable, as in the following listing.\n\nListing 6.8. Remembering state between responses\n\nfunction (request, state) {\n\nfunction csvToObjects (csvData) {...}\n\nfunction hasThreeDaysOutOfRange (humidities)\n\n{...}\n\n// Initialize state arrays if (!state.humidities) {\n\nstate.days = [];\n\nstate.humidities = [];\n\n}\n\nvar rows = csvToObjects(request.body);\n\nrows.forEach(function (row) {\n\nif (state.days.indexOf(row.Day) < 0) { 1\n\nstate.days.push(row.Day);\n\nstate.humidities.push(row.Humidity.replace('%',\n\n'')); 2\n\n}\n\n});\n\nvar hasDayTenPercentOutOfRange =\n\nstate.humidities.some(function (humidity) {\n\n3\n\nreturn humidity >= 70;\n\n});\n\nif (hasDayTenPercentOutOfRange ||\n\nhasThreeDaysOutOfRange(state.humidities)) { return {\n\nstatusCode: 400,\n\nbody: 'Humidity levels dangerous, action",
      "content_length": 1018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "required'\n\n};\n\n}\n\nelse {\n\nreturn { body: 'Humidity levels OK'\n\n};\n\n}\n\n}\n\n1 Only adds to the list if it hasn’t seen this day before\n\n2 Adds new humidity\n\n3 Switches these functions to use state variable\n\nVoilà! Now the virtual roadie service can keep track of\n\nhumidity levels across multiple requests. There is only one feature of injection left to look at, but it is a big topic:\n\nasynchronous operations.\n\n6.2.2. Adding async\n\nAsynchronicity is baked into JavaScript, to the extent\n\nthat it is generally required to access any file or network resource used to craft a dynamic response. Understanding why requires a quick tour of how\n\nprogramming languages manage I/O, as JavaScript is fairly unusual in this regard. Until Microsoft introduced the XMLHttpRequest object that powers AJAX requests, JavaScript lacked any form of I/O found in the\n\nbase class libraries of other languages. It took Node.js to add a full complement of I/O functions to JavaScript, but\n\nit did so following the AJAX pattern familiar to a generation of web developers: using callbacks.\n\nTake a look at the following code to sort the lines in a file. This is in Ruby, but the code would be similar in Python,\n\nJava, C#, and most traditional languages.\n\nListing 6.9. Using traditional I/O to sort lines in a file\n\nlines = File.readlines('input.txt') puts lines.sort",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "puts \"The end...\"\n\nFirst you read all the lines in the input.txt file into an array, then you sort the array, printing the output to the\n\nconsole. Finally, you print “The end...” to the console. At first blush, nothing could be simpler, but the File.readlines function is hiding considerable complexity.\n\nAs shown in figure 6.4, under the hood, Ruby has to make a system call into the operating system (OS), because\n\nonly the OS has the appropriate privileges to interact with the hardware, including the disk storing input.txt.\n\nTo buy time while it waits on the results, the OS scheduler switches to another process to execute for a\n\nperiod of time. When the data from the disk is available, the OS feeds it back into the original process. Computers\n\nmove fast enough that this is largely transparent to the user; for most I/O operations, the application will still feel quite responsive. It’s also transparent to the\n\ndeveloper, as the linear nature of the code matches a mental model, which is why blocking I/O—having the\n\nprocess block until the operation completes—is the most common form of I/O.\n\nFigure 6.4. What happens with traditional blocking I/O",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "JavaScript was born of the web, a programming\n\nenvironment rich with events, such as responding when a user presses a button or types in a text field. AJAX,\n\nwhich made web pages more responsive by allowing the user to fetch data from the server without refreshing the\n\nentire page (a form of I/O involving the network), maintained that event model, treating getting a response\n\nfrom the server as an event. When Ryan Dahl wrote Node.js to add more I/O capability to JavaScript, he\n\nintentionally maintained that event model because he wanted to explore nonblocking I/O in a mainstream\n\nlanguage. The fact that developers were already used to AJAX events made JavaScript a natural fit (figure 6.5).\n\nFigure 6.5. Nonblocking I/O doesnʼt block the process.",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Each I/O operation registers a callback function that\n\nexecutes when the OS has data, and program execution continues immediately to the next line of code. Let’s\n\nrewrite the Ruby file sort operation in JavaScript, using nonblocking I/O, as in the following listing.\n\nListing 6.10. File sort using nonblocking I/O\n\nvar fs = require('fs');\n\n1\n\nfs.readFile('input.txt', function (err, data) {\n\n2\n\nvar lines = data.toString().split('\\n'); console.log(lines.sort());\n\n});\n\nconsole.log('The end...');\n\n3\n\n1 Built-in node library for filesystem operations\n\n2 Ignores error handling\n\n3 Execution continues before the file is actually read",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "The callback function is passed as a parameter to fs.readFile, and execution immediately continues to\n\nthe next line of code. At some later point, when the OS has the contents of input.txt, the callback function will\n\nexecute, providing the sorted lines. That flow means that, in this example, you will print “The end...” to the console\n\nbefore you print the sorted lines of input.txt.\n\nAs of this writing, predicate injection doesn’t support\n\nasynchronous operations. But async support is important when it comes to response injection, as I/O operations\n\nare often valuable in scripting dynamic responses. Let’s show an example by virtualizing a simple OAuth flow in\n\nmountebank.\n\nOAuth\n\nOAuth is a delegated authorization framework. It allows one party (the resource owner) to allow another party\n\n(the client) to act on a resource held by a third party (the resource server), where the identity of the resource\n\nowner is guaranteed by a fourth party (the authorization server). You can often collapse these four roles in various\n\nways, creating a number of alternative flows.\n\nA standard use case is where a person (the resource\n\nowner) allows a web app (the client) to act on a resource held by a third party like GitHub (resource server) after\n\npresenting credentials to that third party (authorization server). This flow allows the web app to perform secure\n\noperations in GitHub on behalf of the user, even though the user never provided their GitHub credentials to that\n\nwebsite.\n\nThis OAuth flow is common, and its mechanics are difficult to stub out. In the section “Virtualizing an\n\nOAuth-backed GitHub client,” I use that as an opportunity to show how to manage async in an inject",
      "content_length": 1692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "response by building a small GitHub web app and virtualizing the GitHub API for testing purposes.\n\nVirtualizing an OAuth-backed GitHub client\n\nGitHub has a robust marketplace of client applications available at https://github.com/marketplace. Unfortunately,\n\nnone of them solve an immediate problem of the readers of this book: adding a star to the mountebank repo.\n\n[3]\n\nBut GitHub has a public RESTful API that allows you to build an app. You will treat that app as your system\n\nunder test, requiring you to virtualize the GitHub API itself.\n\n3\n\nFor readers willing to star the old-fashioned way, you can do so by visiting the repo directly at https://github.com/bbyars/mountebank.\n\nGitHub uses OAuth, which requires a complex set of interactions before GitHub will accept an API call to star\n\n[4]\n\na repo. new application, which you can do at\n\nThe first thing you need to do is to register your\n\nhttps://github.com/settings/applications/new (figure 6.6).\n\n4\n\nWe will use a basic OAuth web flow described at https://developer.github.com/v3/guides/basics-of- authentication/.\n\nFigure 6.6. Registering a GitHub application",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "The OAuth flow expects to call back to complete the\n\nauthentication and uses the URL you provided during registration to call back into. The general flow is shown\n\nin figure 6.7.\n\nFigure 6.7. Understanding the GitHub OAuth flow",
      "content_length": 227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "The application calls the /login/oauth/authorize endpoint, passing a client_id. GitHub calls the callback URL you provided during registration, passing a random code. The application is then expected to call a /login/oauth/access_token URL, sending the client_id, the code and a client_secret. If all of that is done correctly, GitHub sends back a token that\n\nthe application can use to authorize subsequent calls. Although the fictional app is designed to star the\n\nmountebank repo if you haven’t already done so, I will only show how to test whether you have already starred\n\nor not. (On the advice of my legal staff, I have decided to leave starring mountebank as an exercise for the reader.)\n\nThe client_id and client_secret are provided\n\nduring registration on GitHub (figure 6.8). Like a private key, you should keep the client_secret super secret.\n\nYou shouldn’t store it in source control, and you most",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "certainly shouldn’t publish it in a book for millions to read.\n\n[5]\n\n5\n\nDon’t worry—I removed this toy app before you had a chance to read these words. You’ll have to register your own app to fully follow the source code in question, although you can use the same code located at https://github.com/bbyars/mountebank-in-action\n\nFigure 6.8. Viewing the client secret to communicate to GitHub\n\nYour test case should validate this entire flow, requiring\n\nyou to virtualize three GitHub endpoints. Structurally, the GitHub imposter needs to have three stubs representing those endpoints (figure 6.9). You can\n\nvirtualize the last two calls in order to get the token and to check if you have starred the mountebank repo, with simple is responses. You can’t virtualize the first call, to actually authorize, with an is response. in order to\n\nauthorize. The fact that it has to call back into the system under test requires you to move beyond simple stubbing approaches. You’ll use an inject response to do the callback with an easily identifiable test code.\n\nFigure 6.9. The three stubs needed to virtualize this GitHub workflow",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Using easily identifiable names (like TEST-CODE and\n\nTEST-ACCESS-TOKEN) for the test data that the imposter creates is a useful tip to make it easier to spot in\n\na complex workflow.\n\nStarting the OAuth handshake\n\nThe first endpoint (/login/oauth/authorize) starts the OAuth handshake by sending the random code (TEST-\n\nCODE) to your web app. It’s also the most complicated response, involving calling back to the system under test, which you cannot solve using an is or a proxy response. Conceptually, the stub looks like the following\n\nlisting.\n\nListing 6.11. Stub using response injection to make OAuth callback\n\n{ \"predicates\": [{\n\n\"equals\": {\n\n\"method\": \"GET\",\n\n\"path\": \"/login/oauth/authorize\",\n\n\"query\": {\n\n\"client_id\": \"<%=\n\nprocess.env.GH_CLIENT_ID %>\" 1\n\n} }",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "}],\n\n\"responses\": [{\n\n\"comment\": \"This sends back a code of TEST-\n\nCODE\", 2\n\n\"inject\": \"<%- stringify(filename, 'auth.js') %>\" 3\n\n}]\n\n}\n\n1 Adds environment variable\n\n2 Mountebank ignores this line.\n\n3 Brings in injection function\n\nBoth the tests and the example web app use environment variables for the client_id and the client_secret, and you will use EJS templating to interpolate them into your configuration file. Notice also the added comment field in the response. Mountebank ignores any fields it\n\ndoesn’t recognize, so you can always add more metadata. For complicated workflows like this one, such comments\n\ncan help you follow along more easily.\n\nThe following listing shows the injection function in auth.js.\n\nListing 6.12. Injection function to make OAuth callback\n\nfunction (request, state, logger, callback) { var http = require('http'),\n\noptions = {\n\nmethod: 'GET',\n\nhostname: 'localhost',\n\nport: 3000,\n\npath: '/callback?code=TEST-CODE'\n\n1\n\n}, httpRequest = http.request(options, function\n\n(response) {\n\nvar body = '';\n\n2\n\nresponse.setEncoding('utf8');\n\n2\n\nresponse.on('data', function (chunk) {\n\n2 body += chunk;\n\n2",
      "content_length": 1132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "});\n\n2\n\nresponse.on('end', function () {\n\ncallback({ body: body });\n\n3 });\n\n});\n\nhttpRequest.end();\n\n4\n\n}\n\n1 Callback with TEST-CODE\n\n2 Node.js code to collect the response body\n\n3 Asynchronously returns the response\n\n4 Sends the request and returns from the function\n\nMuch of the code is manipulating the Node.js http\n\nmodule to make the call to http://localhost:3000/callback?code=TEST-CODE.\n\nBecause the HTTP call involves network I/O, the function returns immediately after the call to httpRequest.end(). When the network call returns, Node.js invokes the function passed as a parameter to the http.request() call. Node’s http library streams the HTTP response back, so you may receive multiple data events and have to collect the response body as you go. When you have received the entire response, Node triggers the end event, at which point you can create the response you want. In your case, you’ll return the same\n\nbody the callback URL provided. Passing that to the callback function parameter ends your response\n\ninjection, returning the parameter as the response to mountebank.\n\nFor example, if your call to\n\nhttp://localhost:3000/callback?code=TEST-CODE returns a body of “You have already starred the\n\nmountebank repo,” then the end result of the injection",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "function would be equivalent to the following is response:\n\n{\n\n\"is\": {\n\n\"body\": \"You have already starred the\n\nmountebank repo\" }\n\n}\n\nRemember, a key goal of mountebank is to make simple\n\nthings easy to do and to make hard things possible. Virtualizing an OAuth flow is hard. This is about as\n\ncomplicated a workflow as you will see in most tests involving service virtualization. It’s certainly a lot to walk\n\nthrough, but you were able to do the hard bits of it with about 20 lines of JavaScript, and when you run into\n\nsimilar problems, you will be thankful that solving them is at least possible with mountebank.\n\nValidating the OAuth authorization\n\nLet’s look at the next stub, for the\n\n/login/oauth/access_token endpoint. This one should only match if the app reflected back the TEST-CODE in\n\nits request body and correctly sent the preconfigured client_id and client_secret. You can use\n\npredicates and a simple is response to send back a test access token, as shown in the following listing.\n\nListing 6.13. The stub to get an access token\n\n{\n\n\"predicates\": [\n\n{\n\n\"equals\": {\n\n\"method\": \"POST\", \"path\": \"/login/oauth/access_token\"\n\n}\n\n},\n\n{\n\n\"contains\": {\n\n\"body\":\n\n\"client_id=<%= process.env.GH_CLIENT_ID %>\"",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "1\n\n}\n\n},\n\n{\n\n\"contains\": { \"body\":\n\n\"client_secret=<%=\n\nprocess.env.GH_CLIENT_SECRET %>\" 2\n\n}\n\n},\n\n{\n\n\"contains\": {\n\n\"body\": \"code=TEST-CODE\" 3\n\n}\n\n}\n\n],\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": {\n\n\"access_token\": \"TEST-ACCESS-TOKEN\", 4\n\n\"token_type\": \"bearer\",\n\n\"scope\": \"user:email\"\n\n}\n\n}\n\n}]\n\n}\n\n1 Requires client_id from environment\n\n2 Requires client_secret from environment\n\n3 TEST-CODE comes from injection in previous stub\n\n4 Returns a test access token\n\nOnce your web app retrieves the access_token, it has\n\nsuccessfully navigated the OAuth flow.\n\nChecking if youʼve starred mountebank\n\nAt this point, the app should be armed with an access token and should make the GitHub call to check if you’ve\n\nstarred the mountebank repo or not. Your predicate needs to validate the token, and, once again, you can use a simple is response, as shown in the following listing.\n\nListing 6.14. Stub to check if youʼve starred the mountebank repo",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "{\n\n\"predicates\": [{\n\n\"equals\": {\n\n\"method\": \"GET\",\n\n\"path\": \"/user/starred/bbyars/mountebank\",\n\n\"headers\": {\n\n\"Authorization\": \"token TEST-ACCESS- TOKEN\" 1\n\n}\n\n}\n\n}],\n\n\"responses\": [{\n\n\"comment\": \"204=yes, 404=no\",\n\n2\n\n\"is\": { \"statusCode\": 404 } 3\n\n}]\n\n}\n\n1 Validates token\n\n2 Mountebank ignores this line.\n\n3 Indicates that user hasn’t starred repo\n\nWith OAuth now virtualized, any other API endpoints\n\nshould be quite easy to stub out. All you have to do is check for the Authorization header as shown in listing\n\n6.14.\n\n6.2.3. Deciding between response vs. predicate injection\n\nMountebank passes the request object to both\n\npredicate and response injection functions, so you could put conditional logic based on the request in either\n\nlocation. Compared to response injection, predicate injection is relatively easy to use. If you need to send a\n\nstatic response back based on a dynamic condition, programming your own predicate and using an is\n\nresponse stays true to the intention of predicates in mountebank. But response injection is considerably\n\nmore capable, and you won’t hurt my feelings if you move your conditional logic to a response function so you can take advantage of state or async support.",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "A predicate injection function takes only two parameters:\n\nrequest— The protocol-specific request object (HTTP in all of the examples)\n\nlogger— Used to write debugging information to the mountebank logs\n\nThe response injection function includes those\n\nparameters and adds two more:\n\nstate— An initially empty object that you can add state to; will be passed into subsequent calls for the same imposter\n\ncallback— A callback function to support asynchronous operations\n\nResponse functions always have the option of\n\nsynchronously returning a response object. You only need the callback if you use nonblocking I/O and need\n\nto return the response object asynchronously.\n\n6.3. A WORD OF CAUTION: SECURITY MATTERS\n\nJavaScript injection is disabled by default when you start mountebank, and for good reason. With injection\n\nenabled, mountebank becomes a potential remote execution engine accessible to anyone on the network.\n\nWhen mountebank is run naively, an attacker can take advantage of that fact to do evil things on the network\n\nwhile spoofing your identity (figure 6.10).\n\nFigure 6.10. Using mountebank to spoof your identity during a network attack",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Injection is an enormously useful feature, but you have\n\nto use it with an understanding of the security implications. This is why mountebank shows a warning message in the logs every time you start it with the -- allowInjection flag. You can take some precautions\n\nto protect yourself.\n\nThe first precaution is to not run mb under your user account. Starting mountebank as an unprivileged user,\n\nideally one without domain credentials on your network, goes a long way toward protecting yourself. You should\n\nalways use the least privileged user you can get away with, adding in network access only when your tests\n\nrequire it.\n\nThe next layer of security is to restrict which machines can access the mountebank web server. We’ve used the -\n\nlocalOnly flag throughout this chapter, which restricts access to processes running on the same machine. This option is perfect when your tests run on the same machine as mountebank, and it should be the\n\ndefault choice most of the time. When you do require remote tests (during extensive load testing, for example),\n\nyou can still restrict which machines can access mountebank’s web server with the --ipWhitelist flag,",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "which captures a pipe-delimited set of IP addresses. For example:\n\nmb --allowInjection --ipWhitelist\n\n\"10.22.57.137|10.22.57.138\"\n\nIn this example, the only remote IP addresses allowed access to mountebank are 10.22.57.137 and 10.22.57.138.\n\n6.4. DEBUGGING TIPS\n\nWriting injection functions has all the same complexity as writing any code, except that it’s much harder to debug them through an IDE because they run in a\n\nremote process. I often resort to what, back in my college days when we coded in C, we called printf debugging.\n\nIn JavaScript, it looks something like this:\n\nfunction (request) {\n\n// Function definition...\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%',\n\n''));\n\n});\n\nconsole.log(JSON.stringify(humidities)); 1\n\nreturn {}; 2\n\n}\n\n1 Shows full object structure\n\n2 I’ll figure this out later....\n\nThe console.log function in JavaScript prints the\n\nparameter to the standard output of the running process, which in this case is mb. The JSON.stringify function\n\nconverts an object to a JSON string, allowing you to inspect the full object graph. This code is quite ugly—I outdented the console.log function to not lose sight",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "of it, and I returned an empty object for the response, relying on the standard default response. If you’ve\n\nwritten code for longer than a few seconds, you’ll likely recognize the pattern. Most code starts out ugly before\n\nyou figure out how to communicate your intent to a ruthlessly precise computer.\n\nTo make the output a little easier to spot in the logs, mountebank passes another parameter to both predicate\n\nand response injection: the logger itself. In typical logging fashion, the logger has four functions: debug,\n\ninfo, warn, and error. The debug messages are usually not shown to the console (unless you start mb\n\nwith the --loglevel debug flag). To make your debugging messages stand out, use the warn or error\n\nfunctions, which will print your debugging output to the console in a different color:\n\nfunction (request, state, logger) {\n\n// Function definition...\n\nvar rows = csvToObjects(request.body),\n\nhumidities = rows.map(function (row) {\n\nreturn parseInt(row.Humidity.replace('%',\n\n''));\n\n});\n\nlogger.warn(JSON.stringify(humidities));\n\n1\n\nreturn {};\n\n}\n\n1 Prints to the console as yellow text\n\nThe code above shows logging for response injection. Predicate injections pass in the logger as the second\n\nparameter as well.\n\nSUMMARY",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "You can create your own predicates with a JavaScript function that accepts the request object and returns a Boolean representing whether the predicate matched or not.\n\nYou also can create your own response with a JavaScript function that accepts the request object and returns an object representing the response.\n\nIf you need to remember state between requests, mountebank passes an initially empty state object to the response function. Any fields you set on the object will persist between calls.\n\nBecause JavaScript and node.js use nonblocking I/O, most\n\nresponse functions that need to access data outside the process\n\nwill have to return asynchronously. Instead of returning an object, you can pass the response object to the callback parameter.\n\nInjection is powerful, but it also creates a remote execution engine\n\nrunning on your machine. Anytime you run mountebank with injection enabled, you should limit remote connections and use an\n\nunprivileged identity.",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 7. Adding behaviors\n\nThis chapter covers\n\nProgrammatically postprocessing a response\n\nAdding latency to a response\n\nRepeating a response multiple times\n\nCopying input from the request into the response\n\nLooking up data from a CSV file to plug into a response\n\nThe basic is response is easy to understand but limited\n\nin functionality. Proxy responses provide high-fidelity mimicry, but each saved response represents a snapshot\n\nin time. Response injection provides significant flexibility but comes with high complexity. Sometimes, you want the simplicity of is and proxy responses with a touch of dynamism, all without the complexity of inject.\n\nSoftware engineers who hail from the object-oriented\n\nschool of thought use the term decorate to mean intercepting a plain message and augmenting it in some\n\nway before forwarding it on to the recipient. It’s like what the postal service does when it applies a postmark to your letter after sorting it. The original letter you sent is\n\nstill intact, but the postal workers have decorated it to postprocess it with a bit of dynamic information. In\n\nmountebank, behaviors represent a way of decorating responses before the imposter sends them over the wire.\n\nBecause of their flexibility and utility, behaviors also represent one of the most rapidly evolving parts of\n\nmountebank itself. We’ll look at all of the behaviors available as of this writing (representing v1.13), but\n\nexpect more to come in the future.\n\n7.1. UNDERSTANDING BEHAVIORS",
      "content_length": 1497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "If you ignore the complexity of interacting with the network and different protocols, mountebank has only\n\nthree core concepts:\n\nPredicates help route requests on the way in.\n\nResponses generate the responses on the way out.\n\nBehaviors postprocess the responses before shipping them over the wire (figure 7.1).\n\nFigure 7.1. Behaviors can transform a response from a stub before it goes out via the imposter.\n\nAlthough nothing prevents you from using behaviors with inject responses, most behaviors exist to allow you to reduce the amount of complexity inherent in using\n\nJavaScript to craft the response. Behaviors are a way of avoiding the complexity of inject responses in favor of\n\nsimpler is responses, while still being able to provide appropriate dynamism to your response.\n\nBehaviors sit alongside the type of response in the stub definition, as shown in listing 7.1. You can combine multiple behaviors together, but only one of each type.\n\nNo behavior should depend on the order of execution of other behaviors. That is an implementation detail subject\n\nto change without notice.\n\nListing 7.1. Adding behaviors to a stub definition\n\n{\n\n\"responses\": [{\n\n\"is\": { \"statusCode\": 500 },\n\n\"_behaviors\": { 1 \"decorate\": ..., 2\n\n\"wait\": ... 3",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "} 1\n\n}]\n\n}\n\n1 All postprocessing steps on the is response\n\n2 See section 7.2.\n\n3 See section 7.3.\n\nIn this example, the is response will first merge the 500\n\nstatus code into the default response, then it will pass the generated response object to both the decorate and\n\nwait behaviors. Each behavior will postprocess the response in a specific way, which we’ll look at shortly.\n\nSome behaviors still rely on programmatic control of the postprocessing, which requires the --allowInjection flag to be set when starting mb. This carries with it all the\n\nsame security considerations we examined in the last chapter. We look at those behaviors next.\n\n7.2. DECORATING A RESPONSE\n\nThe bluntest instruments in the behavior toolbox are the decorate and shellTransform behaviors, which accept the response object as input and transform it in\n\nsome way, sending a new response object as output (figure 7.2).\n\nFigure 7.2. Decoration allows you to postprocess the response.",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "They’re quite similar to response injection, except they\n\nprovide more focused injection (in the case of decorate), or more flexibility (shellTransform).\n\n7.2.1. Using the decorate function\n\nWithout behaviors, you’d be forced to use an inject\n\nresponse if only one field of the response were dynamic. For example, assume you want to send the following\n\nresponse body back:\n\n{\n\n\"timestamp\": \"2017-07-22T14:49:21.485Z\",\n\n\"givenName\": \"Stubby\",\n\n\"surname\": \"McStubble\",\n\n\"birthDate\": \"1980-01-01\"\n\n}\n\nYou could capture this body in an is response, setting the body field to the JSON, but that would assume that\n\nan outdated timestamp is irrelevant to the test case at hand. That isn’t always a valid assumption. Unfortunately, translating that to an inject response hides the intent, as shown in the following listing.\n\nListing 7.2. Using an inject response to send a dynamic timestamp",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "{\n\n\"responses\": [{\n\n\"inject\": \"function () { return { body: {\n\ntimestamp: new Date(),\n\ngivenName: 'Stubby', surname: 'McStubble',\n\nbirthDate:\n\n'1980-01-01' } } }\"\n\n}]\n\n}\n\nTo make sense of what the response is doing, you have to\n\nextract the JavaScript function and stare at it. Compare that to combining an is response with a decorate\n\nresponse, which sends the same JSON over the wire without awkward translation, as you can see in the\n\n[1]\n\nfollowing listing.\n\n1\n\nAs always, you can follow along with the book’s source code at https://github.com/bbyars/mountebank-in-action.\n\nListing 7.3. Combining an is response with a decorate behavior\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n1 \"body\": {\n\n1\n\n\"givenName\": \"Stubby\",\n\n1\n\n\"surname\": \"McStubble\",\n\n1\n\n\"birthDate\": \"1980-01-01\"\n\n1 }\n\n1\n\n},\n\n\"_behaviors\": {\n\n\"decorate\": \"function (request, response,\n\nlogger) { 2\n\nresponse.body.timestamp = new Date(); }\"\n\n2 }\n\n}]\n\n}",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "1 Return this...\n\n2 ...but add a current timestamp.\n\nAs noted, this is like the post office adding a postmark to your envelope. You provide the core content with an is response, and the decorate behavior adds the current\n\ntimestamp to the message. The end response is the same as with the inject approach, but separating the static\n\npart of the response from the dynamic part often makes the code easier to maintain.\n\nThe decorate function isn’t as capable as a full inject\n\nresponse. Behaviors don’t have access to any user- controlled state like response injection. They don’t allow\n\nasynchronous responses, which eliminates a large class of JavaScript I/O operations, as discussed in chapter 6. That said, the decorate behavior does allow the majority of the response message to be visible in a static is response, which simplifies maintenance of your test data.\n\n7.2.2. Adding decoration to saved proxy responses\n\nBehaviors are agnostic to the type of response they are applied to, which means you can decorate a proxy response as well. But by default, the decoration applies only to the proxy response itself, not to the is response it saves. (See figure 7.3.)\n\nFigure 7.3. Behaviors applied to proxies donʼt transfer to the saved responses.",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "You can add a couple of behaviors to the saved is\n\nresponses, including decorate. You have to configure the proxy with the decorate behavior. We’ll work with\n\na more complicated example than updating a timestamp to show how proxying and decoration work hand in\n\nglove.\n\nMost industrial APIs include some sort of rate limiting to prevent denial of service. The Twitter API represents a standard approach, where Twitter sends back an x- rate-limit-remaining header in the response to let\n\nthe user know how many requests the API user has left for a certain time frame. Once those requests are spent,\n\nTwitter will send a 429 HTTP status code (Too Many Requests) until the time period is up.\n\n[2]\n\nAt times you may\n\nwant to test the consumer’s response when it triggers rate limit errors midstream through a workflow. One\n\noption is to proxy all requests to the downstream rate- limited service (using the proxyAlways mode described\n\nin chapter 5). But it may be difficult to capture a rate limit",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "scenario through proxying real traffic. Another option is to capture the first response and use decoration to\n\ntrigger a rate limit error after a few requests (figure 7.4).\n\n2\n\nYou can read the full details at https://dev.twitter.com/rest/public/rate-limiting.\n\nFigure 7.4. Manufacturing a rate limit exception on a recorded response\n\nSetting up this scenario requires you to proxy to the\n\ndownstream server to save the response but add a decorate behavior on the saved response, as in the\n\nfollowing listing. The original proxy response will be undecorated, returning the response captured from the\n\ndownstream service.\n\nListing 7.4. Adding a decorate behavior to recorded responses\n\n{\n\n\"responses\": [{ \"proxy\": {\n\n\"to\": \"http://downstream-service.com\", 1\n\n\"mode\": \"proxyOnce\", 2\n\n\"addDecorateBehavior\": \"...\" 3",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "}\n\n}]\n\n}\n\n1 The base URL of the downstream service\n\n2 Only captures the first response\n\n3 Adds a decorator to the recorded response (see listing 7.5\n\nfor further content)\n\nThe decorator function will have access to the saved response and can change the x-rate-limit-\n\nremaining header or return an error as desired by the test case. In the code below, you will decrement the\n\nheader 25 requests at a time to accelerate reaching a rate limit error, but you can tweak that value according to your test scenario. Because decorate functions don’t have access to the same ability to save state that inject\n\nresponses do, you have to use a file to store the last value sent for the x-rate-limit-remaining header, as\n\nshown in the following listing.\n\nListing 7.5. Decorator function to accelerate a rate limit exception\n\nfunction (request, response) {\n\nvar fs = require('fs'),\n\n1 currentValue = parseInt(\n\n2\n\nresponse.headers['x-rate-limit-\n\nremaining']), 2\n\ndecrement = 25;\n\nif (fs.existsSync('rate-limit.txt')) {\n\n3 currentValue = parseInt(\n\n3\n\nfs.readFileSync('rate-limit.txt'));\n\n3\n\n}\n\n3\n\nif (currentValue <= 0) { response.statusCode = 429;\n\n4\n\nresponse.body = {",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "4\n\nerrors: [{\n\n4\n\ncode: 88,\n\n4 message: 'Rate limit exceeded'\n\n4\n\n}]\n\n4\n\n};\n\n4\n\nresponse.headers['x-rate-limit-remaining'] =\n\n0; 4 }\n\nelse {\n\nfs.writeFileSync('rate-limit.txt',\n\n5\n\ncurrentValue - decrement);\n\n5\n\nresponse.headers['x-rate-limit-remaining'] =\n\n6 currentValue - decrement;\n\n6\n\n}\n\n}\n\n1 Node’s module for filesystem access\n\n2 Gets the value in the recorded response\n\n3 Gets the saved value\n\n4 Sends the error response\n\n5 Saves the next value\n\n6 Updates the header\n\nWhat happened to nonblocking I/O?\n\nIn the last chapter, I described how JavaScript and\n\nNode.js use nonblocking I/O, which required adding asynchronous support to response injection. That is still\n\ntrue, but Node.js has added a small number of blocking, synchronous calls for common filesystem operations.\n\nNotice how the function names used in listing 7.5 end in Sync (fs.existsSync, fs.readFileSync, and\n\nfs.writeFileSync). These are special convenience functions that break out of the standard nonblocking I/O",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "model. To the best of my knowledge, no such convenience functions exist for I/O that has to traverse\n\nthe network.\n\nYou were forced to use the filesystem to save state\n\nbecause decorators cannot save state directly, and you were forced to use the Sync functions because\n\ndecorators do not support asynchronous operations. Both of these are supported in response injection, as\n\ndescribed in chapter 6. Future versions of mountebank may support them in decorators as well. The next behavior, shellTransform, suffers from neither of these limitations.\n\n7.2.3. Adding middleware through shellTransform\n\nThe next behavior is both the most general purpose and\n\nthe most powerful, and both of those aspects come with added complexity. Like decorate, shellTransform\n\nallows you programmatic postprocessing of the response. But it doesn’t require the use of JavaScript, and it allows\n\nyou to chain together a series of postprocessing transformations (figure 7.5).\n\nFigure 7.5. The shellTransform behavior allows you to combine multiple transformations piped through the shell.",
      "content_length": 1067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "To see how it works, let’s take the two transformations\n\nyou’ve already seen (adding a timestamp and triggering a rate limit exception) and convert them to shellTransform behaviors. You implement each transformation as a command line application that\n\naccepts the JSON-encoded request and JSON-encoded response as parameters passed in on standard input and\n\nreturns the transformed JSON-encoded response on standard output. You’ll start by wiring up the imposter\n\nconfiguration, as shown in the following listing.\n\nListing 7.6. Imposter configuration for shellTransform\n\n{ \"responses\": [{\n\n\"is\": {\n\n\"headers\": { 1\n\n\"x-rate-limit-remaining\": 3 1\n\n}, 1\n\n\"body\": { 2\n\n\"givenName\": \"Stubby\", 2\n\n\"surname\": \"McStubble\", 2 \"birthDate\": \"1980-01-01\" 2",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "} 2\n\n},\n\n\"_behaviors\": {\n\n\"shellTransform\": [\n\n\"ruby scripts/applyRateLimit.rb\", 3 \"ruby scripts/addTimestamp.rb\" 4\n\n]\n\n}\n\n}]\n\n}\n\n1 Transformed with applyRateLimit.rb\n\n2 Transformed with addTimestamp.rb\n\n3 This will execute first.\n\n4 Then this will execute on the transformed response.\n\nIn this example, you have chosen to pipe the\n\ntransformations through Ruby scripts, but it could’ve been any language. The code in applyRateLimit.rb is a\n\nsimple Ruby conversion of the code in listing 7.5. Mountebank passes two parameters on standard input—\n\nthe request and the response. Here you only need the response, as shown in the following listing.\n\nListing 7.7. Ruby script to transform the response to trigger a rate limit error\n\nrequire 'json'\n\n1\n\nresponse = JSON.parse(ARGV[1])\n\n2\n\nheaders = response['headers']\n\ncurrent_value = headers['x-rate-limit- remaining'].to_i\n\nif File.exists?('rate-limit.txt')\n\ncurrent_value = File.read('rate-\n\nlimit.txt').to_i\n\nend\n\nif current_value <= 0 response['statusCode'] = 429\n\nresponse['body'] = {\n\n'errors' => [{\n\n'code' => 88, 'message' => 'Rate limit\n\nexceeded'\n\n}]",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "}\n\nresponse['headers']['x-rate-limit-remaining'] =\n\n0\n\nelse\n\nFile.write('rate-limit.txt', current_value - 25)\n\nheaders['x-rate-limit-remaining'] =\n\ncurrent_value - 25\n\nend\n\nputs response.to_json\n\n3\n\n1 Ruby module imported for JSON handling\n\n2 The second command-line parameter is the current\n\nJSON response.\n\n3 Prints the transformed response to stdout\n\nSome syntax changes are obvious from the JavaScript code to the Ruby code (primarily a different hash syntax) and they use some different functions (to_i instead of parseInt), but most of the code looks like a Ruby\n\ndecorator. The key differences are the input (parsing the response from the command line) and the output\n\n(printing the transformed response to stdout). In the following listing, you do the same thing with\n\naddTimestamp.rb.\n\nListing 7.8. Ruby script to add a timestamp to the response JSON\n\nrequire 'json'\n\nresponse = JSON.parse(ARGV[1]) response['body']['timestamp'] = Time.now.getutc\n\nputs response.to_json\n\nBy chaining together transformations, shellTransform acts as a way of adding a transformation pipeline to your response handling,\n\nallowing as much complexity as you require. My standard advice still applies: it’s good to have such power\n\nfor when you absolutely need it, but try not to need it.",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "7.3. ADDING LATENCY TO A RESPONSE\n\nThe next behavior we’ll look at is a whole lot simpler and doesn’t require you to set the --allowInjection command line flag. Sometimes you need to simulate latency in responses, and the wait behavior tells mountebank to take a nap before returning the response.\n\nYou pass it the number of milliseconds to sleep as follows.\n\nListing 7.9. Using a wait behavior to add latency\n\n{\n\n\"is\": {\n\n\"body\": {\n\n\"name\": \"Sleepy\"\n\n}\n\n}, \"_behaviors\": {\n\n\"wait\": 3000 1\n\n}\n\n}\n\n1 Adds 3 seconds of latency\n\nLike the decorate behavior, you can add the wait\n\nbehavior to saved responses that proxies generate. When you set the addWaitBehavior to true on a proxy, mountebank will automatically fill in the generated wait\n\nbehavior based on how long the real downstream call took. I show how to use that to create robust\n\nperformance tests in chapter 10.\n\n7.4. REPEATING A RESPONSE MULTIPLE TIMES\n\nSometimes you need to send the same response multiple\n\ntimes before moving on to the next response. You can copy the same response multiple times in the responses array, but that practice is generally frowned on by the software community, as it hurts\n\nmaintainability. It’s an important enough concept that it even has its own acronym: DRY (Don’t Repeat Yourself).",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "The repeat behavior lets the computer do the repeating for you (figure 7.6). It accepts the number of times you\n\nwant to repeat the response, and mountebank helps you avoid those snooty software engineers looking down\n\ntheir noses at you for not being DRY enough.\n\nFigure 7.6. Repeating a response multiple times\n\nA common use case involves triggering an error response\n\nafter a set number of happy path responses. An example I already used a couple of times in this book involves\n\nquerying an inventory service. In chapters 3 and 4, I showed how you can use a list of responses to show the\n\n[3]\n\ninventory for a product running out over time:\n\n3\n\nAs we did earlier, we are returning an overly simplistic body to focus on only the point of the example.\n\n{\n\n\"responses\": [\n\n{ \"is\": { \"body\": \"54\" } },\n\n{ \"is\": { \"body\": \"21\" } },\n\n{ \"is\": { \"body\": \"0\" } }\n\n]\n\n}\n\nMost test cases don’t require this level of specificity. In slightly oversimplified terms, a consumer of an inventory service only cares about two scenarios:\n\nThe inventory is greater than zero (or greater than or equal to the\n\nquantity being ordered).\n\nThe inventory is zero (or less than the quantity being ordered).\n\nSimplifying even further, the only two scenarios that matter for this test case are\n\nA happy path",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "An out-of-inventory error\n\nThe only slightly complicated factor is that you might\n\nwant to return a few happy path responses before returning an out-of-inventory error. You can do that with only two responses and a repeat behavior, as shown in the following listing.\n\nListing 7.10. Using a repeat behavior to return an error a\u0000er a small number of successes\n\n{\n\n\"responses\": [\n\n{\n\n\"is\": { \"body\": \"9999\" }, 1\n\n\"_behaviors\": { \"repeat\": 3 } 2 },\n\n{\n\n\"is\": { \"body\": \"0\" } 3\n\n}\n\n]\n\n}\n\n1 Return the happy path...\n\n2 ...three times.\n\n3 Then return the error path.\n\nTest case construction\n\nWe have looked at some advanced mountebank\n\ncapabilities over the course of this book. Sometimes those capabilities are indispensable for solving\n\ncomplicated test problems. But you can simplify most test cases to a small essential core of what you’re trying\n\nto test, and removing the noise in the test data setup helps keep the focus on that core. The repeat example\n\nwe just looked at shows how the thought process of simplifying your test case has knock-on benefits to your\n\ntest data management.\n\n7.5. REPLACING CONTENT IN THE RESPONSE",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "You can always add dynamic data to a response through an inject response, or through the decorate and\n\nshellTransform behaviors. But two additional behaviors support inserting certain types of dynamic\n\ndata into the response without the overhead of programmatic control.\n\n7.5.1. Copying request data to the response\n\nThe copy behavior allows you to capture some part of the request and insert it into the response. Imagine that\n\nthe system under test depended on a service reflecting the account ID from the request URL back in the\n\nresponse body, so that (for example) when you send a GET request to /accounts/8731, you get a response that\n\nreflects that ID and otherwise resembles my account profile in various online forums I participate in:\n\n{\n\n\"id\": \"8731\", 1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon request\"\n\n}\n\n1 This has to match the ID in the path.\n\nThis response has two core aspects:\n\nThe id, which has to match the one provided on the request URL\n\nThe test data you need for your scenario\n\nA standard is response supports managing scenario-\n\nspecific test data, and the copy behavior allows you to insert the id from the request. Copying the id from the\n\nrequest to the response requires you to reserve a slot in the response that you can replace and to be able to select\n\nonly the data you want from the request. The first part is easier—you add any token you choose to the response, as\n\nshown in the following listing.",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Listing 7.11. Specifying a token in the response to replace with a value from the request\n\n{\n\n\"is\": {\n\n\"body\": { \"id\": \"$ID\", 1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon\n\nrequest\"\n\n}\n\n} }\n\n1 A placeholder that the copy behavior will replace with\n\nthe value\n\nThe first section of the copy behavior requires that you\n\nspecify the request field you’re copying from and the response token you need to replace:\n\n{\n\n\"from\": \"path\", \"into\": \"$ID\",\n\n...\n\n}\n\nThe only part you need to fill in is the part that selects the id. The copy behavior (and the lookup behavior, which we look at next) uses some of the same predicate\n\nmatching capabilities we looked at in chapter 4, specifically regular expressions, XPath, and JSONPath.\n\nRecall that each predicate applies a matching operation against a request field. Whereas a predicate tells you whether or not the match was successful, the copy and lookup behaviors are able to grab the specific text in the\n\nrequest field that matched.\n\nFor this example, a regular expression will do the trick. You need to capture a string of digits at the end of the request path. You can use some of the regex primitives we looked at in chapter 4 to make that selection:",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "\\d — A digit, 0–9 (you have to double-escape the backslash in JSON)\n\n+ — One or more times\n\n$ — The end of the string\n\nPutting it all together, the stub would look like the\n\nfollowing listing.\n\nListing 7.12. Using a copy behavior to insert the ID from the URL into the response body\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": { \"id\": \"$ID\",\n\n1\n\n\"name\": \"Brandon Byars\",\n\n\"description\": \"Devilishly handsome\",\n\n\"height\": \"Lots of it\",\n\n\"relationshipStatus\": \"Available upon\n\nrequest\"\n\n} },\n\n\"_behaviors\": {\n\n\"copy\": [{\n\n2\n\n\"from\": \"path\",\n\n3\n\n\"into\": \"$ID\",\n\n4 \"using\": {\n\n5\n\n\"method\": \"regex\",\n\n5\n\n\"selector\": \"\\\\d+$\"\n\n5\n\n}\n\n5 }]\n\n}\n\n}]\n\n}\n\n1 The token to replace\n\n2 An array—multiple replacements are allowed\n\n3 The request field to copy from\n\n4 The token to replace",
      "content_length": 762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "5 The selection criteria to act on the request path\n\nThere’s a lot more to this behavior, but before we get too\n\nfar, I should point out a couple of aspects that you may have already noticed. First, the copy behavior accepts an\n\narray, which means you can make multiple replacements in the response. Each replacement should use a different\n\ntoken, and each one can select from a different part of the request.\n\nThe other thing to notice is that you never specify where\n\nthe token is in the response. That’s by design. You could have put the token in the headers or even the\n\nstatusCode, and mountebank would replace it. If the token is listed multiple times, mountebank will replace\n\neach instance, regardless of where it’s located in the response.\n\nUsing a grouped match\n\nThe previous example made an assumption that you\n\ncould define a regular expression that entirely matched the value you needed to grab and nothing else. That’s a\n\npretty weak assumption.\n\nMany services use some form of a globally unique identifier (GUID) as an id, and the path often extends\n\nbeyond the part containing the id. For example, the path might be /accounts/5ea4d2b5/profile, where\n\n“5ea4d2b5” is the id you need to copy. You can no longer rely on \\\\d+ as a selector because the id contains\n\nmore than digits. You certainly can rely on other mechanisms to match—for instance, by recognizing that the id follows the word “accounts” in the path:\n\naccounts/\\\\w+\n\nThat selector uses the “\\w” regular expression\n\nmetacharacter to capture a word character (letters and",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "digits) and adds the “+” to ensure that you capture one or more of them. Then it prefixes the “accounts/” to ensure\n\nyou’re grabbing the right portion of the path. With this expression, you do successfully grab the id.\n\nUnfortunately, you grab the “accounts/” literal string as well, and the replaced body would look like:\n\n{\n\n\"id\": \"accounts/5ea4d2b5\",\n\n...\n\n}\n\nRegular expressions support grouped matches to allow you to grab only the data you need from a match. Every regular expression has a default first group that’s the\n\nentire match. Every time you surround a portion of the selector with parentheses, you describe another group.\n\nYou will adjust your selector to add a group around the id portion of the path, while leaving the literal string\n\n“accounts/” to make sure you are grabbing the right portion of the path:\n\naccounts/(\\\\w+)\n\nWhen you match this regular expression against the string “/accounts/5ea4d2b5/profile”, you get an array of\n\nmatch groups that looks like\n\n[\n\n\"accounts/5ea4d2b5\",\n\n\"5ea4d2b5\"\n\n]\n\nThe first group is the entire match, and the second is the first parenthetical group. If you place an unadorned\n\ntoken in the response, as you did in the previous section, mountebank will replace it with the first index of the\n\narray, which corresponds to the entire match. But you",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "can add an index to the token corresponding to the index of the match group array, as shown in the following listing, which allows you to pinpoint the part of the path you want to copy with laser precision.\n\nListing 7.13. Using a grouped match to copy a portion of the request path\n\n{\n\n\"is\": {\n\n\"body\": {\n\n\"id\": \"$ID[1]\", 1\n\n...\n\n}\n\n},\n\n\"_behaviors\": { \"copy\": [{\n\n\"from\": \"path\",\n\n\"into\": \"$ID\", 2\n\n\"using\": {\n\n\"method\": \"regex\",\n\n\"selector\": \"accounts/(\\\\w+)\" 3\n\n}\n\n}] }\n\n}\n\n1 Specifies the index in the response token\n\n2 Specifies the base token in the behavior\n\n3 Uses a grouped match for more precision\n\nYou can always use indexed tokens in the response. Assuming you specify a token of $ID like you did in\n\nlisting 7.13, then putting $ID in the response is equivalent to putting $ID[0]. I doubt that matters much\n\nfor regular expressions, as I suspect most real-world use cases will have to use groups to grab the exact value they\n\nwant. That isn’t necessarily true for the other selection approaches that the copy behavior supports: xpath and\n\njsonpath.\n\nUsing an XPath selector\n\nAlthough a regular expression elegantly supports grabbing a value out of any request field, xpath and",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "jsonpath selectors can be useful to match values inside an incoming request body. They work similarly to how xpath and jsonpath predicates work, as described in chapter 4. The key difference is that predicate XPath and\n\nJSONPath selectors are used with a matching operator (like equals) to test whether the request matches,\n\nwhereas using those selectors with behaviors helps grab the matching text in the request to change the response.\n\nTake the following request body from the system under test, representing a list of accounts:\n\n<accounts xmlns=\"https://www.example.com/accounts\">\n\n<account id=\"d0a7b1b8\" />\n\n<account id=\"5ea4d2b5\" />\n\n<account id=\"774d4feb\" />\n\n</accounts>\n\nYour virtual response needs to reflect back details of the second account in the request. Those details are specific\n\nto your test scenario, but the ID has to match what was sent in the request body. You can use an XPath selector to grab the id attribute of the second account attribute, as shown in the following listing.\n\nListing 7.14. Using an XPath selector to copy a value from the request to the response\n\n{\n\n\"responses\": [{\n\n\"is\": {\n\n\"body\": \"<account><id>$ID</id>...\n\n</account>\" 1\n\n},\n\n\"_behaviors\": {\n\n\"copy\": [{ \"from\": \"body\",\n\n\"into\": \"$ID\",\n\n2\n\n\"using\": {\n\n\"method\": \"xpath\",\n\n3\n\n\"selector\": \"//a:account[2]/@id\",\n\n3",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "\"ns\": {\n\n3\n\n\"a\":\n\n\"https://www.example.com/accounts\" 3\n\n} 3\n\n}\n\n}]\n\n}\n\n}]\n\n}\n\n1 Tokenizes the response body\n\n2 Defines a token in the copy behavior\n\n3 Selects the value from the XML request body\n\nThe xpath selector and namespace support work\n\nidentically to xpath predicates (chapter 4) and predicate generators (chapter 5). As you saw with predicates,\n\nmountebank also supports JSONPath. We will look at an example with the lookup behavior shortly.\n\nVirtualizing a CORS preflight response\n\nCross-origin resource sharing, or CORS, is a standard that allows browsers to make cross-domain requests. In\n\nthe olden days, a browser would only execute JavaScript calls to the same domain as the one that served up the\n\nhosting web page. This same-origin policy is the bedrock of browser security, as it helps prevent a host of\n\nmalicious JavaScript injection attacks. But as websites became more dynamic and needed to pull behavior from\n\ndisparate resources spread across multiple domains, it also proved to be too restrictive. Creative developers\n\nfound creative hacks to bypass the same-origin policy, like JSONP, which manipulates the script element in\n\nan HTML document to pass JavaScript from a different domain to a callback function already defined. JSONP is\n\nconfusing and hard to understand because it works around the browser’s built-in security mechanism.",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "The CORS standard evolves the browser’s security model to allow the browser and server to both weigh in on\n\nwhether a cross-domain request is valid or not. The standard requires a preflight request for certain types of\n\ncross-domain requests to determine whether the request is valid. A preflight request is an HTTP OPTIONS call\n\nwith a few special headers that commonly trip up testers when creating virtual services\n\n[4]\n\n(figure 7.7).\n\n4\n\nFuture versions of mountebank likely will make virtualizing CORS preflight requests easier.\n\nFigure 7.7. A CORS preflight request to establish trust\n\nThe browser is set to automatically send these preflight requests for some types of cross-origin requests. If you\n\nwant to virtualize the cross-origin service so you can test the browser application, your virtual service needs to\n\nknow how to respond to a preflight request in a way that enables the browser to make the actual cross-origin request (for example, the call to POST /resource in figure 7.7). Copying the Origin header value from the",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "request header into the response header, as shown in the following listing, is a great example of using the copy\n\nbehavior and demonstrates tokenizing something other than the HTTP body.\n\nListing 7.15. Virtualizing a CORS preflight request\n\n{\n\n\"predicates\": [{\n\n1\n\n\"equals\": { \"method\": \"OPTIONS\" }\n\n1\n\n}],\n\n1\n\n\"responses\": [{ \"is\": {\n\n\"headers\": {\n\n\"Access-Control-Allow-Origin\":\n\n\"${ORIGIN}\", 2\n\n\"Access-Control-Allow-Methods\": \"PUT,\n\nDELETE\"\n\n}\n\n}, \"_behaviors\": {\n\n\"copy\": [{\n\n\"from\": { \"headers\": \"Origin\" },\n\n3\n\n\"into\": \"${ORIGIN}\",\n\n4\n\n\"using\": { \"method\": \"regex\", \"selector\":\n\n\".+\" } 5 }]\n\n}\n\n}]\n\n}\n\n1 Looks for a preflight request signature\n\n2 Tokenizes the response header\n\n3 Looks in the request Origin header\n\n4 Replaces the response header token...\n\n5 ...with the entire request header value.\n\nThe regular expression “.+” means “one or more characters” and effectively captures the entire request\n\nheader. Because you don’t need to use a grouped match, you can use the token in the response without an array",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "index. You can do a lot more with CORS configuration, but the copy approach satisfies the need for creating a\n\nflexible virtual service that reflects the client requests in a way that enables the client to make subsequent\n\nrequests.\n\n7.5.2. Looking up data from an external data source\n\nService virtualization is great for testing error flows,\n\nwhich are often difficult to reproduce on demand in real systems but happen enough in live systems that you still\n\nneed to test for them. The challenge is creating a set of test data that captures all the error flows in a visible and\n\nmaintainable way. Take account creation, for example. Using predicates, you could set up a virtual account\n\nservice that responds with different error conditions based on the name of the account you are trying to\n\ncreate. Let’s say the first error flow you need to test is what happens when the account already exists. The following configuration would ensure your virtual service\n\nreturns an error for a duplicate user when the name is “Kip Brady,” assuming that’s passed in as a JSON name\n\nfield in the request:\n\n{\n\n\"stubs\": [{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Kip Brady\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" }\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"duplicateEntry\",\n\n\"message\": \"User already exists\"\n\n}]\n\n}\n\n} }]",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "}]\n\n}\n\nIf you want “Mary Reynolds” to represent a user too young to register, you can use the same JSONPath\n\nselector to look for a different value:\n\n{\n\n\"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Kip Brady\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" }\n\n}], \"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"duplicateEntry\",\n\n\"message\": \"User already exists\"\n\n}] }\n\n}\n\n}]\n\n},\n\n{\n\n\"predicates\": [{\n\n\"equals\": { \"body\": \"Mary Reynolds\" },\n\n\"jsonpath\": { \"selector\": \"$..name\" } }],\n\n\"responses\": [{\n\n\"is\": {\n\n\"statusCode\": 400,\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"tooYoung\",\n\n\"message\": \"You must be 18 years old to register\"\n\n}]\n\n}\n\n}\n\n}]\n\n}\n\n]\n\n}",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "“Tom Larsen” can represent a 500 server error, and “Harry Smith” can represent an overloaded server. Both\n\nrequire new stubs.\n\nThis is obviously an unsustainable approach to managing\n\ntest data. The JSONPath selector is the same among all the stubs, as is the structure of the JSON body. You’d like\n\nto be able to centralize the test data in a CSV file, as shown in the following listing.\n\nListing 7.16. Centralizing error conditions in a CSV file\n\nname,statusCode,errorCode,errorMessage\n\nTom Larsen,500,serverError,An unexpected error\n\noccurred\n\nKip Brady,400,duplicateEntry,User already exists\n\nMary Reynolds,400,tooYoung,You must be 18 years\n\nold to register Harry Smith,503,serverBusy,Server currently\n\nunavailable\n\nThe lookup behavior, a close cousin of the copy\n\nbehavior, allows you to do this. Like the copy behavior, it replaces tokens in the response with dynamic data. The\n\nkey difference is where that dynamic data comes from. For the copy behavior, it’s the request. For the lookup\n\nbehavior, it’s an external data source. As of this writing, the only data source mountebank supports is a CSV file\n\n(figure 7.8). That likely will change by the time you are reading this.\n\nFigure 7.8. Looking up a value from a CSV file",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Before we get to how the replacement happens, let’s look\n\nat the lookup operation itself. As you can see in figure 7.8, a successful lookup requires three values:\n\nA key selected from the request (Kip Brady)\n\nThe connection to the external data source (data/accounts.csv)\n\nThe key column in the external data source (name)\n\nThose three values are sufficient to capture a row of values you can use in the replacement. You represent them in a way that closely resembles the copy behavior, as shown in the following listing.\n\nListing 7.17. Using a lookup behavior to retrieve external test data\n\n{\n\n\"responses\": [{\n\n\"is\": { ... },\n\n1\n\n\"_behaviors\": {\n\n\"lookup\": [{\n\n\"key\": {\n\n2 \"from\": \"body\",\n\n2\n\n\"using\": {\n\n2",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "\"method\": \"jsonpath\",\n\n2\n\n\"selector\": \"$..name\"\n\n2\n\n} 2\n\n},\n\n2\n\n\"fromDataSource\": {\n\n\"csv\": {\n\n3\n\n\"path\": \"examples/accounts.csv\",\n\n4 \"keyColumn\": \"name\"\n\n5\n\n}\n\n},\n\n\"into\": \"${row}\"\n\n6\n\n}]\n\n} }]\n\n}\n\n1 We’ll come back to this.\n\n2 Selects the value from the request\n\n3 Type of data source\n\n4 Path to CSV file\n\n5 Column name to match request value\n\n6 Response token name\n\nThe key part of the lookup behavior is similar to the\n\ncopy behavior we looked at earlier. It allows you to use a regex, xpath, or jsonpath selector to look in a\n\nrequest field and grab a value. You can add an index field to use a grouped regular expression match.\n\nThe into field is also the same as what you saw with\n\ncopy. Here you used ${row} as the token name. It can be anything you like. As far as mountebank is concerned,\n\nit it’s a string. The addition is what you see in the fromDataSource field. For CSV data sources, you",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "specify the path to the file (relative to the running mb process) and the name of the key column.\n\nIf you pass \"Kip Brady\" in as the name, your token (${row}) matches an entire row of values from the CSV\n\nfile. In JSON format, it would look like this:\n\n{\n\n\"name\": \"Kip Brady\",\n\n\"statusCode\": \"400\",\n\n\"errorCode\": \"duplicateEntry\",\n\n\"errorMessage\": \"User already exists\"\n\n}\n\nThis highlights a secondary difference between copy and lookup: with a lookup behavior, your token represents an entire row of values, meaning each individual\n\nreplacement has to be indexed with the column name. Let’s look at the is response for your example, which\n\ntokenizes the responses you previously had to copy into multiple stubs, in the following listing.\n\nListing 7.18. Using tokens to create a single response for all error conditions\n\n{\n\n\"is\": {\n\n\"statusCode\": \"${row}['statusCode']\", 1\n\n\"body\": {\n\n\"errors\": [{\n\n\"code\": \"${row}['errorCode']\", 1\n\n\"message\": \"${row}['errorMessage']\" 1\n\n}] }\n\n},\n\n\"_behaviors\": {\n\n\"lookup\": [{ ... }] 2\n\n}\n\n}\n\n1 Looks up the appropriate field in the row you looked up\n\n2 See listing 7.17.",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "The lookup behavior treats the token like a JSON object that you can key by the column name. This allows you to retrieve an entire row of data in the lookup behavior and use fields within the row to populate the response.\n\n7.6. A COMPLETE LIST OF BEHAVIORS\n\nFor reference, table 7.1 provides a complete list of behaviors that mountebank supports, including whether\n\nthey support affecting saved proxy responses and whether they require the --allowInjection\n\ncommand-line flag.\n\nTable 7.1. All behaviors that mountebank supports\n\nBehavior\n\nWorks on saved proxy responses?\n\nRequires injection support?\n\nDescription\n\ndecorate\n\nshellTransform\n\nwait\n\nrepeat\n\ncopy\n\nlookup\n\nyes\n\nno\n\nyes\n\nno\n\nno\n\nno\n\nyes\n\nyes\n\nno\n\nno\n\nno\n\nno\n\nUses a JavaScript function to postprocess the response Sends the response through a command- line pipeline for postprocessing Adds latency to a response Repeats a response multiple times Copies a value from the request into the response Replaces data in the response with data from an external data source based on a key from the request\n\nBehaviors are powerful additions to mountebank, and we\n\ncovered a lot of ground in this chapter. We will round out mountebank’s core capabilities in the next chapter when\n\nwe look at protocols.\n\nSUMMARY\n\nThe decorate and shellTransform behaviors are similar to response injection in that they allow programmatic transformation\n\nof the response. But they apply postprocessing transformation, and the shellTransform allows multiple transformations.\n\nThe wait behavior allows you to add latency to a response by passing in the number of milliseconds to delay it.",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "The repeat behavior supports sending the same response multiple times.\n\nThe copy behavior accepts an array of configurations, each of which selects a value from the request and replaces a response\n\ntoken with that value. You can use regular expressions, JSONPath,\n\nand XPath to select the request value.\n\nThe lookup behavior also accepts an array of configurations, each of which looks up a row of data from an external data source based\n\non the value selected from the request using regular expressions,\n\nJSONPath, and XPath. The token in the response is indexed by the field name.",
      "content_length": 582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Chapter 8. Protocols\n\nThis chapter covers\n\nHow protocols work in mountebank\n\nAn in-depth look at the TCP protocol\n\nHow to stub a bespoke text-based TCP protocol\n\nHow to stub a binary .NET Remoting service\n\nLet’s get real: faking it only gets you so far. At some\n\npoint, even the best virtual services have to lay down some real bits on the wire.\n\nThe functionality we have explored so far—responses,\n\npredicates, and behaviors—is largely the realm of stubbing and mocking tools. Those stubbing and\n\nmocking tools were created for creating test doubles in process, allowing you to perform focused tests by\n\nmethodically manipulating dependencies. Responses correspond to what stub functions return, and predicates\n\nexist to provide different results based on the way the function is called (for example, returning a different\n\nresult based on a request parameter). I’m not aware of any in-process mocking tools that have the concept of\n\nbehaviors per se, but there’s nothing preventing them. Behaviors are transformations on the result.\n\nBut there is one thing that virtual services do that\n\ntraditional stubs don’t: they respond over the network. All the responses so far have been HTTP, but\n\nmountebank supports other ways of responding. Enterprise integration is often messy, and sometimes\n\nyou’ll need to virtualize non-HTTP services. Whether you have custom remote procedure calls or a mail server as\n\npart of your stack, service virtualization can help. And now, at long last, it’s time to explore mountebank’s\n\nsupport for network protocols.",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "8.1. HOW PROTOCOLS WORK IN MOUNTEBANK\n\nProtocols are where the rubber meets the road in\n\nmountebank. The core role of the protocol is to translate the incoming network request into a JSON\n\nrepresentation for predicates to operate on, and to translate the JSON mountebank response structure into\n\nthe wire format expected by the system under test (figure 8.1).\n\nFigure 8.1. The flow of a mountebank-generated HTTP response\n\nAll of the mountebank imposters that we’ve seen so far\n\nare full-featured HTTP servers, and the bits they put on the wire conform to the HTTP protocol. That’s the secret\n\nsauce that allows you to repoint the system under test to mountebank with no changes: it sends out an HTTP\n\nrequest and gets an HTTP response back. It has no need to know that an imposter formed the response by\n\nemploying a secret cabal of stubs to match the request",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "with predicates, form a JSON response, and postprocess it with behaviors. All the system under test cares about is\n\nthat mountebank accepts an HTTP request and returns a bunch of bits over the wire that look like an HTTP\n\nresponse.\n\nMountebank, unique in the open source service\n\nvirtualization world, is multiprotocol. The clean separation of concepts allows the stubbing functionality\n\nto work regardless of what protocol your system under test expects. We’ll explore that in the context of older\n\nremote procedure call (RPC) protocols, but before we get there, let’s start with a primer on a foundational building\n\nblock of network communication.\n\n8.2. A TCP PRIMER\n\nMountebank supports the TCP protocol, but TCP isn’t on equal footing with HTTP/S. It’s more accurate to say that\n\nmountebank supports a range of custom application protocols built on top of TCP. Most conceptual\n\nnetworking models show protocols in layers, and it takes a whole suite of protocols to make something as complex\n\nas the internet work. (See figure 8.2.)\n\nFigure 8.2. A client application talking to a server application over the internet",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "The genius of the TCP/IP stack is that clients and server\n\nprocesses can act as if they’re talking directly to each other, even when they’re on remote machines. When you",
      "content_length": 169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "visit the mountebank website (http://www.mbtest.org), your browser can act as if it’s speaking directly with the\n\nweb server on the other end. The same is true when your system under test makes an HTTP call. Regardless of\n\nwhether the service it’s accessing is real or virtualized, the client code can operate as if there’s a direct\n\nconnection to the server.\n\nThe reality is more complicated. HTTP is an application\n\nprotocol and depends on TCP as the transport protocol to deliver to the remote host. TCP in turn relies on\n\ndownstream protocols to route between networks (the IP protocol) and to interface with the routers on the same\n\nnetwork (the network is often referred to as the “link,” which is why the lowest layer is called the link layer in\n\nfigure 8.2).\n\nWhat your web browser is doing is forming an HTTP request and passing it to the local operating system,\n\nwhich hands off the request to the TCP protocol implementation. TCP lovingly wraps the HTTP message\n\nin an envelope that adds a bunch of delivery guarantees and performance optimizations. Then it hands control\n\nover to the IP protocol, which once again wraps the whole message and adds addressing information that the\n\ncore infrastructure of the internet knows how to use to route to the correct remote machine. Finally, the doubly\n\nwrapped HTTP message is handed off to the device driver for the network interface on your computer, which\n\nyet again wraps the message with some Ethernet or Wi- Fi information needed to transmit the whole package to\n\nyour router, which happily forwards it to the next network. (See figure 8.3.) The process works in reverse once it reaches the right server machine.\n\n[1]\n\n1\n\nFigure 8.3 is inspired by a similar image on the Wikipedia page describing the internet protocol suite, which has a more comprehensive explanation of how layering works: https://en.wikipedia.org/wiki/Internet_protocol_suite.",
      "content_length": 1905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Figure 8.3. Transforming an HTTP request to route across the network\n\nTCP enables host-to-host communication, but it’s the\n\napplication protocols on top that allow a client process to talk to a server process. HTTP is the most famous\n\napplication protocol but far from the only one. Mountebank aspires to treat well-known application\n\nprotocols like HTTP as first class citizens, but a host of niche or custom application protocols exist in the archaeological substratum of enterprise integration, and\n\nmountebank’s support of the TCP protocol also provides a way to virtualize them.\n\n8.3. STUBBING TEXT-BASED TCP-BASED RPC\n\nFor those of you who grew up in a world where distributed programming was commonplace, it can be a\n\nbit bewildering to look at some of the ways applications used to integrate. Imagine you are a C++ programmer in\n\na bygone era being paid to integrate two applications over the network. The challenges of distributed\n\nprogramming aren’t commonly understood yet. Early attempts at formalizing RPC using standards like CORBA\n\nmay have happened, but they seem overly complicated. It seems much simpler to pass a function name and a few",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "parameters to a remote socket and expect it to return a status code indicating whether the function succeeded\n\nand a return value. If you take out the networking, it looks similar to how in-process function calls work.\n\nNow, 20 or 30 years later, younger generations are still adding capabilities to your custom RPC code because it’s\n\nso central to keeping the lights on that it’s cheaper to keep it than to rip it out. They may not like it, but none of\n\nthem have ever written code that’s stayed in production for decades. Like it or not, this is a common scenario in\n\nmany long-standing enterprises.\n\nLet’s imagine that the remote server manages core inventory. The payload of a TCP packet making an RPC\n\nrequest to change the inventory may look like this, for example:\n\nupdateInventory 1\n\n5131 2 -5 3\n\n1 Function name\n\n2 First parameter (for example, productId)\n\n3 Second parameter (for example, inventoryDelta)\n\nAnd the response may look like this:\n\n0 1\n\n1343 2\n\n1 Status code\n\n2 Total inventory\n\nIn this example, new lines separate parameters, and the\n\nschema is implicit rather than defined by something like JSON or XML. Mountebank won’t be able to understand\n\nthe semantics of the RPC, so it encapsulates the payload in a single data field.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "The flow of data, as shown in figure 8.4, looks similar to the flow of data in a virtual HTTP service (figure 8.1). The\n\nonly difference is the format for the request and response.\n\nFigure 8.4. Virtualizing a custom TCP protocol\n\n8.3.1. Creating a basic TCP imposter\n\nCreating the TCP imposter is as simple as changing the protocol field to tcp:\n\n{ \"protocol\": \"tcp\",\n\n\"port\": 3000\n\n}\n\nThat configuration is sufficient for mountebank to spin\n\nup a TCP server listening on port 3000. It’ll accept the request, but the response will be blank. If you want to",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "virtualize the call to updateInventory, you do so with the same predicates and response capability that you’ve\n\nseen for HTTP, as shown in the following listing. The only difference is the JSON structure for requests and responses. For tcp, both the request and response contain a single data field.\n\nListing 8.1. Virtualizing a TCP updateInventory call\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"predicates\": [{\n\n\"startsWith\": { \"data\": \"updateInventory\" }\n\n1\n\n}], \"responses\": [{\n\n\"is\": { \"data\": \"0\\n1343\" }\n\n2\n\n}]\n\n}]\n\n}\n\n1 Looks for the appropriate function\n\n2 Returns the protocol-specific response format\n\nYou can test out your imposter using an application like telnet. Telnet opens an interactive TCP connection to a\n\nserver, which makes it tricky to script. Netcat (http://nc110.sourceforge.net/) is like a noninteractive\n\ntelnet, which makes it ideally suited for testing TCP- based services. You can trigger your inventory RPC call\n\nresponse and test out your TCP imposter with netcat using the following command:\n\necho \"updateInventory\\na32fbd\\n-5\" | nc localhost\n\n3000\n\nYou wrap the request message in a string and pipe it to netcat (nc), sending it to the correct socket. It’ll send\n\nback the virtual response you configured to the terminal:",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "0\n\n1343\n\n8.3.2. Creating a TCP proxy\n\nTCP imposters work with the other response types as well. You can record and replay using a proxy like you\n\ncan with HTTP. For example, if the real service is listening on port 3333 of remoteservice.com, you could set up a record/replay imposter by pointing the to field on the proxy configuration to the remote socket, as\n\nshown in the following listing.\n\nListing 8.2. Using a TCP record/replay proxy\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{ \"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"tcp://remoteservice.com:3333\" 1\n\n}\n\n}]\n\n}]\n\n}\n\n1 Destination of remote service\n\nThe proxy behavior is identical to what you saw in chapter 5. In the default mode (proxyOnce),\n\nmountebank will save the response and serve it up on the next request without making the downstream call again.\n\nYou can test this out with netcat:\n\necho \"updateInventory\\na32fbd\\n-5\" | nc -q 1\n\nlocalhost 3000\n\nNotice the added “-q 1” parameter. By default, when netcat makes a TCP request, it closes the client end of the\n\nconnection immediately, which is appropriate for one- way fire-and-forget-style communication. It isn’t",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "appropriate for two-way request-response-style communication common in RPC. Because it takes a\n\nsmall amount of time for mountebank to make the downstream call to get the response, by the time\n\nmountebank tries to respond, it may discover that nobody is listening. The “-q 1” parameter tells netcat to\n\nwait one second before closing the connection, so you’ll see the response on the terminal.\n\nUnfortunately, the “-q” parameter isn’t present on all versions of netcat, including the default version on Mac\n\nand Windows computers. If you leave it off, you won’t get a response on the terminal and you’ll see the error in the\n\nmountebank logs when it cannot send the response. But subsequent calls will still work, as mountebank now has\n\na saved version of the response and can respond immediately.\n\nYou can use predicateGenerators with the TCP\n\nprotocol as well, but it isn’t very discriminatory because there’s only one field. For example, the following\n\nconfiguration makes a new downstream call anytime anything in the RPC call is different.\n\nListing 8.3. A TCP proxy with predicateGenerators\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://remoteservice.com:3333\",\n\n\"predicateGenerators\": [{\n\n\"matches\": { \"data\": true } 1\n\n}]\n\n}\n\n}]\n\n}]\n\n}\n\n1 Generates a new stub for each new payload",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Although it would be nice to generate predicates only on the function name, you can’t. Mountebank has no way of\n\nknowing what the function name is, so any parameter that changes will force a new downstream call.\n\nIf you’re lucky, the custom RPC protocol uses a payload format that mountebank understands: JSON or XML. If\n\nso, then you can get a bit more specific. We look at an example next.\n\n8.3.3. Matching and manipulating an XML payload\n\nI doubt you’ll see many of these custom RPC protocols\n\nthat use JSON, for the simple reason that by the time JSON was created, HTTP was already a predominant\n\napplication integration approach. If you do see it, all of the JSONPath capability you’ve seen so far will work.\n\nXML has been around a bit longer, and in fact one of the\n\nfirst attempts at using HTTP for integration was called POX over HTTP, where POX stood for Plain Ol’ XML. Let’s translate your updateInventory RPC payload to XML:\n\n<functionCall>\n\n<functionName>updateInventory</functionName>\n\n<parameters>\n\n<parameter name=\"productId\" value=\"5131\" /> <parameter name=\"amount\" value=\"-5\" />\n\n</parameters>\n\n</functionCall>\n\nYou can easily imagine other function calls, for example,\n\nthis one might be a call to get the inventory for product 5131:\n\n<functionCall> <functionName>getInventory</functionName>\n\n<parameters>\n\n<parameter name=\"productId\" value=\"5131\" />",
      "content_length": 1368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "</parameters>\n\n</functionCall>\n\nNow it becomes easier to build a more robust set of predicateGenerators for your proxy. If you want to\n\nsave different responses for different combinations of function names and product IDs, you can do so, as\n\nshown in the following listing.\n\nListing 8.4. Using XPath predicateGenerators with a TCP proxy\n\n{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\",\n\n\"predicateGenerators\": [\n\n{\n\n\"matches\": { \"data\": true },\n\n1\n\n\"xpath\": {\n\n1\n\n\"selector\": \"//functionName\" 1\n\n}\n\n1\n\n},\n\n{\n\n\"matches\": { \"data\": true },\n\n2\n\n\"xpath\": { 2\n\n\"selector\":\n\n2\n\n\"//parameter[@name='productId']/@value\"\n\n2\n\n}\n\n2\n\n} ]\n\n}\n\n}]\n\n}\n\n1 The XML functionName must match.\n\n2 The productId must match.",
      "content_length": 713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "All of the behaviors work with the TCP protocol as well, including those like the copy behavior that can use\n\nXPath to select values from the request.\n\n8.4. BINARY SUPPORT\n\nNot all application protocols speak in plain text. Many of\n\nthem instead pass a binary request/response stream, which makes them challenging to virtualize. Challenging,\n\nbut not impossible—remember mountebank’s mission statement: to keep the easy things easy while making the\n\nhard things possible.\n\nMountebank makes virtualizing binary protocols possible in two ways. First, it supports serializing request\n\nand response binary streams using Base64 encoding. Second, nearly all of the predicates work against a binary\n\nstream with exactly the same semantics they use against text.\n\n8.4.1. Using binary mode with Base64 encoding\n\nJSON—the lingua franca of mountebank—doesn’t\n\ndirectly support binary data. The workaround is to encode a binary stream into a string field. Base64\n\nreserves 64 characters—upper- and lowercase letters, digits, and two punctuation marks—and maps them to\n\nthe binary equivalents. Having 64 options allows you to 6 encode six bits at a time (2 = 64).\n\nAny modern language library will support Base64 encoding. Here’s an example in JavaScript (node.js):\n\nvar buffer = new Buffer(\"Hello, world!\"); console.log(buffer.toString(\"base64\"));\n\n1\n\n1 Prints out SGVsbG8sIHdvcmxkIQ==",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Similarly, decoding from Base64 is done using the Buffer type as well, as in this JavaScript example:\n\nvar buffer = new Buffer(\"SGVsbG8sIHdvcmxkIQ==\");\n\n1\n\nconsole.log(buffer.toString(\"utf8\"));\n\n2\n\n1 Base64 encoded value\n\n2 Prints out Hello, world!\n\nHaving a TCP imposter return binary data requires letting mountebank know that you want a binary\n\nimposter and Base64 encoding the response, as shown in the following listing.\n\nListing 8.5. Setting up a binary response from an imposter\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n1\n\n\"stubs\": [{ \"responses\": [{\n\n\"is\": { \"data\": \"SGVsbG8sIHdvcmxkIQ==\" }\n\n2\n\n}]\n\n}]\n\n}\n\n1 Switches to binary mode\n\n2 Returns binary equivalent of “Hello, world!”\n\nOnce you set the mode to binary, mountebank knows to\n\ndo the following:\n\nInterpret all response data as Base64-encoded binary streams,\n\nwhich it will decode when it responds over the wire\n\nSave all proxy responses as Base-64-encoded versions of the wire\n\nresponse\n\nInterpret all predicates as Base-64-encoded binary streams, which\n\nit will decode to match against the raw wire request",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "That last bullet point requires more explanation.\n\n8.4.2. Using predicates in binary mode\n\nBinary data is a stream of bits. It might be, for example,\n\n01001001 00010001. I have spaced the bits into two eight-bit combinations (octets) because, although a long\n\nstring of zeros and ones can be poetry to a computer, it gets a little tricky for us human folk to read. Using two\n\noctets also allows you to encode them as two numbers from 0–255 (2^8). In this case, that would be 73 17, or,\n\nin hexadecimal, 0x49 0x11. Hexadecimal is nice because it lacks any ambiguity—each two-digit hexadecimal\n\nnumber has 256 possibilities (16^2), the same number of possibilities encoded in an eight-bit octet.\n\nLet’s say you wanted to create a predicate that contains\n\nthat binary stream. To do so, you’d first need to encode it:\n\nvar buffer = new Buffer([0x49, 0x11]);\n\nconsole.log(buffer.toString('base64')); 1\n\n1 Prints out “SRE=”\n\nNow the predicate definition is conceptually the same as it is for text, as the following listing demonstrates.\n\nListing 8.6. Using a binary contains predicate\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n1 \"stubs\": [\n\n{\n\n\"predicates\": [{\n\n\"contains\": { \"data\": \"SRE=\" }\n\n2\n\n}],\n\n\"responses\": [{\n\n\"is\": { \"data\": \"TWF0Y2hlZAo=\" } 3",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "}]\n\n},\n\n{\n\n\"responses\": [{\n\n\"is\": { \"data\": \"RGlkIG5vdCBtYXRjaAo=\" } 4\n\n}]\n\n}\n\n]\n\n}\n\n1 Puts the imposter in binary mode\n\n2 0x49 0x11\n\n3 “Matched”\n\n4 “Didn’t match”\n\nIf we add an octet—say 0x10—to our binary stream, the contains predicate still matches. The binary stream 0x10 0x49 0x11 encodes to “EEkR,” which clearly doesn’t\n\ncontain the text “SRE=”. Had you not configured the imposter to be in binary mode, mountebank would’ve\n\nperformed a simple string operation, and the predicate wouldn’t match. By switching to binary mode, you’re\n\ntelling mountebank to instead decode the predicate (SRE=) to a binary array ([0x49, 0x11]) and to see if the\n\nincoming binary stream ([0x10, 0x49, 0x11]) contains those octets. It does, so the predicate matches. You can test on the command line by using the base64 utility, which ships by default with most POSIX shells (for\n\nexample, Mac and Linux):\n\necho EEkR | base64 --decode | nc localhost 3000\n\nYou get a response of “Matched,” which corresponds to\n\nthe first response.\n\nNearly all of the predicates work this way: by matching against a binary array. The one exception is matches.\n\nRegular expressions don’t make sense in a binary world; the metacharacters don’t translate. In practice,",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "contains is probably the most useful binary predicate. It turns out that many binary RPC protocols encode a\n\nfunction name inside the request. The parameters may be serialized objects that are hard to match, but the\n\nfunction name is encoded text. We look at a real-world example next.\n\n8.5. VIRTUALIZING A .NET REMOTING SERVICE\n\nBack in the days of yore, a town crier often made public announcements. A crier would ring a handbell and shout\n\n“Hear ye! Hear ye!” to get everyone’s attention before making the announcement. This neatly solved the\n\nproblem of delivering a message to a public that was still largely illiterate.\n\n[2]\n\nProviding a town crier RPC service may come off as a\n\nlittle antiquated, but it helps get your mindset into those halcyon days of yesteryear when we thought making\n\nremote function calls look like in-process function calls was a good thing.\n\n[3]\n\n.NET Remoting wasn’t the first\n\nattempt at creating a largely transparently distributed RPC mechanism, but it did have a brief period of\n\npopularity and is representative of the broader class of RPC protocols you are likely to run across in the\n\nenterprise.\n\n2\n\nOK, the scenario may not be quite as real-world as advertised, but the protocol is....\n\n3\n\nPeter Deutsch wrote that nearly everyone who first builds a distributed application makes a key set of assumptions, all of which are false in the long run and inevitably cause big trouble. See https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing.\n\n8.5.1. Creating a simple .NET Remoting client\n\n.NET Remoting allows you to call a remote method like\n\nyou would a local method using the .NET framework. For",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "example, let’s assume that to make a pronouncement, you have to fill in an AnnouncementTemplate:\n\n[Serializable]\n\n1\n\npublic class AnnouncementTemplate\n\n{ public AnnouncementTemplate(string greeting,\n\nstring topic)\n\n{\n\nGreeting = greeting;\n\n2\n\nTopic = topic;\n\n3\n\n}\n\npublic string Greeting { get; }\n\n4\n\npublic string Topic { get; }\n\n4\n\n}\n\n1 Ensures that it can be passed over the wire\n\n2 So you can change “Hear ye!” to “Oyez!”\n\n3 The topic to be announced\n\n4 Public getters\n\nDon’t worry if you’re not a C# expert. This code is as\n\nsimple as it gets in most enterprise applications, which often are written in Java or C#. It creates a basic class\n\nthat accepts the greeting and topic for the pronouncement and exposes them as read-only\n\nproperties. The only interesting nuance is the [Serializable] attribute at the top. That’s a bit of C#\n\nmagic that allows the object to be passed between processes.\n\nOnce you create an AnnouncementTemplate, you’ll\n\npass it to the Announce method of the Crier class, as defined in the following listing.\n\n[4]\n\n4",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "The source code for this example is considerably more complicated than most other examples in this book. As always, you can download it at https://github.com/bbyars/mountebank-in-action. In addition, many of the command line examples in the book have shown a bit of a bias toward macOS and Linux. This example is geared toward Windows and will require\n\nadditional effort to run on other operating systems.\n\nListing 8.7. The Crier class definition\n\npublic class Crier : MarshalByRefObject 1\n\n{\n\npublic AnnouncementLog Announce(\n\nAnnouncementTemplate template)\n\n{\n\nreturn new AnnouncementLog(\n\n2\n\n$\"{template.Greeting}! {template.Topic}\");\n\n}\n\n}\n\n1 Allows serialization over the wire\n\n2 Returns a log capturing the announcement\n\nThe Crier class inherits from MarshalByRefObject.\n\nAgain, that is largely irrelevant to the example, except for the fact that it’s the bit of magic that allows an instance of the Crier class to be called from a remote process. The Announce method formats the greeting and topic\n\ninto a single string (that strange $\" {template.Greeting}! {template.Topic}\" line\n\nis C#’s string interpolation) and returns it wrapped inside an AnnouncementLog object, which looks like\n\nthis:\n\n[Serializable]\n\n1\n\npublic class AnnouncementLog\n\n{\n\npublic AnnouncementLog(string announcement)\n\n{ When = DateTime.Now;\n\n2\n\nAnnouncement = announcement;\n\n}\n\npublic DateTime When { get; }",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "public string Announcement { get; }\n\npublic override string ToString()\n\n{\n\nreturn $\"({When}): {Announcement}\"; 3\n\n}\n\n}\n\n1 Makes it remotely accessible\n\n2 Captures the time of the announcement\n\n3 Formats a log entry for the announcement\n\nFor our purposes, the AnnouncementTemplate,\n\nCrier, and AnnouncementLog form the entirety of the domain model. You could’ve simplified it to the Crier\n\nclass, but we used to think that passing entire object graphs over the wire was a good idea, and adding a couple of simple classes that the Crier uses—one as input, one as output—helps make the example ever so\n\nslightly more realistic.\n\nYou can call Crier locally, but that’s kind of boring, and\n\nmore in the realm of traditional mocking tools should you decide to test it. Instead, you will call a remote Crier instance. The source repo for this book shows how to code a simple server that listens on a TCP socket and acts as a remote Crier. We’ll focus on the client, testing it by virtualizing the server. To do that, your virtual service needs to respond like a .NET TCP\n\nRemoting service would.\n\nThe following listing shows the client you’ll test. It represents a gateway to a remote Crier instance.\n\nListing 8.8. A gateway to a remote Crier\n\npublic class TownCrierGateway\n\n{ private readonly string url;\n\npublic TownCrierGateway(int port)",
      "content_length": 1333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "{\n\nurl = $\"tcp://localhost:\n\n{port}/TownCrierService\"; 1\n\n}\n\npublic string AnnounceToServer(\n\nstring greeting, string topic)\n\n{\n\nvar template = new AnnouncementTemplate(\n\ngreeting, topic);\n\nvar crier = (Crier)Activator.GetObject(\n\n2\n\ntypeof(Crier), url); var response = crier.Announce(template);\n\n3\n\nreturn $\"Call Success!\\n{response}\";\n\n4\n\n}\n\n}\n\n1 URL to the remote service\n\n2 Gets a remote object reference\n\n3 Makes the (remote) method call\n\n4 Adds metadata to the response\n\nNotice that the call to crier.Announce looks like a\n\nlocal method call. It’s not. This is the magic of .NET Remoting. The line above retrieves a remote reference to\n\nthe object based on the URL in the constructor. That desire to make remote function calls look like local\n\nfunction calls is highly representative of this era of distributed computing.\n\n8.5.2. Virtualizing the .NET Remoting server\n\nAll the TownCrierGateway class does is add a success\n\nmessage to the response of the remote call. That’s enough for you to write a test without getting lost in too\n\nmuch unnecessary complexity. You could write the test two ways, assuming you aim to virtualize the remote\n\nservice.",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "The first way is to create a mountebank stub that proxies the remote service and captures the response. You could\n\nreplay the response in your test.\n\nThe second way is much cooler. You could create the response (as in an instance of the AnnouncementLog class) in the test itself, as you would with traditional\n\nmocking tools, and have mountebank return it when the client calls the Announce method. Much cooler.\n\nFortunately, Matthew Herman has written an easy to use\n\n[5]\n\nmountebank library for C# called MbDotNet. it to create a test fixture. I like to write tests using the\n\nLet’s use\n\nLaw of Wishful Thinking, by which I mean I write the code I want to see and figure out how to implement it\n\nlater. This allows my test code to clearly specify the intent without getting lost in the details. In this case, I\n\nwant to create the object graph that mountebank will return inside the test itself and pass it off to a function\n\nthat creates the imposter on port 3000 using a contains predicate for the remote method name. That’s\n\na lot to hope for, but I’ve wrapped it up in a function called CreateImposter, as shown in the following\n\nlisting.\n\n5\n\nAn entire ecosystem of these client bindings exists for mountebank. I do my best to maintain a list at http://www.mbtest.org/docs/clientLibraries, but you can always search GitHub for others. Feel free to add a pull request to add your own library to the mountebank website.\n\nListing 8.9. Basic test fixture using MbDotNet\n\n[TestFixture]\n\npublic class TownCrierGatewayTest\n\n{\n\nprivate readonly MountebankClient mb =\n\n1 new MountebankClient();\n\n1\n\n[TearDown]\n\n2",
      "content_length": 1609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "public void TearDown()\n\n2\n\n{\n\n2\n\nmb.DeleteAllImposters(); 2\n\n}\n\n2\n\n[Test]\n\npublic void ClientShouldAddSuccessMessage()\n\n{\n\nvar stubResult = new\n\nAnnouncementLog(\"TEST\"); 3 CreateImposter(3000, \"Announce\",\n\nstubResult); 3\n\nvar gateway = new TownCrierGateway(3000);\n\n3\n\nvar result = gateway.AnnounceToServer(\n\n4\n\n\"ignore\", \"ignore\"); 4\n\nAssert.That(result, Is.EqualTo(\n\n5\n\n$\"Call Success!\\n{stubResult}\"));\n\n5\n\n}\n\n}\n\n1 MbDotNet’s gateway to the mountebank REST API\n\n2 Deletes all imposters after every test\n\n3 Arrange\n\n4 Act\n\n5 Assert\n\n[6]\n\nThis fixture uses NUnit annotations NUnit ensures that the TearDown method will be called after every test, which allows you to elegantly clean up\n\nto define a test.\n\nafter yourself. When you create your test fixture, you create an instance of the mountebank client (which assumes mb is already running on port 2525) and remove all imposters after every test. This is the typical pattern\n\nwhen you use mountebank’s API for functional testing.\n\n6",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "6\n\nA popular C# testing framework; see http://nunit.org/.\n\nThe test itself uses the standard Arrange-Act-Assert\n\npattern of writing tests introduced back in chapter 1. Conceptually, the Arrange stage sets up the system under test, creating the TownCrierGateway and ensuring that when it connects to a virtual service (on\n\nport 3000), the virtual service responds with the wire format for the object graph represented by stubResult.\n\nThe Act stage calls the system under test, and the Assert stage verifies the results. This is nearly identical to what you would do with traditional mocking tools.\n\nWishful thinking only gets you so far. MbDotNet simplifies the process of wiring up your imposter using\n\nC#. You’ll delay only the serialization format for the response under a wishfully-thought-of method I have named Serialize:\n\nprivate void CreateImposter(int port,\n\nstring methodName, AnnouncementLog result)\n\n{\n\nvar imposter = mb.CreateTcpImposter(\n\nport, \"TownCrierService\", TcpMode.Binary);\n\nimposter.AddStub()\n\n.On(ContainsMethodName(methodName))\n\n1\n\n.ReturnsData(Serialize(result));\n\n2\n\nmb.Submit(imposter);\n\n3 }\n\nprivate ContainsPredicate<TcpPredicateFields>\n\nContainsMethodName(\n\nstring methodName)\n\n{\n\nvar predicateFields = new TcpPredicateFields\n\n{ Data = ToBase64(methodName)\n\n};",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "return new\n\nContainsPredicate<TcpPredicateFields>(\n\npredicateFields);\n\n}\n\nprivate string ToBase64(string plaintext)\n\n{\n\nreturn Convert.ToBase64String(\n\nEncoding.UTF8.GetBytes(plaintext));\n\n}\n\n1 Adds predicate\n\n2 Adds response\n\n3 Calls the REST API (We’ll get to the Serialize method\n\nsoon.)\n\nThe CreateImposter and ContainsMethodName\n\nmethods uses the MbDotNet API, which is a simple wrapper around the mountebank REST API. The REST call is made when you call mb.Submit. The ToBase64 method uses the standard .NET library calls to encode a\n\nstring in Base64 format.\n\nAll that’s left is to fill in the Serialize method. This is the method that has to accept the object graph you want\n\nyour virtual service to return and transform it into the stream of bytes that looks like a .NET Remoting\n\nresponse. That means understanding the wire format of .NET Remoting.\n\nThat’s hard.\n\nThe good news is that, with many popular RPC protocols, someone else has usually done the hard work for you.\n\nFor .NET Remoting, that someone else is Xu Huang, who has created .NET Remoting parsers for .NET, Java, and\n\n[7]\n\nJavaScript. create the Serialize function.\n\nYou’ll use the .NET implementation to\n\n7\n\nSee https://github.com/wsky/RemotingProtocolParser.",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "The code appears in listing 8.10. Don’t try too hard to\n\nunderstand it all. The point isn’t to teach you the wire format for .NET Remoting. Instead, it’s to show that,\n\nwith a little bit of work, you can usually create a generalized mechanism for serializing a stub response\n\ninto the wire format for real-world RPC protocols. Once you have done the hard work, you can reuse it\n\nthroughout your test suite to make writing tests as easy as creating the object graph you want the virtual service\n\nto respond with and letting your serialization function do the work of converting it to an RPC-specific format.\n\nListing 8.10. Serializing a stub response for .NET Remoting\n\npublic string Serialize(Object obj)\n\n{\n\nvar messageRequest = new MethodCall(new[] {\n\n1\n\nnew Header(MessageHeader.Uri, \"tcp://localhost:3000/TownCrier\"),\n\nnew Header(MessageHeader.MethodName,\n\n\"Announce\"),\n\nnew Header(MessageHeader.MethodSignature,\n\nSignatureFor(\"Announce\")),\n\nnew Header(MessageHeader.TypeName,\n\ntypeof(Crier).AssemblyQualifiedName),\n\nnew Header(MessageHeader.Args, ArgsFor(\"Announce\"))\n\n});\n\nvar responseMessage = new\n\nMethodResponse(new[] c\n\n{\n\nnew Header(MessageHeader.Return, obj)\n\n}, messageRequest);\n\nvar responseStream =\n\nBinaryFormatterHelper.SerializeObject(\n\nresponseMessage);\n\nusing (var stream = new MemoryStream())\n\n{\n\nvar handle = new\n\nTcpProtocolHandle(stream);\n\nhandle.WritePreamble(); 3\n\nhandle.WriteMajorVersion();\n\n3\n\nhandle.WriteMinorVersion();",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "3\n\nhandle.WriteOperation(TcpOperations.Reply); 3\n\nhandle.WriteContentDelimiter(\n\n3 TcpContentDelimiter.ContentLength);\n\n3\n\nhandle.WriteContentLength(\n\n3\n\nresponseStream.Length);\n\n3\n\nhandle.WriteTransportHeaders(null);\n\n3 handle.WriteContent(responseStream);\n\n4\n\nreturn Convert.ToBase64String(\n\n5\n\nstream.ToArray());\n\n}\n\n}\n\nprivate Type[] SignatureFor(string methodName)\n\n6\n\n{\n\nreturn typeof(Crier)\n\n.GetMethod(methodName)\n\n.GetParameters()\n\n.Select(p => p.ParameterType)\n\n.ToArray(); }\n\nprivate Object[] ArgsFor(string methodName)\n\n6\n\n{\n\nvar length = SignatureFor(methodName).Length;\n\nreturn Enumerable.Repeat(new Object(),\n\nlength).ToArray(); }\n\n1 Request metadata\n\n2 Wraps response\n\n3 Writes response metadata\n\n4 Writes response (with request metadata)\n\n5 Converts to Base64\n\n6 Supports RPC methods other than Announce",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "The SignatureFor and ArgsFor methods are simple helper methods that use .NET reflection (which lets you inspect types at runtime) to make the Serialize method general purpose. The request metadata expects\n\nsome information about the remote function signature, and those two methods allow you to dynamically define\n\nenough information to satisfy the format. The rest of the Serialize method uses Xu Huang’s library to wrap\n\nyour stub response object with the appropriate metadata, so when mountebank returns it over the wire, your .NET\n\nRemoting client will see it as a legitimate RPC response.\n\nRemember the key goal of mountebank: to make easy\n\nthings easy and hard things possible. The fact that, with a little bit of underlying serialization code, you can\n\nelegantly stub out binary .NET Remoting (and some of its cousins) over the wire is a killer feature.\n\nIn case you have forgotten how cool that is, I suggest you\n\nlook back at listing 8.9 and see how simple the test is.\n\n8.5.3. How to tell mountebank where the message ends\n\nThere’s one other bit of complexity you have to deal with to fully virtualize an application protocol using\n\nmountebank’s TCP protocol. We hinted at it back in chapter 3, when we looked at how an HTTP server knows\n\nwhen an HTTP request is complete. You may recall a figure that looked like figure 8.5.\n\nFigure 8.5. Using Content-Length to wrap multiple packets into one HTTP request",
      "content_length": 1416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "As a transport protocol, TCP opens and closes a new\n\nconnection using a handshake. That handshake is transparent to application protocols. TCP then takes the\n\napplication request and chunks it into a series of packets, sending each packet over the wire. A packet will range\n\nbetween 1,500 and around 64,000 bytes, though smaller sizes are possible. You’ll get the larger packet size when\n\nyou test on your local machine (using what’s called the loopback network interface), whereas lower level\n\nprotocols like Ethernet use smaller packet sizes when passing data over the network.\n\nBecause a logical application request may span multiple\n\npackets, the application protocol needs to know when the logical request ends. HTTP often uses the Content-\n\nLength header to provide that information. Because this header occurs early in the HTTP request, the server can\n\nwait until it receives enough bytes to satisfy the given length, regardless of how many packets it takes to deliver\n\nthe full request.",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Every application protocol must have a strategy for determining when the logical request ends. Mountebank\n\nuses two strategies:\n\nThe default strategy, which assumes a one-to-one relationship\n\nbetween a packet and a request\n\nReceiving enough information to know when the request ends\n\nThe examples have worked so far because you’ve only\n\ntested with short requests. You will change that with a simple proxy, saved as remoteCrierProxy.json, as\n\nshown in the following listing.\n\nListing 8.11. Creating a TCP proxy to a .NET Remoting server\n\n{\n\n\"protocol\": \"tcp\",\n\n\"port\": 3000, \"mode\": \"binary\",\n\n\"stubs\": [{\n\n\"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\" }\n\n}]\n\n}]\n\n}\n\nThe source code for this book includes the executable for the .NET Remoting server. You give it the port to listen\n\nto when you start it up:\n\nServer.exe 3333\n\nYou start the mountebank server in the usual way:\n\nmb --configfile remoteCrierProxy.json\n\nFinally, if you start the .NET Remoting client on port 3000, it’s configured by default to send a request\n\ngreeting and topic that’ll exceed the size of a single packet:",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Client.exe 3000\n\nYou can see in the mountebank logs that it tried to proxy,\n\nbut the server didn’t respond, and the client threw an error. By default, mountebank grabs the first packet and\n\nassumes it’s the entire request. It passes it to the server— a real, bona fide .NET Remoting server—which looks\n\ninside the packet and sees that it should expect more packets to come for the request, so it continues to wait.\n\nMountebank, thinking it has seen the entire request, tries to respond. The whole process blows up (figure 8.6).\n\nFigure 8.6. Mismatched expectations around when the request ends\n\nOnce your request reaches a certain size, you have to opt\n\nfor the second strategy: telling mountebank when the request ends. The imposter will keep an internal buffer.\n\nEvery time it receives a new packet, it adds the packet data to the buffer and passes the entire buffer to a JavaScript function that you define, which returns true if the request is complete or false otherwise.\n\nYou pass in the function as the endOfRequestResolver. For this example, you’ll add",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "it using a template to include the function in a separate file called resolver.js, as shown in the following listing.\n\nListing 8.12. Adding an endOfRequestResolver\n\n{ \"protocol\": \"tcp\",\n\n\"port\": 3000,\n\n\"mode\": \"binary\",\n\n\"endOfRequestResolver\": {\n\n\"inject\": \"<%- stringify(filename,\n\n'resolver.js') %>\"\n\n},\n\n\"stubs\": [{ \"responses\": [{\n\n\"proxy\": { \"to\": \"tcp://localhost:3333\" }\n\n}]\n\n}]\n\n}\n\n.NET Remoting embeds a content length in the metadata for the request. You can use that in your function to\n\ndetermine if you’ve collected all the packets for the request or not. You’ll once again rely on Xu Huang’s\n\nparsing library, which includes a Node.js implementation, to do the heavy lifting. As before, the\n\nintention isn’t to learn everything about .NET Remoting; it’s to show how you would virtualize a real-world\n\napplication protocol. Don’t worry too much about the details of the message format. The essential part is that\n\nyou grab the content length from the message and test it against the length of the buffer mountebank passes you\n\nto see whether you’ve received the entire request, as shown in the following listing.\n\nListing 8.13. The function to determine if you have seen the entire request yet\n\nfunction (requestData, logger) {\n\n1\n\nvar path = require('path'),\n\nparserPath = path.join(process.cwd(),\n\n2\n\n'/../RemotingProtocolParser/nodejs/lib/remotingProtocolParser'),\n\nr =",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "require(parserPath).tcpReader(requestData);\n\nlogger.debug('Preamble: %s', r.readPreamble());\n\nlogger.debug('Major: %s', r.readMajorVersion());\n\nlogger.debug('Minor: %s',\n\nr.readMinorVersion());\n\nlogger.debug('Operation: %s',\n\nr.readOperation());\n\nlogger.debug('Delimiter: %s',\n\nr.readContentDelimiter());\n\nlogger.debug('ContentLength: %s', 3\n\nr.readContentLength());\n\n3\n\nlogger.debug('Headers: %s',\n\nJSON.stringify(r.readHeaders()));\n\nvar expectedLength = r.offset + r.contentLength\n\n+ 1; 4 logger.info('Expected length: %s, actual\n\nlength: %s',\n\nexpectedLength, requestData.length);\n\nreturn requestData.length >= expectedLength;\n\n5\n\n}\n\n1 requestData is a Node.js Buffer object.\n\n2 Includes Xu Huang’s library\n\n3 Refers to the length of the content section, not the\n\nentire message\n\n4 Calculates the expected length\n\n5 Tests buffer length against expected\n\nThe parsing library isn’t published as an npm module. If\n\nit was, you could install it locally and include it without referencing a specific file path. In your case, you cloned\n\nHuang’s repository according to the path expected in the [8] second line of the function.\n\n8\n\nI’ve done this in the source code repo for this book, including a complete copy of the library.",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "The parsing library doesn’t support random access, so you can’t ask it the content length and compare that\n\nagainst your request buffer. Instead, it maintains a stateful offset and expects that you read all the\n\nmetadata fields in order. To help you debug, I wrapped those metadata fields in a logger.debug function.\n\nYou’ll be able to see them in the mountebank logs if you run with the --loglevel debug command-line flag.\n\nNow that you’ve written your function, you can try the proxy again. This time, because you’re using a JavaScript function, you have to pass the --allowInjection flag:\n\nmb --configfile imposter.json --allowInjection\n\nRestart the server on port 3333 and run the client again,\n\npointing to the mountebank proxy:\n\nClient.exe 3000\n\nThis time, everything works. You now have a fully functional virtual server for .NET Remoting.\n\nCongratulations! You’ve completed the hardest example in the entire book.\n\nHard, but possible.\n\nAnd with that, you’ve now completed your tour of mountebank. But knowing how to use a tool isn’t the\n\nsame as knowing when you should use it. That’s what the next chapter is about.\n\nAnother example: Java serialization over a Mule ESB\n\nIn late 2013, I was working at a major airline company. Several years earlier, the website had been rewritten to\n\ncommunicate with a service tier over a Mule enterprise",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "service bus (ESB). The ESB connector communicated over TCP and returned a serialized Java object graph.\n\nUnfortunately, passing raw objects over the wire created a tight coupling between the web tier and the service tier.\n\nIt also ran a multibillion-dollar website for most of a decade. Production-hardened enterprise software rarely\n\nlooks like the beautiful architectures you read about.\n\nI was on a team creating REST APIs for a new mobile\n\napp that needed to go out before the website could be replaced, so our APIs had to integrate with the service\n\ntier. Although we had a top-notch team comfortable with automated testing, the friction of testing without also\n\nbreaking the web tier was so painful that we gave up. We wrote automated tests when we could, but usually it was\n\ntoo hard, and bugs started creeping in.\n\nMost of this book has described mountebank for HTTP, but HTTP wasn’t the first protocol mountebank\n\nsupported. I created mountebank to test a binary Mule ESB TCP connector, serving up Java objects in our tests\n\nin much the same way we looked at for .NET Remoting. At the time, a number of quality open source\n\nvirtualization tools were available for HTTP, but none could stub out a binary TCP protocol. That’s largely still\n\ntrue today.\n\nSUMMARY\n\nIn mountebank, protocols are responsible for transforming a\n\nnetwork request into a JSON request for predicate matching, as\n\nwell as taking a mountebank JSON response and transforming it\n\ninto a network response.\n\nMountebank supports adding an application protocol on top of its\n\nTCP protocol. All predicates, response types, and behaviors\n\ncontinue to work with the TCP protocol; only the JSON structure\n\nfor requests and responses differs.\n\nMountebank supports binary payloads by Base64 encoding the data. You have to switch the imposter mode to binary for",
      "content_length": 1828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "mountebank to correctly handle the encoding.\n\nOnce you figure out how to serialize an object graph into the wire\n\nformat expected by an RPC protocol, you can write tests that look\n\nsimilar to ones that use traditional stubbing tools.\n\nBy default, mountebank assumes each incoming packet represents\n\nan entire request when using the TCP protocol. To let mountebank\n\nknow when the request ends, you can pass in an endOfRequestResolver JavaScript function.",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Part 3. Closing the Loop\n\nNow that you have the full breadth of mountebank\n\nfunctionality under your belt, part 3 puts its usage in context.\n\nService virtualization is a powerful tool, but, like any tool, it has its limits. In chapter 9, we explore it in the\n\ncontext of continuous delivery. We’ll build a test pipeline from start to finish for some of the microservices we’ve looked at previously in this book and discuss where\n\nservice virtualization fits and where it doesn’t. We’ll also look at how to gain additional confidence in your test\n\nsuite with contract tests that give you lightweight validation that play together well with your services\n\nwithout going down the path toward full end-to-end testing.\n\nWe close out the book by looking at performance testing, always a difficult subject and one made even more so by\n\nmicroservices. The need to understand your service’s performance characteristics in a networked environment\n\nis challenged by the cost and complexity of securing an end-to-end environment for testing. Service\n\nvirtualization is a natural fit for performance testing and combines many of the features we’ve looked at previously, including proxies and behaviors.",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Chapter 9. Mountebank and continuous delivery\n\nThis chapter covers\n\nA basic refresher on continuous delivery\n\nTesting strategy for continuous delivery and microservices\n\nWhere service virtualization applies inside a broader testing\n\nstrategy\n\nA sysadmin, a DBA, and a developer walk into a bar. The\n\nsysadmin orders a light lager to maximize uptime, the DBA orders a 30-year-aged single malt to avoid undue\n\nadulteration, and the developer orders a Pan Galactic Gargle Blaster because it hasn’t been invented yet. An\n\nhour later, the DBA has gone home already, the developer has moved on to a more modern bar, and the\n\nslightly wobbly and heavily overutilized sysadmin is holding down the fort, while also holding a lager in one\n\nhand, a single malt in another hand, and a Pan Galactic in another hand.\n\n[1]\n\n1\n\nIn The Hitchhikers Guide to the Galaxy, Douglas Adams describes the Pan Galactic Gargle Blaster as the alcoholic equivalent of a mugging—expensive and bad for the head.\n\nTraditional siloed organizational structure forces a complicated dance to get anything done. It’s no surprise\n\nthat, in large enterprises, IT and the business rarely have a healthy relationship. Historically, the common\n\napproach to improving the situation was to add more process discipline, which further complicated the dance,\n\nmaking it harder to release code into production (and, by consequence, reduced value to customers). Having\n\nincreasingly well-defined handoffs between a developer, a DBA, and a sysadmin exemplifies process discipline.\n\nEvery time you fill out a database schema change request",
      "content_length": 1588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "form or an operational handoff document, you have seen process discipline in action.\n\nContinuous delivery changes the equation by emphasizing engineering discipline over process\n\ndiscipline. It’s about automating the steps required to build confidence so that the business can release new\n\ncode on demand. Although engineering discipline encompasses a wide spectrum of practices, testing plays\n\na central role. In this chapter, we look at a sample testing strategy for a microservices world and show where\n\nservice virtualization does and doesn’t fit.\n\n9.1. A CONTINUOUS DELIVERY REFRESHER\n\nJez Humble and Dave Farley wrote Continuous Delivery to capture the key practices they saw enabling the rapid\n\ndelivery of software. In chapter 1, I showed you how the traditional process discipline of centralized release\n\nmanagement and toll gates increases congestion and slows delivery. The emphasis is on safety, providing\n\nadditional checks to increase confidence that the software being delivered will work.\n\nIn contrast, continuous delivery (CD) focuses on\n\nautomation, emphasizing safety, speed, and sustainability of delivering software. It requires the code\n\nto be in a deployable state at all times, forcing you to abandon the ideas of dev complete, feature complete,\n\nand hardening iterations. Those concepts are holdovers from the world of yesteryear, in which we papered over a\n\nlack of engineering discipline by adding more layers of process.\n\nA glossary of terms surrounding continuous delivery\n\nI introduce several important terms in this chapter:",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Continuous integration— Although continuous integration\n\n(CI) is often confused with running an automated build after every\n\ncommit through a tool like Jenkins, it’s actually the practice of\n\nensuring that your code is merged with and works with everyone\n\nelse’s on a continual basis (at least once a day).\n\nContinuous delivery— The set of software development\n\npractices that ensures code is always releasable. The full spectrum of CD practices ranges from developer-facing techniques like\n\nfeature toggles, which provide a way of hiding code that’s still a\n\nwork in progress, to production-facing approaches like monitoring\n\nand canary testing, which scales up a release to a customer base\n\nover time. In between comes testing, the focus of this book.\n\nDeployment pipeline— The path code takes from the time it’s\n\ncommitted to the time it reaches production.\n\nContinuous deployment— An advanced type of continuous\n\ndelivery that removes all manual interventions from the\n\ndeployment pipeline.\n\nIn CD, every commit of the code either fails the build or\n\ncan be released to production. There’s no need to decide up front which commit represents the release version.\n\nAlthough still common, that approach encourages sloppy engineering practices. It enables you to commit code that cannot be released to production, with the expectation\n\nthat you’ll fix it later. That attitude requires IT to own the timing of software delivery, taking control out of the\n\nhands of the business and the product manager.\n\nThe core organizing concept that makes CD possible is\n\nthe deployment pipeline. It represents the value stream of the code’s journey from commit to production and is\n\noften directly represented in continuous integration (CI) tools (figure 9.1).\n\nFigure 9.1. A deployment pipeline defines the path from commit to production.",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Every code commit automatically triggers a build, which\n\nusually includes compilation, running unit tests, and static analysis. A successful build saves off a package in\n\nan artifact repository—usually a binary artifact, even if it’s just a tarball of source code for interpreted languages\n\nlike Ruby and JavaScript. Every set of verifications downstream runs against a deployed instance of that\n\npackage, until it ultimately reaches production.\n\nThe path that code takes on its way to providing value to real users varies from organization to organization and\n\neven between teams within the same organization. Much of it is defined by how you decide to test your\n\napplication.\n\n9.1.1. Testing strategy for CD with microservices\n\nTesting in a very large-scale distributed setting is a major challenge.",
      "content_length": 801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Werner Vogels, Amazon CTO\n\nA common approach to visualizing testing strategy comes in the form of a pyramid. The visual works\n\nbecause it acknowledges that confidence comes from testing at multiple layers. It also shows that there’s value\n\nin pushing as much of the testing as possible into the lower levels, where tests are both easier to maintain and\n\nfaster to run. As you move to higher levels, the tests become harder to write, to maintain, and to troubleshoot\n\nwhen they break. They’re also more comprehensive and often better at catching difficult bugs. Each team will\n\nneed to customize a test pyramid to its needs, but you can think of a template for microservices that looks like\n\n[2]\n\nfigure 9.2.\n\n2\n\nYou also may be interested in Toby Clemson’s description of the types of testing for microservices at http://martinfowler.com/articles/microservice-testing/.\n\nFigure 9.2. Simplified test pyramid for microservices",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "People have argued endlessly over what makes a unit test\n\ndifferent from higher level tests, but for the purposes of this diagram, the key difference is that you should be\n\nable to run a unit test without deploying your service into a runtime. That makes unit tests in-process and\n\nindependent of anything from the environment (see figure 9.3).\n\nFigure 9.3. The basic structure for unit and service tests",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Though there’s some different terminology out there, I’ve\n\nused the term service test to describe a black-box test that validates your service’s behavior over the wire. Such\n\ntests do require a deployment, but you use service virtualization to maintain isolation from your runtime\n\ndependencies. This layer allows you to do out-of-process, black-box testing while maintaining determinism.\n\nService virtualization enables you to remove nondeterminism from your tests by allowing each test to\n\ncontrol the environment it runs in, as shown in figure 9.3.\n\nYou should be able to test the bulk of the behavior of your service through a combination of unit tests and\n\nservice tests. They let you know that your service behaves correctly, assuming certain responses from its\n\ndependencies, but they don’t guarantee that those stubbed responses are appropriate. Contract tests give",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "you validation that breaking-contract-level changes haven’t occurred (see figure 9.4). Service tests say, in\n\neffect, that if the service gets these responses from its dependencies, then it behaves correctly. Contract tests\n\nvalidate that the service does, in fact, get those responses. Good contract tests avoid deep behavioral\n\ntesting of the dependencies—you should test them independently—but give you confidence in your stubs.\n\nFigure 9.4. The basic structure for contract and exploratory tests\n\nI’ve included exploratory testing as part of the test pyramid because most organizations find some value in\n\nmanual testing. Good exploratory testers follow their nose to find gaps in an automated test suite. Such tests\n\ncan be integrated or rely on service virtualization to test certain edge cases. Figure 9.4 shows exploratory testing using service virtualization.",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Other types of testing exist that don’t fit as well in the test pyramid metaphor. Cross-functional requirements\n\nlike security, performance, and failover for availability often require specialized testing and are less about the\n\nbehavior of the system than about its resiliency. Performance testing is an area where service\n\nvirtualization shines, as it allows you to replicate the performance of your dependencies without requiring a\n\nfully integrated, production-like environment to run in. In chapter 10, we explore how service virtualization\n\nenables performance and load testing.\n\nFinally, you should never forget that error prevention is\n\nonly one piece of testing strategy. The rapid release cycles of microservices encourage you to invest heavily in\n\nerror detection and remediation as well as prevention, as they contribute to your overall confidence in releasing software. Although error detection and remediation are\n\nnot the focus of this book, companies that have used microservices effectively generally stage their releases,\n\nsuch that only a small percentage of users can see the new release at first. Robust monitoring detects whether\n\nthe users experience any problem, and rolling back is as easy as switching those users to the code everyone else is\n\nusing. If no problems are detected, the release system will switch more and more users to the new code over\n\ntime until 100% of users are using the release, at which Advanced time you can remove the previous release.\n\n[3]\n\nmonitoring allows you to detect errors before your users do. Although your testing strategy is a key component of\n\ncontinuous delivery, engineering discipline significantly increases the scope of automation.\n\n3\n\nThis is called canary testing. You can read more about it at https://martinfowler.com/bliki/CanaryRelease.html.\n\n9.1.2. Mapping your testing strategy to a deployment pipeline",
      "content_length": 1879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Whatever your particular test pyramid looks like, mapping it to your deployment pipeline is generally\n\npretty straightforward (figure 9.5).\n\nFigure 9.5. Mapping your test pyramid to a deployment pipeline\n\nI like to think of boundary conditions moving from one\n\nstage to the next. In figure 9.5, I’ve shown the following boundaries:\n\nThe boundary of deployment represents the first time you’ve\n\ndeployed the application (or service). All tests to the left are run\n\nin-process; all tests to the right are run out-of-process and\n\nimplicitly test the deployment process itself, as well as the\n\napplication.\n\nThe boundary of determinism represents the first time you’ve\n\nintegrated your application into other applications. Tests to the\n\nright of this boundary may fail because of environmental",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "conditions. Tests to the left of this boundary should fail only for\n\nreasons entirely under the application team’s control.\n\nThe boundary of automation represents where you switch to\n\nmanual, exploratory testing. (Note that the deployment itself is\n\nstill automated, but the trigger to deploy requires a human\n\npressing a button.) Some companies, for some products, have\n\nmanaged to eliminate this boundary altogether, automatically releasing code to production without any manual verifications.\n\nThis is an advanced form of continuous delivery called continuous\n\ndeployment and is clearly not appropriate in all environments.\n\nThe software that helps keep an airplane in the air requires a much\n\nhigher degree of confidence than your favorite social media\n\nplatform.\n\nThe boundary of value is the point at which real users have access\n\nto the new software.\n\nBecause this book is about testing, we only tackle the test pipeline, those early stages of the pipeline that give you\n\nconfidence that you can ship to production. The full deployment pipeline would include the production\n\ndeployment as well.\n\n9.2. CREATING A TEST PIPELINE\n\nIt has been a while since we looked at the example pet\n\nstore website in chapter 2. We used it as a small-scale simulacrum of a microservices-backed e-commerce\n\napplication and focused on the web façade service, which aggregated results from two other services (figure 9.6):\n\nThe product catalog service, responsible for sending product\n\ninformation\n\nThe content service, responsible for sending marketing copy about\n\na product\n\nFigure 9.6. An example set of microservices",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "For demonstration purposes, we’ll take the perspective of\n\nthe team writing the web façade code that aggregates product and marketing data to present to the website.\n\nThe example is simple enough to digest in short order and complicated enough to require meaningful testing.\n\nThe code is a simple Express app (a popular node.js web application framework). The following listing shows\n\nwhat you need to initialize the service.\n\n[4]\n\n4\n\nSee the full source code at https://github.com/bbyars/mountebank-in-action.\n\nListing 9.1. Web façade initialization code\n\nvar express = require('express'),\n\nproductServiceURL =\n\nprocess.env['PRODUCT_SERVICE_URL'], 1 contentServiceURL =\n\nprocess.env['CONTENT_SERVICE_URL'], 1\n\nproductsGateway =\n\nrequire('./models/productsGateway') 2\n\n.create(productServiceURL),\n\n2\n\ncontentGateway =\n\nrequire('./models/contentGateway') 2 .create(contentServiceURL),\n\n2\n\nproductCatalog =\n\nrequire('./models/productCatalog') 3\n\n.create(productsGateway, contentGateway);",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "3\n\nvar app = express();\n\n4\n\n1 Configures external services\n\n2 Gateway code to external services\n\n3 Module to do the aggregation\n\n4 Creates the Express app\n\nUsing environment variables for configuration is a\n\ncommon approach and will allow you to use different URLs in different environments (and to use service\n\nvirtualization for service tests). The two gateway objects are simple wrappers over the HTTP calls, allowing you to\n\ncentralize error handling and logging around external service calls.\n\nThe code that responds to an HTTP request and returns\n\nthe aggregated results of the product and content services is quite straightforward, delegating the complex logic to the productCatalog object, as shown in the following listing.\n\nListing 9.2. Web façade code to aggregate product and content data\n\napp.get('/products', function (request, response)\n\n{ 1\n\nproductCatalog.retrieve().then(function\n\n(results) { 2\n\nresponse.json({ products: results });\n\n3\n\n}, function (err) { 4\n\nresponse.statusCode = 500;\n\n4\n\nresponse.send(err);\n\n4\n\n});\n\n});\n\n1 Responds to GET /products\n\n2 Delegates aggregation",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "3 Returns results as JSON\n\n4 Error handling\n\nYou could’ve left the aggregation logic directly in the\n\nfunction that handles the HTTP request. You didn’t, in part, because that would’ve made it harder to unit test.\n\n9.2.1. Creating unit tests\n\nTest-driven development (TDD) is often also called test-\n\ndriven design because the act of writing unit tests helps enforce loose coupling and high cohesion in your\n\n[5]\n\nproduction code. Bundling all the aggregation logic into your HTTP handling code would have required a\n\nsignificant amount of setup to test it—exactly the type of friction that reduces your motivation to write tests to\n\nbegin with. It also would’ve been a worse design, coupling HTTP handling logic with aggregation logic.\n\nThe drive to make unit testing your code as easy as possible is one of the best influences for keeping your\n\ncodebase modular. Unit tests are as much about helping you design your application as they are about finding\n\nbugs.\n\n5\n\nTDD is more than writing unit tests. It’s a practice that involves writing a small test before the code exists and building just enough code to make the test pass. Those small iterations with\n\nrefactoring in between help grow the design of the codebase organically.\n\nUnit tests should be in-process to the application under\n\ntest, and each unit test should focus on a small piece of code. Consequently, you should never use service\n\nvirtualization in your unit tests. This is the realm of traditional mocks and stubs.\n\nLet’s look at the productCatalog code. We’ll start with the wrapper logic needed to create an instance, as shown\n\nin the following listing, and export it to another JavaScript file.\n\nListing 9.3. The shell of the productCatalog module",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "function create(productsGateway, contentGateway)\n\n{ 1\n\nfunction retrieve () { ... }\n\n2\n\nreturn {\n\n3 retrieve: retrieve\n\n3\n\n};\n\n3\n\n}\n\nmodule.exports = {\n\n4 create: create\n\n4\n\n};\n\n4\n\n1 Creates an instance using dependency injection\n\n2 See listing 9.4.\n\n3 Returns the instance with one function\n\n4 Exports the creation method to other files\n\nMuch of this is JavaScript and node.js plumbing. The creation function accepts the two gateway objects as\n\n[6]\n\nparameters. This pattern—dependency injection—is another area where good unit testing practices intersect with good design. If you created the gateways at the exact\n\nspot you needed them, you wouldn’t be able to swap out the gateway instances with another object for testing.\n\nThat would mean you would be forced to test the full end-to-end flow each time because the gateways are\n\nresponsible for making the HTTP calls to the external services. It also would’ve created a tight coupling,\n\npreventing higher order code from adding decorators around the gateways for added functionality.\n\n6\n\nYou could’ve also used JavaScript constructors, but they add a bit more magic than a simple creation function.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "The retrieve function uses those gateways to retrieve and aggregate the data from the product and content\n\nservices, as shown in the following listing.\n\nListing 9.4. The code that retrieves and aggregates downstream services\n\nfunction retrieve () {\n\nvar products;\n\nreturn productsGateway.getProducts()\n\n1\n\n.then(function (response) {\n\nproducts = response.products;\n\nvar productIds = products.map(function\n\n(product) {\n\nreturn product.id;\n\n2\n\n}); 2\n\nreturn contentGateway.getContent(productIds);\n\n3\n\n}).then(function (response) {\n\nvar contentEntries = response.content;\n\nproducts.forEach(function (product) {\n\nvar contentEntry = contentEntries.find( 4\n\nfunction (entry) {\n\n4\n\nreturn entry.id === product.id;\n\n4\n\n});\n\n4\n\nproduct.copy = contentEntry.copy; 5\n\nproduct.image = contentEntry.image;\n\n5\n\n});\n\nreturn products;\n\n});\n\n}\n\n1 Gets products from the product catalog service\n\n2 Maps to the IDs only\n\n3 Gets content for those products\n\n4 Matches the content entry to the product by ID",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "5 Adds marketing content data\n\nClearly, the bulk of the web façade complexity lies in this\n\nfunction, making it a great place to focus your unit testing efforts. To keep the unit test in-process, you will\n\n[7]\n\nhave to stub out the two gateways. common JavaScript mocking library called Sinon to help. [8]\n\nYou’ll use a\n\nSinon allows you to tell the gateways what to return, which supports a very readable test setup (the Arrange\n\nstep of the standard test pattern of Arrange-Act-Assert), as shown in the following listing.\n\n7\n\nI haven’t shown the gateway code because it’s not relevant to the example. See the GitHub\n\nrepo for details.\n\n8\n\nSee http://sinonjs.org/, although it’s not hard to write your own stubs if you’d rather avoid using an external library.\n\nListing 9.5. The test setup, using dependency injection and stubs\n\nit('should merge results', function (done) { var productsResult = {\n\n1\n\nproducts: [\n\n1\n\n{ id: 1, name: 'PRODUCT-1' },\n\n1\n\n{ id: 2, name: 'PRODUCT-2' }\n\n1 ]\n\n1\n\n},\n\n1\n\nproductsGateway = {\n\n2\n\ngetProducts: sinon\n\n2 .stub()\n\n2\n\n.returns(Q(productsResult)) 2\n\n},\n\n2\n\ncontentResults = {",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "3\n\ncontent: [\n\n3\n\n{ id: 1, copy: 'COPY-1', image: 'IMAGE-1'\n\n}, 3 { id: 2, copy: 'COPY-2', image: 'IMAGE-2'\n\n} 3\n\n]\n\n3\n\n},\n\n3\n\ncontentGateway = { 4\n\ngetContent: sinon\n\n4\n\n.stub()\n\n4\n\n.withArgs([1, 2])\n\n4\n\n.returns(Q(contentResults)) 4\n\n},\n\n4\n\ncatalog = productCatalog.create(\n\n5\n\nproductsGateway, contentGateway);\n\n5\n\n// ACT\n\n6\n\n// ASSERT\n\n6\n\n});\n\n1 Stages the product catalog service results\n\n2 Sets up the product stub\n\n3 Stages the content service results\n\n4 Sets up the content stub\n\n5 Passes the stubs into the catalog\n\n6 See listing 9.6.\n\nMost of the code is setting up the JSON responses that the gateways are responsible for returning. As you have\n\nseen in previous examples, I recommend using test data that’s easy to spot to make the assertions easy to read,",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "which is why I have opted for strings like “COPY-1.” You stub out the two gateway functions—getProducts on\n\nthe productsGateway and getContent on the contentGateway—using Sinon’s stub() function and\n\nchain on the result you want with the returns function. Notice that when you create the stub for the contentGateway, you add a withArgs ([1, 2]) function call. This is like using predicates in\n\nmountebank. You’re telling Sinon to return the given result if the arguments match what you specify.\n\nThe only other nuance to the test code is the mysterious use of the Q function, which is used in the stub\n\nresponses. Q is a promise library that helps contain the complexity of using asynchronous code in JavaScript.\n\nThe real gateways have to reach across the network to retrieve results, and because JavaScript uses nonblocking I/O for network calls, using promises helps\n\nmake the asynchronous code easier to understand. If you look back to the retrieve function in listing 9.4, you’ll\n\nsee that you call a then function after each gateway call and pass in the code to execute when the I/O is finished. Wrapping the objects inside the Q function adds the then function to your stub results, so the production\n\ncode works on the stubs as expected.\n\nLet’s close off the example by looking at the Act and\n\nAssert stages of the test in the following listing.\n\nListing 9.6. The unit test assertion\n\nit('should merge results', function (done) { // ARRANGE 1\n\ncatalog.retrieve().done(function (result) { 2\n\nassert.deepEqual(result, [ 3\n\n{ id: 1, name: 'PRODUCT-1', 3\n\ncopy: 'COPY-1', image: 'IMAGE-1' }, 3\n\n{ id: 2, name: 'PRODUCT-2', 3\n\ncopy: 'COPY-2', image: 'IMAGE-2' }, 3 ]); 3",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "done(); 4\n\n});\n\n});\n\n1 See listing 9.5.\n\n2 Act\n\n3 Assert\n\n4 Tells the test runner that the test has finished\n\nAnytime you’re using asynchronous code, you have to tell\n\nthe test runner that the test is complete. The assertion verifies that you merged the results of the two gateways\n\ncorrectly. The done test parameter is a function that you call after your assertion to signify the end of the test\n\nexecution.\n\nYou could, and should, write many more unit tests on the retrieve function. For example, you could write unit\n\ntests to specify what happens in each of these scenarios:\n\nThere’s no marketing copy for a product.\n\nA downstream service times out (resulting in a gateway error).\n\nYou get missing JSON fields from the marketing content service.\n\nThe marketing content service returns products in a different order than the product catalog service.\n\nIt’s much easier to write the code to support these scenarios in a suite of unit tests than it is to validate\n\nthem with higher level tests. Unit tests should be numerous and run quickly, which is why they form the\n\nbase of the testing pyramid.\n\nYou can create a build script that runs the unit tests and wire that into the first stage of your continuous\n\nintegration tool. Once you have done so, you have automated the first stage of your test pipeline.\n\n9.2.2. Creating service tests",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Service tests should exercise your application over the wire, which rules out in-process stubs. This is where\n\nservice virtualization shines, as service virtualization is the out-of-process equivalent of stubbing.\n\nAlthough you can always set up your imposters using a config file, I recommend using mountebank’s API when\n\npossible for service tests. The API allows you to create the test data for each test separately rather than having\n\nto depend on an implicit linkage between a magic key in your test setup and the scenario you’re testing. You used\n\nmountebank’s API in chapter 2.\n\nI’ve modified the example slightly to highlight the point about keeping the test data as simple as possible. Let’s\n\ntake a fresh look at the test first in the following listing; we look at the helper functions again shortly.\n\nListing 9.7. A service test that validates web façade aggregation\n\nit('aggregates data', function (done) {\n\ncreateProductImposter(['1', '2']).then(function\n\n() { 1\n\nreturn createContentImposter(['1', '2']);\n\n1\n\n}).then(function () {\n\nreturn request(webFacadeURL + '/products'); 2\n\n}).then(function (body) {\n\nvar products = JSON.parse(body).products;\n\nassert.deepEqual(products, [\n\n3\n\n{\n\n\"id\": \"ID-1\", \"name\": \"NAME-1\",\n\n\"description\": \"DESCRIPTION-1\",\n\n\"copy\": \"COPY-1\",\n\n\"image\": \"IMAGE-1\"\n\n},\n\n{\n\n\"id\": \"ID-2\",\n\n\"name\": \"NAME-2\", \"description\": \"DESCRIPTION-2\",\n\n\"copy\": \"COPY-2\",\n\n\"image\": \"IMAGE-2\"",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "}\n\n]);\n\nreturn imposter().destroyAll();\n\n4\n\n}).done(function () { done();\n\n5\n\n});\n\n});\n\n1 Arrange\n\n2 Act\n\n3 Assert\n\n4 Cleanup\n\n5 Tells the test runner you’re done\n\nThe createProductImposter and createContentImposter functions are similar. They\n\nuse mountebank’s API to create the virtual services. Both functions accept an array of suffixes, which they use to\n\nappend to the test data within each field name. You can see what that results in by looking at the assertion in\n\nlisting 9.7. The code to do that does a simple string append to each field name:\n\nfunction addSuffixToObjects (suffixes, fields) { return suffixes.map(function (suffix) {\n\nvar result = {};\n\nfields.forEach(function (field) {\n\nresult[field] = field.toUpperCase() + '-' +\n\nsuffix;\n\n});\n\nreturn result;\n\n}); }\n\nWith that helper function, the imposter creation uses the\n\nsame fluent API you built in chapter 2 that wraps mountebank’s RESTful API, as shown in the following\n\nlisting.\n\nListing 9.8. The imposter creation functions",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "var imposter = require('./imposter'), 1\n\nproductPort = 3000;\n\nfunction createProductImposter (suffixes) {\n\nvar products = addSuffixToObjects(suffixes,\n\n['id', 'name', 'description']);\n\nreturn imposter({\n\nport: productPort,\n\nprotocol: \"http\",\n\nname: \"Product Catalog Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({equals: {path:\n\n\"/products\"}}) .respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"},\n\nbody: { products: products }\n\n})\n\n.create();\n\n}\n\nvar contentPort = 4000;\n\nfunction createContentImposter(suffixes) {\n\nvar contentEntries =\n\naddSuffixToObjects(suffixes,\n\n['id', 'copy', 'image']);\n\nreturn imposter({\n\nport: contentPort,\n\nprotocol: \"http\",\n\nname: \"Marketing Content Service\"\n\n})\n\n.withStub()\n\n.matchingRequest({\n\nequals: { path: \"/content\",\n\nquery: {ids: \"ID-1,ID-2\"}\n\n}\n\n})\n\n.respondingWith({\n\nstatusCode: 200,\n\nheaders: {\"Content-Type\":\n\n\"application/json\"}, body: { content: contentEntries }\n\n})",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": ".create();\n\n}\n\n1 See listing 2.5.\n\nArguably, the hardest part of managing a suite of service tests is maintaining the test data. Test data management\n\nis a complicated subject, and many vendors are willing to sell you solutions that promise to ease the pain. Although\n\nsuch tools may help in complex integrated test scenarios, I believe you should use them sparingly. Too often,\n\nthey’re used as a way of avoiding shifting the tests to the left, where left refers to the left side of the deployment\n\npipeline (close to development).\n\nCreating test cases with appropriate isolation via service virtualization is key. The example in listing 9.7 virtualizes\n\na couple of simple service schemas. Real world schemas are often much more complex. In such cases, you will\n\nwant to save off the responses in separate files and use string interpolation to add in any dynamic data needed.\n\nYou can reference the specific scenario directly in your test using a key that identifies the scenario. For example,\n\nif you wanted to test what happens when the product catalog service returns a product but there’s no marketing content for it, use a product ID of NO- CONTENT. Leaving breadcrumb trails in your test data\n\nwill make maintaining it much easier.\n\nThe team writing tests needs to own the configuration for the virtualized service\n\nGenerally, two types of service virtualization tools are\n\navailable.\n\nThe first type comes from the open source community. They almost always support HTTP/S. Although most of\n\nthem support record-playback through proxying, they",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "often expect the team writing the tests—the client team— to define the data that the virtual service returns.\n\nThe second type represents the commercial virtualization tools. They usually are more feature-rich and support a\n\nmore complete set of protocols. But because of the licensing model, they typically expect a central team to\n\nown the virtual service definitions. For automated service tests, that’s exactly backwards from how it should\n\nbe.\n\nAutomated testing requires fine-grained control over the testing scenarios. Those scenarios will require a different\n\nset of test data than another team’s automated tests, even if both test suites have a shared dependency that\n\nrequires virtualizing. Having to go through a central team to set up your test data adds unnecessary friction,\n\nwhich has the unfortunate side effect of discouraging your developers from writing the tests. The complexity of\n\nthe configuration will also become unwieldy to understand and maintain if your test data is comingled\n\nwith that of other teams, adding more friction.\n\nAt this stage of the deployment pipeline, your team needs to be in complete control of its test data. That means\n\nyour team needs to write the configuration for the virtual services. Relying on a central team or the team\n\nproducing the service that you depend on to write the configuration for you will always result in a deficient test\n\nsuite.\n\nThroughout much of this book, I have described\n\nmountebank’s mission statement as keeping things that should be easy actually easy while making hard things possible. Rephrasing that as a competitive product\n\nstrategy, my goal with mountebank is to provide the power of commercial service virtualization tools with the\n\nCD-friendliness of the open source tools.",
      "content_length": 1761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "9.2.3. Balancing service virtualization with contract tests\n\nUnit tests help you design your application and catch simple bugs created while refactoring. Service tests treat\n\nyour application as a black box and help catch bugs from a consumer’s point of view. Adding service virtualization\n\nkeeps service tests deterministic, sealing your test scenarios in a pristine laboratory environment.\n\nProduction is more like a war zone than a lab. Although\n\nservice tests give you confidence that your application works with certain assumptions about your runtime\n\ndependencies, they do nothing to validate those assumptions. Like it or not, your runtime dependencies\n\nchange over time, as they have their own release cycle, so you need some dynamic way of detecting breaking\n\nchanges in services that you depend on (for example, checking that the marketing content service returns the marketing copy in a top-level JSON field called copy). That’s the responsibility of the next stage of the test\n\npipeline: where you run your contract tests.\n\nContract tests move you into the realm of integration, and any time you integrate to the outside world (code\n\nwritten by another team or another company), you’re no longer in a friendly, deterministic environment. Your\n\ntests may fail because your code has a bug, because their code has a bug, because of configuration bugs in the\n\nenvironment itself, or because the network hiccupped. Consequently, you test as much as you can in the friendly\n\nconfines of unit and service tests. Contract tests shouldn’t be deep behavioral tests; they should be\n\nlightweight validations of your assumptions about your runtime dependencies. In effect, they validate that your\n\nstub definitions are compatible with the real service (figure 9.7).",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Figure 9.7. Contract tests validate the assumptions made in service tests.\n\nYou generally want to avoid behavioral testing of your dependencies—that’s the job of the team building those\n\ndependencies. Except in dysfunctional situations, you are better off treating them no differently than you would a\n\nsoftware as a service (SaaS) application that you’d pay to use, or an API from a third party like Google. Aside from\n\nthe obvious cost of behavioral testing of your dependencies, it also increases the fragility of the\n\nenvironment configuration, as behaviorally testing your dependency requires its dependencies to also be\n\nfunctional. This leads you back down the path of end-to- end integration testing, creating exactly the kind of\n\ntraffic jams on the path to production that you’re trying to avoid by using service virtualization.\n\nA contract test example",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "The example we’ve been looking at is a bit too simplistic to show the value of contract testing, in large part\n\nbecause we asserted only that the marketing content gets correctly merged into the product data. The tests haven’t\n\nput too much emphasis on what that product data is, but the website would depend on a certain set of fields to\n\ndisplay the data appropriately. That means that the web façade service must expect a certain set of fields coming\n\nout of the product catalog service. Contract testing helps verify that those expectations remain true as the product\n\nservice changes.\n\nLet’s say you expect a name, description, and availability\n\ndates for a product. You expect the dates to be in ISO format (YYYY-MM-DD) to ensure that you’re parsing it\n\ncorrectly. A contract test validates that those fields exist in the place and format where you expect them. Let’s start by showing the helpers to validate the type and\n\nformat of data you get back, which use a regular expression to validate the date format:\n\nfunction assertString (obj) {\n\nassert.ok(obj !== null && typeof obj ===\n\n'string');\n\n}\n\nfunction assertISODate (obj) {\n\nassertString(obj);\n\nassert.ok(/201\\d-[01]\\d-[0123]\\d/.test(obj),\n\n'not ISO date');\n\n}\n\nThe regular expression uses the same \\d metacharacter you have seen previously, which represents a digit. The\n\nrest of the regular expression matches either literal numbers (for example, to ensure that the year starts with\n\nthe decade this book was written in—201x), or a limited set of numbers represented in brackets (for example, the\n\nmonth must start with 0 or 1, and the day with 0 through 3). This isn’t a perfect test—it allows 2019-19-39, for",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "example—but it’s probably good enough. If you need more confidence, you can add more advanced date\n\nparsing to the test code.\n\nYou can use the assertString and assertISODate\n\nhelpers to write the test, which validates that the fields exist in the location and format you expect them, as\n\nshown in the following listing.\n\nListing 9.9. A contract test validating the placing and format of fields from a real dependency\n\nit('should return correct product fields',\n\nfunction () { return request(productServiceURL + '/products')\n\n1\n\n.then(function (body) {\n\n1\n\nvar products = JSON.parse(body).products;\n\nassert.ok(products.length > 0, 'catalog\n\nempty'); 2\n\nproducts.forEach(function (product) {\n\n3\n\nassertString(product.name);\n\n3\n\nassertString(product.description);\n\n3\n\nassertISODate(product.availabilityDates.start); 3\n\nassertISODate(product.availabilityDates.end);\n\n3\n\n});\n\n});\n\n});\n\n1 Calls the real product catalog service\n\n2 Performs a sanity check\n\n3 Validates field formats\n\nBefore we get into what this function is testing, let’s take\n\na step back to think about what it’s not testing. It’s not testing the full breadth of the product catalog service. The product catalog service may return dozens of fields",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "that aren’t relevant to your service (the web façade), so you don’t test them. Remember, you are testing your\n\nassumptions about the product catalog service, not the product catalog service itself.\n\nWhat you are testing is that the format of the data you get back corresponds to what you expect. You’re doing\n\nthat for every product returned, but you also could’ve done it for only the first product in the array. It’s a time\n\nversus comprehensiveness tradeoff, and you’ll have to face that tradeoff on a case-by-case basis. It’s generally a\n\nsafe assumption that a well-behaved service will return the same schema for all elements in an array.\n\nManaging test data\n\nThe hardest part—by far—of contract testing is managing\n\ntest data. Our example more or less avoids the problem by testing a read-only endpoint, although we still added a sanity check in the test to ensure that at least one\n\nproduct was returned. Contract testing services that allow you to change state are possible but require some\n\nsupport from the service.\n\nThe cleanest method is to have your test create the data\n\nbefore reading it. For example, you may submit an order by sending a POST to /orders and retrieve it by\n\nsending a GET to /orders/123, where 123 is the order ID in the response to the first call. With this approach,\n\nevery test execution creates new data, which ensures that the data is isolated from every other test execution.\n\nHowever, it does require the ability to create test data. That’s a reasonable enough assumption for the orders\n\nservice, but the product catalog service is unlikely to provide APIs to create new products, as that is generally a back-office process.\n\nAn alternative is to coordinate with the provider team on a set of data you can rely on for testing purposes. The",
      "content_length": 1779,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "provider team is then responsible for maintaining a set of golden test data and ensuring that it’s available with each\n\nrelease of the software into the test environment. Any such golden data should be nonmutable by your tests, so\n\nthey can run repeatedly on the same data.\n\nWhere service virtualization fits\n\nIt’s possible to use contract testing without any virtual services. This assumes that the dependencies are\n\ndeployed in a shared environment with all of their dependencies also available, on down the stack to the\n\nsystems of record. Some organizations are either small enough or have invested heavily enough in shared\n\ninfrastructure to make this possible.\n\nAn alternative strategy is for the team that manages the\n\ndependency to deploy a test instance available for contract testing and stub out its dependency through service virtualization. This is likely to increase the\n\navailability and determinism of the test instance, as it’s now less subject to the whims of environmental hiccups.\n\nAs a client of a service that another team provides, you have the right to set some expectations of that service.\n\nAn expectation that a test instance is available is both reasonable and common. You’re better off if you can\n\ntreat that test instance as a black box and have the provider team decide whether they run their test\n\ninstance integrated or with virtual dependencies.\n\n9.2.4. Exploratory testing\n\nHistorically, tests were divided into scripted and unscripted, where the script referred to a documented set\n\nof steps and expectations a manual tester executed. The heyday of meticulously cataloging test cases in\n\ncommercial tools so that unskilled QA testers (remote from the application team) could execute them without\n\nany system context is gone. You still do scripted tests,",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "but you automate them nowadays. Test design occurs as you write the test, and test execution occurs every time\n\nyou run the test.\n\nExploratory testing combines test design and test\n\nexecution into one activity, bringing discipline to unscripted testing. The ability to follow their nose is one\n\nof the defining characteristics of great QA testers. Exploratory testing allows them to investigate the\n\nsoftware with an attitude of curiosity, unearthing its sharp edges through creativity rather than through\n\npredefined scripts.\n\nThe pendulum has swung quite far from the days of yesteryear, when all scripted tests were executed\n\nmanually, to the point where I occasionally perceive a stigma associated with manual testing altogether. This is\n\nunfortunate. Exploratory testing is a fine art, worthy of Whereas exhaustive automated testing its own study.\n\n[9]\n\nmechanizes the toil of scripted test execution, exploratory testing puts the ghost back in the machine. It\n\nrelies on human ingenuity to find gaps in your automation. Despite the widely held perception that\n\nmicroservices are too technical to manually test, there’s a great deal of similarity between how you manually test\n\n[10]\n\nan API\n\nand how you test a traditional GUI.\n\n9\n\nJames Bach gives a good introductory overview at http://www.satisfice.com/articles/what_is_et.shtml.\n\n10\n\nServices and APIs are often used interchangeably. Here I use “service” (or “microservice”) as the implementation and “API” as the interface. The users of a service only see the API. In fact,\n\nthey have no way of knowing if the API is implemented with one service or multiple, because you could use a reverse proxy to route requests to different services under the hood.\n\nI’ve seen people fall into two traps, both of which hurt their ability to gain confidence in your service. The first\n\nis treating the service as an implementation detail, a cog",
      "content_length": 1890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "in a larger value chain, such that the only meaningful test is of the overall end user delivery (where “end users”\n\nmight be customers or business users). The second trap is believing that the service is too technical to test on its\n\nown.\n\nManually testing APIs\n\nOvercoming the first trap requires a mindset shift. The more your organization thinks of the API exposed by\n\nyour microservice as a product, the more likely you are to gain the scaling benefits of microservices. Amazon\n\nprovides an easy to spot example: Amazon Web Services (AWS). AWS started off as a simple object store (S3),\n\nwith an API to store and retrieve files. In short order, Amazon released EC2, which allowed programmatic\n\naccess to managing virtual machines. Both S3 and EC2 are products, as are the hundreds of other products in\n\nthe AWS suite. They have teams that manage them, they have customers, they provide self-service capabilities,\n\nand they hide the underlying complexity of those capabilities.\n\nAWS represents a collection of public APIs, but the same\n\nprinciple applies for APIs you build for your enterprise. The trick is realizing that your internal development\n\nteams are customers. They have needs and use your service to fulfill those needs, saving them time and\n\nreducing the complexity of their overall solution. Understanding their needs helps focus exploratory\n\ntesting.\n\nOnce you recognize that your API is a black box to your\n\ncustomers, you are free to test whether the black box behaves correctly. A good exploratory testing session would start by trying to solve an end-to-end customer\n\nproblem with your APIs and adjusting the path from one API call to the next based on what you learn as you go.\n\nDo the errors make sense? Does the response provide",
      "content_length": 1752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "hints as to what happens next? Sometimes you might discover that, although your API is functionally stable, it\n\nhas significant usability gaps.\n\nAnother way to look at the second trap is thinking that,\n\nbecause there’s no UI, manual testing of APIs doesn’t make sense. Once you treat your API as a product, this\n\nargument disappears. Anytime you have customers (developers), you have a user interface. For APIs, that UI\n\nhappens to be JSON over HTTP (or equivalent).\n\nIn fact, you have been manually testing an API throughout this entire book. Every time you use curl (or\n\nPostman, a graphical equivalent) to send an HTTP request to mountebank, you’re using mountebank’s\n\ndeveloper-facing UI to test it.\n\nWhere service virtualization fits\n\nYou certainly can do exploratory testing without service virtualization. Indeed, at least some of the time, you\n\nshould. It helps gain full system context and understand the types of data that downstream systems emit.\n\nBut exploratory testing requires QA testers to get\n\ncreative. A large part of the exploration is finding out what they should test, which requires playing with some\n\nunusual setups. Virtualizing the dependencies can help provide additional knobs to tune during the exploration.\n\nA real-world scenario may help make that advice concrete. In chapter 8, I described how we used\n\nmountebank to help test APIs that powered the consumer-facing mobile application for a large airline. Our team was blessed with a couple of superbly capable\n\nQA testers who, through exploratory testing, unearthed several problems with our APIs before we released them\n\nto the public.",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Although some of their testing was manual, they used mountebank to test flows under certain scenarios. Going\n\nto the downstream integration points for those scenarios was quite onerous, so when they wanted to follow a flow\n\ninvolving a canceled flight (or a rerouted flight, overbooked flight, delayed flight, and so on), they used a\n\nset of mountebank imposters to facilitate the testing experience. The first time they tested a flow within a\n\ngiven scenario, like a canceled flight, they did so fully integrated so they could see real data. Once they had the\n\ndata, they used mountebank imposters on subsequent test explorations.\n\nExploratory testing completes our whirlwind tour of the role of testing in a continuous delivery world, including\n\nwhere service virtualization fits and where it doesn’t. We’ll round out the next chapter by using mountebank to help us with performance testing.\n\nSUMMARY\n\nA CD deployment pipeline includes automation that extends\n\nbeyond the realm of testing, but testing is central to providing the\n\nconfidence needed to release software frequently. The testing\n\nportion of the pipeline requires validations at multiple layers.\n\nUnit testing is as much a design activity as it’s a bug-catching\n\nactivity. Unit testing is in-process and, as such, uses traditional\n\nstubbing approaches instead of service virtualization.\n\nService tests are postdeployment black-box tests of your\n\napplication. Service virtualization ensures appropriate\n\ndeterminism.\n\nContract tests help validate the assumptions that your application\n\nand your service tests make. They should focus on testing your\n\nassumptions rather than behaviorally testing the dependent\n\nservice.\n\nExploratory testing unleashes human creativity to find flaws in\n\nyour software. Service virtualization may play a role in validating testers’ hunches, or you may avoid it in favor of deeper integration\n\ntesting.",
      "content_length": 1895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Chapter 10. Performance testing with mountebank\n\nThis chapter covers\n\nHow service virtualization enables performance testing\n\nHow to capture appropriate test data with real latencies for load\n\ntesting\n\nHow to scale mountebank for load purposes\n\nThe final type of testing we’ll look at in this book is\n\nperformance testing, which covers a range of use cases. The simplest type of performance test is a load test,\n\nwhich uncovers the system behavior under a certain expected load. Other types of performance tests include\n\nstress tests, which show system behavior when the load exceeds available capacity, and soak tests, which show\n\nwhat happens to the system as it endures load over an extended amount of time.\n\nAll of the tests we’ve looked at up to this point have attempted to prove system correctness, but with\n\nperformance tests, the goal is to understand system behavior more than to prove its correctness. The understanding gained from performance testing does\n\nhelp to improve correctness through unearthing bugs (such as memory leaks) in the application, and it helps to\n\nensure that the operational environment of the application is capable of supporting expected loads. But\n\nno application can support infinite load, and stress testing in particular is designed to break the application\n\nby finding the upper limits of capacity. While that’s happening, a certain degree of errors is expected in many\n\nkinds of performance testing. The goal is to verify system behavior in aggregate rather than verify each service call\n\nindependently. Performance tests often help define service level objectives—for instance, that a service",
      "content_length": 1635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "responds within 500 milliseconds 99% of the time under expected load.\n\nPerformance testing can be difficult. Fortunately, mountebank is there to help.\n\n10.1. WHY SERVICE VIRTUALIZATION ENABLES PERFORMANCE TESTING\n\nOne of the first difficulties organizations run into when\n\nputting together a performance test plan is finding an environment in which to run it. Sometimes, that\n\nenvironment is production.\n\nBelieve it or not, production can be a natural place to\n\nperformance test under certain conditions. The first time you deploy a new application to production, it’s usually\n\nbefore users are able to use it. That gives you an opportunity to validate the capacity of the system, as long as you are careful with the data. In more advanced\n\nscenarios, with new features in existing applications, you may even want to validate performance by synthetically\n\nmanufacturing load in production before users are aware of the new feature. Facebook calls this dark launching,\n\nand did it for two weeks prior to allowing customers to set their own username. The functionality existed in\n\nproduction but was hidden, and a subset of user queries was routed to the new feature to verify it held up under\n\n[1]\n\nFacebook’s scale may be unique—imagine load. generating load from 1.5 billion people—but approaches\n\nlike dark launching can be valuable anytime you want to have additional confidence that a feature scales before\n\nreleasing it to the public.\n\n1\n\nSee https://www.facebook.com/notes/facebook-engineering/hammering-usernames/96390263919/ for more information.",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Most performance testing happens prior to production, with the unfortunate corollary that it rarely happens with\n\nintegrations scaled to support production load. An all- too-common scenario is that, when performance testing\n\noutside of production, the application dependencies crash well before the application, making it impossible to\n\nverify service-level objectives (figure 10.1). When performance testing the application, you are implicitly\n\nmaking the assumption that the application is the weak link in the system. If it isn’t, then you aren’t really testing\n\nthe application and will be unable to discover the load it can support with the hardware it’s using.\n\nFigure 10.1. When runtime dependencies are unstable, you canʼt verify the performance of your application.\n\nAlthough those runtime dependencies are stable enough\n\nto handle production load in production, creating that level of stability isn’t always economically feasible in\n\nlower environments. Supporting additional load requires",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "additional hardware, and doubling the cost of production hardware for testing purposes is generally a hard sell.\n\nMany other reasons exist, some reasonable and some unfortunate, preventing nonproduction runtime\n\ndependencies from supporting the load you need to make your application the weak link. For example, it’s often\n\ndifficult and expensive to scale COTS (custom off-the- shelf software), especially when that COTS package runs\n\non a mainframe.\n\nFigure 10.1 looks like many other diagrams you have seen\n\nalready, and for good reason. Performance testing sits within a class of problems that requires a more\n\ndeterministic approach to testing an application with nondeterministic runtime dependencies. I hope by now\n\nyou can spot exactly the class of problems that service virtualization aims to help. It’s a problem that says “if the rest of the runtime ecosystem is more stable than my\n\napplication, then I can determine the performance characteristics of my application.” Service virtualization\n\nhelps ensure that the application is the weakest link in the runtime ecosystem.\n\nAt some scale, you run into another problem: the virtualization tool itself becomes a weaker link than the\n\napplication (figure 10.2). It’s in effect a hidden dependency that exposes itself when the application’s\n\ncapacity exceeds that of the virtualization tool.\n\nFigure 10.2. At a certain scale, the virtualization tool itself becomes the problem.",
      "content_length": 1435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "This problem exists with mountebank no differently\n\nfrom any other tool. The solution involves horizontally scaling the virtualization tool—running multiple\n\ninstances with shared test data and using a load balancer to spread the load over the multiple instances. This is\n\nwhere mountebank stands apart from the crowd. Scaling commercial tooling is expensive, requiring additional\n\nlicensing. Although a single instance of mountebank won’t perform as well as a single instance of most of the\n\ncommercial tools in the space, mountebank scales for free.\n\nCapital One went through performance testing pain as it\n\nmoved its mobile servicing platform to the cloud. Jason Valentino wrote about the cloud migration and\n\nacknowledged that they never anticipated the difficulty of challenges like performance testing.\n\n[2]\n\n2",
      "content_length": 816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "See https://medium.com/capital-one-developers/moving-one-of-capital-ones-largest-customer-facing- apps-to-aws-668d797af6fc.\n\nIn fact, halfway through we discovered our corporate mocking software couldn’t handle the sheer amount of performance testing we were\n\nrunning as part of this effort (we completely crushed some pretty industrial enterprise\n\nsoftware in the process). As a result, we made the call to move the entire program over to a\n\nMountebank OSS-based solution with a custom provision to give us the ability to expand/shrink our\n\nmocking needs on demand.\n\nJason Valentino (emphasis his)\n\nIn the remainder of this chapter, we will stress test a\n\nsample service, finding its capacity. In doing so, we will [3] follow a four-step process:\n\n3\n\nThe steps are largely the same for other types of performance tests.\n\nDefine your scenarios.\n\nCapture the test data for each scenario.\n\nCreate the tests for a scenario.\n\nScale mountebank as needed.\n\nLet’s look at each of these steps in turn.\n\n10.2. DEFINING YOUR SCENARIOS\n\nPerformance tests are all about figuring out common paths users will likely take, then calling those paths a lot.\n\nFor example, let’s return to our favorite online pet store,\n\nbut add a new component to make a more realistic performance testing scenario. You’ll add a new adoption\n\nservice that provides the pet adoption information, helping connect potential owners with rescue pets. (See\n\nfigure 10.3.)",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Figure 10.3. Adding the adoption service to your pet store microservices\n\nThe service integrates with a public API from RescueGroups.org,\n\n[4]\n\nwhich makes it a perfect place to\n\nuse service virtualization. Although you’d like to test the adoption service to make sure it can handle load,\n\nslamming a public API providing animal adoption information free of charge seems rude, especially when\n\nit’s only for testing purposes. Every time you run your performance tests connected to a free public pet\n\nadoption service, your unintentional denial of service attack kills a kitten.\n\n4\n\nSee https://userguide.rescuegroups.org/display/APIDG/HTTP+API for API details.\n\nA scenario is a multistep flow that captures user intent. The trick is to put yourself in the mind of a user and\n\nimagine a common sequence of activities the user would want to complete. In this case, because the adoption\n\nservice is an API, the direct users will be other developers, but it will be in support of users on a website\n\nor mobile device, and their intent will be reflected in the sequence of API calls. You’d expect the end customers to",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "search for nearby pets, maybe change the search parameters a few times, then click on a few pets. Let’s\n\nformalize that into a performance test scenario:\n\nUser searches for pets within a 20-mile radius of zip code 75228.\n\nUser searches for pets within a 50-mile radius of zip code 75228.\n\nUser gets details on the first three pets returned.\n\nThat scenario requires two APIs and five API calls in the process of completing two searches and providing three\n\nsets of details. The sequence of API calls with the adoption service would look like this:\n\n[5]\n\n5\n\nThe GitHub repo for this book has the source code: https://github.com/bbyars/mountebank-in- action.\n\nGET /nearbyAnimals?postalCode=75228&maxDistance=20\n\nGET /nearbyAnimals?postalCode=75228&maxDistance=50\n\nGET /animals/10677691\n\nGET /animals/10837552\n\nGET /animals/11618347\n\nThe animal IDs may vary from run to run, as the data\n\nthat the searches return changes over time. A robust test scenario would support dynamically pulling the IDs from\n\nthe search, but you’ll keep it simple to focus on the essentials.\n\nNow that you have a multistep scenario defined, it’s time to capture the test data.\n\n10.3. CAPTURING THE TEST DATA\n\nAccurately simulating a runtime environment for load\n\ntests requires that virtual services both respond similarly to how real services respond and perform like real\n\nservices perform in production. A proxy can capture both bits of information, and for performance testing you’ll almost always want to use proxyAlways mode. The",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "default proxyOnce mode is convenient in situations when you want the saved responses to respond after the\n\nfirst call to the downstream service, but it’s natural to separate the test data capture from the test execution in\n\nperformance testing. Also, the richer set of data you are able to capture with proxyAlways often comes in\n\nhandy. Recall from chapter 5 that proxyAlways mode means that every call will be proxied to the downstream\n\nsystem, allowing you to record multiple responses for the same request (where the request is defined by the predicateGenerators) (figure 10.4).\n\nFigure 10.4. A proxyAlways proxy allows capturing complex test data.\n\nNotice that the leftmost box in figure 10.4 isn’t the\n\nperformance tests themselves. You aren’t ready for those",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "yet; they come after you shut down the connection to the real RescueGroups API. At this stage, you want just\n\nenough load to capture meaningful test data. Anything more than that is an unnecessary load on downstream\n\nservices.\n\n10.3.1. Capturing the responses\n\nThis scenario is simple enough that you can capture data\n\nfor the five API calls and replay it over and over again during the performance test run. Technically, this means you don’t need proxyAlways mode for your proxy, but it’s generally a good idea to use it anyway when you are\n\ndoing anything mildly complicated with test data capture. The proxy stub looks like the following listing.\n\nListing 10.1. Basic proxy response to capture test data\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://api.rescuegroups.org/\", 1\n\n\"predicateGenerators\": [ 2 { \"matches\": { \"body\": true } } 2\n\n] 2\n\n}\n\n}]\n\n}\n\n1 Proxy to real service\n\n2 Saves new response for every unique request body\n\nThe adoption service uses different URLs and query\n\nparameters for each of the five API calls, but behind the curtain, they route to the same URL in the\n\nRescueGroups.org API. RescueGroups.org uses a super- generic API in which every call is an HTTP POST to the\n\nsame path (/http/v2.json). The JSON in the request body defines the intention of the call, any filters used, and so\n\non. Recall from chapter 5 that you use a proxy’s predicateGenerators to define the predicates for the saved response. Because each of your five API calls will",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "send a unique request body to the RescueGroups.org API, differentiating requests by body makes sense. If you\n\nwanted to be more specific, you could use a JSONPath predicate generator to split on the exact fields within the body that are different, but that’s overkill for this example. Once you have the proxy configured, you have\n\nto run your test scenario and save the test data (figure 10.5).\n\nFigure 10.5. Using a proxy to capture test data\n\nAnytime you use service virtualization, you have to be able to swap out the URL of the downstream dependency\n\nin the system under test. The adoption service in the GitHub repo for this chapter supports using an\n\nenvironment variable to change the URL of the downstream service. Assuming you run your proxy imposter on port 3000, you could configure the adoption\n\nservice like this:\n\nexport RESCUE_URL=http://localhost:3000/\n\nWith the proxy running and the adoption service pointed\n\nto it instead of the real service, you can capture the data for your five API calls with whatever HTTP engine you",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "want, including curl. Assuming the adoption service is running on port 5000, that might look like\n\ncurl http://localhost:5000/nearbyAnimals?\n\npostalCode=75228&maxDistance=20\n\ncurl http://localhost:5000/nearbyAnimals?\n\npostalCode=75228&maxDistance=50 curl http://localhost:5000/animals/10677691\n\ncurl http://localhost:5000/animals/10837552\n\ncurl http://localhost:5000/animals/11618347\n\nOnce you have done that, you can save the test data:\n\nmb save --removeProxies --savefile mb.json\n\nWith that, you have your responses. However, you need one more thing.\n\n10.3.2. Capturing the actual latencies\n\nTo get an accurate performance test, you’ll want to\n\nsimulate the actual latency from the downstream service. In chapter 7, we looked at the wait behavior, which\n\nallows you to tack on latency to each response. You can capture it from the downstream system by setting the addWaitBehavior attribute of the proxy to true, as shown in the following listing.\n\nListing 10.2. Capturing latency from the downstream system\n\n{\n\n\"responses\": [{\n\n\"proxy\": {\n\n\"to\": \"https://api.rescuegroups.org/\",\n\n\"predicateGenerators\": [\n\n{ \"matches\": { \"body\": true } }\n\n], \"addWaitBehavior\": true 1\n\n}\n\n}]\n\n}\n\n1 Captures actual latency",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "If you capture your test data again by making the five API calls to the adoption service and saving the data with mb save, the proxy will have automatically added the wait behavior to each of the saved responses. For\n\nexample, in my test run, here’s a trimmed down version of a saved response:\n\n{\n\n\"is\": {\n\n\"statusCode\": 200,\n\n\"headers\": { ... }, 1 \"body\": \"...\", 1\n\n\"_mode\": \"text\"\n\n},\n\n\"_behaviors\": {\n\n\"wait\": 777 2\n\n}\n\n}\n\n1 Omitted for clarity\n\n2 Wait 777 ms before responding\n\nThe two searches in the test run used to prepare this sample took 777 and 667 milliseconds, respectively, and\n\nthe three requests for animal details took 292, 322, and 290 milliseconds. Those wait times were saved with each\n\nresponse to be replayed during the performance test run. The more data you capture during proxying, the more\n\nvariability you’ll have with your latencies.\n\n10.3.3. Simulating wild latency swings\n\nOur example assumes that the downstream system behaves correctly. That’s a perfectly valid assumption to\n\nmake when you want to see what happens when the system under test is the weakest link in the chain, but\n\nsometimes you also want to expose cascading errors that happen when downstream systems become overloaded,\n\nreturning a higher percentage of errors and (even worse) responding increasingly slowly. If the environment\n\nsupports recording proxy data under load, you may be",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "able to capture the data from a downstream test system. If not, you’ll have to simulate it.\n\nThe wait behavior supports an advanced configuration for this use case. Instead of passing the number of\n\nmilliseconds to wait before returning a response, you can pass it a JavaScript function. Assuming you have started mb with the --allowInjection command-line flag, you can simulate a wild latency swing with the following\n\nfunction. It usually responds within a second, but roughly every 10 times it takes an order of magnitude\n\nlonger.\n\nListing 10.3. Adding random latency swings with JavaScript injection\n\nfunction () { var slowdown = Math.random() > 0.9,\n\nmultiplier = slowdown ? 10000 : 1000;\n\nreturn Math.floor(Math.random() * multiplier);\n\n}\n\nInstead of passing an integer representing a number of\n\nmilliseconds, you’d pass the entire JavaScript function to the wait behavior. Assuming you saved the previous\n\nfunction in a file called randomLatency.js, you could use EJS templating:\n\n{ \"is\": { ... },\n\n\"_behaviors\": {\n\n\"wait\": \"<%- stringify(filename,\n\n'randomLatency.js') %>\"\n\n}\n\n}\n\nThe downside is that this doesn’t work naturally with proxying, which captures actual latency.\n\n10.4. RUNNING THE PERFORMANCE TESTS\n\nYou can write all of the tests up to this point in the book\n\nwith traditional unit testing tools from the JUnit family.",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Performance testing requires more specialized tools that provide a few key features the JUnit-style tools don’t:\n\nScenario recording, usually by configuring the tool as a proxy\n\nbetween your HTTP executor (generally a browser) and the application you are testing\n\nDomain-specific languages (DSLs) for adding pauses, simulating\n\nthe think time for users in between actions, and ramping up users\n\nThe ability to use multiple threads to simulate multiple users\n\nsending concurrent requests\n\nReporting capability to give you the performance characteristics of\n\nyour application after a test run\n\nAlthough many commercial options exist, some excellent open source performance testing tools are available that\n\ndon’t require you to open your checkbook. JMeter (http://jmeter.apache.org/) and a newer offshoot called\n\nGatling (https://gatling.io/) are popular choices. You’ll use a Gatling script simple enough to allow you to keep the\n\nfocus on service virtualization without having to learn a whole new tool.\n\nThe Gatling download is a simple zip file, which you can\n\nunpack in any directory you desire. Set the GATLING_HOME environment variable to that directory\n\nto make the example easier to follow. For example, if you have unpacked it on your home directory in Linux or\n\nmacOS, type this in your terminal (assuming you downloaded the same version used for this example):\n\nexport GATLING_HOME=~/gatling-charts-highcharts-\n\nbundle-2.3.0\n\nThe next step is to create a Gatling script that represents your scenario using its Scala-based DSL. I copied the\n\nsample scenario that ships with Gatling and adjusted it as shown in listing 10.4. As someone who has never\n\nprogrammed in Scala before, I found myself able to read and write most of the script quite fluently thanks to the\n\nexpressive DSL. It executes your five API calls with",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "pauses in between, representing the think time in seconds that the end users likely will take to process the\n\nresults.\n\nListing 10.4. A Gatling script for your test scenario\n\nclass SearchForPetSimulation extends Simulation {\n\nval httpProtocol = http\n\n.baseURL(\"http://localhost:5000\")\n\n1\n\nval searchScenario =\n\nscenario(\"SearchForPetSimulation\")\n\n.exec(http(\"first search\")\n\n2\n\n.get(\"/nearbyAnimals?\n\npostalCode=75228&maxDistance=20\")) 2\n\n.pause(10)\n\n2 .exec(http(\"second search\")\n\n2\n\n.get(\"/nearbyAnimals?\n\npostalCode=75228&maxDistance=50\")) 2\n\n.pause(15)\n\n2\n\n.exec(http(\"first animal\")\n\n3 .get(\"/animals/10677691\"))\n\n3\n\n.pause(5)\n\n3\n\n.exec(http(\"second animal\")\n\n3\n\n.get(\"/animals/10837552\"))\n\n3 .pause(5)\n\n3\n\n.exec(http(\"third animal\")\n\n3\n\n.get(\"/animals/11618347\"))\n\n3\n\nsetUp( searchScenario.inject(rampUsers(100) over (10\n\nseconds)) 4\n\n).protocols(httpProtocol)\n\n}\n\n1 Base URL of adoption service",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "2 Searches, with think time\n\n3 Animal details with think time\n\n4 Simulates 100 users\n\nThe most interesting bit is near the bottom, which describes how many concurrent users you want to simulate and how long to ramp them up. It’s fairly\n\nunrealistic to expect that, at max load, all users start at the same time, so most performance test scenarios\n\naccount for a ramp-up period. Although 100 users isn’t much, it helps you test out your scenario.\n\nTo run Gatling, navigate to the code for chapter 10 in the GitHub repo for this book and enter the following into\n\nyour terminal.\n\nListing 10.5. Testing your performance script\n\n$GATLING_HOME/bin/gatling.sh \\\n\nsf gatling/simulations 1 -s adoptionservice.SearchForPetSimulation 2\n\nrf gatling/reports 3\n\n1 Points to your simulations directory\n\n2 Runs the correct scenario\n\n3 Saves the output here\n\nOn my machine, running this scenario for 100 users takes a little under a minute, which is hardly enough to\n\nstress either the software or hardware but enough to validate the script. Once you’re satisfied it’s working,\n\nbump the users up an order of magnitude or two and rerun to see what happens:\n\nsetUp( searchScenario.inject(rampUsers(1000) over (10\n\nseconds))\n\n).protocols(httpProtocol)\n\nOn the MacBook Pro I’m using to create this example,\n\nthe adoption service can handle 1,000 users with no",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "problem but croaks pretty hard at 10,000 users. When I tried running with that many users, Google Chrome\n\ncrashed, my editor froze, and I may have cried a little because of a failure to save work in progress, but,\n\nfortunately, no kittens died.\n\nThat’s useful information—not just the kittens, but the\n\nnumber of users: the adoption service, when run on my laptop, has the capacity to support somewhere between\n\n1,000 and 10,000 users concurrently. In addition to highlighting a horrific lack of error handling in the\n\nadoption service (subsequently improved), that information would help you determine the appropriate\n\nhardware to run in production based on the expected number of concurrent users trying to save a rescue\n\nanimal from the pound.\n\nI experimented a bit until I found a reasonable number of users that stressed the adoption service on my laptop\n\nwithout completely breaking it, which turned out to be 3,125 concurrent users. Understanding your\n\napplication’s behavior under stress is a useful activity for determining what happens at expected peak load and\n\nhelps to validate your service-level objectives.\n\nThe test reports are saved in the “gatling/reports” directory you passed into the -rf parameter when you\n\nstarted Gatling. The HTML page gives you all kinds of information that helps you understand the performance\n\ncharacteristics of your application. The table shown in figure 10.6 comes from a report on one of my runs and\n\nshows the % KO (errors, with KO being both a common boxing abbreviation for knockout and a clever anagram\n\nof OK) and statistical information around response times for each request.\n\nFigure 10.6. Gatling saves error rates and response time data for each step of the scenario.",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Service performance is often quoted as something to the\n\neffect of “99% of the time we promise to return in under 500 milliseconds at or below peak load.” Clearly, you\n\nhave some work in front of you before you can make that kind of guarantee.\n\n10.5. SCALING MOUNTEBANK\n\nGiven that the adoption service is a simple example, I\n\nwas unable to get mountebank to crash under load before the service did. For production-quality services built by\n\nenterprises and deployed in a high-availability environment, that won’t always be the case. When\n\nmountebank itself becomes the weakest link in your chain, you have some options.\n\nThe first and most obvious is to run multiple\n\nmountebank instances behind a load balancer (figure 10.7). This allows different requests to route to different\n\nmountebank instances, each configured with the same test data.\n\nFigure 10.7. Load balancing using multiple mountebank instances",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "One situation requires additional thought. If the test data\n\nsupports sending different responses for the same logical request, then you’ll no longer be able to rely on a\n\ndeterministic ordering of those responses. The diagram in figure 10.8 shows two calls to your virtualized instance\n\nof an inventory service, which should return 54 on the first call and 21 on the second. Instead, it returns 54\n\ntwice in a row.\n\nFigure 10.8. Responses for the same request under load balancing yield unexpected results.",
      "content_length": 507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "There’s no good way around this. Even if you use server affinity, a load balancer configuration that binds a client\n\nto the same mountebank instance for each request, you will likely still have the problem, because it’s the system\n\nunder test, not the test user, that’s making the request to mountebank. In addition to load balancing, you should\n\ndo a few things on each instance of mountebank to guarantee optimal performance.\n\nFirst, avoid using the --debug and --mock command line options when running mb. These options capture\n\nadditional information about the requests that the system under test makes to mountebank, which is useful\n\nboth for debugging the imposter configuration and for verifying that the system under test made the correct\n\nrequests. Although capturing that information during a behavioral test can be useful, performance tests require long-lived imposters. Computer programmers have a\n\ncommon phrase they use to describe the process by which a long-lived system remembers information\n\nwithout any mechanism of forgetting it: a memory leak.\n\nSecond, you will want to decrease the log output of\n\nmountebank. Mountebank uses a standard set of logging configuration levels—debug, info, warn, and error—\n\nand defaults to info. That sends some log output to the terminal and logging file on every request, which is\n\nunnecessary and unhelpful when you intend to send thousands of requests its way. I would recommend running with a warn level when you are writing and debugging performance scripts and with error during\n\nthe test run. You do that by passing --loglevel warn or --loglevel error as a flag to the mb command.\n\nFinally, you’ll generally want to configure the responses\n\nmountebank sends back to use keep-alive connections, which avoid the TCP handshake on every new\n\nconnection. Keep-alive connections are a huge",
      "content_length": 1842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "performance increase, and the proxy will generally capture them because most HTTP servers use keep-alive\n\nconnections by default. Unexpectedly, RescueGroups.org doesn’t, at least in my test run, so the example avoids the\n\nuse of keep-alive connections. This is probably appropriate because you’re trying to accurately simulate\n\nthe downstream system behavior. But it wouldn’t be hard to write a simple script to postprocess your test data file and change the Connection headers to keep-alive in all of your saved responses, should you choose to do so.\n\nAlso remember that, if you’re adding in simulated is or inject responses that you didn’t capture through\n\nproxying, you’ll have to manually set the Connection header to keep-alive, as mountebank defaults to\n\nclose for historical reasons. The easiest way is to change the default response, as you saw in chapter 3:\n\n{\n\n\"protocol\": \"http\",\n\n\"port\": 3000,\n\n\"defaultResponse\": {\n\n\"headers\": {\n\n\"connection\": \"keep-alive\"\n\n}\n\n}, \"stubs\": [...]\n\n}\n\nAnd at long last, you have a complete performance\n\ntesting environment that doesn’t depend on any downstream system. And, best of all, no kittens were\n\nkilled in the process.\n\nPerformance testing wraps up our tour of service\n\nvirtualization. Although mountebank is clearly not the only tool you’ll need in your tool belt, it makes many important contributions to allowing you to validate your\n\napplication through a continuous delivery pipeline, even in a microservices world with a great deal of runtime",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "complexity. Mountebank is always changing, and there’s an active mailing list on the website, which I encourage\n\nyou to use anytime you get stuck. Don’t be a stranger!\n\nSUMMARY\n\nService virtualization enables performance testing by protecting\n\ndownstream systems from load. It enables you to test your application as if it’s the weakest link in the chain.\n\nOnce you’ve determined your scenarios, you’ll generally want to use a proxy in proxyAlways mode to capture test data. Set the addWaitBehavior to true to capture actual latencies.\n\nTools like Gatling and JMeter support converting your test scenarios into robust performance scripts. Ensure you are running\n\nwith virtual services if your goal is to find the capacity of your\n\nservice without impacting downstream services.\n\nIf mountebank itself becomes the constraint, scale through load\n\nbalancing. Improve performance of each instance by decreasing\n\nlogging and using keep-alive connections.",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Index\n\n[SYMBOL][A][B][C][D][E][F][G][H][I][J][K][L][M][N][O][P][Q][R][S][T]\n\n[U][V][W][X][Z]\n\nSYMBOL\n\n& character\n\n$ID token\n\n3A pattern\n\n400 Bad Request message\n\n429 HTTP status code nd\n\n500 status code, 2\n\nA\n\nAbagnale service\n\nAccept header\n\nnd\n\naddWaitBehavior attribute, 2 rd\n\nnd allowInjection flag, 2 , 3 , 4 , 5 , 6\n\nth\n\nth\n\nth\n\nAmazon Web Services (AWS)\n\nAnnounce method\n\nAnnouncementLog class\n\nAnnouncementTemplate\n\nAPIs (application program interfaces)\n\nmanually testing\n\ntest-by-test setup with\n\napplication protocol\n\nArgsFor method\n\nnd\n\nArrange-Act-Assert pattern, 2\n\nassertISODate helper\n\nassertString helper\n\nasterisk character\n\nasynchronous operations, adding",
      "content_length": 674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "starring mountebank repo\n\nstarting OAuth handshakes\n\nvalidating OAuth authorizations\n\nvirtualizing OAuth-backed GitHub clients\n\nautomated testing\n\nautomation, boundary of\n\nAWS (Amazon Web Services)\n\nB\n\nBase64 encoding\n\nbehaviors\n\nadding latency to responses\n\ncomplete list of\n\ndecorating responses\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\noverview of\n\nrepeating responses\n\nreplacing content in responses\n\ncopying request data to responses\n\nlooking up data from external data sources\n\nbinary mode\n\npredicates in\n\nwith Base64 encoding\n\nblocking I/O\n\nboundary of automation\n\nboundary of deployment\n\nboundary of determinism\n\nboundary of value\n\nC\n\ncallback function",
      "content_length": 731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "canned responses\n\ncycling through\n\ndefault responses\n\nchanging\n\nidentifying complete HTTP messages\n\nreusing HTTP connections\n\nnd\n\noverview of, 2\n\ntesting with\n\nHTTPS imposters\n\nsaving responses in configuration files\n\ncapturing\n\nlatencies\n\nresponses\n\ntest data\n\nnd\n\nCAs (Certificate Authorities), 2\n\nnd\n\ncaseSensitive parameter, 2\n\nCD (continuous delivery)\n\ncreating test pipelines\n\nbalancing service virtualization with contract tests\n\ncreating service tests\n\ncreating unit tests\n\nexploratory testing\n\noverview of\n\nwith microservices\n\nCertbot\n\nnd\n\nCertificate Authorities (CAs), 2\n\nCI (continuous integration)\n\nnd\n\ncircular buffer, 2\n\ncommercial virtualization tools\n\nconfigfile command-line option\n\nconfiguration files\n\nsaving multiple imposters in\n\nsaving responses in",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "configuring proxies\n\nadding custom headers\n\nmutual authentication\n\ncongestion\n\nconjunctions with predicates\n\nconsole.log function\n\nnd\n\ncontains predicate, 2\n\nContainsMethodName method\n\ncontent in responses, replacing\n\ncopying request data to responses\n\nlooking up data from external data sources\n\nnd\n\nContent-Length header, 2\n\ncontinuous deployment\n\ncontinuous integration (CI)\n\ncontract tests\n\nexamples of\n\nmanaging test data\n\noverview of\n\nnd\n\nservice virtualization and, 2\n\nconvenience functions\n\ncopy behavior\n\ncopy field\n\nCORS (Cross-Origin Resource Sharing)\n\nCOTS (custom off-the-shelf software)\n\ncreate function\n\nnd\n\ncreateContentImposter function, 2\n\nCreateImposter function\n\nnd\n\ncreateProductImposter function, 2\n\ncreation function\n\nCrier class\n\nnd\n\ncsvToObjects function, 2\n\nnd\n\ncurl command, 2\n\ncustom off-the-shelf software (COTS)",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "D\n\nd@ command-line switch\n\ndata\n\ncapturing test data\n\ncapturing latencies\n\ncapturing responses\n\nsimulating wild latency swings\n\nfrom external data sources\n\nmanaging test data\n\nnd\n\ndebugging, 2\n\ndecorate function\n\ndecorating responses\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\ndecryption\n\nnd\n\ndeepEquals predicate, 2\n\ndefault responses\n\nchanging\n\nidentifying complete HTTP messages\n\noverview of\n\nreusing HTTP connections\n\ndefault stub\n\nnd\n\nDELETE command, 2\n\nnd\n\ndeployment pipelines, 2\n\ndescribe function\n\nnd\n\ndeterministic tests, 2\n\ndev complete\n\ndomain-specific languages (DSLs)\n\ndownstream capacity\n\nDRY (Don’t Repeat Yourself)\n\nDSLs (domain-specific languages)\n\nE",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "EcmaScript (ES)\n\nEJS language\n\nend-to-end testing\n\nendOfRequestResolver function\n\nendsWith predicate\n\nengineering discipline\n\nenterprise service bus (ESB)\n\nequals operator\n\nnd\n\nequals predicates, 2\n\nES (EcmaScript)\n\nESB (enterprise service bus)\n\nescape characters\n\nnd\n\nexists predicate, 2\n\nnd\n\nexploratory testing, 2\n\nmanually testing APIs\n\nservice virtualization and\n\nexternal data sources\n\nF\n\nfallback proxy\n\nfeature complete\n\nFile.readlines function\n\nfilename variable\n\nfromDataSource field\n\nG\n\nGATLING_HOME variable\n\nnd\n\nGET method, 2\n\nGitHub clients\n\ngreaterThan predicate\n\ngrouped matches\n\nGUID (globally unique identifier)\n\nH\n\nnd",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "nd\n\nhandshakes, 2\n\nhardening iterations\n\nhasThreeDaysOutOfRange function\n\nheaders\n\nHTTP\n\nconverting HTTPS to\n\nmessages, identifying\n\nreusing connections\n\nhttpRequest.call() function\n\nhttpRequest.end() function\n\nHTTPS\n\nconverting to HTTP\n\nimposters\n\ntrusted, setting up\n\nusing mutual authentication\n\nI\n\nI/O operations\n\nidentifiers, matching on paths\n\nimposters\n\nHTTPS\n\ntrusted, setting up\n\nusing mutual authentication\n\nmultiple, saving in configuration files nd\n\nrd\n\noverview of, 2 , 3\n\nTCP, creating\n\ninclude function\n\nnd\n\nrd\n\ninject response type, 2 , 3\n\ninventory checks\n\nInvoke-RestMethod command\n\nipWhitelist flag\n\nnd\n\nrd",
      "content_length": 625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "nd\n\nrd\n\nis response, 2 , 3\n\nISO format\n\nJ\n\nJSON format\n\ndirect predicates\n\nvalues\n\npredicates with\n\nselecting with JSONPath\n\nnd\n\njsonpath parameter, 2\n\njsonpath predicateGenerator\n\nJSONPath tool\n\ncapturing multiple values\n\ngenerating predicates\n\nselecting JSON values with\n\nK\n\nkeep-alive connections\n\nkeys\n\nL\n\nlatencies\n\nadding to responses\n\ncapturing\n\nsimulating swings\n\nLet’s Encrypt\n\nlink layer\n\nlocalOnly flag\n\nloglevel debug flag\n\nlookup behavior",
      "content_length": 451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "M\n\nMarshalByRefObject\n\nmatches predicate\n\nmatching\n\ngrouped matches\n\nidentifiers on paths\n\nmultivalued fields\n\nobject request fields\n\nXML payloads\n\nmatchingRequest function\n\nmb protocol\n\nmb replay command\n\nnd\n\nmb save command, 2\n\nnd\n\nmetacharacters, 2\n\nmicroservices\n\nCD with, testing strategy for\n\nevolution of\n\norganizational structure and\n\noverview of\n\ntesting\n\nend-to-end testing, challenges of\n\nmountebank, overview of\n\nservice virtualization\n\nservice virtualization tool ecosystem\n\nmiddleware, adding\n\nmock option\n\nmonoliths\n\nmountebank\n\nHTTP and\n\noverview of\n\nstarring repo\n\nMule ESB",
      "content_length": 590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "multivalued fields, matching nd\n\nmutual authentication, 2\n\nnd\n\nmutualAuth field, 2\n\nN\n\nnamespaces, support for\n\n.NET Remoting framework\n\ncreating clients\n\nvirtualizing servers\n\nvirtualizing services\n\nindicating where messages end\n\noverview of\n\nNetcat\n\nnd\n\nnonblocking I/O, 2\n\nO\n\nOAuth protocol\n\nOAuth-backed GitHub clients, virtualizing\n\nstarting handshakes\n\nvalidating authorizations\n\nobject request fields, matching\n\nOS (operating system)\n\nP\n\nparameterizing predicates\n\nparseInt function\n\npaths, matching identifiers on\n\npayloads, XML\n\nmanipulating\n\nmatching\n\nPEM format",
      "content_length": 572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "performance testing\n\ncapturing test data\n\ncapturing latencies\n\ncapturing responses\n\nsimulating wild latency swings\n\ndefining scenarios\n\nrunning tests\n\nscaling mountebank\n\nservice virtualization enabling\n\npersistent data stores\n\npipelines\n\ndeployment pipelines\n\ntest pipelines\n\nbalancing service virtualization with contract tests\n\ncreating service tests\n\ncreating unit tests\n\nexploratory testing\n\nPostman\n\npredicate injection function\n\nnd\n\npredicateGenerators, 2\n\npredicates\n\nadding parameters\n\ncase-sensitive\n\nconjunctions with\n\nnd\n\ncreating, 2\n\ncapturing multiple JSONPath values\n\ncapturing multiple XPath values\n\nfor JSONPath\n\nfor XPath\n\nwith predicateGenerators\n\ndeepequals predicate\n\nexists predicate\n\nin binary mode\n\nmatches predicate",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "matching multivalued fields\n\nmatching object request fields\n\nnd\n\nrd\n\noverview of, 2 , 3\n\nparameterizing\n\npredicate injections vs responses\n\nreplacing with regular expressions\n\nselecting XML values\n\ntypes of\n\ncomplete list\n\nmatching any identifier on path\n\nwith JSON values\n\ndirect JSON predicates\n\nselecting JSON values with JSONPath\n\npreflight responses\n\nprintf debugging\n\nprivate key\n\nprocess discipline\n\nproduct catalog services\n\nproductCatalog object\n\npromises\n\nprotocols\n\nbinary support\n\nbinary mode with Base64 encoding\n\npredicates in binary mode\n\noverview of\n\nvirtualizing .NET Remoting service\n\ncreating clients\n\nindicating where messages end\n\nvirtualizing .NET Remoting servers\n\nproxies\n\nadding decoration to saved proxy responses\n\nas fallbacks\n\nconfiguring\n\nadding custom headers",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "mutual authentication\n\nreplaying\n\nsetting up\n\nTCP, creating\n\nuse cases\n\nproxy response type\n\nnd\n\nproxyAlways mode, 2 nd\n\nproxyOnce mode, 2\n\nproxyResponseTime field\n\npublic key\n\nPUT command\n\nQ\n\nquery parameter\n\nquerystring parameter\n\nquestion mark character\n\nR\n\nrate limit exception\n\nreal imposter\n\nnd\n\nrecord/replay behavior, 2\n\ncapturing multiple responses for same request\n\nconfiguring proxies\n\nadding custom headers\n\nmutual authentication\n\ngenerating correct predicates\n\nadding predicate parameters\n\ncapturing multiple JSONPath values\n\ncapturing multiple XPath values\n\ncreating predicates with predicateGenerators\n\nproxy use cases\n\nconverting HTTPS to HTTP\n\nusing proxies as fallback\n\nreplaying proxies\n\nsetting up proxies\n\nnd",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "nd\n\nregex (regular expressions), 2\n\nremoveProxies flag\n\nrepeat behavior\n\nrepeating responses\n\nreplayable parameter\n\nreplaying proxies\n\nrequest data, copying to responses\n\nusing grouped matches\n\nusing XPath selectors\n\nvirtualizing CORS preflight responses\n\nrequest library\n\nrequests, capturing responses for\n\nrespondingWith function\n\nnd\n\nresponse injection function, 2\n\nresponses\n\nadding latency to\n\ncapturing\n\nmultiple for same request\n\noverview of\n\ncopying request data to\n\nusing grouped matches\n\nusing XPath selectors\n\nvirtualizing CORS preflight responses\n\ncycling through\n\ndecorating\n\nadding decoration to saved proxy responses\n\nadding middleware through shellTransform\n\nusing decorate function\n\ndynamic, creating\n\nadding async\n\nadding state\n\npredicate injections vs\n\nrepeating\n\nreplacing content in",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "looking up data from external data sources\n\noverview of\n\nnd\n\nsaving in configuration files, 2\n\nretrieve function\n\nRPCs (Remote Procedure Calls), stubbing\n\ncreating TCP imposters\n\ncreating TCP proxies\n\nmanipulating XML payloads\n\nmatching XML payloads\n\nruntime dependencies\n\nS\n\nnd\n\nSaaS (software-as-a-service), 2\n\nsame-origin policy\n\nsavefile argument\n\nnd\n\nscaling, 2\n\nscenarios, defining\n\nsecurity\n\nSerialize method\n\nservers, .Net Remoting\n\nservice test\n\nservice virtualization\n\nnd\n\ncontract tests and, 2\n\nexamples of\n\nmanaging test data\n\nenabling performance testing\n\nexploratory testing and\n\nnd\n\noverview of, 2\n\npersistent data stores\n\nrecord and replay\n\ntest-by-test setup using APIs\n\ntool ecosystem of\n\nnd\n\nrd\n\nshellTransform behaviors, 2 , 3\n\nSignatureFor method\n\nsimulating latency swings",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "soak tests\n\nnd\n\nsoftware-as-a-service (SaaS), 2\n\nspanned reports\n\nstarring mountebank repo\n\nstartsWith predicate\n\nstate parameter\n\nstate.humidities array\n\nstates, adding\n\nstress tests\n\nstring escape character nd\n\nrd\n\nstringify function, 2 , 3\n\nstub() function\n\nstubs\n\nT\n\nTCP (Transmission Control Protocol)\n\ncreating imposters\n\ncreating proxies\n\noverview of\n\nstubbing text-based TCP-based RPCs\n\nmanipulating XML payloads\n\nmatching XML payloads\n\nTDD (test-driven development)\n\nTearDown method\n\ntest case construction\n\ntest-driven development (TDD)\n\ntesting\n\ncontract tests\n\nexamples of\n\nmanaging test data\n\nservice virtualization and, 2nd\n\ncreating test pipelines\n\nend-to-end",
      "content_length": 674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "exploratory testing\n\nmanually testing APIs\n\nservice virtualization and\n\nmapping strategy to deployment pipeline\n\nmicroservices\n\nmountebank, overview of\n\nservice virtualization\n\nservice virtualization tool ecosystem\n\nperformance testing\n\ncapturing test data\n\ndefining scenarios\n\nrunning tests\n\nscaling mountebank\n\nservice virtualization enabling\n\nservice tests, creating\n\nstrategy for CD with microservices\n\ntest-by-test setup using APIs\n\nunit tests, creating\n\nwith canned responses\n\nHTTPS imposters\n\nsaving responses in configuration files\n\nwriting tests\n\nthen function\n\nTLS (transport layer security)\n\nTownCrierGateway function\n\nTransfer-Encoding header\n\ntransport layer security (TLS)\n\ntransport protocol\n\nU\n\nnd\n\nunit testing pattern, 2\n\nnd\n\nupdateInventory function, 2\n\nupstream capacity\n\nV",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "validating OAuth authorizations\n\nvalue, boundary of\n\nvirtual imposter\n\nvirtualization tools\n\nvirtualizing\n\n.NET Remoting servers\n\n.NET Remoting services\n\ncreating .NET Remoting clients\n\nindicating where messages end\n\nCORS preflight responses\n\nexploratory testing and service virtualization\n\nOAuth-backed GitHub clients\n\nproduct catalog services\n\nW\n\nnd\n\nrd\n\nwait behavior, 2 , 3\n\nwildcard certificates\n\nwithStub function\n\nX\n\nnd\n\nx-rate-limit-remaining header, 2\n\nXML language\n\npayloads\n\nmanipulating\n\nmatching\n\nselecting values\n\nXMLHttpRequest object\n\nXPath language\n\ncapturing multiple values\n\ngenerating predicates\n\nselectors\n\nnd\n\nxpath parameter, 2\n\nxpath predicate",
      "content_length": 667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Z\n\nzero-character match",
      "content_length": 23,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "List of Figures\n\nChapter 1. Testing microservices\n\nFigure 1.1. A monolithic application handles view, business, and persistence logic for multiple domains.\n\nFigure 1.2. Scaling a monolith means multiple teams have\n\nto work in the same codebase.\n\nFigure 1.3. Services use different databases for different domains.\n\nFigure 1.4. During normal traffic, the number of lanes and speed limit define throughput and velocity.\n\nFigure 1.5. Having fewer upstream lanes increases\n\ncongestion.\n\nFigure 1.6. Centralized QA processes recouple releases\n\ntogether, causing a bottleneck.\n\nFigure 1.7. Independent testing works to avoid release congestion.\n\nFigure 1.8. End-to-end testing introduces several problems of coordination.\n\nFigure 1.9. Testing using service virtualization\n\nFigure 1.10. Service virtualization supports a standard unit\n\ntesting pattern.\n\nFigure 1.11. Using persistent test data from a data store\n\nFigure 1.12. Capturing real traffic for later replay",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Figure 1.13. Configuring virtual services with a simple mountebank imposter\n\nFigure 1.14. Matching a request to a response with mountebank\n\nFigure 1.15. Response generation in mountebank using\n\npredicates and responses in stubs\n\nChapter 2. Taking mountebank for a test drive\n\nFigure 2.1. Your reference architecture for exploring mountebank\n\nFigure 2.2. Breaking down the HTTP request for products\n\nFigure 2.3. The response from the product catalog\n\nFigure 2.4. Adding a query parameter to an HTTP request\n\nFigure 2.5. How mountebank views an HTTP request\n\nFigure 2.6. How mountebank represents an HTTP\n\nresponse\n\nFigure 2.7. Virtualizing the product catalog service to test\n\nthe web facade\n\nFigure 2.8. Using curl to send a request to your virtual product catalog service\n\nFigure 2.9. Combining product data with marketing copy\n\nFigure 2.10. The steps of the service test to verify web façade’s data aggregating\n\nChapter 3. Testing using canned responses\n\nFigure 3.1. How mountebank selects a response",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Figure 3.2. TCP making the connection for HTTP messages\n\nFigure 3.3. Using chunked encodings or content length to calculate where the body ends\n\nFigure 3.4. Inventory checks return volatile results for the\n\nsame request.\n\nFigure 3.5. Each stub cycles through the responses\n\nforever.\n\nFigure 3.6. The basic structure of SSL/TLS\n\nFigure 3.7. Using two keys prevents attackers from\n\nreading messages in transit even when the encryption key is shared.\n\nFigure 3.8. Setting up a test environment with HTTPS\n\nFigure 3.9. How Let’s Encrypt validates the domain\n\nFigure 3.10. Setting up a test environment with HTTPS to validate clients as well as servers\n\nFigure 3.11. The tree structure for the secure inventory\n\nimposter configuration\n\nFigure 3.12. The tree structure for multiple services\n\nChapter 4. Using predicates to send different responses\n\nFigure 4.1. The Abagnale service adapts its response to the\n\nquestions you ask it.\n\nFigure 4.2. Mountebank matches the request against each stub’s predicates.\n\nFigure 4.3. Mountebank matches the request against each\n\nstub’s predicates.",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Figure 4.4. How simple predicates match against the full request field\n\nFigure 4.5. How a regular expression matches against a string value\n\nFigure 4.6. Emulating the contains predicate with a\n\nregular expression\n\nFigure 4.7. Emulating the startsWith predicate with a\n\nregular expression\n\nFigure 4.8. Emulating the endsWith predicate with a regular expression\n\nFigure 4.9. Emulating the startsWith, contains, and endsWith predicates with one regular expression\n\nFigure 4.10. Emulating the equals predicate with a regular\n\nexpression\n\nFigure 4.11. Breaking down a JSONPath query\n\nFigure 4.12. Breaking down an XPath query\n\nChapter 5. Adding record/replay behavior\n\nFigure 5.1. An imposter acting as a proxy\n\nFigure 5.2. Using a proxy to query the downstream\n\ninventory\n\nFigure 5.3. By default, the proxy returns the first result as\n\nthe response to all subsequent calls.\n\nFigure 5.4. The proxy saves the downstream response in a new stub.\n\nFigure 5.5. In proxyOnce mode, mountebank creates new\n\nstubs before the stub with the proxy.",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "Figure 5.6. Volatile responses for the same request\n\nFigure 5.7. In proxyAlways mode, new stubs are created\n\nafter the stub with the proxy\n\nFigure 5.8. Replaying involves removing the proxy\n\nFigure 5.9. You can configure both the proxy request and\n\nthe generated response.\n\nFigure 5.10. Configuring the proxy to pass a client certificate\n\nFigure 5.11. Proxying compressed responses\n\nFigure 5.12. Mixing canned responses with a fallback proxy\n\nChapter 6. Programming mountebank\n\nFigure 6.1. Service collaboration used to protect my\n\nguitars\n\nFigure 6.2. A test scenario requiring advanced predicate logic\n\nFigure 6.3. Two reports need to be spanned to detect dangerous humidity.\n\nFigure 6.4. What happens with traditional blocking I/O\n\nFigure 6.5. Nonblocking I/O doesn’t block the process.\n\nFigure 6.6. Registering a GitHub application\n\nFigure 6.7. Understanding the GitHub OAuth flow\n\nFigure 6.8. Viewing the client secret to communicate to\n\nGitHub",
      "content_length": 949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Figure 6.9. The three stubs needed to virtualize this GitHub workflow\n\nFigure 6.10. Using mountebank to spoof your identity during a network attack\n\nChapter 7. Adding behaviors\n\nFigure 7.1. Behaviors can transform a response from a\n\nstub before it goes out via the imposter.\n\nFigure 7.2. Decoration allows you to postprocess the response.\n\nFigure 7.3. Behaviors applied to proxies don’t transfer to the saved responses.\n\nFigure 7.4. Manufacturing a rate limit exception on a\n\nrecorded response\n\nFigure 7.5. The shellTransform behavior allows you to combine multiple transformations piped through the shell.\n\nFigure 7.6. Repeating a response multiple times\n\nFigure 7.7. A CORS preflight request to establish trust\n\nFigure 7.8. Looking up a value from a CSV file\n\nChapter 8. Protocols\n\nFigure 8.1. The flow of a mountebank-generated HTTP response\n\nFigure 8.2. A client application talking to a server\n\napplication over the internet\n\nFigure 8.3. Transforming an HTTP request to route across the network\n\nFigure 8.4. Virtualizing a custom TCP protocol",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "Figure 8.5. Using Content-Length to wrap multiple packets into one HTTP request\n\nFigure 8.6. Mismatched expectations around when the request ends\n\nChapter 9. Mountebank and continuous delivery\n\nFigure 9.1. A deployment pipeline defines the path from\n\ncommit to production.\n\nFigure 9.2. Simplified test pyramid for microservices\n\nFigure 9.3. The basic structure for unit and service tests\n\nFigure 9.4. The basic structure for contract and\n\nexploratory tests\n\nFigure 9.5. Mapping your test pyramid to a deployment pipeline\n\nFigure 9.6. An example set of microservices\n\nFigure 9.7. Contract tests validate the assumptions made in service tests.\n\nChapter 10. Performance testing with mountebank\n\nFigure 10.1. When runtime dependencies are unstable, you can’t verify the performance of your application.\n\nFigure 10.2. At a certain scale, the virtualization tool itself becomes the problem.\n\nFigure 10.3. Adding the adoption service to your pet store\n\nmicroservices\n\nFigure 10.4. A proxyAlways proxy allows capturing complex test data.\n\nFigure 10.5. Using a proxy to capture test data",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Figure 10.6. Gatling saves error rates and response time data for each step of the scenario.\n\nFigure 10.7. Load balancing using multiple mountebank instances\n\nFigure 10.8. Responses for the same request under load\n\nbalancing yield unexpected results.",
      "content_length": 250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "List of Tables\n\nChapter 4. Using predicates to send different responses\n\nTable 4.1. Regular expression metacharacters\n\nTable 4.2. All predicates that mountebank supports\n\nChapter 7. Adding behaviors\n\nTable 7.1. All behaviors that mountebank supports",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "List of Listings\n\nChapter 2. Taking mountebank for a test drive\n\nListing 2.1. Creating an imposter on the command line\n\nListing 2.2. The HTTP response from the curl command\n\nListing 2.3. An imposter with predicates\n\nListing 2.4. Combining product data with marketing\n\ncontent\n\nListing 2.5. Using a fluent interface to build imposters in code\n\nListing 2.6. Creating the product imposter in code\n\nListing 2.7. Creating the content imposter\n\nListing 2.8. Verifying the web façade\n\nListing 2.9. Adding the ability to remove imposters\n\nChapter 3. Testing using canned responses\n\nListing 3.1. Hello world! in an HTTP response\n\nListing 3.2. The HTTP response structure in JSON\n\nListing 3.3. The imposter configuration to respond with\n\nHello, world!\n\nListing 3.4. The default response in mountebank\n\nListing 3.5. Changing the default response\n\nListing 3.6. A response using the new defaults",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Listing 3.7. Returning a list of responses for the same stub\n\nListing 3.8. Creating an HTTPS imposter\n\nListing 3.9. Adding mutual authentication to an imposter\n\nListing 3.10. Storing the inventory service in a configuration file\n\nListing 3.11. The root configuration file, referencing other\n\nimposters\n\nListing 3.12. The updated version of the product catalog\n\nimposter configuration\n\nListing 3.13. The updated version of the marketing content imposter configuration\n\nListing 3.14. Using curl to send the JSON to mountebank\n\nChapter 4. Using predicates to send different responses\n\nListing 4.1. Creating an Abagnale imposter\n\nListing 4.2. Using the matches predicate to do the job of a set of startsWith, contains, and endsWith predicates\n\nListing 4.3. Using the matches predicate to match any\n\nidentity resource\n\nListing 4.4. Adding a predicate for a query parameter\n\nListing 4.5. Using predicate arrays\n\nListing 4.6. Combining multiple predicates using and, or,\n\nand not\n\nListing 4.7. Using a case-sensitive predicate\n\nListing 4.8. Using a direct JSON predicate on an HTTP\n\nbody",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Listing 4.9. Using JSONPath to match only the last element of an array\n\nListing 4.10. Using XPath to prevent Abagnale from claiming he taught in Utah\n\nListing 4.11. Using XPath to assert that the name attribute\n\nexists and the name tag doesn’t\n\nChapter 5. Adding record/replay behavior\n\nListing 5.1. Imposter configuration for a basic proxy\n\nListing 5.2. Saved proxy responses change imposter state\n\nListing 5.3. Imposter response that saves a different\n\nresponse for each path\n\nListing 5.4. Saved proxy responses with predicates\n\nListing 5.5. Generating case-sensitive predicates\n\nListing 5.6. Specifying a jsonpath predicateGenerators\n\nListing 5.7. Specifying an xpath predicateGenerators\n\nListing 5.8. Creating a proxyAlways proxy response\n\nListing 5.9. The imposter state after a few calls to a\n\nproxyAlways response\n\nListing 5.10. A proxy configured to pass a client certificate\n\nListing 5.11. Injecting a header into the request to prevent response compression\n\nListing 5.12. Using a partial proxy\n\nListing 5.13. Using a proxy to bridge HTTPS to HTTP\n\nChapter 6. Programming mountebank",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Listing 6.1. A weather CSV service payload\n\nListing 6.2. The structure of an inject predicate\n\nListing 6.3. A JavaScript function to parse CSV data—\n\ncsvToObjects\n\nListing 6.4. Looking for three consecutive days out of\n\nrange\n\nListing 6.5. A predicate to test for excess humidity levels\n\nListing 6.6. The basic structure of response injection\n\nListing 6.7. A response injection function to virtualize the\n\nroadie service humidity checks\n\nListing 6.8. Remembering state between responses\n\nListing 6.9. Using traditional I/O to sort lines in a file\n\nListing 6.10. File sort using nonblocking I/O\n\nListing 6.11. Stub using response injection to make OAuth callback\n\nListing 6.12. Injection function to make OAuth callback\n\nListing 6.13. The stub to get an access token\n\nListing 6.14. Stub to check if you’ve starred the mountebank repo\n\nChapter 7. Adding behaviors\n\nListing 7.1. Adding behaviors to a stub definition\n\nListing 7.2. Using an inject response to send a dynamic timestamp",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "Listing 7.3. Combining an is response with a decorate behavior\n\nListing 7.4. Adding a decorate behavior to recorded responses\n\nListing 7.5. Decorator function to accelerate a rate limit\n\nexception\n\nListing 7.6. Imposter configuration for shellTransform\n\nListing 7.7. Ruby script to transform the response to\n\ntrigger a rate limit error\n\nListing 7.8. Ruby script to add a timestamp to the\n\nresponse JSON\n\nListing 7.9. Using a wait behavior to add latency\n\nListing 7.10. Using a repeat behavior to return an error after a small number of successes\n\nListing 7.11. Specifying a token in the response to replace with a value from the request\n\nListing 7.12. Using a copy behavior to insert the ID from\n\nthe URL into the response body\n\nListing 7.13. Using a grouped match to copy a portion of\n\nthe request path\n\nListing 7.14. Using an XPath selector to copy a value from the request to the response\n\nListing 7.15. Virtualizing a CORS preflight request\n\nListing 7.16. Centralizing error conditions in a CSV file\n\nListing 7.17. Using a lookup behavior to retrieve external test data",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Listing 7.18. Using tokens to create a single response for all error conditions\n\nChapter 8. Protocols\n\nListing 8.1. Virtualizing a TCP updateInventory call\n\nListing 8.2. Using a TCP record/replay proxy\n\nListing 8.3. A TCP proxy with predicateGenerators\n\nListing 8.4. Using XPath predicateGenerators with a TCP\n\nproxy\n\nListing 8.5. Setting up a binary response from an imposter\n\nListing 8.6. Using a binary contains predicate\n\nListing 8.7. The Crier class definition\n\nListing 8.8. A gateway to a remote Crier\n\nListing 8.9. Basic test fixture using MbDotNet\n\nListing 8.10. Serializing a stub response for .NET Remoting\n\nListing 8.11. Creating a TCP proxy to a .NET Remoting server\n\nListing 8.12. Adding an endOfRequestResolver\n\nListing 8.13. The function to determine if you have seen the entire request yet\n\nChapter 9. Mountebank and continuous delivery\n\nListing 9.1. Web façade initialization code\n\nListing 9.2. Web façade code to aggregate product and content data",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Listing 9.3. The shell of the productCatalog module\n\nListing 9.4. The code that retrieves and aggregates\n\ndownstream services\n\nListing 9.5. The test setup, using dependency injection and stubs\n\nListing 9.6. The unit test assertion\n\nListing 9.7. A service test that validates web façade aggregation\n\nListing 9.8. The imposter creation functions\n\nListing 9.9. A contract test validating the placing and format of fields from a real dependency\n\nChapter 10. Performance testing with mountebank\n\nListing 10.1. Basic proxy response to capture test data\n\nListing 10.2. Capturing latency from the downstream\n\nsystem\n\nListing 10.3. Adding random latency swings with\n\nJavaScript injection\n\nListing 10.4. A Gatling script for your test scenario\n\nListing 10.5. Testing your performance script",
      "content_length": 780,
      "extraction_method": "Unstructured"
    }
  ]
}