{
  "metadata": {
    "title": "Game Programming Gems 8",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 557,
    "conversion_date": "2025-11-23T11:13:19.569247",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Game Programming Gems 8.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-13)",
      "start_page": 1,
      "end_page": 13,
      "detection_method": "topic_boundary",
      "content": " \n \nCopyright \n \nGame Programming Gems 8 \nEdited by Adam Lake \n \nPublisher and General Manager, \nCourse Technology PTR: \nStacy L. Hiquet \n \nAssociate Director of Marketing: \nSarah Panella \n \nManager of Editorial Services: \nHeather Talbot \n \nMarketing Manager: \nJordan Castellani \n \nSenior Acquisitions Editor: \nEmi Smith \n \nProject and Copy Editor: \nCathleen D. Small \n \nInterior Layout: \nShawn Morningstar \n \nCover Designer: \nMike Tanamachi \n \nCD-ROM Producer: \nBrandon Penticuff \n \nIndexer: \nKatherine Stimson \n \nProofreader: \nHeather Urschel \n© 2011 Course Technology, a part of Cengage Learning. \nALL RIGHTS RESERVED. No part of this work covered by the copyright herein may be \nreproduced, transmitted, stored, or used in any form or by any means graphic, electronic, \nor mechanical, including but not limited to photocopying, recording, scanning, digitizing, \ntaping, Web distribution, information networks, or information storage and retrieval \nsystems, except as permitted under Section 107 or 108 of the 1976 United States Copyright \nAct, without the prior written permission of the publisher. \nFor product information and technology assistance, contact us at Cengage Learning \nCustomer & Sales Support, 1-800-354-9706 \nFor permission to use material from this text or product, submit all requests online at \ncengage.com/permissions Further permissions questions can be emailed to \n\n\n \n \nFor product information and technology assistance, contact us at Cengage Learning \nCustomer & Sales Support, 1-800-354-9706 \npermissionrequest@cengage.com \n \nAll trademarks are the property of their respective owners. \nCover image used courtesy of Valve Corporation. \nAll other images © Cengage Learning unless otherwise noted. \nLibrary of Congress Control Number: 2010920327 \nISBN-10: 1-58450-702-0 \neISBN-10: 1-43545-771-4 \nCourse Technology, a part of Cengage Learning \n20 Channel Center Street \nBoston, MA 02210 \nUSA \nCengage Learning is a leading provider of customized learning solutions with office locations \naround the globe, including Singapore, the U nited Kingdom, Australia, Mexico, Brazil, and \nJapan. Locate your local office at: international.cengage.com/region \nCengage Learning products are represented in Canada by Nelson Education, Ltd. \nFor your lifelong learning solutions, visit courseptr.com \nVisit our corporate website at cengage.com \nPrinted in the United States of America \n1 2 3 4 5 6 7 12 1 1 10 \n \nPreface \nWelcome to the eighth edition of the Game Programming Gems series, started by Mark \nDeLoura in 2000. The first edition was inspired by Andrew Glassner‘s popular Graphics \nGems series. Since then, other Gems series have started, including AI Gems and a new \nseries focused on the capabilities of programmable graphics, the ShaderX series. These \ntomes serve as an opportunity to share our experience and best practices with the rest of \nthe industry. \nMany readers think of the Game Programming Gems series as a collection of articles with \nsections that target specialists. For me, I‘ve read through them as a way to get exposure to \nthe diverse subsystems used to create games and stay abreast of the latest techniques. For \nexample, I may not be a specialist in networking, but reading this section will often \nenlighten and stimulate connections that I may not have made between areas in which I \nhave expertise and ones in which I do not. \n\n\n \n \nOne statement I‘ve heard recently regarding our industry is the idea that we now have all \nthe horsepower we need to create games, so innovations by hardware companies are not \nneeded. I believe this argument is flawed in many ways. First, there are continued \nadvancements in graphical realism in academia, in R&D labs, and in the film industry that \nhave yet to be incorporated into our real-time pipelines. As developers adopt these new \nfeatures, computational requirements of software will continue to increase. Second, and the \nmore important issue, is that this concept of play isn‘t entirely correct—the very notion of \nwhat gaming serves from an anthropological perspective. Play is fundamental, not just to \nthe human condition, but to the sentient condition. We invent interactive experiences on \nany platform, be it a deck of cards, a set of cardboard cutouts, or a next-gen PC platform \nwith multi-terabyte data and multi-threaded, multi-gigahertz, multi-processor \nenvironments. It‘s as natural as the pursuit of food. This play inspires real-world \napplications and pushes the next generation of platform requirements. It enables \naffordability of ever-increased computational horsepower in our computing platforms. \nThe extension of gaming into other arenas, mobile and netbook platforms, serves only to \nprove the point. While the same ideas and themes may be used in these environments, the \nexperience available to the player is different if the designer is to leverage the full \ncapabilities and differentiating features of the platform. \nThere is an often-chanted ―ever increasing cost of game development‖ quote for console \nand PC platforms. In the same breath, it‘s alluded that this spiral of cost cannot continue. I \nbelieve these issues are of short-term concern. If there is a community willing to play, our \neconomies will figure out a way to satisfy those needs. This will open up new opportunities \nfor venture capital and middleware to reduce those platform complexities and cross-industry \ndevelopment costs, fueling the next generation of interactive experiences. I do believe the \nprocess has changed and will continue to evolve, but game development will continue to \nthrive. Will there be 15 first-person military simulations on a single platform? Perhaps not, \nbut will there continue to be compelling multiplayer and single-player experiences? I believe \nso. The ingenuity of the game developer, when brought to the task of leveraging new \nincarnations of silicon, will continue to create enriching interactive experiences for ever-\nincreasing audiences. \nFinally, I‘d like to take a moment to address another issue often mentioned in the press. In \nNovember 2009, the Wall Street Journal ran an article by Jonathan V. Last from the Weekly \nStandard discussing the social implications of gaming. The majority of his article, \n―Videogames—Not Only for the Lonely,‖ was making this observation in the context of a \nholiday gathering of family members of many generations sharing experiences with their \nNintendo Wii. Near the end of the article, he refers to the fact that ―the shift to videogames \nmight be lamenting if it meant that people who would otherwise be playing mini-golf or \nMonopoly were sealing themselves off and playing Halo 3 death matches across the \nInternet.‖ Much to the contrary, I have personally spent many quality multiplayer hours \ninteracting socially with longtime friends when playing multiplayer games. A few days ago, I \nwas having a conversation with an acquaintance who was thrilled that she could maintain \nher relationship with her brother on the East Coast by playing World of Warcraft with him. \nUltimately, whether we are discussing our individual game experiences with others or \ninteracting directly while playing, games do what they have always done across generations \nand platforms—they bring us together with shared experiences, whether it be cardboard \ncutouts, a deck of cards, or multiplayer capture the flag. Despite the overall informed \nmessage of the article, the writer encouraged a myth I see repeated in the mainstream \npress by those out of touch with the multiplayer, socially interactive game experiences that \nare common today, including Halo 3. \nOverview of Content \nThe graphics section in this edition covers several topics of recent interest, leveraging new \nfeatures of graphics APIs such as Compute Shader, tessellation using DirectX 11, and two \ngems on the implementation details of Screen Space Ambient Occlusion (SSAO). In the \n\n\n \n \nphysics and animation section, we have selected a number of gems that advance beyond \nthe basics of the topics such as IK solvers or fluid simulation in general. Instead, these \ngems go deeper with improvements to existing published techniques based on real-world \nexperience with the current state of the art—for example, a simple, fast, and accurate IK \nsolver, leveraging swarm systems for animation, and modeling air and fluid resistance. \nArtificial intelligence, AI, is one of the hottest areas in game development these days. Game \nplayers want worlds that don‘t just look real, but that also feel and act real. The acting part \nis the responsibility of the AI programmer. Gems in the AI section are diverse, covering \nareas such as decision making, detailed character simulation, and player modeling to solve \nthe problem of gold farm detection. The innovations discussed are sure to influence future \ngems. \nIn the general programming section, we have a number of tools to help with the \ndevelopment, performance, and testing of our game engines. We include gems that deal \nwith multi-threading using Intel‘s Thread Building Blocks, an open-source multithreading \nlibrary, memory allocation and profiling, as well as a useful code coverage system used by \nthe developers at Crytek. The gems in the networking and multiplayer section cover \narchitecture, security, scalability, and the leveraging of social networking applications to \ncreate multiplayer experiences. \nThe audio section had fewer submissions than in past years. Why is this? Is the area of \naudio lacking in innovation? Has it matured to the point where developers are buying off-\nthe-shelf components? Regardless, we‘ve assembled a collection of gems for audio that we \nthink will be of interest. In one of the articles in the audio section, we discuss a relatively \nnew idea—the notion of real-time calculation of the audio signal based on the actual physics \ninstead of using the traditional technique of playing a pre-recorded processed sound. As \ngames become more interactive and physics driven, there will be a corresponding demand \nfor more realistic sound environments generated by such techniques enabled with the \nincreasing computational horsepower Moore‘s Law continues to deliver to game developers. \nI‘m excited to introduce a new section in this edition of Game Programming Gems 8 that I‘m \ncalling ―General Purpose Computing on GPUs.‖ This is a new area for the Gems series, and \nwe wanted to have a real-world case study of a game developer using the GPU for non-\ngraphics tasks. We‘ve collected three gems for this section. The first is about OpenCL, a \nnew open standard for programming heterogeneous platforms of today, and we also have \ntwo gems that leverage PhysX for collision detection and fluid simulation. The PhysX \ncomponents were used in Batman: Arkham Asylum by Rocksteady Studios Ltd. As the \ncomputing capabilities of the platform evolve, I expect game developers will face the \ndecision of what to compute, where to compute, and how to manage the data being \noperated upon. These articles serve as case studies in what others have done in their \ngames. I expect this to be an exciting area of future development. \nWhile we all have our areas of specialty, I think it‘s fair to say game developers are a \nhungry bunch, with a common desire to learn, develop, and challenge ourselves and our \nabilities. These gems are meant to insprire, enlighten, and evolve the industry. As always, \nwe look forward to the contributions and feedback developers have when putting these \ngems into practice. \nAdam Lake \nAdam_t_lake@yahoo.com \nAbout the Cover Image \n© Valve Corporation \n\n\n \n \n \nThe cover of Game Programming Gems 8 features the Engineer from Valve‘s Team Fortress \n2. With their follow-up to the original class-based multiplayer shooter Team Fortress, Valve \nchose to depart from the typical photorealistic military themes of the genre. Instead, they \nemployed an ―illustrative‖ non-photorealistic rendering style, reminiscent of American \ncommercial illustrators of the 1920s. This was motivated by the need for players to be able \nto quickly visually identify each other‘s team, class, and weapon choices in the game. The \nnovel art style and rendering techniques of Team Fortress 2 allowed Valve‘s designers to \nvisually separate the character classes from each other and from the game‘s environments \nthrough the use of strong silhouettes and strategic distribution of color value. \nCD-ROM Downloads \nIf you purchased an ebook version of this book, and the book had a companion CD-ROM, we \nwill mail you a copy of the disc. Please send ptrsupplements@cengage.com the title of the \nbook, the ISBN, your name, address, and phone number. Thank you. \n \nAcknowledgments \nI‘d like to take a moment to acknowledge the section editors that I worked with to create \nthis tome. They are the best and brightest in the industry. The quality of submissions and \ncontent in this book is a testament to this fact. They worked incredibly hard to bring this \nbook together, and I thank them for their time and expertise. Also, I appreciate the time \nand patience that Emi Smith and Cathleen Small at Cengage Learning have put into this \nfirst-time book editor. They were essential in taking care of all the details necessary for \npublication. Finally, I‘d like to acknowledge the artists at Valve who provided the cover \nimage for this edition of Game Programming Gems. \nI have been blessed to have had exposure to numerous inspirational individuals— friends \nwho refused to accept norms, parents who satiated my educational desires, teachers willing \nto spend a few extra minutes on a random tangent, instructors to teach not just what we \nknow about the world, but also to make me aware of the things we do not. Most \nimportantly, I want to acknowledge my wife, Stacey Lake, who remained supportive while I \ntoiled away in the evenings and weekends for the better part of a year on this book. \nI dedicate these efforts to my mother, Amanda Lake. I thank her for teaching me that \neducation is an enjoyable lifelong endeavor. \n \n\n\n \n \nContributors \nFull bios for those contributors who submitted one can be found at www.courseptr.com/ \ndownloads. Contributors to this book include: \nDr. Doug Binks, D.Phil. \nUdeepta Bordoloi \nIgor Borovikov \nCyril Brom \nEric Brown \nPhil Carlisle \nMichael Dailly \nPeter Dalton \nKevin Dill \nJean-Francois Dube \nDominic Filion \nMarco Fratarcangeli \nNico Galoppo \nBenedict R. Gaster \nGero Gerber \nRobert Jay Gould \nNeil Gower \nJoshua Grass, Ph.D. \nHunter Hale \nMark Harris \nThomas Hartley \nKevin He \nClaus Höfele \nAllen Hux \nPeter Iliev \n\n\n \n \nMatthew Jack \nAleksey Kadukin \nNikhil S. Ketkar \nHyunwoo Ki \nAdam Lake \nMichael Lewin \nChris Lomont, Ph.D. \nRicky Lung \nKhaled Mamou \nDave Mark \nQuasim Mehdi \nKrzysztof Mieloszyk \nJason Mitchell \nBen Nicholson \nIan Ni-Lewis \nMat Noguchi \nBorut Pfeifer \nBrian Pickrell \nTomas Poch \nSteve Rabin \nMike Ramsey \nB. Charles Rasco, Ph.D. \nJoão Lucas G. Raza \nAurelio Reis \nZhimin Ren \nMarc Romankewicz \nDario Sancho \nRahul Sathe \n\n\n \n \nSimon Schirm \nBrian Schmidt \nOndřej Šerý \nPhilip Taylor \nRichard Tonge \nSteven Tovey \nGabriel Ware \nBen Wyatt \nG. Michael Youngblood \nJason Zink \nRobert Zubek \n \nSection 1: Graphics \nIntroduction \nFast Font Rendering with Instancing \nPrinciples and Practice of Screen Space Ambient Occlusion \nMulti-Resolution Deferred Shading \nView Frustum Culling of Catmull-Clark Patches in DirectX 11 \nAmbient Occlusion Using DirectX Compute Shader \nEye-View Pixel Anti-Aliasing for Irregular Shadow Mapping \nOverlapped Execution on Programmable Graphics Hardware \nTechniques for Effective Vertex and Fragment Shading on the SPUs \nIntroduction \nJason Mitchell, Valve \njason@pixelmaven.com \nIn this edition of the Game Programming Gems series, we explore a wide range of \nimportant real-time graphics topics, from lynchpin systems such as font rendering to \ncutting-edge hardware architectures, such as Larrabee, PlayStation 3, and the DirectX 11 \ncompute shader. Developers in the trenches at top industry studios such as Blizzard, id, \n\n\n \n \nBizarre Creations, Nexon, and Intel‘s Advanced Visual Computing group share their insights \non optimally exploiting graphics hardware to create high-quality visuals for games. \nTo kick off this section, Aurelio Reis of id Software compares several methods for \naccelerating font rendering by exploiting GPU instancing, settling on a constant-buffer-\nbased method that achieves the best performance. \nWe then move on to two chapters discussing the popular image-space techniques of Screen \nSpace Ambient Occlusion (SSAO) and deferred shading. Dominic Filion of Blizzard \nEntertainment discusses the SSAO algorithms used in StarCraft II, including novel controls \nthat allowed Blizzard‘s artists to tune the look of the effect to suit their vision. Hyunwoo Ki \nof Nexon then describes a multi-resolution acceleration method for deferred shading that \ncomputes low-frequency lighting information at a lower spatial frequency and uses a novel \nmethod for handling high-frequency edge cases. \nFor the remainder of the section, we concentrate on techniques that take advantage of the \nvery latest graphics hardware, from DirectX 11‘s tessellator and compute shader to \nLarrabee and the PlayStation 3. Rahul Sathe of Intel presents a method for culling of Bezier \npatches in the context of the new DirectX 11 pipeline. Jason Zink then describes the new \nDirectX 11 compute shader architecture, using Screen Space Ambient Occlusion as a case \nstudy to illustrate the novel aspects of this new hardware architecture. In a pair of articles \nfrom Intel, Nico Galoppo and Allen Hux describe a method for integrating anti-aliasing into \nthe irregular shadow mapping algorithm as well as a software task system that allows highly \nprogrammable systems such as Larrabee to achieve maximum throughput on this type of \ntechnique. We conclude the section with Steven Tovey‘s look at the SPU units on the \nPlayStation 3 and techniques for achieving maximum performance in the vehicle damage \nand light pre-pass rendering systems in the racing game Blur from Bizarre Creations. \n \n1.1. Fast Font Rendering with Instancing \nAurelio Reis, id Software \nAurelioReis@gmail.com \nFont rendering is an essential component of almost all interactive applications, and while \ntechniques exist to allow for fully scalable vector-based font rendering using modern GPUs, \nthe so-called ―bitmap font‖ is still the most versatile, efficient, and easy-to-implement \nsolution. When implemented on typical graphics APIs, however, this technique uses run-\ntime updated vertex buffers to store per-glyph geometry, resulting in inefficient rendering \nperformance by potentially stalling the graphics pipeline. By leveraging efficient particle \nsystem rendering techniques that were developed previously, it is possible to render \nthousands of glyphs in a single batch without ever touching the vertex buffer. \nIn this article, I propose a simple and efficient method to render fonts utilizing modern \ngraphics hardware when compared to other similar methods. This technique is also useful in \nthat it can be generalized for use in rendering other 2D elements, such as sprites and \ngraphical user interface (GUI) elements. \nText-Rendering Basics \nThe most common font format is the vector-based TrueType format. This format represents \nfont glyphs (in other words, alphabetic characters and other symbols) as vector data, \nspecifically, quadratic Bezier curves and line segments. As a result, TrueType fonts are \ncompact, easy to author, and scale well with different display resolutions. The downside of a \nvector font, however, is that it is not straightforward to directly render this type of data on \n\n\n \n \ngraphics hardware. There are, however, a few different ways to map the vector \nrepresentation to a form that graphics hardware can render. \nOne way is to generate geometry directly from the vector curves, as shown in Figure 1.1.1. \nHowever, while modern GPUs are quite efficient at rendering large numbers of triangles, the \nnumber of polygons generated from converting a large number of complex vector curves to \na triangle mesh could number in the tens of thousands. This increase in triangle throughput \ncan greatly decrease application performance. Some optimizations to this way of rendering \nfonts have been introduced, such as the technique described by Loop and Blinn in which the \npolygonal mesh consists merely of the curve control points while the curve pixels are \ngenerated using a simple and efficient pixel shader [Loop05]. While this is a great \nimprovement over the naive triangulation approach, the number of polygons generated in \nthis approach is still prohibitively high on older graphics hardware (and that of the current \nconsole generation—the target of this article). \nFigure 1.1.1. Vector curves converted into polygonal geometry. \n \n \nBecause of these limitations, the most common approach relies on rasterizing vector \ngraphics into a bitmap and displaying each glyph as a rectangle composed of two triangles \n(from here on referred to as a quad), as shown in Figure 1.1.2. A font texture page is \ngenerated with an additional UV offset table that maps glyphs to a location in that texture \nvery similar to how a texture atlas is used [NVIDIA04]. The most obvious drawback is the \nresolution dependence caused by the font page being rasterized at a predefined resolution, \nwhich leads to distortion when rendering a font at a non-native resolution. Additional \ntechniques exist to supplement this approach with higher quality results while mitigating the \nresolution dependence that leads to blurry and aliased textures, such as the approach \ndescribed by [Green07]. Overall, the benefits of the raster approach outweigh the \ndrawbacks, because rendering bitmap fonts is incredibly easy and efficient. \nFigure 1.1.2. A font page and a glyph rendered on a quad. \n \n \nTo draw glyphs for a bitmap font, the program must bind the texture page matching the \nintended glyph set and draw a quad for each glyph, taking into account spacing for kerning \nor other character-related offsets. While this technique yields very good performance, it can \nstill be inefficient, as the buffers containing the geometry for each batch of glyphs must be \ncontinually updated. Constantly touching these buffers is a sure way to cause GPU stalls, \n\n\n \n \nresulting in decreased performance. For text- or GUI-heavy games, this can lead to an \nunacceptable overall performance hit. \n \nImproving Performance \nOne way to draw the glyphs for the GUI is to create a GUI model that maintains buffers on \nthe graphics card for drawing a predefined maximum number of indexed triangles as quads. \nWhenever a new glyph is to be drawn, its quad is inserted into a list, and the vertex buffer \nfor the model is eventually updated with the needed geometry at a convenient point in the \ngraphics pipeline. When the time comes to render the GUI model, assuming the same \ntexture page is used, only a single draw call is required. As previously mentioned, this \nbuffer must be updated each frame and for each draw batch that must be drawn. Ideally, as \nfew draw batches as possible are needed, as the font texture page should contain all the \nindividual glyphs that would need to be rendered, but on occasion (such as for high-\nresolution fonts or Asian fonts with many glyphs), it‘s not possible to fit them all on one \npage. In the situation where a font glyph must be rendered from a different page, the batch \nis broken and must be presented immediately so that a new one can be started with the \nnew texture. This holds true for any unique rendering states that a glyph may hold, such as \nblending modes or custom shaders. \nLock-Discard \nThe slowest part of the process is when the per-glyph geometry must be uploaded to the \ngraphics card. Placing the buffer memory as close to AGP memory as possible (using API \nhints) helps, but locking and unlocking vertex buffers can still be quite expensive. To \nalleviate the expense, it is possible to use a buffer that is marked to ―discard‖ its existing \nbuffer if the GPU is currently busy with it. By telling the API to discard the existing buffer, a \nnew one is created, which can be written to immediately. Eventually, the old buffer is \npurged by the API under the covers. This use of lock-discard prevents the CPU from waiting \non the GPU to finish consuming the buffer (for example, in the case where it was being \nrendered at the same time). You can specify this with the D3DLOCK_DISCARD flag in \nDirect3D or by passing a NULL pointer to glBufferDataARB and then calling \nglMapBufferARB(). Be aware that although this is quite an improvement, it is still not \nan ideal solution, as the entire buffer must be discarded. Essentially, this makes initiating a \nsmall update to the buffer impossible. \nVertex Compression \nAnother step in improving performance is reducing the amount of memory that needs to be \nsent to the video card. The vertex structure for sending a quad looks something like this \nand takes 28 bytes per vertex (and 112 bytes for each quad): \nstruct GPU_QUAD_VERTEX_POS_TC_COLOR \n{ \n      D3DXVECTOR4 Position; \n      D3DXVECTOR2 Texcoord; \n      D3DCOLOR Color; \n}; \n \nSince the bandwidth across the AGP bus to the video card is not infinite, it is important to \nbe aware of how much memory is being pushed through it. One way to reduce the memory \ncosts is to use an additional vertex stream to update only that information that has changed \non a per-frame basis. Unfortunately, the three essential quad attributes (position, texture \n\n\n \n \ndimensions, and color) could be in a state of constant flux, so there is little frame-to-frame \ncoherency we can exploit. \nThere is one very easy way to reduce at least some of the data that must be sent to the \nvideo card, however. Traditionally, each vertex represents a corner of a quad. This is not \nideal, because this data is relatively static. That is, the size and position of a quad changes, \nbut not the fact that it is a quad. Hicks describes a shader technique that allows for aligning \na billboarded quad toward the screen by storing a rightFactor and upFactor for each \ncorner of the billboard and projecting those vertices along the camera axes [Hicks03]. This \ntechnique is attractive, as it puts the computation of offsetting the vertices on the GPU and \npotentially limits the need for vertex buffer locks to update the quad positions. \nBy using a separate vertex stream that contains unique data, it is possible to represent the \nwidth and height of the quad corners as a 4D unsigned byte vector. (Technically, you could \ngo as small as a Bool if that was supported on modern hardware.) In the vertex declaration, \nit is possible to map the position information to specific vertex semantics, which can then be \naccessed directly in the vertex shader. The vertex structure would look something like this: \nstruct GPU_QUAD_VERTEX \n{ \n      BYTE OffsetXY[ 4 ]; \n}; \n \nAlthough this may seem like an improvement, it really isn‘t, since the same amount of \nmemory must be used to represent the quad attributes (more so since we‘re supplying a 4-\nbyte offset now). There is an easy way to supply this additional information without \nrequiring the redundancy of all those additional vertices. \n \nInstancing Quad Geometry \nIf you‘re lucky enough to support a Shader Model 3 profile, you have hardware support for \nsome form of geometry instancing. OpenGL 2.0 has support for instancing using pseudo-\ninstancing [GLSL04] and the EXT_draw_instanced [EXT06] extension, which uses the \nglDrawArraysInstancedEXT and glDrawElementsInstancedEXT routines to \nrender up to 1,024 instanced primitives that are referenced via an instance identifier in \nshader code. \nAs of DirectX 9, Direct3D also supports instancing, which can be utilized by creating a \nvertex buffer containing the instance geometry and an additional vertex buffer with the per-\ninstance data. By using instancing, we‘re able to completely eliminate our redundant quad \nvertices (and index buffer) at the cost of an additional but smaller buffer that holds only the \nper-instance data. This buffer is directly hooked up to the vertex shader via input semantics \nand can be easily accessed with almost no additional work to the previous method. While \nthis solution sounds ideal, we have found that instancing actually comes with quite a bit of \nper-batch overhead and also requires quite a bit of instanced data to become a win. As a \nresult, it should be noted that performance does not scale quite so well and in some \nsituations can be as poor as that of the original buffer approach (or worse on certain \nhardware)! This is likely attributed to the fact that the graphics hardware must still point to \nthis data in some way or another, and while space is saved, additional logic is required to \ncompute the proper vertex strides. \n \n\n\n \n \nConstant Array Instancing \nAnother way to achieve similar results with better performance is to perform shader \ninstancing using constant arrays. By creating a constant array for each of the separate quad \nattributes (in other words, position/size, texture coordinate position/size, color), it is \npossible to represent all the necessary information without the need for a heavyweight \nvertex structure. See Figure 1.1.3. \nFigure 1.1.3. A number of glyphs referencing their data from a constant array. \n \nSimilar to indexed vertex blending (a.k.a. matrix palette skinning), an index is assigned for \neach group of four vertices required to render a quad, as shown in Figure 1.1.4. To get the \nvalue for the current vertex, all that is needed is to index into the constant array using this \nvalue. Because the number of constants available is usually below 256 on pre–Shader Model \n4 hardware, this index can be packed directly as an additional element in the vertex offset \nvector (thus requiring no additional storage space). It‘s also possible to use geometry \ninstancing to just pass in the quad ID/index in order to bypass the need for a large buffer of \nfour vertices per quad. However, as mentioned previously, we have found that instancing \ncan be unreliable in practice. \nFigure 1.1.4. A quad referencing an element within the attribute constant array. \n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 14-22)",
      "start_page": 14,
      "end_page": 22,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \nThis technique yields fantastic performance but has the downside of only allowing a certain \nnumber of constants, depending on your shader profile. The vertex structure is incredibly \ncompact, weighing in at a mere 4 bytes (16 bytes per quad) with an additional channel still \navailable for use: \nstruct GPU_QUAD_VERTEX \n{ \n      BYTE OffsetXY_IndexZ[ 4 ]; \n}; \n \nGiven the three quad attributes presented above and with a limit of 256 constants, up to 85 \nquads can be rendered per batch. Despite this limitation, performance can still be quite a bit \nbetter than the other approaches, especially as the number of state changes increases \n(driving up the number of batches and driving down the number of quads per batch). \n \nAdditional Considerations \nI will now describe some small but important facets of font rendering, notably an efficient \nuse of clip-space position and a cheap but effective sorting method. Also, in the sample \ncode for this chapter on the book‘s CD, I have provided source code for a texture atlasing \nsolution that readers may find useful in their font rendering systems. \nSorting \n\n\n \n \nFonts are typically drawn in a back-to-front fashion, relying on the painter‘s algorithm to \nachieve correct occlusion. Although this is suitable for most applications, certain situations \nmay require that quads be layered in a different sort order than that in which they were \ndrawn. This is easily implemented by using the remaining available value in the vertex \nstructure offset/index vector as a z value for the quad, allowing for up to 256 layers. \nClip-Space Positions \nTo save a few instructions and the constant space for the world-view-projection matrix (the \nclip matrix), it‘s possible to specify the position directly in clip-space to forego having to \ntransform the vertices from perspective to orthographic space, as illustrated in Figure 1.1.5. \nClip-space positions range from –1 to 1 in the X and Y directions. To remap an absolute \nscreen-space coordinate to clip space, we can just use the equation [cx = –1 + x * (2 / \nscreen_width)], [cy = 1 – y * (2 / screen_height)], where x and y are the screen-space \ncoordinates up to a max of screen_width and screen_height, respectively. \nFigure 1.1.5. A quad/billboard being expanded. \n \n \n \nTexture Atlasing \nOn the book‘s CD, I have provided code for a simple virtual texture system that uses atlases \nto reduce batches. This system attempts to load an atlased version of a texture if possible \nand otherwise loads a texture directly from disk. There are some switches (documented in \nthe code) that demonstrate how to turn this system on and off to demonstrate how \nimportant it can be toward reducing the number of batches and maintaining a high level of \nperformance. \n \nFuture Work \n\n\n \n \nThe techniques demonstrated in this chapter were tailored to work on current console \ntechnology, which is limited to Shader Model 3. In the future, I would like to extend these \ntechniques to take advantage of new hardware features, such as Geometry Shaders and \nStreamOut, to further increase performance, image fidelity, and ease of use. \n \nDemo \nOn the accompanying disc, you‘ll find a Direct3D sample application that demonstrates each \nof the discussed techniques in a text- and GUI-rich presentation. Two scenes are presented: \nOne displays a cityscape for a typical 2D tile-based game, and the other displays a Strange \nAttractor simulation. In addition, there is an option to go overboard with the text rendering. \nFeel free to play around with the code until you get a feel for the strengths and weaknesses \nof the different approaches. \nThe main shader file (Font.fx) contains the shaders of interest as well as some additional \nfunctionality (such as font anti-aliasing/filtering). Please note that certain aspects (such as \nquad expansion) were made for optimum efficiency and not necessarily readability. In \ngeneral, most of the code was meant to be very accessible, and it will be helpful to \nperiodically cross-reference the files GuiModel.cpp and Font.fx. \n \nConclusion \nIn this gem, I demonstrated a way to render font and GUI elements easily and efficiently by \ntaking advantage of readily available hardware features, such as instancing, multiple stream \nsupport, and constant array indexing. As a takeaway item, you should be able to easily \nincorporate such a system into your technology base or improve an existing system with \nonly minor changes. \n \nReferences \n[EXT06] ―EXT_draw_instanced.‖ 2006. Open GL. n.d. \n<http://www.opengl.org/registry/specs/EXT/draw_instanced.txt>. \n[GLSL04] ―GLSL Pseudo-Instancing.‖ 17 Nov. 2004. NVIDIA. n.d. \n<http://http.download.nvidia.com/developer/SDK/Individual_Samples/DEMOS/OpenGL/src/\nglsl_pseudo_instancing/docs/glsl_pseudo_instancing.pdf>. \n[Green07] Green, Chris. ―Improved Alpha-Tested Magnification for Vector Textures and \nSpecial Effects.‖ Course on Advanced Real-Time Rendering in 3D Graphics and Games. \nSIGGRAPH 2007. San Diego Convention Center, San Diego, CA. 8 August 2007. \n[Hicks03] Hicks, O‘Dell. ―Screen-aligned Particles with Minimal VertexBuffer Locking.‖ \nShaderX2: Shader Programming Tips and Tricks with DirectX 9.0. Ed. Wolfgang F. Engel. \nPlano, TX: Wordware Publishing, Inc., 2004. 107–112. \n[Loop05] Loop, Charles and Jim Blinn. ―Resolution Independent Curve Rendering Using \nProgrammable Graphics Hardware.‖ 2005. Microsoft. n.d. \n<http://research.microsoft.com/en-us/um/people/cloop/loopblinn05.pdf>. \n\n\n \n \n[NVIDIA04] ―Improve Batching Using Texture Atlases.‖ 2004. NVIDIA. n.d. \n<http://http.download.nvidia.com/developer/NVTextureSuite/Atlas_Tools/Texture_Atlas_W\nhitepaper.pdf>. \n \n1.2. Principles and Practice of Screen Space Ambient Occlusion \nDominic Filion, Blizzard Entertainment \ndfilion@blizzard.com \nSimulation of direct lighting in modern video games is a well-understood concept, as \nvirtually all of real-time graphics has standardized on the Lambertian and Blinn models for \nsimulating direct lighting. However, indirect lighting (also referred to as global illumination) \nis still an active area of research with a variety of approaches being explored. Moreover, \nalthough some simulation of indirect lighting is possible in real time, full simulation of all its \neffects in real time is very challenging, even on the latest hardware. \nGlobal illumination is based on simulating the effects of light bouncing around a scene \nmultiple times as light is reflected on light surfaces. Computational methods such as \nradiosity attempt to directly model this physical process by modeling the interactions of \nlights and surfaces in an environment, including the bouncing of light off of surfaces. \nAlthough highly realistic, sophisticated global illumination methods are typically too \ncomputationally intensive to perform in real time, especially for games, and thus to achieve \nthe complex shadowing and bounced lighting effects in games, one has to look for \nsimplifications to achieve a comparable result. \nOne possible simplification is to focus on the visual effects of global illumination instead of \nthe physical process and furthermore to aim at a particular subset of effects that global \nillumination achieves. Ambient occlusion is one such subset. Ambient occlusion simplifies \nthe problem space by assuming all indirect light is equally distributed throughout the scene. \nWith this assumption, the amount of indirect light hitting a point on a surface will be directly \nproportional to how much that point is exposed to the scene around it. A point on a plane \nsurface can receive light from a full 180-degree hemisphere around that point and above \nthe plane. In another example, a point in a room‘s corner, as shown in Figure 1.2.1, could \nreceive a smaller amount of light than a point in the middle of the floor, since a greater \namount of its ―upper hemisphere‖ is occluded by the nearby walls. The resulting effect is a \ncrude approximation of global illumination that enhances depth in the scene by shrouding \ncorners, nooks, and crannies in a scene. Artistically, the effect can be controlled by varying \nthe size of the hemisphere within which other objects are considered to occlude neighboring \npoints; large hemisphere ranges will extend the shadow shroud outward from corners and \nrecesses. \nFigure 1.2.1. Ambient occlusion relies on finding how much of the hemisphere \naround the sampling point is blocked by the environment. \n\n\n \n \n \n \nAlthough the global illumination problem has been vastly simplified through this approach, it \ncan still be prohibitively expensive to compute in real time. Every point on every scene \nsurface needs to cast many rays around it to test whether an occluding object might be \nblocking the light, and an ambient occlusion term is computed based on how many rays \nwere occluded from the total amount of rays emitted from that point. Performing arbitrary \nray intersections with the full scene is also difficult to implement on graphics hardware. We \nneed further simplification. \nScreen Space Ambient Occlusion \nWhat is needed is a way to structure the scene so that we can quickly and easily determine \nwhether a given surface point is occluded by nearby geometry. It turns out that the \nstandard depth buffer, which graphics engines already use to perform hidden surface \nremoval, can be used to approximate local occlusion [Shanmugam07, Mittring07]. By \ndefinition, the depth buffer contains the depth of every visible point in the scene. From \nthese depths, we can reconstruct the 3D positions of the visible surface points. Points that \ncan potentially occlude other points are located close to each other in both screen space and \nworld space, making the search for potential occluders straightforward. We need to align a \nhemisphere around each point‘s upper hemisphere as defined by its normal. We will thus \nneed a normal buffer that will encode the normal of every corresponding point in the depth \nbuffer in screen space. \nRather than doing a full ray intersection, we can simply inspect the depths of neighboring \npoints to establish the likelihood that each is occluding the current point. Any neighbor \nwhose 2D position does not fall within the 2D coverage of the hemisphere could not possibly \nbe an occluder. If it does lie within the hemisphere, then the closer the neighbor point‘s \ndepth is to the target point, the higher the odds it is an occluder. If the neighbor‘s depth is \nbehind the point being tested for occlusion, then no occlusion is assumed to occur. All of \nthese calculations can be performed using the screen space buffer of normals and depths, \nhence the name Screen Space Ambient Occlusion (SSAO). \nAt first glance, this may seem like a gross oversimplification. After all, the depth buffer \ndoesn‘t contain the whole scene, just the visible parts of it, and as such is only a partial \nreconstruction of the scene. For example, a point in the background could be occluded by an \nobject that is hidden behind another object in the foreground, which a depth buffer would \n\n\n \n \ncompletely miss. Thus, there would be pixels in the image that should have some amount of \nocclusion but don‘t due to the incomplete representation we have of the scene‘s geometry. \nFigure 1.2.2. SSAO Samples neighbor points to discover the likelihood of occlusion. \nLighter arrows are behind the center point and are considered occluded samples. \n \n \nIt turns out that these kinds of artifacts are not especially objectionable in practice. The eye \nfocuses first on cues from objects within the scene, and missing cues from objects hidden \nbehind one another are not as disturbing. Furthermore, ambient occlusion is a low-\nfrequency phenomenon; what matters more is the general effect rather than specific \ndetailed cues, and taking shortcuts to achieve a similar yet incorrect effect is a fine tradeoff \nin this case. Discovering where the artifacts lie should be more a process of rationalizing the \nerrors than of simply catching them with the untrained eye. \nFrom this brief overview, we can outline the steps we will take to implement Screen Space \nAmbient Occlusion. \n \nWe will first need to have a depth buffer and a normal buffer at our disposal from \nwhich we can extract information. \n \nFrom these screen space maps, we can derive our algorithm. Each pixel in screen \nspace will generate a corresponding ambient occlusion value for that pixel and store \nthat information in a separate render target. For each pixel in our depth buffer, we \nextract that point‘s position and sample n neighboring pixels within the hemisphere \naligned around the point‘s normal. \n \nThe ratio of occluding versus non-occluding points will be our ambient occlusion term \nresult. \n \nThe ambient occlusion render target can then be blended with the color output from \nthe scene generated afterward. \nI will now describe our Screen Space Ambient Occlusion algorithm in greater detail. \n \nGenerating the Source Data \nThe first step in setting up the SSAO algorithm is to prepare the necessary incoming data. \nDepending on how the final compositing is to be done, this can be accomplished in one of \ntwo ways. \nThe first method requires that the scene be rendered twice. The first pass will render the \ndepth and normal data only. The SSAO algorithm can then generate the ambient occlusion \noutput in an intermediate step, and the scene can be rendered again in full color. With this \n\n\n \n \napproach, the ambient occlusion map (in screen space) can be sampled by direct lights from \nthe scene to have their contribution modulated by the ambient occlusion term as well, which \ncan help make the contributions from direct and indirect lighting more coherent with each \nother. This approach is the most flexible but is somewhat less efficient because the \ngeometry has to be passed to the hardware twice, doubling the API batch count and, of \ncourse, the geometry processing load. \nA different approach is to render the scene only once, using multiple render targets bound \nas output to generate the depth and normal information as the scene is first rendered \nwithout an ambient lighting term. SSAO data is then generated as a post-step, and the \nambient lighting term can simply be added. This is a faster approach, but in practice artists \nlose the flexibility to decide which individual lights in the scene may or may not be affected \nby the ambient occlusion term, should they want to do so. Using a fully deferred renderer \nand pushing the entire scene lighting stage to a post-processing step can get around this \nlimitation to allow the entire lighting setup to be configurable to use ambient occlusion per \nlight. \nWhether to use the single-pass or dual-pass method will depend on the constraints that are \nmost important to a given graphics engine. In all cases, a suitable format must be chosen to \nstore the depth and normal information. When supported, a 16-bit floating-point format will \nbe the easiest to work with, storing the normal components in the red, green, and blue \ncomponents and storing depth as the alpha component. \nScreen Space Ambient Occlusion is very bandwidth intensive, and minimizing sampling \nbandwidth is necessary to achieve optimal performance. Moreover, if using the single-pass \nmulti-render target approach, all bound render targets typically need to be of the same bit \ndepth on the graphics hardware. If the main color output is 32-bit RGBA, then outputting to \na 16-bit floating-point buffer at the same time won‘t be possible. To minimize bandwidth \nand storage, the depth and normal can be encoded in as little as a single 32-bit RGBA color, \nstoring the x and y components of the normal in the 8-bit red and green channels while \nstoring a 16-bit depth value in the blue and alpha channels. The HLSL shader code for \nencoding and decoding the normal and depth values is shown in Listing 1.2.1. \nListing 1.2.1. HLSL code to decode the normal on subsequent passes as well as \nHLSL code used to encode and decode the 16-bit depth value \n// Normal encoding simply outputs x and y components in R and G \nin \n// the range 0.. 1 \nfloat3 DecodeNormal( float2 cInput ) { \n     float3 vNormal.xy = 2.0f * cInput.rg - 1.0f; \n     vNormal.z = sqrt(max(0, 1 - dot(vNormal.xy, vNormal.xy))); \n     return vNormal; \n} \n \n// Encode depth to B and A \nfloat2 DepthEncode( float fDepth ) { \n    float2 vResult; \n    // Input depth must be mapped to 0..1 range \n    fDepth = fDepth / p_fScalingFactor; \n \n    // R = Basis = 8 bits = 256 possible values \n    // G = fractional part with each 1/256th slice \n    vResult.ba = frac( float2( fDepth, fDepth * 256.0f )); \n    return vResult; \n\n\n \n \n} \n \nfloat3 DecodeDepth( float4 cInput ) { \n    return dot ( cInput.ba, float2( 1.0f, 1.0f / 256.0f ) * \n                 p_fScalingFactor; \n} \n \n \nSampling Process \nWith the input data in hand, we can begin the ambient occlusion generation process itself. \nAt any visible point on a surface on the screen, we need to explore neighboring points to \ndetermine whether they could occlude our current point. Multiple samples are thus taken \nfrom neighboring points in the scene using a filtering process described by the HLSL shader \ncode in Listing 1.2.2. \nListing 1.2.2. Screen Space Ambient Occlusion filter described in HLSL code \n// i_VPOS is screen pixel coordinate as given by HLSL VPOS \ninterpolant. \n// p_vSSAOSamplePoints is a distribution of sample offsets for \neach sample. \nfloat4 PostProcessSSAO( float 3 i_VPOS ) \n{ \n   float2 vScreenUV;       //←This will become useful later. \n   float3 vViewPos = 2DToViewPos( i_VPOS, vScreenUV); \n \n   half fAccumBlock = 0.Of; \n   for (inti = 0; i < iSampleCount; i++ ) \n   { \n       float3 vSamplePointDelta = p_vSSAOSamplePoints[i]; \n       float fBlock = TestOcclusion( \n                          vViewPos, \n                             vSamplePointDelta, \n                          p_fOcclusionRadius, \n                          p_fFullOcclusionThreshold, \n                          p_fNoOcclusionThreshold, \n                          p_fOcclusionPower ) ) \n       fAccumBlock += fBlock; \n   } \n   fAccumBlock / = iSampleCount; \n \n   return 1.Of - fAccumBlock; \n} \n \nWe start with the current point, p, whose occlusion we are computing. We have the point‘s \n2D coordinate in screen space. Sampling the depth buffer at the corresponding UV \ncoordinates, we can retrieve that point‘s depth. From these three pieces of information, the \n\n\n \n \n3D position of the point within can be reconstructed using the shader code shown in Listing \n1.2.3. \nListing 1.2.3. HLSL shader code used to map a pixel from screen space to view \nspace \n// vRecipDepthBufferSize = 1.0 / depth buffer width and height \nin pixels. \n// p_vCameraFrustrumSize = Full width and height of camera \nfrustum at the \n// camera's near plane in world space. \nfloat2 p_vRecipDepthBufferSize; \nfloat2 p_vCameraFrustrumSize; \nfloat3 2DPosToViewPos( float3 i_VPOS, out float2 vScreenUV ) \n{ \n    float2 vViewSpaceUV = i_VPOS * p_vRecipDepthBufferSize; \n    vScreenUV    = vViewSpaceUV; \n \n    // From 0..1 to to 0..2 \n    vViewSpaceUV = vViewSpaceUV * float2( 2.Of, -2.Of ); \n    // From 0..2 to-1..1 \n    vViewSpaceUV = vViewSpaceUV + float2( -1.0f , 1.Of ); \n    vViewSpaceUV = vViewSpaceUV * p_vCameraFrustrumSize * 0.5f; \n \n    return float3( vViewSpaceUV.x, vViewSpaceUV.y, 1.0f ) * \n                   tex2D( p_sDepthBuffer, vScreenUV ).r; \n} \n \nWe will need to sample the surrounding area of the point p along multiple offsets from its \nposition, giving us n neighbor positions qi. Sampling the normal buffer will give us the \nnormal around which we can align our set of offset vectors, ensuring that all sample offsets \nfall within point p‘s upper hemisphere. Transforming each offset vector by a matrix can be \nexpensive, and one alternative is to perform a dot product between the offset vector and \nthe normal vector at that point and to flip the offset vector if the dot product is negative, as \nshown in Figure 1.2.3. This is a cheaper way to solve for the offset vectors without doing a \nfull matrix transform, but it has the drawback of using fewer samples when samples are \nrejected due to falling behind the plane of the surface of the point p. \nFigure 1.2.3. Samples behind the hemisphere are flipped over to stay within the \nhemisphere. \n",
      "page_number": 14
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 23-30)",
      "start_page": 23,
      "end_page": 30,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \nEach neighbor‘s 3D position can then be transformed back to screen space in 2D, and the \ndepth of the neighbor point can be sampled from the depth buffer. From this neighboring \ndepth value, we can establish whether an object likely occupies that space at the neighbor \npoint. Listing 1.2.4 shows shader code to test for this occlusion. \nListing 1.2.4. HLSL code used to test occlusion by a neighboring pixel \nfloat TestOcclusion( float3 vViewPos, \n                     float3 vSamplePointDelta, \n               float fOcclusionRadius, \n                     float fFullOcclusionThreshold, \n                     float fNoOcclusionThreshold, \n   float fOcclusionPower ) \n{ \n    float3 vSamplePoint = vViewPos + fOcclusionRadius * \nvSamplePointDelta; \n    float2 vSamplePointUV; \n    vSamplePointUV = vSamplePoint.xy / vSamplePoint.z; \n    vSamplePointUV = vSamplePointUV / p_vCameraSize / 0.5f; \n    vSamplePointUV = vSamplePointUV + float2( 1.0f, -1.0f ); \n    vSamplePointUV = vSamplePointUV * float2( 0.5f, -0.5f ); \n \n    float fSampleDepth = tex2D( p_sDepthBuffer, vSamplePointUV \n).r; \n    float fDistance = vSamplePoint.z - fSampleDepth; \n    return OcclusionFunction( fDistance, \nfFullOcclusionThreshold, \n                              fNoOcclusionThreshold, \nfOcclusionPower ); \n} \n \n\n\n \n \nWe now have the 3D positions of both our point p and the neighboring points qi. We also \nhave the depth di of the frontmost object along the ray that connects the eye to each \nneighboring point. How do we determine ambient occlusion? \nThe depth di gives us some hints as to whether a solid object occupies the space at each of \nthe sampled neighboring points. Clearly, if the depth di is behind the sampled point‘s depth, \nit cannot occupy the space at the sampled point. The depth buffer does not give us the \nthickness of the object along the ray from the viewer; thus, if the depth of the object is \nanywhere in front of p, it may occupy the space, though without thickness information, we \ncan‘t know for sure. We can devise some reasonable heuristics with the information we do \nhave and use a probabilistic method. \nThe further in front of the sample point the depth is, the less likely it is to occupy that \nspace. Also, the greater the distance between the point p and the neighbor point, the lesser \nthe occlusion, as the object covers a smaller part of the hemisphere. Thus, we can derive \nsome occlusion heuristics based on: \n \nThe difference between the sampled depth di and the depth of the point qi \n \nThe distance between p and qi \nFor the first relationship, we can formulate an occlusion function to map the depth deltas to \nocclusion values. \nIf the aim is to be physically correct, then the occlusion function should be quadratic. In our \ncase we are more concerned about being able to let our artists adjust the occlusion \nfunction, and thus the occlusion function can be arbitrary. Really, the occlusion function can \nbe any function that adheres to the following criteria: \n \nNegative depth deltas should give zero occlusion. (The occluding surface is behind \nthe sample point.) \n \nSmaller depth deltas should give higher occlusion values. \n \nThe occlusion value needs to fall to zero again beyond a certain depth delta value, as \nthe object is too far away to occlude. \nFor our implementation, we simply chose a linearly stepped function that is entirely \ncontrolled by the artist. A graph of our occlusion function is shown in Figure 1.2.4. There is \na full-occlusion threshold where every positive depth delta smaller than this value gets \ncomplete occlusion of one, and a no-occlusion threshold beyond which no occlusion occurs. \nDepth deltas between these two extremes fall off linearly from one to zero, and the value is \nexponentially raised to a specified occlusion power value. If a more complex occlusion \nfunction is required, it can be pre-computed in a small ID texture to be looked up on \ndemand. \nFigure 1.2.4. SSAO blocker function. \n\n\n \n \n \n \nListing 1.2.5. HLSL code used to implement occlusion function \nfloat OcclusionFunction( float fDistance, \n                         float fNoOcclusionThreshold, \nfloat fFullOcclusionThreshold, \nfloat fOcclusionPower ) \n{ \n    const c_occlusionEpsilon = 0.01f; \n \n    if ( fDistance > c_ occlusionEpsilon ) \n    { \n        // Past this distance there is no occlusion. \n        float fNoOcclusionRange = fNoOcclusionThreshold - \n                                  fFullOcclusionThreshold; \n        if ( fDistance < fFullOcclusionThreshold ) \n            return 1.0f; \n        else return max(1.0f – pow(( ( fDistance – \n             fFullOcclusionThreshold ) / fNoOcclusionRange, \n             fOcclusionPower ) ) ,0.0f ); \n    } else return 0.0f; \n} \n \nOnce we have gathered an occlusion value for each sample point, we can take the average \nof these, weighted by the distance of each sample point to p, and the average will be our \nambient occlusion value for that pixel. \n \nSampling Randomization \n\n\n \n \nSampling neighboring pixels at regular vector offsets will produce glaring artifacts to the \neye, as shown in Figure 1.2.5. \nFigure 1.2.5. SSAO without random sampling. \n \n \nTo smooth out the results of the SSAO lookups, the offset vectors can be randomized. A \ngood approach is to generate a 2D texture of random normal vectors and perform a lookup \non this texture in screen space, thus fetching a unique random vector per pixel on the \nscreen, as illustrated in Figure 1.2.6 [Mittring07]. We have n neighbors we must sample, \nand thus we will need to generate a set of n unique vectors per pixel on the screen. These \nwill be generated by passing a set of offset vectors in the pixel shader constant registers \nand reflecting these vectors through the sampled random vector, resulting in a semi-\nrandom set of vectors at each pixel, as illustrated by Listing 1.2.6. The set of vectors passed \nin as registers is not normalized—having varying lengths helps to smooth out the noise \npattern and produces a more even distribution of the samples inside the occlusion \nhemisphere. The offset vectors must not be too short to avoid clustering samples too close \nto the source point p. In general, varying the offset vectors from half to full length of the \nocclusion hemisphere radius produces good results. The size of the occlusion hemisphere \nbecomes a parameter controllable by the artist that determines the size of the sampling \narea. \nFigure 1.2.6. Randomized sampling process. \n\n\n \n \n \nListing 1.2.6. HLSL code used to generate a set of semi-random 3D vectors at each \npixel \nfloat3 reflect( float 3 vSample, float3 vNormal ) \n{ \n      return normalize ( vSample – 2.0f * dot( vSample, vNormal \n) * vNormal ); \n} \n \nfloat3x3 MakeRotation( float fAngle, float3 vAxis ) \n{ \n    float fS; \n    float fC; \n    sincos( fAngle, fS, fC ); \n    float fXX       = vAxis.x * vAxis.x; \n    float fYY       = vAxis.y * vAxis.y; \n    float fZZ       = vAxis.z * vAxis.z; \n    float fXY       = vAxis.x * vAxis.y; \n    float fYZ       = vAxis.y * vAxis.z; \n    float fZX       = vAxis.z * vAxis.x; \n    float fXS       = vAxis.x * fS; \n    float fYS       = vAxis.y * fS; \n    float fZS       = vAxis.z * fS; \n    float fOneC      = 1.0f - fC; \n \n    float3x3 result = float3x3( \n        fOneC * fXX +  fC,  fOneC * fXY + fZS, fOneC * fZX - \nfYS, \n        fOneC * fXY - fZS,  fOneC * fYY +  fC, fOneC * fYZ + \nfXS, \n        fOneC * fZX + fYS,  fOneC * fYZ - fXS, fOneC * fZZ +  fC \n    ); \n    return result; \n} \n \n\n\n \n \nfloat4 PostProcessSSAO( float3 i_VPOS ) \n{ \n \n      ... \n \n      const float c_scalingConstant = 256.0f; \n \n \n \n \n \n \nfloat3 vRandomNormal = ( \nnormalize( tex2D( p_sSSAONoise, vScreenUV * \n \n \n \n \n \n \np_vSrcImageSize / \nc_scalingConstant ).xyz * 2.0f \n \n \n \n \n \n \n– 1.0f ) ); \n \n \n \n \n \n \nfloat3x3 rotMatrix = \nMakeRotation( 1.0f,vNormal ); \n \n      half fAccumBlock = 0.0f; \n      for ( int i = 0; i < iSampleCount; i++ ) { \n          float3 vSamplePointDelta = reflect( \np_vSSAOSamplePoints[i], \n \n \n \n \n \n \nvRandomNormal ); \n          float fBlock = TestOcclusion( \n                             vViewPos, \n      vSamplePointDelta, \n                             p_fOcclusionRadius, \n                             p_fFullOcclusionThreshold, \n                             p_fNoOcclusionThreshold, \n                             p_fOcclusionPower ) ) { \n       fAccumBlock += fBlock; \n      } \n \n     ... \n \n} \n \n \nAmbient Occlusion Post-Processing \nAs shown in Figure 1.2.7, the previous step helps to break up the noise pattern, producing a \nfiner-grained pattern that is less objectionable. With wider sampling areas, however, a \nfurther blurring of the ambient occlusion result becomes necessary. The ambient occlusion \nresults are low frequency, and losing some of the high-frequency detail due to blurring is \ngenerally preferable to the noisy result obtained by the previous steps. \nFigure 1.2.7. SSAO term after random sampling applied. Applying blur passes will \nfurther reduce the noise to achieve the final look. \n\n\n \n \n \n \nTo smooth out the noise, a separable Gaussian blur can be applied to the ambient occlusion \nbuffer. However, the ambient occlusion must not bleed through edges to objects that are \nphysically separate within the scene. A form of bilateral filtering is used. This filter samples \nthe nearby pixels as a regular Gaussian blur shader would, yet the normal and depth for \neach of the Gaussian samples are sampled as well. (Encoding the normal and depth in the \nsame render targets presents significant advantages here.) If the depth from the Gaussian \nsample differs from the center tap by more than a certain threshold, or the dot product of \nthe Gaussian sample and the center tap normal is less than a certain threshold value, then \nthe Gaussian weight is reduced to zero. The sum of the Gaussian samples is then \nrenormalized to account for the missing samples. \nListing 1.2.7. HLSL code used to blur the ambient occlusion image \n// i_UV : UV of center tap \n// p_fBlurWeights Array of gaussian weights \n// i_GaussianBlurSample: Array of interpolants, with each \ninterpolants \n// packing 2 gaussian sample positions. \nfloat4 PostProcessGaussianBlur( VertexTransport vertOut ) \n{ \n     float2 vCenterTap    = i_UV.xy; \n     float4 cValue        = tex2D( p_sSrcMap, vCenterTap.xy ); \n     float4 cResult       = cValue * p_fBlurWeights[0]; \n     float fTotalWeight   = p_fBlurWeights[0]; \n \n     // Sample normal & depth for center tap \n     float4 vNormalDepth = tex2D( p_sNormalDepthMap, \nvCenterTap.xy ).a; \n     for ( int i = 0; i < b_iSampleInterpolantCount; i++ ) \n     { \n         half4 cValue = tex2D( p_sSrcMap, \n                               i_GaussianBlurSample[i].xy ); \n         half fWeight = p_fBlurWeights[i * 2 + 1]; \n \n\n\n \n \n         float4 vSampleNormalDepth = tex2D( p_sNormalDepthMap, \n                                     i_GaussianBlurSample[i].xy \n); \n         if ( dot( vSampleNormalDepth.rgb, vNormalDepth.rgb) < \n0.9f || \n              abs( vSampleNormalDepth.a – vNormalDepth.a ) > \n0.01f ) \n             fWeight = 0.0f; \n \n         cResult += cValue * fWeight; \n         fTotalWeight += fWeight; \n \n         cValue = tex2D( p_sSeparateBlurMap, \n                         INTERPOLANT_GaussianBlurSample[i].zw ) \n; \n         fWeight = p_fBlurWeights[i * 2 + 2]; \n         vSampleNormalDepth = tex2D( p_sSrcMap, \n                              \nINTERPOLANT_GaussianBlurSample[i].zw ) ; \n         if ( dot( vSampleNormalDepth.rgb, vNormalDepth .rgb < \n0.9f ) || \n                   abs( vSampleNormalDepth.a – vNormalDepth.a ) \n> 0.01f ) \n              fWeight = 0.0f; \n         cResult += cValue * fWeight; \n         fTotalWeight += fWeight; \n     } \n \n     // Rescale result according to number of discarded samples. \n     cResult *= 1.0f / fTotalWeight; \n \n     return cResult; \n} \n \nSeveral blur passes can thus be applied to the ambient occlusion output to completely \neliminate the noisy pattern, trading off some higher-frequency detail in exchange. \nFigure 1.2.8. Result of Gaussian blur. \n",
      "page_number": 23
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 31-40)",
      "start_page": 31,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \n \n \nHandling Edge Cases \nThe offset vectors are in view space, not screen space, and thus the length of the offset \nvectors will vary depending on how far away they are from the viewer. This can result in \nusing an insufficient number of samples at close-up pixels, resulting in a noisier result for \nthese pixels. Of course, samples can also go outside the 2D bounds of the screen. Naturally, \ndepth information outside of the screen is not available. In our implementation, we ensure \nthat samples outside the screen return a large depth value, ensuring they would never \nocclude any neighboring pixels. This can be achieved through the ―border color‖ texture \nwrapping state, setting the border color to a suitably high depth value. \nTo prevent unacceptable breakdown of the SSAO quality in extreme close-ups, the number \nof samples can be increased dynamically in the shader based on the distance of the point p \nto the viewer. This can improve the quality of the visual results but can result in erratic \nperformance. Alternatively, the 2D offset vector lengths can be artificially capped to some \nthreshold value regardless of distance from viewer. In effect, if the camera is very close to \nan object and the SSAO samples end up being too wide, the SSAO area consistency \nconstraint is violated so that the noise pattern doesn‘t become too noticeable. \n \nOptimizing Performance \nScreen Space Ambient Occlusion can have a significant payoff in terms of mood and visual \nquality of the image, but it can be quite an expensive effect. The main bottleneck of the \nalgorithm is the sampling itself. The semi-random nature of the sampling, which is \nnecessary to minimize banding, wreaks havoc with the GPU‘s texture cache system and can \nbecome a problem if not managed. The performance of the texture cache will also be very \ndependent on the sampling area size, with wider areas straining the cache more and \nyielding poorer performance. Our artists quickly got in the habit of using SSAO to achieve a \n\n\n \n \nfaked global illumination look that suited their purposes. This required more samples and \nwider sampling areas, so extensive optimization became necessary for us. \nOne method to bring SSAO to an acceptable performance level relies on the fact that \nambient occlusion is a low-frequency phenomenon. Thus, there is generally no need for the \ndepth buffer sampled by the SSAO algorithm to be at full-screen resolution. The initial depth \nbuffer can be generated at screen resolution, since the depth information is generally \nreused for other effects, and it potentially has to fit the size of other render targets, but it \ncan thereafter be downsampled to a smaller depth buffer that is a quarter size of the \noriginal on each side. The downsampling itself does have some cost, but the payback in \nimproved throughput is very significant. Downsampling the depth buffer also makes it \npossible to convert it from a wide 16-bit floating-point format to a more bandwidth-friendly \n32-bit packed format. \n \nFake Global Illumination and Artistic Styling \nIf the ambient occlusion hemisphere is large enough, the SSAO algorithm eventually starts \nto mimic behavior seen from general global illumination; a character relatively far away \nfrom a wall could cause the wall to catch some of the subtle shadowing cues a global \nillumination algorithm would detect. If the sampling area of the SSAO is wide enough, the \nlook of the scene changes from darkness in nooks and crannies to a softer, ambient feel. \nThis can pull the art direction in two somewhat conflicting directions: on the one hand, the \nneed for tighter, high-contrast occluded zones in deeper recesses, and on the other hand, \nthe desire for the larger, softer, ambient look of the wide-area sampling. \nOne approach is to split the SSAO samples between two different sets of SSAO parameters: \nSome samples are concentrated in a small area with a rapidly increasing occlusion function \n(generally a quarter of all samples), while the remaining samples use a wide sampling area \nwith a gentler function slope. The two sets are then averaged independently, and the final \nresult uses the value from the set that produces the most (darkest) occlusion. This is the \napproach that was used in StarCraft II. \nFigure 1.2.9. SSAO with different sampling-area radii. \n\n\n \n \n \n \nThe edge-enhancing component of the ambient occlusion does not require as many samples \nas the global illumination one, thus a quarter of the samples can be assigned to crease \nenhancement while the remainder are assigned for the larger area threshold. \nThough SSAO provides for important lighting cues to enhance the depth of the scene, there \nwas still a demand from our artist for more accurate control that was only feasible through \nthe use of some painted-in ambient occlusion. The creases from SSAO in particular cannot \nreach the accuracy that using a simple texture can without using an enormous amount of \nsamples. Thus the usage of SSAO does not preclude the need for some static ambient \nocclusion maps to be blended in with the final ambient occlusion result, which we have done \nhere. \nFigure 1.2.10. Combined small- and large-area SSAO result. \n \n\n\n \n \n \nFor our project, complaints about image noise, balanced with concerns about performance, \nwere the main issues to deal with for the technique to gain acceptance among our artists. \nIncreasing SSAO samples helps improve the noise, yet it takes an ever-increasing number \nof samples to get ever smaller gains in image quality. Past 16 samples, we‘ve found it‘s \nmore effective to use additional blur passes to smooth away the noise pattern, at the \nexpense of some loss of definition around depth discontinuities in the image. \n \nTransparency \nIt should be noted the depth buffer can only contain one depth value per pixel, and thus \ntransparencies cannot be fully supported. This is generally a problem with all algorithms \nthat rely on screen space depth information. There is no easy solution to this, and the SSAO \nprocess itself is intensive enough that dealing with edge cases can push the algorithm \noutside of the real-time realm. In practice, for the vast majority of scenes, correct ambient \nocclusion for transparencies is a luxury that can be skimped on. Very transparent objects \nwill typically be barely visible either way. For transparent objects that are nearly opaque, \nthe choice can be given to the artist to allow some transparencies to write to the depth \nbuffer input to the SSAO algorithm (not the z-buffer used for hidden surface removal), \noverriding opaque objects behind them. \n \nFinal Results \nColor Plate 1 shows some results portraying what the algorithm contributes in its final form. \nThe top-left pane shows lighting without the ambient occlusion, while the top-right pane \nshows lighting with the SSAO component mixed in. The final colored result is shown in the \nbottom pane. Here the SSAO samples are very wide, bathing the background area with an \neffect that would otherwise only be obtained with a full global illumination algorithm. The \nSSAO term adds depth to the scene and helps anchor the characters within the \nenvironment. \nColor Plate 2 shows the contrast between the large-area, low-contrast SSAO sampling \ncomponent on the bar surface and background and the tighter, higher-contrast SSAO \nsamples apparent within the helmet, nooks, and crannies found on the character‘s \nspacesuit. \n \nConclusion \nThis gem has described the Screen Space Ambient Occlusion technique used at Blizzard and \npresented various problems and solutions that arise. Screen Space Ambient Occlusion offers \na different perspective in achieving results that closely resemble what the eye expects from \nambient occlusion. The technique is reasonably simple to implement and amenable to \nartistic tweaks in real time to make it ideal to fit an artistic vision. \n \nReferences \n\n\n \n \n[Bavoil] Bavoil, Louis and Miguel Sainz. ―Image-Space Horizon-Based Ambient Occlusion.‖ \nShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River \nMedia, 2009. Section 6.2. \n[Bavoil09] Bavoil, Louis and Miguel Sainz. ―Multi-Layer Dual-Resolution Screen-Space \nAmbient Occlusion.‖ 2009. NVIDIA. n.d. \n<http://developer.download.nvidia.com/presentations/2009/SIGGRAPH/Bavoil_MultiLayerD\nualResolutionSSAO.pdf>. \n[Bavoil08] Bavoil, Louis and Miguel Sainz. ―Screen Space Ambient Occlusion.‖ Sept. 2008. \nNVIDIA. n.d. \n<http://developer.download.nvidia.com/SDK/10.5/direct3d/Source/ScreenSpaceAO/doc/Scr\neenSpaceAO.pdf>. \n[Fox08] Fox, Megan. ―Ambient Occlusive Crease Shading.‖ Game Developer. March 2008. \n[Kajalin] Kajalin, Vladimir. ―Screen Space Ambient Occlusion.‖ ShaderX7: Advanced \nRendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. Section \n6.1. \n[Lajzer] Lajzer, Brett and Dan Nottingham. ―Combining Screen-Space Ambient Occlusion \nand Cartoon Rendering on Graphics Hardware.‖ n.d. Brett Lajzer. \nn.d.<http://brettlajzer.com/pub/graphics/final/nprssao_final_presentation.pdf>. \n[Luft06] Luft, Thomas, Carsten Colditz, and Oliver Deussen. ―Image Enhancement by \nUnsharp Masking the Depth Buffer.‖ Course on Non-Photorealistic Rendering. SIGGRAPH \n2006. Boston Convention and Exhibition Center, Boston, MA. 3 August 2006. \n[Mittring07] Mittring, Martin. ―Finding Next Gen—CryEngine 2.0.‖ Course on Advanced Real-\nTime Rendering in 3D Graphics and Games. SIGGRAPH 2007. San Diego Convention Center, \nSan Diego, CA. 8 August 2007. \n[Pesce] Pesce, Angelo. ―Variance Methods for Screen-Space Ambient Occlusion.‖ ShaderX7: \nAdvanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. \nSection 6.7. \n[Ritschel09] Ritschel, Tobias, Thorsten Grosch, and Hans-Peter Seidel. ―Approximating \nDynamic Global Illumination in Image Space.‖ 2009. Max Planck Institut Informatik. n.d. \n<http://www.mpi-inf.mpg.de/~ritschel/Papers/SSDO.pdf>. \n[Sains08] Sains, Miguel. ―Real-Time Depth Buffer Based Ambient Occlusion.‖ Game \nDevelopers Conference. Moscone Center, San Francisco, CA. 18–22 February 2008. \n[Shamugan07] Shanmugam, Perumaal and Okan Arikan. ―Hardware Accelerated Ambient \nOcclusion Techniques on GPUs.‖ 2007. Google Sites. \nn.d.<http://perumaal.googlepages.com/ao.pdf>. \n[Sloan07] Sloan, Peter-Pike, Naga K. Govindaraju, Derek Nowrouzezahrai, and John Snyder. \n―Image-Based Proxy Accumulation for Real-Time Soft Global Illumination.‖ Pacific Graphics \nConference. The Royal Lahaina Resort, Maui, Hawaii. 29 October 2007. \n[Tomasi98] Tomasi, Carlo and Roberto Manduchi. ―Bilateral Filtering for Gray and Color \nImages.‖ IEEE International Conference on Computer Vision. Homi Bhabha Auditorium, \nBombay, India. 7 January 1998. \n \n\n\n \n \n1.3. Multi-Resolution Deferred Shading \nHyunwoo Ki, INNOACE Co., Ltd \npsykinu@gmail.com \nRecently, deferred shading has become a popular rendering technique for real-time games. \nDeferred shading enables game engines to handle many local lights without repeated \ngeometry processing because it replaces geometry processing with pixel processing \n[Saito90, Shishkovtsov05, Valient07, Koonce07, Engel09, Kircher09]. In other words, \nshading costs are independent of geometric complexity, which is important as the CPU cost \nof scene-graph traversal and the GPU cost of geometry processing grows with scene \ncomplexity. Despite this decoupling of shading cost from geometric complexity, we still seek \nto optimize the pixel processing necessary to handle many local lights, soft shadows, and \nother per-pixel effects. In this gem, we present a technique that we call multi-resolution \ndeferred shading, which provides adaptive sub-sampling using a hierarchical approach to \nshading by exploiting spatial coherence of the scene. Multi-resolution deferred shading \nefficiently reduces pixel shading costs as compared to traditional deferred shading without \nnoticeable aliasing. As shown in Figure 1.3.1, our technique allows us to achieve a \nsignificant improvement in performance with negligible visual degradation relative to a more \nexpensive full-resolution deferred shading approach. \nFigure 1.3.1. Deferred shading (left: 20 fps), multi-resolution deferred shading \n(center: 38 fps), and their difference image (right). There are 40 spot lights, \nincluding fuzzy shadows (1024×1024 pixels with 24 shadow samples per pixel). \n \n \n \nDeferred Shading \nUnlike traditional forward rendering approaches, deferred shading costs are independent of \nscene complexity. This is because deferred shading techniques store geometry information \nin textures, often called G-buffers, replacing geometry processing with pixel processing \n[Saito90, Shishkovtsov05, Valient07, Koonce07]. \nDeferred shading techniques start by rendering the scene into a G-buffer, which is typically \nimplemented using multiple render targets to store geometry information, such as positions, \nnormals, and other quantities instead of final shading results. Next, deferred shading \nsystems render a screen-aligned quad to invoke a pixel shader at all pixels in the output \nimage. The pixel shader retrieves the geometry information from the G-buffer and performs \nshading operations as a post process. Naturally, one must carefully choose the data formats \nand precise quantities to store in a G-buffer in order to make the best possible use of both \nmemory and memory bandwidth. For example, the game Killzone 2 utilizes four buffers \ncontaining lighting accumulation and intensity, normal XY in 16-bit floating-point format, \nmotion vector XY, specular and diffuse albedo, and sun occlusion [Valient07]. The Z \ncomponent of the normal is computed from normal XY, and position is computed from depth \n\n\n \n \nand pixel coordinates. These types of encodings are a tradeoff between decode/encode cost \nand the memory and memory bandwidth consumed by the G-buffer. As shown in Color Plate \n3, we simply use two four-channel buffers of 16-bit floating-point precision per channel \nwithout any advanced encoding schemes for ease of description and implementation. The \nfirst of our buffers contains view-space position in the RGB channels and a material ID in \nthe alpha channel. The other buffer contains view-space normal in the RGB channels and \ndepth in the alpha channel. \nWe could also use material buffers that store diffuse reflectance, specular reflectance, \nshininess, and so on. However, material buffers are not necessary if we separate lighting \nand material phases from the shading phase using light pre-pass rendering [Engel09]. \nUnlike traditional deferred shading, light pre-pass rendering first computes lighting results \ninstead of full shading. This method can then incorporate material properties in an \nadditional material phase with forward rendering. Although this technique requires a second \ngeometry rendering pass, such separation of lighting and material phases gives added \nflexibility during material shading and is compatible with hardware multi-sample anti-\naliasing. A related technique, inferred lighting, stores lighting results in a single low-\nresolution buffer instead of the full-resolution buffer [Kircher09]. To avoid discontinuity \nproblems, this technique filters edges using depth and object ID comparison in the material \nphase. As we will describe in the next section, our technique is similar to inferred lighting, \nbut our method finds discontinuous areas based on spatial proximity and then solves the \ndiscontinuity problems using a multi-resolution approach during the lighting (or shading) \nphase. \n \nMulti-Resolution Deferred Shading \nAlthough deferred shading improves lighting efficiency, computing illumination for every \npixel is still expensive, despite the fact that it is often fairly low frequency. We have \ndeveloped a multi-resolution deferred shading approach to exploit the low-frequency nature \nof illumination. We perform lighting in a lower-resolution buffer for spatially coherent areas \nand then interpolate results into a higher-resolution buffer. This key concept is based upon \nour prior work [Ki07a]. Here, we generalize this work and improve upon it to reduce \naliasing. \nThe algorithm has three steps, as shown in Color Plate 4: geometry pass, multi-resolution \nrendering pass, and composite pass. The geometry pass populates the G-buffers. Our \ntechnique is compatible with any sort of G-buffer organization, but for ease of explanation, \nwe will stick with the 8-channel G-buffer layout described previously. \nThe next step is multi-resolution rendering, which consists of resolution selection (non-edge \ndetection), shading (lighting), and interpolation (up-sampling). We allocate buffers to store \nrendering results at various resolutions. We call these buffers R-buffers, where the ―R‖ \nstands for ―Result‖ or ―Resolution.‖ In this chapter, we will use three R-buffers: full \nresolution, quarter resolution, and 1/16th resolution (for example, 1280×1024, 640×512, \nand 320×256). If the full-resolution image is especially high, we could choose to decrease \nthe resolutions of the R-buffers even more drastically than just one-quarter resolution in \neach step. \nMulti-resolution rendering uses rendering iterations from lower-resolution to higher-\nresolution R-buffers. We prevent repeated pixel processing by exploiting early-Z culling to \nskip pixels processed in earlier iterations using lower-resolution R-buffers [Mitchell04]. To \nstart shading our R-buffers, we set the lowest-resolution R-buffer as the current render \ntarget and clear its depth buffer with one depth (farthest). Next, we determine pixels being \nrendered in this resolution by rendering a screen-aligned quad with Zi = 1.0 – i * 0.1, \nwhere i is the current iteration, writing only depth. During this pass, the pixel shader reads \ngeometry information from mip-mapped versions of our G-buffers and estimates spatial \n\n\n \n \nproximity for non-edge detection. To estimate spatial proximity, we first compare the \ncurrent pixel‘s material ID with the material IDs of neighboring pixels. Then, we compare \nthe difference of normal and depth values using tunable thresholds. If spatial proximity is \nlow for the current pixel, we should use a higher-resolution R-buffer for better quality, and \nthus we discard the current pixel in the shader to skip writing Z. After this pass, pixels \nwhose spatial proximity is high (in other words, non-edge) in the current resolution contain \nmeaningful Z values because they were not discarded. The pixels whose spatial proximity is \nlow (in other words, edges) still have farthest Z values left over from the initial clear. \nWe then perform shading (or lighting) by rendering a screen-aligned quad with Zi = 1.0 – i \n* 0.1 again, but the Z function is changed to Equal. This means that only spatially coherent \npixels in this resolution will pass the Z-test, as illustrated in Color Plate 4. In the pixel \nshader, we read geometric data from G-buffers and compute illumination as in light pre-\npass rendering. On a textured surface, such as wall and floor, although spatial proximity \nbetween neighboring pixels is high, these pixel colors are often different. Such cases can \ncause serious aliasing in the resulting images. To solve this problem, we store only lighting \nresults instead of full shading results into R-buffers, and we handle material properties with \nstored illumination in R-buffers in the composite pass. \nAfter shading, we copy the current shading/lighting results and depth to the next higher-\nresolution R-buffer, allowing the hardware‘s bilinear units to do a simple interpolation as we \nup-sample. We have found that bilinear filtering is adequate, though we could use bi-cubic \nfiltering or other higher-order filtering for better quality. \nWe repeat the process described above at the next higher resolution, estimating spatial \nproximity and writing Z and computing illumination until we reach the full-resolution R-\nbuffer. A full-screen quad is drawn three times per iteration. If a given pixel was shaded on \na prior iteration in a lower-resolution R-buffer, that pixel is not shaded again at the higher \nresolution due to early-Z culling. In this way, we are able to perform our screen-space \nshading operations at the appropriate resolution for different regions of the screen. In \nFigure 1.3.2, we visualize the distribution of pixels shaded at each level of our hierarchy. \nFigure 1.3.2. Visualization of hierarchical pixel processing. \nNon-black pixels were shaded in the first pass at 1/16th resolution as in the image on the \nleft. The middle image shows the pixels shaded in the second iteration at one-quarter \nresolution, and only the pixels in the image on the right were shaded at full image \nresolution. \n \n \nBecause this approach exploits image scaling from low resolution to high resolution with \ninterpolation, discontinuity artifacts can appear at boundaries of lighting or shadows. We \naddress this issue during the multi-resolution rendering phase. We write 1.0 to the alpha \nchannel of R-buffer pixels that are lit; otherwise, we write zero. If pixels are lit by the same \nlights (or the same number of lights), their neighbors‘ alpha values will be equal. Therefore, \nwe interpolate these pixels to a higher-resolution buffer. Otherwise, we consider these \npixels within the boundary, and thus we discard them in the interpolation pass (see Figure \n1.3.3). We can handle shadow boundaries similarly. \n\n\n \n \nFigure 1.3.3. A boundary-check algorithm. If a pixel is lit by a light, we add one \nalpha for this pixel in the lighting phase. In the interpolation pass, we consider \npixels that are in boundaries whose neighbor pixels’ alpha values are different to \nothers, and thus we use a higher-resolution buffer without interpolation. \n \n \nIf shadow color is neither zero nor one (in other words, penumbra), we also set a pixel \nalpha to zero and thus discard it in the interpolation work. \nIn the composite pass, we render a screen-aligned quad, reading shading results from the \nfull-resolution R-buffer and material properties such as albedo to compute the final shading \nresult. We could draw scene geometry instead of drawing a screen quad for MSAA, similar \nto light pre-pass rendering. \nIn contrast to traditional deferred shading and light pre-pass rendering, multi-resolution \ndeferred shading reduces rendering costs for low-frequency pixels. Our multi-resolution \ndeferred shading is also more efficient than inferred lighting due to the hierarchical \napproach. Multi-resolution deferred shading can also be used for other rendering \ntechniques, such as the GPU-based light clustering technique for diffuse interreflection and \nsubsurface light diffusion called Light Pyramids [Ki08]. The Light Pyramids technique stores \nfirst-bounced lights in shadow maps and groups them by considering their angular and \nspatial similarity. Although such light clustering dramatically reduces the number of lights, it \nstill requires hundreds of lights for each pixel. Figure 1.3.4 shows an example of a \ncombination of Light Pyramids and multi-resolution deferred shading. Thanks to our pixel \nclustering, we achieved a performance improvement of approximately 1.5 to 2.0 times \nwithout noticeable quality loss. As pixel processing increases in complexity—for example, \nusing higher resolution or using more lights—the relative performance improvement also \nincreases. \nFigure 1.3.4. Indirect illumination using Light Pyramids [Ki08] based on \ntraditional deferred shading (left) and multi-resolution deferred shading (right: \n1.7 times faster). \n\n\n \n \n \n \n \n \nConclusion and Future Work \nWe have presented a multi-resolution deferred shading technique that performs lighting and \nshading computations at appropriate screen-space frequency in order to improve the \nefficiency of deferred shading without aliasing. In the future, we would also like to develop \neven more efficient resolution-selection algorithms, and we also seek to handle a wider \nvariety of surface reflection models. We also hope to integrate transparent rendering of \ninferred lighting into our method. We believe that our method could be applied for not only \nlighting but also other rendering operations with high per-pixel overhead, such as per-pixel \ndisplacement mapping [Ki07b]. \n \nReferences \n[Engel09] Engel, Wolfgang. ―Designing a Renderer for Multiple Lights: The Light Pre-Pass \nRenderer.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: \nCharles River Media, 2009. 655-666. \n[Ki08] Ki, Hyunwoo. ―A GPU-Based Light Hierarchy for Real-Time Approximate Illumination.‖ \nThe Visual Computer 24.7–9 (July 2008): 649–658. \n[Ki07a] Ki, Hyunwoo. ―Hierarchical Rendering Techniques for Real-Time Approximate \nIllumination on Programmable Graphics Hardware.‖ Master‘s Thesis. Soongsil University, \n2007. \n[Ki07b] Ki, Hyunwoo and Kyoungsu Oh. ―Accurate Per-Pixel Displacement Mapping using a \nPyramid Structure.‖ 2007. Hyunwoo Ki. n.d. <http://ki-h.com/article/ipdm.html>. \n[Kircher09] Kircher, Scott and Alan Lawrance. ―Inferred Lighting: Fast Dynamic Lighting and \nShadows for Opaque and Translucent Objects.‖ Course on 3D and the Cinematic in Games. \nSIGGRAPH 2009. Ernest N. Morial Convention Center, New Orleans, LA. 6 August 2009. \n[Koonce07] Koonce, Rusty. ―Deferred Shading in Tabula Rasa.‖ GPU Gems 3. Ed. Hurbert \nNguyen. Kendallville, KY: Addison-Wesley, 2007. 429–458. \n",
      "page_number": 31
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 41-49)",
      "start_page": 41,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": " \n \n[Mitchell04] Mitchell, Jason and Pedro Sander. ―Applications of Explicit Early-Z Culling.‖ \nCourse on Real-Time Shading. SIGGRAPH 2004. Los Angeles Convention Center, Los \nAngeles, CA. 8 August 2004. \n[Saito90] Saito, Takafumi and Tokiichiro Takahashi. ―Comprehensible Rendering of 3-D \nShapes.‖ ACM SIGGRAPH Computer Graphics 24.4 (August 1990): 197–206. \n[Shishkovtsov05] Shishkovtsov, Oles. ―Deferred Shading in S.T.A.L.K.E.R.‖ GPU Gems 2: \nProgramming Techniques for High-Performance Graphics and General Purpose Computation. \nEd. Matt Pharr. Kendallville, Ky: Addison-Wesley, 2005. 143–166. \n[Valient07] Valient, Michal. ―Deferred Rendering in Killzone 2.‖ Develop Conference 2007. \nBrighton Hilton Metropole, Brighton, England, UK. 25 July 2007. \n \n1.4. View Frustum Culling of Catmull-Clark Patches in DirectX 11 \nRahul P. Sathe, Advanced Visual Computing, Intel Corp \nrahul.p.sathe@intel.com \nDirectX 11 has introduced hardware tessellation in order to enable high geometric detail \nwithout increasing memory usage or memory bandwidth demands. Higher-order surface \npatches with displacements are of prime interest to game developers, and we would like to \nrender them as efficiently as possible. For example, we would like to cull subdivision surface \npatches (instead of the resulting triangles) that will not affect the final image. Culling a \ngiven patch avoids higher-order surface evaluation of domain points in that patch as well as \nprocessing of the triangles generated for the patch. The nature of higher-order surface \npatches coupled with displacements and animation make the process of culling them non-\ntrivial, since the exact geometric bounds are not known until well after the opportunity to \ncull a given patch. In this chapter, we will present an algorithm that evaluates conservative \nbounding boxes for displaced approximate Catmull-Clark subdivision surface patches at run \ntime, allowing us to perform view frustum culling on the patches. With this method, we \nachieve performance improvement with minimal overhead. \nBackground \nBefore describing our culling strategy, we must review the fundamentals of Catmull-Clark \nsubdivision surfaces, displacement mapping, and the methods that are currently in use to \napproximate Catmull-Clark subdivision surfaces on DirectX 11. \nDisplaced Subdivision Surfaces and Catmull-Clark Surfaces \nCatmull-Clark subdivision surfaces have become an increasingly popular modeling primitive \nand have been extensively used in offline rendering [DeRose98]. In general, subdivision \nsurfaces can be described as recursive refinement of a polygonal mesh. Starting with a \ncoarse polygonal mesh M0, one can introduce new vertices along the edges and faces and \nupdate the connectivity to get a mesh M1, and repeat this process to get meshes M2, M3, \nand so on. In the limit, this process approaches a smooth surface S. This smooth surface S \nis called the subdivision limit surface, and the original mesh M0 is often referred to as the \ncontrol mesh. \nThe control mesh consists of vertices connected to each other to form edges and faces. The \nnumber of other vertices that a given vertex is connected to directly by shared edges is \ncalled the valence of a vertex. In the realm of Catmull-Clark subdivision surfaces, a vertex \nis called a regular or ordinary vertex if it has a valence of four. If the valences of all of the \n\n\n \n \nvertices of a given quad are four, then that quad is called an ordinary quad or an ordinary \npatch. The faces that have at least one vertex that is not valence four are called \nextraordinary faces (or patches). \nApproximate Catmull-Clark Subdivision Surfaces \nRecently, Loop and Schaefer introduced a hardware-friendly method of rendering \nApproximate Catmull Clark (ACC) subdivision surfaces, which maps very naturally to the \nDirectX 11 pipeline [Loop08]. At its core, the ACC scheme maps each quadrilateral from the \noriginal control mesh to a bi-cubic Bezier patch. Loop and Schaefer show that, for ordinary \npatches, the bi-cubic Bezier corresponds exactly to the Catmull-Clark limit surface. \nExtraordinary patches do not correspond exactly to the limit surface, but Loop and Schaefer \ndecouple the patch description for position attributes and normal attributes in order to \nreduce the visual impact of the resulting discontinuities. To do this, for extraordinary \npatches, ACC generates separate normal and bi-tangent patches in order to impose GN \ncontinuity at patch boundaries. The word ―approximate‖ in ACC has its roots in the fact that \nthese extraordinary patches are GN continuous, and this GN continuity only guarantees the \nsame direction of partial derivatives but not the magnitudes across the patch boundaries. \nThe ACC scheme describes the normals and bi-tangents using additional Bezier patches, \nwhich results in a continuous normal field even across edges of extraordinary patches. \nDisplacement \nAlthough it is very empowering to be able to generate smooth surfaces from polygonal \nmeshes procedurally, such smooth surfaces are rarely encountered in real life and lack \nrealism without additional high-frequency geometric detail. This is where displacement \nmaps come into the picture. Displacement maps are simply textures that can be used to \nstore geometric perturbations from a smooth surface. Although normal maps and \ndisplacement maps have the similar effect of adding high-frequency detail, the difference is \nnotable around the silhouettes of objects. A normal mapped object‘s silhouette lacks \ngeometric detail because only per-pixel normals are perturbed and not the underlying \ngeometry, as illustrated in Figure 1.4.1. To add this high-frequency detail, displacement \nmaps can be applied to subdivision surfaces. \nFigure 1.4.1. Normal mapping versus displacement mapping. \n \n \n \nDirectX 11 Pipeline \nDirectX 11 has introduced three new stages to the graphics pipeline to enable dynamic on \nchip tessellation, as shown in Figure 1.4.4. The two new programmable pipeline stages are \nthe hull shader and the domain shader. Between these two programmable stages lies a new \nfixed function stage, the tessellator. Fortunately for us, ACC and Direct3D 11 were designed \nwith each other in mind, and there is a natural mapping of the ACC algorithm onto the \nDirect3D 11 pipeline. \n\n\n \n \nHull Shader \nAs illustrated in Figure 1.4.1, the new hull shader stage follows the traditional vertex \nshader. In a typical implementation of ACC on Direct3D 11, the vertex shader is responsible \nfor performing animation of the control mesh vertices. In the hull shader, each \nquadrilateral‘s four vertices and its one-ring neighborhood are gathered from the output of \nthe vertex shader. These vertices are used to define the control points of a bi-cubic Bezier \npatch. This basis conversion process that generates the Bezier patch control points is SIMD \nfriendly, and every output control point can be calculated independently of others. In order \nto exploit this opportunity for parallelism, this control point phase of the hull shader is \ninvoked once per control point. In the case of ACC, the basis conversion process depends on \nthe topology of the incoming patch, but the output control points are always a 4×4 Bezier \ncontrol mesh. Please refer to the sample code on the CD. \nFigure 1.4.2. Basis conversion for an irregular patch. \n \n \nIn addition to the computation of the Bezier control points, the hull shader can optionally \ncalculate edge tessellation factors in order to manage level of detail. One can assign \narbitrary tessellation factors to the edges of a patch (within some constraints, defined by \nthe DirectX 11 tessellator specifications). Because the hull shader is programmable, one can \nchoose any metric to calculate edge tessellation factors. Typical metrics may include screen \nspace projection, proximity to silhouette, luminosity reaching the patch, and so on. The \ncalculation of each edge tessellation factor is typically independent of the others, and hence \nthe edge tessellation factors can also be computed in parallel in a separate phase of the hull \nshader called the fork phase. The final stage of hull shader is called the join phase (or patch \nconstant phase) and is a phase in which the shader can efficiently compute data that is \nconstant for the entire patch. This stage is of most interest to us in this chapter. \nTessellator \nThe tessellator accepts edge LODs of a patch and other tessellator-specific states that \ncontrol how it generates domain locations and connectivity. Some of these states include \npatch topology (quad, tri, or isoline), inside reduction function (how to calculate inner \ntessellation factor(s) using outer tessellation factors), one-axis versus two-axis reduction \n(whether to reduce only one inner tessellation factor or two—once per each domain axis), \nand scale (how much to scale inner LOD). The tessellator feeds domain values to the \ndomain shader and connectivity information to the rest of the pipeline via the geometry \nshader. \nDomain Shader \nIn the case of quadrilateral patch rendering, the domain shader is invoked at domain values \n(u,v) determined by the tessellator. (In the case of triangular patches, the barycentric \n\n\n \n \ncoordinates (u,v,w); w = 1 – u – v are used.) Naturally, the domain shader has access to \noutput control points from the hull shader. Typically, the domain shader evaluates a higher-\norder surface at these domain locations using the control points provided by the hull shader \nas the basis. After evaluating the surface, the domain shader can perform arbitrary \noperations on the surface position, such as displacing the geometry using a displacement \nmap. \nIn ACC, we evaluate position using bi-cubic polynomials for a given (u,v). Our domain \nshader interpolates texture coordinates (s,t) from the four vertices using bilinear \ninterpolation to generate the texture coordinates for the given (u,v). We also optionally \nsample a displacement map at these interpolated texture coordinates. As mentioned earlier, \nnormal calculation is different for ordinary and extraordinary patches. For ordinary patches, \nwe just calculate d/du and d/dv of the position and take the cross-product. For \nextraordinary patches, we evaluate tangent and bi-tangent patches separately and take \ntheir cross-product. \n \nCulling \nThe mapping of ACC to the DirectX 11 pipeline that we have described allows us to render \nsmooth surfaces with adaptive tessellation and displacement mapping, resulting in a \ncompelling visual quality improvement while maintaining a modest memory footprint. At the \nend of the day, however, we are still rendering triangles, and the remaining stages of the \ngraphics pipeline are largely unchanged, including the hardware stages that perform \ntriangle setup and culling. This means that we perform vertex shading, domain shading, \ntessellation, and hull shading of all patches submitted to the graphics pipeline, including \nthose patches that are completely outside of the view frustum. Clearly, this provides an \nopportunity for optimization. The main contribution of this chapter is a method for frustum \nculling patches early in the pipeline in order to avoid unnecessary computations. Of course, \nwe must account for mesh animation and displacement, both of which deform a given patch \nin a way that complicates culling. An elegant generalized solution to surface patch culling \nhas been proposed by Hasselgren et al. that generates culling shaders, looking at domain \nshaders using Taylor Arithmetic [Hasselgren09]. This article proposes a simplified version of \nideas discussed in their work to cull the approximate Catmull-Clark patches against view \nfrustum. \n \nPre-Processing Step \nWe perform a pre-processing step on a given control mesh and displacement map in order \nto find the maximum displacement for each patch. Please note, although the positions are \nevaluated as bi-cubic polynomials using the new basis, the texture coordinates for those \npoints are the result of bilinear interpolation of texture coordinates of the corners. This is \ndue to the fact that the local (per-patch) uv-parameterization used to describe the Catmull-\nClark surface and the global uv-parameterization done while creating the displacement map \nare linearly dependent on each other. Figure 1.4.3 shows one such patch. This linear \ndependence means that straight lines u = 0, v = 0, u = 1, and v = 1 in the patch \nparameterization are also straight lines in the global parameterization. Due to this linear \nrelationship, we know the exact area in the displacement map from which the displacements \nwill be sampled in the domain shader for that patch. The maximum displacement in the \ngiven patch can be found by calculating the maximum displacement in the region confined \nby patch boundaries in the displacement map. Even if the displacement map stores vector-\nvalued displacements, the mapping is still linear, so we can still find the magnitude of the \nmaximum displacement for a given patch. Based on this, we can create a buffer for the \nentire mesh that stores this maximum displacement per patch. \n\n\n \n \nFigure 1.4.3. Mapping between global (s-t) and local (u-v) parameterization is \nlinear. The figure on the left shows (u,v) parameterization that is used for patch \nevaluation. The figure on the right shows the global parameterization (s,t) that \nwas used while unwrapping original mesh. Bold lines correspond to u=0, v=0, \nu=1, and v=1 lines in the figure on the left. \n \n \n \nRun-Time Step \nAt run time, the patch vertices of the control mesh go through the vertex shader, which \nanimates the control mesh. The hull shader then operates on each quad patch, performing \nthe basis transformation to Bezier control points. One convenient property of Bezier patches \nis that they always stay within the convex hull of the control mesh defining the patch. Using \nthe maximum displacement computed previously, we can move the convex hull planes of a \ngiven patch outward by the maximum displacement, resulting in conservative bounds \nsuitable for culling a given patch. Although moving the convex hull planes out by the max \ndisplacement may give tighter bounds compared to an axis-aligned bounding box (AABB) \nfor the control mesh, calculating the corner points can be tricky because it requires \ncalculation of plane intersections. It is simpler and more efficient to compute an AABB of the \ncontrol mesh and offset the AABB planes by the maximum displacement. \nIn Figure 1.4.5, we show a 2D representation of this process for illustration. Dotted black \nlines represent the basis-converted Bezier control mesh. The actual Bezier curve is shown in \nbold black, displacements along the curve normal (scalar valued displacements) are shown \nin solid gray, and the maximum displacement for this curve segment is denoted as d. An \nAABB for the Bezier curve is shown in dashed lines (the inner bounding box), and the \nconservative AABB that takes displacements into account is shown in dashed and dotted \nlines (the outer bounding box). \nFigure 1.4.4. The DirectX11 pipeline. Normally, triangles get culled after primitive \nassembly, just before rasterization. The proposed scheme culls the patches in the \n\n\n \n \nhull shader, and all the associated triangles from that patch get culled as a result, \nfreeing up compute resources. \n \n \nFigure 1.4.5. Conservative AABB for a displaced Bezier curve. The Bezier curve is \nshown in bold black, the control mesh in dotted lines, and displacements in solid \ngray lines. AABB for the Bezier curve without displacements is shown in dashed \nlines (inner bounding box), and conservative AABB for the displaced Bezier curve \nis shown in dashed and dotted lines (outer bounding box). \n \n \nAs you can see, the corners of inner and outer enclosures are more than d distance apart, \nso we are being more conservative than we need to be for the ease and speed of \ncomputation. \n\n\n \n \nAt this point, we have a conservative patch AABB that takes displacements into account. If \nthe AABB for a patch is outside the view frustum, we know that the entire patch is outside \nthe view frustum and can be safely culled. If we make the view frustum‘s plane equations \navailable as shader constants, then our shader can test the AABB using in-out tests for view \nfrustum. Alternatively, one can transform the AABB into normalized device coordinates \n(NDC), and the in-out tests can be done in NDC space. In-out tests in NDC space are easier \nthan world space tests because they involve comparing only with +1 or –1. If the AABB is \noutside the view frustum, we set the edge LODs for that patch to be negative, which \nindicates to the graphics hardware that the patch should be culled. We perform the culling \ntest during the join phase (a.k.a. patch constant phase) of the hull shader because this \noperation only needs to be performed once per patch. \n \nPerformance \nFor each culled patch, we eliminate unnecessary tessellator and domain shader work for \nthat patch. All patches, whether or not they‘re culled, take on the additional computational \nburden of computing the conservative AABB and testing against the view frustum. When \nmost of the character is visible on the screen (for example, Figure 1.4.9 (a)), culling \noverhead is at its worst. Figure 1.4.6 shows that, even in this case, culling overhead is \nminimal and is seen only at very low levels of tessellation. At LOD=3, the gains due to \nculling a very small number of patches (around the character‘s feet) start offsetting the \ncycles spent on culling tests. \nFigure 1.4.6. Culling overhead is the worst when nothing gets culled. Culling \noverhead is minimal except at very low levels of tessellation. ―NO CULL‖ indicates \nthe fps measured when no culling code was running. ―CULL Overhead‖ shows the \nfps measured when culling code was running in the patch constant phase of \nshaders. \n \nWhen about half of the patches in our test model are outside of the view frustum (see \nFigure 1.4.9 (b)), the overhead of the AABB computations is offset by the gains from culling \nthe offscreen patches. The gains from culling patches are more noticeable at higher levels of \ntessellation. This is shown graphically in Figures 1.4.7 and 1.4.8. Figure 1.4.7 shows how \n\n\n \n \nfps changes with the edge tessellation factor (edge LOD) when about half of the patches are \nculled. As you can see, at moderate levels of tessellation, we strike the balance between \nbenefits of the proposed algorithm at increased level of detail. Figure 1.4.8 shows the same \ndata as percentage speed-up. \nFigure 1.4.7. Culling benefits go up with the level of tessellation, except at the \nsuper-high levels of tessellation where culling patches doesn’t help. At moderate \nlevels of tessellation, we get benefits of the proposed algorithm and still see high \ngeometric details. \n \nFigure 1.4.8. Culling benefits shown as percentage increase in fps against edge \nLODs (edge tessellation factor). \n \n\n\n \n \nFigure 1.4.9. Screenshots showing our algorithm in action. We saw about 8.9 fps \nfor the view on the left and 15.1 fps for the view on the right on the ATI Radeon \n5870. Increase in the frame rate was due to view frustum culling patches. \n \n \nWe performed all our tests on the ATI Radeon 5870 card, with 1 GB GDDR. The benefits of \nthis algorithm increase with domain shader complexity and tessellation level, whereas the \nper-patch overhead of the culling tests remains constant. It is easy to imagine an \napplication strategy that first tests an object‘s bounding box against the frustum to \ndetermine whether patch culling should be performed at all for a given object, thus avoiding \nthe culling overhead for objects that are known to be mostly onscreen. \n \nConclusion \nWe have presented a method for culling Catmull-Clark patches against the view frustum \nusing the DirectX 11 pipeline. Applications will benefit the most from this algorithm at \nmoderate to high levels of tessellation. In the future, we would like to extend this technique \nto account for occluded and back-facing patches with displacements. \n \nReferences \n[DeRose98] DeRose, Tony, Michael Kass, and Tien Truong. ―Subdivision Surfaces in \nCharacter Animation.‖ Proceedings of the 25th Annual Conference on Computer Graphics \nand Interactive Techniques. 1998. ACM SIGGRAPH. n.d. \n<http://doi.acm.org/10.1145/280814.280826>. \n[Hasselgren09] Hasselgren, Jon, Jacob Munkberg, and Tomas Akenine-Möller. ―Automatic \nPre-Tessellation Culling.‖ ACM Transactions on Graphics 28.2 (April 2009): n.p. ACM Portal. \n[Loop08] Loop, Charles and Scott Schaefer. ―Approximating Catmull-Clark Subdivision \nSurfaces with Bicubic Patches.‖ ACM Transactions on Graphics 27.1 (March 2008): n.p. ACM \nPortal. \n[Microsoft09] Microsoft Corporation. DirectX SDK. August 2009. \n[Reif95] Reif, Ulrich. ―A Unified Approach to Subdivision Algorithms Near Extraordinary \nVertices.‖ Computer Aided Geometric Design 12.2 (March 1995): 153–174. ACM Portal. \n[Stam98] Stam, Jos. ―Exact Evaluation of Catmull-Clark Subdivision Surfaces at Arbitrary \nParameter Values.‖ Proceedings of the 25th Annual Conference on Computer Graphics and \nInteractive Techniques (1998): 395–404. ACM Portal. \n",
      "page_number": 41
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 50-58)",
      "start_page": 50,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": " \n \n[Zorin2000] Zorin, Dennis and Peter Schroder. ―Subdivision for Modeling and Animation.‖ \nSIGGRAPH. 2000. 85–94. \n \n1.5. Ambient Occlusion Using DirectX Compute Shader \nJason Zink \njzink_1@yahoo.com \nMicrosoft has recently released DirectX 11, which brings with it significant changes in \nseveral of its APIs. Among these new and updated APIs is the latest version of Direct3D. \nDirect3D 11 provides the ability to perform multi-threaded rendering calls, a shader \ninterface system for providing an abstraction layer to shader code, and the addition of \nseveral new programmable shader stages. One of these new shader stages is the compute \nshader, which provides a significantly more flexible processing paradigm than was available \nin previous iterations of the Direct3D API. Specifically, the compute shader allows for a \ncontrollable threading model, sharing memory between processing threads, synchronization \nof primitive functions, and several new resource types to allow read/write access to \nresources. \nThis gem will provide an introduction to the compute shader and its new features. In \naddition, we will take an in-depth look at a Screen Space Ambient Occlusion (SSAO) \nalgorithm implemented on the compute shader to show how to take advantage of this new \nprocessing paradigm. We will examine the SSAO algorithm in detail and provide a sample \nimplementation to demonstrate how the compute shader can work together with the \ntraditional rendering pipeline. Finally, we will wrap up with a discussion of our results and \nfuture work. \nThe Compute Shader \nBefore we begin to apply the compute shader‘s capabilities to a particular problem domain, \nlet‘s take a closer look at the compute shader itself and the general concepts needed to \nprogram it. \nOverview \nThe compute shader is a new programmable shader stage that is actually not simply \ninserted into the traditional rendering pipeline like some of the other new DirectX 11 \npipeline stages discussed in Sathe‘s Gem 1.4. Rather, the compute shader is conceptually a \nstandalone processing element that has access to the majority of the functionality available \nin the common shader core, but with some important additional functionality. The two most \nimportant new mechanics are fine-grained control over how each thread is used in a given \nshader invocation and new synchronization primitives that allow threads to synchronize. The \nthreads also have read/write access to a common memory pool, which provides the \nopportunity for threads to share intermediate calculations with one another. These new \ncapabilities are the basic building blocks for advanced algorithms that have yet to be \ndeveloped, while at the same time allowing for traditional algorithms to be implemented in \ndifferent ways in order to achieve performance improvements. \nCompute Shader Threading Model \nTo use the compute shader, we need to understand its threading model. The main concept \nis that of a Thread Group. A Thread Group defines the number of threads that will be \nexecuting in parallel that will have the ability to communicate with one another. The threads \nwithin the Thread Group are conceptually organized in a 3D grid layout, as shown in Figure \n\n\n \n \n1.5.1, with the sizes along each axis of the grid determined by the developer. The choice of \nthe layout provides a simple addressing scheme used in the compute shader code to have \neach thread perform an operation on a particular portion of the input resources. When a \nparticular thread is running, it executes the compute shader code and has access to several \nsystem value input attributes that uniquely identify the given thread. \nFigure 1.5.1. Thread Groups visualized as a 3D volume. \n \n \nTo actually execute the compute shader, we tell the API to execute a given number of \nThread Groups via the Dispatch method, as illustrated in Figure 1.5.2. \nFigure 1.5.2. Visualization of the Dispatch method. \n \n\n\n \n \n \nWith these two layout definitions in mind, we can look at how they affect the addressing \nscheme of the compute shader. The following list of system values is available to the \ncompute shader: \n \nSV_GroupID. This system value identifies the Thread Group that a thread belongs \nto with a 3-tuple of zero-based indices. \n \nSV_GroupThreadID. This system value identifies the thread index within the \ncurrent Thread Group with a 3-tuple of zero-based indices. \n \nSV_DispatchThreadID. This system value identifies the current thread identifier \nover a complete Dispatch call with a 3-tuple of zero-based indices. \n \nSV_GroupIndex. This system value is a single integer value representing a flat \nindex of the current thread within the group. \nThe individual threads running the compute shader have access to these system values and \ncan use the values to determine, for example, which portions of input to use or which \noutput resources to compute. For example, if we wanted a compute shader to perform an \noperation on each pixel of an input texture, we would define the thread group to be of size \n(x, y, 1) and call the Dispatch method with a size of (m, n, 1) where x*m is the width of the \nimage and y*n is the height of the image. In this case, the shader code would use the \nSV_DispatchThreadID system value to determine the location in the input image from \nwhich to load data and where the result should be stored in the output image. \nFigure 1.5.3 illustrates one way in which a 2D workload might be partitioned using this \nmethod. In this example, we have an image with a size of 32×32 pixels. If we wanted to \nprocess the image with a total of 4×4 (m = 4, n = 4) Thread Groups as shown, then we \nwould need to define the Thread Groups to each have 8×8 (x = 8 and y = 8) threads. This \ngives us the total number of threads needed to process all 32×32 (x*m and y*n) pixels of \nthe input image. \n\n\n \n \nFigure 1.5.3. Visualization of Thread Group distribution for a 2D workload, where \nthe number of Thread Groups (m = 4, n = 4) and the number of threads (x = 8, y = \n \n \n \nCompute Shader Thread Interactions \nIn addition to providing an easy-to-use thread addressing scheme, the compute shader also \nallows each Thread Group to declare a block of Group Shared Memory (GSM). This memory \nis basically defined as an array of variables that are accessible to all of the threads in the \nThread Group. The array itself can be composed of any native data types as well as \nstructures, allowing for flexible grouping of data. In practice, the Group Shared Memory is \nexpected to be on-chip register-based memory that should be significantly faster to access \nthan general texture memory, which can have unpredictable performance depending on \naccess patterns. \nSimilar to CPU-based multi-threaded programming, when you have multiple threads reading \nand writing to the same area of memory there is the potential that the same memory can \n\n\n \n \nbe accessed simultaneously by more than one thread. To provide some form of control over \nthe sequences of access, the compute shader introduces several atomic functions for thread \nsynchronization. For example, there is an atomic function for adding called \nInterlockedAdd. This can be used to have all threads perform a test sequence and then \nuse the InterlockedAdd function to increment a variable in the Group Shared Memory \nto tabulate an overall number of test sequences that produce a particular result. \nAnother atomic function is the InterlockedCompareExchange function, which \ncompares a shared variable with one argument and sets the variable to a second argument \nif the variable has the same value as the first argument. This provides the basic building \nblocks of creating a mutex system in the compute shader, where a shared variable serves \nas the mutex. Each thread can call this function on the mutex variable and only take action \nif it is able to update the variable to its own identifier. Since the compute shader is intended \nto provide massively parallel execution, a mutex is not really a preferred choice, but in \nsome situations it may be a desirable avenue to follow, such as when a single resource \nmust be shared across many threads. The Direct3D 11 documentation can be referenced for \na complete list of these atomic functions and how they can be used. \nAlso similar to CPU-based multi-threaded programming is the fact that it is more efficient to \ndesign your algorithms to operate in parallel while minimizing the number of times that they \nmust synchronize data with one another. The fastest synchronization operation is the one \nthat you don‘t have to perform! \nCompute Shader Resources \nNew resource types introduced in Direct3D 11 include Structured Buffers, Byte Address \nBuffers, and Append/Consume Buffers. Structured Buffers provide what they sound like—ID \nbuffers of structures available in your shader code. The Byte Address Buffers are similar, \nexcept that they are a general block of 32-bit memory elements. \nThe Append/Consume Buffers allow for stack/queue-like access to a resource, allowing the \nshader to consume the elements of a buffer one at a time and append results to an output \nbuffer one at a time. This should also provide some simplified processing paradigms in \nwhich the absolute position of an element is less important than the relative order in which \nit was added to the buffer. \nTo further facilitate the compute shader‘s parallel-processing capabilities, Direct3D 11 \nprovides a new resource view called an Unordered Access View (UAV). This type of view \nallows the compute shader (as well as the pixel shader) to have read and write access to a \nresource, where any thread can access any portion of the resource. This is a big departure \nfrom the traditional shader resource access paradigm; typically, a shader can only read \nfrom or write to a given resource during a shader invocation, but not both. The UAV can be \nused to provide random access to both the new and existing resource types, which provides \nsignificant freedom in designing the input and output structure of compute shader–based \nalgorithms. \nWith a general understanding of the new capabilities of the compute shader, we can now \ntake a look at a concrete example in order to better understand the details. We will discuss \nthe general concepts of the SSAO algorithm and then describe how we can use the compute \nshader‘s features to build an efficient implementation of the technique. \n \nScreen Space Ambient Occlusion \nScreen Space Ambient Occlusion is a relatively recently developed technique for \napproximating global illumination in the ambient lighting term based solely on the \n\n\n \n \ninformation present in a given frame‘s depth buffer [Mittring07]. As described in detail in \nGem 1.2 by Filion, an approximate amount of ambient light that reaches a given pixel can \nbe computed by sampling the area around the pixel in screen space. This technique \nprovides a convincing approximation to global illumination and performs at a usable speed \nfor high-end applications. \nThe quality of the algorithm depends on the number of samples and subsequent calculations \nthat are performed for each pixel. In the past few years, a variety of techniques have been \nproposed to modify the general SSAO algorithm with varying levels of quality versus \nperformance tradeoffs, such as HBAO [Bavoil09a] and SSDO [Ritschel09]. While these new \nvariants of the original algorithm provide improvements in image quality or performance, \nthe basic underlying concepts are shared across all implementations, and hence the \ncompute shader should be applicable in general. \nWe will now review some of these recent SSAO techniques and discuss several areas of the \nunderlying algorithm that can benefit from the compute shader‘s new capabilities. Then we \nwill look at an implementation that takes advantage of some of these possible \nimprovements. \nSSAO Algorithm \nAmbient occlusion techniques have been around for some time and have found uses \nprimarily in offline rendering applications [Landis02]. The concept behind these techniques \nis to utilize the geometric shape of a model to calculate which portions of the model would \nbe more likely to be occluded than others. If a given point on a model is located on a flat \nsurface, it will be less occluded than another point that is located at a fold in the surface. \nThis relationship is based on the following integral for the reflected radiance: \n \nIn this integral, Lin is the incident radiation from direction ω, and the surface normal vector \nis n. This integral indicates that the amount of light reflected at a given surface point is a \nfunction of the incident radiance and the angle at which it reaches that point. If there is \nnearby geometry blocking some portion of the surface surrounding the surface point, then \nwe can generally conclude that less radiant energy will reach the surface. With this in mind, \nthe ambient lighting term can be modulated by an occlusion factor to approximately \nrepresent this geometric relationship. \nOne way to perform this geometric calculation is to project a series of rays from each \nsurface point being tested. The amount of occlusion is then calculated depending on the \nnumber of rays that intersect another part of the model within a given radius from the \nsurface point. This effectively determines how much ―background‖ light can reach that point \nby performing the inverse operation of the radiance integral described previously. Instead of \nintegrating the incident radiance coming into that point over the surface of a hemisphere, \nwe shoot rays out from the surface point over the hemisphere to test for occlusion within \nthe immediate area. The overall occlusion factor is then calculated by accumulating the ray \ntest results and finding the ratio of occluded rays versus non-occluded rays. Once it is \ncalculated, this occlusion factor is then stored either per vertex or per pixel in a texture map \nand is used to modulate the ambient lighting term of that object when rendered. This \nproduces a rough approximation of global illumination. Figure 1.5.4 demonstrates this ray \ncasting technique. \nFigure 1.5.4. Side profile of a ray casting technique for approximating occlusion. \n\n\n \n \n \n \nThis technique works quite well for static scenes or individual static geometric models, but \nthe pre-computation requirements are not practical for dynamic geometry, such as skinned \nmeshes. Several alternative techniques have been suggested to allow for dynamic ambient \nocclusion calculations, such as [Bunnell05], which generalizes the geometric object into \ndisks to reduce the computational complexity of the occlusion calculations. This allows real-\ntime operation of the algorithm, but it still requires some pre-processing of the models \nbeing rendered to determine where to place the disks in the approximated models. In \naddition, the cost of performing the occlusion calculation scales with increased scene \ncomplexity. \nThe Screen Space Ambient Occlusion algorithm provides an interesting alternative technique \nfor determining an approximate occlusion value. Instead of computing an occlusion value \nbased on the geometric representation of a scene by performing ray casting, the occlusion \ncalculation is delayed until after the scene has been rasterized. Once the scene has been \nrasterized, an approximated amount of occlusion is determined by inspecting the contents \nof the scene‘s depth buffer only—the geometric queries are carried out on the depth buffer \ninstead of on the geometric models. This effectively moves the operation from an object \nspace operation to a screen space operation—which is one of the major benefits of this \nalgorithm. Since it operates at the screen space level, the algorithm‘s performance is less \nsensitive to the amount of geometry being rendered and is more sensitive to the resolution \nof the buffers being used. \nThe scene‘s depth buffer can be obtained by utilizing the actual Z-buffer used during \nrendering, by performing a separate rendering pass that writes the linear depth to a render \ntarget, or by using the depth information from a deferred rendering G-buffer. Regardless of \nhow the buffer is generated, the algorithm performs a processing pass that uses the depth \nbuffer as an input and generates an output texture that holds the occlusion values for the \nentire visible scene. Each pixel of the output is calculated using the depth information within \na given radius of its local area, which can be considered an approximation to ambient \nocclusion. I will refer to this output in the remainder of this document as the occlusion \nbuffer. When the final scene rendering is performed, the occlusion buffer is sampled based \non screen space location and used to modulate the ambient term of each object in the final \nscene. \nSSAO Algorithm Details \nScreen Space Ambient Occlusion has provided a significant improvement over previous \nambient occlusion algorithms. Due to the fact that the algorithm runs after a scene is \nrendered, it focuses the processing time on only the portion of the scene that is visible for \n\n\n \n \nthe current frame, saving a significant amount of computation and allowing the algorithm to \nbe run in real-time applications without pre-computation. However, the use of the depth \nbuffer also introduces a few obstacles to overcome. \nThere is the potential that some occluders will not be visible in the depth buffer if there is \nanother object in front of it. Since the depth buffer only records one depth sample per pixel, \nthere is no additional information about the occluders behind the foreground object. This is \ntypically handled by defaulting to zero occlusion if the depth sample read from the depth \nbuffer is too far away from the current pixel being processed. If a more accurate solution is \nneeded, depth peeling can be used to perform multiple occlusion queries, as described in \n[Bavoil09b]. \nAdditionally, if an object is offscreen but is still occluding an object that is visible onscreen, \nthen the occlusion is not taken into account. This leads to some incorrect occlusion \ncalculations around the outer edge of the image, but solutions have also been proposed to \nminimize or eliminate these issues. One possibility is to render the depth buffer with a \nlarger field of view than the final rendering to allow objects to be visible to the algorithm \naround the perimeter of the view port [Bavoil09a]. \nAnother issue with the algorithm is that a relatively large number of samples needs to be \ntaken in order to generate a complete representation of the geometry around each pixel. If \nperformance were not a concern, we could sample the entire area around the pixel P in a \nregular sampling pattern, but in real-time applications this quickly becomes impractical. \nInstead of a regular sampling pattern, a common solution is to use a sparse sampling kernel \nto choose sampling points around the current pixel. This roughly approximates the \nsurrounding area, but the decreased sampling rate may miss some detail. \nTo compensate for the decreased sampling, it is common to use a stochastic sampling \ntechnique instead. By varying the sampling kernel shape and/or orientation for each pixel \nand then sharing the results between neighboring pixels, an approximation to the more \nexpensive regular sampling pattern can be achieved. Since a typical 3D scene is composed \nof groups of connected triangles, the majority of the contents of the depth buffer will \ncontain roughly similar depth values in neighborhoods of pixels except at geometric \nsilhouette edges. The variation of the sampling kernel between pixels in combination with \nthis spatial coherence of the depth buffer allows us to share a combined larger number of \nsample results per pixel while reducing the overall number of calculations that need to be \nperformed. \nThis helps to effectively widen the sampling kernel, but it also introduces some additional \nhigh-frequency noise into the occlusion buffer. To compensate for this effect, it is common \nto perform a filtering pass over the entire occlusion buffer that blurs the occlusion values \nwithout bleeding across object boundaries. This type of a filter is referred to as a bilateral \nfilter, which takes into account both the spatial distance between pixels and the intensity \nvalues stored in neighboring pixels when calculating the weights to apply to a sample \n[Tomasi98]. This allows the filter to remove high-frequency noise and at the same time \npreserve the edges that are present in the occlusion buffer. In addition, the randomization \nprocess can be repeated over a small range to facilitate easier filtering later on. Figures \n1.5.5 and 1.5.6 show ambient occlusion results before and after bilateral filtering. \nFigure 1.5.5. A sample scene rendered without bilateral filtering. \n\n\n \n \n \n \nFigure 1.5.6. A sample scene after bilateral filtering. \n \n \nAs mentioned before, the algorithm is performed after rasterization, meaning that its \nperformance is directly related to the screen resolution being used. In fact, this dependency \non screen resolution has been exploited to speed up the algorithm as described in Gem 1.3. \nThe depth buffer and/or the occlusion buffer can be generated at a decreased resolution. If \nthe screen resolution is decreased by a factor of 2 in the x and y directions, there is an \noverall factor of 4 reduction in the number of occlusion pixels that need to be calculated. \nThen the occlusion buffer can either be upsampled with a bilateral filter or just directly used \nat the lower resolution. \nThis strategy can still lead to fairly pleasing results, since the contents of the occlusion \nbuffer are relatively low frequency. \nSSAO Meets the Compute Shader \nWhen looking at the block diagram of the SSAO algorithm in Figure 1.5.7, we can begin to \ncompare these high-level operations with the new capabilities of the compute shader to see \nhow we can build a more efficient implementation. We will now go over the steps of the \nalgorithm and discuss potential strategies for mapping to the compute shader. \nFigure 1.5.7. Block diagram of the SSAO algorithm. \n",
      "page_number": 50
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-68)",
      "start_page": 59,
      "end_page": 68,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \n \nCalculation Setup \nThe first step shown in the block diagram is to initialize the computations for the current \npixel. This entails sampling the depth buffer to obtain the pixel‘s depth. One of the benefits \nof having a Group Shared Memory that can be shared by all threads in a Thread Group is \nthe possibility to share texture samples among the entire Thread Group. Because the shared \nmemory is supposed to be significantly faster than a direct texture sample, if each thread \nrequests a depth sample to initialize its own calculations, then it can also write that depth \nvalue to the shared memory for use later on by other threads. The net effect of every \nthread in a Thread Group doing this is to have a copy of the complete local depth data in \nthe Group Shared Memory. Later, as each thread begins calculating the relative occlusion \nagainst the local area, it can read the needed depth values from the Group Shared Memory \ninstead of directly loading from texture memory. Figure 1.5.8 shows this process. \nFigure 1.5.8. Comparison of directly sampling versus using the Group Shared \nMemory for cached sampling. \n\n\n \n \n \n \nThere are a few additional notes to consider on this topic, however. There is some overhead \nassociated with reading the depth values and then storing them to the Group Shared \nMemory. In addition, the texture cache can often provide very fast results from memory \nsample requests if the result was in the cache. Thus, depending on the hardware being run \nand the patterns and frequency of memory access, it may or may not provide a speed \nincrease to use the Group Shared Memory in practice. \nRandomize Sampling Kernel \nThe next step in the SSAO block diagram is to somehow randomize the sampling kernel that \nwill be used to later look up the surrounding area. This is typically done by acquiring a \nrandom vector and then performing a ―reflect‖ operation on each of the sampling kernel \nvectors around the random vector. Probably the most common way to acquire this vector is \nto build a small texture with randomized normal vectors inside. The shader can load a single \nnormalized reflection vector based on the screen space position of the pixel being processed \n[Kajalin09]. This makes removing the ―salt-and-pepper‖ noise easier in the filtering stage of \nthe algorithm. \nIn the past, SSAO was performed in the pixel shader, which means that the pixel shader \nrequired a screen space position as a fragment attribute to be passed by the vertex or \ngeometry shader. The compute shader can help to simplify this operation somewhat. By \nutilizing the Dispatch ID system value, we can automatically receive the integer ID of each \npixel being processed in our compute shader code. To create our repeating pattern of \nreflection vectors in screen space, we can simply perform a bitwise AND operation on the \nleast significant bits of the dispatch ID—in other words, if we wanted to repeat every 4×4 \nblock of pixels, we would mask off all but the two least significant bits of the ID. \nIn fact, we can even store the randomized vectors as an array of constants in our shader. \nThis eliminates the need for a texture sample and the repeating texture of normalized \nreflection vectors altogether. Of course this is predicated on the fact that we don‘t use too \nmany vectors, but we could always use the standard approach if that is needed. \nAcquire Depth Data \n\n\n \n \nOnce the sampling kernel has been randomized, we can acquire each individual depth \nsample. In a traditional SSAO algorithm, this is done with a sampler that uses the x and y \ncoordinates of the current sampling kernel vector to offset from the current pixel location. \nSince the sampling kernel has been pseudo-randomized, there is a potential for reduced \ntexture cache efficiency if the sampling kernel width is large enough. \nIf we utilize the Group Shared Memory as described previously, then the depth values that \nwe need to acquire could already be available in the GSM. However, there are several points \nto consider before embarking on this strategy as well. Since the Thread Group will only be \noperating on one block of the depth data at a time—for example, a 16×16 block—then we \nneed to consider what happens at the edges of that block. The pixels along the outer edges \nof the block will need access to the depth samples within our sampling radius, and they \nwould not already be pre-loaded. This provides a choice—we could either pre-load a larger \nportion of the depth buffer to include the surrounding area or we could dynamically check to \nsee whether the data has been loaded to the GSM yet, and, if not, then directly get it from \nthe depth buffer. \nBoth options could have performance penalties. Pre-loading large bands of depth data \naround each block may end up increasing the number of depth samples to the point that it \nwould be just as efficient to perform the sampling in the traditional manner. If we \ndynamically decide whether or not to fetch data from the depth buffer, then we could \nperform a large number of dynamic branches in the shader, which could also be detrimental \nto performance. These factors need to be weighed against the increased access speed \nprovided by using the GSM instead of direct sampling. With the texture cache providing \nsimilar fast access for at least a portion of the texture samples, it is altogether possible that \nthe standard approach would be faster. Of course, any discussion of texture cache \nperformance depends on the hardware that the algorithm is running on, so this should be \ntested against your target platform to see which would be a better choice. \nThe other point to consider with using the GSM is that there is no native support for bilinear \nfiltering of the GSM data. If you wanted to filter the depth values for each depth sample \nbased on the floating-point values of the kernel offset vector, then you would need to \nimplement this functionality in the shader code itself. However, since the depth buffer \ncontains relatively low-frequency data, this is not likely to affect image quality in this case. \nPerform Partial Occlusion Calculation (per Sample) \nOnce we have obtained a depth sample to compare to our current pixel depth, we can move \nto the partial occlusion calculations. In this step, we determine whether our sample depth \ncauses any occlusion at the current pixel. There are many different varieties of calculations \navailable to perform here, from a binary test of the sample point being above or below the \nkernel offset vector [Kajalin09] all the way up to a piecewise defined function read from a \ntexture [Filion08]. \nRegardless of how the calculation is performed, there is an interesting possibility that the \ncompute shader introduces if the calculation is only a function of the depth delta—sharing \nocclusion calculations between pixels. If we call our current pixel point P and our current \nsample point S, then the occlusion caused at point P by point S is inherently related to the \ninverse occlusion at point S by point P. Since the compute shader can perform scatter \noperations, a single thread can calculate the occlusion for one pair of locations and then \nwrite the result to point P and the inverse of the calculation to point S. \nThis would save the number of required calculations by nearly a factor of 2, but it would \nalso introduce the need for some type of communication mechanism to get the values to \nboth occlusion buffer values. Since there is the possibility that multiple pixels would be \ntrying to write a result to the same pixel, we could attempt to use the atomic operations for \nupdating the values, but this could lead to a large number of synchronization events \nbetween threads. At the same time, these occlusion values can be accumulated in the GSM \n\n\n \n \nfor fast access by each thread. Again, the cost of the synchronization events will likely vary \nacross hardware, so further testing would be needed to see how much of a benefit could \ncome from this implementation. \nPerform Complete Occlusion Calculation \nThe final step in this process is to calculate the final occlusion value that will end up in the \nocclusion buffer for use in the final rendering. This is normally done by performing a simple \naverage of all of the partial occlusion calculations. In this way, we can scale the number of \nsamples used to calculate the occlusion according to the performance level of the target \nhardware. \nAs described earlier, there is typically some form of a bilateral filter applied to the occlusion \nbuffer after all pixels have a final occlusion value calculated. In general, filtering is one area \nthat could potentially see huge benefits from compute shader implementations. Since \nfiltering generally has an exact predetermined access pattern for the input image, the Group \nShared Memory can directly be used to pre-load the exact texture data needed. This is \nespecially beneficial when implementing 2D separable filters due to the ability to perform \nthe filtering pass in one direction, store the result into the GSM, then perform the second \nfiltering pass in the other direction over the values in the GSM without ever writing the \nresults back to the output buffer in between steps. Even though the bilateral filter is non-\nseparable, it has been shown that a decent approximation of it can be achieved with a \nseparable implementation [Pham05]. \n \nCompute Shader Implementation Details \nAfter reviewing some of the new features available in the compute shader and how they can \nbe used with the SSAO algorithm, we can now look at a sample implementation. Since the \ncompute shader techniques are relatively new, the focus of this implementation will be to \ndemonstrate some of its new features and draw some conclusions about appropriate-use \ncases for them. These features are described briefly here, with additional detail provided in \nthe following sections. \nThis implementation will utilize two different-size thread groups, 16×16 and 32×32, to \ngenerate the occlusion buffer. Using two different sizes will allow us to see whether the \nThread Group size has any effect on the performance of the algorithm. We will also \ndemonstrate the use of the GSM as a cache for the depth values and compare how well this \ntactic performs relative to directly loading samples from the depth buffer. In addition to \nusing the GSM, we also utilize the Gather sampling function for filling the GSM with depth \nvalues to see whether there is any impact on overall performance. The randomization \nsystem will utilize one of the new thread addressing system values to select a reflection \nvector, eliminating the need for a randomization texture. After the occlusion buffer has been \ngenerated, we will utilize a separable version of the bilateral filter to demonstrate the ability \nof the compute shader to efficiently perform filtering operations. \nImplementation Overview \nThe process is started by rendering a linear depth buffer at full-screen resolution with the \ntraditional rendering pipeline. Stored along with the depth value is the view space normal \nvector, which will be used during the occlusion calculations. This depth/normal buffer serves \nas the primary input to the compute shader to calculate a raw, unfiltered occlusion buffer. \nFinally, we use the depth/normal buffer and the raw occlusion buffer to perform separable \nbilateral filtering to produce a final occlusion buffer suitable for rendering the scene with the \nstandard rendering pipeline. \n\n\n \n \nDepth/Normal Buffer Generation \nThe depth/normal buffer will consist of a four-component floating-point texture, and each of \nthe occlusion buffers will consist of a single floating-point component. The depth/normal \nvectors are generated by rendering the linear view space depth and view space normal \nvectors into the depth/normal buffer. The depth value is calculated by simply scaling the \nview space depth by the distance to the far clipping plane. This ensures an output in the \nrange of [0,1]. The normal vector is calculated by transforming the normal vector into view \nspace and then scaling and biasing the vector components. Listing 1.5.1 shows the code for \ndoing so. \nListing 1.5.1. Generation of the view space depth and normal vector buffer \noutput.position = \nmul( float4( v.position, 1.0f ) ,WorldViewProjMatrix ); \n \nfloat3 ViewSpaceNormals = \nmul( float4( v.normal, 0.0f ) ,WorldViewMatrix ).xyz; \n \noutput.depth.xyz = ViewSpaceNormals * 0.5f + 0.5f; \noutput.depth.w = output.position.w / 50.0f; \n \nDepending on the depth precision required for your scene, you can choose an appropriate \nimage format—either 16 or 32 bits. This sample implementation utilizes 16-bit formats. \nRaw Occlusion Buffer Generation \nNext, we generate the raw occlusion buffer in the compute shader. This represents the \nheart of the SSAO algorithm. As mentioned earlier, we will utilize two different Thread \nGroup sizes. The occlusion calculations will be performed in Thread Groups of size 16×16×1 \nand 32×32×1. Since we can adjust the number of Thread Groups executed in the \napplication‘s Dispatch call, either Thread Group size can be used to generate the raw \nocclusion buffer. However, if there is any performance difference between the two Thread \nGroup sizes, this will provide some insight into the proper usage of the compute shader. \nRegardless of the size of the Thread Groups, each one will generate one portion of the raw \nocclusion buffer equivalent to its size. Each thread will calculate a single pixel of the raw \nocclusion buffer that corresponds to the thread‘s Dispatch thread ID system value. This \nDispatch thread ID is also used to determine the appropriate location in the depth/normal \nbuffer to load. The depth value and normal vector are loaded from the texture and \nconverted back into their original formats for use later. \nDepth Value Cache with the GSM \nWe will also set up the compute shader to cache local depth values in the GSM. Once the \ndepth values of the surrounding area are loaded into the GSM, all subsequent depth \nsampling can be performed on the GSM instead of loading directly from texture memory. \nBefore we discuss how to set up and use the GSM, we need to consider the desired layout \nfor the data. Since we are utilizing two different Thread Group sizes, we will specify a \ndifferent layout for each. Each of the Thread Groups requires the corresponding depth \nregion that it represents to be present in the GSM. In addition, the area surrounding the \nThread Group‘s boundary is also needed to allow the occlusion calculations for the border \npixels to be carried out correctly. This requires each thread to sample not only its own \ndepth/normal vector, but also some additional depth values to properly load the GSM for \n\n\n \n \nuse later. If we stipulate that each thread will load four depth values into the GSM, then our \n16×16 thread group will provide a 32×32 overall region in the GSM (the original 16×16 \nblock with an 8-pixel boundary). The 32×32 Thread Group size will provide a 64×64 region \n(the original 32×32 block with a 16-pixel boundary). \nFortunately, the Gather instruction can be utilized to increase the number of depth values \nthat are sampled for each thread. The Gather instruction returns the four point-sampled \nsingle component texture samples that would normally have been used for bilinear \ninterpolation—which is perfect for pre-loading the GSM since we are using only single \ncomponent depth values. This effectively increases the number of depth samples per \ntexture instruction by a factor of 4. If we use each thread to perform a single Gather \ninstruction, then we can easily fill the required areas of 32×32 and 64×64. The required \nsamples are obtained by having each thread perform the Gather instruction and store the \nresults in the GSM for all other threads within the group to utilize. This is demonstrated in \nListing 1.5.2. \nListing 1.5.2. Declaring and populating the Group Shared Memory with depth data \n#define USE_GSM \n \n#ifdef USE_GSM \n    // Declare enough shared memory for the padded thread group \nsize \n    groupshared float LoadedDepths[padded_x][padded_y]; \n#endif \n \n    int3 OffsetLocation = \n        int3( GroupID.x*size_x - kernel_x, GroupID.y*size_y - \nkernel_y, 0 ); \n    int3 ThreadLocation = GroupThreadID * 2; \n \n    float2 fGatherSample; \n    fGatherSample.x = ((float)GroupID.x * (float)size_x – \n        (float)kernel_x + (float)GroupThreadID.x * 2.0f ) / \nxres; \n    fGatherSample.y = ((float)GroupID.y * (float)size_y – \n        (float)kernel_y + (float)GroupThreadID.y * 2.0f ) / \nyres; \n \n    float4 fDepths = DepthMap.GatherAlpha( DepthSampler, \nfGatherSample + \n        float2( 0.5f / (float)xres, 0.5f / (float)yres ) ) * zf; \n \n    LoadedDepths[ThreadLocation.x][ThreadLocation.y] = \nfDepths.w; \n    LoadedDepths[ThreadLocation.x+1][ThreadLocation.y] = \nfDepths.z; \n    LoadedDepths[ThreadLocation.x+1][ThreadLocation.y+1] = \nfDepths.y; \n    LoadedDepths[ThreadLocation.x][ThreadLocation.y+1] = \nfDepths.x; \n \n    GroupMemoryBarrierWithGroupSync(); \n\n\n \n \n \nThe number of depth values loaded into the GSM can be increased as needed by having \neach thread perform additional Gather instructions. The Group Shared Memory is defined as \na 2D array corresponding to the size of the area that will be loaded and cached. After all of \nthe depth values have been loaded, we introduce a synchronization among threads in the \nThread Group with the GroupMemoryBarrierWithGroupSync() intrinsic function. \nThis function ensures that all threads have finished writing to the GSM up to this point in \nthe compute shader before continuing execution. \nA compile-time switch is provided in the sample code to allow switching between filling the \nGSM to use the cached depth values or to directly access the depth texture. Since the GSM \nhas the potential to improve the sampling performance depending on the access pattern, \nthis will allow an easy switch between techniques for a clear efficiency comparison. \nNext, we initialize the randomization of the sampling kernel with the lowest four bits of the \nDispatch thread ID x and y coordinates, as shown in Listing 1.5.3. The lowest four bits in \neach direction are used to select a reflection vector from a 2D array of rotation vectors, \nwhich are predefined and stored in a constant array. This eliminates the need for a separate \ntexture and range expansion calculations, but it requires a relatively large array to be \nloaded when the compute shader is loaded. After it is selected, the reflection vector is then \nused to modify the orientation of the sampling kernel by reflecting each of the kernel \nvectors about the reflection vector. This provides a different sampling kernel for each \nconsecutive pixel in the occlusion buffer. \nListing 1.5.3. Definition of the sampling kernel and selection of the randomization \nvector \nconst float3 kernel[8] = \n{ \n    normalize( float3(  1, 1, 1 )), \n    normalize( float3( -1,-1,-1 )), \n    normalize( float3( -1,-1, 1 )), \n    normalize( float3( -1, 1,-1 )), \n    normalize( float3( -1, 1, 1 )), \n    normalize( float3(  1,-1,-1 )), \n    normalize( float3(  1,-1, 1 )), \n    normalize( float3(  1, 1,-1 )) \n}; \n \nconst float3 rotation[16][16] = \n{ \n    { {...},{...},{...},{...}, ... } \n}; \n \nint rotx = DispatchThreadID.x & 0xF; \nint roty = DispatchThreadID.y & 0xF; \nfloat3 reflection = rotation[rotx][roty]; \n \nWith a random reflection vector selected, we can begin the iteration process by sampling a \ndepth value at the location determined by the randomized sampling kernel offsets. The \n\n\n \n \nsample location is found by determining the current pixel‘s view space 3D position and then \nadding the reoriented sampling kernel vectors as offsets from the pixel‘s location. This new \nview space position is then converted back to screen space, producing an (x, y) coordinate \npair that can then be used to select the depth sample from either the GSM or the \ndepth/normal texture. This is shown in Listing 1.5.4. \nListing 1.5.4. Sampling location flipping and re-projection from view space to \nscreen space \nfloat3 vRotatedOffset = \n        reflect( kernel[y], rotation[rotx][roty] ); \n \n    float fSign = dot( fPixelNormal, vRotatedOffset ); \n    if ( fSign < 0.0f ) \n        vFlippedOffset = -vFlippedOffset; \n \n    float3 Sample3D = PixelPosVS + vFlippedOffset * scale; \n    int3 newoffset = ViewPosToScreenPos( Sample3D ); \n \n#ifndef USE_GSM \n    float fSample = DepthMap.Load( iNewOffset ).w * zf; \n#else \n    float fSample = LoadDepth( iNewOffset - OffsetLocation ); \n#endif \n \nThe pixel‘s view space normal vector is used to determine whether the kernel offset vector \npoints away from the current pixel. If so, then the direction of the offset vector is negated \nto provide an additional sample that is more relevant for determining occlusion. This \nprovides additional samples in the visible hemisphere of the pixel, which increases the \nusable sample density for the pixel. The final screen space sample location is then used to \nlook up the depth sample either directly from the texture or from the GSM by calling the \nLoadDepth() function. \nAfter the depth has been loaded, the occlusion at the current pixel from this sample is \ncalculated. The calculation that is used is similar to the one presented in [Filion08] and \n[Lake10], using a linear occlusion falloff function raised to a power. This produces a smooth \ngradual falloff from full occlusion to zero occlusion and provides easy-to-use parameters for \nadjusting the occlusion values. \nThe partial occlusion calculation is repeated for a given number of samples, implemented as \na multiple of the number of elements in the sampling kernel. In this implementation, the \nnumber of samples can be chosen in multiples of eight. All of these individual occlusion \nvalues are averaged and then stored in the raw occlusion buffer for further processing. \nSeparable Bilateral Filter \nThe final step in our occlusion value generation is to perform the bilateral blur. As described \nearlier, we are able to use a separable version of the filter, even though it is not perfectly \naccurate to do so. The bilateral filter passes are implemented in the compute shader, with \neach of the separable passes being performed in an individual dispatch call. Since we are \nonly processing one direction at a time, we will first use one Thread Group for each row of \nthe image and then process the resulting image with one Thread Group for each column of \nthe image. In this arrangement, we can load the entire contents of a Thread Group‘s row or \n\n\n \n \ncolumn into the GSM, and then each thread can directly read its neighbor values from it. \nThis should minimize the cost of sampling a texture for filtering and allow larger filter sizes \nto be used. This implementation uses 7×7 bilateral filters, but this can easily be increased \nor decreased as needed. Listing 1.5.5 shows how the separable filter pass loads its data into \nthe GSM. \nListing 1.5.5. Loading and storing the depth and occlusion values into the GSM for \nthe horizontal portion of a separable bilateral filter \n// Declare enough shared memory for the padded group size \ngroupshared float2 horizontalpoints[totalsize_x]; \n... \n \nint textureindex = DispatchThreadID.x + DispatchThreadID.y * \ntotalsize_x; \n \n// Each thread will load its own depth/occlusion values \nfloat fCenterDepth = DepthMap.Load(DispatchThreadID).w; \nfloat fCenterOcclusion = AmbientOcclusionTarget[textureindex].x; \n \n// Then store them in the GSM for everyone touse \nhorizontalpoints[GroupIndex].x = fCenterDepth; \nhorizontalpoints[GroupIndex].y = fCenterOcclusion; \n \n// Synchronize all threads \nGroupMemoryBarrierWithGroupSync(); \n \nOne thread is declared for each pixel of the row/column, and each thread loads a single \nvalue out of the raw occlusion buffer and stores that value in the GSM. Once the value has \nbeen stored, a synchronization point is used to ensure that all of the memory accesses have \ncompleted and that the values that have been stored can be safely read by other threads. \nThe bilateral filter weights consist of two components: a spatially based weighting and a \nrange-based weighting. The spatial weights utilize a fixed Gaussian kernel with a size of 7 \ntaps in each direction. A separate Gaussian weighting value is calculated based on the \ndifference between the center pixel and each of the samples to determine the weighting to \napply to that sample. Modifying the sigma values used in the range-based Gaussian allows \nfor easy adjustment of the range-filtering properties of the bilateral filter. Listing 1.5.6 \nshows how this calculation is performed. \nListing 1.5.6. Horizontal portion of a separable bilateral filter in the compute \nshader \nconst float avKernel7 = \n{ 0.004431f, 0.05402f, 0.2420f, 0.3990f, 0.2420f, 0.05402f, \n0.004431f }; \n \nconst float rsigma = 0.0051f; \nfloat fBilateral = 0.0f; \nfloat fWeight = 0.0f; \n \nfor ( int x = -3; x <= 3; x++ ) \n\n\n \n \n{ \n    int location = GroupIndex + x; \n    float fSampleDepth = horizontalpoints[location].x; \n    float fSampleOcclusion = horizontalpoints[location].y; \n \n    float fDelta = fCenterDepth - fSampleDepth; \n    float fRange = \n        exp( ( -1.0f * fDelta * fDelta ) / ( 2.0f * rsigma * \nrsigma ) ); \n \n    fBilateral += fSampleOcclusion * fRange * avKernel[x+3]; \n    fWeight += fRange * avKernel[x+3]; \n} \n \nAmbientOcclusionTarget[textureindex] = fBilateral / fWeight; \n \nFinally, once both passes of the bilateral filter are performed, the values written to the final \noutput buffer can be used to supply the ambient lighting term of an output image. The \nvalue stored in the occlusion buffer represents visibility, and thus can be directly used as \nthe ambient lighting term without modification. The sample implementation provides its \noutput using only the occlusion value—no other lighting is applied to the scene. \n \nResults \nFigure 1.5.9 shows the end result of our compute shader–based implementation of the \nalgorithm. All images and performance numbers were generated on an AMD 57xx series \nGPU. The following image was generated using 32 depth samples for each occlusion pixel \nand with the 7×7 separable bilateral filter applied twice. \nFigure 1.5.9. Final results of the compute shader SSAO implementation. \n \n \nTo gain some insight into how well the new implementation techniques perform, we can \nreview the overall frame time for each of our optional configurations. Table 1.5.1 provides \n",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 69-86)",
      "start_page": 69,
      "end_page": 86,
      "detection_method": "topic_boundary",
      "content": " \n \nthe performance metrics for the two Thread Group sizes for varying numbers of samples \nused in the occlusion calculation. These frames-per-second figures were generated with the \ndirect sampling technique on a 640×480 render target with no bilateral filtering applied. \nTable 1.5.1.  \nNO GSM 8 \n16 \n24 \n32 \n40 \n48 \n56 \n64 \n16×16 \n947 608 437 349 288 241 212 188 \n32×32 \n961 610 445 350 288 245 214 187 \n \nThe two Thread Group sizes produce nearly identical performance numbers. This indicates \nthat the Thread Group size does not have a significant impact on this type of iterative \nalgorithm. When considering the actual frame time for each test (the inverse of the fps), we \nsee a linear increase in frame time for each of the additional sets of samples. In \ncomparison, Table 1.5.2 provides the same metrics generated with the GSM caching \ntechnique. \nTable 1.5.2.  \nW/GSM 8 \n16 \n24 \n32 \n40 \n48 \n56 \n64 \n16×16 811 699 531 426 357 306 269 240 \n32×32 764 625 485 391 330 286 252 225 \n \nTable 1.5.2 shows a different performance characteristic. When compared to the direct \nloading technique, we see the GSM technique performs slower at the 8-sample level. \nHowever, for all sample levels above this, the GSM technique significantly outperforms the \ndirect sampling method. For all of these higher sampling levels, we see a similar linear \nincrease in frame time but a smaller slope than the direct sampling method. Figure 1.5.10 \nshows the frame times for the four different cases. \nFigure 1.5.10. Comparison of frame times with and without the GSM as a cache. \n\n\n \n \n \nThe slower performance with the GSM at lower sampling rates can be attributed to the \noverhead of loading and storing all of the additional depth data. However, there is a clear \nperformance gain for each additional sample used in the occlusion calculation. \nWith this performance advantage also comes some limitations. In both Thread Group sizes, \nwe defined a fixed border size. In some cases, when a pixel is close to the viewer, the offset \nvector can produce a screen space offset much larger than this border size. This can be \novercome either by scaling the size of the sampling kernel according to the distance from \nthe camera or by dynamically determining whether the sample location is available in the \nGSM and directly loading the sample if needed. \n \nConclusion and Future Work \nIn this chapter, we have applied the compute shader to the Screen Space Ambient \nOcclusion algorithm and discussed the implications of various implementation choices. This \nimplementation provides a basic framework upon which further proposed extensions can be \nimplemented relatively easily. Additional research can be directed at sharing partial \nocclusion values between neighboring pixels for each occlusion calculation, which is now \npossible due to the scatter capabilities of the compute shader. In addition, further \nexploration on the use of the GSM as a caching mechanism for regional depth averages \ncould be investigated. Finally, there have been several recent findings using multi-resolution \nrendering solutions for SSAO, which should also benefit from the compute shader \nimplementations. \n \nReferences \n[Bavoil09a] Bavoil, Louis and Miguel Sainz. ―Image-Space Horizon-Based Ambient \nOcclusion.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: \nCharles River Media, 2009. Section 6.2. \n\n\n \n \n[Bavoil09b] Bavoil, Louis and Miguel Sainz. ―Multi-Layer Dual-Resolution Screen-Space \nAmbient Occlusion.‖ 2009. SlideShare. n.d. <http://www.slideshare.net/NVIDIA/multilayer-\ndualresolution-screenspace-ambient-occlusion>. \n[Bunnell05] Bunnell, Michael. ―Dynamic Ambient Occlusion and Indirect Lighting.‖ 2005. \nGamedev.net. n.d. <http://downloads.gamedev.net/pdf/Pharr_ch14.pdf>. \n[Filion08] Filion, Dominic and Rob McNaughton. ―Effects and Techniques.‖ Course on \nAdvances in Real-Time Rendering in 3D Graphics and Games Course. SIGGRAPH 2008. Los \nAngeles Convention Center, Los Angeles, CA. 11 August 2008. \n[Kajalin09] Kajalin, Vladimir. ―Screen Space Ambient Occlusion.‖ ShaderX7: Advanced \nRendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. Section \n6.1. \n[Lake10] Lake, Adam, ed. Game Programming Gems 8. Boston: Charles River Media, 2010. \n[Landis02] Landis, Hayden. ―Production-Ready Global Illumination.‖ Course notes on \nRenderMan in Production. SIGGRAPH 2002. Henry B. Gonzalez Convention Center, San \nAntonio, TX. 21 July 2002. Chapter 5. \n[Mittring07] Mittring, Martin. ―Finding Next Gen – CryEngine 2.0.‖ Course notes on \nAdvanced Real-Time Rendering in 3D Graphics and Games. SIGGRAPH 2007. San Diego \nConvention Center, San Diego, CA. 8 August 2007. 97–121. \n[Pham05] Pham, T.Q. and L.J. van Vliet. ―Separable Bilateral Filtering for Fast Video \nProcessing.‖ IEEE International Conference on Multimedia and Expo. Amsterdam, The \nNetherlands. July 2005. \n[Ritschel09] Ritschel, Tobias, Thorsten Grosch, and Hans-Peter Seidel. ―Approximating \nDynamic Global Illumination in Image Space.‖ Proceedings of the 2009 Symposium on \nInteractive 3D Graphics and Games (2009): 75–82. \n[Tomasi98] Tomasi, Carlo and Roberto Manduchi. ―Bilateral Filtering for Gray and Color \nImages.‖ Proceedings of the Sixth International Conference on Computer Vision. (1998): \n839–846. \n \n1.6. Eye-View Pixel Anti-Aliasing for Irregular Shadow Mapping \nNico Galoppo, Intel Advanced Visual Computing (AVC) \nnico.galoppo@intel.com \nThe irregular shadows algorithm (also known as Irregular Z-Buffer shadows) combines the \nimage quality and sampling characteristics of ray-traced shadows with the performance \nadvantages of depth buffer–based hardware pipelines [Johnson04]. Irregular shadows are \nfree from aliasing from the perspective of the light source because the occlusion of each \neye-view sample is evaluated at sub-pixel precision in the light view. However, irregular \nshadow mapping suffers from pixel aliasing in the final shadowed image due to the fact that \nshadow edges and high-frequency shadows are not correctly captured by the resolution of \nthe eye-view image. Brute-force super-sampling of eye-view pixels decreases shadow \naliasing overall but incurs impractical memory and computational requirements. \nIn this gem, we present an efficient algorithm to compute anti-aliased occlusion values. \nRather than brute-force super-sampling all pixels, we propose adaptively adding shadow \n\n\n \n \nevaluation samples for a small fraction of potentially aliased pixels. We construct a \nconservative estimate of eye-view pixels that are not fully lit and not fully occluded. Multiple \nshadow samples are then inserted into the irregular Z-buffer based on the footprint of the \nlight-view projection of potentially aliased pixels. Finally, the individual shadow sample \nocclusion values are combined into fractional and properly anti-aliased occlusion values. Our \nalgorithm requires minimal additional storage and shadow evaluation cost but results in \nsignificantly better image quality of shadow edges and improved temporal behavior of high-\nfrequency shadow content. \nPreviously, architectural constraints of traditional GPUs have inhibited per-frame \nconstruction and traversal of irregular data structures in terms of both performance and \nprogrammer flexibility. Our implementation of anti-aliased irregular shadow mapping \nexploits many strengths of the Larrabee architecture, one of which is the ability to write to \nrun-time computed addresses in global memory space. Additionally, we were able to do so \nusing the conventional C programming model and incorporate the adaptive nature of our \ntechnique with little effort. In comparison, traditional GPU architectures do not offer \nprogramming semantics for such global scatter operations, or they do so at extremely low \nperformance due to their highly specialized but constrained (localized) memory hierarchy \n(for example, the CUDA programming model), in the worst case falling back to main \nmemory writes [Sintorn08, Baumann05]. \nBackground and Problem: Shadow Edge Aliasing \nIn this section, we‘ll describe the characteristics of various popular shadow generation \nalgorithms and how they cope with different forms of aliasing, and we‘ll describe the \nproblem of screen-space shadow edge aliasing, which affects many current algorithms. \nPixel-Perfect Shadows with the Irregular Z-Buffer \nConventional shadow mapping renders the scene from the eye and the light, and in the final \ncompositing pass, the two views are compared to identify points that are in shadow \n[Williams78]. Light-view aliasing results from misalignment of these two views, as shown in \nFigure 1.6.1(a). There are several variants of shadow mapping that reduce but do not \neliminate sampling and self-shadowing artifacts [Fernando01, Stamminger02, Sen03, \nLloyd08, Lefohn07], because none of them resolves the fundamental mismatch in sampling \npatterns between the eye and light views, which is the root cause of most shadow mapping \nartifacts. \nFigure 1.6.1. Conventional versus irregular shadow mapping. In conventional \nshadow mapping (left), both the eye-view and light-view images are rendered \nwith the classic Z-buffer, leading to a mismatch between the desired and actual \nsample locations in the shadow map. Irregular shadow mapping (right) avoids this \nmismatch by rendering the light-view image with the irregular Z-buffer. \n\n\n \n \n \nIrregular shadow mapping addresses the root cause of visual artifacts in conventional \nshadow mapping by basing the light-view sampling pattern on the positions of pixels in the \neye-view raster and their corresponding depth values, therefore perfectly aligning the \ncompared occluder surface point with the projection of the shadow sample, as illustrated in \nFigure 1.6.1(b) [Johnson04, Johnson05]. The density of shadow samples varies significantly \nacross the image plane (as seen in Figure 1.6.2), which illustrates the need for an irregular \ndata structure during the light pass. \nFigure 1.6.2. The classic Z-buffer (a) samples a scene at regularly spaced points \non the light image plane. The irregular Z-buffer (b) samples a scene at arbitrary \npoints on the light image plane. Irregular shadow mapping (d) eliminates aliasing \nartifacts typically associated with conventional shadow mapping (c). \n \nIrregular shadow mapping utilizes the irregular Z-buffer in this context. This data structure \nexplicitly stores all of the sample locations in a two-dimensional spatial data structure rather \nthan implicitly representing them with a regular pattern. The data structure can be any \nspatial data structure that supports efficient range queries, such as a k-d tree or a grid. Just \nas in conventional shadow mapping, irregular shadow mapping projects triangles onto the \nlight-view image plane one at a time and then determines which samples lie inside a \ntriangle. Unlike conventional shadow mapping, this determination is made by querying the \nirregular Z-buffer. Finally, for each sample inside a triangle, irregular shadow mapping \nperforms the standard depth comparison and updates the sample‘s occlusion value. \n\n\n \n \nNote that when a conventional rasterizer is used during light-view projection of occluder \ntriangles, it is necessary to scan-convert expanded triangles to ensure fragments will be \ngenerated for any cell touched by the unexpanded triangle (also known as conservative \nrasterization [Akenine-Möller05]), since irregular Z-buffer samples may lie anywhere within \nthe cell bounds, as illustrated in Figure 1.6.3. For reference, [Hasselgren05] describes a \nshader implementation with example code. On the other hand, the advantage of a software \nrasterizer (for example, on Larrabee) is that a special rasterization path can be \nimplemented to apply custom rasterization rules that enable conservative rasterization \ndirectly without triangle expansion. \nFigure 1.6.3. Conservative rasterization versus conventional rasterization. Scan-\nconverted triangles have to be expanded during light-view projection to ensure \nfragments will be generated for any cell touched by the unexpanded triangle \n(shaded cells), since irregular Z-buffer samples (circles) may lie anywhere within \nthe pixel bounds. \n \n \n \nEye-View Aliasing \nWhile irregular shadow mapping is free of light-view aliasing, it still suffers from eye-view \naliasing of pixels, as illustrated in Figure 1.6.4. Such aliasing is a common problem in \ncomputer graphics. For example, it is also encountered in ray casting with a single eye ray \nper pixel. The problem is that thin geometry (high-frequency screen content) cannot be \ncaptured by a single ray, because the rays of two neighboring pixels may miss some \ngeometry even though the geometry projects to part of those pixels. Similarly, in the case \nof shadows in a rasterizer, it is possible that a surface point projected to the center of an \neye-view pixel is lit, but the entire area of the pixel is not lit. This phenomenon, known as \neye-view shadow aliasing, is caused by the fact that a single bit occlusion value is not \nsufficient to represent the occlusion value of aliased pixels. Anti-aliased occlusion values are \nfractional values that represent the fraction of the total pixel area that is lit. Recently, a few \nnovel shadow mapping techniques [Brabec01, Lauritzen06, Salvi08] have addressed this \nproblem and provide good solutions for eye-view aliasing but still expose light-view aliasing. \nFigure 1.6.4. The thin geometry in the tower causes eye-view aliasing of the \nprojected shadow. Note that some of the tower’s connected features are \ndisconnected in the shadow. \n\n\n \n \n \n \nThe most obvious approach to produce anti-aliased shadows with irregular shadow mapping \nis super-sampling of the entire screen by generating and evaluating multiple shadow \nsamples for each eye-view pixel. The anti-aliased occlusion value for a pixel is then simply \nthe average of the individual sample occlusion values. While this brute-force approach \ncertainly works, as illustrated in Figure 1.6.5, the computational and storage costs quickly \nbecome impractical. Data structure construction, traversal times, and storage requirements \nof the irregular Z-buffer are proportional to the number of shadow samples, making real-\ntime performance impossible on current hardware for even as little as four shadow samples \nper pixel. \nFigure 1.6.5. A four-times super-sampled irregular shadow mapping result image \nof the tower scene. \n \n \nThe recent method by [Robison09] provides a solution to compute anti-aliased shadows \nfrom the (aliased) output of irregular shadow mapping, but in essence it is also a brute-\nforce approach in screen space that does not exploit the irregular Z-buffer acceleration \nstructure and is therefore at a computational disadvantage compared to our approach. \n \n\n\n \n \nSolution: Adaptive Multi-Sampling of Irregular Shadows \nWe observed in Figure 1.6.5 that accumulating shadow evaluation results of multiple \nsamples per eye-view pixels provides a nice anti-aliased shadow and that potentially \nshadow-aliased pixels are those pixels that lie on a projected shadow edge. Therefore, we \npropose an efficient algorithm for anti-aliased irregular shadow mapping by adaptive multi-\nsampling of only those pixels that potentially lie on a shadow edge. Since only a marginal \nfraction of all screen pixels are shadow-edge pixels, this approach results in substantial \ngains in computational and storage costs compared to the brute-force approach. \nEssentially, our method is an extension of the original irregular shadow mapping algorithm, \nwhere the irregular Z-buffer acceleration structure remains a light space–oriented \nacceleration structure for the projected eye-view shadow samples. However, during \nirregular Z-buffer construction, potential shadow edge pixels are detected using a \nconservative shadow edge stencil buffer. Such pixels generate multiple shadow samples \ndistributed over the pixel‘s extent and are inserted in the irregular Z-buffer (shadow sample \nsplatting). Non-shadow-edge pixels are treated just as in the original irregular shadow \nmapping algorithm—a single shadow sample is sufficient to detect the occlusion value of the \nentire pixel. In the final shadow evaluation step, shadow occlusion values are averaged over \neach eye-view pixel‘s sample, resulting in a properly anti-aliased fractional occlusion value. \nThis value approximates the fraction of the pixel‘s area that is occluded, and it goes toward \nthe true value in the limit as the number of samples per pixel increases. \nAlgorithm: Anti-Aliased Irregular Shadow Mapping \nWe will now give an overview of the complete algorithm to provide structure to the \nremainder of the algorithm description in this section. Then we describe how to determine \nwhich pixels are potentially aliased by constructing a conservative shadow edge stencil \nbuffer and how to splat multiple samples into the irregular Z-buffer efficiently. Finally, we \nput it all together and present the complete algorithm in practice. \nWe can formulate our approach in the following top-level description of our algorithm: \n1.  Render the scene conservatively from the light‘s point of view to a variance shadow \nmap. \n2.  Render the scene from the eye point to a conventional Z-buffer—depth values only \n(gives points P0). \n3.  Construct a conservative shadow edge stencil buffer using a variance shadow map and \nlight-space projection of P0. \n4.  Using the stencil in Step 3, generate N extra eye-view samples Pi for potential shadow \nedge pixels only. \n5.  Transform eye-view samples Pi to light space P‘i (shadow sample splatting). \n6.  Insert all samples P‘i in the irregular Z-buffer. \n7.  Render the scene from the light‘s point of view while testing against samples in the \nirregular Z-buffer, tagging occluded samples. \n8.  Render the scene from the eye point, using the result from Step 7 and the conservative \nshadow edge stencil buffer. Multi-sampled eye-view pixels accumulate shadow sample \n\n\n \n \nvalues into a fractional (anti-aliased) shadow value. \nConservative Shadow Edge Stencil Buffer \nTo adaptively add shadow samples at shadow-edge pixels, we construct a special stencil \nbuffer that answers the following question: Is there any chance that this eye-space pixel is \npartially occluded by geometry in this light-space texel? We call this stencil buffer the \nconservative shadow edge stencil buffer. Giving an exact answer to the aforementioned \nquestion is impossible because it is essentially solving the shadowing problem. However, we \ncan use a probabilistic technique to answer the question conservatively with sufficient \nconfidence. A conservative answer is sufficient for our purpose, since multi-sampling of non-\nshadow-edge pixels does not alter the correctness of the result—it only adds some extra \ncost. Obviously, we strive to make the stencil buffer only as conservative as necessary. \nWe employ a technique called Variance Shadow Mapping [Lauritzen06]. Variance shadow \nmaps encode a distribution of depths at each light-space texel by determining the mean and \nvariance of depth (the first two moments of the depth distribution). These moments are \nconstructed through mip-mapping of the variance shadow map. When querying the variance \nshadow map, we use these moments to compute an upper bound on the fraction of the \ndistribution that is more distant than the surface being shaded, and therefore this bound \ncan be used to cull eye-view pixels that have very little probability to be in shadow. \nIn particular, the cumulative distribution function F(t) = P(x ≥ t) can be used as a measure \nof the fraction of the eye-view fragment that is lit, where t is the distance of the eye-view \nsample to the light, x is the occluder depth distribution, and P stands for the probability \nfunction. While we cannot compute this function F(t) exactly, Chebyshev‘s inequality gives \nan upper bound: \n \nThe upper bound Pmax (t) and the true probability Plit (t) are depicted in Figure 1.6.6. Thus, \nwe can determine that it is almost certain that a projected eye-view sample with light depth \nt is in shadow (for example, with 99-percent certainty) by comparing Pmax to 1% (Pmax < \nimplies Plit (t) < 0.01). \nFigure 1.6.6. Pin shadow (t) and Plit (t), in addition to their conservative upper bounds \nPmax (t) and p′max (t). \n \n\n\n \n \nConversely, we can use the same distribution to construct a bound to cull eye-view pixels \nthat have very high probability of being lit: \n \nIn summary, the conservative shadow edge stencil buffer can be constructed in the \nfollowing steps: \n1.  Render the scene from the light‘s point of view, writing out depth x and depth squared \nx2 to a variance shadow map texture (VSM). \n2.  Mip-map the resulting texture, effectively computing E(x) and E(x2), the mean and \nvariance of the depth distribution. \n3.  Render the scene from the eye point, computing for each sample: \na. The light depth t by projection of the sample to light space. \nb. E(x) and E(x2) by texture-sampling the mip-mapped VSM with the appropriate \nfilter width, determined by the extent of the light projection of the pixel area. \nc. μ = E(x), ζ2 = E(x2) – E(x) and Pmax (t), p′max (t) \n4.  Compare Pmax (t) and p′max (t) to a chosen threshold (for example, 1 percent). Set the \nstencil buffer bit if either one is smaller than the threshold. \nThese steps can be implemented in HLSL shader pseudocode, as shown in Listing 1.6.1. \nListing 1.6.1. Conservative shadow edge stencil buffer construction HLSL shader \nfloat2 ComputeMoments(float Depth) \n{ \n    // Compute first few moments of depth \n    float2 Moments; \n    Moments.x = Depth; \n    Moments.y = Depth * Depth; \n    return Moments; \n} \n \nfloat ChebyshevUpperBound( \n  float2 moments, float mean, float minVariance) \n{ \n    // Compute variance \n    float variance = max( \n               minVariance, \n               moments.y - (moments.x * moments.x)); \n    float d        = mean - moments.x; \n    float pMax     = variance / (variance + (d * d)); \n \n    // One-tailed Chebyshev's Inequality \n\n\n \n \n    return (mean <= moments.x ? 1.0f : pMax); \n} \n \nbool IsPotentialShadowEdge(float2 texCoord, \n                           float2 texCoordDX, \n                           float2 texCoordDY, \n                           float  depth) \n{ \n    float4 occluderData; \n    // Variance Shadow Map mip-mapped LOD tex lookup \n    occluderData = texShadowMap.SampleGrad( \n      sampShadowMap, \n      texCoord, texCoordDX, texCoordDY); \n \n    float2 posMoments = occluderData.xy; \n \n    // Minimum variance to take account for variance \n    // across entire pixel \n    float gMinVariance = 0.000001f; \n \n    float pMaxLit = ChebyshevUpperBound( \n        posMoments, depth, gMinVariance); \n    float pMaxShadow = ChebyshevLowerBound( \n        posMoments, depth, gMinVariance); \n \n    if (PMaxLit < .01 || PMaxShadow < .01) { \n      return true; \n    } \n \n    return false; \n} \n \nNote that while conventional rasterization of the scene from the light‘s point of view in Step \n1 earlier is sufficient for generating a conventional variance shadow map, it is not sufficient \nfor generating our conservative stencil buffer. Conventional rasterization does not guarantee \nthat a primitive‘s depth contributes to the minimum depth of each light-view texel that it \ntouches. Hence, the conservativeness of the stencil buffer would not be preserved as \nillustrated in Figure 1.6.7, which depicts potential shadow-edge pixels in overlay but misses \nquite a few due to the low resolution of the variance shadow map. \nFigure 1.6.7. Conservative shadow edge stencil map with regular rasterization. \nMany potential shadow-edge pixels (overlay) are missed due to low resolution of \nthe variance shadow map. \n\n\n \n \n \n \nTo preserve conservativeness, it is required to perform conservative rasterization in the \nlight-view render of Step 1, just as we do during the light-view render of irregular shadow \nmapping illustrated in Figure 1.6.3. Figure 1.6.8 depicts correctly detected potential \nshadow-edge pixels in overlay, regardless of the variance shadow map resolution. \nFigure 1.6.8. Conservative shadow edge stencil map with conservative \nrasterization. All potential shadow-edge pixels are detected (overlay), regardless \nof the variance shadow map resolution. \n \n \n \nShadow Sample Splatting \nIn the irregular Z-buffer construction phase, when the time comes to generate additional \nsamples for potentially aliased pixels, as defined by the conservative shadow edge stencil \nbuffer, we insert the eye-view samples in each light-view grid cell that is touched by the \npixel samples. We call this process shadow sample splatting, because we conceptually splat \nthe projection of the pixel footprint into the light space grid data structure. This process is \nas follows: \n1. In addition to light view coordinates of the eye-view pixel center, also generate \nmultiple samples per eye-view pixel. We have achieved very good results with \n\n\n \n \nrotated grid 4× multi-sampling, but higher sample rates and even jittered sampling \nstrategies can be used to increase the quality of the anti-aliasing. \n2. Project all samples into light space as in the original irregular shadow algorithm. \n3. Insert all samples into the irregular Z-buffer as in the conventional irregular \nshadowing algorithm. Potentially multiple light grid cells are touched by the set of \nsamples of a pixel. \n \nResults and Discussion \nWe will now show the results of our algorithm for two different scenes. The first scene \nconsists of a tower construction with fine geometry casting high-frequency shadows onto \nthe rest of the scene. The second scene is the view of a fan at the end of a tunnel, viewed \nfrom the inside. The fan geometry casts high-frequency shadows on the inside walls of the \ntunnel. The tunnel walls are almost parallel to the eye and light directions, a setup that is \nparticularly hard for many shadow mapping algorithms. Irregular shadow mapping shows its \nstrength in the tunnel scene because no shadow map resolution management is required to \navoid light-view aliasing. However, severe eye-view aliasing artifacts are present for the \nsingle-sample irregular shadow algorithm (see Figures 1.6.10(a) and 1.6.11(a)). Figure \n1.6.9 illustrates the result of computing the conservative shadow edge stencil buffer on both \nscenes: Potential shadow edge pixels are rendered with an overlay. Figure 1.6.10 compares \nsingle-sample irregular shadows with 4× rotated grid multi-sampling on potential shadow \nedge pixels only. Note the significant improvement in the tower shadow, where many \ndisconnected features in the shadow are now correctly connected in the improved \nalgorithm. Figure 1.6.11 illustrates the same comparison for the tunnel scene. There is a \ngreat improvement in shadow quality toward the far end of the tunnel, where high-\nfrequency shadows cause significant aliasing when using only a single eye-view shadow \nsample. \nFigure 1.6.9. Result of the conservative shadow edge stencil buffer on the tower \n(a) and tunnel (b) scenes. Potential shadow edge pixels are rendered with an \noverlay. \n \n \nFigure 1.6.10. Tower scene: (a) Single-sample irregular shadows and (b) 4× \nrotated grid multi-sampling on potential shadow edge pixels only. Note the \nsignificant improvement in the tower shadow, where many disconnected features \nin the shadow are now correctly connected in the improved algorithm. \n\n\n \n \n \n \nFigure 1.6.11. Tunnel scene: (a) Single-sample irregular shadows and (b) 4× \nrotated grid multi-sampling on potential shadow edge pixels only. There is a great \nimprovement in shadow quality toward the end of the tunnel, where high-\nfrequency shadows caused significant anti-aliasing when using only a single eye-\nview shadow sample. \n \n \n \nImplementation Details \nOn Larrabee, we have implemented Steps 2 and 3 of our algorithm in an efficient post-\nprocess over all eye-view pixels in parallel. However, Step 3 is identical to the conventional \nirregular shadowing algorithm; therefore, it could be implemented as in [Arvo07] as well. \nConceptually, we use a grid-of-lists representation for the irregular Z-buffer. This \nrepresentation is well-suited to parallel and streaming computer architectures and produces \nhigh-quality shadows in real time in game scenes [Johnson05]. The following chapter of this \nbook [Hux10], in particular Figure 1.7.1, explains our grid-of-lists representation and its \nconstruction in more detail. \nFinally, our solution was implemented in a deferred renderer, but it could also be \nimplemented in a forward renderer with a few modifications. \nCompute Requirements \nSince only a marginal fraction of all screen pixels are shadow-edge pixels, this approach \nresults in substantial gains in computational and storage costs compared to the brute-force \n\n\n \n \napproach. Compared to the single-sample irregular shadow maps, the additional \ncomputational cost is relatively small. For example, let‘s assume the number of potential \nshadow-edge pixels is ~10 percent of all eye-view pixels, and that we generate N additional \nsamples per potential shadow-edge pixel. Since data structure construction and traversal \ntimes are proportional to the number of shadow map samples, this means an additional cost \nof 10N percent for anti-aliasing. \nAdditionally, there is an extra cost associated with creating the conservative shadow edge \nstencil buffer. In our implementation inside a deferred rendering, much of the required \ninformation was already computed—therefore, that extra cost is small. However, our \nalgorithm does require an extra light-space pass, per light, to capture the depth distribution \ninto the variance shadow map. \nStorage Requirements \nThe storage cost is the same as the standard irregular Z-buffer, proportional to the number \nof samples. Again, for 10-percent extra samples, 10N-percent extra storage is required, \ndepending on the implementation. Storage of the stencil buffer requires only 1 bit per eye-\nview pixel and can easily be packed into one of the existing eye-view buffers of the irregular \nshadowing algorithm. \nFuture Work \nGoing forward, we would like to investigate the benefits of merging our algorithm that \nadaptively samples potential shadow edge pixels multiple times with conventional multi-\nsampling techniques that adaptively sample the geometry silhouette pixels multiple times—\nfor best performance, preferably through the use of common data structures and shared \nrendering passes. Additionally, it should be fairly straightforward to extend our approach to \nsoft irregular shadow mapping, where the concept of anti-aliasing is implicit, as both the \nsoft and hard irregular shadow mapping algorithms share the same algorithmic framework \n[Johnson09]. For soft shadows, one may envision extending the conservative stencil to \nshadow penumbra detection. \n \nConclusion \nThe main advantage of irregular shadow maps with respect to conventional shadow maps is \nthat they bear no light-view aliasing. However, irregular shadow maps are affected by eye-\nview aliasing of the shadow result. Recent pre-filterable shadow mapping algorithms and \nbrute-force eye-view techniques have provided solutions for anti-aliased shadows, but none \nof them exploits the irregular Z-buffer acceleration structure directly. The method in this \nchapter is an extension of irregular shadow mapping, exploits the same irregular data \nstructure, and is therefore the first algorithm to produce anti-aliased shadows by means of \nadaptive multi-sampling of irregular shadow maps, while keeping all its other positive \ncharacteristics, such as pixel-perfect ray-traced quality shadows and the complete lack of \nlight-view aliasing. \n \nAcknowledgements \nWe‘d like to thank the people at the 3D Graphics and Advanced Rendering teams at the \nIntel Visual Computing group for their continued input while developing the methods \ndescribed here. Many thanks go out in particular to Jeffery A. Williams for providing the art \nassets in the tower and tunnel scenes and to David Bookout for persistent support and \nfeedback. \n\n\n \n \n \nReferences \n[Akenine-Möller05] Akenine-Möller, Tomas and Timo Aila. ―Conservative and Tiled \nRasterization Using a Modified Triangle Setup.‖ Journal of Graphics, GPU, and Game Tools \n10.3 (2005): 1–8. \n[Arvo07] Arvo, Jukka. ―Alias-Free Shadow Maps using Graphics Hardware.‖ Journal of \nGraphics, GPU, and Game Tools 12.1 (2007): 47–59. \n[Baumann05] Baumann, Dave. ―ATI Xenos: Xbox 360 Graphics Demystified.‖ 13 June 2005. \nBeyond 3D. n.d. http://www.beyond3d.com/content/articles/4/>. \n[Brabec01] Brabec, Stefan and Hans-Peter Seidel. ―Hardware-Accelerated Rendering of \nAntialiased Shadows with Shadow Maps.‖ Proceedings of the International Conference on \nComputer Graphics (July 2001): 209. ACM Portal. \n[Fernando01] Fernando, Randima, Sebastian Fernandez, Kavita Bala, and Donald P. \nGreenberg. ―Adaptive Shadow Maps.‖ Proceedings of the 28th Annual Conference on \nComputer Graphics and interactive Techniques (2001): 387–390. ACM Portal. \n[Hasselgren05] Hasselgren, Jon, Tomas Akenine-Möller, and Lennart Ohlsson. ―Conservative \nRasterization.‖ GPU Gems 2. 2005. NVIDIA. n.d. \n<http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter42.html>. \n[Hux10] Hux, Allen. ―Overlapped Execution on Programmable Graphics Hardware.‖ Game \nProgramming Gems 8. Ed. Adam Lake. Boston: Charles River Media, 2010. \n[Johnson04] Johnson, Gregory S., William R. Mark, and Christopher A. Burns. ―The Irregular \nZ-Buffer and its Application to Shadow Mapping.‖ April 2004. The University of Texas at \nAustin. n.d. <http://www.cs.utexas.edu/ftp/pub/techreports/tr04-09.pdf>. \n[Johnson05] Johnson, Gregory S., Juhyun Lee, Christopher A. Burns, and William R. Mark. \n―The Irregular Z-Buffer: Hardware Acceleration for Irregular Data Structures.‖ ACM \nTransactions on Graphics 24.4 (October 2005): 1462–1482. ACM Portal. \n[Johnson09] Johnson, Gregory S., Allen Hux, Christopher A. Burns, Warren A. Hunt, William \nR. Mark, and Stephen Junkins. ―Soft Irregular Shadow Mapping: Fast, High-Quality, and \nRobust Soft Shadows.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames. (2009): 57–66. ACM Portal. \n[Lauritzen06] Lauritzen, Andrew and William Donnelly. ―Variance Shadow Maps.‖ \nProceedings of the 2006 Symposium on Interactive 3D Graphics and Games. (2006): 161–\n165. ACM Portal. \n[Lefohn07] Lefohn, Aaron E., Shubhabrata Sengupta, and John D. Owens. ―Resolution-\nMatched Shadow Maps.‖ ACM Transactions on Graphics 26.4 (Oct. 2007): 20. ACM Portal. \n[Lloyd08] Lloyd, D. Brandon, Naga K. Govindaraju, Cory Quammen, Steven E. Molnar, and \nDinesh Manocha. ―Logarithmic Perspective Shadow Maps.‖ ACM Transactions on Graphics \n27.4 (Oct. 2008): 1–32. ACM Portal. \n[Robison09] Robison, Austin, and Peter Shirley. ―Image Space Gathering.‖ Proceedings of \nthe Conference on High Performance Graphics 2009 (2009): 91–98. ACM Portal. \n\n\n \n \n[Salvi08] Salvi, Marco. ―Rendering Filtered Shadows with Exponential Shadow Maps.‖ \nShaderX6: Advanced Rendering Techniques. Ed. Wolfgang Engel. Boston: Charles River \nMedia, 2008. 257–274. \n[Sen03] Sen, Pradeep, Mike Cammarano, and Pat Hanrahan. ―Shadow Silhouette Maps.‖ \nACM Transactions on Graphics 22.3 (July 2003): 521–526. ACM Portal. \n[Sintorn08] Sintorn, Erik, Elmar Eisemann, and Ulf Assarsson. ―Sample Based Visibility for \nSoft Shadows using Alias-free Shadow Maps.‖ Computer Graphics Forum: Proceedings of \nthe Eurographics Symposium on Rendering 2008 27.4 (June 2008): 1285–1292. \n[Stamminger02] Stamminger, Marc and George Drettakis. ―Perspective Shadow Maps.‖ \nProceedings of the 29th Annual Conference on Computer Graphics and Interactive \nTechniques (2002): 557–562. ACM Portal. \n[Williams78] Williams, Lance. ―Casting Curved Shadows on Curved Surfaces.‖ ACM \nSIGGRAPH Computer Graphics 12.3 (Aug. 1978): 270–274. ACM Portal. \n \n1.7. Overlapped Execution on Programmable Graphics Hardware \nAllen Hux, Intel Advanced Visual Computing (AVC) \nallen.hux@intel.com \nSome graphics algorithms require data structure construction and traversal steps that do \nnot map well to constrained graphics pipelines. Additionally, because of the dependencies \nbetween rendering and non-rendering passes, much (or all) of the compute power of the \ndevice may go idle between steps of a given algorithm. In this gem, we examine techniques \nfor executing non-rendering algorithms concurrently with traditional rendering on \nprogrammable graphics hardware, such as Larrabee. Such programmable graphics devices \nenable fine-grained signaling and event graphs, allowing algorithmic stages to ―overlap.‖ As \na working model, we present an implementation of Irregular Z-Buffer (IZB) shadows—an \nalgorithm requiring both standard rendering passes (for example, depth-only pre-pass) and \nparallelized data structure construction. Identification of rendering and non-rendering work \nthat is not dependent reveals opportunities to remove the stalls that currently occur when \nswitching between the two types of workloads. The APIs discussed in this article are \nexamples and not necessarily representative of APIs provided with a particular product. \nIntroduction to Irregular Z-Buffer Shadows \nThe simplest shadow mapping algorithm requires two passes. First, render the scene from \nthe light view to get a light-view depth buffer (at one resolution). Second, render the scene \nfrom the eye view (at the same or different resolution). For each point, test the visibility of \nthat point by projecting it into the light view and comparing its depth to the one found in \nthe first pass. If the eye-view depth is greater, then something must be between the light \nand the point being tested, and therefore it must be in shadow. This algorithm fits entirely \nwithin conventional rendering pipelines [Williams78]. \nThe problem with this approach is aliasing: The resolution of the light-view plane will never \nprecisely match the sampling frequency when projecting from the eye view, resulting in \nvisual artifacts. As described in the previous gem, an irregular Z-buffer stores precise eye-\nview positions in the light-view grid, enabling accurate shadow determination from the eye \nview [Johnson04, Johnson09, Galoppo10]. Shadow mapping with an irregular Z-buffer is a \nmulti-step process involving three rendering passes interleaved with two non-rendering \nsteps. \n\n\n \n \n1.  Render the scene from the eye view, depth only. \n2.  Transform the eye-view points to light view. \na. For each point, atomic increment the corresponding pixel in the light-view plane. \n(A bigger plane improves parallelism at the cost of memory.) \nb. Parallel prefix sum the indices, resulting in a mapping from each eye-view \nreceiver to a light-view pixel. \nc. Scatter the light-view values into a light-view-friendly 1D structure. Indices into \nthe 1D structure are stored in the light-view plane. \n3.  Render the scene from the light view. Instead of executing a traditional pixel shader, \ntest the triangle bounds against the points in the data structure (referring to the indices \nfrom Step 2c). Points that are inside and behind the triangle are in shadow. Set a bit in \nthe data structure marking this point as in shadow (occluded). \n4.  Create a standard shadow map by traversing the data structure and scattering out the \nocclusion value to a traditional 2D image. We call this the ―deswizzle‖ step. \n5.  Render the scene from the eye view again, using the shadow map. \nFor our purposes, this serves as an example of an algorithm that has some rasterization \nsteps intermingled with some algorithmic work one might normally implement in C++. The \nsecond step in particular can result in quite a bit of idle hardware, because it requires a \ndependency chain of parallel and serial workloads. \nAssuming we have a graphics system capable of performing the compute steps of IZB \n(shown in Figure 1.7.1) and executing a pixel shader capable of traversing the IZB data \nstructure, we get the simple dependency graph of tasks shown in Figure 1.7.2. Each stage \nin the graph cannot start until the last thread completes the last task of the prior stage. \nFigure 1.7.1. Building the IZB data structure, effectively a grid of lists. Regular \npoints from the eye view (top left) are transformed into the light view (top \nmiddle). A count of the number of pixels is kept in the light grid (bottom left). A \nprefix sum of the counters in the light grid results in offsets (bottom right) into \nthe 1D data structure (top right). Point data is scattered to the 1D data structure, \nincluding position and occlusion state (initially 0). The number of points in a light-\nview pixel can be determined by subtracting the current pixel value from the value \nto the right. The offset table combined with the 1D data structure forms the grid-\nof-lists IZB data structure. \n",
      "page_number": 69
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 87-94)",
      "start_page": 87,
      "end_page": 94,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \nFigure 1.7.2. IZB dependency graph. (Tasks are completed in the order they are \nsubmitted.) \n \nBecause there is almost always a ―long pole‖ that determines the duration of a stage in the \nalgorithm, nearly every thread in the system experiences some amount of idle time. This is \nsuggested by the activity bars in Figure 1.7.2. (Imagine these correspond to the execution \ntime on a system with four threads.) In the remainder of the gem, we will describe how we \ncan reclaim some of those lost cycles on a programmable architecture, such as Larrabee. \n \nOverview of Programmable Graphics Hardware \nAs graphics devices become more general, they can be viewed as many-core compute \ndevices with threads that can communicate amongst themselves (for example, via global \nvariables and atomic instructions). Consider a representative programmable graphics \narchitecture, Larrabee. Larrabee consists of many in-order cores on a single chip, each \nexecuting four threads in round-robin fashion, with an extended instruction set supporting \n16-element vectors [Seiler08]. The cores are connected by a very high-bandwidth ring that \nmaintains cache coherency. Several hardware texture samplers are distributed around the \nring, as well as connections to GDDR. Traditional graphics APIs (DirectX, OpenGL) could be \nimplemented on Larrabee as a typical process running within a relatively conventional \noperating system [Abrash09a]. \n\n\n \n \nA Larrabee architecture device could be an add-in adapter discrete from the traditional host \nCPU, it could be on-chip or on-die with the host CPU, or it could be the only processor in the \nsystem. For the purposes of this gem, we ignore the transport mechanism that enables our \nprograms to run on the device, but it is important to realize that the techniques and code \nthat follow are designed to execute directly on an architecture such as Larrabee. \nImplementing an efficient rasterizer within a many-threaded, 16-wide SIMD platform is \nbeyond the scope of this chapter, but we can summarize it here. Maximizing parallelism is \nthe main design goal. (Keeping the cores busy will be a recurring theme.) The approach \ndescribed by Abrash [Abrash09b] is to use a binning architecture where the bin dimensions \nare chosen such that the data accessed by each core (the depth buffer format and the pixel \nformat) does not exceed the cache size of the core (256 KB L2, 32 KB L1 in the current \nLarrabee architecture). Done properly, operations such as depth tests would require no off-\ncore bandwidth (neither ring nor GDDR accesses)—unless, of course, something wants to \naccess that depth buffer later, which merely requires a one-time write at the end. \nRemember that textures are accessed via hardware texture samplers, which have their own \ncaches. Despite the attention to bandwidth, the primary motivation for binning is to produce \na lot of independent work that can be executed in parallel. \nA programmable graphics device, such as a software rasterizer, would build a dependency \ngraph of rendering tasks to complete (which we will call a render graph). We can expect \nrendering tasks to be roughly divided between front end (for example, vertex \nprocessing/binning) and back end (rasterization/pixel shading). A dependency graph \nenables independent tasks to run concurrently; for example, if a core completes pixel \nshading within a bin, it can work on a different bin or begin vertex shading for the next \nframe. Dependencies can be defined in terms of resources—for example, a render task may \nsignal that it has completed writing to a render target resource (write dependency) or wait \nto start until a shadow map resource (texture) is ready for use (read dependency). The task \naffected (front end or back end) is a function of whether the resource is bound to the vertex \nshader or pixel shader. \nCommands can be inserted within the graph to be executed when a resource is ready—for \nexample, CopySubResource of a render target. We can also create nodes in the graph \nthat are signaled by outside events, such as direct memory access (DMA) transfer \ncompletions. We will discuss dependency graphs in more detail in the subsequent sections. \n \nOverview of Task-Based Parallelism \nNon-rendering algorithms hoping to effectively use many-core architectures require an \nefficient tasking system, such as Cilk [Blumofe95]. Such a task system leverages a thread \npool (where the number of software threads is less than or equal to the number of hardware \nthreads) to avoid operating system overhead from switching between threads. A good \ntasking system also provides the following features: \n \nThe ability to create individual tasks or sets of tasks (task sets). A task set calls the \nsame task function a user-defined number of times (in parallel). \n \nTasks and task sets may depend upon each other. Specifically, a task or task set will \nnot start until the tasks or task sets it depends on have completed. \n \nTasks and task sets may also depend upon user-defined events. Events can be \nsignaled by simply calling a NotifyEvent API. \n \nAn efficient work-stealing scheduler to automatically spawn and load-balance among \nindependent tasks. \nFollowing is an example of what such a task API might look like. Tasks call a user function \nwith user data. Task sets call a user function with user data and a number indicating which \ninstance this is (0..numTasks-1). \n\n\n \n \n// create an individual work item \ntypedef void (* const TaskFunction) \n    (void* in_pTaskFunctionArg); \nSyncObject CreateTask( \n    TaskFunction in_pTaskFunc, void* in_pData, \n    SyncObject* in_pDependencies, \n    int in_numDependencies); \n \n// create work items that do a subset of work \ntypedef void (* const TaskSetFunction) \n    (void* in_pTaskSetFunctionArg, \n    int in_taskIndex, int in_taskSetSize); \nSyncObject CreateTaskSet( \n    int in_numTasksInSet, \n    TaskSetFunction in_pTaskFunc, void* in_pData, \n    SyncObject* in_pDependencies, \n    int in_numDependencies); \n \nSyncObject CreateEvent(bool initialState); \n \nSince the task system itself is software, common-sense performance heuristics apply: The \namount of work done in the task should be sufficient to compensate for the overhead of the \ntasking system, which includes the function call into the task as well as some amount of \ncommunication to synchronize with internal task queues. For example, to perform an \noperation across a 1024×1024 image, don‘t create one million tasks. Instead, create a \nmultiple of the number of hardware threads in the system. For a system with 100 hardware \nthreads, 400 or 500 equally sized tasks would give the task system some opportunity to \ngreedily load balance, while each task of approximately 2,000 pixels would give good \nopportunities for prefetching and loop unrolling. (A Larrabee optimized routine would \noperate on 16 pixels at a time, hence a loop of only about 100 iterations.) Our experiments \non desktop x86 machines show that tasks of a few thousand clocks each achieve 90 percent \nor better overall efficiency. \nEfficient graphics processing requires thread affinity knowledge. That is, the rasterizer \nassigns threads to cores with the expectation that those threads will share render target \ndata in their cache. Non-rendering tasks typically function more opportunistically, executing \nwhenever a thread becomes idle. Hence, we design our tasks to be independent of the \nthreads they may execute on. Since the hardware is programmable, finer control is possible \nbut adds complexity. \nEven with equal-sized tasks, in a machine with that many threads, contention for resources \n(caches and GDDR bandwidth) will cause tasks to have variable durations. For very irregular \nworkloads, such as irregular Z-buffer, optimal performance may require thousands of \ntasks—it all depends on the algorithm. \n \nCombining Render Graphs and Task Graphs \nThe ability to create non-rendering task sets that depend on, or are dependencies of, \nrendering tasks is the key to achieving maximum hardware utilization for these mixed-\nusage algorithms. To do this, we need a way to interact with the rendering dependency \ngraph from user code. Following is a method to inject a non-rendering task into the render \ngraph, referring to the current render context (analogous to a DirectX or OpenGL context), \n\n\n \n \na function to call when the dependencies are met, user data to pass to the function, and a \nlist of all the resource dependencies. \nvoid CreateRenderTask(in_pRenderContext, \n    in_pUserFunc, in_pUserData, \n    in_pReadDependencies, in_numReadDependencies, \n    in_pWriteDependencies, in_numWriteDependencies, \n    out_pRenderTask); \n \nWe then need a way to notify the render system that the render task and its read and write \ndependencies, as declared above, are complete and available. \nvoid NotifyRenderTaskComplete(in_pRenderTask); \n \nNow we have a way for tasks created using our task system to define dependencies with \nrendering tasks and for rendering tasks to very finely interact with our task system. For \nexample, if a render pass has pixel shaders bound to a resource declared as a write \ndependency of a user task, the front end of the render pass can start (transform/ bin), but \nthe back end cannot (rasterize/pixel shade). \nWe need a little helper glue to efficiently communicate between ―render‖ work and the tasks \ncreated for our ―client‖ work. Following, we show how an event that waits on a task set can \ncall NotifyRenderTaskComplete() to enable dependent render work. We also show \nhow render work can cause a callback declared in CreateRenderTask() to signal an \nevent, thereby starting dependent client work. \nOn the left side of Figure 1.7.3, we show how a render pass can be made to wait on client \nwork. First, create a task set that does some client work, such as builds a data structure. \nNext, create a render task with the data structure (resource) written to by the task set as a \nwrite dependency, no read dependencies, and no callback. Then, create a render pass with \nthe data structure resource as a read dependency. Finally, create a task that depends on \nthe task set that will call NotifyRenderTaskComplete(). The render pass cannot start \nuntil the task set is complete. \nFigure 1.7.3. Detail of connecting client task sets to render passes via render \ntasks. \n\n\n \n \n \n \nOn the right side of Figure 1.7.3, we show how client work can be made dependent on a \nrender pass. First, create an event that we will signal and a task set that depends on the \nevent. (It would do work on the render target.) Next, create the render pass, which in this \ncase writes to a render target (write dependency). Finally, create a render task with the \nrender target as a read dependency and a callback that will set the event. The task set \ncannot start until the render pass is complete. \n \nCombined Dependency Graph \nTo reduce the idle time, we need to build a complete dependency graph including both \nrendering and non-rendering tasks. Below, we work with the following constraints: \n \nTasks or task sets must have their dependencies described at creation time. \n \nTasks or tasks sets can depend on tasks, task sets, or events. \nThese constraints force us to work from the end of the algorithm backwards. Since a task \nwill start immediately if it has no dependencies, we create events in a not-signaled state to \nact as gates for the task sets. \n1. Create a build data structure event (not signaled). \n\n\n \n \n2. Create a build data structure task set that depends on event (1). \n3. Create a deswizzle event (not signaled). \n4. Create a deswizzle task set that depends on event (3). \n5. Create a light-view render pass where: \na. Rasterization depends on resource from task set (2). \nb. Render target resource complete signals event (3). \n6. Create a final eye-view render pass where rasterization depends on the shadow map \nresource from task set (4). \n7. Create a depth-only eye-view render pass that signals event (1) when its render \ntarget resource is complete. \nAs soon as we complete the seventh step, creating the depth-only eye-view render with no \ndependencies, the whole algorithm will fall into place. As shown in Figure 1.7.4, when the \ndepth-only pass (7) completes, it signals the build event (1), which enables the build task \nset (2) to start. Completion of the build task set (2) enables the light-view rasterization (5) \nto start. (Transform and binning should already be complete.) When the light-view render \n(5) completes, it signals the deswizzle event (3), which enables the deswizzle task set (4) to \nstart. When the deswizzle task set (4) completes, it enables the final eye-view rasterization \n(6) to start. (Transform and binning should already be complete.) \nFigure 1.7.4. Order of creation of events, task sets, and render graph nodes for \noverlapped execution of IZB. \n \n \n \n \n\n\n \n \nIdle-Free Irregular Z-Buffer Shadows \nFigure 1.7.5 shows the naïve linear dependency graph of the IZB algorithm discussed \nearlier, showing the render stages expanded into front end (transform + binning) and back \nend (rasterization + pixel shading) for a total of eight stages, or task sets. \nFigure 1.7.5. IZB dependency graph with render stages expanded into front and \nback end. \n \nFigure 1.7.6 shows how threads that would otherwise have become idle can instead work on \nnon-dependent front-end rendering tasks if we start all three render passes immediately. \nThis overlapped execution is especially helpful for improving the performance of our \nirregular shadow-mapping tasks, allowing us (in this example) to fully hide the cost of the \nfront-end rendering tasks. Another way to interpret this is that the compute part of irregular \nshadow mapping is essentially free when overlapped with rendering. \nFigure 1.7.6. IZB with dependency graph integrated with flexible rendering \npipeline. Xform/bin tasks can complete as threads become available from non-\nrendering tasks, filling in gaps in execution. \n \nThis demonstrates another advantage of programmable hardware: Maximum performance is \nachieved by enabling flexible hardware to execute whatever tasks are available, rather than \nby partitioning the hardware into dedicated islands of computation. Compare this to the \nearly days of graphics devices with dedicated pixel shader and vertex shader hardware: \nWhen there was more vertex work than pixel work, the idle pixel shading hardware could \nnot be reconfigured to help out. In modern architectures, graphics processors dynamically \nload balance across all execution units. On a programmable architecture such as Larrabee, \nthis load balancing can be controlled by the programmer, enabling the overlap of rendering \nand non-rendering tasks. \n \nConclusion \n\n\n \n \nMaximizing performance on modern, many-core platforms requires identifying independent \nwork to be executed in parallel. Non-graphics workloads leverage a system of dependent \ntasks and task sets to manage parallel computation. On programmable graphics devices \nsuch as Larrabee, a similar system of task dependencies can be used to identify \nindependent and dependent graphics work—for example, binning versus pixel shading. By \nconnecting these graphs, we can further exploit available execution units by exposing more \nopportunities for independent tasks to run concurrently. \n \nReferences \n[Abrash09a] Abrash, Michael. ―A First Look at the Larrabee New Instructions (LRBni).‖ Dr. \nDobb‘s. 1 April 2009. <http://www.ddj.com/hpc-high-performance-\ncomputing/216402188>. \n[Abrash09b] Abrash, Michael. ―Rasterization on Larrabee: A First Look at the Larrabee New \nInstructions (LRBni) in Action.‖ Intel. March 2009. <http://software.intel.com/file/15542>. \n[Blumofe95] Blumofe, Robert D., Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. \nLeiserson, Keith H. Randall, and Yuli Zhou. ―Cilk: An Efficient Multithreaded Runtime \nSystem.‖ Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of \nParallel Programming (July 1995): 207–216. \n[Galoppo10] Galoppo, Nico. ―Eye-view Pixel Anti-Aliasing for Irregular Shadow Mapping.‖ \nGame Programming Gems 8. Ed. Adam Lake. Boston: Charles River Media, 2010. \n[Johnson04] Johnson, Gregory S., William R. Mark, and Christopher A. Burns. ―The Irregular \nZ-Buffer and its Application to Shadow Mapping.‖ April 2004. The University of Texas at \nAustin. n.d. <http://www.cs.utexas.edu/ftp/pub/techreports/tr04-09.pdf>. \n[Johnson09] Johnson, Gregory S., Allen Hux, Christopher A. Burns, Warren A. Hunt, William \nR. Mark, and Stephen Junkins. ―Soft Irregular Shadow Mapping: Fast, High-Quality, and \nRobust Soft Shadows.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames. (2009): 57–66. ACM Portal. \n[Seiler08] Seiler, Larry, et al. ―Larrabee: A Many Core x86 Architecture for Visual \nComputing.‖ ACM Transactions on Graphics 27.3 (Aug. 2008): n.p. ACM Portal. \n[Williams78] Williams, Lance. ―Casting Curved Shadows on Curved Surfaces.‖ ACM \nSIGGRAPH Computer Graphics 12.3 (Aug. 1978): 270–274. ACM Portal. \n \n1.8. Techniques for Effective Vertex and Fragment Shading on the \nSPUs \nSteven Tovey, Bizarre Creations Ltd. \nsteven.tovey@bizarrecreations.com \nWhen the Cell Broadband Engine was designed, Sony and the other corporations in the STI \ncoalition always had one eye on the Cell‘s ability to support a GPU in its processing activities \n[Shippy09]. The Cell has been with us for three years now, and like any new piece of \nhardware, it has taken time for developers to understand the best ways of pushing the \nhardware to its limits. The likes of Mike Acton and the Insomniac Games Technology Team \n",
      "page_number": 87
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 95-103)",
      "start_page": 95,
      "end_page": 103,
      "detection_method": "topic_boundary",
      "content": " \n \nhave been instrumental in pushing general development and coding strategies for the Cell \nforward, but there has been little discussion about ways that the SPUs can support a GPU in \nits processing activities specifically. This chapter aims to introduce fundamental techniques \nthat can be employed when developing code for the CBE that will allow it to aid the GPU in \nperforming rendering tasks. \nThe CBE as Part of a Real-World System \nUnderstanding Cell‘s place in a real-world system is useful to our discussion, and, as such, \nwe will use Sony‘s PlayStation 3 as our case study. PlayStation 3 contains the Cell \nBroadband Engine, which was developed jointly by Sony Computer Entertainment, Toshiba \nInc., and IBM Corp. [Shippy09, Möller08, IBM08]. The Cell forms part of the overall \narchitecture of the console along with the Reality Synthesizer, RSX, and two types of \nmemory. Figure 1.8.1 shows a high-level view of the architecture. \nFigure 1.8.1. The PlayStation 3 architecture (illustration modeled after [Möller08, \nPerthuis06]). \n \n \nThe Cell contains two distinctly different types of processor: the PowerPC Processing \nElement (PPE) and the Synergistic Processing Element (SPE). The PPE is essentially the \nbrains of the chip [Shippy09] and is capable of running an operating system in addition to \ncoordinating the processing activities of its counterpart processing elements, the SPEs. \nInside PlayStation 3, there are eight SPEs. However, to increase chip yield, one is locked \nout, and Sony reserves another for their operating system, leaving a total of six SPEs \navailable for application programmers. All processing elements in the Cell are connected by \na token-ring bus, as shown in Figure 1.8.2. \nFigure 1.8.2. The Cell Broadband Engine (modeled after [IBM08]). \n \n \nBecause the SPEs are the main focus of this chapter, they are discussed in much greater \ndetail in the forthcoming sections. \n\n\n \n \n \nThe SPEs \nEach Synergistic Processing Element is composed of two major components: the Synergistic \nProcessing Unit (SPU) and the Memory Flow Controller (MFC). \nThe SPU \nDetailed knowledge of the SPU instruction set and internal execution model are critical to \nachieving peak performance on the PlayStation 3. In the following sections, we will highlight \nsome important facets of this unique processor. \nThe Synergistic Execution Unit and SPU ISA \nThe Synergistic Execution Unit (SXU), part of the SPU, is responsible for the execution of \ninstructions. Inside the SXU are two pipelines: the odd pipeline and the even pipeline. \nInstructions are issued to exactly one of these pipelines, depending on the group the issued \ninstruction falls into (see Table 1.8.1). The SXU supports the dual issue of instructions (one \nfrom each pipeline) if and only if a very strict set of requirements is met. We will discuss \nthese requirements in detail later. \nTable 1.8.1. A List of Instruction Groups Together with Their Associated Execution \nPipes and Latencies \nInstruction Group \nPipeline \nLatency \n(Cycles) \nIssue \n(Cycles) \nSingle precision floating-point operations \nEVEN \n6 \n1 \nDouble precision floating-point operations \nEVEN \n7 \n6 \nInteger multiplies, integer/float conversions, and \ninterpolation \nEVEN \n7 \n1 \nImmediate loads, logical operations, integer \naddition/subtraction, carry/borrow generate \nEVEN \n2 \n1 \nElement-wise rotates and shifts, special byte \noperations \nEVEN \n4 \n1 \nLoads and stores, branch hints, channel operations \nODD \n6 \n1 \nShuffle bytes, qword rotates and shifts, estimates, \ngather, selection mask formation and branches \nODD \n4 \n1 \n \nThe SPU has a particularly large register file to facilitate the execution of pipelined, unrolled \ncode without the need for excessive register spilling. Unlike its counterpart, the PPE, the \nregister file of the SPU is unified. That is, floating-point, integer, and vector operations act \non the same registers without having to move through memory. As the SPU is a vector \n\n\n \n \nprocessing unit at its heart, its Instruction Set Architecture (ISA) is designed specifically for \nvector processing [IBM08a]. All 128 of the SPU‘s registers are 16 bytes in size, allowing for \nup to four 32-bit floating-point values or eight 16-bit integers to be processed with each \ninstruction. \nWhile a full analysis of the SPU‘s ISA is beyond the scope of this gem, there are a number \nof instructions worth discussing in greater detail that are particularly important for efficient \nprogramming of the SPU. The first of these instructions is selb, or ―select bits.‖ The selb \ninstruction performs branchless selection on a bitwise basis and takes the form selb rt, \nra, rb, rm. For each bit of a quadword, this instruction uses the mask register (rm) to \ndetermine which bits of the source registers (ra and rb) should be placed in the \ncorresponding bits of the target register (rt). Comparison instructions all return a \nquadword selection mask that can be used with selb[1]. \n[1] The fsmbi instruction is also very useful for efficiently constructing a selection mask for use with \nselb. \nThe shuffle bytes instruction, shufb, is the key instruction in data manipulation on the \nSPU. The shufb instruction takes four operands, all of which are registers. The first \noperand, rt, is the target register. The next two operands, ra and rb, are the two \nquadwords that will be manipulated by the quadword pattern from the fourth operand, rp. \nThe manipulations controlled by this fourth operand, known as the shuffle pattern, are \nparticularly interesting. \nA shuffle pattern is a quadword value that works on a byte level. Each of the 16 bytes in the \nquadword controls the contents of the corresponding byte in the target register. For \nexample, the 0th byte of the pattern quadword controls the value that will ultimately be \nplaced into the 0th byte of the target register, the 1th byte controls the value of the 1th \nbyte placed into the target register, and so on, for all 16 bytes of the quad word. Listing \n1.8.1 provides an example shuffle pattern. \nListing 1.8.1. An example shuffle pattern \nconst vector unsigned char _example1 = \n{ 0x00, 0x11, 0x02, 0x13, \n  0x04, 0x15, 0x06, 0x17, \n  0x08, 0x19, 0x0a, 0x1b, \n  0x0c, 0x1d, 0x0e, 0x1f }; \n \nThe above pattern performs a perfect shuffle, but on a byte level. (The term ―perfect \nshuffle‖ typically refers to the interleaving of bits from two words.) The lower 4 bits of each \nbyte can essentially be thought of as an index into the bytes of the first or second operand \nquadword. Similarly, the upper 4 bits can be thought of as an index into the registers \nreferred to in the instruction‘s operands. Since there are only two, we need only concern \nourselves with the LSB of this 4-bit group—in other words, 0x0x (where x denotes some \nother value of the lower 4 bits of the byte) would index into the contents of the ra register, \nand 0x1x would access the second. It is worth noting that there are special case values that \ncan be used to load constants with shufb; an interested reader can refer to [IBM08a] for \ndetails. A further example in Listing 1.8.2 will aid us in our understanding. \nListing 1.8.2. An example of using shufb \n\n\n \n \nconst vector unsigned char _example2 = \n{ 0x00, 0x01, 0x02, 0x03, \n  0x14, 0x15, 0x16, 0x17, \n  0x08, 0x09, 0x0a, 0x0b, \n  0x1c, 0x1d, 0x1e, 0x1f }; \n \nqword pattern = (const qword)_example2; \nqword ra = si_ilhu(0x3f80); // ra contains: 1.0f, 1.0f, 1.0f, \n1.0f \nqword rb = si_ilhu(0x4000); // rb contains: 2.0f, 2.0f, 2.0f, \n2.0f \n \n// result contains: 1.0f, 2.0f, 1.0f, 2.0f \nqword result = si_shufb(ra, rb, pattern); \n \nIn many programs, simply inlining of shuffle patterns for data manipulation requirements \nwill suffice, but since the terminal operand to shufb is simply a register, there is nothing to \nstop you from computing the patterns dynamically in your program or from forming them \nwith the constant formation instructions (as should be preferred when lower latency can be \nachieved than the 6-cycle load from the local store). As it turns out, dynamic shuffle pattern \ncomputation is actually critical to performing unaligned loads from the local store in a \nvaguely efficient manner, as we shall see later. In-depth details of the SPU ISA can be \nfound in [IBM08a]. \nLocal Store and Memory Flow Controller \nAs previously mentioned, each of the SPUs in the Cell is individually endowed with its own \nmemory, known as its local store. The local store is (at least on current implementations of \nthe CBE) 256 KB in size and can essentially be thought of as an L1 cache for the Synergistic \nExecution Unit. Data can be copied into and out of the local store by way of the DMA engine \nin the MFC, which resides on each SPE and acts asynchronously of the SXU. Loads and \nstores to and from the local store are always 16-byte aligned and sized. Hence, processing \ndata smaller than 16 bytes requires use of a less-than-efficient load-modify-store pattern. \nAccesses to the local store are arbitrated by the SPU Store and Load unit (SLS) based on a \npriority; the DMA engine always has priority over the SXU for local store accesses. \nEach DMA is part of a programmer-specified tag group. This provides a mechanism for a \nprogrammer to poll the state of the MFC to find out if a specific DMA has completed. A tag \ngroup is able to contain multiple DMAs. The tag group is denoted by a 5-bit value internally, \nand, as such, the MFC supports 32 distinct tag groups [Bader07]. The DMA queue (DMAQ) \nis 16 entries deep in current implementations of the CBE. \n \nData Management \nIn many ways, the choice of data structure is more important than the efficiency of the \noperations that must be performed on it. In the following sections, we will describe a variety \nof data management strategies and their tradeoffs in the context of the SPU. \nMulti-Buffering \n\n\n \n \nAll graphics programmers will be familiar with the concept of a double buffer. The multi-\nbuffer is simply a term that generalizes the concept to an arbitrary number of buffers. In \nmany cases two buffers will be sufficient, but sometimes a third buffer will be required to \neffectively hide the latency of transfers to and from the effective address space. Figure \n1.8.3 shows the concept of multi-buffering. \nFigure 1.8.3. Multi-buffering data to hide latency (modeled after [Bader07]). \n \n \nBader suggests that each buffer should use a separate tag group in order to prevent \nunnecessary stalling of the SPU waiting for data that will be processed sometime in the \nfuture. Barriers and fences should be used to order DMAs within a tag group and the DMA \nqueue, respectively [Bader07]. Multi-buffering can yield significant performance increases, \nbut it does have a downside. Because the buffers are resident in the local store, it does \nmean that SPE programs must be careful not to exceed the 256-KB limit. \nUsing a reasonable size for each of the buffers in your multi-buffer (about 16 KB) in order to \nallow the SPU to process several vertices or pixels before requiring more data from the main \naddress space is a fine strategy. However, the pointer wrangling can become a little \ncomplicated if one‘s goal is to support a list of arbitrarily sized (and hence aligned) vertex \nformats. Conversely, alignments do tend to be a little more favorable and can be easily \ncontrolled by carefully selecting a reasonably sized unit of work when processing pixels. \nStructure-of-Arrays versus Array-of-Structures \nThe design of data is paramount when hoping to write performant software for the SPU. \nSince the SPU is a SIMD vector processor, concepts familiar to those who have programmed \nwith other vector ISAs, such as SSE on Intel chips, Altivec on PowerPC chips, or even the \nVU on the PlayStation 2, are immediately transferable to the SPU. One such concept is \nparallel array data layout, better known as Structure-of-Arrays (SOA). By laying data out in \na format that is the transpose of its natural layout (Array-of-Structures), as can be seen in \nFigure 1.8.4, a programmer is often able to produce much more efficient code (most \nnotably in those cases where vectorized data is interacting with scalar data). \n\n\n \n \nFigure 1.8.4. An Array-of-Structures layout on the left is transposed into a \nStructure-of-Arrays layout (illustration modeled after [Tovey10]). \n \n \nThe benefits of using an SOA layout are substantial in a lot of common cases. Listing 1.8.3 \nillustrates this by way of computing the squared length of a vector. \nListing 1.8.3. Two versions of a function to calculate the squared length of a \nvector. The first assumes Array-of-Structures data layout, and the second \nStructure-of-Arrays layout. \n// Version 1: AOS mode - 1 vector, ~18 cycles. \n      qword dot_xx                  = si_fm(v, v); \n      qword dot_xx_r4               = si_rotqbyi(dot_xx, 4); \n            dot_xx                  = si_fa(dot_xx, dot_xx_r4); \n      qword dot_xx_r8               = si_rotqbyi(dot_xx, 8); \n            dot_xx                  = si_fa(dot_xx, dot_xx_r8); \n      return si_to_float(dot_xx); \n \n      // Version 2: SOA mode - 4 vectors, ~8 cycles. \n      qword dot_x                   = si_fm(x, x); \n      qword dot_y                   = si_fma(y, y, dot_x); \n      qword dot_z                   = si_fma(z, z, dot_y); \n      return dot_z; \n \nBranch-Free DMAs \nThe cost of badly predicted branches on the SPU is quite significant. Given that the SPU \ndoes not contain any dedicated branch prediction hardware[2], the burden of responsibility \nfalls squarely on the shoulders of the programmer (or in the majority of cases, the \ncompiler). There are built-in language extensions available in most SPU compilers that allow \nthe programmer to supply branch hints, but such things assume that you have sufficient \ntime in order to make the prediction (that is, more than 11 cycles) and that the branch is \nintrinsically predictable, which may not be the case. It is therefore recommended that \nprogrammers avoid branches entirely [Acton08]. Others have discussed this topic at length \n[Acton08, Kapoulkine09], so I will refrain from doing so here; however, I do wish to touch \nupon one common case where branch avoidance is not entirely obvious but is entirely \ntrivial. \n[2] The SXU adopts the default prediction strategy that all branches are not taken. \nIBM‘s SDK provides several MFC functions to initiate DMA without resorting to the manual \nwriting of registers[3]. An unfortunate side effect of such functions is that they seem to \nactively encourage code such as that presented in Listing 1.8.4. \n\n\n \n \n[3] SPU-initiated DMAs are performed by the writing of special-purpose registers in the MFC using the \nwrch instruction. There are six such registers that must be written in order to initiate a DMA. These \nmay be written arbitrarily as long as the command register is written terminally [IBM08]. \nListing 1.8.4. All-too-often encountered code to avoid issuing unwanted DMAs \nif(si_to_uint(counter)> 0) ) \n       mfc_put(si_to_uint(lsa), \n                     si_to_uint(ea), \n                     si_to_uint(size), \n                     si_to_uint(tag)); \n \nHowever, a little knowledge of the MFC can help avoid the branch in this case. The MFC \ncontains the DMA queue (DMAQ). This queue contains SPU-initiated commands to the MFC‘s \nDMA engine. Similar to a CPU or GPU, the MFC supports the concept of a NOP. A NOP is an \noperation that can be inserted into the DMAQ but doesn‘t result in any data being \ntransferred. A NOP for the MFC is denoted by any DMA command being written that has \nzero size. The resulting code looks something like Listing 1.8.5. \nListing 1.8.5. Branch-free issue of DMA \nqword cmp_mask    = si_cgti(counter, 0x0); \nqword cmp         = si_andi(cmp_mask, 0x1); // bottom bit only. \nqword dma_size    = si_mpy(size, cmp);    // size < 2^16 \n                    mfc_put(si_to_uint(lsa), \n                                  si_to_uint(ea), \n                                  si_to_uint(dma_size), \n                                  si_to_uint(tag)); \n \nUnfortunately, the hardware is not smart enough to discard zero-sized DMA commands \nimmediately upon the command register being written, and these commands are inserted \ninto the 16-entry DMAQ for processing. The entry into the queue is immediately discarded \nwhen the DMA engine attempts to process this element of the queue. However, this causes \na subtle downside to the employment of this technique for branch avoidance. SPE programs \nthat issue a lot of DMAs can quickly back up the DMAQ, and issuing a zero-sized DMA can \nstall the SPU while it flushes the entire DMAQ. Luckily, this state of affairs can be almost \nentirely mitigated by a well-designed SPE program, which issues fewer, but larger DMAs. \n \nVertex/Geometry Shading \nThe SPUs can also lend a hand in various vertex processing tasks and, because of their \ngeneral nature, can help overcome some of the shortcomings of the GPU programming \nmodel. In Blur, we were able to use the SPU to deal with awkward vertex sizes and to \noptimize the vehicle damage system. \nHandling Strange Alignments When Multi-Buffering \n\n\n \n \nVertex data comes in all shapes and sizes, and, as a result, multi-buffering this type of data \npresents some challenges. When vertex buffers are created, contiguous vertices are packed \ntightly together in the buffer to both save memory and improve the performance of the pre-\ntransform cache on the GPU. This presents an SPU programmer with a challenge when \nattempting to process buffers whose per-vertex alignment may not be a multiple of 16 \nbytes. This is a problem for two reasons. First, the DMA engine in the MFC transfers 1, 2, 4, \n8, or multiples of 16 bytes, meaning that we must be careful not to overwrite parts of the \nbuffer that we do not mean to modify. Second, loads and stores performed by the SXU itself \nare always 16-byte aligned [IBM08]. \nThere are a lot of cases where a single vertex will straddle the boundary of two multi-\nbuffers, due to vertex structures that have alignments that are sub-optimal from an SPU \nprocessing point of view. The best way of coding around this problem is to simply copy the \nend of a multi-buffer to its nearest 16-byte boundary into the start of the second multi-\nbuffer and offset the pointer to the element you are currently processing. This means that \nwhen the second multi-buffer is transferred back to the main address space, it will not \ncorrupt the vertices you had previously processed and transferred out of the first multi-\nbuffer, as shown in Figure 1.8.5. Listing 1.8.6 contains code demonstrating how to handle \nunaligned loads from the local store. \nFigure 1.8.5. Avoid buffer corruption by copying a small chunk from the end of one \nmulti-buffer into the start of another. \n \n \nCase Study: Car Damage in Blur \nThe car damage system in Blur works by manipulating a lattice of nodes that roughly \nrepresent the volume of the car. The GPU implementation makes use of a volume texture \ncontaining vectors representing the offset of these nodes‘ positions from their original \npositions. This is then sampled based on the position of a vertex being processed relative to \na volume that loosely represents the car in order to calculate position and normal offsets \n(see Figure 1.8.6). The texture is updated each time impacts are applied to the lattice, or \nwhen the car is repaired. \nFigure 1.8.6. Position and normal offsets are applied to each vertex based on \ndeltas stored in a volume texture. \n\n\n \n \n \n \nThe GPU performs the deformation every frame because the damage is stateless and a \nfunction of the volume texture and the undamaged vertex data. Given the amount of work \ninvolved and the additional performance hit from sampling textures in the vertex unit, the \nperformance of rendering cars in Blur was heavily vertex limited. This was something we \nwanted to tackle, and the SPUs were useful in doing so. Porting the entire vertex shader to \nthe SPU was not practical given the timeframe and memory budgets, so instead we focused \non moving just the damage calculations to the SPUs. This meant that the car damage vertex \nprocessing would only occur when damage needed to be inflicted on the car (instead of \nevery frame with the equivalent GPU implementation), and it would greatly reduce the \ncomplexity of the vertex shader running on the GPU. \nThe damage offsets are a function of the vertex‘s position and the state of the node lattice. \nGiven the need for original position, we must transfer the vertex data for the cars to the \nlocal store via DMA and read the position data corresponding to each vertex. This is done \nusing a multi-buffering strategy. Because different components of the car utilize different \nmaterials (and hence have different vertex formats), we were also forced to a variety of \nvertex alignments as described earlier. With the vertex data of the car in the SPU local \nstore, we are able to calculate a position and normal offset for each vertex and write these \nout to a separate vertex buffer. Each of these values is stored as a float4, which means \nthe additional vertex stream has a stride of 32 bytes per vertex. An astute GPU programmer \nwill notice the potential to pack this data into fewer bits to improve cache utilization. This is \nundesirable, however. The data in its 32-bytes-per-vertex form is ideal for the DMA engine \nbecause the MFC natively works in 16-byte chunks, meaning from the point of view of other \nprocessing elements (in our case, the GPU), a given vertex is either deformed or it is not. \nThis is one of the tradeoffs made to mitigate the use of a double buffer. Color Plate 5 has a \nscreen-shot of this technique. \nTo GPU Types and Back Again \nFor the most part, GPUs do their best to support common type formats found in CPUs. The \nIEEE754 floating-point format is (for better or worse) the de facto floating-point standard on \npretty much all modern hardware that supports floating point[4]. \n[4] Ironically, the SPUs do not offer full IEEE754 support, but it‘s very close. \nHowever, in addition to the IEEE754 standard 32-bit floats and 64-bit doubles, most \nshading languages offer a 16-bit counterpart known as half. The format of the half is not \ndefined by any standard, and, as such, chip designers are free to implement their own \nfloating-point formats for this data type on their GPUs. Fortunately, almost all GPU vendors \nhave adopted the half format formalized by Industrial Light & Magic for their OpenEXR HDR \nfile format [ILM09]. This format uses a single bit to denote the sign of the number, 5 bits \nfor the exponent, and the remaining 10 bits for its mantissa or significand. \nSince the half type is regrettably absent from the C98 and C++ standards, it falls to the \nprogrammer to write routines to convert to other data types. Acton has made available an \nentirely branch-free version of these conversion functions at [Acton06]. For the general \ncase, you would be hard-pressed to better Acton‘s code (assuming you don‘t have the \nmemory for a lookup table as in [ILM09]). However, in many constrained cases, we have \n",
      "page_number": 95
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 104-112)",
      "start_page": 104,
      "end_page": 112,
      "detection_method": "topic_boundary",
      "content": " \n \nknowledge about our data that allows us to omit support for floating-point special cases that \nrequire heavyweight conversion logic (NaNs and de-normalized numbers). Listing 1.8.6 \ncontains code to convert between an unaligned half4 and float4 but omits support for \nNaNs. This is an optimization that was employed in Blur‘s damage system. The inverse of \nthis function is left as an exercise for the reader. \nListing 1.8.6. Code to convert an unaligned half4 to a qword \nstatic inline const qword ld_float16_4(void * __restrict__ addr) \n{ \n     const vector unsigned char _loader = \n     { 0x80, 0x80, 0x00, 0x01, \n       0x80, 0x80, 0x02, 0x03, \n       0x80, 0x80, 0x04, 0x05, \n       0x80, 0x80, 0x06, 0x07 }; \n     const vector unsigned char _shft = \n     { 0x00, 0x01, 0x02, 0x03, \n       0x04, 0x05, 0x06, 0x07, \n       0x08, 0x09, 0x0a, 0x0b, \n       0x0c, 0x0d, 0x0e, 0x0f }; \n \n     qword target           = si_from_ptr(addr); \n     qword val_lo           = si_lqd(target, 0x00); \n     qword val_hi           = si_lqd(target, 0x10); \n     qword sign_bit_mask    = si_ilhu(0x0); \n           sign_bit_mask    = si_iohl(sign_bit_mask, 0x8000); \n     qword mant_bit_mask    = si_ilhu(0x0); \n           mant_bit_mask    = si_iohl(mant_bit_mask, 0x7fff); \n     qword expo_bias        = si_ilhu(0x3800); \n     qword loader           = (const qword)_loader; \n     qword shft             = (const qword)_shft; \n     qword offset           = si_andi(target, 0x0f); \n     qword lo_byte_pat      = si_ilh(0x0303); \n     qword offset_pat       = si_shufb(offset, offset, \nlo_byte_pat); \n     qword mod_shuf         = si_a(shft, offset_pat); \n     qword val              = si_shufb(val_lo, val_hi, \nmod_shuf); \n     qword result           = si_shufb(val, val, loader); // \naligned \n     qword sign_bit         = si_and(result, sign_bit_mask); \n           sign_bit         = si_shli(sign_bit, 0x10); \n     qword significand      = si_and(result, mant_bit_mask); \n           significand      = si_shli(significand, 0xd); \n     qword is_zero_mask     = si_cgti(significand, 0x0); \n           expo_bias        = si_and(is_zero_mask, expo_bias); \n     qword exponent_bias    = si_a(significand, expo_bias); \n     qword final_result     = si_or(exponent_bias, sign_bit); \n     return final_result; \n} \n \n\n\n \n \nBenefits versus Drawbacks \nProcessing vertex data on the SPUs has a number of advantages; one of the most \nsignificant is that the rigidity of the GPU‘s processing model is largely circumvented as you \nare performing processing on a general-purpose CPU. Access to mesh topology is \nsupported, but one must be careful that these accesses do not introduce unwanted stalls as \nthe data is fetched from the main address space. In addition, since we are using a CPU \ncapable of general-purpose program execution, we are able to employ higher-level \noptimization tactics, such as early outs or faster code paths, which would be tricky or \nimpossible under the rigid processing model adopted by GPUs. The ability to split workloads \nbetween the SPUs and the GPU is also useful in striking the ideal balance for a given \napplication. \nAs with most things in graphics programming, there are some tradeoffs to be made. Vertex \nprocessing on the SPU can in many cases require that vertex buffers are double buffered, \nmeaning a significantly increased memory footprint. The situation is only aggravated if \nthere is a requirement to support multiple instances of the same model. In this case, each \ninstance of the base model may also require a double buffer. This can be mitigated to some \nextent by carefully designing the vertex format to support atomic writes of individual \nelements by the DMA engine, but the practicality of this is highly application-specific and \ncertainly doesn‘t work in the case of instances. Clever use of a ring buffer can also solve this \nproblem to some extent, but it introduces additional problems with SPU/GPU inter-processor \ncommunication. \n \nFragment Shading \nFragment shading in the traditional sense is heavily tied to the output of the GPU‘s \nrasterizer. Arbitrarily ―hooking into‖ the graphics pipeline to have the SPUs perform general-\npurpose fragment shading with current generations of graphics hardware is effectively \nimpossible. However, performing the heavy lifting for certain types of fragment shading that \ndo not necessarily require the use of the rasterizer, or even helping out the GPU with some \npre-processing as in [Swoboda09], is certainly feasible and in our experience has yielded \nsignificant performance benefits in real-world applications [Tovey10]. This section discusses \nsome of the techniques that will help you get the most out of the SPUs when shading \nfragments. \nBatch! Batch! Batch! \nIt might be tempting with initial implementations of pixel processing code on the SPU to \nadopt the approach of video hardware, such as the RSX. RSX processes pixels in groups of \nfour, known as quads [Möller08]. For sufficiently interleavable program code—in other \nwords, program code that contains little dependency between operations that follow one \nanother—this may be a good approach. However, in our experience, larger batches can \nproduce better results with respect to pixel throughput because there is a greater volume of \ninterleavable operations. Too few pixels result in large stalls between dependant operations, \ntime that could be better spent performing pixel shading, while larger batches cause high \nregister pressure and ultimately spilling. Moreover, in many applications that have a fixed \nsetup cost for each processing batch, you are doing more work for little to no extra setup \noverhead. \nSo, what is the upper bound on the number of pixels to process in a single batch of work? \nCan we simply process the entire buffer at once? The answer to this is not obvious and \ndepends on a number of factors, including the complexity of your fragment program and the \nnumber of intermediate values that you have occupying registers at any one time. Typically, \nthe two are inextricably linked. \n\n\n \n \nAs mentioned earlier, the SXU contains 128 registers, each 16 bytes in size. It is the task of \nthe compiler to multiplex all live variables in your program onto a limited register file[5]. \nWhen there are more live variables than there are registers—in other words, when register \npressure is high—the contents of some or all of the registers (depending on the size of the \nregister file) have to be written back to main memory and restored later. This is known as \nspilling registers. The more pixels one attempts to process in a batch, the higher the \nregister pressure for that function will be, and the likelihood that the compiler will have to \nspill registers back to the stack becomes greater. Spilling registers can become very \nexpensive if done to excess. The optimum batch size is hence the largest number of pixels \nthat one can reasonably process without spilling any registers back to the local store and \nwithout adding expense to the setup code for the batch of pixels. \n[5] The process of mapping multiple live variables onto a limited register file is known as register \ncoloring. Register coloring is a topic in its own right, and we will not cover it in detail here. \nPipeline Balance Is Key! \nAn efficient, well-written program will be limited by the number of instructions issued to the \nprocessor. Those processors with dual-issue capabilities, such as the SPU, have the \npotential to dramatically decrease the number of cycles that a program consumes. Pipeline \nbalance between the odd and even execution pipelines is critical to achieving good \nperformance with SPU programs. We will now discuss the requirements for instruction dual-\nissue and touch briefly on techniques to maximize instruction issue (through dual-issue) for \nthose programmers writing in assembly. \nThe SPU can dual-issue instructions under a very specific set of circumstances. Instructions \nare fetched in pairs from two very small instruction buffers [Bader07], and the following \nmust all be true if dual-issue is to occur: \n \nThe instructions in the fetch group must be capable of dispatch to separate execution \npipelines. \n \nThe alignment of the instructions must be such that the even pipeline instruction \noccupies an even-aligned address in the fetch group, and the odd pipeline in the \nodd-aligned address. \n \nFinally, there must be no dependencies either between the two instructions in the \nfetch group or between any one of the instructions in the fetch group and another \ninstruction currently being executed in either of the pipelines. \nProgrammers writing code with intrinsics rarely need to worry about instruction alignment. \nThe addition of nops and lnops in intrinsic form does not typically help the compiler to \nbetter align your code for dual-issue, and, in many cases, the compiler will do a reasonable \njob of instruction balancing. However, if you‘re programming in assembly language, the use \nof nop (and its odd-pipeline equivalent, lnop) will be useful in ensuring that code is \ncorrectly aligned for dual-issue. Of course, care must be taken not to overdo it and actually \nmake the resulting code slower. A good rule of thumb is never to insert more than two \nnops/lnops. \nCase Study: Light Pre-Pass Rendering in Blur \nLight pre-pass rendering is a variant of deferred shading first introduced by Wolfgang Engel \non his blog [Engel08] and later in [Engel09, Engel09a] at the same time it was derived \nindependently by Balestra et al. for use in Uncharted: Drake‘s Fortune [Balestra08]. The \ntechniques behind light pre-pass rendering are well understood and are discussed elsewhere \n[Engel08, Balestra08, Engel09, Engel09a, Tovey10], so a brief summary will suffice here. \nAs with all deferred rendering, the shading of pixels is decoupled from scene complexity by \nrendering out ―fat‖ frame buffers for use in an image space pass [Deering88, Saito90]. Light \npre-pass rendering differs slightly from traditional deferred shading in that only the data \n\n\n \n \nrequired for lighting calculations is written to the frame buffer during an initial rendering \npass of the scene. This has several advantages, including a warm Z-buffer and a reduced \nimpact on bandwidth requirements, at the expense of rendering the scene geometry twice. \nBecause one of the main requirements for the new engine written for Blur was that it should \nbe equipped to handle a large number of dynamic lights, the light pre-pass renderer was a \nvery attractive option. After implementing a light pre-pass renderer for Blur (which ran on \nthe RSX), it became apparent that we could get significant performance gains from \noffloading the screen-space lighting pass to the SPUs[6]. \n[6] Coincidentally, it was around this time that Matt Swoboda presented his work in a similar area, in \nwhich he moved a fully deferred renderer to the SPUs in [Swoboda09]; Matt‘s work and willingness to \ncommunicate with us was useful in laying the ground work for our implementation in Blur. \nThe lighting calculations in Blur are performed on the SPU in parallel with other non-\ndependent parts of the frame. This means that as long as we have enough rendering work \nfor the RSX, the lighting has no impact on the latency of a frame. Processing of the lighting \nbuffer is done in tiles, the selection of which is managed through the use of the SPE‘s \natomic unit. When the tiles are processed, the RSX is free to access the lighting buffer \nduring the rendering of the main pass. The results of our technique are shown in Color Plate \n6 and discussed in greater detail in [Swoboda09, Tovey10]. \nBenefits versus Drawbacks \nThe SPUs are powerful enough to perform fragment processing. This has been \ndemonstrated by developers with deferred shading, post-processing, and so on \n[Swoboda09, van der Leeuw09, Tovey10]. While general-purpose fragment shading is not \npossible, it is possible to perform a plethora of image-space techniques on the SPUs, \nincluding motion blur, depth of field, shadowing, and lighting. Parallelization with other non-\nrelated rendering work on the GPU can provide an extra gain if one‘s goal is to minimize \nframe latency. Such gains can even be made without the expense of an increased memory \nfootprint. \nRasterization on the SPUs has been achieved by a number of studios with good results, but \nthe use cases for this technique are somewhat restricted, usually being reserved for \nocclusion culling and the like rather than general-purpose rendering. Rasterization aside, \nthe most serious drawback to performing fragment shading on the SPUs is the lack of \ndedicated texture-mapping hardware. Small textures may be feasible, as they will fit in the \nlimited local store, but for larger textures or multiple textures, software caching is currently \nconsidered to be the best approach [Swoboda09]. \n \nFurther Work \nDue to the highly flexible nature of the SPUs in augmenting the processing power of the \nGPU, it is hard to suggest avenues of further work with any certainty. However, there are a \nfew significant challenges that warrant additional research efforts in order to further \nimprove the feasibility of some graphics techniques on the SPUs. \nTexture mapping is one such avenue of research. Currently, the best that has been done is \nthe use of a good software cache [Swoboda09] to try and minimize the latency of texture \naccesses from the SPUs. Taking inspiration from other convergent architectures, namely \nIntel Larrabee [Seiler08], we believe that the employment of user-level threads on the SPUs \nas a mechanism for hiding latency could certainly go some way to helping the prohibitively \nslow texture access speeds currently endured by graphics programmers seeking to help the \nGPU along with the SPUs. Running two to four copies of the same SPU program (albeit with \noffline modifications to the program‘s byte code) could allow a programmer to trade space \n\n\n \n \nin the local store for processing speed. The idea is simple: Each time a DMA is initiated, the \nprogrammer performs a lightweight context-switch to another version of the program \nresiding in the local store, which can be done cheaply if the second copy does not make use \nof the same registers. The hope is that by the time we return the original copy, the data we \nrequested has arrived in the local store, allowing us to process it without delay. Such a \nscheme would impose some limitations but could be feasible for small-stream kernels, such \nas shaders. \n \nConclusion \nThe SPUs are fast enough to perform high-end vertex and fragment processing. While they \nare almost certainly not going to beat the GPU in a like-for-like race (in other words, the \nimplementation of a full graphics pipeline), they can be used in synergy with the GPU to \nsupplement processing activities traditionally associated with rendering. The option to split \nwork between the two processing elements makes them great tools for optimizing the \nrendering of specific objects in a scene. The deferred lighting and car damage systems in \nBlur demonstrate the potential of the SPUs to work harmoniously with the GPU to produce \nimpressive results. \nLooking to the future, the ever-growing popularity and prevalence of deferred rendering \ntechniques in current generations of hardware further empower the SPUs to deliver \nimpressive improvements to the latency of a frame and allow game developers to get closer \nto synthesizing reality than ever before. \n \nAcknowledgements \nI would like to thank the supremely talented individuals of the Bizarre Creations Core \nTechnologies Team for being such a great bunch to work with, with special thanks reserved \nfor Steve McAuley for being my partner in crime with our SPU lighting implementation. \nThanks also go to Andrew Newton and Neil Purvey at Juice Games for our numerous \ndiscussions about SPU coding, to Matt Swoboda of SCEE R&D for our useful discussions \nabout SPU-based image processing, and to Wade Brainerd of Activision Central Technology \nfor his helpful comments, corrections, and suggestions. Last but not least, thanks also to \nJason Mitchell of Valve for being an understanding and knowledgeable section editor! \n \nReferences \n[Acton06] Acton, Mike. ―Branch-Free Implementation of Half Precision Floating Point.‖ \nCellPerformance. 17 July 2006. Mike Acton. 2 July 2009. \n<http://cellperformance.beyond3d.com/articles/2006/07/update-19-july-06-added.html>. \n[Acton08] Acton, Mike and Eric Christensen. ―Insomniac SPU Best Practices.‖ Insomniac \nGames. 2008. Insomniac Games. 2 July 2009. \n<http://www.insomniacgames.com/tech/articles/0208/files/insomniac_spu_programming_g\ndc08.ppt>. \n[Bader07] Bader, David A. ―Cell Programming Tips & Techniques.‖ One-Day IBM Cell \nProgramming Workshop at Georgia Tech. 6 Feb. 2007. Georgia Tech College of Computing. \n2 July 2009. <http://www.cc.gatech.edu/~bader/CellProgramming.html>. \n\n\n \n \n[Balestra08] Balestra, Christophe and Pal-Kristian Engstad. ―The Technology of Uncharted: \nDrake‘s Fortune.‖ Game Developers Conference. 2008. Naughty Dog Inc. n.d. \n<http://www.naughtydog.com/docs/Naughty-Dog-GDC08-UNCHARTED-Tech.pdf>. \n[Deering88] Deering, Michael, et al. ―The Triangle Processor and Normal Vector Shader: A \nVLSI System for High Performance Graphics.‖ Proceedings of the 15th Annual Conference on \nComputer Graphics and Interactive Techniques (1988): 21–30. ACM Portal. \n[Engel08] Engel, Wolfgang. ―Light Pre-Pass Renderer.‖ Diary of a Graphics Programmer. 16 \nMarch 2008. Blogspot.com. 4 July 2009. \n<http://diaryofagraphicsprogrammer.blogspot.com/2008/03/light-pre-pass-renderer.html>. \n[Engel09] Engel, Wolfgang. ―Designing a Renderer for Multiple Lights: The Light Pre-Pass \nRenderer.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang Engel. Boston: \nCharles River Media, 2009. 655–666. \n[Engel09a] Engel, Wolfgang. ―The Light Pre-Pass Renderer Mach III.‖ To appear in \nproceedings of ACM SIGGRAPH09, 2009. \n[IBM08] ―Cell Broadband Engine Programming Handbook.‖ IBM. 19 April 2006. IBM. n.d. \nhttps://www-\n01.ibm.com/chips/techlib/techlib.nsf/techdocs/7A77CCDF14FE70D5852575CA0074E8ED>. \n[IBM08a] ―Synergistic Processing Unit Instruction Set Architecture.‖ IBM. 27 Jan. 2007. \nIBM. n.d. <https://www-\n01.ibm.com/chips/techlib/techlib.nsf/techdocs/76CA6C7304210F3987257060006F2C44/$file\n/SPU_ISA_v1.2_27Jan2007_pub.pdf.> \n[IBM09] ―The Cell Project at IBM Research.‖ IBM. n.d. IBM. 4 July 2009. \n<http://researchweb.watson.ibm.com/cell/home.html>. \n[ILM09] ―OpenEXR.‖ OpenEXR. n.d. Lucas Digital Limited. 4 July 2009. \n<http://www.openexr.com>. \n[Kapoulkine09] Kapoulkine, Arseny. ―View frustum culling optimization—never let me \nbranch.‖ What Your Mother Never Told You About Graphics Development. 1 March 2009. \nBlogspot.com. 21 July 2009. <http://zeuxcg.blogspot.com/2009/03/view-frustum-culling-\noptimization-never.html>. \n[Möller08] Akenine-Möller, Thomas, Eric Haines, and Naty Hoffman. Real-Time Rendering, \n3rd Edition. Wellesley, MA: A.K. Peters, Ltd, 2008. \n[Perthuis06] Perthuis, Cedric. ―Introduction to the Graphics Pipeline of the PS3.‖ \nEurographics 2006. Austrian Academy of Sciences, Vienna, Austria. 6 Sept. 2006. \n[Saito90] Saito, Takafumi and Tokiichiro Takahashi. ―Comprehensible Rendering of 3-D \nShapes.‖ ACM SIGGRAPH Computer Graphics 24.4 (1990): 197-206. ACM Portal. \n[Seiler08] Seiler, Larry, et al. ―Larrabee: A Many Core x86 Architecture for Visual \nComputing.‖ ACM Transactions on Graphics 27.3 (Aug. 2008): n.p. ACM Portal. \n[Shippy09] Shippy, David and Mickie Phipps. The Race for a New Games Machine: Creating \nthe Chips Inside The New Xbox360 & The Playstation 3. New York: Citadel Press, 2009. \n[Swoboda09] Swoboda, Matt. ―Deferred Lighting and Post Processing on PLAYSTATION®3.‖ \nGame Developers Conference. 2009. Sony Computer Entertainment Eurpoe, Ltd. n.d. \n\n\n \n \n<http://www.technology.scee.net/files/presentations/gdc2009/DeferredLightingandPostProc\nessingonPS3.ppt>. \n[Tovey10] Tovey, Steven and Steven McAuley. ―Parallelized Light Pre-Pass Rendering with \nthe Cell Broadband Engine™.‖ GPU Pro: Advanced Rendering Techniques. Natick, MA: A K \nPeters Ltd., 2010. \n[van der Leeuw09] van der Leeuw, Michiel. ―The PLAYSTATION3‘s SPUs in the Real World—\nKILLZONE2 Case Study.‖ Game Developers Conference 2009. Moscone Center, San \nFrancisco, CA. 25 March 2009. \n \nSection 2: Physics and Animation \nIntroduction \nA Versatile and Interactive Anatomical Human Face Model \nCurved Paths for Seamless Character Animation \nNon-Iterative, Closed-Form, Inverse Kinematic Chain Solver (NCF IK) \nParticle Swarm Optimization for Game Programming \nImproved Numerical Integration with Analytical Techniques \nWhat a Drag: Modeling Realistic Three-Dimensional Air and Fluid Resistance \nApplication of Quasi-Fluid Dynamics for Arbitrary Closed Meshes \nApproximate Convex Decomposition for Real-Time Collision Detection \nIntroduction \nJeff Lander, Darwin 3D, LLC \njeffl@darwin3d.com \nGame creation as a business and an art has become much more mature, in years of \nexperience, in complexity, and in controversy of the material covered. For the most part, \nlong gone are the days when text adventures, simple frame flipping, and sprite-based \nanimation ruled the top-ten lists of gamers‘ hearts. Our games need to be much more real \nand complex to compete with the ever-increasing expectations of our audience. \nNowhere is this more evident than in the character performances and physical simulations \nof real (or imaginary) worlds. Players exposed to the amazing worlds that television and \nfilmmakers can create with visual effects rightly believe that their games should reflect \nthese advances and expectations as well. We need to bring our characters to life. We need \nto create worlds where the rules and systems that govern the reality have a basis in \nphysical realism and have a consistency that is at once familiar and exciting for our players. \nWe are past the point where players can be amazed by a simple animation clip of a \ncharacter running or watching a ball bounce on the ground. They have seen that all before. \nThey now expect a game‘s characters to react to the worlds around them in an intelligent \nway, as well as interact with the world in a way that models the physical interactions in our \nreal-life experiences. \n\n\n \n \nThe gems in this section represent the intersection of the physical interactions and animated \nperformances that we need in order to bring more of the illusion of life to our characters \nand worlds. In ―A Versatile and Interactive Anatomical Human Face Model,‖ Marco \nFratarcangeli discusses how to bring more realistic movement to facial animation to directly \nattack the problems with facial performance. This gem models the underlying facial \nanimation systems with physical simulation. In this same way, physical simulation is used in \nthe gems ―Application of Quasi-Fluid Dynamics for Arbitrary Closed Meshes‖ and ―What a \nDrag: Modeling Realistic Three-Dimensional Air and Fluid Resistance‖ to improve the realism \nin our interactive worlds. ―Particle Swarm Optimization for Game Programming‖ discusses \napplying easy-to-use particle simulation techniques to a variety of optimization problems. \nMuch of this is pretty advanced stuff. We will not be discussing how to play back an \nanimation on a hierarchical character or how to simulate and detect a collision between two \nobjects. You are expected to be masters of that kind of low-level system by now. \nWe are attacking larger problems now. For example, it is no longer sufficient for our \ncharacters to follow a simple piecewise linear path when moving across a space. That simply \nlooks too mechanical and robotic. In ―Curved Paths for Seamless Character Animation,‖ \nMichael Lewin discusses how it is necessary to smooth the results from our AI pathfinding \nsystems to create a movement path that follows a much more realistic curve, while still \navoiding all the obstacles that may be in the way. Adding to our character performance \nimprovements, in Philip Taylor‘s ―Non-Iterative, Closed-Form, Inverse Kinematic Chain \nSolver,‖ the existing iterative IK techniques are improved with an easy-to-understand, \nclosed-form solution. It is also not enough to take for granted little code snippets for \nnumerical integration we have seen posted on the net. It is important now for us to have a \ndeeper understanding of what is going on when we use something such as Euler integration \nfor a physical simulation and why it is important that we understand the error inherent in \nthese algorithms. ―Improved Numerical Integration with Analytical Techniques‖ looks \ndirectly at these issues and proposes methods to increase the accuracy in our simulations. \nAs we continue to create amazing new projects and push the envelope for what is possible \nto do in a game, I believe some of the most important steps forward will come at this \nintersection of animation and physics. As our animated characters become more physically \naware and grounded in our game environments, and our simulated worlds become inhabited \nby these responsive and emotional characters, our games will make huge leaps forward in \nconnecting to our audience. I hope these gems provoke some new ideas and encourage you \nall to inject just a little more life into your virtual creations. \n \n2.1. A Versatile and Interactive Anatomical Human Face Model \nMarco Fratarcangeli \nfrat@dis.uniroma1.it \nIn a compelling virtual world, virtual humans play an important role in improving the illusion \nof life and interacting with the user in a natural way. In particular, face motion is crucial to \nrepresent a talking person and convey emotive states. In a modern video game, \napproximately 5 to 10 percent of the frame cycle is devoted to the animation and rendering \nof virtual characters, including face, body, hair, cloth, and interaction with the surrounding \nvirtual environment and with the final user. Facial blend shapes are commonly adopted in \nthe industry to efficiently animate virtual faces. Each blend shape represents a key pose of \nthe face while performing an action (for example, a smiling face or raised eyebrows). By \ninterpolating the blend shapes with each other, as depicted in Figure 2.1.1, the artist can \nachieve a great number of facial expressions in real time at a negligible computational cost. \nFigure 2.1.1. Blend shapes interpolation. \n\n\n \n \n \n \nHowever, creation of the facial blend shapes is a difficult and time-consuming task, and, \nmore likely than not, it is still heavily dependant on the manual work of talented artists. \nIn this gem, I share principles and ideas for a tool that assists the artist in authoring \nhumanoid facial blend shapes. It is based on a physical simulation of the human head, \nwhich is able to mimic the dynamic behavior of the skull, passive tissues, muscles, and skin. \nThe idea is to let the artist design blend shapes by simply adjusting the contraction of the \nvirtual muscles and rotating the bony jaw. The anatomical simulation is fast enough to feed \nback the results in real time, allowing the artist to tune the anatomical parameters \ninteractively. \nOverview \nOur objective is to build a virtual model of the human head that simulates its anatomy and \ncan be adapted to simulate the dynamics of different facial meshes. The artist models a \nstatic face mesh, and then the anatomical simulation is used to generate its blend shapes. \nThe goal of the simulation is to create shapes that generate realistic motion. The exact \naccuracy of the individual muscles in a real anatomical model is not the goal. \nThe anatomical elements are simple yet expressive and able to capture the dynamics of a \nreal head. The anatomical simulation must also be fast enough to be interactive (in other \nwords, run at least at 30 fps) to allow the artist to quickly sketch, prototype, tune, and, \nwhere needed, discard facial poses. \nThe basic anatomical element is the skull. The anatomical model is not bound to the \npolygonal resolution or to the shape of the skull mesh; we require only that the skull mesh \nhas a movable jaw. On top of the skull, the artist may design several layers of muscles and \npassive tissue (such as the fat under the cheeks), the so-called muscle map. The skull and \nthe muscle map form the musculoskeletal structure that can be saved and reused for \ndifferent faces. The musculoskeletal structure is morphed to fit the shape of the target face. \nThen, the face is bound to the muscles and to the skull, and thus it is animated through a \nsimple and efficient numerical integration scheme. \n \nNumerical Simulation \nThe anatomical model is composed of different parts, most of which are deformable bodies, \nsuch as muscles, fat, and the skin. The dynamics algorithm must be stable enough to allow \ninteraction among these parts, it must be computationally cheap to carry out the \ncomputation at an interactive rate, and it must be controllable to allow precise tuning of the \nmuscles‘ contractions. To meet these requirements, we will use Position Based Dynamics \n(PBD), a method introduced in [Müller06] and recently embedded in the PhysX and Bullet \nengines. A less formal (although limited) description was introduced in [Jakobsen03] and \nemployed in the popular game Hitman: Codename 47. \n",
      "page_number": 104
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 113-120)",
      "start_page": 113,
      "end_page": 120,
      "detection_method": "topic_boundary",
      "content": " \n \nFor an easy-to-understand and complete formulation of PBD, as well as other useful \nknowledge about physics-based animation, you can review the publicly available SIGGRAPH \ncourse [Müller08]. In this gem, I describe PBD from an implementation point of view and \nfocus on the aspects needed for the anatomical simulation. \nIn most of the numerical methods used in games, the position of particles is computed \nstarting from the forces that are applied to the physical system. For each integration step at \na given time, we obtain velocities by integrating forces, and eventually we obtain the \nparticle‘s position by integrating velocities. This means that, in general, we can only \ninfluence a particle‘s position through forces. \nPBD works in a different way. It is based on a simple yet effective concept: The current \nposition of particles can be directly set according to a customizable set of geometrical \nconstraints \n, where \nis the set of particles involved in the simulation. Because \nthe constraints could cause the resulting position to change slightly, the velocity must be \nrecalculated using the new position and the position at the previous time step. As the \nposition is computed, the velocity is adjusted according to the current position and the \nposition at the previous time step. The integration steps are: \n(1) for each particle i do \n \n(2) for each particle i do \n \n(3) loop nbIterations times \n(4) solve \n \n(5) for each particle i \n(6) \n \nFor example, let us consider the simple case of two particles traveling in the space, which \nmust stick to a fixed distance d from each other. In this case, there is only one constraint C, \nwhich is: \nEquation 1 \n \n \nAssuming a particle mass m = 1kg, Step (3) in the Algorithm 1 is solved by: \n \n\n\n \n \nIn this example, if a force is applied to \n, it will gain acceleration in the direction of the \nforce, and it will move. Then, both \nand \nwill be displaced in Step (3) in order to \nmaintain the given distance d. \nGiven \n, finding the change in position Δpi for each particle is not difficult; however, it \nrequires some notions of vectorial calculus, which are outside the scope of this gem. The \nprocess is explained in [Müller06, Müller08] and partly extended in [Müller08b]. \nSteps (3) and (4) solve the set of constraints in an iterative way. That is, each constraint is \nsolved separately, one after the other. When the last constraint is solved, the iteration \nstarts again from the first one, and the loop is repeated nbIterations times, eventually \nconverging to a solution. Then, the velocity is accommodated in order to compensate the \nchange in position Δpi. In general, using a high value for nbIterations improves the \nprecision of the solution and the stiffness of the system, but it slows the computation. \nFurthermore, an exact solution is not always guaranteed because the solution of a \nconstraint may violate another constraint. \nThis issue is partly solved by simply multiplying the change in position Δpi by a scalar \nconstant K∊[0,..,1], the so-called constraint stiffness. For example, choosing k< for a \ndistance constraint leads to a dynamics behavior similar to the one of a soft spring. Using \nsoft constraints leads to soft dynamics and improves drastically the probability of finding an \nacceptable (and approximated) solution for the constraint set. For example, think about two \nrigid spheres that must be accommodated inside a cube with a diagonal length smaller than \nthe sum of the diameters of the two spheres: The spheres simply will not fit. However, if the \nspheres were soft enough, they would change shape and eventually find a steady \nconfiguration to fit in the cube. This is exactly how soft constraints work: If they are soft \nenough, they will adapt and converge toward a steady state. \nIn my experiments, I used a time step of 16.6 ms and found that one iteration is enough in \nmost of the cases to solve the set of constraints. Rarely did I use more iterations, and never \nbeyond four. \nSimilar to the distance constraint, other constraints can be formulated considering further \ngeometric entities, such as areas, angles, or volumes. The set of constraints defined over \nthe particles defines the dynamics of a deformable body represented by the triangulated \nmesh. I provide the source code for the distance, bending, triangular area, and volume \nconstraints on the companion CD-ROM. You are encouraged to experiment with PBD and \nbuild deformable bodies following the examples in the source code. \nPBD has several advantages: \n \nThe overshooting problem typical of force-driven techniques, such as mass-spring \nnetworks, is avoided. \n \nYou can control exactly the position of a subset of particles by applying proper \nconstraints; thus, the remaining particles will displace accordingly. \n \nPBD scales up very well with spatial dimensions because the constraint stiffness \nparameter is an adimensional number, without unit of measure. \n \nBuilding the Anatomical Model \nWe begin the process of building our virtual head by building up the low-level pieces that \nmake up the foundation of the physical motion. To do this, we take an anatomical approach. \nThe Skull \n\n\n \n \nThe skull is the basis upon which the entire computational model of the face is built. It is \nrepresented by a triangulated mesh chosen by the artist. Figure 2.1.2 illustrates an example \nof the mesh used in our prototype. The skull mesh is divided in two parts: the fixed upper \nskull and the movable mandible. The latter will move by applying the rigid transformation \ndepicted in Figure 2.1.2. \nFigure 2.1.2. (1) Example skull mesh, (2) jaw lateral slide, (3) jaw opening, (4) \njaw protruding. \n \n \n \nInteractive Sketching of the Muscles \nIn our model, muscles are represented by rectangular parallelepipeds, which are deformed \nto match the shape of the facial muscles. To define the shape of the muscles, we draw a \nclosed contour directly on the skull surface and the already-made muscles. Figure 2.1.3 \nshows an example of the definition of the initial shape of a muscle in rest state. The closed \ncontour is defined upon underlying anatomical structures—in this case, the skull. Then, a \nhexahedral mesh M is morphed to fit the contour. M is passively deformed as the skull \nmoves. \nFigure 2.1.3. Defining the shape of a muscle on the skull. \n \nThe closed contour is drawn through a simple ray-casting algorithm. The position of the \npointing device (I used a mouse) is projected into the 3D scene, and a ray is cast from the \nnear plane of the frustum to the far plane, as shown in Figure 2.1.4. \nFigure 2.1.4. Casting a ray to determine the intersection point. \n\n\n \n \n \n \nThe intersection points form the basis of the muscle geometry, the so-called action lines. \nAn action line is a piecewise linear curve lying on at least one mesh. The purpose of the \naction lines is twofold: (1) They define the bottom contour of the muscle geometry during \nthe simulation, and (2) they provide a mechanism to control the active contraction of the \nmuscle itself. A surface point is a point sp in S, where S is the surface represented by the \nmesh. A surface point sp is uniquely described by the homogeneous barycentric coordinates \n(t1, t2, t3) with respect to the vertices A1A2A3 of the triangular facet to which it belongs, as \nshown in Figure 2.1.5. \nFigure 2.1.5. (a) A surface point sp in A1A2A3 is defined by the homogeneous \nbarycentric coordinates (t1, t2, t3) with respect to the triangle vertices. (b) When \nthe triangle deforms, the triple (t1, t2, t3) does not change, and sp is updated to \nsp’. \n \nThe relevant attributes of a surface point are position and normal; both of them are \nobtained through the linear combination of the barycentric coordinates with the \ncorresponding attributes of the triangle vertices. When the latter displace due to a \ndeformation of the triangle, the new position and normal of sp are updated and use the new \nattributes of the vertices (Figure 2.1.5 (b)). Each linear segment of the action line is defined \nby two surface points; thus, an action line is completely described by the ordered list of its \nsurface points. Note that each single surface point may belong to a different surface S. So, \nfor example, an action line may start on a surface, continue on another surface, and finish \n\n\n \n \non a third surface. When the underlying surfaces deform, the surface points displace, and \nthe action line deforms accordingly. \nSoft Model for a Facial Muscle \nStarting from a triangulated, rectangular parallelepiped (or hexahedron), each vertex is \nconsidered as a particle with mass m = 1; particles are connected with each other to form a \nnetwork of distance constraints, as shown in Figure 2.1.6. \nFigure 2.1.6. Distance constraints used in the muscle model. \n \nThese constraints replicate some aspects of the dynamic behavior of a real face muscle, in \nparticular resistance to in-plane compression, shearing, and tension stresses. Note that \ndistance constraints are placed over the surface of the hexahedron, not internally. For \ncompleting the muscle model, we add bending constraints among the triangular faces of the \nmesh to conserve superficial tension. We also add a further volume constraint over all the \nparticles, which makes the muscle thicker due to compression and thinner due to \nelongation. \nThe Muscle Map \nUsing the interactive editor to sketch the shape of the soft tissues over the skull, we define \nthe structure made up of intertwined muscles, cartilage, and facial tissue, mostly fat. The \nmuscles are organized in layers. Each layer influences the deformation of the layers on top \nof it, but not those underlying it. See Figure 2.1.7. \nFigure 2.1.7. Different layers forming the muscle map used in the experiments. \n \n \n\n\n \n \nThe muscle map comprises 25 linear muscles and one circular muscle. This map does not \nrepresent the real muscular structure of the human head; this is due to the simulated \nmuscle model, which has simplified dynamics compared to the real musculoskeletal system. \nHowever, even though there may be not a one-to-one mapping with the muscle map in a \nreal head, this virtual muscle map has been devised to mimic all the main expressive \nfunctionalities of the real one. \nFor instance, on the forehead area of a real head, there is a single large, flat sheet muscle, \nthe frontalis belly, which causes almost all the motion of the eyebrows. In the virtual model, \nthis has been represented by two separate groups of muscles, each one on a separate side \nof the forehead. Each group is formed by a flat linear muscle (the frontalis) and on top of it \ntwo additional muscles (named, for convenience, frontalis inner and frontalis outer). On top \nof them, there is the corrugator, which ends on the nasal region of the skull. Combining \nthese muscles, the dynamics of the real frontalis belly are reproduced with a satisfying \ndegree of visual realism, even though the single linear muscle models have simple dynamics \ncompared to the corresponding real ones. \nEach simulated muscle is linked to the underlying structures through position constraints \nfollowing the position of surface points. Thus, when an anatomical structure deforms, the \nentire set of surface points lying on it moves as well, which in turn influences the motion of \nthe above linked structures. For instance, when the jaw, which is part of the deepest layer, \nrotates, all the deformable tissues that totally or partially lie on it will be deformed as well, \nand so on, in a sort of chain reaction that eventually arrives at the skin. \nActive contraction of a muscle is achieved by simply moving the surface points along the \naction lines. Given that the bottom surface of the muscles is anchored to the surface points \nthrough position constraints, when the latter move, the muscle contracts or elongates, \ndepending on the direction of motion of the surface points. \nFigure 2.1.8. The example muscle map is deformed by rotating the jaw and \ncontracting the frontalis belly. Note how all the above muscles are properly \ndeformed. \n \n \n \nMorphing the Muscle Map into the Target Face Mesh \nOnce the skull and the muscle map are ready, they can be morphed to fit inside the target \nfacial mesh, which represents the external skin. The morphing is done through an \ninterpolation function, which relies on the so-called Radial Basis Functions [Fang96]. We \ndefine two sets of 3D points, P and Q. P is a set of points defined over the surface of the \nskull and the muscles, and Q is a set of points defined over the skin mesh. Each point of P \n\n\n \n \ncorresponds to one, and only one, point of Q. The position of the points in P and Q are \nillustrated in Figure 2.1.9 (a) and must be manually picked by the artist. These positions \nhave been proven to be effective for describing the shape of the skull and of the human face \nin the context of image-based coding [Pandzic and Forchheimer02]. Given P and Q, we find \nthe interpolation function G(p), which transforms a point pi in P and a point qi in Q. Once \nG(p) is defined, we apply it to all the vertices of the skull and muscle meshes, fitting them \nin the target mesh. \nFigure 2.1.9. (a) The set of points P and Q picked on the skull and on the face \nmesh, respectively. (b) The outcome of the morphing technique. \n \n \nFinding the interpolation function G(p) requires solving a system of linear independent \nequations. For each couple 〈pi,qi〉, where pi is in P and qi is the corresponding point in Q, we \nset an equation like: \n \nWhere n is the number of points in each set, di is the distance of pi from pj, ri is a positive \nnumber that controls the ―stiffness‖ of the morphing, and hi is the unknown. \nSolving the system leads to the values of hi, i = 1, .., n, and thus G(p): \n \nThis particular form of Radial Basis Functions is demonstrated to always have a solution, it \nis computationally cheap, and it is particularly effective when the number of points in P and \nQ is scarce. Figure 2.1.9 (b) shows an example of fitting the skull into a target skin mesh. \nSkin \n\n\n \n \nSkin is modeled as a deformable body; its properties are defined by geometrical constraints \nin a similar way to the muscles. The skin is built starting from the target face mesh provided \nin input. Each vertex in the mesh is handled as a particle with a mass, which is set to 1.0. \nAfter the skull and the muscle map are fitted onto the skin mesh, further constraints are \ndefined to bind the skin to the underlying musculoskeletal structure. For each particle p in \nthe skin mesh, a ray is cast along the normal that is toward the outer direction. In fact, \nafter the fitting, portions of some muscles may stay outside the skin. By projecting in the \nouter direction, the skin vertices are first bound to these muscles. If no intersection is \nfound, then another ray is cast in the opposite direction of the normal, toward the inner part \nof the head. The ray is tested against the muscles from the most superficial to the deepest \none. If the ray does not intersect any muscle, then the skull is tested. The normal of the \nskin particle is created by averaging the normals of the star of faces to which the particle \nbelongs. \nIf an intersection is found, then it is defined as a surface point sp on the intersected \ntriangular face in the position where the ray intersects the face. A particle q is added to the \nsystem, and it is bound through a position constraint to sp. A stretching constraint is placed \namong the particles p and q. When the skull and the muscles move, the position of the \nsurface points will change accordingly. The set of added particle q is updated as well \nbecause it is bound to the surface points through the corresponding position constraint and \nwill displace the skin particles, whose final motion will depend also on the other involved \nconstraints. \n \nConclusion \nAlthough not very accurate from the point of view of biomechanics, the presented \nanatomical model is able to simulate convincing facial poses, including macro-wrinkles, \nwhich can be used as blend shapes, as shown in Color Plate 7. The model is stable, robust, \nand controllable, which is critical in interactive tools for producing video game content. The \nanatomical model is adaptive enough to animate directly the face mesh provided by the \nartist, thus it does not lead to the artifacts associated with motion retargeting. It does not \nrequire expensive hardware, and it may run on a consumer-class PC while still providing \ninteractive feedback to the artist. \n \nReferences \n[Fang96] Fang, Shiaofen, Raghu Raghavan, and Joan T. Richtsmeier. ―Volume Morphing \nMethods for Landmark Based 3D Image Deformation.‖ International Symposium on Medical \nImaging. 2710 (1996): 404–415. \n[Fratarcangeli08]. Fratarcangeli, Marco. ―A Computational Musco-Skeletal Model for \nAnimating Virtual Faces.‖ Ph.D. thesis, Universitá degli Studi di Roma ―La Sapienza.‖ 2008. \n[Pandzic and Forchheimer02] Pandzic, Igor S. and Robert Forchheimer. MPEG-4 Facial \nAnimation—The Standard, Implementation and Applications, 1st ed. John Wiley & Sons, \n2002. \n[Jakobsen03] Jakobsen, T. ―Advanced Character Physics.‖ 2003. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/2904/advanced_character_physics.php, 2003>. \n[Müller06] Müller, M., B. Heidelberger, M. Hennix, and J. Ratcliff. ―Position Based \nDynamics.‖ J. Vis. Commun. 18.2 (2007): 109–118. \n",
      "page_number": 113
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 121-128)",
      "start_page": 121,
      "end_page": 128,
      "detection_method": "topic_boundary",
      "content": " \n \n[Müller08] Müller M., D. James, J. Stam, and N. Thuerey. ―Real Time Physics.‖ 2008. \nMatthias Muller. n.d. <http://www.matthiasmueller.info/realtimephysics/index.html>. \n[Müller08b] Müller, M. ―Hierarchical Position Based Dynamics.‖ Proceedings of Virtual Reality \nInteractions and Physical Simulations. Grenoble, 2008. \n \n2.2. Curved Paths for Seamless Character Animation \nMichael Lewin \nmikelewin@cantab.net \nThe latest generation of consoles brings with it the promise of almost lifelike graphics and \nanimation. Yet for all the beautiful artwork, we are still seeing artifacts, such as character \nsliding, that break the illusion of the virtual world. Sliding occurs when we apply any extra \nrotation or translation to an animation that does not correspond to the way the character‘s \nlimbs are moving. This causes movement in which the feet are not planted firmly on the \nground, such as running on the spot, shifting left or right while walking, rotating unnaturally \nwhile walking or standing, or sliding upwards or downwards while walking on stairs. \nA tension exists between our desire for realistic human animation and our need to maintain \nprecise control of the character‘s position, orientation, and velocity. In many projects, there \nis not sufficient synergy between the AI and animation layers to satisfy both requirements \nsimultaneously. This gem presents a general way to adapt pathfinding techniques to better \ninteract with animation selection. We developed a technique at Sony Computer \nEntertainment that uses cubic Bezier curves to allow the AI system to be more closely \ncoupled with dynamics and animation, considering the constraints of the character‘s \nmovement. \nRelated Work \n[Johnson06] describes a method for fitting Bezier curves inside the cells of a navigation \nmesh to create a smooth path. This gem describes in a more general way that Bezier curves \ncan be fitted to any piecewise linear path. \n \nFinding a Path \nAll characters need to move around their environment, which usually involves fast-paced \ndynamic changes and contains unpredicted obstacles. This problem is just as relevant to \nrobot motion planning in the real world as it is to character movement in simulations such \nas games, and there is a wealth of literature on the subject applied to both domains. \nThe vast majority of pathfinding solutions create a piecewise linear path first and then fit a \ncurved path to it. There are a few examples where this intermediate step is not needed, \nsuch as using potential fields [Ratering95], but they do not give the same level of control as \nother methods. I will not consider them further in this article. \nThere are many different choices of representation when constructing a piecewise linear \npath. The A* algorithm is usually applied because it is simple, fast, and guaranteed to be \noptimal. Where techniques for generating pathfinding solutions differ lies in how to \nrepresent the physical space. It can be represented as a set of connected cells or points. I \nbriefly explain some of the main techniques; a thorough review can be found in [Tozour03] \n\n\n \n \nor [Latombe91], and an edifying demo can be downloaded at [Tozour05]. Figure 2.2.1 \nshows an example that compares the results of using the various methods. \nFigure 2.2.1. The various methods of creating a piecewise linear path: (a) regular \ngrid, (b) quad tree, (c) navigation mesh, (d) Voronoi diagram, and (e) visibility \ngraph. \n \n \nThe simplest method is to divide up the space into a grid of regular rectangular cells with a \nresolution that is small compared to the size of obstacles. A slightly more sophisticated \n\n\n \n \nversion of this is to use a quad tree instead of identical rectangles, so that areas of different \nresolution are defined as needed. \nNavigation meshes, as described in [Snook00, Tozour02], are an increasingly popular \nalternative to the grid solutions described previously. They partition the space using a \nvariety of irregular polygons so that each cell is either entirely filled by an obstacle or \nentirely empty. The partitioning may be performed by hand when the environment is \nauthored, or it can be generated automatically and then hand-edited later. [Axelrod08, \nMarden08] describe algorithms for dynamically recalculating the mesh to account for \nmoving obstacles. \nVoronoi diagrams are on first appearance similar to navigation meshes, but they yield quite \ndifferent paths. Obstacles are represented as a finite set of points, and the space is \npartitioned such that each point is inside a region, where that region is defined as the set of \npoints closer to that obstacle point than any other. The edges of the resulting diagram can \ndefine paths around the space provided that all edges that pass through an obstacle are \nremoved first. \nVisibility graphs are different from the previously described methods because they generate \na set of waypoints rather than partitioning the space into cells. The graph is defined by \npoints in the environment, in which pairs of points are connected by an edge if they can be \njoined by a straight line without intersecting any obstacles. The points are selected to be \nclose to the corners of obstacles (this is known as a corner graph), so that the character can \npass close to obstacles without colliding with them. A good demonstration of the technique \ncan be seen at [Paluszewski]. These methods are shown in Figure 2.2.1. \n \nSmoothing the Path \nThe AI programmer‘s challenge is to create a piecewise linear path through the environment \nso that the character appears as lifelike and natural as possible, which means it cannot \ninclude jagged edges. Piecewise polynomial parametric curves, more commonly known as \nsplines, are well suited to this task, and in particular cubic Bezier curves. This is a \nparametric curve of the form: \n \nAs Figure 2.2.2 illustrates, the control points P0 and P3 define the start and end points, \nrespectively, while the control points P1 and P2 define the curvature at the end points. The \nvectors P1-P0 and P3-P2 are referred to as the start and end velocity, respectively, because \nthey define the tangent and curvature at the two end points. In practical terms, this means \nwe can adjust their direction to control the character‘s initial and final direction of motion, \nand we can adjust their magnitude to control the curvature of the curve. Another useful \nproperty is that the curve will always lie within the boundary of the quadrilateral convex hull \ndefined by the four control points. \nFigure 2.2.2. Bezier curves are well suited to path fitting. The four control points \ndefine the curve’s position, shape, and curvature. The curve will always be \ncontained inside the convex hull defined by the control points. \n\n\n \n \n \n \nHaving constructed a piecewise linear path, we can fit cubic Bezier curves to it for a smooth \npath. By choosing the control points so that for each consecutive pair of curves the output \ndirection of the first matches the input direction of the second, we achieve continuity and \nsmoothness at the endpoints. Our objective is to adhere to the linear path only as much as \nnecessary. If the character‘s momentum is high and a large sweeping curve looks most \nnatural, and provided there are no obstacles in the way of that path, we want to select such \na path rather than sticking rigidly to the linear path. However, if the path is very close to \nobstacles and there is little room to maneuver, we want a path that is as curved as possible \nbut still avoids those obstacles. \nThe solution to this problem comes from an unlikely source: The open source graph plotting \nsoftware Graphviz [Graphviz] makes use of the same technique for drawing edges between \nits nodes [Dobkin97]. The inspiration comes from a 1990 article in the first Graphics Gems \nbook that was presented as a way to render fonts using vector graphics [Schneider90, \nSchneider90_2]. The basic principle is described by the following pseudocode: \nFitcurve(startPos, endPos, startVel, endVel) \n \ncreate a Bezier curve from startPos to endPos, \nusing startVel and endVel to define the control points \n \nwhile the curve intersects an obstacle: \n    reduce startVel and endVel \n    recalculate the Bezier curve \n    if startVel and endVel reach a user-defined minimum: \n        break \n \nif the curve still intersects an obstacle: \n    divide the path in two by choosing a point newPos along path \n    set newVel using the vector between two neighbors of newPos \n    Fitcurve(startPos, newPos, startVel, newVel) \n    Fitcurve(newPos, endPos, newVel, endVel) \n \nThe whole process of calculating the piecewise linear path and fitting the curved path to it is \nfast enough to run in a single frame. The path can therefore be recalculated whenever a \nnew obstacle renders the current path unfeasible. Note that the above recursive algorithm is \nguaranteed to terminate with a valid path because, in the limiting case, we are left with the \nsame piecewise linear path that we began with. \n\n\n \n \nFigure 2.2.3 shows an example. A piecewise linear path is created that avoids an obstacle. \nThen a single cubic Bezier curve is fitted to it, which takes into account the character‘s \nstarting momentum. But this curve intersects with another obstacle, so a new path is \ncreated, using two cubic Bezier curves, that passes through another point on the piecewise \nlinear path. \nFigure 2.2.3. Fitting a curve that accounts for character momentum. A single cubic \nBezier curve A (light dashed line) is fitted to the piecewise linear curve (light filled \nline). But this passes through another obstacle, so a new path B (heavy dashed \nline) is constructed from two cubic Bezier curves that passes through an \nintermediate point on the piecewise linear path. \n \n \nIn order for a character to walk along the Bezier path, we need to choose the maximum and \nminimum velocities for the curve. Consider the distance d from the start point to the end \npoint of a single Bezier curve. If the sum of the magnitudes of the start and end velocities \nexceeds d, the resulting curve can contain a loop. Usually we do not want this, so we can \nimpose d/2 as a maximum. A value of zero works perfectly well for a minimum, but we can \nalso use this as an opportunity to impose constraints based on the character‘s initial \nmomentum, so that sharp turns are not allowed if the character is moving fast. \nUnfortunately, the character‘s physical velocity is not identical to the concept of velocity in \nthe Bezier curve. This is because the latter is related to the curve‘s internal parametrization \nand therefore varies with curvature and curve length. Nonetheless, good results can be \nachieved by imposing a minimum kv/d, where v is the character‘s speed and k is a constant \ncalculated empirically. \n \nAnimation Selection \nThus we have created a smooth path to the end point that avoids any obstacles in the \nenvironment. All that remains is to select the appropriate animation for the character that \nwill take it along this path. There is a wealth of literature on this complicated topic, using \ntechniques such as motion graphs and inverse kinematics, which are beyond the scope of \nthis article. Instead, I will present a simple solution using animation blending that gives a \nsatisfactory degree of accuracy. \nTo test the system, I created a character with a family of hand-designed animations for \neach different gait (for example, walking, running, sprinting). Within each family, all the \nanimations had the same duration and path length but different amounts of rotation (for \nexample, 0, 22.5, 45, 90 degrees). In this way, we can generate motion with any amount of \nrotation by blending two of the animations together. Linear blending of two or more \nanimations simply means averaging the position and rotation of the skeleton joints. This is \ndone separately for each frame of the animation. The resulting output is therefore a mixture \nof the inputs. This will only look natural when the animations are quite similar to begin with, \nas is the case here. Figure 2.2.4 shows an example. \n\n\n \n \nFigure 2.2.4. A family of animations that can be blended together. Each has the \nsame path length but a different amount of rotation (0, 22.5, 45, and 90 degrees). \n \n \nWe can chain the animations together because each one begins and ends at the same point \nin the character‘s stride. We can transition seamlessly between different gaits using \ntransition animations, authored in the same way as a family of animations with different \ndegrees of rotation. \nAs one animation ends, a new one must be selected. It is possible at this moment that the \ncharacter‘s position and orientation do not perfectly match the curved path. We could \nchoose an animation that will return the character to the path, but if his orientation is \nwrong, this will result in a zigzagging motion as the character veers too far one way with \none step and then too far the other way with the next. It is better to ensure the character‘s \nfinal orientation matches the tangent to the path, even if this means there is some error in \nhis position. \nThe method described gave sufficiently accurate results with a wide range of walking and \nrunning gaits; Figure 2.2.5 shows an example. This technique is limited, however, as Figure \n2.2.6 shows. The animations we can create by blending in this way always describe a \ncircular arc. All we can do is set the curvature of this arc. Some portions of a Bezier curve, \nhowever, cannot be described by a circular arc, and this is what causes the character to \ndeviate from the path. For this reason, more sophisticated animation selection techniques \nwould work better. \nFigure 2.2.5. There is not much error between the dotted line, made of circular \narcs, and the full line, made of four cubic Bezier curves. Each dot represents the \nstart of a new arc. \n\n\n \n \n \n \nFigure 2.2.6. (a) A possible blended output: a circular arc with a rotation of 67.5 \ndegrees. (b) An impossible output: No single circular arc can fit it. \n \n \n \n \nConclusion \nIn conclusion, this gem has presented a simple and effective way to generate a curved path \nfrom any piecewise linear path that facilitates animation selection and promotes interaction \nbetween the AI and animation layers of a character. Future work should look at applying \nthis to common physical constraints, such as enforcing a run-up before jumping or picking \nup an object. An important future extension to this work is to address how best to manage \nfeatures of a three-dimensional terrain, such as stairs, gaps, and ledges. \n \nReferences \n\n\n \n \n[Axelrod08] Axelrod, Ramon. ―Navigation Graph Generation in Highly Dynamic Worlds.‖ AI \nGame Programming Wisdom 4. Boston: Charles River Media, 2008. \n[Dobkin97] Dobkin, David P., et al. ―Implementing a General-Purpose Edge Router.‖ \nProceedings of Graph Drawing 1997: 262–271. \n[Graphviz] Graphviz ―Spline-o-matic.‖ n.d. Graphviz. n.d. \n<http://www.graphviz.org/Misc/spline-o-matic/>. \n[Johnson06] Johnson, Geraint. ―Smoothing a Navigation Mesh Path.‖ AI Game Programming \nWisdom 3. Boston: Charles River Media, 2006. \n[Latombe91] Latombe, Jean-Claude. Robot Motion Planning. Kluwer Academic Publishers, \n1991. \n[Marden08] Marden, Paul. ―Dynamically Updating a Navigation Mesh via Efficient Polygon \nSubdivision.‖ AI Game Programming Wisdom 4. Boston: Charles River Media, 2008. \n[Paluszewski] Paluszewski, Martin. ―Robot Motion Planning (applet).‖ n.d. University of \nCopenhagen. n.d. <http://people.binf.ku.dk/palu/robotmotion/index.html>. \n[Ratering95] Ratering, Steven and Maria Gini. ―Robot Navigation in a Known Environment \nwith Unknown Moving Obstacles.‖ Autonomous Robots. 1.1 (June 1995): n.p. \n[Schneider90] Schneider, Philip J. ―An Algorithm for Automatically Fitting Digitized Curves.‖ \nGraphics Gems. Academic Press Professional, Inc., 1990. \n[Schneider90_2] Schneider, Philip J. ―A Bezier Curve-Based Root-Finder.‖ Graphics Gems. \nAcademic Press Professional, Inc., 1990. \n[Snook00] Snook, Greg. ―Simplified 3D Movement and Pathfinding Using Navigation \nMeshes.‖ Game Programming Gems. Boston: Charles River Media, 2000. \n[Tozour02] Tozour, Paul. ―Building a Near-Optimal Navigation Mesh.‖ AI Game Programming \nWisdom. Boston: Charles River Media, 2002. \n[Tozour03] Tozour, Paul. ―Search Space Representations.‖ AI Game Programming Wisdom \n2. Boston: Charles River Media, 2003. \n[Tozour05] Tozour, Paul. ―Pathfinding Algorithms & Search Space Representations Demo.‖ \n16 July 2005. <http://www.ai-blog.net/archives/000091.html>. \n \n2.3. Non-Iterative, Closed-Form, Inverse Kinematic Chain Solver \n(NCF IK) \nPhilip Taylor \nptaylor@trapdoorinc.com \nInverse kinematics (IK) has many uses in games. A primary use is to control the limbs of \ncharacters—to fit the pose of the character to the terrain it is standing on or to pin a foot \nwhile walking to reduce foot sliding, as described in [Forsyth04]. For many characters, a \nsimple two-bone solver is all that is required because, as in the case of human characters, \nthere are only two major bones in a chain that need to be modified. \n",
      "page_number": 121
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 129-138)",
      "start_page": 129,
      "end_page": 138,
      "detection_method": "topic_boundary",
      "content": " \n \nThe problem of solving a two-bone chain is often reduced to a two-dimensional problem by \nconstraining the solution to lie on a plane, usually defined by the root of the chain, the IK \ngoal, and an up-vector position value. These two-bone solutions are considered ―closed \nform‖ because the solution can be found using trigonometry [Lander98]. When it comes to \nchains with more than two bones, there are several well-known algorithms used to solve \nthis problem. Coordinate Cyclic Descent (CCD), Jacobian Transpose, or Pseudo-Inverse are \ncommonly used. \nThese algorithms suffer from performance issues because multiple iterations are required to \nconverge on a solution. Without sufficient iterations, the chain may not reach the goal \nwithin acceptable limits, and the chain may exhibit irregular movements between frames, \ncausing visual artifacts. Additionally, they lack precise control over the resulting shape of \nthe chain. \nIn this gem, I present a new method for solving inverse kinematics on chains comprising \nany number of bones by solving each bone separately in two dimensions. This solution is \nnon-iterative in that only a single evaluation per bone is required, while guaranteeing a \ncorrect solution if the goal is within reach of the chain. The algorithm does not require extra \nparameters, such as up-vector position values or preferred angles. Instead, the algorithm \nattempts to preserve the initial shape of the chain while reaching for the goal, maintaining \nthe integrity of any pose or animation data that was present on the chain prior to the IK \nsolver‘s evaluation. \nContext \nConsider the context of the character within a game. We are usually not building a set of \nrotations to define the chain pose; rather, we are modifying a set of orientations to more \nclosely fit a world space constraint. \nDuring the evaluation of a game scene, the animation system is sampled to provide a local \nspace transform per bone, and these local space pose transforms get concatenated together \nto build a global space pose. The pose at this point in the engine‘s evaluation is artist \ndefined and part of what describes the style and personality of the character. \nMaintaining the integrity of the character‘s pose, or motion, while introducing new \nconstraints, such as foot planting or lever pulling, is desirable as significant investment has \nbeen made in defining these motions. Any modifications made to the pose should be \nminimized to avoid breaking the original intent of the motion. \nIn this gem, the pose defined by the animation system is referred to as the forward \nkinematic pose, or FK pose. Forward kinematic refers to the way that the pose was built, by \naccumulating local space transforms down through the hierarchy to generate global space \ntransforms. \n \nBone Definition \nFor the purpose of this gem, a bone is defined as containing a position and an orientation. \nThe position is represented as a vector triple, and the orientation is represented as a \nquaternion. As illustrated in Figure 2.3.1, we refer to the vector that runs along its length as \nthe bone length vector, and the length of a bone is defined as the length of this vector. By \nconvention, bones are aligned with their local X-axis. A chain is defined as a linear hierarchy \nof bones, usually lined end to end. \nFigure 2.3.1. Bone definition. \n\n\n \n \n \n \n \n \nIK Goal Position \nWithout considering hands, feet, or any other child bones of the chain, a solution is defined \nas solving the chain such that the tip of the last bone touches, or comes as close to \ntouching as possible, some predetermined IK goal position. The exact location of this IK \ngoal position is defined by the engine and is not within the scope of this gem. \nThe animation engine might be locking the foot to a plant position over the course of a \ncharacter‘s step, or the hand of a character could be constrained to the steering wheel of a \nvehicle. In both cases, we have a bone chain with animation applied and a desired goal \nposition, which is defined by the character‘s environment. \nOne-Bone IK \nThe simplest chain would only comprise one bone, and the closest valid solution would be to \nalign the bone with the goal. While this example may appear trivial, the rest of this gem \nbuilds upon this concept. \nThe vector between the bone and the goal is called the bone-to-goal vector, and we refer to \nthe desired angle between the bone length vector and the bone-to-goal vector as the IK \nangle. \nWhen solving a chain comprising one bone, the best that can be achieved is to adjust the \nbone‘s pose such that the bone length vector is aimed at the IK goal, as shown in Figure \n2.3.2. This is done by incrementing the rotation using the angle between the bone length \nvector and the bone-to-goal vector. The axis of rotation is perpendicular to both the bone \nlength vector and the bone-to-goal vector. This rotation is the shortest arc of rotation that \nwill transform the bone vector onto the bone-to-goal vector. Computing this quaternion \ndirectly is described in the article by [Melax06]. \nFigure 2.3.2. Aiming a bone at an IK goal position. \n\n\n \n \n \n \n \nTwo-Bone IK \nWhen an additional bone is added into the system, the mathematics becomes more \ncomplex. \nAligning the Chain to the IK Goal \nMaintaining the shape of the limb while solving IK requires that each bone retain the \nrespective transforms with bones prior to it and after it in the chain. For example, if the \nbones in the chain all lie on one plane prior to solving, then after solving the bones should \nall still lie on a single plane. \nThe first step to solving a chain is to offset the orientation of each bone by the shortest arc \nbetween the root of the chain and the chain tip, and the root of the chain and the IK goal \nposition, as shown in Figure 2.3.3. This step ensures that all further deformation will occur \non the same plane with respect to the overall chain shape, as the original chain pose \ndescribes. This step also has the effect of biasing most of the deformation to the first joint \nin the chain. For most characters‘ limbs this is desirable, but joint limits can be imposed to \nrestrict this deformation. See the ―Future Work‖ section. \nFigure 2.3.3. Aligning the chain to the IK goal. \n \nWe first apply the overall alignment to each bone and then solve using the appropriate \nmethod for that bone. \n\n\n \n \nCalculating Bone 0 in a Two-Bone Chain \nThe vector from a bone to the tip of the chain prior to solving is called the bone-to-chain-tip \nvector, and the angle between the bone length vector and the bone-to-chain tip is the FK \nangle, as shown in Figure 2.3.4. To calculate the IK angle for Bone 0, we use the law of \ncosines equation providing the parameters a, b, and c. \nEquation 1  \n \n \nFigure 2.3.4. Solving the first bone in a two-bone chain. \n \n \nOnce we have calculated the IK angle, we subtract this from the current angle between the \nbone length vector and the bone-to-goal vector to get a delta angle, which we will use to \nmodify the bone‘s pose. The axis around which we rotate the bone is the cross product of \nthe bone-to-goal vector and the bone-to-chain-tip vector. We then modify the bone‘s \norientation by incrementing the rotation using the axis and the delta angle. \nSolving the Last Bone \nBefore any child bone can be solved, its new global position must be calculated by \ntransforming the bone‘s length vector using the new orientation of its parent and adding it \nto the parent‘s position. Bone 1, or the last bone in any chain, is solved using the one-bone \nsolution described previously in the section ―One-Bone IK.‖ \nThree-Bone IK Solver \n\n\n \n \nI have shown that a two-bone problem can be decomposed into two types of bones that are \neach solved using an appropriate method. The first thing to consider about a three-bone \nchain is that it encapsulates the two-bone chain described previously with one extra bone at \nthe start of the chain, as shown in Figure 2.3.5. Therefore, once a solution can be found for \nBone 0, then bones one and two can be derived using the previously described methods. \nFigure 2.3.5. Three-bone chain. \n \n \n \nMaximum Bone Angles \nConsider that the initial pose of the chain defines an ideal pose for the limb, and any \nmodification of the limb pose must be minimized. By analyzing the FK pose of the chain, for \nexample, in Figure 2.3.6, we can determine a value that describes how bent Bone 0 is with \nrespect to the rest of the chain. \nFigure 2.3.6. Calculating the bone angle and the maximum FK and IK bone angles. \n\n\n \n \n \n \nWe can use the law of cosines to calculate a value that defines the angle of Bone 0 if the \nremaining chain were to be laid out in a straight line. The remaining chain length, the \ndistance to the FK chain tip, and the bone length are used to calculate the max bone angle. \nThis angle is referred to as the max FK bone angle. Comparing this max FK bone angle to \nthe actual FK bone angle gives us a value that defines our FK bone angle relative to the \ndistance to the FK chain tip. \nEquation 2  \n \n \nThe bone angle fraction is defined relative to the initial shape of the chain and is a \ncorrelation between the bone‘s orientation and the rest of the chain pose. Conversely, the \nremaining bone length can also be used to calculate the maximum possible angle that the \nbone can assume in IK. This maximum IK bone angle is multiplied by the bone angle \nfraction value to determine the new IK bone angle. \nFour-Bone IK Solver \nFigure 2.3.7 illustrates that with chains comprising three or more bones, the bones before \nthe last two bones are all solved using the method described previously. Considering only \nthe bone length, the remaining bone lengths, the distance to the FK chain tip, and the \ndistance to the IK goal, a bone angle can be calculated. Once the IK bone angle has been \ncalculated for Bone 0, then we simply continue down the chain, and Bone 0 can be solved in \nexactly the same way. Bone 2 can then be solved using trigonometry, and Bone 3 is simply \naligned with the target. \nFigure 2.3.7. Applying the method to a four-bone chain. \n\n\n \n \n \n \n \nN-Bone IK Solver \nAn N-bone chain consists of three categories of problems. Bones 0 to N–3 are all solved \nusing the method described previously of calculating maximum IK and FK bone angles to \nderive an IK bone angle. Bone N–2 can be solved using trigonometry, and Bone N–1 is \nsimply aligned with the goal. \nFunction SolveIKChain( chain ) \nbegin \n    calculate chain target alignment \n \n    for each bone in chain \n    begin \n        apply chain target alignment to bone \n \n        if bone is last bone \n            aim bone at target \n        else if bone is second last \n            use trigonometry to calculate bone angle \n        else \n        begin \n            determine FK bone angle \n            determine maximum FK bone angle \n            determine maximum IK bone angle \n \n\n\n \n \n            IK bone angle = ( FK bone angle / maximum FK bone \nangle ) * \n                            maximum IK bone angle \n        end \n    end \nend \n \n \n \nHandling Extreme Deformation \nIn some cases, the remaining chain length is greater than the bone length plus the distance \nto the chain tip or IK goal. In these cases, the technique described previously of using \ntrigonometry to calculate the maximum FK and IK angles cannot generate a maximum \nangle greater than π. Furthermore, during animation there may be visual artifacts if the \nmax IK bone angles hit this limit. \nAs shown in Figure 2.3.8, if the distance between the bone and the chain tip is greater than \nthe remaining chain length, then trigonometry is used as described previously. Once the \ndistance to the IK goal is less than the remaining chain length, the maximum IK and FK \nbone angle values can be calculated. \nFigure 2.3.8. Handling chains with extreme curvature. \n \n \nThe following pseudocode describes how to calculate the maximum FK bone angle for a long \nchain. \nif( distToFkChainTip > remainingChainLength ) \n{ \n\n\n \n \n       use trigonometry to calculate maximum bone angle \n} \nelse \n{ \n       maxFkBoneAngle = acos( ( boneLength/2 ) / \nremainingChainLength ) \n       maxFkBoneAngle += ( remainingChainLength - \ndistToFkChainTip ) / \nboneLength; \n       } \n \nEffectively, the remaining chain length is applied in an arc around the one position, allowing \nus to define maximum angles greater than π. This technique gives a much greater range of \nangles for the maximum FK/IK bone angle values, while maintaining the important limits \nwhen the chain is extended. \n \nAdditional Details \nWe have only discussed the basic case in this gem. Here are some additional details that \nmay be helpful for specific cases. \nIK/FK Blending \nIn many cases, you will want to turn off IK because it may not be applicable. Simply \ndisabling IK evaluation will cause a visual pop in the pose of your character. A better \napproach is to interpolate your IK pose back to your original FK pose before disabling IK. To \ndo this, you need to keep an IK pose buffer separate from your FK pose buffer. \nAnimated Joint Offsets \nSo far in this gem, we have only considered chains with static local offsets along the local X-\naxis. This solver can readily be extended to support animated joint offsets. \nThe local position offset of a joint can be used as the bone length vector of the parent bone, \nas shown in Figure 2.3.9. The FK bone angle and max FK/IK bone angles are all calculated \nusing this vector and its length, as described in this gem. This ensures that the bones pivot \nin the same position relative to their parent bone and that local position offsets are still \napplied in parent space. Because we are required to know the total length of the chain \nbefore solving, the entire chain‘s length needs to be measured prior to solving for IK. \nFigure 2.3.9. Animated joint offsets. \n \n \n \n\n\n \n \nExtension Limit \nOne concern with applying inverse kinematics to a chain in a game is that often the IK \nsolvers will generate artifacts in the motion of the limb when the limb reaches the limit of its \nextension. The term hyperextension refers to the visual pop that happens when a chain \nreaches the limits of its reach. Fixing this issue is a minor addition to the IK solver \nalgorithm. In the code samples provided with the book, extension dampening has been \nimplemented to show how this can be achieved. \n \nFuture Work \nOur algorithm will not address all cases that you will come across. In the future, we plan to \nextend the system in several ways. \nSliding Joints \nThis IK solver will not currently calculate new local joint position offsets but only calculates \norientation changes for each bone in the hierarchy. For example, it will not extend a \nhydraulic joint to reach the goal. Sliding joints could be implemented using a similar \ntechnique as that used to calculate angles. By defining the FK bone‘s local position relative \nto a slide vector, a new local offset position could be defined by comparing the distance to \nthe FK chain tip and the actual distance to the IK goal. \nJoint Limits \nAside from limiting the overall extension of the chain, it may be required that certain bones \ndo not rotate beyond a certain limit or that they are not free to rotate on any plane. \nBecause this chain is evaluated from top to bottom, it is possible to start imposing limits on \nangles. A simple way to do this is to limit the angle generated for each bone. In this way, \nwe can limit the angle of a bone with respect to its parent. \nDOF Constraints \nCreature limbs are usually made up of a ball joint followed by a collection of hinge joints. In \nthis gem, however, no twisting is ever applied to the chain, meaning that the solver only \nmodifies the pose of the root bone on two axes, and every other joint rotates on only one \naxis. The axis used to modify the bone‘s orientation is assumed to be perpendicular to the \nbone vector and the bone-to-goal vector. Chains with limited degrees of freedom are not \nsupported in the algorithm presented in this gem. If an axis of motion were defined for a \nparticular bone, then solving would still be feasible by calculating the maximum bone angles \nprojected onto the plane defined by this axis. \nGeneral-Purpose Programming on Graphics Processors \nThe algorithm presented in this gem does not employ any recursion, or major branching, \nand has a fixed cost per evaluation. These features make it an ideal candidate for \nimplementations using some of the newer SIMT architectures, such as CUDA or OpenCL. \nPerhaps, as in the case of shaders, specific versions of the solver would be generated for \ncategories of chains and the loops unrolled. A two-, three-, and four-bone version could be \ngenerated and the loops unrolled at compile time. Depending on the bone count, a chain \nwould be solved in a batch with all other limbs of the same structure. This would avoid \ncostly synchronization points, which would slow all chain evaluations to the cost of the \nlongest chain. \n \n",
      "page_number": 129
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 139-147)",
      "start_page": 139,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": " \n \nConclusion \nIn this gem, I have presented a new method for solving inverse kinematics on chains of \nbones that is simple, fast, and accurate. This is achieved by calculating bone angles directly \nusing values derived from the initial pose of the chain and the position of the IK goal. This \nmethod minimizes the modification of the original chain‘s pose, while ensuring the goal is \nalways reached if within range. There are many possibilities for further development to \nexpand this concept beyond a simple chain solver. \n \nReferences \n[Forsyth04] Forsyth, Tom. ―How to Walk.‖ 2004. Game Tech. n.d. <http://www.game-\ntech.com/Talks/HowToWalk.ppt>. \n[Lander98] Lander, Jeff. ―Oh My God, I Inverted Kine!‖ Game Developer. (September \n1998): 9–14. \n[Melax06] Melax, Stan. ―The Shortest Arc Quaternion.‖ Game Programming Gems. Boston: \nCharles River Media, 2006. 214–218. \n \n2.4. Particle Swarm Optimization for Game Programming \nDario L. Sancho-Pradel \ndariosancho@gmail.com \nThis gem presents the foundations of Particle Swarm Optimization (PSO), a simple yet \npowerful meta-heuristic optimization technique that can be applied to complex non-linear \nproblems, even in the absence of a precise analytical formulation of the system. As a result, \nPSO is used in a variety of engineering applications, such as artificial neural network \ntraining, mechanical design, and telecommunications. I have also applied this technique to \nrobotic systems. PSO is related to other population-based search strategies, such as Genetic \nAlgorithms (GA), and can solve similar problems. However, PSO works natively with real \nnumbers and tends to be more efficient than GA, often reaching a near-optimal solution in \nfewer function evaluations [Hassan05]. Next-generation consoles are pushing the \nboundaries of realism and complexity in game development. Often, games contain large \nsets of parameters that drive or influence the behavior of various systems, such as physics, \nanimation, or AI. More often than not, the final values of many of these parameters are the \nresult of a manual selection process based on experience and trial and error. Under certain \nbasic conditions, PSO can be a valuable tool for better tuning these parameters in an \nautomated fashion. \nA Few Words on Optimization \nOptimization problems are typically modeled as finding the maximum or minimum value of \na set of m-dimensional real functions, whose variables are normally subject to constraints. \nMaximizing a function f is equivalent to minimizing the function –f and vice versa. Using \nvector notation, the optimization problem is reduced to finding the roots (solutions) of the \nvector function: \nEquation 1  \n\n\n \n \n \n \nwhere, for the general case of p non-linear equations in m variables: \nF(X) = [f1 (X),..., fp (X)]T, X = x1,..., xm]T and 0 = [0,...,0]T \nIn some cases, Equation (1) may be solved analytically, which means that the optimum \npoint (or set of points) can be calculated exactly. In most real applications, however, this is \nnot possible, and numerical methods are applied to find an approximate solution. \nNumerical Methods \nNumerical methods are algorithms that return a numerical value that in most cases \nrepresents an approximation to the exact solution of a problem. Classical optimization \ntechniques have been successful at solving many optimization problems common in industry \nand science. However, there are optimization scenarios that classical approaches cannot \nsolve in a reasonable amount of time. Combinatorial optimization problems—in other words, \nproblems with a discrete set of solutions from which we want to find the optimal one—and \nNP-hard problems in general are good examples. For those cases, approximate solutions \ncan be obtained relatively quickly using meta-heuristic approaches (that is, high-level \nstrategies used for guiding different heuristics in search problems). \nNo Free Lunch (NFL) Theorem for Optimization \nOptimizing can be regarded as a search problem where the selected optimization method \nrepresents the particular style or mechanism of executing the search. The NFL theorem \n[Wolpert97] states that for finite spaces and algorithms that do not resample points, the \nperformance of all search (optimization) algorithms averaged over all possible objective \nfunctions is the same. In practice, however, we do observe that some algorithms perform, \non average, significantly better than others over certain objective functions. This is because \nthe objective functions considered in most real optimization scenarios have a structure that \nis far from random. As a result, algorithms that exploit such a structure will perform, on \naverage, better than ―uninformed‖ search strategies (for example, a linear sequential \nsearch). The PSO search strategy is based on an exchange of information between members \nof a swarm of candidate solutions and operates under the assumption that ―good solutions‖ \nare close together in the search space. If this is not the case, PSO will not perform better \nthan a random search. \n \nThe PSO Paradigm and Its Canonical Formulation \nPSO is a stochastic, population-based computer algorithm modeled on swarm intelligence. \nThe PSO paradigm was originally developed by Dr. Eberhart and Dr. Kennedy [Kennedy95], \nan electrical engineer and a social psychologist, respectively, who had the idea of applying \nsocial behavior to continuous non-linear optimization problems. \nThey reasoned that social behavior is so ubiquitous in the animal kingdom because it \noptimizes results. Social behavior is the result of an exchange of information within the \nmembers of the society. The original PSO algorithm evolved from a bird flocking simulator \ninto an optimization tool motivated by the hypothesis that social sharing of information \namong members of the society offers an evolutionary advantage. In order to implement and \nexploit that flow of information, each member was provided with a small amount of memory \nand a mechanism to exchange some knowledge with its social network (in other words, its \nneighbors). Although different improvements and variations have been proposed since the \n\n\n \n \ninitial PSO algorithm was presented, most of them resemble closely the original formulation. \nA significant exception is the Quantum PSO (QPSO) algorithm [Sun04], which for reasons of \nspace will not be covered here. \nCanonical Equations of Motion \nThe way PSO works is by ―flying‖ a swarm of collision-free particles over the search space \n(also called the problem space). Each particle represents a candidate solution of the \noptimization problem, and each element (dimension) of the particle represents a parameter \nto be optimized. The movement of each particle in the swarm is dictated by the equations of \nmotion that the particular flavor of the PSO algorithm defines. In its canonical form, these \nequations can be expressed as: \nEquation 2  \n \n \nwhere x(t) and V(t) represent respectively the particle‘s position and velocity at time t. By \nchoosing Δt = 1, t becomes the iteration step. Note that x(t) and V(t) are m-dimensional \nvectors. At t = 0, the position of each particle is selected from a uniform random \ndistribution, i.e. xj (0) = U(xj min, xj max), j = 1,2,...m, where xj min, xj max are the \nsearch space boundaries for the j-th dimension. V(0) can either be randomly initialized or \nset equal to the zero vector. \nVelocity Update \nAs Equation (2) shows, the search process is driven by the velocity update, based on: \n \nCognitive information. Experience gained during the particle‘s search, expressed \nas the best location ever visited by the particle xCognitive_Best. \n \nSocially exchanged information. The best location found so far by any member of \nthe particle‘s social network (xSocial_Best). The calculation of xSocial_Best depends on the \nchosen network‘s topology. Figure 2.4.1 shows some common topologies. In a fully \nconnected network (for example, Star), xSocial_Best represents the best location found \nby any particle in the swarm, hence called global best (gBest). If the swarm is not \nfully connected xSocial_Best becomes the best position found by any particle in its local \nneighborhood, hence called local best (IBest). Using IBest tends to provide more \naccurate results, whereas algorithms running gBest execute faster [Engelbrecht02]. \nThe neighborhood is typically defined based on the particles‘ indices, although spatial \ninformation could also be used. Allowing neighborhood overlapping helps the \ninformation exchange across the swarm. \nFigure 2.4.1. Three classical social network structures: (a) Star topology \n(fully connected), (b) Ring topology, (c) Cluster topology. \n\n\n \n \n \n \nInertia. Provides certain continuity to the motion of the particle. Adding inertia to \nthe velocity update helps to decrease the change in momentum between two \nconsecutive iterations and provides a means of controlling the balance between the \nparticle‘s exploration and exploitation. An exploratory strategy tends to direct the \nparticle toward unexplored areas of the problem space. Exploitation refers to the \nmovement of a particle around previously explored areas, resulting in a finer-grain \nsearch on these areas. Clearly, low inertia will favor exploitation, whereas higher \nvalues will result in a more exploratory behavior. \nIn general, the PSO strategy is to use the cognitive and social best positions as attractors in \nthe search space of the particle. It achieves that by defining the particle‘s velocity vectors \nas: \n \nwhere ω∊(0,1). r1 and r2 are random m-dimensional vectors taken from a uniform random \ndistribution, in other words, ri = {ri\nj ∊ U(0,1)}, j = 1,2,...,m, i=1,2. The values of the two \nacceleration coefficients are normally chosen to be the same (typically cCognitive = cSocial ∊ \n(0,2.05]). It is important to notice that here the operator ⊗ denotes a per-component \nmultiplication of two vectors, and therefore its result is another vector. \nFinally, the equations of motion for the basic canonical PSO system can be expressed as: \nEquation 3  \n \n \nAlgorithm 1 details the basic canonical PSO formulation implementing the gBest strategy, \nand Figure 2.4.2 illustrates graphically the position and velocity update of one particle based \non the equations of motion presented in Equation (3). \nAlgorithm 1: Canonical PSO (<<eq32.pdf>> version) \n 1: // Create a swarm of N randomly distributed particles \n 2: FOR EACH particle(P) i=1,...,N do \n 3:          FOR EACH dimension j=1,..,M do \n 4:              P[i].X[j] = Xmin[j]+rand(0,1)*(Xmax[j]- \nXmin[j]); \n\n\n \n \n 5:              P[i].V[j] = rand(0,1)*Vmax[j]*sign(rand(0,1)-\n0.5);// or =0; \n 6:          END \n 7: END \n 8: g_Best = P[0]; \n 9: numIterations = 1; \n10: // Iterative optimisation process \n11: REPEAT \n12:    FOR EACH particle(P) i=1,...,N do \n13:          IF Eval(P[i].X) BETTER_THAN Eval(g_Best.X) \n14:              g_Best = P[i]; \n15:          END \n16:          IF Eval(P[i].X) BETTER_THAN Eval(P[i].x_best) \n17:              P[i].x_best = P[i].X; \n18:          END \n19:    END \n20:    // Apply_Equations_of_Motion \n21:    FOR EACH particle i=1,...,N do \n22:          FOR EACH dimension j=1,..,M do \n23:              V_Inertia = w*P[i].V[j]; \n24:              V_social = c1*rand(0,1)*(g_Best.X[j] - \nP[i].X[j]); \n25:              V_cognitive = c2*rand(0,1)*(P[i].x_best[j] - \nP[i].X[j]); \n26: \n27:              P[i].V[j] = V_Inertia + V_social + V_cognitive; \n28:                     Clamp(P[i].V[j]); // optional \n29:              P[i].X[j] = P[i].X[j] + P[i].V[j]; \n30:                     Clamp(P[i].X[j]); // or any other \nstrategy \n31:          END \n32:  END \n33:  numIterations++; \n34: UNTIL (m_numIterations > MAX_NUM_ITER || \nGOOD_ENOUGH(Eval(g_Best))) \n \nFigure 2.4.2. Illustration of the velocity and position updates of a particle during \nthe optimization of a simple 2D parabolic function f(x,y). (Left) Geometrical \nillustration showing contour lines of f(x,y). (Right) Sketch of f(x,y) illustrating the \nparticle’s relevant information and labels. \n\n\n \n \n \n \nThe position update may take the particle outside of the predefined boundary. In fact, it has \nbeen proven that in high-dimensional swarms, most of the particles will leave the search \nspace after the first iteration [Helwig08]. Some strategies to constrain the particles to the \nsearch space include: \n \nMoving the particle to its closest boundary and setting its velocity to zero. \n \nAssuming a cyclic problem space. For instance, for ε < | x j max – x j min |, if x j \n(t) = x j max + ε, then it is recalculated as x j (t) = x j min + ε . \n \nAllowing the particle to leave the boundary, omitting its evaluation if the objective \nfunction is not defined at the new coordinates. \n \nReinitializing all components whose values are outside of the search space. \nEvaluating the Particles: The Objective Function \nThe objective function, also referred to as the fitness function or cost function, does not \nneed to be the actual mathematical representation of the system to be optimized, but a \nmeasure of the quality of any solution found. Therefore, each call to Eval(P[i].X) in \nAlgorithm 1 requires the simulation of the system using the parameters encoded in P[i]. \nDuring the simulation, various indicators related to the performance of the solution are \nrecorded and combined in a function to provide a numerical measure of performance. This \nfunction is the objective function. \nImagine that we want to optimize the parameters that define the way an NPC performs a \ncertain task, such as handling a car. Most likely, we do not have a dynamic model of the car \nand its controllers, so we cannot directly optimize the non-linear system of equations that \ndefines the problem at hand. Nevertheless, by defining an objective function that penalizes, \nfor instance, the time the NPC spends outside the training path and the time required to \ndrive a predetermined distance, we are effectively giving the particles valid feedback to \nevaluate their quality, and therefore a means of improving in later iterations. \n \nAdd-Ons to the Classical Formulation \nBeyond the basic formulation, we can add a few improvements to deal with specific cases \nthat may come up in a particular use case. \nVelocity Clamping \n\n\n \n \nThe formulation described so far may generate particles with increasingly large oscillations \naround a potential optimum due to an ―explosion‖ in the velocity values. This issue was \naddressed in [Kennedy01] by the introduction of a positive velocity clamping parameter \nVmax, which modifies the velocity update as follows: \n \nwhere V j culated (t + 1) is the j-th component of the velocity vector calculated using \nEquation (3). The selection of Vmax is problem dependent. Typically, each of its components \nis initialized within the size of the search space boundaries, in other words, V j max = κ · | x j \nmax – x j min |, κ ∊ (0,1). Initially, we want κ to be close to 1 in order to favor exploration. \nDuring the optimization process, Vmax can be periodically adjusted by: \n \nSetting Vmax to the dynamic range of the particle. \n \nUsing linearly or exponentially decaying Vmax in order to progressively move toward \nexploitation (fine-grained searches) of the area around the best particle(s). \n \nIncreasing or decreasing Vmax after α iterations without improvement in the best \nfound solution. \nVariable Inertia Weight ω(t) \nThe inertia coefficient has a clear influence in the particle‘s search strategy. Large values of \nω favor exploring new areas, while small values favor the exploitation of known areas. \nInitially, we would like the algorithm to favor exploration, and as we approach the vicinity of \nthe optimum, we could decrease the value of ω in order to perform finer-grain searches. \nThere are many ways to define a decreasing ω(t) . A common one is a linear interpolation \nbetween the desired values of ωInitial and ωFinal : \n \nwhere t is the current iteration and T represents the number of iterations required to reach \nωFinal, keeping ω(t) = ω,Final for t > T. Another strategy is to select T = MAX_NUM_ITER/N, \nresetting ω(t) and applying again the interpolation every N iterations. \nConstriction Coefficient \nBased on the dynamic analysis of a simplified and deterministic PSO model, Kennedy and \nClerc [Clerc02] reformulated the velocity update in terms of the constriction coefficient χ. \nThey also analyzed the convergence properties of the algorithm, obtaining a set of \ntheoretical optimal values for the PSO parameters to control both convergence and the \nvelocity explosion. A simple constriction model for velocity update can be expressed as: \nV (t + 1) = χ · [V(t) + φ1 · (X Cognitive_Best – X (t)) + φ 2 · (X Social_Best – X(t))] X (t + \n1) = X(t) + V(t + 1) \nwhere X is a diagonal matrix, whose elements are calculated as: \n\n\n \n \n \nand: \nφ = φ1 + φ2, φ > 4, x jj ∊ (0,1), κ ∊ (0,1) \nLow values of κ encourage exploitation, whereas high values result in a more exploratory \nsystem. Typical values are χjj = 0.7298 and θ1 = θ2 = 2.05. Notice that this model can be \neasily converted into the original inertia-based formulation. \nExtended Full Model for Velocity Update \nThe velocity update could include both the global best and the local best particle: \nV (t + 1) = ω · V(t) + Θ 1 ⊗ (X Cognitive._Best – X(t)) + Θ 2 ⊗(g Best – X(t)) + Θ 3 ⊗ \n(IBest; – X(t)) \nwhere typically Θ i ∊ U(0,1). \nAdding Constraints \nIn an optimization scenario, the constraints define feasible subspaces within the search \nspace of the unconstrained problem. In the original PSO algorithm, each dimension is \nbounded individually. (See Line 4 of Algorithm 1.) This translates into an axes-aligned \nhyper-rectangular search space. Therefore, constraints such as g (x) ≤ b cannot be \ngenerally defined. As a result, neither a simple circular search space (x2 + y2 ≤ radius 2), \nnor a combination of constraints (for example, a rectangular search space with a \nrectangular hole inside), is supported. Different approaches have been proposed to extend \nthe constraint-handling capabilities of evolutionary algorithms. (For a review, see \n[Michalewicz96] and [Mezura09].) A simple approach presented in [Hu02] uses the \ncanonical PSO algorithm and introduces two modifications: \n \nAll particles are initialized inside the feasible region. \n \nAfter evaluating the population, only particles within the feasible region can update \nthe values of xCognitive_Best and xSocial_Best. \nInteger Optimization \nAlthough PSO deals natively with optimization problems of continuous variables, it can also \nbe modified to cope with discrete and mixed-parameter problems. The simplest way to do \nthat is to use the same equations of motion, but round the results of the integer \ncomponents (at least for the position update) to the nearest integer. In some cases this can \nresult in invalid particles (for example, duplicated elements in a particle are generally not \nallowed in permutation problems), and particles need to be ―fixed‖ after every position \nupdate. The fixing mechanism is problem dependent, but it may consist of making all the \nelements of a particle different, while keeping their values within a certain range. \nMaintaining Diversity \nA common problem in numerical optimization is premature convergence—in other words, \ngetting trapped around a local optimum. In stochastic methods, the diversity of the \npopulation has a major effect on the convergence of the problem; hence, diversity loss is \ncommonly regarded as a threat to the optimization process. Some ways of favoring diversity \nare: \n\n\n \n \n \nAlternating periodically PSO with other search methods. \n \nEvery α iterations, reinitializing (randomizing) the location of a random number of \nparticles in the swarm. \n \nEvery β stalls, reinitializing the system. \n \nIncluding mutation and crossover operators and applying them to the position of a \nsmall subset of particles in the swarm. \n \nRunning multiple independent swarms, allowing an occasional exchange of \ninformation (for example, gBest) and/or particles between swarms. Furthermore, each \nswarm could use different search strategies (for example, some could use IBest, some \nothers could use gBest, some could use parameters that favor exploration, some \nothers could use more exploitative ones, and so on). \nIt is important to notice that the selected networking strategy also has an impact on the \ndiversity of the population. Distributed topologies, such as Ring or Cluster, generate a \nnumber of local attractors and tend to maintain higher levels of diversity, leading to good \nsolutions more often than other centralized approaches. \n \nA Note on Randomness \nPSO is a stochastic optimization technique that initializes its particles by placing them \nrandomly in the search space. In order to do so, pseudo-random values are drawn from a \nuniform distribution. The reason is that in the absence of any prior knowledge about the \nlocation of the optimum, it is best to distribute the particles randomly, covering as much of \nthe problem space as possible. However, as Figure 2.4.3(a) shows, the result may be less \nuniform than desired. The spatial uniformity of the randomized particles can be improved by \nusing Halton point sets, which are uniformly distributed and stochastic-looking sampling \npatterns, generated by a deterministic formula at low computational cost [Wong97]. \nFigure 2.4.3. One instance of two sets of 25 particles initialized in a two-\ndimensional space using (a) a uniform random distribution and (b) a Halton \nsequence. \n \n \n \n \nCase Study 1: Typical Benchmarking Functions \n",
      "page_number": 139
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 148-160)",
      "start_page": 148,
      "end_page": 160,
      "detection_method": "topic_boundary",
      "content": " \n \nThere are sets of functions that are commonly used for benchmarking optimization \nalgorithms. These functions tend to have areas with a very low gradient (for example, \nRosenbrock‘s saddle) and/or numerous local optima (for example, Rastrigin‘s function). \nFigure 2.4.4 illustrates the 2D versions of three of them. The accompanying CD includes the \nmultidimensional version of a larger set of benchmarking functions together with a PSO \nalgorithm implementation. You are encouraged to experiment with the different PSO \nparameters, observing the variation in convergence speed and accuracy. More information \nabout different benchmarking functions can be found in [Onwubolu04]. \nFigure 2.4.4. 2D versions of some common benchmarking functions. \n \n \n \nCase Study 2: Optimization of Physical Parameters for In-Game \nVehicle Simulation \nTo obtain realistic vehicle responses in a game, simplified physical models of their key \ncomponents are generated. These models contain a number of physical parameters that \ndefine the way the vehicle reacts in many situations. Those parameters are often obtained \nbased on experience and a trial-and-error process of selection. Such a manual process \ntends to be inevitably slow, and as a result, little of the search space is often covered. It is \nclear that there is room for improvement, and PSO is a good candidate for automating the \nsearch process. The only prerequisite is to be able to estimate numerically the quality of \neach candidate solution. The following example illustrates the general principle. We want to \noptimize the performance of a car, whose simplified model is shown in Figure 2.4.5. The \nmodel includes friction between the tires and the ground, the suspension, and the car‘s \nmass distribution. Based on the model‘s parameters, the particle‘s components can be \ndefined as: \nP.X[0] = Forward Friction Coeff.; P.X[1] = Side Friction Coeff.; \nP.X[2] = Spring Constant (Rear); P.X[3] = Damper Constant \n(Rear); \nP.X[4] = Spring Constant (Front); P.X[5] = Damper Constant \n(Front); \nP.X[6] = Mass 1; P.X[7] = Mass 1 location (along Z); P.X[8] = \nMass 2; \n \nFigure 2.4.5. A simplified physics model of a car. \n\n\n \n \n \n \nAs an example of dimensionality reduction, the value and location of the mass M3 and the \nlocation of the mass M2 have been fixed. They can be included at any moment in the \noptimization by extending the dimensionality of the particle. \nMost importantly, we need to define an objective function capable of quantifying the \nresponse of the vehicle while following a path along a network of roads. The objective \nfunction to be minimized could include the following variables: distance to goal on a crash, \ntotal distance driven without being on the marked spline, accumulated distance from the \ncar‘s center of mass to the marked spline (shown as d in Figure 2.4.6 (b)), accumulated \nheading deviation with respect to the selected path, oscillations around the Z-axis, and tilt \naround the Z-axis on curves (c.f. Figure 2.4.6 (a)). The function can also include big \npenalties for crashing and in cases where only two wheels are in contact with the road. The \nfinal objective function could be a quadratic combination of each of these variables ri. \n \nFigure 2.4.6. Illustration of some of the parameters included in the objective \nfunction. (a) Tilt around the Z-axis during a curve. (b) Distance to the path’s \nsplines and heading error. \n \n \n\n\n \n \nThe normalization constant ηi ensures that all the variables have the same weight (typically \nbetween 0 and 1), regardless of their particular ranges. The relative importance of ri, with \nrespect to the rest of the variables, is represented by the coefficient ki, which can also be \nused for scaling the objective function. Squaring ri is useful in cases where only the \nmagnitude of ri is relevant, ensuring that the objective function does not decrease based on \nthe sign of the variables. \nAs Algorithm 1 showed, each iteration requires the evaluation of the whole population (in \nother words, N particles). Therefore, it would be convenient to evaluate N vehicles \nsimultaneously following identical paths. \n \nCase Study 3: Physics-Based Animation of Mechanical Systems \nPhysics-based animation can improve the gaming experience by adding physical realism and \nbelievability to the game. Cloth and hair procedural animation, ragdolls, and general \ncollisions between rigid bodies are some examples featured in many titles. Extending this \nconcept to complex mechanical systems, such as robotic NPCs and vehicles, can result in \nmore realistic behaviors that are in accordance with our experience of how mechanical \nsystems move. \nFigure 2.4.7 shows a hexapod NPC robot exhibiting a tripod gait, which is a gait pattern \noften used by insects and other animals due to its performance and stability. Gait patterns \ncan be defined by a series of parameters (for example, maximum joint angles, feet \ntrajectory, stance phase offset, stance duty factor, step duration) whose values will depend \non the physical characteristics of the robot, the environment, the pattern configuration, the \ntask (for example, turn, go forward, climb, and so on), and the task qualifiers (for example, \nfast, safe, and so on). PSO can be used for finding good values for these parameters. Each \ngait could be optimized individually by using specialized objective functions and running \nsingle-gait simulations. Next, gait transitions could be worked out. Once the optimization is \nfinished, the result would be a hexapod robotic NPC with newly acquired locomotion skills. \nFigure 2.4.7. Snapshot of a hexapod NPC robot using a tripod gait. The legs shown \nin white are lifted, while the darkened ones are in stance. \n \n \nThe design of the robot can be taken one step further. The configuration of the legs could \nbe optimized by adding the length of their segments and the number of joints into the \noptimization‘s parameters list. Ultimately, PSO could be used as a tool for evolving different \ntypes of robots by including the number of legs and their position in body coordinates as \noptimization parameters. \nOnce the robot design is finished and the gait parameters optimized, the gait controller of \nthe robot would send the appropriate sequence of actions to the different joints, and the \nphysics engine would do the rest. Incidentally, the controller of each joint (typically some \n\n\n \n \nflavor of proportional-integral-derivative controller, or PID) could also be optimized using \nPSO. \nLet‘s see one final example. We want to find the parameters that generate the fastest \nwalking gait for the particular robotic NPC illustrated in Figure 2.4.8.Figure 2.4.8 (b) shows \nthe feet trajectory selected for this gait. This half-ellipse trajectory is defined by two \nparameters, namely the step height (h) and the step size (s). Another important parameter \nis the duration of the step (Ts). To improve stability, the torso of the robot is initially tilted \nsideways to shift its center of mass toward the supporting leg (refer to Figure 2.4.8 (a)). \nThis can be parameterized by a maximum tilt angle (α) and the percentage of step time that \nthe body is tilted toward the leg in stance (Tt) before shifting the weight to the other side. \nAssuming a 50-percent duty cycle for each leg, the structure of the particles could be \nP.X[]= {h,s,Ts,α,Tt}. \nFigure 2.4.8. Illustration of some basic parameters in a humanoid-like robot. (a) \nWeight-shifting angle for improving stability, (b) half-ellipse gate parameters, (c) \nthe different joint angles in a crouching action. \n \n \nThe objective function could be designed as in Case Study 2, taking into account the \nwalking time, the distance to the goal before falling, the accumulative heading error, the \naccumulative position error with respect to the given path, and the stability of the gait \nmeasure in terms of oscillations around various axes. It is worth noticing that the gait \nparameters obtained are subject to external factors, such as the friction between the robot‘s \nfeet and the ground. In principle, it could be possible to find a continuous mapping between \nthe walking parameters and different types of terrains by feeding the friction data and the \nPSO solutions to a neural network or a fuzzy inference system. In that way, the robotic NPC \ncould try to adapt its gait to optimize the walking performance when the type of terrain \nchanges. \nGaits are not the only movements that could be optimized. Learning other actions, such as \ncrouching (refer to Figure 2.4.8 (c)) or climbing stairs would follow the same principles. \n \nConclusion \nThe aim of meta-heuristic optimization algorithms is to provide approximated solutions to \ncomplex non-linear optimization problems that more traditional approaches have difficulties \n\n\n \n \naddressing. These algorithms are not guaranteed to converge at a good solution, but they \nare designed to find good approximations to the global optimum with high probability. \nPopulation-based algorithms are sensitive to the diversity of their individuals and to their \nconfiguration parameters. PSO relies on a small set of intuitive parameters, such as the \nnumber of particles, the maximum number of iterations, and the topology of the social \nnetwork. Often, the mapping between an optimization problem and the function that \nquantifies the quality of a solution is not unique. In these cases, the quality of the selected \nobjective function has a significant impact on the convergence of the algorithm. This gem \nhas shown a number of variations proposed around the canonical PSO algorithm. The \nsimplicity of the PSO paradigm makes its extension a relatively simple task. \nGames are becoming extremely complex and highly parameterized software products. In \nmany cases, the response of different systems is driven, or at least influenced, by sets of \nparameters stored in configuration files. PSO could be a useful tool for optimizing some of \nthese parameters in an automated fashion. AI, physics, and animation are examples of \nsystems that could benefit from this optimization technique. \n \nReferences \n[Clerc02] Clerc, M. and J. Kennedy. ―The Particle Swarm-Explosion, Stability and \nConvergence in a Multidimensional Complex Space.‖ IEEE Transactions on Evolutionary \nComputation 6 (2002): 58–73. \n[Engelbrecht02] Engelbrecht, A.P. Computational Intelligence: An Introduction. Wiley, 2002. \n[Hassan05] Hassan, R., B. Cohanim, and O. de Weck. ―A Comparison of Particle Swarm \nOptimization and the Genetic Algorithm.‖ Proceedings of 46th AIAA/ASME/ASCE/AHS/ASC \nStructures, Structural Dynamics and Materials Conference. 2005. \n[Helwig08] Helwig, S. and R. Wanka. ―Theoretical Analysis of Initial Particle Swarm \nBehavior.‖ Proceedings of the 10th International Conference on Parallel Problem Solving \nfrom Nature (Sept 2008): 889–898. \n[Hu02] Hu, X. and R. Eberhart. ―Solving Constrained Nonlinear Optimization Problems with \nParticle Swarm Optimization.‖ 6th World Multiconference on Systemics, Cybernetics and \nInformatics (SCI 2002): 203–206. \n[Kennedy95] Kennedy, J. and R. Eberhart. ―Particle Swarm Optimization.‖ Proceeding of \nIEEE International Conference on Neural Networks 4 (Dec. 1995): 1942–1948. \n[Kennedy01] Kennedy, J., R.C. Eberhart, and Y. Shi. Swarm Intelligence. Morgan Kaufmann \nPublishers, 2001. \n[Mezura09] Mezura-Montes, E. Constraint-Handling in Evolutionary Optimization. Springer, \n2009. \n[Michalewicz96] Michalewicz, Z. and M. Schoenauer. ―Evolutionary Algorithms for \nConstrained Parameter Optimization Problems.‖ Evolutionary Computation 4 (1996): 1–32. \n[Onwubolu04] Onwubolu, G. and B. Babu. New Optimization Techniques in Engineering. \nSpringer, 2004. \n[Sun04] Sun, J., B. Feng, and Wenbo Xu. ―Particle Swarm Optimization with Particles \nHaving Quantum Behavior.‖ Proc. Cong. Evolutionary Computation 1 (June 2004): 325–331. \n\n\n \n \n[Wolpert97] Wolpert, D.H. and W.G. Macready. ―No Free Lunch Theorems for Search.‖ IEEE \nTransactions on Evolutionary Computation 1.1 (April 1997): 67–82. \n[Wong97] Wong, Tien-Tsin, Wai-Shing Luk, and Pheng-Ann Heng. ―Sampling with \nHammersley and Halton Points.‖ Journal of Graphics Tools 2.2 (1997): 9–24. \n \n2.5. Improved Numerical Integration with Analytical Techniques \nEric Brown \nanalytic.spinors@gmail.com \nThere is a fairly standard recipe for integrating the equation of motion in the context of a \ngame physics engine. Usually the integration technique is based on the Symplectic Euler \nstepping equations. These equations are fed an acceleration, which is accumulated over the \ncurrent time step. Such integration methods are useful when the exact nature of the forces \nacting on an object is unknown. In a video game, the forces that are acting on an object at \nany given moment are not known beforehand. Therefore, such a numerical technique is \nvery appropriate. \nHowever, though we may not know beforehand the exact nature of the forces that act on an \nobject, we usually know the exact nature of forces that are currently acting on an object. If \nthis were not so, we would not be able to provide the stepping equations with the current \nacceleration of the body. If it were possible to leverage our knowledge of these current \nforces, then we might expect to decrease the error of the integration dramatically. \nThis gem proposes such a method. This method allows for the separation of numerical \nintegration from analytic integration. The numerical integration steps the state of the body \nforward in time, based on the previous state. The analytic integration takes into account the \neffect of acceleration acting over the course of the current time step. This gem describes in \ndetail the differences and implications of the integration techniques to aid the physics \ndeveloper in understanding design choices for position, velocity, and acceleration updates in \nphysics simulation. \nAs we build up to the introduction of this method, we will first discuss a heuristic model for \nclassifying errors of integration techniques. \nClassifying Errors \nOften, the method for classifying errors in integration techniques is to label them as first \norder, second order, and so on. Methods that are first order have an upper bound error on \nthe order of the time step taken to the first power. Methods that are second order have an \nupper bound error on the order of the time step to the second power. Taking a small \nnumber to a large power makes the small number smaller. Thus, higher-order methods \nyield more accuracy. \nError can also be classified in terms of how well an integrator conserves energy. Integrators \nmight add or remove energy from the system. Some integrators can conserve energy on \naverage. For instance, the semi-implicit, or Symplectic Euler, method is a first-order \nmethod, but it conserves energy on average. If an integrator adds energy to the system, \nthe system can become unstable and diverge, especially at higher time steps. The accuracy \nof a method can affect its stability, but it does not determine it, as shown by the Symplectic \nEuler method. More often than not, it is the stability of a method that we desire, more than \nthe accuracy. \n\n\n \n \nIn this gem we will be taking a different approach to classifying error. This approach is \nbased on the fact that the stepping equations usually assume that the acceleration is \nconstant over the course of a time step. The kinematics of constant acceleration are a \nproblem that can be solved easily and exactly. Comparing the kinematic equations of \nconstant acceleration with the results of a numerical method provides qualitative insight into \nsources of error in the method. \nWhen derivatives are discretized, it is done by means of a finite difference. Such a finite \ndifference of positions implies that the velocity is constant over the course of the time step. \nIn order to introduce a non-constant velocity, you must explicitly introduce an equation \ninvolving acceleration. Similarly, the only way to deal with non-constant acceleration is to \nexplicitly introduce an equation involving the derivative of acceleration. Since many \nnumerical methods do not involve any such equation, we are safe in making the comparison \nwith the kinematic equations for constant acceleration, at least over the course of a single \ntime step. \n \nKinematics of Constant Acceleration \nWe know the exact form of the trajectory of particles that are subject to a constant \nacceleration. \n \nWe can compare this set of equations to the results of common numerical methods in order \nto gain a qualitative idea about the error in the method. Consider the standard Euler \nmethod: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn Δ t \nThis set of equations can be transformed into a set that more closely resembles the \nkinematic equations by inserting the velocity equation into the position equation: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn-1 Δ t + an-1 Δ t2 \nThe appearance of vn-1 in the position equation is due to the fact that we must insert vn and \nmust therefore re-index the velocity equation. The differences in form of this equation to \nthe kinematic equations can be considered as qualitatively representative of the error of the \nmethod. \nWe may perform this same procedure with the Symplectic Euler method: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn+1 Δ t \nInsert the velocity equation into the position equation to transform to the kinematic form: \n\n\n \n \nvn+1 = vn + an Δ t \nxn+1 = xn + vn Δ t + an Δ t2 \nThis equation is a much closer match in form to the kinematic equations. We can use this \nresemblance to justify the fact that the Symplectic Euler method must in some way be \nbetter than the standard Euler method. \n \nThe Kinematic Integrator \nIf we are trying to find an integration method that when converted to a kinematic form is \nidentical to the kinematic equations, why not just use the kinematic equations as the \nintegration method? \n \nIf we do this, then we are guaranteed to get trajectories that are exact, within the \nassumption that acceleration is constant over the course of the time step. However, the \naccelerations we are usually interested in modeling are not constant over the course of the \ntime step. We must use a value for the acceleration that encapsulates the fact that the \nacceleration is changing. \nWe could use the acceleration averaged over the time step, \n, as the constant \nacceleration value. By inserting the average acceleration into the kinematic equations, we \nachieve a method that we will refer to as the average acceleration method. In order to \ncalculate this average exactly, we must analytically integrate the acceleration over the time \nstep, which in many instances can be done easily. The average acceleration method \ntherefore represents a blend between numerical and analytic integration. We are \nnumerically integrating the current position and velocity from the previous position and \nvelocity, but we are analytically integrating accelerations that are acting during the current \ntime step. \nOf course, calculating the average acceleration exactly requires that we know how to \nintegrate the particular force in question. Luckily, most forces that are applied in game \nphysics are analytic models that are easily integrated. Calculating the average acceleration \nfrom an analytic model of a force is usually just as easy as calculating the acceleration at an \ninstant of time. \nIf the average acceleration is calculated analytically, then the velocity portion of the \nkinematic equations produces exact results. However, the position portion would require a \ndouble integral in order to achieve an exact result. If the forces that we are dealing with \nfollow simple analytic models, then calculating a double integral is usually just as easy as \ncalculating a single integral. \nWe will generalize the idea of the average acceleration method in order to introduce the \nkinematic integrator. The kinematic integrator is a set of stepping equations that allow for \nexact analytic calculation of both the velocity integral and the position integral. \nvn+1 = vn + dv \n\n\n \n \nxn+1 = xn + vn Δ t + dx \nThe exact method uses the following definitions for dv and dx: \n \nIf we are using the average acceleration method, then we define dv and dx as: \n \nIn the case of constant acceleration, it is very easy to perform both the single and the \ndouble integral. The integral contributions of a constant acceleration are given as: \n \nIf there are multiple forces acting on a body, we can express the integral contributions as a \nsum of contributions that are due to each force. \ndv = dv1 + dv2 + … \ndx = dx1 + dx2 + … \nThus for the kinematic integrator we accumulate dv‘s and dx‘s rather than accelerations. All \nforces acting on a body provide contributions to dv and dx. The amount that is contributed \nis dependent on the nature of the force and can usually be calculated exactly. If all forces \nacting on a body are integrable, then every contribution is exact. \nThe kinematic integrator can be used to perform the Symplectic Euler method with the \nfollowing integral contributions: \ndv = an Δ t \ndx = an Δ t2 \nThis method is useful if the acceleration is not integrable (or if we are too lazy to calculate \nthe integrals). These contributions are not going to be exact, but they will at least conserve \nenergy, which will maintain stability. \n\n\n \n \nThe kinematic integrator does not represent a specific integration method, but rather the \nability to isolate the portions of the stepping equations that actually require integration. \nSince the method used to evaluate the integral contributions is not explicitly specified, we \nhave a degree of freedom in choosing which method might be best for a particular force. For \ninstance, if there is a contribution from a constant gravitational force, then we can easily \nuse the exact method. We will see that for some forces we will want to use the average \nacceleration method. Or we could use the Symplectic Euler method if the force in question is \ntoo complicated to integrate or if we are performing a first pass on the implementation of a \nparticular force. \n \nIntegral Contributions Due to a Spring Force \nProbably one of the most common forces, next to a constant uniform force, is a spring \nforce. The spring force is proportional to the displacement of the spring from equilibrium. \nThis results in the following equation of motion: \n \nThe solution of this equation of motion is analytically solvable and is given by: \nx – l = A cos (ωt) + B sin(ωt) \nv = –A sin(ωt) + ωB cos(ωt) \nUsing this exact trajectory, we can determine the integral contributions: \n \nwhere c and s represent the cosine and sine, respectively, of ω(t). We could represent this \ncalculation as a matrix operation. \n \nThe components of this matrix depend on the size of the time step Δt, the mass of the body \nm, the strength of the spring k, and the equilibrium position of the spring l. The components \nof the matrix can be cached and reused until any of these parameters change. In many \ninstances, these parameters do not change for the lifetime of the spring. Thus, the \ncalculation of the integral contributions of a spring is relatively trivial—in other words, six \nmultiplies and four adds. \nWe can consider that the spring that we have been discussing is anchored to an infinitely \nrigid object at the origin, since we have only taken into account the action of the spring on a \nsingle body, and the spring coordinates are the same as the body coordinates. It is only \n\n\n \n \nslightly more complicated to calculate the integral contributions due to a spring that \nconnects two movable bodies. \n \nMultiple Forces \nBefore discussing the integral contributions of other possible forces, we need to discuss \nwhat happens when multiple forces act on a body. \nIf all of the forces acting on a body depend only on time, then the result of accumulating \nexact integral contributions will be exact. But consider the case where at least one of the \nforces depends on the position of the body, such as the spring force. \nThe integral contribution of the spring takes into account the position of the body at \nintermediate values as the spring acts over the course of the time interval. However, the \ncalculation is not aware of intermediate changes in position that are due to other forces. The \nresult is a very slight numerical error in the resulting trajectory of the particle. \nAs an example, consider two springs that are acting on the same body. For simplicity, both \nsprings are attached to an infinitely rigid body at the origin, and the rest length of both \nsprings is zero. The springs have spring constants k1 and k2. The acceleration becomes: \na = –(ω1)2 x – (ω2)2x \nIf we are to handle these forces separately, then we would exactly calculate the integral \ncontributions of two springs, with frequencies ω1 and ω2, and accumulate the results. \nThe springs can also be combined into a single force. \n \nIn this case, only a single integral contribution would be calculated. It might be surprising to \ndiscover that these two methods produce different results. The second method is exact, \nwhile the first contains a slight amount of numerical error. This seems to imply that: \n∫(a1 + a2)dt ≠ ∫ a1dt + ∫ a2dt \nwhich is usually not true. However, in the current circumstance, if the acceleration is a \nfunction of the position, which in turn is a function of the acceleration, then the integrals \nhave feedback. The effect of this feedback is that we cannot separate the sum into \nindependent integrals, since the independent integrals will not receive feedback from each \nother. \nIf the acceleration only depends on time, there is no feedback, and the integrals can safely \nbe separated. Because of this, you may want to separate out forces that depend only on \ntime and accumulate their integral contributions as a group. The integral contributions of \nthese forces can be safely added together without introducing numerical error. \nUnfortunately, most of the forces that are applicable to game physics depend on the \nposition of the body. \nThough the error due to multiple forces is incredibly small, it represents a tiny violation of \nenergy conservation. If this violation persists for long enough, then the trajectory can \n\n\n \n \neventually diverge. The error due to multiple forces is initially much less than the error in \nthe Symplectic Euler method, but it can eventually grow until it is out of control. \nSince we can integrate exactly the system with two springs, we can use this example to \ngauge the error present in the different methods for calculating the integral contributions. \nResults of this error calculation are contained in the figures. The first figure contains the \nresults of integrating one spring; the second figure represents errors due to integrating two \nsprings. The integration takes place over one half of the period of the two-spring system. \nThe Symplectic Euler method as well as the pulse method (which will be introduced later) \nboth conserve energy on average. Neither the average acceleration method nor the exact \nmethod conserve energy over long intervals of time. It takes quite a while, but the exact \nmethod will eventually diverge. The average acceleration method will eventually dampen \nout. For this reason, the average acceleration method is preferred if the integration is going \nto take place over a long interval. \nFigure 2.5.1. The relative errors in the case of one spring and two springs. \n \n \nIntegral Contributions of a Pulse \nOne solution to the problem due to multiple forces is to approximate the action of the force \nas a pulse. A pulse is a force that acts very briefly over the course of the time step. A \nperfect pulse acts instantaneously. Pulses acting at the beginning of the time step do not \ndepend on intermediate values of position or velocity; therefore, pulses are immune to the \nmultiple-force problems. \nConsider a rectangular pulse. The area under the curve of a rectangular pulse is dv = adt, \nwhere dt is the width of the pulse, and a is the magnitude of the pulse. It is possible to \nshrink the width of the pulse while maintaining the same area. To do this, we must, of \ncourse, increase the magnitude of the pulse. If we allow the pulse width to go to zero, the \nheight diverges to infinity. However, the product of the width and the height is still equal to \ndv. In the case where dv = 1, there is a special name for this infinite pulse. It is called a \ndelta function, and is denoted as δ(t). The delta function is zero at all values of t, except at \nt = 0. At t = 0, the delta function is equal to infinity. Integrating the delta function is very \neasy to do. \n \n\n\n \n \nfor all values of a and b where a < 0 < b. If the interval between a and b does not contain \nzero, then the result of the integral is zero. \nTo represent the force as a pulse, we define the acceleration as: \na = dvδ(t) \nHere, dv is the area under the curve of this instantaneous pulse, as previously described. \nIntegrating this acceleration is easy to do: \n∫ adt = dv \nIntegrating the position contribution is now just integrating a constant. \n \nWe can choose dv as the amount of acceleration we want to pack into the pulse. If we want \nto pack all of the acceleration due to an arbitrary force into the pulse, then we use the \naverage value of the acceleration \n. This produces stepping equations of the form: In a \nmore familiar form, the stepping equations for the pulse would appear as: \n \nIn a more familiar form, the stepping equations for the pulse would appear as: \n \nIf you make the assumption that the acceleration is constant over the time interval, then \nyou can replace \nwith an. Doing this, you will arrive back at the stepping equations for \nthe standard Symplectic Euler method. This gives meaningful insight into the Symplectic \nEuler method. It is a method that assumes that the acceleration is constant over the course \nof the time step and delivers all of the acceleration in an instantaneous pulse at the \nbeginning of the time step. Since pulses can be accumulated without introducing error, the \nSymplectic Euler method can be considered to be an exact solution to forces of an \napproximated form. Since the method is exact, it intrinsically conserves energy. The error in \nthe method is entirely due to the differences between the actual form of the force and the \napproximate form of the pulse. \n \nIntegral Contributions of Collision Forces \nOne very common thing that needs to be done in a game physics engine is to resolve \ncollisions. To calculate our collision, we will assume that the forces due to the collision act \n",
      "page_number": 148
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 161-171)",
      "start_page": 161,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": " \n \nwithin some subinterval that fits entirely within our given time interval. The nature of the \nforce does not matter as much as the fact that it operates on a very small timescale and \nachieves the desired result. \nFor simplicity, we will assume that the object has normal incidence with a plane boundary. \nWe will also assume that the collision with the boundary is going to take place within the \ntime interval. \nTo handle the collision correctly, we should sweep the trajectory of the body between xn and \nxn+1 and find the exact position and the exact time where the collision begins to take place. \nIf we want to find the projected position at the next time step xn+1, we should wait to apply \nthe collision force until all other forces have been applied, so that the projected next \nposition is as accurate as possible. \nFor the purpose of simplification, we will consider that the trajectory is a straight line that \nconnects the position xn with xn+1. The position of the body at some intermediate value of \nthe time step is given by: \nXf = Xn(1–f)+Xn+1(f) \nwhere f is the fraction of the time step that represents the moment of impact. The \nintermediate velocity is simplified as well, to be: \nVf = Vn(1–f)+Vn+1(f) \nWith this physically inaccurate yet simplified trajectory, we can perform a sweep of the body \nagainst the collision boundary. The result of this sweep will determine the time fraction f. \nThis will tell us the position and velocity (almost) of the body at the moment of impact. To \ndetermine the force that is applied in order to resolve the collision, we need to apply a pulse \nforce that reflects the component of velocity vf about the contact plane. This reflection \nincorporates the interaction of the body with the contact surface; thus, the result should \nincorporate surface friction. For now, we will assume that there is no friction. \nTo apply the pulse force, we will begin by applying a constant force over a small subinterval \nof the time step. The subinterval begins at time fΔt and ends at (f + df)Δt. The velocity of \nthe body at the end of the subinterval is given by: \nvf + df = vf + avdf \nAccording to our initial simplification, the velocity is aligned with the normal of the contact \nplane. Thus, all of the velocity is reflected. We choose av to enact this reflection. \n \nThe integral contributions of this force in the limit that df → 0 are given by: \ndvv – 2vf \ndxv = 0 \nSince this force pulse does not contribute anything to the position integral, the application \nof this force does not change the value of xn+1 from what it would have been. Thus, the \nvalue of xn+1 will still violate the collision constraint. \n\n\n \n \nWe need to apply a second force pulse, which moves the position of the body out of the \ncollision surface. Assuming that the body has normal incidence to the plane, then the \namount the body needs to be pushed is given by Δx = xf – xn+1. Again, we are going to \napply a constant force on a subinterval. The acceleration of this force ax is determined by \nconsidering the kinematic equations for constant acceleration. \n \nThe integral contributions for this acceleration as df → 0 are: \n \nAdding these two sets of contributions together gives the final result for the collision force. \nThis set of contributions can be expressed in terms of the position at the beginning and end \nof the interval, as well as the parameter f, which represents the moment when the collision \nfirst begins to happen. \n \n \nIntegral Contributions of Viscous Forces \nA viscous force is a force that is related to, and usually opposing, the velocity of an object. \nF = –KV \nThe equation can be solved for the velocity in terms of the constant k and the mass m. This \nis done by expressing the acceleration as the derivative of velocity. \n \nThis equation can be integrated to get: \n \n\n\n \n \nThe integral contributions of this force can be determined exactly and are given by: \n \nIt may very well be sufficient to approximate the exponent function with the first-order \nTaylor series expansion. Since the force dissipates energy, exactness for the sake of \naccuracy is not required. \n \nViscosity can be applied in an anisotropic fashion, meaning that the viscosity constant k can \nactually be a vector \n. The viscosity force then contains the dot product of \nwith vn. \nA simple method of introducing surface contact friction is to generate an anisotropic viscous \nforce in the event that a collision is determined. The viscosity in the direction of the collision \nsurface normal can be tuned separately from the components in the surface plane in order \nto decouple the restitution of reflected velocity with sliding friction. Of course, this general \nviscosity force does not accurately model dry friction, where there is a transition from static \nto kinetic friction. But it is a good place to start. \n \nIntegral Contributions of Constraint Forces \nMany physics engines offer a variety of constraints on objects. We will not calculate the \nintegral contributions of every possible constraint, but rather suggest a general mechanism \nof determining resolution forces to enforce that constraints are satisfied. In many physics \nengines, collisions are handled as constraints. The pattern for evaluating the resolution \nforces for a collision can apply to all constraints. This general pattern is as follows. \n1. Approximate the trajectory and find the value of f, which represents the moment \nwhen the constraint violation began. \n2. Forces are applied as pulses. Using the Jacobian of the constraint equations to \ndetermine the direction of the force, the magnitude is calculated in order to bring the \nvelocity vn+1 into compliance with the constraint. \n3. If needed, another force pulse is applied in the same direction in order to bring the \nposition vn+1 into compliance. \n4. The contributions for both pulses are added together. \n \n\n\n \n \nSummary \nWhen using the kinematic integrator, the state of a body is defined by the position xn and \nvelocity vn at the beginning of the current time step tn. \nThe state of a body is stepped forward in time using the kinematic integration equations: \nvn+1 = vn + dv \nxn+1 = xn + vn Δt + dx \nThe quantities dv and dx represent the portions of the stepping equations that can be \nanalytically integrated per force and accumulated. \nThe integral contributions can be calculated using the exact method as: \n \nwhere the inner integral of dx is indefinite, and the outer integral is a definite integral. \nThe integral contributions for the average acceleration method are: \n \nThe integral contributions for a pulse are: \n \nIf desired the integral contributions of the Symplectic Euler method are defined as: \ndv = an Δt \ndx = an Δt2 \nThese different methods can be mixed, depending on the desired results. When there is \nmore than one force applied that depends on position, then the average acceleration \nmethod can be much more accurate than the exact method. \nThe acceleration contributions for the spring with a spring constant k and a natural length l \nare: \n\n\n \n \n \nThe contribution of a viscous force, with a viscosity constant v, is: \n \nFor a collision response force, the integral contributions are defined as: \n \nwhere f is the fraction of the time step when the collision begins. \nFor a general constraint, the generic framework is as follows: \n1. Approximate the trajectory and find the value of f, which represents the moment \nwhen the constraint violation began. \n2. Forces are applied as pulses. Using the Jacobian of the constraint equations to \ndetermine the direction of the force, the magnitude is calculated in order to bring the \nvelocity vn+1 into compliance with the constraint. \n3. If needed, another force pulse is applied in the same direction in order to bring the \nposition xn+1 into compliance. \n4. The contributions for both pulses are added together. \n \nConclusion \nUsing a well-chosen mix of numerical integration and analytic integration, it is possible to \nachieve exact trajectories for some force models. If there are multiple forces applied, error \nmay accumulate, since the analytic integration of individual forces cannot take other forces \ninto account. Using the average acceleration method for calculating the integral \n\n\n \n \ncontributions can result in relative errors that are millions of times smaller than the errors \nfrom the Symplectic Euler method alone. \nWe have seen that the Symplectic Euler method can be thought of as an exact method, \nwhich approximates the force as pulses. The errors in this method are due to this \nmisrepresentation of the force. \nThis gem has demonstrated the fact that the kinematics of simple physical models, which \nare prevalent in game-related physics, can be leveraged to dramatically reduce and \nsometimes eliminate the error of integration methods. \nGoing forward, work on this topic might include determining the integral contributions due \nto the different flavors of translational and rotational constraints. Also, determining whether \nthere is a way to pre-accumulate elements of the acceleration integrands prior to \nintegration would provide a very natural solution to the problems that arise because of \nmultiple forces that depend on position. \n \n2.6. What a Drag: Modeling Realistic Three-Dimensional Air and Fluid \nResistance \nB. Charles Rasco, Ph.D., President, Smarter Than You Software \ncharlie@smarterthanyou.com \nBasic physics simulations have been around in games for quite a while. But beyond simple \ngravity, simple 2D collisions, and not-so-simple 3D collisions, there is a lack of further \nrefinement of object motion. Drag physics is one area that has not been adequately \naddressed. Most games simulate drag physics with a simple linear model, if they simulate it \nat all. This gem demonstrates and contrasts two different mathematical models that \nsimulate drag: a linear drag model and a quadratic drag model. The quadratic drag model is \nmore realistic, but both models have applicability to different situations in physical \nsimulations for many different types of games. The gem also defines and explains relevant \nparameters of drag physics, such as the terminal velocity and the characteristic decay time. \nThe linear three-dimensional drag problem and both the one-dimensional linear and parts of \nthe quadratic drag problems are discussed in [Davis86]. This gem adds a stable \nimplementation of quadratic drag in three dimensions. The integrals used in this article may \nbe found in any calculus book or on the web [Salas82, MathWorld]. \nGames could use improved drag physics for many things: artillery shells, bullets from \nsmaller guns, golf ball trajectories, car racing games (air resistance is the main force that \nlimits how fast a car can go), boats in water, and space games that involve landing on alien \nplanets. Even casual games could use improved drag physics. The quadratic drag model in \nthis gem was developed to make the iPhone and iPod Touch app Golf Globe by ProGyr. In \nthis game, the user tries to get a golf ball onto a tee inside a snow globe. Originally the \ngame was designed with linear water resistance, but it did not feel quite right. This was the \nmotivation to improve the water resistance to the more realistic, and more complicated, \nquadratic model. I have a real golf ball in a snow globe and have never been able to get it \nonto the tee. To be honest, I have never gotten the ball on the tee in the Golf Globe game \non the 3D eagle level, but in the game the easy levels are much simpler because the easier \nlevels are two-dimensional. \nPhysics \n\n\n \n \nThis section provides the general mathematics and physics background for both the linear \nand the quadratic drag models. \nThree-Dimensional Physics \nThe forces for the linear model are: \n \nAnd the forces for the quadratic model are given by: \n \nwhere \nis the velocity squared, and \nis \nthe unit vector that points in the direction of the velocity. \nThe first term of both equations is the force due to gravity, and in my representation down \nis the negative z direction. The direction of gravity depends on your coordinate system. The \nsecond term of both equations is the drag term. I chose the alpha and beta as different \nlabels for the drag coefficient in order to keep it clear which version of air resistance I am \nreferencing. For both models, if this value is big, there is a lot of fluid resistance, and if it is \nsmall, there is little fluid resistance. This parameter should always have positive value and \ndoes not actually need to remain constant. It can change if an object changes shape or \nangle from the direction of the fluid motion. A sail in the wind does not have a constant \nvalue, but changes depending on how taut the sail is and how perpendicular it is relative to \nthe wind, among other factors. The velocity in these equations is the velocity of the object \nin the fluid, so if there is wind, then it would be the velocity of the object minus the velocity \nof the wind. \nThe linear equation is solvable in a complete analytical closed-form solution. Since the \nquadratic mixes all of the components of the velocity vector together, it is not solvable in a \nclosed form, but we can solve it numerically. In addition, the quadratic drag model has the \nadvantage of being a better approximation of realistic air and fluid resistance. Interestingly, \nboth are exactly solvable when considered as one-dimensional problems. This will be \ndiscussed in more detail later in this gem. \nConversion from Three-Dimensional Physics to One-Dimensional Physics \nTo convert the drag problem from three dimensions into one dimension, we need to break \nthe object‘s motion into two components: motion along the velocity (one dimension) and \nmotion perpendicular to the current velocity (two dimensions). The drag force only acts \nalong the current direction of velocity; thus, we can treat it as a one-dimensional problem. \nThe unit vector in the direction of the velocity is given by \n, or if \nthe velocity is zero it is valid to set \n. Setting the normal to zero works because if the \nobject is not moving, there will be no fluid resistance. The components of the forces along \nthe direction of motion and perpendicular to the direction of motion are shown in Figure \n2.6.1. We can update the velocity along the current direction of motion as well as \nperpendicular to it once it is decomposed this way. \nFigure 2.6.1. Breaking three-dimensional space into components along the \ndirection of motion and perpendicular to the current direction of motion. \n\n\n \n \n \n \n \nOne-Dimensional Solutions \nFor a one-dimensional solution, we start from the one-dimensional version of Newton‘s \nSecond Law, F = MA, and from the definition of acceleration, a = dv/dt. If the force is a \nfunction of the velocity alone, then these two equations can be combined to get time as a \nfunction of velocity, as shown here: \n \nThe force equations that are relevant to the present matter are F = mg cos(θ)– αv, the \nlinear fluid resistance equation, and F = mg cos(θ)±βv2, the quadratic fluid resistance \nequation. The cosine in these equations is the cosine of the angle between the current \nvelocity and the vertical. For the linear equation, the negative sign in front of the velocity \nterm means the force always opposes the direction of motion. If the velocity is positive, \nthen the force is in the negative direction. If the velocity is negative, then the force is in the \npositive direction, which is taken care of automatically by the linear equation. In the \nquadratic equation, the different sign must be applied based on which way the object is \ntraveling, since squaring the velocity loses the directional information. \nFor the linear equation, we have: \n \nwhere vt1 = mg cos(θ)/α is the terminal velocity. There is one other variable that describes \nthe motion, and that is the characteristic time, η1 = m/α. The characteristic time is a \nmeasure of how quickly (or slowly) the object speeds up or slows down to the terminal \nvelocity, and it shows up in the equations after we integrate. The formula for the quadratic \ndrag model‘s terminal velocity and characteristic time is different than the linear drag \nmodel‘s characteristic time and terminal velocity. The linear drag models are labeled with \nsubscript 1s. Another way to calculate the terminal velocity is to set the force equation to \n\n\n \n \nzero and solve for the velocity. This integral is evaluated with a change of variables x = 1 – \nv/vt1, so that dx = – dv/vt1. \n \nLastly, we solve this somewhat intimidating equation for vf, which turns out nicely as: \nEquation 1  \n \n \nIf you feel like practicing taking limits, see what happens when the air resistance coefficient \ngoes to zero. The traditional vf = v0 + g cos(θ)t pops up. The first few terms in this \nexpansion are useful if you want to approximate the exact solution in order to avoid the \nexpensive exponential function calls. \nFor the quadratic case, we solve the equations in the same way. We start with the general \nequation: \n \nFrom this equation we define the terminal velocity, \n, and the \ncharacteristic time is (not obvious from the equation) \n, which \nshows up after we integrate. Integrating this equation is straightforward. Although related, \nthe plus version and the minus version are different integrals, and we need to evaluate \nboth. \nFirst, let us do the integral when the terminal velocity term (labeled with subscript 2s for \nthe quadratic model) and the velocity square terms have the same sign. This is the case \nwhere the force of gravity and the fluid resistance are in the same direction, which happens \nwhen the particle is traveling up and both forces are pointing down. \nTo keep track of the overall sign relative to the velocity direction, we introduce the variable, \nsign, which is either 1 or –1, on the outside of the integral. This is important to get the \ncorrect overall sign compared to the direction of the velocity unit vector, \n. We continue to \nsolve for the time as a function of initial and final velocity to obtain: \n \nNow we solve this equation for the final velocity and obtain: \nEquation 2  \n\n\n \n \n \n \nSecondly, we solve for the case where the terminal velocity and velocity squared terms \nhave opposite signs: \nThere is a relevant connection between the inverse hyperbolic tangent and the natural log \nof the form: \n \nThe absolute value bars from the integral are important in order to get the equation to \ninvolve the inverse hyperbolic tangent correctly. We must answer whether the initial and \nfinal velocities are bigger than the terminal velocity. In addition, given that the initial \nvelocity is less than the terminal velocity, is the final velocity necessarily less than the \nterminal velocity? It turns out that it is (see the solution below), and it is acceptable to \nconvert both natural logarithms to inverse hyperbolic tangents. A similar question can be \nasked if the initial velocity is greater then the terminal velocity. \nSo there are four different cases with which we need to deal: V0 <Vt2, V0 > Vt2, V0 = Vt2, and \nVt2 = 0. \nFor the first case V0 < Vt2: \nand solving for the final velocity: \nEquation 3a  \n \n \nFor the second case V0 > Vt2: \n\n\n \n \nand solving for the final velocity: \nEquation 3b  \n \n \nFor the third case, v0 = vt2, where the object begins traveling at the terminal velocity in the \nsame direction as the force of gravity, we get: \nEquation 3c  \n \n \nThere are several ways to arrive at this result. We can take a limit of both of the first two \ncases, Equations (3a) and (3b), or if we look at the force in the quadratic force equation, it \nis zero, hence the object must travel at a constant velocity. \nAnd lastly is the case when the terminal velocity is identically zero, Vt2 = 0. It is easiest to \nstart from the original equation and solve like the previous cases. \n \nAs previously, we solve this for the final velocity: \nEquation 3d  \n \n \nAll right, this is all the information we need to solve for the linear and quadratic three-\ndimensional cases, and we now transition to solving the three-dimensional linear case. \n \nSolution of the Three-Dimensional Linear Case \nWe could solve the linear three-dimensional case in a similar manner as we solve the \nquadratic version. However, we choose to treat it as three separate one-dimensional \n",
      "page_number": 161
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 172-180)",
      "start_page": 172,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": " \n \nproblems because the results can be used by the AI in your game. The previous equations \nare modified to suit each of the Cartesian directions x, y, and z. Again, gravity is in the \nnegative z direction. The solutions of the three different velocity equations are: \nvxf = vx0 exp(–t/η1) \nvyf = vy0 exp(–t/η1 \nand \nvzf = (vz0 – vtl)exp(– t/η1) + vtl \nwith vtl = –mg/α and η1 = m/α. The terminal velocities in the horizontal direction are zero. \nIf we integrate these equations with respect to time, then we get the position as a function \nof time. \nx = vx0 η1 (1 – exp(–t/η1) + x0 \ny = vy0 η1 (1 – exp(–t/η1)) + y0 \nand \nz = (vz0 – vtl)η1(1 – exp(–t/η1)) + vtlt + z0 \nThe complete distance equations are useful for AI targeting, AI pathfinding, and calculating \ntargeting info for players, without actually propagating the solutions forward in time step by \nstep. In general, the distance equations are well approximated for most video games with x \n= x0 + vt, since the time for most games is very small and is much faster than updating \nwith the complete position equations. \n \nSolution of the Three-Dimensional Quadratic Case \nThe three-dimensional quadratic equations are \n, \n. It is impossible to solve these as \nthree independent equations since they are all coupled by the quadratic term of the fluid \nresistance. One way to solve this is to break the motion into two directions, along the \ncurrent velocity and perpendicular to the current velocity, and then update the velocity \nbased on the force calculated. \nAlong the direction of motion, the force is Fparallel = –mgnz – βv2, where nz is the z \ncomponent of \nand is equal to negative of the cosine of the angle between the direction of \nmotion \nand the gravity vector. Notice that the drag term is always negative since it is in \nthe opposite direction as the current velocity. \nPerpendicular to the direction of motion, the forces are \n. If the velocity is either straight \nup or straight down, nz = ±1 and nx = ny, there is no component of force perpendicular to \nthe current velocity. And if the velocity is completely horizontal, the perpendicular force is \ncompletely in the negative z direction. If you have a different coordinate system, this \nequation will look slightly different. \n\n\n \n \nFor the motion along the velocity normal, there are three different cases to consider: nz> 0, \nnz< 0, and nz= 0. If nz > 0, the object is moving upwards, the opposite direction from \ngravity, and the fluid resistance is in the same direction as gravity. If nz< 0, then the object \nis moving in the same direction as gravity, and the fluid resistance force is in the opposite \ndirection as gravity. Lastly, if nz= 0, then the object is moving horizontally, and gravity has \nno effect on horizontal motion. \nFor nz> 0, the relevant solution is Equation (2) with \n, \n, and sign = – 1 The velocity is: \n \nFor nz < 0, the relevant solution is Equation (3a), (3b), (3c), or (3d) depending on the \ncurrent velocity relevant to the current terminal velocity with \n, \n, and sign = – 1. The velocity update is: \n \nor \n \nNotice that the characteristic time for the last instance is undefined, but it is never used in \nthe solution, so there should not be a problem. \nFor the force perpendicular to the initial velocity, the updated velocity is the same as \nEquation (3) with v0= 0 and its own distinct terminal velocity, \n, and characteristic time, \n. \nThis leads to: \n\n\n \n \n \nFor most cases this can be approximated (since the characteristic time is usually large and \nthe update time is usually small for games) as: \nEquation 4  \n \n \nThis last equation should look familiar. So usually there is no need to calculate the separate \nterminal velocity or the characteristic time for the perpendicular motion if the mentioned \napproximations are valid. \nThere is one issue that occurs when a slow-moving object changes from traveling upward to \ngoing downward, as shown in Figure 2.6.2. If this case is not handled, then the object \nappears to go back in the direction it came from, when in fact it would stop traveling in the \nhorizontal direction and travel down along the direction of gravity. \nFigure 2.6.2. A bad thing that can happen for slow-moving particles and/or large \nupdate times. \n \n \nThere are several ways to handle this. The most straightforward approach is to propagate \nthe object for the time it takes to get to the top with Equation (2) and then for the rest of \nthe time propagate with Equation (3a). From Equation (2), we can calculate the time it \ntakes to stop moving in the positive velocity direction. The result is: \nEquation 5  \n\n\n \n \n \n \nIf the time to the top is less than the total time to propagate, then subtract the time to the \ntop from the total time to propagate. Then use the downward velocity update for the time \nremaining and use zero as the initial velocity. There might be a slight discontinuity in the \nobject‘s smooth motion on the screen, but usually there is not. If there is too much, then \nupdate the position in a more rigorous manner. \nThe last thing to consider is how to update the position. All of these equations are \nintegrable, which is more accurate, but they provide little extra accuracy noticeable in a \ngaming environment and are computationally expensive to evaluate. For most game \npurposes, it is accurate enough to use Euler integration to calculate the position Xf = X0 + \nVt, where the time, t, in the equation is the time since the last update and is not the global \ntime. For the Golf Globe game on an iPod Touch, the game runs smoothly with no visible \nnumerical issues at 60 frames per second by updating the position using Euler integration. \n \nPseudocode \nThis is the pseudocode for a function that calculates the final velocity, velocityFinal, \ngiven the initial velocity, velocityInitial, of an object by the time, deltaTime. This \ndoes not update the position of the object, which should be updated separately but with the \nsame deltaTime. \nvoid updateVelocityWithFluidResistance( \n      const float deltaTime, \n      const ThreeVector& velocityInitial, \n      ThreeVector& velocityFinal ) \n{ \n      Calculate Velocity Normal or zero it. \n      Calculate terminal velocity and characteristic time along \nvelocity \nnormal. \n \n      For objects going up use Equation (2), but first check \nto make sure the velocity does not reach the top \nEquation (5). \n \n      For objects going down use either Equation (3a), (3b), \n(3c), or (3d) as is appropriate to the initial velocity at the \nbeginning of the frame. \n \n      Update the velocity perpendicular to the initial velocity \nwith \nEquation (4). \n} \n \n \n\n\n \n \n \nComparison of Linear versus Quadratic Solutions \nThere are benefits to using either the linear version or the quadratic version. The \ncomputational advantages lie with the linear solution. The quadratic computational cost is \nhardly prohibitive, especially if there are not too many objects updated with the quadratic \nmodel. And the quadratic model looks and feels much nicer, especially in a game where the \nuser is getting feedback from movement due to handheld motion, such as from the iPod \nTouch or another handheld device. \nThere are several advantages to the linear solution: There are exact velocity and position \nequations for all time, which is useful for AI and targeting; it is faster computationally; and \nthe exponent in the solution is easily expanded, making it only minutely more \ncomputationally intense than updates with gravity only. \nOne disadvantage to the linear solution is that it is not as realistic, especially when \ncompared directly to the quadratic case. \nThere are some advantages to using the quadratic solution: It is more realistic and it is not \nhugely more computationally intense (as long as there are not too many objects). There are \nseveral disadvantages to the quadratic solution: It is computationally more intense, \nespecially if there are many objects; there is no analytic solution for all time, thus it is not \nas easy to use for AI and player targeting; and its approximations are more difficult and are \nmore numerically touchy to the input drag parameters. \nThere is a possible way to balance the two approaches if there are too many objects to \nupdate with the quadratic drag model. Use the linear model for the vast majority of these \nobjects and then use the quadratic for the more important, or highly visible, objects. \nFor both the linear and the quadratic solutions, it is a good approximation to use Euler \nintegration to update the object position. It will save big on computational cost if there are \nmany objects, especially for the quadratic case. \n \nConclusion \nThe concept of drag is simple: Slow things down in the opposite direction they are moving. \nHowever, it is difficult to implement in a numerically accurate and stable manner. The \nsimple and mathematically accurate linear drag model as presented here would be an \nimprovement to many current games. The mathematically and physically accurate quadratic \nmodel is an additional improvement. There are many areas of simulation in games that \nwould visibly benefit from using the mathematically and physically accurate quadratic drag \nmodel instead of the linear drag model: boats, sailboats (both with respect to the water and \nthe wind in the sails), parachutes, car racing, snow-globe games, golf ball trajectories, craft \nflying through atmospheres, and artillery shells. \nAlong the lines of this gem, there are several areas that would be interesting to research \nfurther. Physically based drag coefficients depend on all sorts of factors, including \ntemperature, density, type of fluid, and the shape of the object. For spacecraft entering an \natmosphere, several of these things are relevant and can make for some interesting \ngameplay while flying into land or chasing AIs or other players in a multiplayer game. \n \n\n\n \n \nReferences \n[Davis86] Davis, A. Douglas. Classical Mechanics. Academic Press, Inc., 1986. \n[MathWorld] ―Wolfram Math World.‖ n.d. <http://mathworld.wolfram.com/>. \n[Salas82] Salas, S. L. and Einar Hille. Calculus: One and Several Variables with Analytic \nGeometry: Part 1. John Wiley and Sons, 1982. \n \n2.7. Application of Quasi-Fluid Dynamics for Arbitrary Closed Meshes \nKrzysztof Mieloszyk, Gdansk University of Technology \nkrzysztof.mieloszyk@gmail.com \nPhysics in real-time simulations as well as in computer games has been gaining importance. \nThe game producers have observed that the advanced realism of the graphics is not enough \nto keep players content and that the virtual world created in a game should follow the rules \nof physics in order to behave more realistically. Until now, such realism has been mainly \nreflected by illustrating the interactions between the material objects. However, the \nphenomenon of reciprocity between such media as liquid or gas and more complex objects \nis also of high importance. With increasing processor power, the growing number of physics \nsimulation methods that were initially developed purely for scientific reasons are being \nutilized in the entertainment market. However, simulation methods commonly used in \ngames tend to be relatively simple in the computational sense or very focused on a specific \ncase. An attempt to adapt them to other game variants is often impossible or requires an \napplication of complicated parameters with values that are difficult to acquire. Hence, some \nmore universal methods that allow for more easily modeled game physics are valuable and \nattractive tools for hobbyists and creators of later game extensions. \nRequirements for the Mesh Representing the Object \nCommonly, the medium is a physical factor that in various ways affects the complete \nsurface of the object. Hence, for the purpose of fluid dynamics simulation, every shape \nneeds to be represented as a closed body with all its faces pointing outward and fully \ncovering its outside surface. The inside should be looked upon as full; it must not have any \nempty spaces or ―bubbles.‖ Every edge should directly contact exactly two surfaces, while \nthe entire object mesh must fulfill the following condition: \nEquation 1  \n \n \nwhere V= vertices, F= faces, and E= edges. \nBecause the preliminary interaction value is calculated on points, it is convenient to \norganize the mesh in a list of points in the form of coordinates and a list of triangles \nrepresented as three sequential indices of its vertices. Additionally, every triangle should \nhave a normal vector of its surface, as well as the surface value. In some cases, depending \non the computational method used, the position of the triangle center can also be useful. In \nthis article, we will sometimes refer to vertices as points and triangle meshes as faces. \n\n\n \n \n \nPhysical Rudiments \nUsing some physical basics of fluid mechanics (referring among others to Pascal‘s Law), \npressure is defined as a quotient of force F, which exercises perpendicularly to the surface \nsector limiting the object given, and the surface S of this sector [Bilkowski83]. \nEquation 2  \n \n \nThe total pressure affecting the body surface in a given medium point can be represented \nby a sum of two components: static and dynamic pressure. \nEquation 3  \n \n \nIn order to define the static pressure that is needed to create buoyancy in a selected \nmedium, we need the medium density ρ[kg/m3], gravitational field intensity g [m/s2], and \nthe distance to the medium surface h. (In the case of air, the altitude is needed.) \nEquation 4  \n \n \nDynamic pressure originates from the kinetic energy pressure of the medium, which results \nfrom its movement and can be determined by knowing the medium density ρ as well as the \nfluid velocity V [m/s] at a selected position (applicable for subsonic speed). \nEquation 5  \n \n \nEquation (5) can be extended with a nondimensional pressure coefficient cp, whose value \ncan be calculated as a cosine of the angle between the normal vector of the surface sector \nand the vector of the actual velocity in a selected point (see Figure 2.7.1). This can also be \nrepresented as a scalar product of the normal surface and the normalized velocity vector. \nFigure 2.7.1. Normal vector of the surface and normal vector of velocity and the \nangle between them. \n\n\n \n \n \n \nEquation 6  \n \n \nAs an effect, we obtain a general equation for dynamic pressure exerted on a segment of \nthe surface shown in Equation (7). The method presented in this gem solves the \ncomputation of dynamic pressure based on velocity and assumes that we use a rigid body. \nThe use of physical models based on joints or springs is also possible. \nEquation 7  \n \n \n \n \nPre-Computation Based on a Triangle Mesh \nThe whole issue of calculating the pressure distribution on a mesh comes down to \ncomputing the values on the mesh vertices (or center of triangles) taking into consideration \nthe immersion, velocity, surface area, and normal of each of the triangles that form the \nobject. This allows us to calculate the force of the pressure that influences the physical \nmodel. Additional problems arise in the case of the object being placed in two different \nmedia simultaneously. Here the division plane separates the object faces into those faces \nthat are totally immersed in only one medium and faces that are located in both of them. \nThe factors requiring serious consideration are characterized in the following section. \nCalculating the Distance to Medium Boundary \nFor calculations of the static pressure affecting the object, it is necessary to define the \ndistance h between the selected point and the medium boundary (for example, a water \nsurface). For this, the scalar product can be applied, giving us the following Equation (8), \nschematically illustrated in Figure 2.7.2. \nEquation 8  \n \n\n\n \n \n \nFigure 2.7.2. Defining the distance of the point from the surface. \n \n \nAssuming that the normal vector of the surface is pointing upward, the negative value of \nthe obtained distance means that the point is located under the ―water‖ surface, and the \npositive is above its surface. In cases when the boundary is located at the virtual world ―sea \nlevel,‖ we know in which of the two media the point is located. To reduce the possibility of a \ncalculation error, point Psurface (through which the surface dividing the media is crossing) \nshould correspond to the projection of the object‘s center onto this surface, in accordance \nwith its normal vector. \nCalculating the Point Velocity \nBecause the objects simulated in the virtual world are frequently moving, one of the most \nimportant factors needing to be improved in their dynamics is the influence of the \nsurrounding medium. Therefore, it is required to include the velocities of each of the object \nmesh points (Equation (9)) as a sum of the mass center‘s linear velocity Vobj and the \nobject‘s angle velocity (Figure 2.7.3). The velocity Vp obtained this way represents the \nglobal velocity for selected point P. To calculate the dynamic pressure, the local velocity \nvector, which is equal to the negative global point velocity, is required [Padfield96]. \nFigure 2.7.3. Velocity of the arbitrary point belonging to the moving object. \n \n",
      "page_number": 172
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 181-188)",
      "start_page": 181,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": " \n \n \nWe represent this mathematically as follows: \nEquation 9  \n \n \n \nMorphing Shape Objects \nThe simulated object is not always a rigid body, because some objects move by changing \ntheir body shape. A fish or a bird can be used as an example here because various body \nfragments of those animals move with different velocities in relation to other parts. For such \nobjects, when calculating the point velocity, it is necessary to take the so-called morphing \neffect into consideration. The simplest method here is to find the change between the old \nand new local positions of the mesh points (see Figure 2.7.4), followed by the velocity \ncomputation, keeping the time difference Dt between the morphing animation frames in \nmind. As a result and an extension to Equation (9), we obtain Equation (10), which is \npresented below. \nEquation 10 \n \n \nFigure 2.7.4. Moving vertex position in morphing mesh. \n \n \n \n\n\n \n \nTriangle Clipping \nBecause there is a situation in which the object triangle can be located in both media \nsimultaneously, it is necessary to determine the borderline and find those parts that are \ncompletely submerged in only one of the media. To achieve this, the triangle should be \ndivided into three triangles calculated by cutting points of the edges that cross the medium \nborder. By applying Equation (11), we obtain PAB, PAC, and dividing the triangle ABC, we \ncreate a triangle A PAB PAC, located in one medium, and two arbitrary triangles located in \nthe other medium (for example, B C PAB and C PAC PAB), as presented in Figure 2.7.5. \nFigure 2.7.5. Assigning the cutting points on the surface by edge AB. \n \n \nIt is important to keep in mind that the newly created triangles must have the same normal \nvectors as the primary triangle. Even though each of the new triangles has the same \nnormal, it is still necessary to recalculate the current surface area of each of those triangles. \nIt might be very useful to apply the cross-product property, described in Equation (12), for \nthe exemplary triangle ABC. This surface area can then be used to calculate the pressure \nforce. \nEquation 11 \n \n \nEquation 12 \n \n \n \n \nCalculation of the Pressure on the Triangle Surface \n\n\n \n \nTo calculate the pressure force exerted on the arbitrary triangle of the body, the pressure at \neach of the triangle vertices should be taken into consideration. Hence, we need to calculate \nthe distance of each vertex from the medium surface for static pressure (Equation (6)) and \nthe vertex local velocity, taking the angle between the velocity and the triangle surface \nnormal into account, for dynamic pressure (Equation (7)). In the next step, the average \ncomplete pressure of the triangle vertices is calculated. However, it is useful to know that \nsome simplifications are possible for cases when any mesh vertex has the velocity \ncalculated from ωx r that is considerably smaller than the object linear velocity Vobj or \nwhen ω is close to zero. As a result of this, the pressure gradient distribution is almost \nlinear, and the calculation of the complete pressure exerted on the triangle surface is \nreduced to obtaining the pressure in the triangle center (based on h and Vp). Thus, the \npressure force is a product of a triangle normal, its surface area, and pressure (Equation \n(2)). For models based on a rigid body, calculation of the complete force and the torque \ncomes down to summing the pressure forces from each surface fragment, as well summing \nall torques as pressure force vector cross-products and the forces at the center of the \ntriangles. \n \nSimple Correction of Dynamic Pressure \nThe method for computing the static pressure presented in this gem is based solely on \nsimple physical rules. However, the effect of dynamic pressure is one of the most \ncomplicated problems of the mechanics, for which no simple modeling method exists. The \ncomputational method used here is highly simplified and based on a heuristic parameter cp. \nThis is sufficient for the purpose of physical simulation, which visually resembles the correct \none; however, obtained values strongly diverge from the real values, resulting in too low of \nlift and too high of drag values. Hence, when calculating a force vector obtained from \ndynamic pressure, it is beneficial to introduce a correction in accordance with the velocity \nvector at a selected point. As in Equation (13), the factors used here are determined \nempirically and should be interpreted as recommended, not required. The corrected \ndynamic pressure force vector of the selected triangle has to be summed with the static \npressure force vector occurring on its surface. The results obtained in that way approximate \nthe results of the real object (for example, an airplane) for which the dynamic pressure is \nmost important. \nEquation 13 \n \n \n \n \nConclusion \nThe simplified method of computing the flow around an arbitrary closed body presented \nhere allows us to easily obtain approximate values of forces surrounding a medium. This is \npossible thanks to simulating some fundamental physical phenomena, such as static and \ndynamic pressure. Figure 2.7.6 shows screenshots from an application that interactively \nsimulates real time using the algorithm presented here. This clearly shows a potential \npossibility for use in simulating the behavior of objects with fairly complicated shapes. The \ncontrol here is carried out by changing the interaction between the medium and the object \nobtained, by morphing the shape of the object‘s surface, or by moving or rotating the object \nelements. This allows us to create controlled planes, ships, airships, submarines, hydrofoils, \n\n\n \n \nseaplanes, swimming surfers, or drifting objects, as well as objects that undergo morphing, \nsuch as birds or fish. All of this is simulated based on the object mesh only. \nFigure 2.7.6. Examples of applications: drifting object (top), static object (middle), \nmorphing object (bottom). (Grayscale marks the pressure distribution in the \nmiddle and bottom images.) \n \n \nThe simplifications used in the algorithm bring some inaccuracies. The main problem here is \nthe lack of consideration for the reciprocal interferential influences for each face on the final \npressure values. This is a consequence of analyzing each face separately, as if it were \nmoving in the flow consistent with its own velocity. In reality, the velocity of the flow at a \ngiven fragment of the body surface depends on the velocity over the adjacent surface \nfragments [Wendt96, Ferziger96]. Dependence of the value on the pressure coefficient \nparameter cp (as angle cosine) is a generalization. In fact, this issue would require some \nmore specialized characteristics. Another drawback of the material presented here is that \nthe medium viscosity has not been taken into consideration, which also adds to inaccuracies \nin the results. These computation versus performance tradeoffs need to be evaluated when \ncomparing real-time methods and those that may be used in fluid dynamics engineering and \ndesign. \nImplementations of the equations in this article motivated by specific examples are \navailable on the CD-ROM. \n \nReferences \n[Bilkowski83] Bilkowski, J. and J. Trylski. Physics. Warsaw: PWN, 1983. \n\n\n \n \n[Ferziger96] Ferziger, Joel H. and Milovan Peri. Computational Methods for Fluid Dynamics. \nBerlin: Springer-Verlag, 1996. \n[Padfield96] Padfield, Gareth D. Helicopter Flight Dynamic. Oxford: Blackwell Science \nLimited, 1996. \n[Wendt96] Wendt, John F. Computational Fluid Dynamics. Berlin: Springer-Verlag 1996. \n \n2.8. Approximate Convex Decomposition for Real-Time Collision \nDetection \nKhaled Mamou \nKhaled_mamou@yahoo.fr \nCollision detection is essential for realistic physical interactions in video games and \nphysically based modeling. To ensure real-time interactivity with the player, video game \ndevelopers usually approximate the 3D models, such as game characters and static objects, \nwith a set of simple convex shapes, such as ellipsoids, capsules, or convex hulls. While \nadequate for some simple interactions, these basic shapes provide poor approximations for \nconcave surfaces and generate false collision detections (see Figure 2.8.1). \nFigure 2.8.1. Convex hull versus approximate convex decomposition. \n \n \nIn this gem, we present a simple and efficient approach to decomposing a 3D mesh into a \nset of nearly convex surfaces. This decomposition is used to compute a faithful \napproximation of the original 3D mesh, particularly adapted to collision detection. First, we \nintroduce the approximate convex decomposition problem. Next, our proposed \nsegmentation technique and performance characteristics will be evaluated. Finally, we \nconclude with some areas of future work. \nApproximate Convex Decomposition \nLet S be a triangular mesh of connectivity k and geometry γ. Intuitively, S is a piecewise \nlinear surface composed of a set of triangles stitched along their edges. The connectivity k is \nrepresented as a simplical complex describing the topology of the mesh. k is composed of a \nset of vertices X = {v1,v2,...,vv} ∊ IN (IN denotes the set of positive integers and V the \nnumber of vertices of k), together with a set of non-empty subsets of X, called simplices \nand verifying the following conditions: \n \nEach vertex v ∊ X is a simplex of k, and \n \nEach subset of a simplex of k is also a simplex of k. \nAd-simplex is defined as a simplex composed of d + 1 elements of X. The 0-simplices \ncorrespond to the set of vertices of k, the 1-simplices to the set of its edges, and the 2-\n\n\n \n \nsimplices to the set of its triangles, denoted as θ = {t1,t2, ...,tT}. (T represents the number \nof triangles.) \nThe geometry γ specifies the shape of the surface by associating 3D positions and usually \nsurface normals to the vertices of k. A surface S is convex if it is a subset of the boundary \nof its convex hull (in other words, the minimal convex volume containing S). Computing an \nexact convex decomposition of an arbitrary surface S consists of partitioning it into a \nminimal set of convex sub-surfaces. Chazelle, et al. prove that computing such \ndecomposition is an NP-hard problem and evaluate different heuristics to resolve it \n[Chazelle95]. Lien, et al. claim that the exact convex decomposition algorithms are \nimpractical since they produce a high number of clusters, as shown in Figure 2.8.2 [Lien04]. \nTo provide a tractable solution, they propose to relax the exact convexity constraint and \nconsider instead the problem of computing an approximate convex decomposition (ACD) of \nS. Here, for a fixed parameter ε>0, the goal is to determine a partition Π = {π1,π2, ...,πK} of \nθ with a minimal number of clusters K and verify that each cluster has concavity lower than \nε. \nFigure 2.8.2. Exact convex decomposition versus approximate convex \ndecomposition. \n \n \nThe ACD problem has been addressed in a number of recent publications [Lien04, Lien08, \nKraevoy07, Attene08]. Attene, et al. apply a hierarchical segmentation approach to a \ntetrahedral mesh generated from S [Attene08]. The tetrahedralization process exploited by \nAttene, et al. is, in practice, hard to compute and introduces extra computational \ncomplexity. Other methods avoid this limitation by considering the original 3D mesh directly \n[Lien04, Lien08, Kraevoy07]. Kraevoy, et al. introduce an incremental Lloyd-type \nsegmentation technique exploiting a concavity-based seed placement strategy [Kraevoy07]. \nHere, the concavity measure is defined as the area weighted average of the distances from \nthe clusters to their corresponding convex hulls. Lien, et al. claim that such concavity \nmeasure does not efficiently capture the important features of the surface [Lien04, Lien08]. \nThey propose instead to compute the maximal distance between the mesh vertices and the \nclusters‘ convex hulls. Their divide-and-conquer approach iteratively divides the mesh until \nthe concavity of each sub-part is lower than the threshold ζ, as shown in Figure 2.8.3. Here, \nat each step i, the vertex \nwith the highest concavity is selected, and the cluster to which \nit belongs is divided into two sub-clusters by considering a bisection plane incident to \n. \nFigure 2.8.3. The divide-and-conquer ACD approach introduced in [Lien04] and \n[Lien08]. \n\n\n \n \n \n \nThe main limitation of this approach is related to the choice of the ―best‖ cut plane, which \nrequires a sophisticated analysis of the model‘s features. A public implementation of \n[Lien04] is provided in [Ratcliff06]. Here, the sophisticated feature analysis procedure is \nreplaced by a simple cut plane selection strategy that splits each cluster according to its \nlongest direction. This suboptimal choice generates over-segmentations, which are \noptimized by applying a post-processing procedure aiming at aggregating the maximal \nnumber of clusters while ensuring the maximal concavity constraint. As illustrated in Figure \n2.8.7, the ACDs produced by Ratcliff are, in practice, suboptimal, since the aggregation \nprocedure is applied to clusters generated only by plane-based bisections. \nTo overcome the aforementioned limitations, we introduce, in the next section, a simple and \nefficient hierarchical ACD approach for 3D meshes. \n \nHierarchical Approximate Convex Decomposition \nOur proposed hierarchical approximate convex decomposition (HACD) proceeds as follows. \nFirst, the dual graph of the mesh is computed. (See the upcoming ―Dual Graph‖ section.) \nThen, its vertices are iteratively clustered by successively applying topological decimation \noperations, while minimizing a cost function related to the concavity and the aspect ratio of \nthe produced segmentation clusters. Finally, by approximating each cluster with the \nboundary of its convex hull [Preparata77], a faithful approximation of the original mesh is \ncomputed. This surface approximation is piecewise convex and has a low number of \ntriangles (when compared to T), which makes it particularly well adapted for collision \ndetection. \nLet‘s first recall the definition of the dual graph associated with a 3D mesh. \nDual Graph \nThe dual graph S* associated with the mesh S is defined as follows. Each vertex of S* \ncorresponds to a triangle of S. Two vertices of S* are neighbors (in other words, connected \nby an edge of the dual graph) if and only if their corresponding triangles in S share an edge. \nFigure 2.8.4 illustrates an example of a dual graph for a simple 3D mesh. \nFigure 2.8.4. Example of a dual graph. \n\n\n \n \n \n \nDecimation Operator \nOnce the dual graph is computed, the algorithm starts the decimation stage, which consists \nof successively applying half-edge collapse operations to S*. Each half-edge collapse \noperation applied to an edge (v,w), denoted hecol (v,w), merges the two vertices v and w, \nas illustrated in Figure 2.8.5. The vertex w is deleted, and all its incident edges are \nconnected to v. \nFigure 2.8.5. Half-edge collapse decimation operation. \n \n \nLet A(v) be the list of the ancestors of the vertex v. Initially, A(v) is empty. At each \noperation hecol (v,w) applied to the vertex v, the list A(v) is updated as follows: \nEquation 1 \n \n \n \nSimplification Strategy \n",
      "page_number": 181
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": " \n \nThe decimation process described in the previous section is guided by a cost function \ndescribing the concavity and the aspect ratio of the surface S(v,w) resulting from the \nunification of the vertices v and w and their ancestors [Garland01]: \nEquation 2 \n \n \nAs in [Garland01], we define the aspect ratio Eshape(V,W) of the surface S (v,w) as follows: \nEquation 3 \n \n \nwhere ρ(S(v,w)) and ζ(S(v,w)) represent the perimeter and the area of S (v,w), \nrespectively. \nThe cost EShape(v,w) was introduced in order to favor the generation of compact clusters. In \nthe case of a disk, the cost EShape equals one. The more irregular a surface, the higher its \naspect ratio cost. \nInspired by [Lien08], we define the concavity C(v,w) of S(v,w), as follows (see Figure \n2.8.6): \nFigure 2.8.6. Concavity measure for a 3D mesh. \n \n \nEquation 4 \n \n \n\n\n \n \nwhere P(M) represents the projection of the point M on the convex hull CH(u,w) of S(v,w), \nwith respect to the half ray with origin M and direction normal to the surface S(v,w) at M. \nThe global decimation cost E(v,w) associated with the edge (v,w) is given by: \nEquation 5 \n \n \nwhere \n \nD is a normalization factor equal to the diagonal of the bounding box of S, and \n \nα is a parameter controlling the contribution of the shape factor EShape(v,w) with \nrespect to the concavity cost. (See the upcoming ―Choice of the Parameter α‖ \nsection.) \nAt each step of the decimation process, the hecol (v,w) operation with the lowest \ndecimation cost is applied, and a new partition Π(i)= {π1(i),π2(i), ...,πK(i) } is computed as \nfollows: \nEquation 6 \n \n \nwhere \nrepresents the vertices of the dual graph S* obtained after i half-\nedge collapse operations. This process is iterated until all the edges of S* generating \nclusters with concavities lower than ω are decimated. \nChoice of the Parameter α \nThe clusters detected during the early stages of the algorithm are composed of a low \nnumber of adjacent triangles with a concavity almost equal to zero. Therefore, the \ndecimation cost E is dominated by the aspect ratio related cost EShape, which favors the \ngeneration of compact surfaces. This behavior is progressively inverted during the \ndecimation process, since the clusters become more and more concave. To ensure that the \ncost (α.EShape) has no influence on the choice of the later decimation operations, we have set \nthe parameter α as follows: \nEquation 7 \n \n \nThis choice guarantees, for disk-shaped clusters, that the cost (α.EShape) is 10 times lower \nthan the concavity-related cost \n. \n\n\n \n \n \nExperimental Results \nTo validate our approach, we have first compared its segmentation results to those of \n[Ratcliff06], which provides a simplified version of the original algorithm described in \n[Lien04] and [Lien08]. In Figure 2.8.7, we compare the ACDs generated by our approach to \nthose obtained by using Ratcliff‘s method. Here, the accuracy of the generated piecewise \nconvex approximations is objectively evaluated by using the root mean squares (RMS) and \nHausdorff errors [Aspert02]. Let‘s recall that the RMS error measures the mean distance \nfrom the original mesh S to its piecewise convex approximation S′. It is defined as follows: \nEquation 8 \n \n \nFigure 2.8.7. Comparative evaluation: (a,d,g,j,m,p) original meshes, (b,e,h,k,n,q) \npiecewise convex approximations generated by [Ratcliff06], and (c,f,i,l,o,r) \npiecewise convex approximations generated by the proposed HACD technique. \n\n\n \n \n \nwhere D is the diagonal of the bounding box of S, α(S) is its area, and d(p,S′) is the \ndistance from a point p∊S to S′. The distance d(p,S′) is given by: \nEquation 9 \n \n \n\n\n \n \nThe Hausdorff error, denoted H, measures the maximal distance from S to S′ and is defined \nas follows: \nEquation 10 \n \n \nThe reported results show that the proposed HACD technique provides significantly (that is, \nfrom 20 percent to 80 percent) lower RMS and H errors, while detecting a lower number of \nclusters. Figures 2.8.7(j) through 2.8.7(r) clearly show the limitations of the plane-based \nbisection strategy and the aggregation post-processing procedure of Ratcliff, which \ngenerates over-segmentations and poor ACDs. \nColor Plate 8 presents the segmentation results and the approximate convex \ndecompositions generated by our approach for different 3D meshes. For all the models, the \ngenerated segmentations ensure a concavity lower than ε and guarantee that the maximal \ndistance from S to S′ is lower than 3 percent of D. Therefore, the generated piecewise \nconvex approximations provide faithful approximations of the original meshes with a small \nnumber of clusters. Moreover, our technique successfully detects the convex parts and the \nanatomical structure of the analyzed 3D models. \nFor all of the models shown in Color Plate 8, the piecewise convex approximations were \ncomputed by considering an approximation of the clusters‘ convex hulls with a maximum of \n32 vertices for each. The number of triangles composing the obtained convex surfaces is \nlower than 8 percent of T. Furthermore, the piecewise convexity property makes the \ngenerated approximations particularly well suited to collision detection. \n \nConclusion \nWe have presented a hierarchical segmentation approach for approximate convex \ndecomposition of 3D meshes. The generated segmentations are exploited to construct \nfaithful approximations of the original mesh by a set of convex surfaces. This new \nrepresentation is particularly well suited for collision detection. We have shown that our \nproposed technique efficiently decomposes a concave 3D model into a small set of nearly \nconvex surfaces while automatically detecting its anatomical structure. This property makes \nthe proposed HACD technique an ideal candidate for skeleton extraction and pattern \nrecognition applications. \n \nReferences \n[Attene08] Attene, M., et al. ―Hierarchical Convex Approximation of 3D Shapes for Fast \nRegion Selection.‖ Computer Graphics Forum 27.5 (2008): 503–522. \n[Chazelle95] Chazelle, B., et al. ―Strategies for Polyhedral Surface Decomposition: An \nExperimental Study.‖ Symposium on Computational Geometry (1995): 297–305. \n[Garland01] Garland, M., et al. ―Hierarchical Face Clustering on Polygonal Surfaces.‖ \nSymposium on Interactive 3D Graphics (2001): 49–58. \n\n\n \n \n[Hoppe96] Hoppe, H. ―Progressive Meshes.‖ International Conference on Computer Graphics \nand Interactive Techniques (1996): 99–108. \n[Kraevoy07] Kraevoy, V., et al. ―Model Composition from Interchangeable Components.‖ \nPacific Conference on Computer Graphics and Applications (2008): 129–138. \n[Lien04] Lien, J.M., et al. ―Approximate Convex Decomposition.‖ Symposium on \nComputational Geometry (2004): 457–458. \n[Lien08] Lien, J.M., et al. ―Approximate Convex Decomposition of Polyhedra and Its \nApplications.‖ Computer Aided Geometric Design (2008): 503–522. \n[Aspert02] Aspert, N., et al. ―MESH: Measuring Error Between Surfaces Using the Hausdorff \nDistance.‖ IEEE International Conference on Multimedia and Expo 1 (2002): 705–708. \n[Preparata77] Preparata, F. P., et al. ―Convex Hulls of Finite Sets of Points in Two and Three \nDimensions.‖ ACM Communication 29.2 (1977): 87–93. \n[Ratcliff06] Ratcliff, J. ―Approximate Convex Decomposition.‖ John Ratcliff‘s Code \nSuppository. April 2006. Bolgspot.com. n.d. \n<http://codesuppository.blogspot.com/2006/04/approximate-convex-decomposition.html>. \n \nSection 3: AI \nIntroduction \nAI Level of Detail for Really Large Worlds \nA Pattern-Based Approach to Modular AI for Games \nAutomated Navigation Mesh Generation Using Advanced Growth-Based Techniques \nA Practical Spatial Architecture for Animal and Agent Navigation \nApplying Control Theory to Game AI and Physics \nAdaptive Tactic Selection in First-Person Shooter (FPS) Games \nEmbracing Chaos Theory: Generating Apparent Unpredictability through Deterministic \nSystems \nNeeds-Based AI \nA Framework for Emotional Digital Actors \nScalable Dialog Authoring \nGraph-Based Data Mining for Player Trace Analysis in MMORPGs \nIntroduction \nBorut Pfeifer \n\n\n \n \nborut_p@yahoo.com \nWith recent advances in graphics and animation, AI is one of the areas of game \nprogramming with the most potential for growth. The techniques in this section address \nsome of the most troublesome and encouraging future areas for game AI development. \nAdvances in AI architecture, believable decision-making, more detailed character \nsimulation, and player modeling all offer the possibility of creating new or improved aspects \nof gameplay, as well as enhancing our ability to actually build that gameplay more quickly. \nAI architecture is crucial for effective development. A poor architecture can bring the \ndevelopment of new AI gameplay on a project to a halt. A good one can empower new \nfeatures and new experiences. Cyril Brom, Tomáš Poch, and Ondřej Šerý discuss creating \nworlds with high numbers of NPCs in their gem, ―AI Level of Detail for Really Large Worlds,‖ \nmanaging multiple simulation levels for each area of the game world. Kevin Dill writes about \nusing patterns in AI decision-making code in his gem ―A Pattern-Based Approach to Modular \nAI for Games.‖ The more effective an AI programmer is in creating reusable and scalable \ncode, the more time there is to iterate on gameplay. \nAI movement and pathfinding are always the earliest difficult problems an AI programmer \nhas to solve on a project. ―Automated Navigation Mesh Generation Using Advanced Growth-\nBased Techniques,‖ by D. Hunter Hale and G. Michael Youngblood, details their research on \nnew methods of creating navigation meshes using space-filling algorithms. Michael Ramsey \ncovers the pathfinding used for a wide variety of animals and movement types in World of \nZoo in his gem ―A Practical Spatial Architecture for Animal and Agent Navigation.‖ Brian \nPickrell sheds some light on the often-overlooked area of control theory and how it can be \napplied to an agent‘s steering in ―Applying Control Theory to Game AI and Physics.‖ \nDecision-making is the aspect of AI that affects players most directly. The challenge is \nfinding meaningful decisions characters can make to appear intelligent, while also being \nunderstandable to the player. It is much easier to create decision-making that allows for an \nNPC‘s success than it is to balance understandable behavior with the complexity and depth \nrequired to appear intelligent. Thomas Hartley and Quasim Mehdi describe a method to \nallow NPCs to adapt better to players‘ combat behavior over time with their gem ―Adaptive \nTactic Selection in First-Person Shooter (FPS) Games.‖ Dave Mark discusses how to use \ncomplexity in building AI decision-making models in ―Embracing Chaos Theory: Generating \nApparent Unpredictability through Deterministic Systems.‖ \nMore detailed character simulation can also help create that depth and believability. Rob \nZubek breaks down an AI decision-making approach used in games such as the Sims series \nin his gem, ―Needs-Based AI.‖ Phil Carlisle takes a look at how we can easily add emotional \nmodeling to a behavior tree–based architecture in his gem, ―A Framework for Emotional \nDigital Actors.‖ Baylor Wetzel‘s gem, ―Scalable Dialog Authoring,‖ confronts the difficult \nproblem of creating dialog for many NPCs by abstracting concepts, such as a cultural group \nan NPC belongs to, and describes how to empower designers to author more variety in \ndialog with these abstractions. \nPlayer modeling is a burgeoning area of game AI. It can be used to help improve a player‘s \nexperience, such as in Left 4 Dead. In the gem ―Graph-Based Data Mining for Player Trace \nAnalysis in MMORPGs,‖ Nikhil Ketkar and G. Michael Youngblood write about their work \nmodeling players in massively multiplayer games (MMOs). They‘ve used MMO player data to \ndetermine models to detect gold farmers and bots as well to determine effective locations \nfor in-game advertising. Such models and processes can be used to improve a wide variety \nof aspects of the player experience. \nAs game AI programmers, we are now in the spotlight to create the next level of innovative \ngame experiences. People are no longer impressed by the same fancy normal maps and \nmotion-captured animation; they want their characters to be more believable and more \n\n\n \n \nentertaining in how they act. It‘s up us to make this a reality, and I hope the gems in this \nsection will help give you ideas to solve some of these problems. \n \n3.1. AI Level of Detail for Really Large Worlds \nCyril Brom, Charles University in Prague \nbrom@ksvi.mff.cuni.cz \nTomáš Poch \npoch@dsrg.mff.cuni.cz \nOndřej Šerý \nondrej.sery@dsrg.mff.cuni.cz \nOne challenge for games featuring large worlds with many non-player characters (NPCs) is \nto find a good balance between the consumption of computational resources and simulation \nbelievability. On the one hand, the cost of simulation of a whole world, including all the \nNPCs, is enormous. On the other hand, if one simulates just the proximity of a player, one \nasks for plausibility troubles. For instance, when a player is supposed to return to a once \nleft area, NPCs and objects in this area may not be in a believable state—the area has not \nchanged since it was left, people have not moved, the ice cream left on the table has not \nmelted, and so on. Additionally, this approach cannot handle NPCs that can move freely \naround the world. \nTo compromise between these two extremes, a number of level-of-detail AI techniques \n(LOD AI) have been invented. While LOD AI for a traffic simulation [Chenney01] and for \nenemies in an RPG [Brockington02] and an action game [Grinke04] has already been \nwritten about, less is known about how to vary simulation detail for general NPCs. \nThis gem presents a LOD AI technique tailored for simulations of large worlds featuring \nhundreds of commonplace NPCs with relatively complex behavior. These NPCs can perform \ntasks that include manipulation with several objects and require a variety of everyday \nbehaviors, such as harvesting, merchandising, or watering a garden (as opposed to pure \nwalking and fighting). The NPCs are interactive, and they can move around the world as the \nstory dictates. The technique is gradual, which means that it allows for several levels of \ndetail (LOD) based on the distance from the player or important places (Figure 3.1.1). The \ntechnique also considers the fact that whole locations containing objects and NPCs may \ncease to exist when the LOD decreases and need to be re-created when it increases again. \nFigure 3.1.1. Three types of level-of-detail AI techniques. Places are projected on \nthe X-axis. \n",
      "page_number": 189
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 197-204)",
      "start_page": 197,
      "end_page": 204,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \n \nGradual LOD Example \nThink of a classic fantasy RPG setting: a medieval county with many villages. There is a pub \nin one of them, where miners go to have some fun after their shift. A player can go there as \nwell; she can leave and return any time. You want to avoid the situation in which she would \nrealize that the miners were not being simulated properly when she was at the other end of \nthe village, but you want to save the resources when she is out. You may define the \nfollowing LODs ranging from the full simulation to almost no simulation. \n \nDetail 5: Full simulation—used when the player is in the pub or in its proximity. \n \nDetail 4: Every room from the pub is abstracted to a single point. The tables in the \nsaloon are organized just in a list or an abstract graph; exact positions become \nunimportant. Miners (and other NPCs) are still sitting at the tables, but they are not \ndrinking properly; they just empty the whole glass of beer in one go, say, every 20 \nto 40 minutes. The barman brings new beer, but now he is not walking as he would \nwith LOD 5. Instead, he ―jumps‖ from table to table. The beer still has to be paid for. \nA miner can jump to another table as well, or to a next room, or he can leave the \npub. LOD 4 will be used typically if the player is, say, 100 to 300 meters from the \npub. \n \nDetail 3: The glasses, the tables, and the pub‘s rooms cease to exist. The whole pub \nis abstracted to a single point. Yet the miners can still be there (we don‘t know \nwhere exactly, but we need not care, as will be detailed later). The beer level in the \nbarrel will decrease a bit every half an hour or so based on the number of miners in \nthe pub. The miners can leave the pub or enter. LOD 3 will be used if the player is at \nthe other end of the village. Notice that when she is approaching the pub, the detail \nfirst jumps to 4 and only then to 5. Additionally, when it jumps to 4 from 3, the \nminers must be assigned to tables, and glasses must be generated properly (for \nexample, at the tables). \n \nDetail 2: The whole village is abstracted to a single point. The barman is not \nsimulated because his story, as specified by designers, never leads him out of the \nbar, while the miners still are; they may go from a village (meaning the pub or \nhome) to the mine (in other words, to work). This detail is used when the player is \nnot in the village but is in its proximity. \n \nDetail 1: The village and its surroundings are abstracted to a single point; no \nvillage‘s inhabitants are simulated, but a foreigner may pay a visit. \nYou can imagine that the foreigner is a story-important persona. His importance can \ndemand that the area he is located in has an LOD of at least 4. Thus, as he moves, he \nredefines LOD similarly to the player. Alternatively, some events may be considered \n\n\n \n \nimportant. When miners—in the pub, at LOD 3—start a brawl, the brawl can level up the \ndetail to 5 no matter where the user is. \nThroughout, we will assume that LOD increases or decreases just by 1; larger changes can \nbe done by repetition. We will assume one player in the simulation, but the technique can \nbe used for multiplayer games as well. \nThe LOD technique introduced in this game has been implemented as a part of a general \nsimulator of 2D virtual words, including the aforementioned example (with some \nmodifications). The simulator and the example are included on the CD. \n \nGraphical LOD versus LOD AI \nConceptually, it is often useful to think about a game‘s control mechanisms in terms of a \ntwo-layered architecture. While AI is the higher layer, physics and graphics are the lower \nlayer. (See [Chenney01] for more on this point.) This is actually a simplified view, \nnevertheless useful for explanatory purposes. According to this metaphor, the player‘s \ndirect experience is provided by the lower animation layer, which is only influenced by the \nhigher layer. The LOD AI technique is only related to the higher layer. \nSince the position of the player‘s avatar forces the maximum LOD AI in correctly designed \nworlds, only the areas (or their parts) simulated at the maximum detail may be visualized. \nThe animation layer takes the outcome of the AI layer as an abstract prescription of what to \nshow. The animation layer operates with several graphical levels of detail, and it may add \nadditional complexity above the finest LOD AI detail. In our example, LOD AI Detail 5 takes \ncare of whether a miner will drink a beer or whether he will go to the waypoint on the right \nor on the left, but the movement of his hand or walking smoothly will be dealt with by the \nanimation engine. \n \nSimulation at the Full Detail \nThis gem concerns itself only with the AI layer. Assume to start that our goal is to simulate \nthe whole world at the full AI LOD and that we need not care about animation. A good way \nto think about what happens in the AI layer is in terms of a discrete-event simulation \nparadigm. According to this view, time is represented by a chronological sequence of \nsimulation events, which are ordered in an event list. These simulation events are abstract \nentities that mark time instants in which the state of the game is modified. Additionally, \nevery simulation event can generate a new simulation event to the event list or remove an \nexisting event. Technically, every simulation event is associated with a piece of code. In a \nnutshell, after initialization, the whole system works in the following cycle: \n1. Take the first simulation event from the event list and remove it from the list. \n2. Process this event; that is, run the code associated with the event. As a part of this: \na. Change the state variables of some entities. \nb. Insert new simulation events to the event list at appropriate places. \nc. Remove some simulation events from the event list. \nWhen this paradigm is used, it is important to distinguish between real time, which is the \ntime the user experiences, and simulation time, which is the time of the simulated world as \nrepresented by the event list. One processing cycle happens, by definition, in zero \nsimulation time (though it cannot happen in zero real time). For real-time games, these two \ntimes must of course be synchronized. For more on discrete-event simulations and event \nhandling, see [Channey01, Harvey02, Wiki09]. \n\n\n \n \nThe simulation paradigm per se says nothing about how the simulation events relate to \nwhat really happens in the game. There are designers who specify this; they must create \nsimulation events around those changes in the state of the game that the AI layer should be \naware of. In a sense, the simulation events present the AI layer‘s window into the virtual \nworld. \nOne class of simulation events is story-important events (for example, the dungeon‘s gate \nwill open every day at midnight). These events typically will be pre-scheduled in the event \nlist from the beginning or hooked into the event list by a trigger in run time (for example, \nfrom the moment the player enters the village, the neighboring dungeon will open every \nday at midnight). A different class of events is due to slow changes of objects‘ states (for \nexample, increasing the rust level of a sword every week by one). But the most important \nclass is due to changes caused by atomic actions of NPCs or the player; these actions must \nbe represented by simulation events in the event list. \nBecause these actions are indivisible from the standpoint of the AI layer, it suffices to \nrepresent each of them by a start event and an end event. Typically, these two events will \nnot be present in the event list simultaneously. During processing of a start event, first, \nother parts of the game are notified that the action starts, and second, it is estimated how \nlong the action will last, and its end event is hooked to the event list at the appropriate \nplace (Figure 3.1.2). When this time comes and the end event is processed, states of some \nobjects are changed, and the NPC‘s AI controller is invoked to decide what the next action \nof this NPC is, hooking the start event of that action to the event list. In fact, because start \nevents tend to be hooked at the beginning of the event list, it is often possible to skip their \ngeneration and to hook the respective end events directly. Note that this mechanism allows \nyou to have atomic actions with various durations. \nFigure 3.1.2. Atomic action of sipping a beer is represented in the event list by a \nstart event and an end event. \n \n \nClassic discrete-event simulations typically work only with simulation events hooked into the \nevent list. However, for games, asynchronous events are also needed. That is, sometimes \nanother part of the game can generate an event that has to be propagated to the AI layer \nand processed by it immediately. For instance, the collision detection may recognize that \nsomeone has nudged the sipping person with an elbow. This means that the atomic action \nof sipping a beer cannot be finished. Thus, the AI layer must delete its end event and \ngenerate the start event of spilling action instead. \nAnother issue is that the time of actions‘ ends (thus, the time of end events) is only \nestimated by the AI layer. This is fine when we simulate a part of the world that is not \nvisualized: An estimate becomes the actual duration provided the action is not interrupted \nby an asynchronous event. However, for visualized parts, duration of some atomic actions \nwill be determined by the animation engine. End events of these actions need to be \nsynchronized with the actual end of the action. \n\n\n \n \n \nToward LOD AI: Hierarchical Behavior \nThe question of this gem is how to make the mechanism of start events and end events \ncheaper when the full detail is not required. Our solution will capitalize on the fact that \nbehavior of NPCs can be represented hierarchically. This means, more or less, that the NPCs \nare conceived as having a list of behaviors that are decomposed to sub-behaviors, which \nare further refined until some atomic actions are reached. Figure 3.1.3 shows how this \napplies to a drinking miner. \nFigure 3.1.3. Hierarchical representation of a drinking miner’s behavior. Atomic \nactions are in gray. Other nodes represent tasks. Note that one detail can be \nassigned to more levels. \n \n \nConceptually, this follows the Belief-Desire-Intention architecture [Bratman87]. This will \nmost likely be implemented with behavior trees or hierarchical FSMs—for example \n[Fu&Houlette04, Isla05, Champandard08]. \nIn academic literature, the decomposition of high-level behaviors to sub-behaviors is often \nmore complicated. There, the distinction between goals and tasks is often made. While \ngoals represent what shall be achieved, tasks represent how to achieve this. Every goal can \nbe accomplished by several tasks, and every task can be achieved by adopting some sub-\ngoals. Importantly, an NPC needs to perform only one task to achieve a goal, provided there \nis no failure, but it must fulfill all sub-goals, or most of them, to solve a task. Consequently, \nthe behavioral decomposition is given by an AND-OR tree (AND levels for goals, OR levels \nfor tasks). The CD examples feature such AND-OR trees; however, the LOD technique works \nwell with any behavioral decomposition; the distinction between goals and tasks is \nunimportant for it. Thus, for explanatory purposes, we assume throughout that we have just \nordinary sub-behaviors, and we call them tasks. \n\n\n \n \nWhatever the exact representation is, the key is that a) the hierarchy can be constructed so \nthat its highest level represents abstract behavior to which lower levels add more detail, \nand b) each level can be made executable. Thus: \n1. In the design phase, construct the hierarchy in this way and manually assign LODs to \nits levels (refer to Figure 3.1.3). \n2. During execution, determine what level of the behavioral hierarchy is the lowest that \nshould be executed. This is the level of the LOD that corresponds to the detail of the \nsimulation at the place where the NPCs are located (see Figure 3.1.4). \nFigure 3.1.4. ―Drinking at a table‖ is executed as atomic. Atomic tasks are in \ngray. \n \n3. Execute this task as atomic, approximating what would happen when the simulation \nis run at a finer detail. \nUnfortunately, these three points brings many problems. We will address them in turn. \nWhat Does It Mean to Execute a Task as Atomic? \nAtomic execution means two things. First, every task to be executed atomically must be \nrepresented in the event list by a start event and an end event, similar to atomic actions. \nFor instance, assume that there is LOD 4 in the pub. The corresponding behavioral level \ncontains the task of drinking at a table. The event list will contain the end event of this act \nbut not of atomic actions of sipping a beer or chatting. \nSecond, for every task, designers must specify a) its result, and b) its estimated duration. \nWhen a task is simulated in full detail, its particular runs can have different durations. \nOften, the exact duration of a run will be influenced by the animation layer (for example, an \nNPC walks from one room to another in a different amount of time based on the positions of \nobjects the NPC has to avoid). Fortunately, with lower LODs, you need not worry about \nexactly how long the task would have lasted had the simulation run in the full detail. All you \nneed is a plausible estimate of that time. Based on this estimate, you place the task‘s end \nevent in the event list. Note that you may generate this estimate from a probabilistic \ndistribution given by designers. \n\n\n \n \nWhen should the result (in other words, Point A) be revealed? There are three possibilities: \n1) at the beginning of the task (that is, when the start event is processed); 2) at the end (in \nother words, when the end event is processed); or 3) during the task. For the drinking \nminer, (1) would mean drinking the glass at once and then doing nothing for 20 to 40 \nminutes, while (2) would be doing nothing for 20 to 40 minutes and then drinking the glass \nat once. Variant (3) is not consistent, and (1) changes the world sooner than it is known \nthat the task is successfully finished. Thus, we recommend using (2). \nHowever, designing the scenario at different levels presents extra work, and you have to \nconsider in which situations these shortcuts are really needed. In extreme, you program the \nwhole simulation from scratch at each LOD, though programming more abstract levels is \nmuch simpler than the full detail. \nWhat to Do When the Detail Increases \nSadly, the detail can elevate in the middle of a task‘s atomic execution—in other words, \nbetween its start event and end event. For instance, the player may enter the pub while the \nminers are drinking at a table atomically. We call such situation a task expansion and the \npart of the task that has already been performed at the lower detail a task stub (see Figure \n3.1.5). When this happens, you need to: \n1. Remove the end event of the expanded task from the event list. \n2. Compute the partial effect of the task stub, if needed. \n3. Start the simulation at the higher detail. \nFigure 3.1.5. LOD has increased from 4 (up) to 5 (bottom), creating a stub from \ndrinking atomically. The original end event must be removed. \n \n \nThe partial effect of a task stub should approximate the outcome of the stub‘s detailed \nsimulation (in other words, as if the task was running from the beginning until the moment \nof expansion at the higher detail). You again need the designer‘s specification and the code \nfor determining the partial effect, which is extra work. \nActually, it is necessary to perform Point 2 only if the user may notice the discrepancy; \notherwise, it is sufficient to start execution of the expanded task at the lower detail from its \nbeginning, pretending that the task stub has never happened. In other words, you specify \nthat the partial effect is nothing. In our drinking example, this would mean that all the \ndrinkers would start with a full glass at the time of the LOD increase, but this may be fine if \n\n\n \n \nyou do not visualize how much beer is in the glasses. However, consider watering a garden: \nHere, the player may find it strange that the gardener is always just starting with the first \ngarden bed at the time the player enters the garden. It may be necessary to decide that \nsome fraction of the garden has already been watered at the time of expansion. \nWhen you need to compute the partial effect of a task stub, what are the options? Often, it \nis desirable to avoid simulation of the task stub at the higher detail. This would give you an \nexact result, but at the cost of too much overhead. Instead, you need to consider which \nstate variables the task changes predictably (see Figure 3.1.6, left and middle) and which it \ndoes not (see Figure 3.1.6, right). In the former case, the partial effect can be determined \nby a simple formula. In the latter case, a more sophisticated ad hoc mechanism has to be \ncreated. \nFigure 3.1.6. Although the final outcome of three tasks being executed atomically \nis the same (Point A), their details differ. At the finer LOD, the state variable may \nchange in a predictable way (left, middle) or in an unpredictable way (right). The \narrows denote the LOD increase. \n \n \nFor instance, think of the LOD increase from Detail 3 to 4 in the pub. On one hand, you can \neasily figure out how much beer has been drunk based on the number of miners in the pub. \nOn the other hand, you also need to assign the miners to the tables. To do the latter, the \ndesigner has to come up with an ad hoc mechanism. \nWhat to Do When the Detail Decreases \nLet us have a location in which detail n should be decreased to n-1. We say that tasks (or \natomic actions) corresponding to detail n are being pruned away, whereas tasks \ncorresponding to detail n-1 are being shrunk—that is, starting to be executed as atomic. \nSince the LOD decrease must apply to a whole area, as detailed later, more tasks may need \nto be pruned away—for example, drinking tasks of all individual miners in the pub. These \ntasks may not end at the same moment. Thus, they should be stopped using the \nmechanism of partial effect described previously. Only then can you execute shrinking tasks \nas atomic. It is important to stop the tasks being pruned away at one instant; otherwise, \nthe area will end up in an inconsistent state, with some tasks being simulated at detail n \nand others at n-1. \nAn important point of any LOD AI is that, by definition, some information is lost during a \nLOD decrease. This brings two problems. First, the partial effect of tasks and actions being \npruned away should be partly forgotten but also partly exploited by the subsequent \nsimulation at LOD n-1. Second, when the LOD increases again, the missing information \nshould be reconstructed. This is similar to lossy compression. \nThe second problem was partly treated earlier (assigning miners to tables), and we will also \nreturn to it later. The first problem is exemplified now in Figure 3.1.7, showing a barman \nwalking to a table while LOD goes from 5 to 4. In this example, we are concerned with the \nbarman‘s position, but similar reasoning applies for any state of an object. \n\n\n \n \nFigure 3.1.7. LOD decrease. ―Steps‖ are being pruned away at t2, while ―go-to-\nnext-object‖ task is being shrunk into the atomic ―jump.‖ \n \n \nRecall that the saloon is a single point at LOD 4. Figure 3.1.7 shows that when the \nsimulation runs on the LOD 4, the walking barman is engaged in the task ―go-to-next-\nobject,‖ which makes the barman ―jump‖ from one object to another. These objects are \ntables or the bar. If the saloon is not too oblong, we may further assume unitary distances \n(at LOD 4) between all pairs of these objects. At LOD 5, ―go-to-next-object‖ breaks down \ninto a sequence of ―step-to‖ atomic actions. \nNow, consider the LOD decrease from 5 to 4 at time t2 —that is, around the middle of the \n―go-to-next-object‖ from Object X to the table. Should the barman start his ―jump‖ from \nObject X or Place Y, and how long should it take? \nBecause Place Y does not exist at LOD 4, it seems that the best option is to let the barman \nstart his ―jump‖ from X. However, since you cannot roll back what has already happened, \nthe barman would walk from X to Y twice, and a clever user may notice this should the \ndetail increase soon (say, at time t3, when the barman is expected to be around Z). \nThe second option is to say that the barman is somewhere on the way and compute how \nlong the rest of the ―jump‖ (in other words, from Y to the table) will last. This is more \naccurate, but it creates an extra work. Additionally, this works well only when the object‘s \nstate is updated in a predictable way at the finer detail (refer to Figure 3.1.6, left and \nmiddle). \nThe first method actually works acceptably if you manage to avoid an early LOD increase. \nThe longer the period between a LOD decrease and the subsequent LOD increase, the more \ninconsistencies due to a simplified determination of the initial state of the simulation at LOD \nn-1 are disguised. A simple mechanism for avoiding early LOD increase will be shown later. \n \n",
      "page_number": 197
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 205-213)",
      "start_page": 205,
      "end_page": 213,
      "detection_method": "topic_boundary",
      "content": " \n \nSpace Representation \nSo far, we have spoken about how to simplify behavior given a LOD, but we also need to \nknow the value of the LOD and its radius, and we need these values for all parts of the \nvirtual world. Perhaps the simplest way to get this information is to represent the world \nhierarchically, as you can examine on the CD example. The spatial hierarchy keeps \ninformation about children, parents, and neighbors of every location except for the leaves \nand the root, which lack children or has only them, respectively. \nFor simplification, assume now that the number of LODs equals the number of spatial levels. \n(This is not a strict requirement.) Now, a membrane metaphor can be used to describe \nwhich LOD is where. Imagine an elastic membrane cutting through the spatial hierarchy \n(see Figure 3.1.8), touching some locations. We say that every location or an atomic place \nthat is at the membrane at a particular instant is simulated as an abstract point. No location \n―below‖ the membrane exists. Every NPC is simulated at the LOD equal to the level on \nwhich the membrane is touching the area in which that NPC is located; spatial LOD \ndetermines behavioral LOD. \nFigure 3.1.8. LOD membrane. \n \n \nFormally, the membrane is nothing more than a list of locations that are simulated as \nabstract points at a particular moment. The membrane can be reshaped in every time step. \nFor the purposes of coherence, we enforce the following shaping rules: \n1. If a location or an atomic place X is at the membrane, every location or atomic place \nwith the same parent as X is at the membrane, too. \n2. If a location Y is above the membrane, every location with the same parent as Y is \nabove or at least at the membrane, too. For example, if an atomic place from the \nsaloon is at the membrane, all the saloon‘s atomic places will be at the membrane, \ntoo (Rule 1). This ensures that when the simulation runs in the full detail somewhere \nin the saloon, it will run in the full detail everywhere here. Because the saloon itself \nis above the membrane in this case, all rooms from the pub must be simulated at \nleast as abstract points (Rule 2). This ensures that almost always at least something \nhappens in the locations near the center of attention. \n\n\n \n \nBecause LODs of two adjacent locations can differ (that is, when they do not have \nthe same parent), the visibility of a player should be limited to the location she is in. \nIf this is not possible, such as in open spaces, another shaping rule must be settled. \n3. The detail must be maximum in all locations the player can see at a given instant. \nLimitations of Rules 1 through 3 \nRules 1 through 3 work fine, but you should know that there is an exception in which they \ndo not guarantee that something happens near the center of attention. The problem is that \ntwo neighboring locations may have different parents, thus their LODs can differ by more \nthan 1. Think of a world with two kingdoms. There are two houses there next to each other, \nbut the border goes right between them. Even though the first house is simulated in full \ndetail, the second kingdom may still have just LOD 1; the houses do not have a common \nparent. \nTo deal with this, a mechanism of influences is needed. This would allow you to specify \nanother shaping rule based on the influence a location has on its all neighboring locations. \nThe trouble with this mechanism is that it may cause cascade effects during membrane \nreshaping, increasing the overhead. You can read more about this topic in the \ndocumentation on the CD. \nPathfinding \nSo far, we have been silent on pathfinding. With the spatial hierarchy, it is apparent that a \nhierarchical refinement of A* can be used easily for simplifying pathfinding at a lower LOD. \n \nPositions of Objects and NPCs \nWe know that the pub is abstracted to a single point at LOD 3. Assume there is one miner \nand one barman there. What happens with their positions during LOD changes? \nBecause the pub‘s rooms do not exist at LOD 3, both the barman and the miner are, so to \nspeak, spread out in the whole pub as two uncollapsed quantum particles; they are at the \nmembrane. Assume further that the LOD decreases to 2. While this lifts the miner a level \nup, spreading him out in the village, the barman becomes non-simulated because his story \nnever leads him out of the pub (see Figure 3.1.9). This means that the barman ―fell \nthrough‖ the membrane; his traces have been lost for the rest of the simulation. \nFigure 3.1.9. LOD increase. Some objects are ―lifted up,‖ while others are not. \n\n\n \n \n \n \nThe problem is that we need to narrow down object and NPC positions after the detail \nelevates. When the LOD goes from 2 to 3, we know that the barman has to be collapsed \ninto the pub, but this is not the case of the miner. We do not know where to generate him; \nhe can be collapsed into the pub or into his house or on the street, and so on. A similar \nproblem arises for the LOD increase from Detail 3 to 4. \nWe now introduce a basic mechanism dealing with generation of positions of objects. The \nreal case is actually more complex than this mechanism. Following, we will extend it and \ncomment on how to use it for NPCs. \n1. Objects in every location should be divided into two groups—those that are location-\nnative, for objects owned by the location, and those that are location-foreign, for \nobjects owned by other locations. (A glass is location-native in the pub, as opposed \nto a mine truck.) An object can be location-native in more locations. (A glass is \nlocation-native in the bar as well as in the kitchen.) \n2. Every location from the nth level of the spatial hierarchy should have a method \nimplementing where to generate location-native objects if the detail elevates from n \nto n+1 (for example, during the Detail 3 to 4 transition, the pub ―knows‖ where to \ngenerate the tables and glasses). \n3. When LOD decreases in a particular location, the detailed positional information for \nall the location-foreign objects having been ―lifted up‖ is memorized, but not for the \nlocation-native objects having been ―lifted up‖ (due to Point 2). \n4. When location-native objects ―fall through‖ the membrane, only their total number in \nthe area where they ―fall though‖ is remembered (for example, after the 4-to-3 \ntransition, it will be remembered that there are, say, 27 beer glasses in the pub). For \nlocation-foreign objects, the exact positions are remembered. (After the same \ntransition, the watering can will become non-simulated, but two memory records will \nbe kept: for the 5-to-4 and 4-to-3 transitions.) \n5. When LOD increases, location-foreign objects are generated based on their stored \npositional information. In some rare situations, this information may not be \navailable; this will be discussed in a moment. \nThe idea behind this is that during the design phase, one can specify relatively easily which \nobjects are native in which locations and implement placing algorithms appropriately. \nStorage of Positional Information \n\n\n \n \nWhen a location-foreign object is simulated, it can hold its own positional information. The \nrecords about location-foreign non-simulated objects‘ positions and about numbers of \nlocation-native non-simulated objects can be kept by parent locations. (For example, the \npub ―knows‖ that it contains 27 glasses.) \nOften, when the parent location ceases to exist, the player is so far away that the records \nheld by this location can be happily forgotten. Nevertheless, should a piece of information \nsurvive destruction of the location to which it is attached, you can do the following. In the \ndesign phase, specify the information level denoting the LOD at which this particular kind of \ninformation can be forgotten. At run time, after the location ―falls through‖ the membrane, \nattach your record to the upper location (in our case, the village) provided the new LOD is \nstill higher than or equal to the information level of this record. (Mind the direction: A higher \nLOD is lower in the hierarchy!) \nGeneral Information and Expirations \nThe aforementioned mechanism can be generalized in two ways. First, one can store not \nonly positional information, but any state information. Assume a player has broken a \nwindow in the pub. A clever player would not return soon, but if she does, she may expect \nthe window to still be broken. The record about the broken window can be stored by the \npub. Similarly, sometimes it may be beneficial to record the exact positional information for \na location-native object. If the player moves a table a bit, and the pub‘s LOD goes to and \nfro, the basic aforementioned mechanism would generate the table incorrectly, on the \noriginal place. The extra record will remedy this problem. \nAfter a while, this record may become useless; sooner or later, someone may repair the \nwindow and move the table back. If you need this feature, you can basically delete a record \nafter a specified amount of time using a mechanism of expirations. \nInitialization and Traveling Objects \nThere are two situations in which it is necessary to initialize objects‘ positions. The first one \noccurs after the simulation starts. Additionally, sometimes it is necessary to initialize \nobjects‘ positions when the objects move between locations. \nThe first situation is trivial: You must have an implicit initialization procedure. Let us now \nelaborate on the second case. Assume the pub is simulated at LOD 4, meaning all the pub‘s \nrooms are single points. A miner puts down a watering can in the saloon, where the can is \nlocation-foreign. Now, the LOD elevates to 5. Where do you place the can? Sometimes, the \nprocedure initializing positions at the beginning of the simulation can help. More often, you \nwould find useful a general placing mechanism that groups objects based on their typical \nplaces of occurrence into floor-objects, table-objects, and so on, and that generates these \nobjects randomly within constraints of the objects‘ categories. Consider now that LOD \ndecreases later from 5 to 4, and the miner picks up the can in the pub and takes it out. If \nthis happens, do not forget to delete the record about the can‘s detailed position. \nNPCs \nWhen the detail increases, we have to refine a) positions of NPCs, and b) tasks they are \nengaged in. For (a), we can treat NPCs as objects. For (b), the task expansion mechanism \ndescribed earlier should be used. \n \nReshaping the Membrane \n\n\n \n \nThe final question is how to shape the membrane. A simple solution, adopted by the \nexample on the CD, is to assign an existence level and a view level to every object. If the \ndetail decreases below the existence level, the object ceases to exist; otherwise, it is to be \nsimulated. The view level then determines the detail required by the object after it starts to \nbe simulated. Hence, the view level is always equal to or higher than the existence level. \nFor many common objects, these levels will be equal, but not for story-important objects or \nNPCs. Every user‘s avatar will have the existence level equal to 1 and the view level equal \nto the maximum. \nAssume now a traveler with his view level set to 4 and his existence level to 2. He will not \nexist until the detail is at least 2, but when it elevates to this value, the traveler will demand \nthat in the location to which he has been generated, the detail goes further to 4. Note that \naccording to the shaping rules, this further determines increasing detail in some of the \nneighboring locations. Because this may create new objects demanding additional LOD \nincreases, a cascade of changes may be triggered. The algorithm for doing this consistently \nis detailed in [Šerý06]. \nSometimes, you may want to increase the detail even if no important object is around; for \ninstance, think of a brawl being simulated in a village far away that is expected to grow into \na local uprising. You may need to show the behavior of those NPCs to the player in a cut \nscene. To do this, you can simply put an invisible object into the simulation with the \nappropriate view level to ensure the player will see the AI behavior. \nWhat Is the Radius of LODs? \nUsing this technique, all users and important objects/NPCs tend to automatically create a \n―simulation crater‖ around them due to the spatial hierarchy and the shaping rules. As they \nmove, the detail elevates (typically) by one in locations at the edge of the crater, avoiding \nabrupt jumps of LOD. LODs are not specified in terms of metrical distances, but in terms of \nnumber of locations between the object/NPC and the edge of the crater. \nThis has two advantages. First, the LOD does not change all the time, as would be the case \nwith pure metrical distances, helping to reduce the overhead. Further, the overhead is \nspread in time: If you go from 1 to 5, you do this in more steps. Figure 3.1.10 \ndemonstrates that there is indeed a qualitative difference in overhead between the 4-to-5 \nincrease (left) and 3-to-5 (right) in the pub in our CD example. \nFigure 3.1.10. Processor consumption during the LOD increase. \nThe X-axis represents the time in the game. The LOD increases at 10:15 p.m.; this time is \narbitrary. This figure shows data from three particular runs. Note that the data for the 3-\nto-4 increase resembles the data for the 4-to-5 increase. \n \nSecond, by the time a person arrives at a location, that location typically has been \nsimulated for a while, disguising inconsistencies caused due to lower LODs. \n\n\n \n \nWhen to Decrease the Detail \nThere is a problem with reshaping the LOD membrane when an object moves between two \nlocations repeatedly. The LOD may start to oscillate, increasing the resources‘ consumption. \nYou can do either of the following two things. First, you can have a larger crater \ndetermining when to decrease the LOD. This larger crater would embrace the smaller crater \nthat enforces the LOD increase. Second, you can use, as we did, a garbage mechanism. \nWith the garbage mechanism, you do not decrease LOD until the resources are needed for \nsomething else. This mimics the larger crater automatically. However, because the cleanup \ncauses overhead, it is better to use the garbage mechanism a bit sooner than the resources \nare actually needed, letting the mechanism work over several time steps. \n \nCreating the Structure of the World \nIn practice, it rarely makes sense to have more LODs than levels of the spatial hierarchy. In \nthe spatial hierarchy, you should keep a reasonable number of sublocations for every \nparent—neither 1 nor 50. This is not a strict rule, but if you violate this principle too often, \nthe hierarchy should be reconstructed. Even though higher numbers may make sense \nlogically (a building with many small rooms), with respect to the technique presented here, \nthis would increase the overhead for a LOD change. \nRecall that LODs also have to be assigned to the behavioral hierarchy. You should do this \nconsistently, meaning the degree of abstraction of two tasks from a particular level should \nbe similar. For instance, if LOD 3 is assigned to watering a garden, it should also be \nassigned to cooking, but not peeling potatoes. For technical purposes, you may need more \nlevels in behavioral hierarchy than LODs, as demonstrated on the CD example. \nAnother aspect to keep in mind is that after assigning LODs to tasks, objects required for \nthese tasks must have their existence level set accordingly. Assume watering a garden has \nLOD 3, and the garden is an abstract point at LOD 3. If it is required by designers that \nwatering a garden has to run with a watering can at this LOD, meaning the gardener has to \npick up the can before he comes to the garden, the existence level of the can must be 3 or \nless. Watering a garden would then mean something like, ―Stay there holding the can for a \nspecified amount of time, after which the garden will become watered at one instant.‖ Of \ncourse, designers might also specify that the gardener is able to water the garden without \nthe can at LOD 3; the task‘s outcome would be the same, only the can would not be \nrequired. In the latter case, you would need to generate the can after the LOD increase. \nNote that the technique does not allow for two existence levels for one object kind—for \nexample, it is not possible that a knife for fighting has existence level 3, while a knife for \ncooking has 5. You must define either one object kind with the lower number or two \ndifferent object kinds with two different existence levels. \n \nSource Code Summary \nThe application included on the CD is a simulator of virtual worlds with LOD AI (in other \nwords, it is not a game). The examples discussed in this chapter are based on the \nimplemented demo world, which features five LODs and three kinds of NPCs: miners, \nbarmen, and singers. The following behaviors have been implemented: walking home from \na pub or a mine and vice versa, leisure time behavior (for pubs), and working behavior (for \nmines). The code is in Java, and the documentation that is included details the LOD \ntechnique further. \n\n\n \n \n \nConclusion \nLevel-of-detail techniques allow for compromising between the consumption of \ncomputational resources and simulation plausibility. However, every LOD technique \npresents extra work for designers and programmers. Thus, it should be contemplated \ncarefully which LOD approach is needed and whether it is needed at all. \nThe technique introduced in this gem fits well for large worlds with many NPCs with \neveryday behavior. It capitalizes on the fact that both space and behavior of NPCs can be, \nand often already are, represented hierarchically. The different number of LODs helps not \nonly with simulation plausibility, but also with keeping the overhead under control during \nLOD changes. \nThe technique is generic, which means that for specific domains, special-purpose \nmechanisms can outperform it—for example, for fighting behavior [Brockington02, \nGrinke04]. Even in complex worlds, any special-purpose mechanism can be augmented with \nsome or all of this technique if needed. \n \nAcknowledgements \nThis technique and the simulator on the CD were developed as a part of research projects \n1ET100300517 of the Program Information Society and MSM0021620838 of the Ministry of \nEducation of the Czech Republic. We want to thank several students who participated in the \nsimulator development: Martin Juhász, Jan Kubr, Jiří Kulhánek, Pavel Šafrata, Zdeněk Šulc, \nJiří Vorba, and Petr Zíta. \n \nReferences \n[Bratman87] Bratman, Michael E. Intention, Plans, and Practical Reason. Harvard University \nPress, 1987. \n[Brockington02] Brockington, Mark. ―Level-Of-Detail AI for a Large Role-Playing Game.‖ AI \nGame Programming Wisdom I. Boston: Charles River Media, 2002. 419–425. \n[Champandard08] Champandard, Alex J. ―Getting Started with Decision Making and Control \nSystems.‖ AI Game Programming Wisdom IV. Boston: Charles River Media, 2008. 257–264. \n[Chenney01] Chenney, Stephen. ―Simulation Level-Of-Detail.‖ 2001. University of \nWisconsin. n.d. <http://www.cs.wisc.edu/~schenney/research/culling/chenney-\ngdc2001.pdf>. \n[Fu04] Fu, Dan, and Ryan Houlette. ―The Ultimate Guide to FSMs in Games.‖ AI Game \nProgramming Wisdom II. Boston: Charles River Media, 2004. 283–302. \n[Grinke04] Grinke, Sebastian. ―Minimizing Agent Processing in ‗Conflict: Desert Storm.‘‖ AI \nGame Programming Wisdom II. Boston: Charles River Media, 2004. 373–378. \n[Harvey02] Harvey, Michael, and Carl Marshall. ―Scheduling Game Events.‖ Game \nProgramming Gems 3. Boston: Charles River Media, 2002. 5–14. \n\n\n \n \n[Isla05] Isla, Damian. ―Handling Complexity in Halo 2.‖ 3 Nov. 2005. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/2250/gdc_2005_proceeding_handling_php>. \n[Šerý06] Šerý, Ondřej, et al. ―Level-Of-Detail in Behaviour of Virtual Humans.‖ Proceedings \nof SOFSEM 2006: Theory and Practice of Computer Science 3831 (2006): 565–574. \n[Wiki09] Wikipedia, The Free Encyclopedia. ―Discrete Event Simulation.‖ 2009. Wikipedia. \nn.d. <http://en.wikipedia.org/wiki/Discrete_event_simulation>. \n \n3.2. A Pattern-Based Approach to Modular AI for Games \nKevin Dill, Boston University \nkdill4@gmail.com \nA great deal of time and effort is spent developing the AI for the average modern game. \nYears ago, AI was often an afterthought for a single gameplay programmer, but these days \nmost game projects employ at least one dedicated AI specialist, and entire AI teams are \nbecoming increasingly common. At the same time, more and more developers are coming \nto realize that, even in multiplayer games, AI is not only a critical component for providing \nfun gameplay, but it is also essential if we are going to continue to increase the sense of \nrealism and believability that were previously the domain of physics and rendering. It does \nno good to have a brilliantly rendered game with true-to-life physics if your characters feel \nlike cardboard cutouts or zombie robots. \nGiven this increase in team size, the increasing prominence of AI in the success or failure of \na game, and the inevitable balancing and feature creep that occur toward the end of every \nproject, it behooves us to search for AI techniques that enable fast implementation, shared \nconventions between team members, and easy modification. Toward that end, this gem \ndescribes methods for applying patterns to our AI in such a way as to allow it to be built, \ntuned, and extended in a modular fashion. \nThe key insight that drives the entirety of this work is that the decisions made by the AI can \ntypically be broken down into much smaller considerations, individual tests used in \ncombination to make a single decision. The same considerations can apply to many different \ndecisions. Furthermore, the evaluation of those considerations can be performed \nindependent of the larger decision, and then the results can be combined as necessary into \na final decision. Thus, we can implement the logic for each consideration once, test it \nextensively, and then reuse that logic throughout our AI. \nNone of the core principles described here are new. They can be found throughout software \nengineering and academic AI in a variety of forms. However, all too often we game \nprogrammers rush into building code, trying to solve our specific problem of the day and get \nthe product out the door, without taking a step back and thinking about how to improve \nthose systems. With some thought and organization, it could be easier to change, extend, \nand even reuse bits and pieces in future projects. Taking the time to do that would pay off \nboth in the short run, making life easier (and thus improving the final result) for the current \ntitle, and in the long run, as more AI code is carried forward from game to game. \nA Real-World Example: Apartment Shopping \nLet‘s begin with a real-world example that illustrates the general ideas behind this work. \nImagine that you have just taken a new job as an AI engineer at Middle of Nowhere Games, \nand you are in the process of searching for an apartment in some faraway city. You might \n\n\n \n \nvisit a variety of candidates, write down a list of the advantages and disadvantages of each, \nand then use that list to guide your final decision. \nFor an apartment in a large complex near a busy shopping area, for example, your list \nmight look something like this: \n606 Automobile Way, Apt 316 \nPros \nCons \nClose to work \nGreat view…of a used car lot \nEasy highway access \nNo off-street parking \nConvenient shopping district Highway noise \n \nAnother apartment, located in the attic of a kindly old lady‘s country house, might have a \nwholly different list: \n10-B Placid Avenue \nPros \nCons \nLow rent \n45-minute commute \nNearby woods, biking trails \nNo shopping nearby \nElectricity and water included Thin walls, landlady downstairs \n \nThese lists clearly reflect the decision-maker‘s personal taste—in fact, it seems likely that if \ntwo people were to make lists for the same apartment, their lists would have little in \ncommon. It is not the actual decision being made that‘s important, but rather the process \nbeing used to arrive at that decision. That is to say, given a large decision (Where should I \nlive for the next several years of my life?), this process breaks that decision down into a \nnumber of independent considerations, each of which can be evaluated in isolation. Only \nafter each consideration has been properly evaluated do we tackle the larger decision. \nThere is a reasonably finite set of common considerations (such as the rent, ease of \ncommute, size of the apartment, aesthetics of the apartment and surrounding environment, \nand so forth) that would commonly be taken into account by apartment shoppers. From an \nAI point of view, if we can encode those considerations, we can then share the logic for \nthem from actor to actor, and in some cases even from decision to decision. \nAs an example of the latter advantage, imagine that we wanted to select a location for a \npicnic. Several of the considerations used for apartments—such as overall cost, the \naesthetics of the surrounding environment, and the length of the drive to get there—would \nalso be used when picking a picnic spot. Again, if it were an NPC making this decision, then \ncleverly designed code could be shared in a modular way and could perhaps even be \n",
      "page_number": 205
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 214-221)",
      "start_page": 214,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": " \n \nconfigured by a designer once the initial work of implementing the overall architecture and \nindividual considerations was complete. \nAnother advantage of this type of approach is that it supports extensibility. For example, \nimagine that after visiting several apartments, we came across one that had a hot tub and \npool or a tennis court. Previously, we had not even considered the availability of these \nfeatures. However, we can now add this consideration to our list of pros and cons without \ndisturbing the remainder of our logic. \nAt this point we have made a number of grandiose claims—hopefully sufficient to pique the \nreader‘s interest—but we clearly have some practical problems as well. What is described \nabove is a wholly human approach to decision-making, not easily replicated in code. Our \nnext step, then, should be to see whether we can apply a similar approach to making \nrelatively simple decisions, such as those that rely on a single yes or no answer. \n \nBoolean Decisions \nMany common architectures rely on simple Boolean logic at their core. For example, from a \nfunctional point of view, finite state machines have a Boolean decision-maker attached to \neach transition, determining whether to take that transition given the current situation. \nBehavior trees navigate the tree through a series of Boolean decisions (take this branch or \ndon‘t take this branch) until they arrive at an action they want to take. Rule-based \narchitectures consist of a series of ―rules,‖ each of which is a Boolean decision stating \nwhether or not to execute the associated action. And so forth. \nConstructing Decisions \nTo build a pattern-based architecture for our AI, we first need to define a shared interface \nfor our considerations and then decide how to combine the results of their evaluation into a \nfinal decision. For Boolean decisions, consider the following interface: \nclass IConsiderationBoolean \n{ \npublic: \n    IConsiderationBoolean()                    {} \n    virtual ~IConsiderationBoolean()           {} \n \n    // Evaluate this consideration \n    bool Evaluate(const DecisionContext& context) = 0; \n    // Load the data that controls our decisions \n    void LoadData(const DataNode& node) = 0; \n} \n \nEvery consideration will inherit from this interface and therefore will be required to specify \nthe Evaluate() and LoadData() methods. \nEvaluate() takes a DecisionContext as its only argument. The context contains \nwhatever information might be needed to make a decision. For example, it might include \nthe current game time, a pointer to the actor being controlled, a pointer to the game world, \nand so on. Alternatively, it might simply be a pointer to the actor‘s knowledge base, where \nbeliefs about the world are stored. Regardless, Evaluate() processes the state of the world \n(as contained in the context) and then returns true if execution is approved or false \notherwise. \n\n\n \n \nThe other mandatory function is LoadData(). The data being loaded specifies how the \ndecision should be made. We will go into more detail on this issue later in this gem. \nGames are rife with examples of considerations that can be encoded in this way. For \nexample, we might have a health consideration that will only allow an action to be taken if a \ncharacter‘s hit points are in a specified range. A time-of-day consideration might only allow \nan action to take place during the day (or at night or between noon and 1:00 p.m.). A cool-\ndown consideration could prevent an action from being taken if it has been executed in the \nrecent past. \nThe simplest approach for combining Boolean considerations into a final decision is to give \neach consideration veto power on the overall decision. That is, take the action being gated \nby the decision if and only if every consideration returns true. Obviously, more robust \ntechniques can be implemented, up to and including full predicate logic, but this simple \napproach works well for a surprisingly large number of cases and has the advantage of \nbeing extremely simple to implement. \nA Simple Example: First-Person Shooter AI \nAs an example of this approach in a game environment, consider the simple state machine \nin Figure 3.2.1, which is a simplified version of what might be found in a typical first-person \nshooter‘s combat AI (for example). \nFigure 3.2.1. A simplified state machine for FPS combat AI. \n \n \nIn this AI, Chase is our default state. We exit it briefly to dodge or take a shot, but then \nreturn to it as soon as we‘re done with that action. Here are the considerations we might \nattach to each transition: \nChase ⇒ Shoot: \n \nWe have a line of sight to the player. \n \nIt has been at least two seconds since our last shot. \n \nIt has been at least one second since we last dodged. \n \nThe player‘s health is over 0 percent (that is, he‘s not dead yet). \nChase ⇒ Dodge: \n \nWe have a line of sight to the player. \n \nThe player is aiming at us. \n \nIt has been at least one seconds since our last shot. \n \nIt has been at least five seconds since we last dodged. \n \nOur health is below 60 percent. (As we take more damage, we become more \ncautious.) \n\n\n \n \n \nOur health is less than 1.2 times the player‘s health. (If we‘re winning, we become \nmore aggressive.) \nShoot ⇒ Chase: \n \nWe‘ve completed the Shoot action. \nDodge ⇒ Chase: \n \nWe‘ve completed the Dodge action. \nOne advantage of breaking down our logic in this way is that we can encapsulate the shared \nlogic from each decision in a single place. Here are the considerations used above: \n \nLine of Sight Consideration. Checks the line of sight from our actor to the player \n(or, more generally, from our actor to an arbitrary target that can be specified in the \ndata or the DecisionContext). \n \nAiming at Consideration. Checks whether the player‘s weapon is pointed toward \nour actor. \n \nCool-down Consideration. Checks elapsed time since a specified type of action \nwas last taken. \n \nAbsolute Health Consideration. Checks whether the current health of our actor \n(or the player) is over (or under) a specified cutoff. \n \nHealth Comparison Consideration. Checks the ratio between our actor‘s health \nand the player‘s health. \n \nCompletion Consideration. Checks whether the current action is complete. \nEach of those considerations represents a common pattern that can be found not only in \nthis specific example, but also in a great many decisions in a great many games. These \npatterns are not unique to these particular decisions or even this particular genre. In fact, \nmany of them can be seen in one form or another in virtually every game that has ever \nbeen written. Line-of-sight checks, for example, or cool-downs to prevent abilities from \nbeing used too frequently…these are basic to game AI. \nAI Specification \nAt this point we have all the pieces we need. We know what behaviors we plan to support \n(in this case, one behavior per state), we know all of the decisions that need to be made \n(represented by the transitions), and we know the considerations that go into each decision. \nHowever, there is still some work to do to put it all together. \nMost of what needs to be done is straightforward. We can implement a character class that \ncontains an AI. The AI contains a set of states. Each state contains a list of transitions, and \neach transition contains a list of considerations. \nKeep in mind that the considerations don‘t always do the same thing. For example, both \nChase ⇒ Shoot and Chase ⇒ Dodge contain an Absolute Health consideration, but those \nconsiderations are expected to return true under very different conditions. For Chase ⇒ \nShoot, we return true if the health of the player is above 0 percent. For Chase ⇒ Dodge, on \nthe other hand, we return true if the health of our actor is below 60 percent. More \ngenerally, each decision that includes this condition also needs to specify whether it should \nexamine the health of the player or our actor, whether it should return true when that value \nis above or below the cutoff, and what cutoff it should use. The information used to specify \nhow each instance of a consideration should evaluate the world is obtained through the \nLoadData() function: \nvoid CsdrHealth::LoadData(const DataNode& node) \n\n\n \n \n{ \n    // If true we check the player’s health, otherwise \n    // we check our actor’s health. \n    m_CheckPlayer = node.GetBoolean(―CheckPlayer‖); \n \n    // The cutoff for our health check – may be the \n    //  upper or lower limit, depending on the value \n    //  of m_HighIsGood. \n    m_Cutoff = node.GetFloat(―Cutoff‖); \n \n    // If true then we return true when our health is \n    //  above the cutoff, otherwise we return true when \n    //  our health is below the cutoff. \n    m_HighIsGood = node.GetBoolean(―HighIsGood‖); \n} \n \nAs you can see, this is fairly straightforward. We simply acquire the three values the \nEvaluate() function will need from our data node. With that in mind, here is the \nEvaluate() function itself: \nbool CsdrHealth::Evaluate(const DecisionContext& ctxt) \n{ \n   // Get the health that we’re checking – either ours \n   // or the player’s. \n   float health; \n   if (m_CheckPlayer) \n       health = ctxt.GetHealth(ctxt.GetPlayer()); \n   else \n       health = ctxt.GetHealth(ctxt.GetMyActor()); \n \n   // Do the check. \n   if (m_HighIsGood) \n       return currentHealth >= m_Cutoff; \n   else \n       return currentHealth <= m_Cutoff; \n   } \n} \n \nAgain, there‘s nothing complicated here. We get the appropriate health value (either ours or \nthe player‘s, depending on what LoadData() told us) from the context and compare it to \nthe cutoff. \nThe simplicity of this code is in many ways the entire point. Each consideration is simple \nand easy to test in its own right, but combined they become powerfully expressive. \nExtending the AI \nNow that we have built a functional core AI for our shiny new FPS game, it‘s time to start \niterating on that AI, finding ways in which it‘s less than perfect, and fixing them. As a first \nstep, let‘s imagine that we wanted to add two new states: Flee and Search (as seen in \nFigure 3.2.2). Like all existing states, these new states transition to and from the Chase \nstate. Here are the considerations for our new transitions: \n\n\n \n \nFigure 3.2.2. Our state machine with two new states. \n \n \nChase ⇒ Flee: \n \nOur health is below 15 percent. \n \nThe player‘s health is above 10 percent. (If he‘s almost dead, finish him.) \nFlee ⇒ Chase: \n \nThe player‘s health is below 10 percent. \n \nWe have a line of sight to the player. \nChase ⇒ Search: \n \nWe don‘t have a line of sight to the player. \nSearch ⇒ Chase: \n \nWe have a line of sight to the player. \nAs you can see, adding these states should be fairly simple. We will need to implement the \nnew behaviors, but all of the considerations needed to decide whether to execute those \nbehaviors already exist. \nOf course, not all changes can be made using existing considerations. For example, imagine \nthat we want to make two further changes to our AI: \n \nQA complains that dodging is too predictable. Instead of always using a five-second \ncool-down, they want us to use a cool-down that varies randomly between three and \nseven seconds. \n \nThe artists would like to add a really cool-looking animation for drawing your \nweapon. In order to support this, the designers have asked us to have the characters \ndraw their weapons when they enter Chase and then holster them again if they go \ninto Search. \nThe key is to find a way to modify our existing code so that we can support these new \nspecifications without affecting any other portion of the AI. We certainly don‘t want to have \nto go through the entire AI for every character, find every place that these considerations \nare used, and change the data for all of them. \n\n\n \n \nImplementing the variable cool-down is fairly straightforward. Previously the Cool-Down \nconsideration took a single argument to specify the length of the cool-down. We‘ll modify it \nto optionally take minimum and maximum values instead. Thus, all the existing cases will \ncontinue to work (with an exact value specified), but our Cool-Down consideration will now \nhave improved functionality that can be used to fix this bug and can also be used in the \nfuture as we continue to build the AI. We‘ll have to take some care in making sure that we \ncheck that exactly one type of cool-down is specified. In other words, the user needs to \nspecify either an exact cool-down or a variable cool-down; he or she can‘t specify both. An \nAssert in LoadData() should be sufficient. \nFor the second change, we can make the Chase behavior automatically draw the weapon (if \nit‘s not drawn already), so that doesn‘t require a change to our decision logic. We do need \nto ensure that we don‘t start shooting until the weapon is fully drawn, however. In order to \ndo that, we simply implement an Is Weapon Drawn consideration and add it to the \ntransition from Chase to Shoot. \nData-Driven AI \nOne thing to notice is that all of the values required to specify our AI are selected at game \ndesign time. That is, we determine up front what decisions our AI needs to make, what \nconsiderations are necessary to support those decisions, and what tuning values we should \nspecify for each consideration. Once the game is running, they are always the same. For \nexample, our actor‘s hit points might go up or down, but if a decision uses an Absolute \nHealth consideration, then the threshold at which we switch between true and false never \nchanges during gameplay. \nSince none of this changes during gameplay, nearly all of the decision-making logic can be \nspecified in data. We can specify the AI for each character, where the AI contains a set of \nstates, each state contains a list of transitions, each transition contains a list of \nconsiderations, and each consideration contains the tuning values used for that portion of \nthe AI. This sort of hierarchical structure is something that XML does well, making it an \nexcellent choice for our data specification. \nAs with all data-driven architectures, the big advantage is that if we want to change the way \nthe decisions are made, we only have to change data, not code. No recompile is required. If \nyou‘ve implemented the ability to reload the data while the game is live, you don‘t even \nneed to restart the game, which can save a lot of time when testing a situation that is tricky \nto re-create in-game. Of course, some changes, such as implementing an entirely new \nconsideration, will require code changes, but much of the tuning and tweaking—and \nsometimes even more sizeable adjustments—will not. \nOne thing to consider when taking this approach is whether it‘s worth investing some time \ninto the tools you use for data specification. Our experience has been that the time spent \nspecifying and tuning behavior is significantly greater than the time spent writing the core \nreasoner. Good tools can not only make those adjustments quicker and easier (especially if \nthey‘re integrated into the game so that adjustments can be made in real time), but they \ncan also include error checking for common mistakes and help to avoid subtle behavioral \nbugs that might otherwise be hard to catch. Further, since many of these considerations are \nbroadly applicable to a variety of games, not only the considerations but also the tools for \nspecifying them could be carried from game to game as part of your engine (or even \nintegrated into a new engine). Finally, it is often possible to allow designers and artists to \nspecify AI logic if you create good tools for doing so, giving them more direct control over \nthe look and feel of the game and freeing you up to focus on issues that require your \ntechnical expertise. \nMeta-Considerations \n\n\n \n \nOne of the benefits of this approach is that it reduces the amount of duplicate code in your \nAI. For example, if there are seven different decisions that evaluate the player‘s hit points, \ninstead of writing that evaluation code in seven places we write it once, in the form of the \nAbsolute Health consideration. \nAs our work on the AI progresses, we might quickly find that we also have an Absolute \nMana consideration, an Absolute Stamina consideration, a Time of Day consideration, a \nCool-Down consideration, and a Distance to Player consideration. Although each of these \nconsiders a different aspect of the situation in-game, under the covers they each do exactly \nthe same thing. That is, they compare a floating-point value from the game (such as the \nplayer‘s hit points, the current time of day, the distance to the player, and so forth) to one \nor two static values that are specified in data. \nWith that in mind, it‘s worth looking for opportunities to further reduce duplicate code by \nbuilding meta-considerations, which is to say high-level considerations that handle the \nimplementation of more specific, low-level considerations, such as the ones given above. \nThis has the advantage of not only further reducing the duplication of code, but also \nenforcing a uniform set of conventions for specifying data. In other words, if all of those \nconsiderations inherit from a Float Comparison consideration base class, then the data for \nthem is likely to look remarkably similar, and a designer specifying data for one that he \nhasn‘t used before is likely to get the result he expects on his first try, because it works the \nsame way as every other Float Comparison consideration that he‘s used before. \n \nFloat-Based Decisions \nWhile nearly all decisions are ultimately Boolean (that is, an AI either takes an action or it \ndoesn‘t), it is often useful to evaluate the suitability of a variety of options and then allow \nthat evaluation to guide our decisions. As with Boolean approaches, there are a variety of \napproaches for doing this. Discussion of these approaches can be found throughout the AI \nliterature. A few game-specific examples include [Dill06, Dill08, and Mark09]. For the \npurposes of this gem, however, the interesting question is not how and when to use a float-\nbased approach, but rather how to build modular, pattern-based evaluation functions when \nwe do. \nAn Example: Attack Goals \nImagine that we are responsible for building the opposing-player AI for a real-time strategy \ngame. Such an AI would need to decide when and where to attack. In order to do this, we \nmight periodically score several prospective targets. For each target we would consider a \nnumber of factors, including its economic value (that is, whether it generates revenue or \ncosts money to maintain), its impact on the strategic situation (for example, would it allow \nyou access to the enemy‘s territory, consolidate your defenses, or protect your lines of \nsupply), and the overall military situation (in other words, whether you can win this fight). \nOur next step should be to find a way for each of these considerations to evaluate the \nsituation independently that enables us to easily combine the results of all of those \nevaluations into a single score, which can be used to make our final decision. \nCreating an evaluation function is as much art as science, and in fact there is an entire book \ndedicated to this subject [Mark09]. However, just as with Boolean decisions, there are \nsimple tricks that can be used to handle the vast majority of situations. Specifically, we can \nmodify our Evaluate() function so that it returns two values: a base priority and a final \nmultiplier. When every consideration has been evaluated, we first add all of the base \npriorities together and then multiply that total by the product of the final multipliers. This \n\n\n \n \nallows us to create considerations that are either additive or multiplicative in nature, which \nare two of the most common techniques for creating priority values. \nComing back to our example, the considerations for economic value and strategic value \nmight both return base priorities between –500 and 500, generating an overall base priority \nbetween –1,000 and 1,000 for each target. They would return a negative value if, from their \npoint of view, taking this action would be a bad idea. For example, capturing a building that \nhas an ongoing upkeep cost might receive a negative value from the economic \nconsideration (unless it had some economic benefit to offset that cost), because once you \nown it, you‘ll have to start paying its upkeep. Similarly, attacking a position that, if \nobtained, would leave you overextended and exposed would receive a negative value from \nthe strategic consideration. These considerations could return a final multiplier of 1. \nThe consideration for the military situation, however, could be multiplicative in nature. That \nis, it would return a base priority of 0 but would return a final multiplier between 0 and 3. \n(For a better idea of how to generate that multiplier, see our previous work [Dill06].) Thus if \nthe military situation is completely untenable (in other words, the defensive forces are much \nstronger than the units we would use to attack), then we could return a very small \nmultiplier (such as 0.000001), making it unlikely that this target would be chosen no matter \nhow attractive it is from an economic or strategic standpoint. On the other hand, if the \nmilitary situation is very favorable, then we would strongly consider this target (by \nspecifying a multiplier of 3), even if it is not tremendously important in an economic or \nstrategic sense. If we have no military units to attack with, then this consideration might \neven return a multiplier of 0. \nWe do not execute an action whose overall priority is less than or equal to zero. Thus if all \nof the base priorities add up to a negative value, or if any consideration returns a final \nmultiplier of zero, then the action will not be executed. Just as in the Boolean logic case, a \nsingle consideration can effectively veto an action by returning a final multiplier of zero. \nAs with the previous examples, the greatest value to be found is that these same \nconsiderations can be reused elsewhere in the AI. For example, the economic value might \nbe used when selecting buildings to build or technologies to research. The strategic value \nmight be used when selecting locations for military forts and other defensive structures. The \nmilitary situation would be considered not only for attacks, but also when deciding where to \ndefend, and perhaps even when deciding whether to build structures in contested areas of \nthe map. \nAlternate Approaches \nOne weakness of the aforementioned approach is that it only allows considerations to have \nadditive or multiplicative effects on one another. Certainly there are many other ways to \ncombine techniques—in fact, much of the field of mathematics addresses this topic! One \ncommon trick, for example, is to use an exponent to change the shape of the curve when \ncomparing two values. Certainly we can extend our architecture to include this, perhaps \nincluding an exponent to the set of values returned by our Evaluate() function and \napplying all of the exponents after the final multipliers. Doing so significantly increases the \ncomplexity of our AI, however, because this new value needs to be taken into account with \nevery consideration we implement and every decision we make. This may not seem like a \nbig deal, but it can make the task of specifying, tuning, and debugging the AI significantly \nharder than it would otherwise be—especially if it is to be done by non-technical folks who \nmay not have the same intuitive sense of how the numbers combine as an experienced AI \nengineer would. \nAlong the same lines, in many cases even the final multiplier is overkill. For simpler \ndecisions (such as those in an FPS game or an action game), we can often have the \nEvaluate() function return a base priority as before, but instead of returning a multiplier, \nit can simply return a Boolean value that specifies whether it wants to veto this action. If \n",
      "page_number": 214
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 222-231)",
      "start_page": 222,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": " \n \nany consideration returns false, then the action is not taken (the score is set to 0); \notherwise, the score is the sum of the base priorities. \n \nConclusion \nThis gem has presented a set of approaches for building modular, pattern-based \narchitectures for game AI. All of these approaches function by breaking a decision into \nseparate considerations and then encoding each consideration independently. There are \nseveral advantages to techniques of this type: \n \nCode duplication between decisions is dramatically reduced, because the code for \neach consideration goes in a single place. \n \nConsiderations are reusable not only within a single project, but in some cases also \nbetween multiple projects. As a result, AI implementation will become easier as the \nlibrary of considerations grows larger and more robust. \n \nMuch of the AI can be specified in data, with all the advantages that implies. \n \nWith proper tools and a good library of considerations, designers and artists can be \nenabled to specify AI logic directly, both getting them more directly involved in the \ngame AI and freeing up the programmer for other tasks. \n \nReferences \n[Dill06] Dill, Kevin. ―Prioritizing Actions in a Goal-Based AI.‖ AI Game Programming Wisdom \n3. Boston: Charles River Media Inc., 2006. 321–330. \n[Dill08] Dill, Kevin. ―Embracing Declarative AI with a Goal-Based Approach.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media Inc., 2008. 229–238. \n[Mark09] Mark, Dave. Behavioral Mathematics for Game AI. Course Technology PTR, 2009. \n \n3.3. Automated Navigation Mesh Generation Using Advanced \nGrowth-Based Techniques \nD. Hunter Hale \nG. Michael Youngblood \nWhen implementing a navigation system for intelligent agents in a virtual environment, the \nagent‘s world representation is one of the most important decisions of the development \nprocess [Tozour04]. A good world representation provides the agent with a wealth of \ninformation about its environment and how to navigate through it. Conversely, a bad \nrepresentation of the world can confuse or mislead an agent and become more of a \nhindrance than an aid. Currently, the most common type of world representation is the \nnavigation mesh [McAnils08]. This mesh contains a complete listing of all of the navigable \nareas (negative space) and occupied areas (positive space) present in a level or area of a \ngame. \nTraditional methods of generating the navigation mesh focus on using the vertices of \nobjects to generate series of triangles. These triangles then become the navigation mesh \n[Tozour02]. This does generate high-coverage navigation meshes, but the meshes tend to \n\n\n \n \nhave areas that can cause problems for agents navigating through the world. These \nproblem areas take the form of many separate triangular negative space areas coming \ntogether at a single point. Agents or other objects that are standing on or near this point \nare simultaneously in more than one region. The presence of objects in more than one \nregion at once means that every region will have to be evaluated for events involving the \noverlapping objects instead of just a single region if objects were well localized. \nInstead of using a triangulation-based navigation mesh generation technique, we \napproached the problem using the Space Filling Volume (SFV) algorithm [Tozour04] as a \nbase. SFV is a growth-based technique that first seeds the empty areas of a game world \nwith quads or cubes and then expands the objects in every direction until they hit an \nobstruction. The quads and the connections between them define the navigation mesh. By \nusing quads as the core shape in the algorithm, the problem of many regions coming \ntogether in a single point is dramatically reduced, since quads can only meet at most four \ncorners. This basic approach works well for worlds composed of axis-aligned obstructions \nbut produces low-coverage navigation meshes when applied to non-axis-aligned worlds or \nhighly complex worlds. Our improved algorithms address these limitations, as well as \nprovide several other benefits over traditional navigation mesh generation techniques. \nThe two algorithms described here are enhancements to the traditional implementations of \n2D and 3D Space Filling Volumes. The first is a 2D algorithm called Planar Adaptive Space \nFilling Volumes (PASFV). PASFV consumes a representation of an arbitrary non-axis-aligned \n3D environment, similar to a blueprint of a building, and then generates a high-quality \ndecomposition. Our new decomposition algorithms seed the world with growing quads, \nwhich, when a collision with geometry occurs, dynamically increase their number of sides to \nbetter approximate the shape the growing region intersected. Using the ability to \ndynamically create higher-order polygons from quads along with a few other features not \npresent in classic SFV, PASFV generates almost 100-percent coverage navigation meshes \nfor levels where it is possible to generate planar splices of the obstructing geometry in the \nlevel. \nThe second algorithm, Volumetric Adaptive Space Filling Volumes (VASFV), is, as the name \nimplies, a native 3D implementation of Adaptive Space Filling Volumes with several \nenhancements. The enhancements allow VASFV to grow cuboids that morph into complex \nshapes to better adapt to the geometry of the level they are decomposing, similar to the 2D \nversion. This algorithm, like its 2D cousin, generates a high-coverage decomposition of the \nenvironment; however, the world does not need to be projected into a planar \nrepresentation, and the native geometry can be decomposed without simplification. In \naddition, this algorithm has a speed/time advantage over the PASFV in post-processing \nbecause it can consume complex levels in a single run. PASFV generally has to decompose a \nlevel one floor at a time, and the generated navigation meshes must then be reconnected to \nbe useful. VASFV will generate a single navigation mesh per level, removing a potentially \nexpensive step from the process of generating a spatial decomposition. \nThe Algorithms \nBoth the PASFV and VASFV algorithms work off the common principle of expanding a grid of \npre-seeded regions in a game environment to fill all of the available negative space. In \npractice, this filling effect looks somewhat similar to a marshmallow that has been heated in \nthe microwave. Both of these algorithms use a similar approach, but the implementations \nare sufficiently different that each algorithm deserves a full explanation. \nPASFV \nThe PASFV algorithm [Hale08] is an iterative algorithm that can be broken down into a \nseries of simple steps. First, a set of invariants and starting conditions must be established \nand maintained. For our implementation of PASFV, all of the input geometry must be \nconvex. This allows the use of the point-in-convex-object collision test [Schneider03] when \n\n\n \n \ndetermining whether a growing region has intruded into a positive space area. If the input \ngeometry is not natively convex, it can be converted to be convex by using a triangular \nsubdivision algorithm. While the algorithm is running, the following two conditions must be \nmaintained to have a successful decomposition. First, at the end of every growth cycle, all \nthe negative space regions in the world must be convex; otherwise, the collision detection \ntests, which are based on an assumption of convexity, will return invalid data. Second, if a \nregion has ended a growth cycle covering an area of the level, it must continue to cover \nthat area. If this restriction is not maintained, there will be gaps in the final decomposition. \nOur algorithm begins in a state we refer to as the initial seeding state, where the world is \n―seeded‖ with a user-defined grid at specified intervals of negative space regions. These \nregions will grow and decompose the world. If the proposed seed placement falls within a \npositive space obstruction, it is discarded. Initial regions are unit squares (a unit square is a \nsquare with an edge length equal to one of the base units of the world) with four edges \narranged in a counterclockwise direction from the point closest to the origin. \nThe initial placement of these regions in the world is such that they are axis-aligned. After \nbeing seeded in the world, each of the placed regions is iteratively provided a chance to \ngrow. Growth is defined for a region as a chance to move each edge outward individually in \nthe direction of each edge‘s normal. The decomposition of a level may take two general \ncases. The first case occurs when all of the positive space regions are axis-aligned. The \nmore advanced decomposition case occurs if there is non-axis-aligned geometry. \nFirst, we will examine the base case or axis-aligned case for a spatial decomposition in \nPASFV. Growth occurs in the direction of the normal for each of the edges of a region and is \na single unit in length. After an edge has advanced, we verify the new regional coverage \nwith three collision detection tests. We want to guarantee that no points from our newly \nexpanded region have intruded into any of the other regions or any positive space \nobstructions. We also want to prove that no points from other regions or obstructions would \nbe contained within the growing region. Finally, the region will perform a self-test to ensure \nthat the region is still convex. This final check is not necessary for the base case of the axis-\naligned world and can be omitted if there are no non-axis-aligned collision objects. \nAssuming all tests return results showing there are no collisions or violations of convexity, \nthe region finalizes its current shape, and the next region grows. \nIf a collision is detected, several things must be done to correct it. First, the growing region \nmust return to its previous shape. At this point, since both the region and the obstruction it \ncollided with are axis-aligned, we know the region is parallel and adjacent to the object, as \nshown in Figure 3.3.1(a). Stopping growth here will provide an excellent representation of \nfree space near the collided object. Finally, we set a flag on the edge of the region where \nthe collision occurred. This flag indicates the edge should not attempt to grow again. The \niterative growth proceeds until no region is able to grow. This method of growth is sufficient \nto deal with axis-aligned worlds and produces results similar to traditional SFV. \nFigure 3.3.1. In this illustration we see all of the potential collision cases in PASFV. \nThe growing negative space regions are shown as white boxes, and the direction \nof growth is marked with an arrow. Positive space regions are drawn in gray. In \n(a) we see the most basic axis-aligned collision case. Then (b) shows the collision \ncase that occurs when a vertex of a positive space object intersects a growing \nnegative space region. Finally, in (c) we illustrate the most complex case, where a \nnegative space region is subdivided into a higher-order polygon to better adapt to \nworld geometry. \n\n\n \n \n \n \nThe advanced case algorithm for PASFV is able to deal with a much wider variety of world \nenvironments. This case begins by building from the base case algorithm; however, because \nit needs to deal with non-axis-aligned positive space regions, it incorporates several new \nways of dealing with potential collisions. When a collision with a positive space object \noccurs, one of three cases handles the collision. The first case occurs if the colliding edge of \nthe positive space object is parallel to the edge of the growing negative space region. The \nparticular obstruction is axis-aligned, so we revert to the base case. \nThe second case occurs when a single vertex of an obstruction collides with a growing edge, \nas shown in Figure 3.3.1(b). In this case, there is, unfortunately, nothing we can do to grow \nfurther in this direction. This occurs because unless we are willing to lose the convex \nproperty of the region or relinquish some of the area already covered by the region, doing \neither of these things would violate our invariants. This case reverts to the base case, and \nthe negative space around the object will have to be covered by additional seeding passes, \nwhich are described at the end of this section. \nThe final, most complex, collision case occurs when a vertex of the growing region collides \nwith a non-axis-aligned edge of a positive space obstruction, as shown in Figure 3.3.1(c). In \nthis case, the colliding vertex must be split into a pair of new vertices and a new edge \ninserted between them. This increases the order of the polygon that collided with the \nobstruction. The directions of growth for these two newly created vertices are modified so \nthey will follow the line equation for the edge that they collided with instead of the normal \nof the edge they were on. \nIn addition, potential expansions of these new vertices are limited to the extent of the \npositive space edge they collided with. In this manner, the original edges, which were \nadjacent to the collision point, grow outward. This outward movement expands the newly \ncreated edge, so that it is spread out along the obstruction. Limiting the growth of these \nvertices is important because they are creating a non-axis-aligned edge as they expand. As \nlong as this newly generated non-axis-aligned edge is adjacent to positive space, no other \nregion can interact with it, and we limit region-to-region collisions to the base case. \nBy using these three advanced case collision solutions, we are able to generate high-quality \ndecompositions for non-axis-aligned worlds. As in the simple case with an axis-aligned \nworld, the algorithm stops once all of the regions present in the world are unable to grow \nany further. \n\n\n \n \nThe aforementioned growth methods are not, by themselves, enough to ensure that the \nentirety of the world is covered by the resulting navigation mesh. In particular, the \ndecompositions resulting from the second collision case are suboptimal. In order to deal \nwith these issues, the second half of the PASFV algorithm comes into play. After all growth \nhas terminated, the algorithm enters a new seeding phase. In this phase, each region \nplaces new regions (seeds) in any adjacent unclaimed free space, and then we flag the \nregion as seeded so they will not be considered if there are any later seeding passes. If any \nseeds are placed, they are provided the opportunity for growth like the originally placed \nregions. This cycle of growth and seeding repeats until there are no new seeds placed in the \nworld, as shown in Figure 3.3.2. At this point, the algorithm has fully decomposed the world \nand terminates. \nFigure 3.3.2. In this illustration we see several growth and seeding cycles in the \nPASFV algorithm to decompose an area. The growing negative space regions are \nshown as white boxes, and the direction of growth is marked with an arrow. \nPositive space regions are drawn in gray. \n \n \n \nVASFV \nThe Volumetric Adaptive Space Filling Volume (VASFV) algorithm [Hale09] is a natural \nextension of the PASFV algorithm. Unlike PASFV, the VASFV algorithm works in native 3D \nand grows cubes and higher-order polyhedrons instead of quads. The initial constraint of \nconvex input shapes is the same for both algorithms. In addition, the requirements that \ndecomposed areas remain decomposed and that all regions always end a growth step in a \nconvex state are still necessary. \nThe initial setup of VASFV is very similar to its predecessor. Both algorithms begin by \nseeding a grid of initial regions throughout the world. In VASFV, the grid extends upward \nalong the Z-axis of the world as well as X-axis and Y-axis. Then, in the first of many \ndepartures from the previous algorithm, the seeds fall down in the direction of gravity until \nthey come to rest on an object. Seeds that end in the same location are removed. This \nhelps to prevent the formation of large, relatively useless regions that float above the level \nand allows the ground-based regions to grow up to the maximum allowable height. These \nseeds are initially spawned as unit cubes represented as regions with four faces listed in a \ncounterclockwise order, starting with the closest to the origin followed by the bottom and \ntop faces, respectively. At this point, the regions may grow and expand. \n\n\n \n \nAs with the planar version of this algorithm, there are two main cases to deal with: axis-\naligned and non-axis-aligned worlds. The base case for the axis-aligned world proceeds in a \nmanner almost identical to the planar algorithm. Each region is iteratively provided the \nopportunity to grow out each face one unit in the direction of the normal of each face. We \nthen run the same three tests for error conditions as performed in the planar version of the \nalgorithm. As a reminder, these three tests are checks to ensure that the growing region \nhas not intersected an existing region or obstruction with one of its vertices, that no \nregion‘s or object‘s vertices have intersected the newly expanded region with their vertices, \nand that the region is still convex. If any of these checks fail, the region will revert to its \nprevious size and not attempt to grow again in that direction. These steps are identical to \nthe planar case; however, the secondary algorithms required to implement them are more \ncomplex in 3D. As with the planar version of this algorithm, this base case will produce a \nvery good decomposition of an axis-aligned world. \nLike the planar version of the algorithm, the advanced growth case for dealing with \ncollisions with non-axis-aligned geometry can be broken down into four cases (shown in \nFigure 3.3.3). The primary determining factor of which of the cases the algorithm will go \ninto is determined by how many vertices are in collision and whether negative space \nvertices collide with positive space, or vice versa. The simplest case occurs when the \ngrowing cubic region has intersected one or more vertices of a positive space obstruction. \nJust like in 2D, since there is nothing that the growth algorithm can do to better \napproximate the free space around the object it has collided with, it returns to its previous \nvalid shape, halting further growth in that direction. \nFigure 3.3.3. This illustration shows the possible collision cases for the VASFV \nalgorithm. The growing negative space regions are shown in white. The positive \nspace objects are shown in gray. Section (a) shows the base axis-aligned collision \ncase from above. Section (b) shows the more complex positive space vertex \ncollision case, again from above. Sections (c) and (d) are illustrated slightly \ndifferently. In order to clearly show how the negative space region reacts to a \npositive space collision, the positive space object is not drawn, and the colliding \nvertex is marked with a circle. Section (c) shows the single vertex collision case \nwhere a new triangular face is inserted into the negative space region. Section (d) \nshows the more complex two-vertex collision where a quadrilateral face is \ninserted into a negative space region to better approximate the object it collided \nwith. \n\n\n \n \n \nThe next three collision cases occur when vertices from a single face of the growing \nnegative space region intersect positive space. The first and simplest of these cases occurs \nwhen three or more vertices of a negative space region intersect the same face of a positive \nspace object. When this happens, it means that the growing face of the negative space \nobject is parallel to and collinear with the face of the positive space obstruction it \nintersected. Therefore, these two faces are both axis-aligned. We know this because three \npoints define a plane, so by sharing these three points, both of these faces are on the same \nplane. This tells us that since the negative space face is axis-aligned, the positive space face \nwe collided with must be as well. We can thus revert to the base case for this collision. \nThe final two collision cases require the insertion of a new face into the negative space \nregion so that it adapts to the face of the collided object. The first case occurs when a single \nvertex of the region intersects an obstruction. In this case, the vertex will be subdivided into \nthree new vertices, and a new triangular face is inserted (which has the same plane \nequation as the face of the object it collided with). The normal of this new face will be the \ninverse of the normal of the face of the intersected obstruction. These new points are \nrestricted to prevent them from growing beyond the face of the obstruction they collided \nwith in order to not create more non-axis-aligned geometry. \nThe final collision case occurs when exactly two vertices of a negative space region intersect \nanother object. This means that a single edge of the region is in contact with the shape it \ncollided with and that edge needs to be split. We split the edge by adding a new rectangular \nface to the region and by subdividing each colliding vertex into two vertices. This new face \nis once again created using the negation of the normal of the face it would intersect. The \npoints involved in the collision are restricted to growing just along the collided obstruction. \nWith the last two special collision cases, it is possible to generate navigation meshes with a \ndegree of accuracy and fidelity to the underlying level that is not possible using previous \ngrowth-based techniques. \nLike the planar version of this algorithm, VASFV also uses a seeding algorithm to ensure full \ncoverage of a level. However, the volumetric seeding approach is slightly different from the \nplanar version. Once each region has reached its maximum possible extents, the seeding \n\n\n \n \nalgorithm iteratively provides each region a chance to create new seeds in adjacent \nnegative space regions. However, instead of immediately growing the new regions, the \nnewly placed seeds are subjected to a simulated gravity and projected downward until they \nhit something, and duplicate seeds that end up occupying the same space as already placed \nseeds are removed. At this point, the algorithm allows the newly placed seeds a chance to \ngrow. This cycle of growth and seeding repeats until no new seeds are successfully placed, \nat which point the algorithm terminates. \nThe application of gravity to seeds might not be the most obvious approach to seeding in \n3D, but it serves an important purpose in orienting region growth to better accommodate \nagent movement through the world. A good example of this occurs on staircases. First, \nconsider the case where seeds do not drop due to gravity. As shown in Figure 3.3.4, a \nregion that grows up adjacent to the bottom of the stairs will generate a single seed midway \nup the stairs, which will contain air space above the first several stairs and only land on one \nof the middle stairs. Then later seedings will result in a confusing mess of regions, none of \nwhich accurately models stair usage. Some of these regions require the agent to crawl \nthrough, while other regions require the agent to fly. \nFigure 3.3.4. This figure serves to illustrate how gravity affects the seeding \nprocess for VASFV. In this figure, we see a staircase viewed from the side. Positive \nspace regions are marked in white. Negative space regions in this illustration are \nmarked with the light gray gradient. The upper set of four time steps shows what \nhappens if the generated seeds are allowed to float freely and grow in midair. The \nlower six time steps show how the decomposition changes to better model the \nstair steps after the application of gravity to each generated seed. \n \nNow consider the same staircase decomposed using a gravity-based seeding method. With \ngravity-assisted seeding, when a seed is generated from the initial region at the bottom of \nthe stairs, it falls into the floor space of the first stair. The seed then grows outward to fully \ndecompose the floor of that single stair and grows up into the airspace over the stair. In this \nmanner, the seeds gradually climb the stairs, and each stair will become its own logical \nregion, which makes sense given how stairs are typically traversed. This gravity-based \nseeding is also applicable to other methods of world space traversal, such as flight, because \nbiasing the decomposition world in respect to the features present on the ground still makes \nsense. \n\n\n \n \n \nPost-Processing \nDecompositions generated using either of the algorithms presented here can be improved \nby the application of several simple post-processing steps. Most importantly, if any two \nregions are positioned such that they could be combined into a single region while \nmaintaining convexity, these regions should combine into a single region. The same \ntechnique can be applied to compress three input regions into two convex regions. This \ntechnique can be extended to higher numbers of regions, but it becomes harder to \nimplement and the returns generally decline, because larger combinations are less likely to \nyield new convex shapes. All of the negative space regions should be examined for zero-\nlength edges or collinear vertices, which, if detected, should be removed. \nA full navigation mesh can be constructed from the decomposition by linking adjacent \nnegative space regions. Aside from which negative space regions connect to each other, \neach region can also store least-cost paths to every other region. Other navigation mesh \nquality metrics can be applied to the generated mesh to determine whether it is good \nenough for its intended purpose or if some of the input parameters for initial seeding should \nbe adjusted and a better navigation mesh generated. \n \nConclusion \nIn this gem, we have presented two new growth-based methods of generating navigation \nmeshes. Both of these new methods are derived from the classic Space Filling Volumes \nalgorithm. These two methods each have areas where they specialize, and both generate \nexcellent decompositions, as shown in Figure 3.3.5. \nFigure 3.3.5. The first image shows the results of PASFV. Black areas indicate \nobstructions, while the variously colored regions show negative space. The second \nimage shows a navigation mesh generated by VASFV on a non-axis-aligned \nstaircase and viewed from the side. \n \n \nThe PASFV technique works very well for levels that can be projected to a single 2D plane. \nIt also can deal with levels that have more than one 2D representation, though these will \nrequire a touch more post-processing to combine all of the navigation meshes into a single \n\n\n \n \nmesh. The navigation meshes produced by this algorithm are very clean with few sharp \npoints or narrow regions that taper off to a point. Such regions are problematic because \nthey cannot contain all of an agent moving through the world, and an agent can end up in \nmany different regions at the same time. This is not a problem for PASFV because at most \nfour different negative space regions can come together at a point, since all negative-space-\nto-negative-space collisions will be axis-aligned, and the angles involved in these collisions \ntend to be around 90 degrees. PASFV is also highly comparable to other navigation mesh \ngeneration algorithms in terms of speed, as it can be shown to run in O(n1/x) with an upper \nbound of O(n), where n is the number of square units of space to decompose and x is a \nfunction of how many seeds are placed in the world [Hale09b]. \nFor more complex world environments that do not approximate to 2D very well or that \nwould require multiple 2D approximations, Volumetric Adaptive Space Filling Volumes is a \nsolution for navigation mesh generation, even though it is slightly harder to implement and \ntakes longer to run due to the more complex collision calculations. VASFV will also provide \ngood decompositions that have few narrow corners or poorly accessible regions. In addition, \nbecause of its gravity-based seeding, VASFV will better model how agents move, resulting \nin superior decompositions to traditional triangulation-based methods [Hale09a]. The run \ntime for VASFV is algorithmically the same as PASFV; however, instead of n being the \nnumber of square units in the world, it is the number of cubic units. \nBy presenting both 2D and 3D algorithms for generating spatial decompositions, we are \noffering multiple options to move beyond traditional triangulation-based methods of \nproducing a navigation mesh and into advanced growth-based techniques. The most current \nimplementations of both of these algorithms can be found at \nhttp://gameintelligencegroup.org/projects/cgul/deaccon/, along with some other interesting \ntools and techniques for navigation mesh generation and evaluation. \n \nReferences \n[Hale08] Hale, D. H., G. M. Youngblood, and P. Dixit. ―Automatically-Generated Convex \nRegion Decomposition for Real-time Spatial Agent Navigation in Virtual Worlds.‖ Artificial \nIntelligence and Interactive Digital Entertainment (AIIDE). Stanford University, Stanford, \nCA. 2008. \n[Hale09a] Hale, D. H., and G. M. Youngblood. ―Full 3D Spatial Decomposition for the \nGeneration of Navigation Meshes.‖ Artificial Intelligence and Interactive Digital \nEntertainment (AIIDE). Stanford University, Stanford, CA. 2009. \n[Hale09b] Hale, D. H., and G. M. Youngblood. ―Dynamic Updating of Navigation Meshes in \nResponse to Changes in a GameWorld.‖ Florida Artificial Intelligence Research Society \n(FLAIRS). The Shores Resort and Spa, Daytona Beach, FL. 2009. \n[McAnils08] McAnils, C., and J. Stewart. ―Intrinsic Detail in Navigation Mesh Generation.‖ AI \nGame Programming Wisdom 4. Boston: Charles River Media, 2008. 95–112. \n[Touzor02] Tozour, P. ―Building a Near-Optimal Navigation Mesh.‖ AI Game Programming \nWisdom. Boston: Charles River Media, 2002. 171–185. \n[Tozour04] Tozour, P. ―Search Space Representations.‖ AI Game Programming Wisdom 2. \nBoston: Charles River Media, 2004. 85–102. \n[Schneider03] Schneider, P. and D. Eberly. ―Point in Polygon/Polyhedron.‖ Geometric Tools \nfor Computer Graphics. Morgan Kaufmann Publishers, 2003. 695–713. \n",
      "page_number": 222
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 232-239)",
      "start_page": 232,
      "end_page": 239,
      "detection_method": "topic_boundary",
      "content": " \n \n \n3.4. A Practical Spatial Architecture for Animal and Agent Navigation \nMichael Ramsey—Blue Fang Games, LLC. \nmiker@masterempire.com \n―Not so many years ago, the word ‗space‘ had a strictly geometrical meaning: \nthe idea evoked was simply that of an empty area.‖ \n—Henri Lefebvre \nGame literature is inundated with various techniques to facilitate navigation in an \nenvironment. However, many of them fail to take into account the primary unifying medium \nthat animals and agents use as locomotion in the real world. And that unifying medium is \nspace [Lefebvre97]. The architectonics[1] of space relative to an animal‘s or agent‘s motion \nin a game environment is the motivation for this gem. Traditional game development \nfocuses on modeling what is physically in the environment, so it may seem counterintuitive \nto model what is not there, but one of the primary reasons for modeling the empty space of \nan environment is that it is this spatial vacuum that frames our interactions (be they \nlocomotion or a simple idle animation) within that environment. Space is the associative \nsystem between objects in our environments. \n[1] A unifying structure is commonly referred to as an architectonic, as it is used to describe and \nassociate elements that are separated into a perceived whole. \nThis article will discuss this spatial paradigm and the techniques that we used during the \ndevelopment of a multi-platform game, entitled World of Zoo (WOZ). WOZ was a \nchallenging project not only by any standard definition of game development, but also \nbecause we desired our animals‘ motion to be credible. \nAn important aspect of any animal‘s believability is that they are not only aware of their \nsurroundings, but that they also move through a dynamic environment (Color Plates 1 and \n2 contain examples of WOZ‘s environment) in a spatially appropriate and consistent \nmanner. This maxim had to hold true whether the animal was locomoting over land, over \nwater, or even through air! To help facilitate the representation of our spatial environments, \nwe used several old tools in new ways, and in conjunction with a few inventions of our own, \nwe believe we accomplished our goals. \nFundamental Components of the Spatial Representation System \nThe primary element for constructing a spatial representation in WOZ was the sphere, \ntermed a navsphere. Figure 3.4.1 shows a subset of a navsphere layout from an exhibit. \nThe navsphere is fundamentally important because not only is it used to generate the \nnavigable representation of the world (see below), but more importantly, it defines the \ninteractable spatial dimensions of the world. What this means is that we define in 3D space \nwhere an animal can go, not just where it cannot. To keep animals from going places, we \nrely not only upon the tried-and-true techniques of collision detection [Bergen04], but also \non collision determination. \nFigure 3.4.1. Two navspheres in a level. Connectivity information between \nneighboring navspheres is accomplished by having a slight overlap. \n\n\n \n \n \n \nCollision determination is a technique of knowing ahead of time that a collision may occur \n(similar to how the majority of physics packages handle contact points). This determination \nof a potential collision is implicit when using navspheres, because we are able to determine \nthat an animal is nearing the edge of navigable representation. These edges are termed \nspatial boundaries, and in this specific example, it defines the implicit relationship between \nthe navspheres and the outer lying geometry [Gibson86]. Because of this knowledge, we \ncan augment the animal‘s behavior to slow down, start to turn away, or skid to a stop. \nIt should be noted that the navsphere is an approximation of space, and it is not an exact \nspatial inverse of the placed geometry. A spatial system that is modeled from constructive \nsolid geometry (CSG) principles would definitely be ideal, but the dynamic nature of WOZ‘s \nenvironments made this unfeasible. However, we did invest some initial time into \ninvestigating and utilizing various CSG techniques. The primary aspect of CSG that we \ndiscovered to be applicable for a spatially accurate representation of an environment was \nthe Boolean intersection operator, which is the overlap of two objects merged into a convex \ncomponent. The cumulative Boolean intersections would form the space through which our \nanimals would move. The complexity and cost would come from the determination of an \nagent‘s occupancy within that environment‘s CSG representation, as they are not primitives \nbut complex convex objects. This approach would definitely be more accurate than spheres, \nbut at the cost of not being usable on current-generation consoles or the typical desktop PC \n(mainly due to the arbitrary manner in which WOZ‘s environments were modeled). \nAs an animal moves through an environment, there need to be mechanisms in place to help \ncontrol an animal‘s interaction with navspheres—we do this by assigning properties to the \nnavsphere that define how certain interactions will occur. Some of these properties include \ndefining the type of animal locomotion allowed in that navshape (whether it be land, water, \nor air) and spatial parameters for certain animal sizes. \nHaving a system that is spatially centric requires a single enabling component that allows it \nto be easily accessed by other game systems. By allowing navspheres to overlap, we are \ncapable of generating a navigable representation—a navrep—of the environment. A similar \nsystem is the circle-based waypoint graph [Tozour03]. For a visual example of this process, \ncompare Figure 3.4.2 and Figure 3.4.3. Figure 3.4.2 shows the overlapping navspheres for \none of WOZ‘s nursery levels. Figure 3.4.3 shows the type of connectivity information \ngenerated from that navsphere layout. You can think of the navrep generated as the \nwalkable surface for the world; however, note that the navrep is in 3D space and can wrap \naround other geometry as well as other navreps. \nFigure 3.4.2. A navsphere layout for the bear nursery. \n\n\n \n \n \n \nFigure 3.4.3. This figure shows how the connectivity information is generated from \nthe overlapping navspheres in Figure 3.4.2. \n \n \nThe primary reason to construct a connectivity graph from the spatial representation is that \nwe need to execute potentially expensive operations on the game world, such as pathing \nqueries, reachability tests, and visibility queries. The basic algorithm for generating the \nnavrep is to iterate over the navspheres, searching for overlap with any other navspheres. \nIf we find any overlap, we establish a bidirectional link between the navspheres. Later on in \nthe development of WOZ, we also found that we could use the same mechanism for one-\nway links by embedding directed connectivity information in the navsphere itself; this \nmanifested itself in game objects such as one-way teleport doors. \n \nNavigation System Architecture \nAs we move on to discussing the spatial aspects of the WOZ navigation system, it will help \nto understand the basic structure and components of the system as a whole (see Figure \n3.4.4). The primary interface between the navigation system and the other components of \nthe WOZ game is the navigation manager. The navigation manager facilitates access to the \nplanner. The planner contains the navigable representation of the environment—both the \nspatial vacuum and the generated connectivity graph. The pathfinder uses the A* algorithm, \nwhich provides support for both spatial and routing biases [Hart68, Stout00]. \nFigure 3.4.4. The navigation system. \n\n\n \n \n \n \nAlso provided is a general utilities object that contains general navigation code. When an \nanimal (noted as an entity in Figure 3.4.4) needs to route through an environment, it will \nissue a call through the animal planner into the navigation manager. The navigation \nmanager will then access its own world planner, and using the navrep it generates a coarse \nroute through the environment. This coarse route is then returned to the animal‘s planner \nfor use. As you‘ll notice in Figure 3.4.4, WOZ has two planners: the navigation planner and \nthe animal planner. The animal planner handles any immediate spatial or interanimal tasks, \nwhile the navigation planner handles the more rudimentary routing operations (for example, \npath biasing) as well as handling the interactions with the navsphere reservation system. \n \nNavrep Continuity and Stitching \nNavreps are not necessarily continuous. What this means is that the level designers can \nauthor disparate navreps based upon differing navrep types (for example, land or water), as \nwell as navreps that represent differing levels of elevation in a zoo exhibit, such as the \nledges on a cliff face. Linking multiple, non-overlapping navreps in the navigation system \nrequires the creation of navfeelers. Figure 3.4.5 (left) contains an example of two navreps \nthat were authored as disconnected. There is a navrep on the ledge and also a navrep on \nthe base of the exhibit. The navfeeler is the fishing pole–like extrusion from the top \nnavsphere to one of the bottom navspheres. \nFigure 3.4.5. On the left is an example of a navfeeler authored in level to connect \ntwo disparate navreps with the final result, on the right, of a navigable navrep. \n\n\n \n \n \n \nNavfeelers allow the level authors to link these navreps together. It attempts to find any \nnavsphere below; if it finds a navsphere below itself, we then establish a bidirectional link \nbetween the two. The analogy that we used during development to help explain this was to \nthink of a fisherman at the end of pier, with his fishing pole sticking out over the water at \nroughly 45 degrees. His line would dangle into the water, which effectively links land and \nwater for the navigation system. If the navfeeler finds a navsphere below it, we then \nestablish a bidirectional link between the two navspheres. Once these navreps are linked \ntogether (refer to Figure 3.4.5, right), the navigation system can then pathfind over the \nmultiple navreps. Although this example is shown for a land-to-land navrep, the same \nmechanism is used for land-to-water bridges, which allows the penguins to dive into and \njump out of water. \n \nHandling Locomotion and Turning \nLocomotion in WOZ is executed by root accumulation of multiple animations that form a \nfinal pose. By root accumulation, we mean that the animators author with full displacement; \nany movement of the root of the animal‘s skeleton is contained in the animation. This allows \nthe animations to retain all the inherent tweaks, such as deceleration, that might otherwise \nbe lost in the typical engineer-centric approach, where the animators are required to author \nanimations on the spot. While root motion is very necessary for exhibiting animator-\nenvisaged motion, it also does not mesh well with traditional navigation paradigms. \nTurning was handled independently of locomotion, which was advantageous because it \nallowed the animators to avoid generating a host of different turn animations. To generate a \nturn angle, an animal selects a target point, such as the next point along a route or a game \nobject. This target point is then turned into a turn angle (by doing a dot product between \nthe target point and the heading of the animal and then solving for theta), which is then \nused to twist the spine of an animal in the desired direction. If we had an animal with a \nfour-bone spine and a turn angle of 40 degrees, we would simply apply 10 degrees of twist \nto each of the bones. In this section I‘ll talk about how we handled two central components \nof progression through our navigation system: locomotion and turning. \nWhile the navigation system plans through the world using the connectivity graph (which \nwas generated from the navspheres), we turn to the spatial representation of the world in \norder to facilitate locomotion and turning validation. Each animal has associated with itself \nan occupancy shape (see Figure 3.4.6). This occupancy shape is used to control progression \nthrough the suggested route. The occupancy shape is not static; it can grow in size, as well \n\n\n \n \nas placement, according to the current animation. A fast-galloping zebra will have its \noccupancy shape projected out in front, whereas a slow-moving zebra would have the \noccupancy shape centered more on itself. \nFigure 3.4.6. Each animal has an occupancy shape. This variable occupancy shape \nis used to denote the rough spatial representation relative to the navigation \nsystem. \n \n \nOne key differentiation in the navigation system implemented for WOZ versus other games \nis that during normal locomotion an animation could and generally would deviate from the \nproposed path through the environment. A combination of root motion, behavioral prodding, \nand turn speeds that vary based upon the state in the behavior graph makes following an \nexact route impossible without adversely affecting the quality of an animal‘s movement. \nThis is not dissimilar to how real animals or humans move through the world. Humans don‘t \nplan exact motions; we don‘t plan our exact muscle contractions—we move in accordance \nwith our understanding of the space made available to us. It‘s this space that allows us to \nidentify boundaries and make use of the relationships between the space and objects we are \nafforded [Gibson86]. So it made sense to ensure that WOZ‘s animals respect the spatial \nrelationships of the environment accordingly. \nTo help influence the turning of an animal, we implemented a system that uses a series of \nspheres projected around an animal in order to determine the suggested turn angles. We \naccomplished this by determining whether the projected spheres were inside the navrep. If \none of the projected spheres was completely outside the navrep, we would execute a turn in \nthe opposite direction. For example, if an animal‘s motion wanted to move it into a wall, we \ncould execute a tight turn in the opposite direction simply by altering the blend weights of \nthe current animation. The blend weights would effectively bias the animal away from \nobstructions. It is important to remember that we modeled the world not only \ngeometrically, but also its spatial representation, so this type of inside or outside test would \nbe possible. By projecting these spheres around the animal, we could make turns that \nwould take the animal away from objects according to their spatial proximity to the navrep \nboundaries. Another positive side effect of this approach is that we avoided doing costly and \npotentially numerous raycasts for this operation. \n \nConclusion \n\n\n \n \nWhile there are many navigable representations that can be used in a game, very few of \nthem concern themselves with the spatial vacuum that exists in between the static \ngeometry. This gem has shown you how to represent this space using several common \ntools, in conjunction with some new approaches to modeling locomotion. While modeling \nlocomotion based on a desired sense of progression through the environment is different \nthan most locomotion models, it also opens the door to modeling the actual motion of an \nanimal or agent as it occurs in the real world. Animals or agents don‘t follow exact routes; \nthey alter their movement according to an understanding of not only the static objects in \nthe world, but also the available space in which to exhibit their behaviors. \nHopefully, the presentation of a few old ideas intermixed with a few new ones will help you \nrethink an animal‘s or agent‘s interactions inside an environment. By considering the space \nin between physical aspects of an environment, we now have the capability to make \ndecisions about our environment that are not purely reactive by nature. We are afforded the \nmechanisms to forecast environmental interactions, which is perhaps one of the first steps \ntoward a credible motion management system. \n \nAcknowledgements \nI wish to thank the following team members for their contributive efforts to the WOZ AI \nsystem: Bruce Blumberg, Steve Gargolinksi, Ralph Hebb, and Natalia Murray. \n \nReferences \n[Bergen04] Bergen, Gino Van Den. Collision Detection in Interactive 3D Environments. \nMorgan Kaufmann, 2004. \n[Ericson05] Ericson, Christer. Real-Time Collision Detection. Morgan Kaufmann, 2005. \n[Gibson86] Gibson, James J. The Ecological Approach to Visual Perception. LEA, 1986. \n[Hart68] Hart, P. E., N. J. Nilsson, and B. Raphael. ―A Formal Basis for the Heuristic \nDetermination of Minimum Cost Paths.‖ IEEE Transactions on Systems Science and \nCybernetics 4.2 (1968): 100–107. \n[Lefebvre97] Lefebvre, Henri. The Production of Space. Blackwell Publishing, 1997. \n[Richenbach58] Reichenbach, Hans. The Philosophy of Space and Time. Dover, 1958. \n[Stout00] Stout, Bryan. ―The Basics of A* for Path Planning.‖ Game Programming Gems. \nBoston: Charles River Media, 2000. 254–263. \n[Tozour03] Tozour, P. ―Search Space Representations.‖ AI Game Programming Wisdom 2. \nBoston: Charles River Media, 2003. 85–102. \n[Week01] Week, Jeffrey. The Shaping of Space. Marcel Dekker, Inc., 2001. \n \n3.5. Applying Control Theory to Game AI and Physics \n\n\n \n \nBrian Pickrell \nbobthrollop@brandx.net \nControl theory is the engineering study of dynamic systems, such as airplanes and other \nmachines. The name is a bit misleading, since designing controls (in other words, airplane \nautopilots or missile guidance systems) is only one application of the theory. Control theory \nis actually the analysis of equations to extract some fundamental information about how \nentire classes of systems act in all possible circumstances. \nControl theory is something of a sister science to simulation. A simulation looks at a specific \nsituation and predicts what the system will do in great detail, but it doesn‘t explain how the \nsystem behaves in general. Control theory, on the other hand, doesn‘t predict anything at \nall, but it gives general information in the form of quantifiable measurements that are true \nfor any situation or inputs. In other words, it tells you what the system can and cannot do. \nThese measures are mathematical and quite abstract, but they are important, and for the \nmost part, they are things that simulation engines just do not provide. Some of the ideas \ndescribed in this gem may seem like shortcuts to avoid doing proper simulation, but in fact \nthey have just as much scientific validity. It is better to think of controls analysis as \ncomplementary to simulation, and as a way to do some things that your physics engine was \nnot designed for. \nOne task for which control theory is better suited is designing the steering and motion of \nphysics objects in games. To give a specific example, how would you depict the motion of \nan in-game car making a sudden turn? We all know that a simple, abrupt change of \ndirection is not realistic and doesn‘t look believable—the vehicle should swerve and sway a \nlittle bit while turning. If you just program a motion curve by guessing, the results will not \nbe much more believable. If you use a high-fidelity physics engine to simulate the turn, you \nwill likely have to work out the forces and other parameters required to make it sway \nconvincingly by trial and error, and the results of this tweaking cannot be reused for other \ncars in other turns. \nOur first formula describes the family of functions that you can use for this situation. An \nobject such as a moving vehicle must obey these functions in its motions. If your game \nobject doesn‘t move like this, it‘s not physically realistic. Conversely, you can create a \nplausible motion curve at very little cost in physics analysis by following this formula and \nusing some common-sense rules of thumb. \nHere is that result: Any unforced dynamic system moving under a set of linear differential \nequations has an output motion of the form: \nEquation 1  \n \n \nThis is a set of harmonic oscillators, each of the form: \nEquation 2  \n \n",
      "page_number": 232
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 240-247)",
      "start_page": 240,
      "end_page": 247,
      "detection_method": "topic_boundary",
      "content": " \n \n \nThis is a sinusoidal wave where A is the amplitude and ω (omega) is the frequency, and the \noscillations die out (or grow) exponentially according to a damping coefficient ζ (zeta). A \nsingle harmonic oscillator looks something like Figure 3.5.1. \nFigure 3.5.1. Simple damped harmonic oscillator. \n \n \nSeveral harmonic modes added together, each with its own amplitude, frequency, and \ndamping coefficient, as in Equation (1), look like Figure 3.5.2. \nFigure 3.5.2. Oscillator with two harmonic modes. \n \n \nWe will spend most of the rest of this gem defining the concepts behind Equation(1) and \nshowing how they were derived. \nDynamic Systems \n\n\n \n \nThe concept of a dynamic system in control theory is the same as that used in simulation; \nthat is, a system is any set of parts that act on each other in some quantifiable way and \nchange over time. The underlying mathematics applies equally to all different sorts of \nsystems, using any units of measurement that are appropriate and any scientific laws that \nproduce linear equations. The ―parts‖ themselves may be conceptual rather than physical, \nas when measuring the concentrations of different reagents in a chemical reaction. The \n―output‖ can be any measurable quantity at some point in the system, measured in \nwhatever units are appropriate. In this gem, we will mostly refer to objects moving under \nthe physical laws of mass, force, and so on, but with the understanding that other dynamic \nsystems work the same way. \nWe did not say what the units of Equation (1) are or how you should implement the result. \nThis is up to the game programmer to decide. Your output y(t) can be distance \nmeasurements (xyz coordinates) or velocities or angular units, such as steering heading, as \nlong as your system is linear in the units you choose. This explains how an airplane flying in \na circle (which doesn‘t look like Equation (1) at all) is consistent with linear control theory: \nbecause in angular units, its heading changes at a constant rate, which does fit the \ntemplate of Equation (1). This should be good news to programmers of cockpit view–style \ngames. Everything we tell you here can be implemented directly in angular units, without \nrequiring messy polar coordinate conversions. \nLinear Systems \nWe have already stated the requirement that a system be linear. What does this mean? A \nlinear dynamic system is one whose defining equations of motion follow the form: \nEquation 3  \n \n \nwhere all of the coefficients An are constants. The reason that we could say with such \ncertainty that all linear systems follow the form of Equation (1) is that the result is a \nnecessary mathematical consequence of Equation (3). If you don‘t make that assumption, \nthen the results don‘t look like those sinusoidal functions. Engineers talk all the time about \nnon-linearities in their systems; this usually means that the coefficients A in the equations \nof motion are not constant. They are functions of some other value, or they change over \ntime. (It can also mean that something else is being done with the derivatives—for \nexample, \nis not linear.) When engineers try to apply linear control theory to systems \nwith non-linearities (and they always do; a good part of controls engineering consists of \nfinding ways to approximate non-linear reality with linear equations), the results often look \nsimilar to Equation (1) but the ω, ζ, and A values of the different modes keep changing, or \nthe poles and frequency responses in the charts we‘ll soon see keep moving around. \nAll that said, it is safe in an in-game world to define all physical responses as being always \nlinear. Then we can go ahead and use our linear results without worry. \nFeedback, Damping, and Stability \nSystems oscillate the way they do because of internal feedback among components. \nSometimes unexpected feedback effects can cause a system to oscillate more rather than \nless; in fact, the 19th-century origins of control theory lie in explanations of why a steam \nengine‘s mechanical governor couldn‘t keep it running at a steady speed. One of the \n\n\n \n \nprimary questions that controls engineers analyze is whether the system is stable. If any of \nthe damping coefficients ζ in Equation (1) is negative, then the entire system is unstable (in \nother words, the equation does not converge as t → ∞). Notice that in such a case, the \nexponential part of the equation is positive, so the sinusoidal oscillations expand \ncorrespondingly. Fortunately, instability is never a surprise in the game world, since we \nhave the luxury of declaring positive ζ ‘s for all of our systems. However, you will see this \neffect in physics simulations that drift out of limits and in game object movements that go \nwild because of player overcontrolling. \nMost programmers know a little bit about feedback and use it occasionally but do not have a \nway to quantify the system stability or instability that results. Here are some different types \nof damping: \nζ > 2 ω \nOverdamped (stable, no oscillations) \nζ = 2 ω \nCritical damping (stable) \n0 < ζ < 2 ω Underdamped (stable) \nζ = 0 \nUndamped (oscillates) \nζ < 0 \nUnstable \n \nSecond-Order Linear Differential Equations \nLet‘s look at how Equation (2) is derived for a couple of simple harmonic oscillators. \nEquation (2) is the general solution of the second-order linear differential equation (LDE); \nthat is, one where the differential terms go as high as the second derivative \nbut no \nhigher. Each of these examples, then, can form a single component of a more complicated \nsystem that has multiple oscillatory modes. \nBoth of these examples are basic textbook cases. You can see from them how the same \ncontrol theory applies in so many different scientific fields; it is because many natural laws \n(in these cases, Coulomb‘s law and Newton‘s second law) express themselves as similar \nfirst- and second-order differential equations. \nExample 1: Mass and Spring \nFigure 3.5.3 shows a heavy object M bouncing up and down on a spring K, obeying \nNewton‘s laws of motion. The ―output‖ value being measured is x2, the position of the object \nmeasured in units of distance. We‘ll pretend that all of the friction in the system occurs at \ndashpot B. (A dashpot is the working part of a shock absorber.) The equations show that \nthe weight moves under the forces of the spring and friction. The spring force changes as \nthe object moves, a built-in feedback. The friction force depends on velocity (which is the \nderivative of the position). Newton‘s second law converts force to acceleration, which is the \nsecond derivative of position. Outside forces pushing on the system are represented by \nx1(t). (We‘ll get to that later; for now, assume it is zero.) \nFigure 3.5.3. Mass and spring. \n\n\n \n \n \n \nExample 2: RCL Circuit \nFigure 3.5.4 shows a very simple electronic circuit with a resistor R, a capacitor C, an \ninductor L, and a voltage input E. The inductor tends to keep current i flowing once started, \nwhile the capacitor builds up a charge that fights against the current. The interaction of \nthese two causes the current (and voltage) in the circuit to oscillate back and forth. Either \nvoltage or current can be read as the output value; in this case, we‘re using current. \nFigure 3.5.4. RCL circuit. \n \nSome Math Formulae \nWe are almost ready to give the solution to the general second-order LDE, but let‘s review \ntwo fundamental mathematical formulae we will use. \nEuler’s Formula \nEquation 4  \n \n \nThis is a fundamental identity from complex mathematics that relates exponentials to \ntrigonometric functions. It says that e to any power that is a pure imaginary number lies on \nthe unit circle in the complex plane, and that for an exponential with a complex exponent, \nthe imaginary part of the result oscillates in a sine-like way while the real part either grows \n\n\n \n \nor diminishes exponentially (depending on whether the real part is positive or negative), as \nshown in Figure 3.5.5. \nFigure 3.5.5. Roots and exponentials in the complex plane. \n \n \nFundamental Theorem of Algebra \nA real polynomial of degree n: \nEquation 5  \n \n \nhas n roots (including complex and multiple roots). Complex roots always come in \nconjugate pairs a+bi, a-bi. \nBetween them, these two formulae mean that, in the complex number domain, sine and \nexponential functions are the same thing! \nHarmonic Oscillator—Derivation \nNow for the solution to the equation resulting from Examples 1 and 2: \nFind all equations y(t) that obey the differential relation \nEquation 6  \n \n \nThe derivation of the result is interesting because it shows how imaginary numbers keep \ncoming up in control theory. Solving a differential equation, solving an exponential \nequation, and finding roots of a polynomial are tantamount to the same thing. Also, we‘ll \nsee how a complex exponential can be separated into two parts: an exponential (ζ) and a \n\n\n \n \nsinusoidal component (ω). Control engineers tend to talk lightly of complex ―roots‖ as \ndamping ratios and frequencies, an abstraction that can be weird and confusing for \noutsiders. This is where that equivalency comes from. \nThe solution starts by assuming that all solutions are variations on the basic form y = ert \nwhere r is a complex number. \nEquation 7  \n \n \nThe general solution, then, is: \nEquation 8  \n \n \nGeneral solution of second-order LDE (unforced)[1] \n[1] It would be a fair question to ask what the meaning is of the imaginary part of the equation, since \nyou can‘t have imaginary distances or voltages. An (evasive) answer is that the imaginary part is \nrequired to be zero as a constraint condition in an analysis, which we‘re skipping over. The reader may \njust ignore the imaginary part and read only the real part of the equation. \nwhere ζ and ω can be derived directly from the coefficients A, B, and C once you know the \nsolution. c1 and c2 are arbitrary constants—any values are valid, and the values for a \nparticular case depend on the initial conditions. You can find the system‘s frequency and \ndamping ratio directly from the system constants (such as inductance and capacitance) \nwithout going back through the differential equation. Here is another formulation that is \nhandy to have: \nEquation 9  \n\n\n \n \n \n \nBlock Diagrams \nWhat about a system that has more parts than our examples? When an analysis extends to \nmultiple equations of motion, the interrelationships quickly become much harder to sort out. \nA block diagram analysis is one way to manage this. Making a suitable block diagram to \nrepresent a complex system is a matter of engineering judgment, much like designing a \nsystem model for a simulation. We do not want to explain all the ins and outs of block \ndiagramming, but a block diagram can help explain how control inputs apply to the \nequations we‘ve done so far. \nWe said that Equations (1) and (2) applied to unforced systems—that is, ones with no \ninputs. In the case of modeling a car‘s steering, that would mean that there was no one at \nthe wheel. What use is a model like that? One answer is that we can include the steering \ninside the block being modeled. \nEquations (1) and (2) are what are called transfer functions. In a block diagram, you can \nrepresent an entire subsystem as a single box with an input and an output, and the transfer \nfunction is the conversion between input and output. Figure 3.5.6 shows a diagram that \ndemonstrates ―closing the loop‖—that is, converting an open-loop (uncontrolled) equation \ninto one that accounts for the effects of steering or other control inputs. \nFigure 3.5.6. Closed-loop block diagram. \n \n \nG(s) is our original transfer function. R(s) is an external input (steering), and G(s) converts \nthe input to the final output value C(s). H(s) is a feedback control system that reads the \noutput, transforms it in a certain way, and uses it to modify the command input. \nThe following is important if you want to model autonomously steered (NPC) vehicles in a \ngame: H(s), which represents the internal dynamics of the steering system, is modeled as a \nlinear system of components exactly like the main system G(s). H(s) could be a collection of \nlevers, springs, hydraulics, electronics, and so forth—it is just another linear system and is \nconceptually no different than G(s). Furthermore, the entire diagram that results, including \ncontrols, is also linear and can be redrawn as a single ―black box‖ transfer function. \nTherefore, you can legitimately represent a vehicle and its driver as a single, complicated \ntransfer function where the input is the desired course and the output is the actual motions \nof the vehicle. This does not work for a real person but can represent any linear controls, \n\n\n \n \nincluding a simple autopilot, a smart or dumb robot driver, or a human non-player character \n(NPC) whose driving style is a linear function. \nThe caveat to this is that you would be simulating the steering behavior of a control system \nwithout actually doing the steering. Only in extremely simple cases is it possible to extract \nthe implied control responses from a system transfer function. The demo on the CD shows \nan example of this. \nLaplace Transform \nThe block diagram of Figure 3.5.6 replaces differential equations with Laplace transforms. \nFor our examples, G(s) is the Laplacian L of Equation (6), which is As2 + Bs + C. The \nLaplace transform is a clever mathematical trick that converts differential equations to more \nmanageable polynomial functions. \nEquation 10  \n \n \nDefinition of Laplace Transform[2] \n[2] By convention, a lowercase letter denotes an original function and a capital letter denotes a Laplace \ntransform with the parameter s. It is important to notice which is which throughout this article. \nLaplace transforms are written as capital letters in the block diagrams, and the original \nfunction parameter—time t—becomes s, which is not a measurable value but an abstract \ncomplex number. Laplace transforms have several useful properties. First of all, the \ntransforms of the most common functions are all polynomials. Also, they are linear and can \nbe added and multiplied by constants. They support integration and differentiation; they \nconvert step inputs and other discontinuous functions into continuous functions. And, \nfunction composition is the same as multiplying in the Laplacian domain—in other words: \nEquation 11  \n \n \nYet another handy property of the Laplacian is that it is units-independent. Since all units of \nmeasurement transform to the same s, it is possible to mix transfer functions whose \noutputs are in different units in the same block diagram. Using the properties of the Laplace \ntransform, Figure 3.5.6 leads to the following relationships: \nEquation 12  \n",
      "page_number": 240
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 248-260)",
      "start_page": 248,
      "end_page": 260,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \nThe denominator of the last function is called the characteristic equation[3] of the system. \n[3] This is the same as the characteristic equations found in linear algebra, if you represent the \nunderlying system of differential equations as a matrix. \nEquation 13  \n \n \nFrom Characteristic Equation to Equations of Motion \nThe next step is simple but powerful. The Laplace transform implicitly does the same \nconversion to exponentials that was done in the derivation of Equation (8). Therefore, the \nroots of the characteristic equation of the system correspond to the n harmonic modes in \nEquation (1), with –ζ as the real part and ω as the imaginary part[4]. If you can factor a \nsystem‘s characteristic equation into the form: \n[4] Note the – sign in front of ζ. A positive real part of a root means a negative damping coefficient. \nEquation 14  \n \n \nthen any function matching Equation (1) with any constants An is a possible motion of this \nsystem.[5] \n[5] Phase offsets of the various modes, which we have neglected in this gem, are also allowable and in \nfact are very important in matching Equation (1) to real motion curves. \nIf you‘re not allowed to make up an answer, characteristic equations are hard to solve! The \ntransfer function from our examples was easy, but in general both G(s) and H (s) may be \npolynomial fractions (one polynomial divided by another) that require numerical methods to \nfactor. But for game simulations, you may invent a characteristic equation in already-\nfactored form! If you do so, you are implicitly stating that the physics of the system are \nunknown, the internal logic of the controls is unknown, the roots were found in an unknown \nway, but the overall response of the entire system including controls is just what the \ndesigner specified. \nRoot-Locus Plots \n\n\n \n \nRoot-locus plots are a bit of a digression, but they give a measure of insight into how one \nwould choose the invented roots mentioned earlier. Aeronautical engineers found that in \nmany cases, control systems included an electronic amplifier or the equivalent, and the \namplification, or gain, came out as a constant multiplier in H (s) in Figure 3.5.6 and that \nfurthermore, the stability or instability of the entire system depended in hard-to-anticipate \nways on what gain (K) was chosen. The original root-locus method[6] used some extremely \nclever pencil-and-ruler methods to replace the computation-intensive factoring of Equation \n(13) into Equation (14); the plots are still useful even though we‘re doing them by \ncomputer now[7]. A root-locus plot is a plot of all possible roots of the characteristic equation \nfor values of K from zero to ∞. Here are some examples: \n[6] The root-locus method is also known as the Evans method. It was developed in 1948 by Walter R. \nEvans, a graduate student at UCLA. \n[7] For instance, Matlab‘s rlocus() function, or [El-Dawy03], a freeware root-locus plotting program. \nIn a root-locus plot, circles represent roots of the numerator of the characteristic equation, \ncalled zeroes. X‘s represent roots of the denominator, called poles because the function is \ninfinite at the poles. The actual roots begin at the poles when gain K=0 and move toward \nthe zeroes as K increases. The graph is drawn in complex s-space, where the real (X-axis) \ncomponent is damping coefficient – ζ and the imaginary (vertical axis) component is \nfrequency ω. Again, each root corresponds to one of the modes of Equation (1), so you can \ndraw an output function directly if you know what the roots are. \nAs mentioned, if any mode of a system has a negative damping ratio, the entire system is \nunstable. In a root-locus plot, this means that if any root lies on the right of the Y-axis, the \nsystem is unstable. This happens in Figure 3.5.7(c) —the system is stable at low gain, but if \nyou turn up the gain past a certain point, it becomes unstable. \nFigure 3.5.7. Root loci of characteristic equations. \n\n\n \n \n \nIn this example, notice that (b) and (c) are modifications of (a) made by adding a zero and \na pole, respectively, to the characteristic equation. This represents adding components \n(such as resistors and capacitors) to the control system. The shape of the plot and the \nstability of the system changed dramatically each time. Controls engineers make it their \nbusiness to know what to add to a circuit to shape the root locus the way they want. \nFortunately for the game designer, we can usually just place roots where we like and \npretend that a root-locus analysis has been done. A root close to the vertical axis represents \nunderdamping and an oscillation that takes a long time to die out; roots far to the left \nrepresent great stability and rapid response. \nFrequency Analysis[8] \n[8] See [D‘Azzo60], chapters 8–1010, and [Thaler60], §4.8, 7.5. \nThis gem has completely skipped over a branch of control theory that is actually of equal \nimportance. This is frequency analysis. It is based on the idea that a system‘s behavior can \nbe broken down into its responses to sine-wave inputs at different frequencies. \nIf the input to a linear system is a pure sinusoidal function—in other words, sin(ϕt), where ϕ \nis any frequency (not necessarily ω)—the output will always be a sinusoidal wave at the \nsame frequency ϕ, but with a different amplitude A. It will also be delayed from the input \n\n\n \n \nwave by a time offset that is expressed as a phase angle θ. A and θ are different for every \ninput frequency ϕ, and a graph of frequency response A(ϕ) and phase delay θ(ϕ) is a \nfrequency analysis. The best-known plot types are the Bode plot and the Nyquist plot[9]. \n[9] Developed by Hendrik Bode and Harry Nyquist, respectively, who were both engineers at Bell \nLaboratories in the 1930s. \nAny input function can be broken down into a sum of a (possibly infinite) set of sinusoidal \nfunctions and can usually be approximated by just a few. (This is the basis of Fourier \ntheory.) Therefore, a frequency analysis can be used to break down a system‘s motions into \na few harmonic modes based on A and θ rather than ζ and ω. The demo on the book‘s CD \nhas chosen not to follow this route, for simplicity. \nCode Example: Control Law Racers \nThe demo program on the book‘s CD shows one way to apply the ideas behind a root-locus \nanalysis in a game. Rather than modeling a real system and then analyzing it, the demo \ngoes the other way by asking what behavior a system ought to have and then displaying it \nwithout actually doing a model. \nThe demo shows several slot cars moving down a track. The simulation uses a side-scrolling \nformat, so there is just one degree of freedom: y position. When the player tells them to \nchange lanes, the cars swerve abruptly and then maneuver into the new lane. Their steering \nis a bit weak compared to the weight of the cars, so they weave back and forth before \nfinding their target. What is interesting is that every car is a little bit different and steers \ndifferently. The steering behavior of the cars is encapsulated in the mode structure, which \ncontains the frequencies and damping rates of their motions. \nstruct mode { \n  float zeta;  // damping coefficient \n  float omega; // frequency \n  float A;     // Initial amplitude of wave \n}; \n \nEach mode supplies one of the terms in Equation (1). Each car can have any number of \nmodes. There is no steering or simulation loop in this demo; the control law models both \nthe physics of the car itself and the actions of its driver. Since a car with only one mode is \nno more complex than the weight and spring of Example 1, this may be a simple autopilot \nindeed. \nTo design your car‘s control law, add modes. Imagine a root-locus plot like Figure 3.5.7. But \ninstead of plotting a real function, just draw one or more X‘s for the roots. Think about how \nwell you want your car to handle and how quickly you want it to sway. A real car weaving \nback and forth on the road may move with a period of one to five seconds or so; that is, an \nomega of 0.2–1.0. As for damping, if you want your car to keep shimmying for a long time, \nthen give it a zeta close to 0; if you want it to handle well and accurately, then give it a \nlarge zeta. A critical damping ratio of zeta = 2 * omega will produce fastest settling. \nThe in-code comments explain how to add modes to your car. The step after that, giving an \nactual steering command by setting the A for each mode, is handled arbitrarily by the \ndemo. The next step of interest is the navigation step. This is where the values in the car‘s \nmodes are applied to Equation (1). \nvector<driver>::iterator drivIt; \n \n// <for-each> car \n\n\n \n \nfor( drivIt = driverList.begin(); \n     drivIt != driverList.end(); \n     drivIt++ ) \n{ \n     // Sum of all the oscillatory modes \n \n     // measuring distance from target lane implicitly commands \nthe \n     // car to go there when all oscillations have settled \n     drivIt->driverPos.y = targetLane + Y_BASE; \n \n     modeList::iterator theModeIter; \n     // <for-each> mode \n     for( theModeIter = drivIt->modes.begin(); \n          theModeIter != drivIt->modes.end(); \n          theModeIter++ ) \n     { \n         // Add this mode’s contribution to position \n         float zeta = theModeIter->zeta; \n         float omega = theModeIter->omega; \n         float A = theModeIter->A; \n         // Time conversion from dimensionless units (radians \nand \n         // rad/sec) to full cycles per second \n         float t = 2.0 * PI * resetTime; \n         drivIt->driverPos.y += A * exp(-zeta * t) * cos(omega * \nt); \n     } \n} \n \nThe last statement computes the output position driverPos.y directly, without going \nthrough a simulation step. \nYou will have to go into source code to change the cars. Try adding more cars and changing \nmodes. See what happens when a car‘s modes include both large and small damping \ncoefficients. \nConclusion \nThe control theory presented in this gem is the so-called classical control theory, based on \nlinear systems, that was current practice in the aerospace and electronics industries from \nroughly the 1930s through the 1960s and has been eclipsed by the rise of computers but \nhas certainly not gone away. It‘s a bit surprising that such a well-established body of \nknowledge is so forgotten in the game industry. This gem is meant to introduce \nprogrammers and designers to the concepts of control theory as well as to present one way \nto simulate and direct physical objects. Other applications of these ideas provide a virtually \nunexplored field for the industry. \n \nReferences \n\n\n \n \n[D‘Azzo60] D‘Azzo, John J., and Constantine H. Houpis. Feedback Control System Analysis \nand Synthesis. McGraw-Hill Book Company, Inc., 1960. \n[El-Dawy03] El-Dawy, Ahmed Saad. ―RootLocus.‖ n.d. Geocities. n.d. \n<http://www.geocities.com/aseldawy/root_locus.html>. \n[Graham61] Graham, Dunstan, and Duane McRuer. Analysis of Nonlinear Control Systems. \nJohn Wiley & Sons, Inc., 1960. \n[Thaler60] Thaler, George J., and Robert G. Brown. Analysis and Design of Feedback Control \nSystems. McGraw-Hill Book Company, Inc., 1960. \n \n3.6. Adaptive Tactic Selection in First-Person Shooter (FPS) Games \nThomas Hartley, Institute of Gaming and Animation (IGA), University of \nWolverhampton \ntom.hartley5@googlemail.com \nand \nQuasim Mehdi, Institute of Gaming and Animation (IGA), University of \nWolverhampton \nOne of the key capabilities of human game players that is not typically employed by non-\nplayer characters (NPCs) in commercial first-person shooter (FPS) games is in-game \nlearning and adaptation. The ability of a human player to adapt to opponents‘ tactics is an \nimportant skill and one that separates an expert game player from a novice. NPCs that \nincorporate in-game learning and adaptation are more responsive to human players‘ actions \nand are therefore more capable opponents. This gem presents a practical in-game approach \nto adapting an NPC‘s selection of combat tactics. \nA Dynamic Approach to Adaptive Tactic Selection \nTo achieve successful in-game (that is, run-time) tactic selection in FPS games, we \nproposed to adapt the online adaptation algorithm dynamic scripting [Spronck06, \nSpronck05]. The reinforcement learning–inspired dynamic scripting algorithm has been \ndesigned specifically for use in online learning scenarios and has previously shown \nsignificant promise in other genres of scripted games, such as role playing and strategy \ngames. Consequently, the approach offers an interesting augmentation to the traditionally \nscripted tactic selection techniques used in commercial FPS games. However, the approach \ndoes need to be adapted to make it suitable for use in an FPS environment. \nThe FPS version of the dynamic scripting algorithm presented in this gem has been adapted \nto the selection of tactics rather than rules for scripts, so it has a number of differences \nfrom previously implemented versions. First, the library of tactics in the developed system \nis organized in groups according to a dual-layered state representation. Previous \nimplementations organize the dynamic scripting rulebases according to NPC type and high-\nlevel game states [Ponsen and Spronck04]. However this limited state representation is not \nsuited to FPS games, as the selection of player-versus-player tactics greatly depends on the \ncurrent state of the NPC [Thurau06]. The second contribution is the development of fitness \nfunctions to evaluate the success of a tactic and the encounter with an opponent. The third \ncontribution is the development of a tactic selection process that makes use of a prioritized \nlist approach in combination with the K-Nearest Neighbor algorithm (K-NN) [Mitchell97]. \n\n\n \n \nThe goal of this approach is to organize the tactics so that the most successful ones are \nmore likely to be selected first. \n \nOverview of the Adaptive Tactic Selection Architecture \nTactics in the system are organized into tactic libraries, which in turn are organized \naccording to a dual-layered representation of the game environment. As illustrated in Figure \n3.6.1, the upper layer determines which tactic library should be selected according to an \nabstract state representation and/or rules, which are typically described in terms of a \nbehavior type or goal. For example, an ―engage enemy‖ library of tactics could be defined \nby health above 50 percent and the possession of a high-powered weapon. This process can \nbe performed manually using a game developer‘s domain knowledge and traditional game \ndevelopment techniques, such as finite state machines (FSM), decision trees, or rules. For \nexample, the abstract state space in Figure 3.6.2 is based on the hierarchical \nFSM/prioritized list approach used to control NPC behavior in Halo 2 [Isla05]. The highest \npriority state (arranged right to left) that matches the current game state is selected. This \napproach should allow the tactic selection architecture to be easily integrated with existing \ngame AI techniques. \nFigure 3.6.1. High-level overview of the tactic selection architecture. \n \n \nAs illustrated in Figure 3.6.2, the lower state space contains instances of a library‘s tactics \nat points in an n-dimensional feature space. The feature space and the number of tactic \n\n\n \n \ninstances within a tactic library are kept relatively small in order to maintain performance. \nOnce the upper layer has determined the current tactic library, the K closest library \ninstance(s) to the current game state are selected and used to determine the NPC‘s current \nlist of tactics. The following procedure outlines how an instance of the lower state space is \nselected. If the closest library instance to the query state (in other words, the NPC‘s current \nstate) is equal or below a predefined threshold, it is used to determine a prioritized tactic \nlist. If the closest library instance is above a predefined query threshold, the K closest tactic \ninstances to the current environment state are used for tactic selection. This multi-layered \napproach to selection allows tactics to be associated with detailed game states, while also \nbeing efficient in retrieving from the library. \nFigure 3.6.2. Overview of the tactic selection process and the state organization. \nThe K closest library instances are selected. The weights of each tactic are combined using \ndistance weighting and are used to determine a prioritized list. \n \n \n\n\n \n \nAll tactics have an associated weight that influences the order in which they are selected in \na given state. The weights reflect the success or failure of a tactic during previous \nencounters with an opponent. Learning only occurs between the tactics in each library \ninstance, and one library does not affect the learning in another, even if the same tactic is \nfound in both libraries. Tactic selection is achieved through a prioritized list [Isla05], which \nis adapted to support online list generation. \nFirst, the procedure outlined above is used to determine the K library instance(s) closest to \nthe current game state. If K > 1 and the closest tactic instance is above a query threshold, \nthe tactic‘s weights are combined using the distance-weighted K-NN algorithm. The \nindividual or combined tactic library instance is used to create the prioritized list. The K-NN \nalgorithm classifies an unlabeled instance (for example, s in Figure 3.6.2) by comparing its \nn-dimensional feature vector to stored instances. The K-Nearest Neighbors to the query \ninstance are found and used to determine a combined weight for a tactic. This is achieved \nthrough a weighted average that reflects a stored instance‘s distance to the query instance. \nThe position of a tactic within the prioritized list is determined by ordering tactics according \nto their weight. (For example, the tactics with the largest weights have the highest priority \nof selection.) A new prioritized list of tactics is generated whenever an upper layer state \nchange occurs or when the library instance changes. If a distance-weighted prioritized list \nhas been created, a predefined distance threshold is used to determine whether a new list \nshould be generated. The distance threshold is based on the distance from the current list‘s \nquery state to the NPC‘s present world state. \nOnce a list of tactics is generated, the tactic that has the highest priority and is capable of \nrunning is selected as the current behavior. If higher-priority tactics become available, they \ncan interrupt the current tactic on the next game loop. To avoid repetitive behavior, the \ntactic selection process can also include a check to prevent the same variant of tactic from \nfollowing each other. (For example, ―strafe a short distance‖ would not follow ―strafe a long \ndistance.‖) Therefore, if the next tactic is a similar variant to the current tactic, it is skipped, \nand the subsequent tactic is evaluated. This rule doesn‘t apply to tactic interrupts, which \noccur regardless of tactic variation. \nWhen a tactic is complete, is no longer capable of running, or a fixed amount of time has \nelapsed, the next tactic in the list that is capable of running is selected. When a tactic is \ninterrupted, is completed, or times out; a library change occurs; or a library instance \nchange occurs, the tactic has its weight updated according to a weight-update and fitness \nfunction (outlined in the next section). If the query instance was above a distance \nthreshold, the weight update is applied to the tactics in each K close library instance \naccording to the instance‘s distance from the initial query point. \n \nFitness and Weight-Update Functions \nThe weight-update function alters the weights associated with tactics in response to a \nreinforcement signal that is supplied by tactic and encounter fitness functions. The aim of \nthe fitness function is to reward tactics that defeated an opponent or improved the NPC‘s \nchance of defeating the opponent. The fitness functions for the tactic and the encounter are \ndefined below. Each function returns a value in the range of [–1,1]. \nEquation 1  \n \n \n\n\n \n \nThe equation above evaluates the fitness of a tactic and contains two components. In the \nequation, f refers to the fitness of the tactic t that is being evaluated. The first component \nA(t) ∊ [–1,1] determines the difference in life caused by the tactic [Andrade04]. That is the \nlife lost from the opponent minus the life lost from the NPC. The difference in life lost \nrepresents a key metric in measuring the performance of a tactic, as removing all an \nopponent‘s life is the ultimate goal of FPS combat. \nThe second component of the fitness function S(t) ∊ [–1,1] evaluates the surprise of the \ntactic. A surprise is the anticipation of an experience that the actual experience does not \nfulfil [Saunders02]. In the tactic selection architecture, the experience is the average value \nof the difference in life lost from previous encounters, and the actual experience is the \ncurrent difference in life lost. \nEach component of the tactic fitness function is weighted according to its contribution as in \nthe standard dynamic scripting approach [Spronck05]. The selection of the fitness function‘s \ncontribution is based on game goals. (For example, an NPC places high value on its health.) \nThe two components of the tactic fitness function are determined using Equations (2) and \n(3). The function hl (NPC) in Equations (2) and (3) refers to the health (in other words, life) \nlost from the adapting NPC, and hl(opp) refers to the health lost from an opponent (in other \nwords, the damage caused by the adapting NPC). AvghLifeLostn in Equation (3) contains the \naverage difference in life caused by the tactic from the previous n weight updates. \nEquation 2  \n \n \nEquation 3  \n \n \nEquation (4) evaluates the fitness of an encounter, that is, the tactics used by the adapting \nNPC during a combat to reach a winning or losing state. When an NPC reaches a terminal \nstate, the tactics used during the encounter are updated according to whether the NPC won \nor lost. In Equation 4, f refers to the fitness of the encounter e that is being evaluated. \nhlt(NPC) refers to the health (in other words, life) lost from the adapting NPC during the \nperformance of tactic t. hlt(opp) refers to the health lost from an opponent during the \nperformance of tactic t. \nEquation 4  \n \n \nEquations (2), (3), and (4) assume that the life lost or damage is in the range of 0 to 100. \nEquation (3) divides surprise by 200 so that the returned value will be in the range of –1 to \n1. Equation (4) divides health lost by 125 in order to give the tactic a guaranteed reward or \npunishment depending on whether the encounter was winning or losing. The scale of the \nreward or punishment can be set according to game requirements. \n\n\n \n \nThe weight update function uses the tactic and encounter fitness functions to generate a \nweight adaptation for the library tactics (i.e. an amount to adjust the current weight of a \ntactic). The function is based on the standard dynamic scripting technique, where weights \nare bound by a range [Wmin, Wmax] and surplus weights (that is, weight adjustments that \nresult in a remainder outside the minimum and maximum range) are shared between all \nother weights [Spronck05]. However the fitness functions outlined in Equations (1) and (4) \nreturn a value between [–1, 1], where a negative fitness value indicates a losing tactic, \nwhile a positive fitness value indicates a winning tactic. Therefore a simplified weight update \nfunction is used in this approach. In Equation (5), Pmax is the maximum penalty, Rmax is the \nmaximum reward, F is the fitness of the tactic, and b is set to 0 and is the break-even point \nfor the tactic‘s fitness. Above this point, the tactic‘s fitness is considered to be winning, \nwhile below this point the tactic is considered to be losing. In this simplified weight update \nfunction, the break-even point is built into the function itself. A fitness of 0 means the \nweights are unchanged. \nEquation 5  \n \n \nWhen multiple tactic library instances are used to create a combined prioritized list, the \nweight update is shared between the K instances according to their distance to the initial \nquery state (in other words, the point that resulted in the library instances being retrieved). \nThe sharing of the weight update is performed using Equation (6), where Δ wi is the weight \nupdate for a library instance, di is the distance to the library instance from the query state, \nand d is the total weight. \nEquation 6  \n \n \n \n \nAdapting Tactic Selection \nTo illustrate the tactic selection architecture, we describe the example of an NPC learning to \nadapt its selection of combat tactics. In this example, the adapting NPC is endowed with an \n―Attack‖ library of tactics that could include melee attack, charge attack, strafe charge \nattack, circle strafe attack, and attack from cover. Tactics are manually coded and comprise \nmultiple lines of logic that define the behavior to be performed (for example, movement and \nweapon handling code). In addition to behavior logic, there should also be code to \ndetermine whether a tactic is capable of running. For example, the circle strafe tactic may \nnot be capable of running in a narrow corridor. \nFor clarity, only one tactic library is used in this example. The upper state space controls \nthe selection of tactic libraries using conventional techniques, and adaptation occurs \n\n\n \n \nindependently within each tactic library instance; therefore, additional tactic libraries can be \neasily added. After a library has been defined, the default weight of the tactics needs to be \nset. Tactics can be given the same initial weight, or a weight can be selected based on \ndomain knowledge. In this example, all the tactic weights are initially set to the same \ndefault weight; therefore, the position of tactics within the initial prioritized list is randomly \ndetermined. \nOnce a tactic library has been defined, the next step is to determine the instances of the \nlibrary. This involves defining the state space, the number of instances, the position of the \ninstances within the state space, and how the state space is organized. In this example, the \nlower state space is defined as the relative distance between the adapting NPC and the \ntargeted opponent. This simple feature vector was selected because distance is a prominent \nfactor in the selection of FPS tactics. In most cases the requirements of the game can be \nused to determine the most appropriate state space representation. \nThe number of library instances is determined by the size of the state space. In this \nexample, the state space is compact; therefore, only a few instances are needed. The \nrequisite number of instances is also affected by their position in the state space and how \nthe state space is organized. The position of tactic instances in the state space can be \nmanually managed using domain knowledge or automatically added using a distance metric, \nsuch as: if s > r, then create a new instance of the tactic library, where s is the distance \nbetween the current state and the closest tactic instance and r is a predefined distance \nthreshold. If the distance is below the threshold, then the closest tactic cases are used. \nFigure 3.6.3 illustrates an example of tactic library instances and state space organization. \nFigure 3.6.3. Example arrangement of the lower state space. \n \n \nAt the start of a combat encounter, the tactic selection architecture generates a list of \ntactics that are ordered by their associated weight. The NPC attempts to defeat the \nopponent by executing the first tactic in the list. If the tactic is not capable of running, the \nnext tactic in the list is selected. This is repeated until a tactic capable of running is found. \nThe list of tactics is generated by determining the closest K library instances to the NPC‘s \nposition in the lower state space. Once a tactic and encounter are complete, the weight of \nthe tactics used are updated using Equations (1) and (4). In this example, K is set to 1, and \nthe initial weights of tactics are the same; therefore, only the closest library instance is \nused, and the order of the tactics is initially random. The process of an NPC selecting and \nadjusting its weights is summarized below: \n\n\n \n \n \nA prioritized list of tactics is generated, and the melee attack tactic is selected for \nexecution. The tactic performs poorly during the encounter and results in 100-\npercent health loss for the adapting NPC and 5-percent health loss for the opponent. \n \nThe encounter finishes with the death of the adapting NPC; therefore, the tactic and \nencounter fitness are determined. The tactic‘s weight is updated based on its \nsuccess. The first step in the process is to determine a tactic‘s fitness—in this \nexample, hl_npc equals 100 and hl_opp equals 5. \n \nint hl_npc = botDamage() \n \nint hl_opp = playerDamage() \n \nfloat a = (hl_opp - hl_npc) / 100 \n \nfloat s = ( (hl_opp - hl_npc) - avgDiffInLifeLost ) / 200 \nfloat fitness = 1 / 10 * (7 * a + 3 * s) \n \nOnce the fitness of the tactic has been determined, the weights of all the tactics in \nthe library instance are updated. If the number of library instances equals 1, as in \nthis example, the weight adjustment for the performed tactic is determined as \nfollows: \n \nif (fitness < 0) \n \n    weightAdjustment = tacticPenaltyMax * fitness \n \nelse \n    weightAdjustment = tacticRewardMax * fitness \n \nNext, the encounter weight update is performed. This process involves the algorithm \nlooping through the tactics used during the encounter, determining their encounter \nfitness, weight adjustment, and weight update. The calculation of the encounter \nfitness is shown below. The tacticResults array contains information on each \ntactic used during the encounter and its performance. \n \nif (hasWon == true) \n \n    fitness = 1 - (tacticResults[i].getHl_npc() / 125) \n \nelse \n    fitness = -1 + (tacticResults[i].getHl_opp() / 125) \n \nAfter the tactic weights have been updated, the adapting NPC can generate a new \nprioritized list. In this example, the melee attack tactic will be at the end of the list \ndue to its poor performance. \n \nConclusion \nThis gem has outlined an approach to the in-game adaptation of scripted tactical behavior in \nFPS computer games. The technique enables NPCs in these games to adapt their selection \nof tactics in a given state based on their experience of the tactics‘ success. Less successful \ntactics are not selected or are only selected when more preferred tactics are not available. \n \nReferences \n[Andrade04] Andrade, G. D., H. P. Santana, A. W. B. Furtado, A. R. G. A. Leitão, and G. L. \nRamalho. ―Online Adaptation of Computer Games Agents: A Reinforcement Learning \nApproach.‖ 1st Brazilian Symposium on Computer Games and Digital Entertainment \n(SBGames2004). \n",
      "page_number": 248
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 261-272)",
      "start_page": 261,
      "end_page": 272,
      "detection_method": "topic_boundary",
      "content": " \n \n[Isla05] Isla, D. ―Handling Complexity in the Halo 2 AI.‖ GDC 2005 Proceedings. 2005. \nGamasutra. n.d. <http://www.gamasutra.com/gdc2005/features/20050311/isla_01.shtml>. \n[Mitchell97] Mitchell, T. Machine Learning. McGraw-Hill, 1997. \n[Ponsen and Spronck04] Ponsen, M. and P. Spronck. ―Improving Adaptive Game AI with \nEvolutionary Learning.‖ Computer Games: Artificial Intelligence, Design and Education \n(CGAIDE 2004): 389–396. \n[Saunders02] Saunders, R. ―Curious Design Agents and Artificial Creativity: A Synthetic \nApproach to the Study of Creative Behaviour.‖ Ph.D. thesis. University of Sydney, Sydney. \n2002. \n[Spronck05] Spronck, P. ―Adaptive Game AI.‖ Ph.D. Thesis. Universiteit Maastricht, \nMaastricht. 2005. \n[Spronck06] Spronck, P. ―Dynamic Scripting.‖ AI Game Programming Wisdom 3. Ed. S. \nRabin. Boston: Charles River Media, 2006. 661–675. \n[Thurau06] Thurau, C. ―Behavior Acquisition in Artificial Agents.‖ Ph.D. thesis. Bielefeld \nUniversity, Bielefeld. 2006. \n \n3.7. Embracing Chaos Theory: Generating Apparent Unpredictability \nthrough Deterministic Systems \nDave Mark, Intrinsic Algorithm LLC \ndave@intrinsicalgorithm.com \nOne of the challenges of creating deep, interesting behaviors in games and simulations is to \nenable our agents to select from a wide variety of actions while not abandoning completely \ndeterministic systems. On the one hand, we want to step away from having very obvious \nif/then triggers or monotonous sequences of actions for our agents. On the other hand, the \nneed for simple testing and debugging necessitates the avoidance of introducing random \nselection to our algorithms. \nThis gem shows how, through embracing the concept and deterministic techniques of chaos \ntheory, we can achieve complex-looking behaviors that are reasonable yet not immediately \npredictable by the viewer. By citing examples from nature and science (such as weather) as \nwell as the simple artificial simulations of cellular automation, the gem explains what causes \nchaotic-looking systems through purely deterministic rules. The gem then presents some \nsample, purely deterministic behavior systems that exhibit complex, observably \nunpredictable sequences of behavior. The gem concludes by explaining how these sorts of \nalgorithms can be easily integrated into game AI and simulation models to generate deeper, \nmore immersive behavior. \nThe Need for Predictability \nThe game development industry often finds itself in a curious predicament with regard to \nrandomness in games. Game developers rely heavily on deterministic systems. \nProgramming is inherently a deterministic environment. Even looking only at the lowly \nif/then statement, it is obvious that computers themselves are most ―comfortable‖ in a \nrealm where there is a hard-coded relationship between cause and effect. Even non-binary \nsystems, such as fuzzy state machines and response curves, could theoretically be reduced \n\n\n \n \nto a potentially infinite sequence of statements that state, ―Given the value x, the one and \nonly result is y.‖ \nGame designers, programmers, and testers also feel comfortable with this technological \nbedrock. After all, in the course of designing, developing, and observing the complex \nalgorithms and behaviors, they often have the need to be able to say, with certainty, ―Given \nthis set of parameters, this is what should happen.‖ Often the only metric of the success or \nfailure of the development process we have is the question, ―Is this action what we \nexpected? If not, what went wrong?‖ \n \nShaking Things Up \nGame players, on the other hand, have a different perspective on the situation. The very \nfactor that comforts the programmer—the knowledge that his program is doing exactly what \nhe predicts—is the factor that can annoy the player…the program is doing exactly what he \npredicts. From the standpoint of the player, predictability in game characters can lead to \nrepetitive, and therefore monotonous, gameplay. \nThe inclusion of randomness can be a powerful and effective tool to simulate the wide \nvariety of choices that intelligent agents are inclined to make [Mark09]. Used correctly and \nlimiting the application to the selection of behaviors that are reasonable for the NPC‘s \narchetype, randomness can create deeper, more believable characters [Ellinger08]. While \nthis approach provides a realistic depth of behavior that can be attractive to the game \nplayer, it is this same abandonment of a predictable environment that makes the analysis, \ntesting, and debugging of behaviors more complicated. Complete unpredictability can also \nlead to player frustration, as they are unable to progress in the game by learning complex \nagent behavior. The only recourse that a programming staff has in controlling random \nbehaviors is through tight selection of random seeds. In a dynamic environment, however, \nthe juxtaposition of random selection of AI with the unpredictable nature of the player‘s \nactions can lead to a combinatorial explosion of possible scenario-to-reaction mappings. \nThis is often a situation that is an unwanted—or even unacceptable—risk or burden for a \ndevelopment staff. \nThe solution lies in questioning one of the premises in the above analysis—that is, that the \nexperience of the players is improved through the inclusion of randomness in the decision \nmodels of the agent AI. While that statement may well be true, the premise in question is \nthat the experience that the player has is based on the actual random number call in the \ncode. The random number generation in the agent AI is merely a tool in a greater process. \nWhat is important is that the player cannot perceive excessive predictable regularity in the \nactions of the agent. As we shall discuss in this article, accomplishing the goal of \nunpredictability can exist without sacrificing the moment-by-moment logical determinism \nthat developers need in order to confidently craft their agent code. \n \nA Brief History of Chaos \nThe central point of how and why this approach is viable can be illustrated simply by \nanalyzing the term chaos theory. The word chaos is defined as ―a state of utter confusion or \ndisorder; a total lack of organization or order.‖ This is also how we tend to use it in general \nspeech. By stating that there is no organization or order to a system, we imply randomness. \nHowever, chaos theory deals entirely within the realm of purely deterministic systems; \nthere is no randomness involved. In this sense, the idea of chaos is more aligned with the \nidea of ―extremely complex information‖ than with the absence of order. To the point of this \narticle, because the information is so complex, we observers are unable to adequately \n\n\n \n \nperceive the complexity of the interactions. Given a momentary initial state (the input), we \nfail to determine the rule set that was in effect that led to the next momentary state (the \noutput). \nOur inability to perceive order falls into two general categories. First, we are often limited \nby flawed perception of information. This occurs by not perceiving the existence of relevant \ninformation and not perceiving relevant information with great enough accuracy to \ndetermine the ultimate effect of the information on the system. \nThe second failure is to adequately perceive and understand the relationships that define \nthe systems. Even with perfect perception of information, if we are not aware of how that \ninformation interacts, we will not be able to understand the dynamics of the system. We \nmay not perceive a relationship in its entirety or we may not be clear on the exact \nmagnitude that a relationship has. For example, while we may realize that A and B are \nrelated in some way, we may not know exactly what the details of that relationship are. \nPerceiving Error \nChaos theory is based largely on the first of these two categories—the inability to perceive \nthe accuracy of the information. In 1873, the Scottish theoretical physicist and \nmathematician James Clerk Maxwell hypothesized that there are classes of phenomena \naffected by ―influences whose physical magnitude is too small to be taken account of by a \nfinite being, [but which] may produce results of the highest importance.‖ \nAs prophetic as this speculation is, it was the French mathematician Henri Poincaré, \nconsidered by some to be the father of chaos theory, who put it to more formal study in his \nexamination of the ―three-body problem‖ in 1887. Despite inventing an entirely new branch \nof mathematics, algebraic topology, to tackle the problem, he never completely succeeded. \nWhat he found in the process, however, was profound in its own right. He summed up his \nfindings as follows: \nIf we knew exactly the laws of nature and the situation of the univ erse at the \ninitial moment, we could predict exactly the situation of the same univ erse at \na succeeding moment. But even if it were the case that the natural laws had \nno longer any secret for us, we could still know the situation approximately. If \nthat enabled us to predict the succeeding situation with the same \napproximation, that is all we require, and we should say that the \nphenomenon had been predicted, that it is governed by the laws. But [it] is \nnot always so; it may happen that small differences in the initial conditions \nproduce very great ones in the final phenomena. A small error in the former \nwill produce an enormous error in the later. Prediction becomes impossible…. \n[Wikipedia09] \nThis concept eventually led to what is popularly referred to as the butterfly effect. The origin \nof the term is somewhat nebulous, but it is most often linked to the work of Edward Lorenz. \nIn 1961, Lorenz was working on the issue of weather prediction using a large computer. \nDue to logistics, he had to terminate a particularly long run of processing midway through. \nIn order to resume the calculations at a later time, he made a note of all the relevant \nvariables in the registers. When it was time to continue the process, he re-entered the \nvalues that he had recorded previously. Rather than reenter one value as 0.506127, he \nsimply entered 0.506. Eventually, the complex simulation diverged significantly from what \nhe had predicted. He later determined that the removal of 0.000127 from the data was \nwhat had dramatically changed the course of the dynamic system—in this case, resulting in \na dramatically different weather system. In 1963, he wrote of his findings in a paper for the \nNew York Academy of Sciences, noting that, ―One meteorologist remarked that if the theory \nwere correct, one flap of a seagull‘s wings could change the course of weather forever.‖ (He \nlater substituted ―butterfly‖ for ―seagull‖ for poetic effect.) \n\n\n \n \nDespite being an inherently deterministic environment, much of the problem with predicting \nweather lies in the size of the scope. Certainly, it is too much to ask scientists to predict on \nwhich city blocks rain will fall and on which it will not during an isolated shower. However, \neven predicting the single broad path of a large, seemingly well-organized storm system, \nsuch as a hurricane, baffles current technology. Even without accounting for the intensity of \nthe storm as a whole, much less the individual bands of rain and wind, the various forecasts \nof simply the path of the eye of the hurricane that the different prediction algorithms churn \nout lay out like the strings of a discarded tassel. That these mathematical models all process \nthe same information in such widely divergent ways speaks to the complexity of the \nproblem. \nThankfully, the mathematical error issue is not much of a factor in the closed system of a \ncomputer game. We do not have to worry about errors in initial observations of the world, \nbecause our modeling system is actually a part of the world. If we restart the model from \nthe same initial point, we can guarantee that, unlike Lorenz‘ misfortune, we won‘t have an \nerror of 0.000127 to send our calculations spinning off wildly into the solar system. \n(Interestingly, in our quest for randomness, we can build a system that relies on a truly \nrandom seed to provide interesting variation—the player.) Additionally, we don‘t have to \nworry about differences in mathematical calculation on any given run. All other things being \nequal (for example, processor type), a given combination of formula and input will always \nyield the same output. These two factors are important in constructing a reliable \ndeterministic system that is entirely under our control. \nBrownian Motion \nAs mentioned earlier, the second reason why people mistake deterministic chaos for \nrandomness is that we often lack the ability to perceive or realize the relationships between \nentities in a system. In fact, we often are not aware of some of the entities at all. This was \nthe case with the discovery of a phenomenon that eventually became known as Brownian \nmotion. Although there had been observations of the seemingly random movement of \nparticles before, the accepted genesis of this idea is the work of botanist Robert Brown in \n1827. As he watched the microscopic inner workings of pollen grains, he observed minute \n―jittery‖ movement by vacuoles. Over time, the vacuoles would even seem to travel around \ntheir neighborhood in an ―alive-looking‖ manner. Not having a convenient explanation for \nthis motion, he assumed that pollen was ―alive‖ and was, after the way of living things, \nmoving of its own accord. He later repeated the experiments with dust, which ruled out the \n―alive‖ theory but did nothing to explain the motion of the particles. \nThe real reason for the motion of the vacuoles was due to the molecular and atomic level \nvibrations due to heat. Each atom in the neighborhood of the target vibrates on its own \npattern and schedule, with each vibration nudging both the target and other adjacent atoms \nslightly. The combination of many atoms doing so in myriad directions and amounts \nprovides a staggering level of complexity. While completely deterministic from one moment \nto the next—that is, ―A will nudge B n distance in d direction‖— the combinatorial explosion \nof interconnected factors goes well beyond the paltry scope of Poincaré‘s three-body \nproblem. \nThe problem that Brown had was that he could not perceive the existence of the atoms \nbuffeting the visible grains. What‘s more, even when the existence of those atoms is known \n(and more to the point, once the heat-induced vibration of molecules is understood), there \nis no way that anyone can know what that relationship between cause and effect is from \nmoment to moment. We only know that there will be an effect. \nThis speaks to the second of the reasons we listed earlier—that we often lack the ability to \nperceive or realize the relationships between entities in a system. This effect is easier for us \nto take advantage of in order to accomplish our goal. By incorporating connections between \nagents and world data that are beyond the ability of the player to adequately perceive, we \ncan generate purely deterministic cause/effect chains that look either random or at least \nreasonably unpredictable. \n\n\n \n \n \nExploring Cellular Automata \nOne well-known example bed for purely deterministic environments is the world of cellular \nautomata. Accordingly, one of the most well-known examples of cellular automata is \nConway‘s Game of Life. Conway‘s creation (because the term ―game‖ is probably pushing \nthings a little) started as an attempt to boil down John von Neumann‘s theories of self-\nreplicating Turing machines. What spilled out of his project was an interesting vantage point \non emergent behavior and, more to the point of this gem, the appearance of seemingly \ncoordinated, logical behavior. Using Conway‘s Life as an example, we will show how \napplying simple, deterministic rules produce this seemingly random behavior. \nThe environment for Life is a square grid of cells. A cell can be either on or off. Its state in \nany given time slice is based on the states of the eight cells in its immediate neighborhood. \nThe number of possible combinations of the cells in the local neighborhood is 28 or 256. (If \nyou account for mirroring or rotation of the state space, the actual number of unique \narrangements is somewhat smaller.) The reason that the Game of Life is easy for us to \ndigest is its brevity and simplicity, however. We do not care about the orientation of the live \nneighbors, but only a sum of how many are alive at that moment. The only rules that are in \neffect are: \n1. Any live cell with two or three live neighbors lives on to the next generation. \n2. Any live cell with fewer than two live neighbors dies (loneliness/starvation). \n3. Any live cell with more than three live neighbors dies (overcrowding). \n4. Any dead cell with exactly three live neighbors becomes a live cell (birth). \nFigure 3.7.1 shows a very simple example of these rules in action. In the initial grid, there \nare three ―live‖ cells shown in black. Additionally, each cell contains a number showing how \nmany neighbors that cell currently has. Note that two of the ―dead‖ cells (shown in gray) \nhave three neighbors, which, according to Rule 4 above, means they will become alive in \nthe next iteration. The other dead cells have zero, one, or two neighbors, meaning they will \nremain at a status quo (in other words, dead) for the next round. The center ―live‖ cell has \ntwo neighbors, which, according to Rule 1 above, allows it to continue living. On the other \nhand, the two end cells have only a single live neighbor (the center cell) and will therefore \ndie of starvation the next round. The results are shown on the right of Figure 3.7.1. Two of \nthe prior cells are now dead (shown in gray), and two new cells have been born to join the \nsingle surviving cell. \nFigure 3.7.1. A simple example of the rule set in Conway’s Game of Life. In this \ncase, a two-step repeating figure called a blinker is generated by the three boxes. \n \n \nInterestingly, this pattern repeats such that the next iteration will be identical to the first (a \nhorizontal line), and so on. This is one of the many stable or tightly repeating patterns that \ncan be found in Life. Specifically, this one is commonly called a blinker. \n\n\n \n \nFigure 3.7.2 shows another, slightly more involved example. The numbers in the initial \nframe make it easier to understand why the results are there. Even without the numbers, \nhowever, the relationships between the initial state and the subsequent one are relatively \neasy to discern on this small scale. \nFigure 3.7.2. The dynamic nature of the cells acting together. \n \n \nWhile the rule set seems simple and intuitive enough, when placed on a larger scale and run \nover time, the ―behavior‖ of the entire ―colony‖ of cells starts to look random. This is due to \nthe overwhelming number of interactions that we perceive at any one moment. Looking at \nthe four panels of Figure 3.7.3, it is difficult for us to intuitively predict what would have \nhappened next except for either in the most general sense (for example, that solid blob in \nthe middle is too crowded and will likely collapse) or in regard to very specific subsets of the \nwhole (for example, that is a blinker in the lower-left corner). \nFigure 3.7.3. When taken as a whole, the simple cells in Life take on a variety of \ncomplex-looking behaviors. While each cell’s state is purely deterministic, it is \ndifficult for the human mind to quickly predict what the next overall image will \nlook like. \n \n \nInterestingly, it is this very combination of ―reasonable‖ expectations with not knowing \nexactly what is going to appear next that gives depth to Conway‘s simulation. Over time, \none develops a sense of familiarity with the generalized feel of the simulation. For example, \nwe can expect that overcrowded sections will collapse under their own weight, and isolated \npockets will die off or stagnate. We also recognize how certain static or repeating features \nwill persist until they are interfered with—even slightly— by an outside influence. Still, the \ncasual observer will still perceive the unfolding (and seemingly complex) action as being \nsomewhat random…or at least undirected. Short of pausing to analyze every cell on every \nframe, the underlying strictly rule-based engine goes unnoticed. \n\n\n \n \nOn the other hand, from the standpoint of the designer and tester, this simulation model is \nelegantly simplistic. The very fact that you can pause the simulation and confirm that each \ncell is behaving properly is a boon. For any given cell at any stage, it is a trivial problem to \nconfirm that the resulting state change is performing exactly as designed. \n \nLeveraging Chaos Theory in Games \nWhat sets Conway‘s Life apart from many game scenarios is not the complexity of the rule \nset, but rather the depth of it. On its own, the process of passing the sum of eight binary \ninputs through four rules to receive a new binary state does not seem terribly complex. \nWhen we compare it to what a typical AI entity in a game may use as its decision model, we \nrealize that it is actually a relatively robust model. \nFor instance, imagine a very simple AI agent in a first-person shooter game (see Figure \n3.7.4). It may take into account the distance to the player and the direction in which the \nplayer lies. When the player enters a specified range, the NPC ―wakes up,‖ turns, and \nmoves toward the player. There is one input state—distance—and two output states: ―idle‖ \nand ―move toward player.‖ While this seems extraordinarily simple, as recently as 10 to 15 \nyears ago, this was still common for enemy AI. Needless to say, the threshold and resultant \nbehavior were easy to discern over time. Players could perceive both the cause and the \neffect with very little difficulty. Likewise, designers and programmers could test this \nbehavior with something as simple as an onscreen distance counter. At this point, there is \nvery little divergence between the simplicity for the player and the simplicity for the \nprogrammer. \nFigure 3.7.4. If there is only one criterion in a decision model, it is relatively \nsimple for the player to determine not only what the criterion is, but what the \ncritical threshold value is for that criterion to trigger the behavior. \n \n \n \nAdding a Second Factor \nIf we were to add a second criterion to the decision, the situation does not necessarily \nbecome much more complicated. For example, we could add a criterion stating that the \n\n\n \n \nagent will only attack the player when he is in range and carrying a weapon. This is an \nintuitively sound addition and is likely something that the player will quickly understand. On \nthe other hand, this also means that the enemy is again rigidly predictable. \nOther factors can be added to a decision model, however, which could obscure the point at \nwhich a behavior change should occur. Even the addition of other binary factors (such as \nthe states of the cells in Life) can complicate things quickly for the observer if they aren‘t \nintuitively obvious. For instance, imagine that the rule for attacking the player was no \nlonger ―if the distance from player to enemy < n‖ but rather ―if the player‘s distance to two \nenemies < n‖ (see Figure 3.7.5). As the player approaches the first enemy, there would be \nno reaction from that first enemy until a second enemy is within range as well. While this \nmay seem like a contrived rule, it stresses an important point. The player will most certainly \nbe interested in the actions of the first enemy and will not easily recognize that its reaction \nwas ultimately based on the distance to the second enemy. \nFigure 3.7.5. The inclusion of a second criterion can obscure the threshold value \nfor—or even the existence of—the first criterion. In this case, because the second \nenemy is included, the player is not attacked as he enters the normal decision \nradius of the first enemy. \n \n \nThe player may not be able to adequately determine what the trigger conditions are \nbecause we have masked the existence of the second criterion from the player. People \nassume that causes and effects are linked in some fashion. In this example, because there \nis no intuitive link between the player‘s distance to the second enemy and the actions of the \nfirst, the player will be at a loss to determine when the first enemy will attack. The benefit \nof this approach is that the enemy no longer seems like it is acting strictly on the whims of \nthe player. That is, the player is no longer deciding when he wants the enemy to attack—it \nis seemingly attacking on its own. This imparts an aura of unpredictability on the enemy, \nwhich, in essence, makes it seem more autonomous. \nOf course, we programmers know that the agent is not truly autonomous but merely acting \nas a result of second criterion. In fact, this new rule set is almost as simple to monitor and \ntest as it is to program in the first place. We have the benefit of knowing what the two rules \nare and how they interact—something that is somewhat opaque to the player. \nSelecting the Right Factors \n\n\n \n \nAs mentioned, the inclusion of the player‘s distance to the second agent as a criterion for \nthe decisions of the first agent is a little contrived. In fact, it has the potential for \nembarrassing error. If the second agent was far away, it is possible that the player could \nwalk right up to the first one and not be inside the threshold radius of the second. In this \ncase, the first agent would check his decision criteria and determine that the player was not \nin range of two people and, as a result, would not attack the player. This does not mean \nthat there is a flaw in the use of more than one criterion for the decision—simply that there \nis a flaw in which criteria are being used in conjunction. In this case, the criterion that was \nbased on the position of the second agent—and, more specifically, the player‘s proximity to \nthe second agent—was arbitrary. It was not directly related to the decision that the agent is \nmaking. \nThe solution to this is to include factors on which it is reasonable for an agent to base his \ndecisions. In this example, the factors may include items such as: \n \nDistance to player \n \nPerceived threat (for example, visible weapon) \n \nAgent‘s health \n \nAgent‘s ammo \n \nAgent‘s alertness level \n \nPlayer‘s distance to sensitive location \nWe already covered the first two. The others are simply examples of information that could \nbe considered relevant. For the last one, we could use the distance measurement from the \nplayer to a location such as a bomb detonator. If the player is too close to it, the agent will \nattack. This is different than the example with two agents above in that the distance to a \ndetonator is presumably more relevant to the agent‘s job than the player‘s proximity to two \nseparate agents. \nWhile a number of the criteria listed previously could be expressed as continuous values, \nsuch as the agent‘s health ranging from 0 to 100, for the sake of simplicity, they can also \nbe reduced to Boolean values. We could rephrase ―agent‘s health‖ as ―agent has low \nhealth,‖ for instance. If we define ―low health‖ as a value below 25, we are now able to \nreduce that criterion to a Boolean value. The same could be done with ―agent‘s ammo.‖ \nThis, of course, is very similar to what we did with the distance. We could assert that ―if \nagent has less than 10 shots remaining,‖ then ―agent has low ammo.‖ \nWhat we have achieved with the above list could be summed up with the following pseudo-\ncode query: \nIf  PlayerTooCloseToMe() \n    or PlayerCloseToTarget() \n    and WeaponVisible() \n    and IAmHealthy() \n    and IHaveEnoughAmmo() \n    and IAmAlert()) \n    then Attack() \n \nEven with these criteria, the number of possible configurations is 27 or 128. (Incidentally, \nthe number of configurations of cells in Life is 28 or 256.) In our initial example, it would \ntake only a short amount of time to determine the distance threshold at which the agent \nattacks. By the inclusion of so many relevant factors into the agent‘s behavior model, the \nonly way that any one threshold can be ascertained is with the inclusion of the caveat ―all \nother things being equal.‖ Certainly in a dynamic environment, it is a difficult prospect to \ncontrol for all variables simultaneously. \n\n\n \n \nWhile having 128 possible configurations seems like a lot, it is not necessarily the number of \npossible configurations of data that will obscure the agent‘s possible selection from the \nplayer. Much of the difficulty that a player would have in knowing exactly what reaction the \nagent will have is due to the fact that the player cannot perceive all the data. This is similar \nto the impasse at which Robert Brown found himself. He could not detect the actual \nunderlying cause of the jitteriness of the pollen grains and dust particles. His observation, \ntherefore, was that the motion was random yet reasonable; he perceived lifelike motion \nwhere there was no life. \nA good way of illustrating this point is by working backward—that is, looking at the situation \nfrom the point of the player. If the agent‘s behavior changes from one moment to the next, \nthe player may not be able to determine which of the aforementioned factors crossed one of \nthe defined thresholds to trigger the change. In some cases, this would be easy. For \nexample, if the player draws his weapon and the agent attacks, the player can make the \nassertion that the weapon was the deciding factor. However, if the agent does not attack \nand, for instance, runs away instead, the player may not be able to determine whether it \nwas due to the agent having low health or low ammo. \nSimilarly, if the player is moving near the agent with his weapon drawn, and the agent \nbegins to attack, the player may not be able to ascertain whether it was his proximity to the \nagent, a secondary point (for example, a detonator), or a change in the agent‘s alertness \nstatus that caused the transition to occur. Once combat is engaged and values such as \nhealth and ammo are changing regularly, the number of possible reasons for a change in \nbehavior increases significantly. \nOf course, this is the reason why it is important to use relevant information as part of your \ndecision. If you use rational bases for your decisions, it makes it more likely that the \ndecision can at least be understood after it happens. There is a big difference between \npredictability and understandability. The player may not know exactly when the agent is \ngoing to change behaviors, but he should be able to offer a reasonable guess as to why it \nhappened. \nFrom a development and testing standpoint, the important issue to note here is that this is \nstill purely deterministic. There is no randomness included at all. A simple code trace or \nonscreen debug information would confirm the status of all seven of these criteria. When \ncompared against the decision code, the developer can confirm whether the agents are \noperating as planned or, in the case that they are not, determine which of the criteria needs \nto be adjusted. \nBeyond Booleans \nWe can extend the aforementioned ideas to go beyond purely Boolean flags, however. By \nincorporating fuzzy values and appropriate systems to handle them, we could have more \nthan one threshold value on any of the previous criteria. For instance, we could use the \nseven aforementioned factors to select from a variety of behaviors. Rather than simply \ndetermining whether the agent will attack the player, for example, we could include actions \nsuch as finding cover, running away, reloading, or calling for help. In order to do this, we \ncould partition one or many of the factors into multiple zones. \nFor example, if we were to arrange two factors on two axes and determine a threshold \nacross each, we would arrive at four distinct ―zones‖ (see Figure 3.7.6, Panel 1). Each of \nthese zones can be assigned to a behavior. In this case, using only two factors and two \nthreshold values, we can arrive at four distinct behaviors. The more thresholds we insert, \nthe more zones are created. Using our two-axis example, by increasing the threshold values \nfrom 1 to 3 in each direction, we increase the number of result spaces from 4 to 16 (see \nFigure 3.7.6, Panel 2). \n\n\n \n \nFigure 3.7.6. As the number of logical partitions through axes is increased, the \nnumber of potential results increases exponentially as a factor of the number of \naxes involved. By combining the factors prior to partitioning (Panel 3), thresholds \ncan be made more expressive. \n \n \nWe can visualize how this would affect behaviors if we imagine the values of our two factors \nmoving independently along their respective axes. For example, imagine a data point \nlocated in Behavior G in Panel 2. If Factor 1‘s value were to change, we could expect to see \nBehaviors F and H—and even E—depending on the amount of change in Factor 1. If Factor 2 \nwere the only one changing, we could expect to see changes to Behaviors C, K, and O. If \nchanges were occurring in only Factor 1 or 2, by observing the changes in behavior that \noccurred, we could eventually determine where those thresholds between behaviors are. \nHowever, if both factors were continually changing independent of each other, we now could \npossibly see any one of the 16 behaviors. This would make it significantly more difficult to \nexactly predict the cause-and-effect chain between factors and behaviors. \nFor instance, assume once again that we start with the data point in G. If we were \nwitnessing a reduction in Factor 1, we may see a state change to B, C, F, J, or K. All of \nthose states can be reached from G if Factor 1 is decreasing. What we would have to realize \nis that the ultimate state is decided by Factor 2 as well—for example, G→C could happen if \nFactor 1 was decreasing slowly and Factor 2 was increasing. Correspondingly, G→K could \nhappen if Factor 1 was decreasing slowly and Factor 2 was decreasing. Naturally, similar \nextensions of logic apply to result states of B and J. The conclusion is, while we can make a \ngeneral statement about where our data point may end up while Factor 1 is decreasing, we \ncan‘t know the eventual result state without also knowing the behavior of Factor 2. \nThe examples shown in Figure 3.7.6 only show two dimensions due the limitations of what \ncan be shown on paper. Our decision model does not share those limitations, however. By \nusing each potential factor in our decision model and defining one or more thresholds of \nmeaning, we can create a complex state space of potential results. \nAs a measurement of how this expands the potential number of actions, we can compare \nthe number of possible outcomes. When the system was composed of 7 Boolean values, we \nhad 128 possible combinations of data. Even by simply partitioning each of the 7 inputs into \n3 ranges rather than 2, the number of combinations becomes 37 or 2,187. These results \ncould then be mapped to a startling array of behaviors. Of course, not all the result zones \nneed to represent individual behaviors. Imagine mapping the 2,187 possible result spaces \nonto 30 different behaviors, for instance. \nTo further leverage the power of this system, we can step beyond the mentality of mapping \na single value onto a series of states. Instead, we can use systems that can take multiple \ninputs in conjunction and select a state that is dependant on values of those multiple inputs. \n\n\n \n \nFor example, we may want our agent to consider a combination of its distance to the player \nand their own health. That is, the state of their own health becomes more important if the \nplayer is closer. The two endpoints of the ―relevancy vector‖ of this decision would be \nbetween (―player far‖ + ―perfect health‖) and (―player near‖ + ―zero health‖). As either of \nthose factors moves from one end to the other, it has less effect on the outcome than if \nboth of these factors were moving simultaneously. \nFigure 3.7.6, Panel 3 shows a two-dimensional visualization of this effect. In this case, the \nfactors themselves are not partitioned. Instead, they remain continuous variables. When \ncombined, however, they create a new directed axis in the state space— in this case shown \nby the combined shading. We can set a threshold across that new axis (dotted line) that can \nbe used to determine where the behavior changes. Now, from the point of view of the \nplayer, he cannot determine at what point on Factor 1 the behavior changes without taking \ninto account changes in Factor 2 as well. \nJust as we did with single-axis-based thresholds, by determining one or more multi-axis \nthreshold lines in an n-dimensional space, we partition our space into a variety of zones. By \nanalyzing in which sections of the resulting partitioned space our current input data falls, we \ncan select from multiple potential outputs. By combining values in this way, we can \npotentially arrive at more expressive outputs at any stage of our decision model. \nSpecific techniques for accomplishing and managing this sort of decision complexity can be \nfound elsewhere [Mark08]. The methods that we use to arrive at the resulting values are \nnot the focus here, however. The important part is that we are doing all of this in a purely \ndeterministic fashion—that is, we could verify that any given combination of factors is \nmapped to the appropriate action. While there is still no random factor being included in \nthese calculations, the dizzying number of potential combinations provides for reasonable-\nlooking, yet not inherently predictable results. \n \nConclusion \nTo sum up, while our desire as game developers may be to express a variety of reasonable-\nlooking but slightly unpredictable behaviors, we do not have to resort to randomness in \norder to generate that effect. By including more than one or two simple, easily perceivable \ncriteria in our decision models, we can begin to obscure the workings of that model from the \nplayer, yet leave it perfectly exposed and understandable to the programmer and even the \ndesign team. However, in order to avoid the potential for arbitrary-looking decisions by our \nagents, we must be careful to select criteria that are relevant to the decision being made. In \nthis way we are also providing deeper, more realistic-looking, and potentially more \nimmersive behaviors for our agents. \n \nReferences \n[Ellinger08] Ellinger, Benjamin. ―Artificial Personality: A Personal Approach to AI.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media, 2008. \n[Mark08] Mark, Dave. ―Multi-Axial, Dynamic Threshold Fuzzy State Machine.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media, 2008. \n[Mark09] Mark, Dave. Behavioral Mathematics for Game AI. Boston: Charles River Media, \n2009. \n",
      "page_number": 261
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 273-280)",
      "start_page": 273,
      "end_page": 280,
      "detection_method": "topic_boundary",
      "content": " \n \n[Wikipedia09] ―Henri Poincaré.‖ n.d. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Henri_Poincaré>. \n \n3.8. Needs-Based AI \nRobert Zubek \nNeeds-based AI is a general term for action selection based on attempting to fulfill a set of \nmutually competing needs. An artificial agent is modeled as having a set of conflicting \nmotivations, such as the need to eat or sleep, and the world is modeled as full of objects \nthat can satisfy those needs at some cost. \nThe core of the AI is an action selection algorithm that weighs those various possible actions \nagainst each other, trying to find the best one given the agent‘s needs at the given \nmoment. The result is a very flexible and intuitive method for building moderately complex \nautonomous agents, which are nevertheless efficient and easy to understand. \nThis gem presents some technical details of needs-based AI. We begin with a general \noverview and then dive directly into technical implementation, presenting both general \ninformation and some specific hints born out of experience implementing this AI. Finally, we \nfinish with an overview of some design consequences of using this style of AI. \nBackground \nIn terms of its historical context, the needs-based AI approach is related to the family of \nbehavior-with-activation-level action selection methods common in autonomous robotics. \n(For an overview, see [Arkin98], page 141.) In game development, it was also \nindependently rediscovered by The Sims, where it has been enjoyed by millions of game \nplayers. The Sims also contributed a very useful innovation on knowledge representation, \nwhere behaviors and their advertisements are literally distributed ―in the world‖ in the \ngame, and therefore are very easily configurable. \nMy own interest in this style of AI was mainly driven by working with The Sims (at \nNorthwestern University and later at EA/Maxis). I have since reimplemented variations on \nthis approach in two other games: Roller Coaster Kingdom, a web business simulation \ngame, and an unpublished RPG from the Lord of the Rings franchise. I found the technique \nto be extremely useful, even across such a range of genres and platforms. \nUnfortunately, very few resources about the original Sims AI remain available; of those, \nonly a set of course notes by Ken Forbus and Will Wright, plus a Sims 2 presentation by \nJake Simpson are freely downloadable on the web at this point. (Links can be found in the \n―References‖ section at the end of this gem.) My goal here is to present some of this \nknowledge to a wider audience, based on what I‘ve gained from robotics and The Sims, as \nwell as personal experience building such agents in other games. \n \nNeeds-Based AI Overview \nThere are many ways to drive an artificial agent; some games use finite state machines, \nothers use behavior trees, and so on. Needs-based AI is an alternative with an exciting \nbenefit: The smarts for picking the next action configure themselves automatically, based \non the agent‘s situation as well as internal state; yet the entire algorithm remains easy to \nunderstand and implement. \n\n\n \n \nEach agent has some set of ever-changing needs that demand to be satisfied. When \ndeciding what to do, the agent looks around the world and figures out what can be done \nbased on what‘s in the area. Then it scores all those possibilities, based on how beneficial \nthey are in satisfying its internal needs. Finally, it picks an appropriate one based on the \nscore, finds what concrete sequence of actions it requires, and pushes those onto its action \nqueue. \nThe highest-level AI loop looks like this: \n \nWhile there are actions in the queue, pop the next one off, perform it, and maybe \nget a reward. \n \nIf you run out of actions, perform action selection, based on current needs, to find \nmore actions. \n \nIf you still have nothing to do, do some fallback actions. \nThat second step, the action selection point, is where the actual choice happens. It \ndecomposes as follows: \n1. Examine objects around you and find out what they advertise. \n2. Score each advertisement based on your current needs. \n3. Pick the best advertisement and get its action sequence. \n4. Push the action sequence on your queue. \nThe next sections will delve more deeply into each of these steps. \nNeeds \nNeeds correspond to individual motivations—for example, the need to eat, drink, or rest. \nThe choice of needs depends very much on the game. The Sims, being a simulator of \neveryday people, borrowed heavily from Maslow‘s hierarchy (a theory of human behavior \nbased on increasingly important psychological needs) and ended up with a mix of basic \nbiological and emotional drivers. A different game should include a more specific set of \nmotivations, based on what the agents should care about in their context. \nInside the engine, needs are routinely represented as an array of numeric values, which \ndecay over time. In this discussion we use the range of [0, 100]. Depending on the context, \nwe use the term ―need‖ to describe both the motivation itself (written in boldface—for \nexample, hunger) and its numeric value (for example, 50). \nNeeds routinely have the semantics of ―lower is worse and more urgent,‖ so that \nhunger=30 means ―I‘m pretty hungry,‖ while hunger=90 means ―I‘m satiated.‖ Need values \nshould decay over time to simulate unattended needs getting increasingly worse and more \nurgent. Performing an appropriate action then refills the need, raising it back to a higher \nvalue. \nFor example, we simulate agents getting hungry if they don‘t eat by decaying the hunger \nvalue over time. Performing the ―eat‖ action would then refill it, causing it to become less \nurgent (for a while). \n \nAdvertisements and Action Selection \nWhen the time comes to pick a new set of actions, the agent looks at what can be done in \nthe environment around them and evaluates the effect of the available actions. \n\n\n \n \nEach object in the world advertises a set of action/reward tuples—some actions to be taken \nwith a promise that they will refill some needs by some amount. For example, a fridge \nmight advertise a ―prepare food‖ action with a reward of +30 hunger and ―clean‖ with the \nreward of +10 environment. \nTo pick an action, the agent examines the various objects around them and finds out what \nthey advertise. Once we know what advertisements are available, each of them gets scored, \nas described in the next section. The agent then picks the best advertisement using the \nscore and adds its actions to their pending action queue. \nAdvertisement Decoupling \nPlease notice that the discovery of what actions are available is decoupled from choosing \namong them: The agent ―asks‖ each object what it advertises, and only then scores what‘s \navailable. The object completely controls what it advertises, so it‘s easy to enable or disable \nactions based on object state. This provides great flexibility. For example, a working fridge \nmight advertise ―prepare food‖ by default; once it‘s been used several times, it also starts \nadvertising ―clean me‖; finally, once it breaks, it stops advertising anything other than ―fix \nme‖ until it‘s repaired. \nWithout this decoupling, imagine coding all those choices and possibilities into the agent \nitself, not just for the fridge but also for all the possible objects in the world—it would be a \ndisaster and impossible to maintain. \nOn the other side of the responsibility divide, the agent can also be selective about what \nkinds of advertisements it accepts. We can use this to build different agent subtypes or \npersonalities. For example, in a later section we will describe how to use advertisement \nfiltering to implement child agents with different abilities and opportunities than adults. \nAdvertisement Scoring \nOnce we have an object‘s advertisements, we need to score them and stack them against \nall the other advertisements from other objects. We score each advertisement separately, \nbased on the reward it promises (for example, +10 environment) and the agent‘s current \nneeds. Of course it‘s not strictly necessary that those rewards actually be granted as \npromised; this is known as false advertising, and it can be used with some interesting \neffects, as described later. \nHere are some common scoring functions, from the simplest to the more sophisticated: \nA. Trivial scoring \nfuture valueneed = current valueneed + advertised deltaneed \nscore = ∑ all needs (future valueneed) \nUnder this model, we go through each need, look up the promised future need value, \nand add them up. For example, if the agent‘s hunger is at 70, an advertisement of \n+20 hunger means the future value of hunger will be 90; the final score is the sum \nof all future values. \nThis model is trivially easy and has significant drawbacks: It‘s only sensitive to the \nmagnitude of changes, and it doesn‘t differentiate between urgent and non-urgent \nneeds. So increasing hunger from 70 to 90 has the same score as increasing thirst \nfrom 10 to 30—but the latter should be much more important, considering the agent \nis very thirsty! \n\n\n \n \nB. Attenuated need scoring \nNeeds at low levels should be much more urgent than those at high levels. To model \nthis, we introduce a non-linear attenuation function for each need. So the score \nbecomes: \nscore = ∑ all needs Aneed (future valueneed) \nwhere Aneed is the attenuation function, mapping from a need value to some numeric \nvalue. The attenuation function is commonly non-linear and non-increasing: It starts \nout high when the need level is low and then drops quickly as the need level \nincreases. \nFor example, consider the attenuation function A(x) = 10/x. An action that increases \nhunger to 90 will have a score of 1/9, while an action that increases thirst to 30 will \nhave a score of 1/3, so three times higher, because low thirst is much more \nimportant to fulfill. These attenuation functions are a major tuning knob in needs-\nbased AI. \nYou might also notice one drawback: Under this scheme, improving hunger from 30 \nto 90 would have the same score as improving it from 50 to 90. Worse yet, \nworsening hunger from 100 to 90 would have the same score as well! This detail \nmay not be noticeable in a running system, but it‘s easy to fix by examining the \nneed delta as well. \nC. Attenuated need-delta scoring \nIt‘s better to eat a filling meal than a snack, especially when you‘re hungry, and it‘s \nworse to eat something that leaves you hungrier than before. To model this, we can \nscore based on need level difference: \nscore = ∑ all needs (Aneed (current valueneed) – Aneed (future valueneed)) \nFor example, let‘s consider our attenuation function A(x) = 10/x again. Increasing \nhunger from 30 to 90 will now score 1/3 – 1/9 = 2/9, while increasing it from 60 to \n90 will score 1/6 – 1/9 = 1/18, so only a quarter as high. Also, decreasing hunger \nfrom 100 to 90 will have a negative score, so it will not be selected unless there is \nnothing else to do. \nAction Selection \nOnce we know the scores, it‘s easy to pick the best one. Several approaches for arbitration \nare standard: \n \nWinner-takes-all: The highest-scoring action always gets picked. \n \nWeighted-random: Do a random selection from the top n (for example, top three) \nhigh-scoring advertisements, with probability proportional to score. \n \nOther approaches are easy to imagine, such as a priority-based behavior stack. \nIn everyday implementation, weighted-random is a good compromise between having some \npredictability about what will happen and not having the agent look unpleasantly \ndeterministic. \nAction Selection Additions \nThe model described earlier can be extended in many directions to add more flexibility or \nnuance. Here are a few additions, along with their advantages and disadvantages: \n\n\n \n \nA. Attenuating score based on distance \nGiven two objects with identical advertisements, an agent should tend to pick the \none closer to them. We can do this by attenuating each object‘s score based on \ndistance or containment: \nscore = D ( ∑ all needs ( … ) ) \nwhere D is some distance-based attenuation function, commonly a non-increasing \none, such as the physically inspired D(x) = x / distance2. However, distance \nattenuation can be difficult to tune, because a distant object‘s advertisement will be \nlowered not just compared to other objects of this type, but also compared to all \nother advertisements. This may lead to a ―bird in hand‖ kind of behavior, where the \nagent always prefers a much worse action nearby rather than a better one farther \naway. \nB. Filtering advertisements before scoring \nIt‘s useful to add prerequisites to advertisements. For example, kids should not be \nable to operate stoves, so the stove should not advertise the ―cook‖ action to them. \nThis can be implemented in several ways, from simple attribute tests to a full \nlanguage for expressing predicates. \nIt‘s often best to start with a simple filter mechanism, because complex prerequisites \nare more difficult to debug when there are many agents running around. An easy \nprerequisites system could be as simple as setting Boolean attributes on characters \n(for example, is-adult, and so on) and adding an attribute mask on each \nadvertisement; action selection would only consider advertisements whose mask \nmatches up against the agent‘s attributes. \nC. Tuning need decay \nAgents‘ need levels should decay over time. This causes agents to change their \npriorities as they go through the game. We can tune this system by modifying need \ndecay rates individually. For example, if an agent‘s hunger doesn‘t decay as quickly, \nthey will not need to eat as often and will have more time for other pursuits. \nWe can use this to model a bare-bones personality profile—for example, whether \nsomeone needs to eat/drink/entertain themselves more or less often. It can also be \nused for difficulty tuning—agents whose needs decay more quickly are harder to \nplease. \nD. Tuning advertisement scores \nThe scoring function can also simulate simple personality types directly, by tuning \ndown particular advertisement scores. To do this, we would have each agent contain \na set of tuning parameters, one for each need, that modify that need‘s score: \nnew scoreagent,need = old scoreagent,need * tuningagent,need \nFor example, by tuning down the +hunger advertisement‘s score, we‘ll get an agent \nthat has a stronger preference for highly fulfilling food; tuning up a +thirst \nadvertisement will produce an agent that will happily opt for less satisfying drinks, \nand so on. \nE. Attenuation function tuning \n\n\n \n \nAttenuation functions map from low need levels to high scores. Each need can be \nattenuated differently, since some needs are more urgent than others. As such, they \nare a major tuning knob in games, but a delicate one because their effects are \nglobal, affecting all agents. This requires good design iterations, but analytic \nfunctions (for example, A(x) = 10/x) are not easy for designers to tweak or reason \nabout. \nA happy medium can be found by defining attenuation functions using piecewise-\nlinear functions (in other words, point pairs that define individual straight-line \nsegments, rather than continuous, analytic formulas). These can be stored and \ngraphed in a spreadsheet file and loaded during the game. \n \nAction Performance \nHaving chosen something to do, we push the advertisement‘s actions on the agent‘s action \nqueue, to be performed in order. Each action would routinely be a complete mini-script. For \nexample, the stove‘s ―clean‖ action might be small script that: \n \nAnimates the agent getting out a sponge and scrubbing the stove \n \nRuns the animation loop and an animated stove condition meter \n \nGrants the promised reward \nIt‘s important that the actual reward be granted manually as part of the action, and not be \nawarded automatically. This gives us two benefits: \n \nInterrupted actions will not be rewarded. \n \nObjects can falsely advertise and not actually grant the rewards they promised. \nFalse advertisement is an especially powerful but dangerous option. For example, suppose \nthat we have a food item that advertises a hunger reward but doesn‘t actually award it. A \nhungry agent would be likely to pick that action—but since they got no reward, at the next \nselection point they would again likely pick, and then again, and again. This quickly leads to \nvery intriguing ―addictive‖ behaviors. \nThis may seem like a useful way to force agents to perform an action. But it‘s just as hard \nto make them stop once they‘ve started. False advertisements create action loops that are \nvery difficult to tune. In practice, forcing an action is easier done by just pushing the \ndesired action on the agent‘s action queue. \nAction Chaining \nPerforming a complex action, such as cooking a meal, usually involves several steps (such \nas preparing and cooking) and several objects (a fridge, a cutting board, a stove). This \nsequence must not be atomic—steps can be interrupted, or they can fail due to some \nexternal factors. \nComplex sequences are implemented by chaining multiple actions together. For example, \neating dinner might decompose into several separate actions: \n \nTake a food item from the fridge. \n \nPrepare the food item on a counter. \n \nCook the food item on the stove. \n \nSit down and eat, thereby getting a hunger reward. \n\n\n \n \nIt would be suboptimal to implement this as a single action; there is too much variability in \nthe world for it to always work out perfectly. \nWe can create action sequences in two ways. The simpler way is to just manufacture the \nentire sequence of actions right away and push the whole thing on the agent‘s queue. Of \ncourse, these steps can fail, in which case the remaining actions should also be aborted. For \nsome interesting side effects, aborting an action chain could create new actions in its place. \nFor example, a failed ―cook food‖ action sequence could create a new ―burned food‖ object \nthat needs to be cleaned up. \nThe second method, more powerful but more difficult, is to implement action chaining by \n―lazy evaluation.‖ In this approach, only one action step is created and run at a time, and \nwhen it ends, it knows how to create the next action and front-loads it on the queue. \nFor an example of how that might look, consider eating dinner again. The refrigerator‘s \nadvertisement would specify only one action: ―take food.‖ That action, toward the end, \nwould then find the nearest kitchen counter object, ask it for the ―prepare food‖ action, and \nload that on the queue. Once ―prepare food‖ was done, it would find the nearest stove, ask \nit for a new ―cook food‖ action, and so on. \nLazy action chaining makes it possible to modify the chain based on what objects are \navailable to the agent. For example, a microwave oven might create a different ―cook food‖ \naction than a stove would, providing more variety and surprise for the player. Second, it \nmakes interesting failures easier. For example, the stove can look up some internal variable \n(for example, repair level) to determine failure and randomly push a ―create a kitchen fire‖ \naction instead. \nIn either case, using an action queue provides nice modularity. Sequences of smaller action \ncomponents are more loosely coupled and arguably more maintainable than standard state \nmachines. \nAction Chain State Saving \nWhen an action chain is interrupted, we might want to be able to save its state somehow so \nthat it gets picked up later. \nSince all actions are done on objects, one way to do this is to mutate the state of the object \nin question. For example, the progress of ―cleaning‖ can be stored as a separate numeric \ncleanness value on an object, which gets continuously increased while the action is running. \nBut sometimes actions involve multiple objects, or the state is more complicated. Another \nway to implement this is by creating new state objects. An intuitive example is food from \nthe original Sims: The action of prepping food creates a ―prepped food‖ object, cooking then \nturns it into a pot of ―cooked food,‖ which can be plated and turned into a ―dinner plate.‖ \nThe state of preparation is then embedded right in the world; if the agent is interrupted \nwhile prepping, the cut-up food will just sit there until the agent picks it up later and puts it \non the stove. \n \nDesign Consequences of Needs-Based AI \nWith the technical details of needs-based AI behind us, let‘s also consider some of the \ndesign implications of this style of development, since it‘s different from more traditional \ntechniques. \n\n\n \n \nFirst of all, the player‘s experience with this AI really benefits from adding some feedback to \nthe agents. Developers can just look at the internal variables and immediately see ―the \nagent is doing this because it‘s hungry, or sleepy, or other such.‖ But the player will have \nno such access and is likely to build an entirely different mental model of what the agent is \ndoing. Little bits of feedback, like thought bubbles about what needs are being fulfilled, are \neasy to implement and go a long way toward making the system comprehensible to the \nplayer. \nA second point is about tuning. Some of the tunable parameters have global effect and are \ntherefore very difficult to tune after the game has grown past a certain size. The set of \nneeds, their decay rates, score attenuation functions, and other such elements will apply to \nall characters in the game equally, so tuning them globally requires a lot of testing and a \ndelicate touch. \nIf a lot of variety is desired between different parts of the game, it might be a good idea to \nsplit the game into a number of smaller logical partitions (levels, and so on) and have a \ndifferent set of those tunable parameters, one for each partition. Ideally, there would be a \nset of global tuning defaults, which work for the entire game, and each partition could \nspecifically override some of them as needed. Partitioning and overriding tuning values buys \nus greater flexibility, although at the cost of having to tune each partition separately. \nThird, this AI approach tends heavily toward simulation and makes it hard to do scripted \nscenes or other triggered actions. Imagine implementing some actions on a trigger, such as \nhaving the agent approach the player when he comes into view. One might be tempted to \ntry to implement that using just needs and advertisements, but the result will be brittle. \nIf particular one-off scripted behaviors are desired, it would be better to just manually \nmanufacture appropriate action sequences and forcibly push them on the agent‘s action \nqueue. But in general, this overall approach is not very good for games that need a lot of \ntriggered, scripted sequences (for example, shooter level designs). Needs-based AI works \nbetter for simulated worlds than for scripted ones. \n \nConclusion \nNeeds-based AI is computationally very efficient; only a trivial amount of the CPU is \nrequired to pick what to do and to handle the resulting action sequence. The system‘s \ninternals are very easy to understand; by just inspecting the agent‘s internal needs values, \nyou can get a good idea of why it does what it does. And by externalizing the set of possible \nactions into the world, the system also achieves great modularity—the AI can be \n―reconfigured‖ literally by adding or removing objects around the agent. \nIn spite of unusual design consequences, the needs-based approach is very capable, easy to \nimplement, and effective at creating good characters. It‘s a powerful tool for many \nsituations. \n \nAcknowledgements \nThanks to Ken Forbus and Richard Evans, from whom I‘ve learned most of what I know \nabout this style of AI. \n \n",
      "page_number": 273
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 281-288)",
      "start_page": 281,
      "end_page": 288,
      "detection_method": "topic_boundary",
      "content": " \n \nReferences \n[Arkin98] Arkin, R. Behavior Based Robotics. MIT Press, 1998. \n[Forbus02] Forbus, Ken, and Will Wright. ―Simulation and Modeling: Under the Hood of The \nSims.‖ 2002. Northwestern University. n.d. <http://www.cs.northwestern.edu/~forbus/c95-\ngd/lectures/The_Sims_Under_the_Hood_files/frame.htm>. \n[Simpson] Simpson, Jake. ―Making The Sims the Sims.‖ n.d. \n<https://www.cmpevents.com/Sessions/GD/ScriptingAndSims2.ppt>. \n \n3.9. A Framework for Emotional Digital Actors \nPhil Carlisle \nzoombapup@gmail.com \nIn this gem, we will describe a framework that may be used to endow game characters with \nsome level of emotional behavior. Using a simple behavior tree implementation augmented \nwith supporting appraisal and blackboard implementations, we will demonstrate that \nemotions can be easily implemented and can enhance the behavior and expression of game \ncharacters. We use the term ―digital actor‖ because although the emotional system we \npresent is based on sound academic research in psychology and cognitive science, the \nintention is to produce the ―illusion‖ of emotion, rather than specifically trying to model \nemotion itself. However, the reader is advised to read [Oatley06] as an accessible \nintroduction to emotional psychology, which is a useful starting point for anyone trying to \nadd more emotion to their games. \nThe first section of this gem will describe models of personality, mood, and emotion derived \nfrom academic literature in the area. The second section will describe how these models are \nincorporated into an emotional framework. The third section will describe how the \nframework is used in a number of game scenarios. In conclusion, we will offer some ideas \nfor further work. \nModels of Emotion, Mood, and Personality \nIn addition to emotion, we need to represent both personality and mood in order to have a \ncomplete emotional framework. It is useful to consider these elements in terms of the \ntimescale required for change. \nTypically, our personality changes very slowly, if at all. It can take many years for our \npersonalities to alter. Mood, on the other hand, tends to change in a relatively shorter \nperiod of days or weeks. Finally, we have emotions, which are frequently expressed minute \nto minute and can often only be portrayed for fleeting seconds. For instance, it is not \nunusual to hear someone described as having a ―quick temper.‖ \nEmotion \nThere are many theories of emotion and models of personality associated with them. For \nexample, [Eyesenck65] describes models of personality and emotion often cited in academic \nliterature. One model of emotion used in a large volume of academic literature is the OCC \nmodel [Ortony88] created by Ortony, Chlore, and Collins. This model is discussed very well \nin [Bartneck02], and, as is the case with many academic implementations, we are going to \nuse a subset of the OCC model for our emotional representation. \n\n\n \n \nThe OCC model typically represents 22 discrete emotional categories. In practice, this is \nprobably too complex a model for most video games; thus, the model should be simplified \nbased on the requirements for a particular game. The OCC model, broadly speaking, breaks \ndown emotions into three categories: emotional reaction to objects, events, and agents. \nEach category is then broken down further depending on whether the emotion is related to \nself or other and whether the emotion is considered a good thing or a bad thing. The \ncomplexity arises because we have to account for the consequences for others and our \nrelationship with them. Consider the following case: \n \nAgent A likes apples. \n \nAgent B likes apples and Agent A. \n \nAgent C likes apples but not Agent A. \nIn a simulation with all three agents and one apple, assuming that Agent A acquires the \napple, we then have the following emotional reactions: \n \nAgent A is happy (due to acquiring the apple). \n \nAgent B is unhappy at not acquiring the apple but happy at Agent A acquiring the \napple. \n \nAgent C is unhappy at not acquiring the apple and is even less happy at seeing \nAgent A acquire the apple. \nThese are relatively simple direct relationships between goal (acquire apple, self/ friend) \nand emotion (happy, sad). But human emotion is a little more complicated. In this case, \nAgent C may immediately feel unhappy, but that feeling may then cause him to feel \nashamed that he was made to feel unhappy by the actions of A. Clearly, we are going to \nhave to simplify the model somewhat to be useful in a video-game context, but be aware \nthat sometimes the most interesting emotional expression comes from the feelings created \nby social situations exactly like this. \nMood \nMood is generally more changeable than personality. However it is often represented very \nsimply. A useful paper that represents mood simply is [Egges04], which maps mood to a \nsingle floating-point number in the range of –1..1 to represent negative or positive moods, \nrespectively. Typically, mood changes over weeks or months and acts as an overall bias that \nchanges as different emotional events occur. \nInterestingly, mood can color our perception of emotional events. For instance, people in a \nnegative mood may view all emotional events as negative, even if the event is generally to \ntheir advantage. In the case of the earlier example, a negative mood might be used within \nthe behavior tree to disable certain actions. For instance, if Agent A is in a highly negative \nmood, even the perception of an apple may not trigger the behavior tree action required to \nseek attainment of the apple. This can be accomplished only by adding knowledge of the \navailable apple to the agent‘s blackboard when it has a mood that is higher than a given \nthreshold, although care must be taken to allow the desire to maintain the agent‘s health \n(by acquiring food, for example) to override this behavior. See the description of the \nappraisal class later in this gem for further information. \nPersonality \nPersonality is often represented in the academic literature via the OCEAN model of \n[McRae96]. This personality model represents a person‘s personality across five dimensions \nrepresenting: \n \nOpenness \n \nConscientiousness \n \nAgreeability \n\n\n \n \n \nExtroversion \n \nNeuroticism \nThe values portrayed for each of the five dimensions are typically in the range of 0..1, \nwhere, for example, 0 for openness means that the person is fully closed off and +1 means \nthe person is fully open. Essentially, the model of personality provides a default biasing \nmechanism for further formulas and allows us to represent different personality types by \nsimply configuring each dimension with different default floating-point values. \nPersonality is the least changeable part of our emotional makeup, so it is reasonable to \nsimply store personality as a series of fixed values. Personality provides bias toward a \nparticular set of possible actions within the behavior tree. Referring to the earlier example \nscenario for Agent B, given the choice of two potential actions of either trying to obtain the \napple for oneself or allowing Agent A to obtain the apple, the personality model can be used \nto select from the two choices. An agent with high agreeability would choose to allow Agent \nA to obtain the apple, whereas the converse would simply try to obtain the apple for itself. \nThus, the personality model allows us to create unique behavior for each agent without the \nneed for per-agent behavior trees. \n \nThe Emotional Framework \nGiven the emotional model described in the previous section, we need to be able to \nincorporate code that represents the model within an architecture that enables it to affect \nour characters‘ behavior. In this example, we will incorporate the emotional model by \nimplementing an appraisal class, which modifies values within a character‘s blackboard. \n(See [Isla02] for information concerning blackboards.) The blackboard will be inspected by \na simple behavior tree in order to incorporate the emotional values within the characters‘ \nupdate logic, and the same mechanism can also be used to incorporate the emotional \nvalues with the characters‘ movement logic—for example, animating with a sullen walk \ncycle if the emotional mood is negative. A very good reference concerning the use of \nbehavior trees and emotional models is the work of the Oz project group at Carnegie Mellon \nUniversity [Reilly96], which went on to be used in the game Façade. \nFigure 3.9.1. The emotional framework architecture. \n \n \n\n\n \n \nAppraisal/Arousal \nTypically, emotion is broken up into appraisal, where goals are created and events and \nobjects are classified, and arousal, where the magnitude of the reaction to the sensory input \nis processed. The purpose of the appraisal class is to map any sensory input to changes in \nthe agent‘s emotional variables and blackboard data. In our framework, for convenience this \nclass handles both appraisal and arousal. During appraisal, sensory input and event data \nare fed to the appraisal class, which then determines the appropriate changes in blackboard \ndata. During the appraisal, data may be added to the knowledge representation, changes \nmay be made to the emotional variables, and in certain cases input may be ignored. \nSimilarly, the appraisal class is responsible for determining the changes occurring from the \nsuccess or failure of the agent‘s goals, which may again alter the agent‘s blackboard or \nemotional variables. \nKnowledge Representation \nThere is a choice to be made with regard to the appraisal class relating to its usage of \nmemory and dynamic data structures. In a human context, we are capable of learning \nabout new objects we have not encountered before, new events that happen, or new agents \n(human or otherwise) that we meet. This implies that we are able to store information as \nwe build up an emotional picture relating to these objects, events, and agents. In a game \ncontext, we may or may not be able to spare the memory to process previously unknown \ninformation. \nThe easiest case for implementation and perhaps the more robust case for design is that we \nexplicitly determine all possible known objects/events/agents a priori and simply load that \ninformation at run time. This is the method chosen for the example implementation. An \neasy method of expanding on this simple implementation is to incorporate a classifier \nsystem such that instead of storing a reaction to an individual object/event/agent, we \nclassify them and store a reaction to the ―class‖ rather than a specific instance of the class. \nAn alternative method is to simply place an upper limit on the number of memories stored \ndynamically, allowing new memories to be created. These memories would then expire over \ntime to allow further memories to be formed. In addition, this scheme may be extended \nwith different limits on the amount of memories stored for each type of event, thus allowing \nfor more ―important‖ memories to be stored. The use of a blackboard allows us to store all \nper-agent data in a generic structure that allows access from all of our actor‘s systems. \nTypically, this is used to store agent goals or attributes, such as the currently selected \ntarget enemy. However, the blackboard is a useful storage mechanism to employ for the \ncreation of emotional agents. \nIt is worth bearing in mind that the most common case of retrieval from the blackboard—to \nretrieve a particular value associated with a new sensed object/agent/ event—must be as \nefficient as possible. An alternative structure, such as a semantic network [Sowa92], may \nprove to be a far better solution, if slightly more complex in terms of code. \nFor convenience, each agent‘s blackboard and behavior tree configurations are parsed from \nXML data. This allows for run-time configuration of each agent, using a unique blackboard, a \nunique behavior tree, or both. \nXML data for an example blackboard: \n<BB> \n<Personality O=\"0.1\" C=\"0.6\" E=\"0.1\" A=\"0.9\" \nN=\"0.2\"></Personality> \n<Mood default=\"1.0\"></Mood> \n<Event name=\"PickupApple\"   valence=\"1.0\">   </Event> \n<Event name=\"PickupOrange\"  valence=\"-1.0\">  </Event> \n\n\n \n \n<Event name=\"PickupGrenade\" valence=\"-10.0\"> </Event> \n<Object name=\"Apple\"        valence=\"1.0\">   </Object> \n<Object name=\"Orange\"       valence=\"-1.0\">  </Object> \n<Object name=\"Grenade\"      valence=\"-10.0\"> </Object> \n<Agent name=\"Fred\"          valence=\"2.0\">   </Agent> \n<Agent name=\"Wilma\"         valence=\"3.0\">   </Agent> \n<Agent name=\"Betty\"         valence=\"30.0\">  </Agent> \n</BB> \n \nHere we can see a simple blackboard specification for an agent. Each sub-element with a \ntag of <agent>, < object>, or <event> defines a unique structure that is stored within \nthe blackboard. Each element is stored internally as a simple stl::vector of the \nappropriate type. \nThe Appraisal Process \nTo modify our behavior based on the emotional framework, we need to consider the steps \nthat occur when new emotional responses are required. \nSensing a New Object \nAn agent‘s perception system typically responds to queries instigated by its behavior tree \n(which we will refer to as a BT for brevity‘s sake). For example, the BT may have executed \na sequence of nodes that resulted in a query for the availability of nearby food. Our goal in \nthis case is to determine the agent‘s emotional reaction to each sensed object—specifically, \nthe more useful case of allowing the agent to determine the selection of which food object \nto try to obtain based on the emotional reward associated with those available. \nConsider the case where a query returns three different food items within the query radius. \nIn the simplest case, we can simply determine our like/dislike of the available food items \nbased on a simple classification, such as whether the item is fruit or vegetable or whether \nthe item is sweet or sour. In the ideal case, we need to consider our past experiences either \nwith the unique object or with objects with a similar classification. It is beyond the scope of \nthis gem to discuss the intricacies of human memory and its method of classification. \nAnother aspect of this pattern of memory is that the relative novelty of an object can \ngreatly alter the intensity of the reaction to the object. An agent who is unused to seeing \nguns may react significantly to the sight of an armed friend, whereas a gangster would be \nless likely to have a similar reaction. The final aspect we should consider when dealing with \nobjects is the penalty or reward associated with interactions. For example, an agent may \nhave a strong liking for apples, but if the agent consumes an apple that is sour, it should \nhave some effect on the subsequent desire for more apples. \nIn practical terms, in the case of our object queries, we first use the appraisal class to \nreturn preference values for each object in turn. We then simply choose the object with the \nhighest appraisal value for interaction. Once an object has been selected, we store that \nobject as a goal within the blackboard as an object to obtain. It is important to constrain \nour memory usage at this point, as new objects may be perceived quite often and marked \nfor attainment. We can achieve this by attaching an expiration value to each new object \nattainment goal. The blackboard then removes all expired goals within its update loop. \nOnce an object is attained (typically via another node in the behavior tree), we then \nconsider any attainment goals relating to the object. If a specific attainment goal is found \nwithin the blackboard, we then consider the arousal value of achieving the specific goal. \nThis arousal processing takes into account the goals for attainment of the object. There are \ntwo major reactions to consider here. The first is that the agent must consider his own \nreaction to the object. Typically, we would try to obtain objects we like, but if the game \n\n\n \n \nallows attaining of objects by other means—for example, by allowing agents to simply give \nobjects to each other—there may be negative consequences. An agent who obtains a ticking \ntime bomb should definitely not be happy about its attainment. The second reaction to the \nattainment of an object is with respect to other agents. An agent who obtains an object that \nis highly desired by another agent, depending on whether the other agent is liked or \ndisliked, may feel guilty or happy for acquiring the item, respectively. \nAlternatively, if no specific attainment goals are stored in the blackboard, we can simply \nconsider the attainment goals of other agents for the object, or we can consider the general \nvalence of the object and react based on our positive or negative feelings about it. For \nexample, an object that is attained may allow us to accomplish a goal for another agent if \nwe give it to him. In this case, we may simply decide to create a goal to pass on the object \nif it achieves a goal of an agent we have positive affect towards, or we may decide to keep \nor dispose of the object if it denies a goal of an agent we have negative affect towards. \nSensing a New Agent \nThe term ―agent‖ in the OCC model does not describe an AI agent, but instead describes an \nagent of change. Typically, these are often other characters in a game context. However, \nthis is not always the case, and an agent in OCC terms may be some other external force \nthat has an effect on the world. In the most common case, the agents in our emotional \nframework will actually describe characters within the game world. With this in mind, we will \nuse the term ―agent‖ to mean both the OCC model of an agent and the AI game character \nagent. \nModeling of inter-agent affect affords us some unique social interactions, such as the \nseemingly altruistic act of passing on an object. The most obvious use of agent knowledge \nwhen considering emotion is for a like/dislike evaluation. This can be simply stored as a \npositive or negative value associated with a unique identifier. In the example framework, \nagents are stored by name with a valence value associated with them within the \nblackboard. This value is useful for when any opportunities are presented to the agent, such \nas denying another agent a resource or being able to give another agent something they \nrequire. \nThis brings up an important point about agent-to-agent interactions. Typically, when \nhumans interact, they create mental models of the motivations of the interacting agent in \norder to determine how to proceed with the interaction. For instance, when a human \nconsiders giving a gift to another, they try and imagine the reaction to the gift the other \nperson will have, using this as a method of deciding whether to proceed with the \ninteraction. This agent mental modeling is important for social interactions; however, it is \nproblematic for games because of the amount of memory and processing time required for \nimplementation. The problem is compounded by the notion that humans often model other \nhumans‘ models of themselves (in other words, how does this person feel about us?). In the \nexample framework, we have decided to leave this mental modeling unimplemented for the \nsake of brevity and for practical purposes. However, for a truly deep social simulation, this \nmodeling is a highly desirable feature. \nSensing an Event \nA great deal of an agent‘s behavior will generally stem from sensing some event that occurs \nwithin the world. This event could be an object attainment event, for self or for other. It \nmight also be an important event that requires immediate action, such as hearing a grenade \ndrop at the agent‘s feet. In this situation, the appraisal class uses its understanding of \navailable events to create the associated knowledge within the agent‘s blackboard. \nIn the case of the grenade event, the appraisal class simply adds a threat object with a high \nreaction value to the blackboard. This will then allow the BT that has a branch ―respond to \nthreat‖ to take the appropriate actions. The reason why it is useful to pass events via the \n\n\n \n \nappraisal class is that the importance of an event can change over time as an agent \nresponds to more of the same event. Consider the case of the grenade event. There is a \nradius within which we can expect to take damage, but outside of that radius, the reaction \nto the grenade can change depending on how many times we have seen grenades explode. \nIf we know from experience that outside of a certain radius we may sustain injury but that \nthe injury is entirely random, then we may, over time, be conditioned to simply block the \ngrenade from thought. This effect can happen in any stressful situation where our emotions \nallow us to regulate our reactions, essentially becoming ―numb‖ to what would normally be \nhighly stressful situations over time. This change in attitude toward events is due to the \nappraisal/arousal process. Essentially, this is a feedback loop that changes an agent‘s \nresponse over time as the agent adds positive or negative arousal to the event depending \non the event‘s outcome; in addition, this effect is subject to some decay in the dulling of the \narousal. In essence, this means that events that occur frequently become less arousing \nemotionally, but that if the event has not occurred for some time, the arousal may once \nagain be relatively high. In the example framework, this dulling of arousal values for events \nhappens by a simple scaling factor being applied to the arousal value for the event. Over \ntime, the scaling factor is reset to 1.0, with any events of that type causing the scale factor \nto reduce slightly. Thus, over time, the arousal associated with the event can move between \n0..1 depending on the frequency of the event. \nAnother aspect to the appraisal class is the incorporation of personality and mood into the \nemotional outcomes expressed in the blackboard. In effect, personality and mood modulate \nthe intensity of the emotional reaction to any given stimulus. For a good introduction to why \nthis is important, see [Eckman04]. To correctly simulate the effect of personality, mood, \nvarying arousal, and decay, we therefore apply different functions for calculating the effect \nof any given emotional stimulus and then apply the results of these functions to the agent‘s \nblackboard. For event stimulus, the first role of the appraisal is to determine whether to \nrespond to the event at all. Some events can simply be ignored, especially when in a state \nof high overall arousal. For instance, if a grenade event is perceived, any subsequent event \nis blocked from being perceived until the agent has dealt with the response. This simulates \nthe effect of our emotions, which act as regulatory systems allowing for rapid response to \ndangerous threats. Given a relatively low state of overall arousal, we can often respond to \nrelatively minor events. \nThe personality model acts to bias available choices within the behavior tree. Given two \npossible outcomes for any sensory input, we can use a scale factor based on the personality \nvariable associated with a given choice to determine which outcome has a higher priority. \nFor example, when given the choice to interact in a conversation with another agent or to \nobtain a required item, an agent with an introverted personality would choose the latter. \nThe mechanism for this choice involves classifying each behavior tree selection with respect \nto personality and then using this as a scaling value when doing priority selection, each \nchoice essentially scaling its priority up or down based on the personality trait variable \nvalue. \nThe model of mood, although simplistic in nature, allows us to further apply some filtering \non the selection of available behavior tree choices. The mood value is initially used in the \ninput phase of the appraisal class. This simply scales the emotional valence of input senses, \nwhich may cause some sensory input to be ignored when it otherwise may have been acted \nupon. For example, if we perceive an object that is beneficial to another agent, we calculate \na valence for the goal of attaining the apple for the other agent. Normally, we would then \ncreate the goal for the attainment of the apple by adding the apple to the blackboard. \nHowever, when the mood is negative, the positive affect generated by attaining the apple \nfor the other agent is cancelled out, and we simply never add the apple to the blackboard. \nGiven the framework described thus far, what does a typical update loop look like for the \nagent? See Figure 3.9.2. \nFigure 3.9.2. Agent update loop. \n\n\n \n \n \nThe update loop begins with sensory input. In practical usage, the event input can happen \nat any point in time, but it follows a very similar cyclic pattern represented by the sensory \nupdate loop represented in Figure 3.9.2. Input is fed into the appraisal class for processing. \nThis appraisal class is used to then map the input into changes in the agent blackboard. The \nappraisal may add or remove goals within the blackboard depending on the input. It also \nmay affect the emotional values associated with agents, objects, or events stored within the \nblackboard, which in turn may affect the processing within the appraisal class during the \nnext update cycle. Then the behavior tree responds to new knowledge within its blackboard \nand in turn effects changes in the game world. \nThe cycle is then repeated continually. This reactive framework simply represents a sense > \nthink > act cycle that is enhanced with emotion to become a sense > feel > think > act \ncycle. The blackboard may also be interrogated by animation and locomotion systems. For \nexample, to alter walk-cycle blending to allow for a display of mood, an agent with a \nnegative mood value may blend in more of a labored walk cycle. In comparison, an agent \nwith a positive mood may blend more of a bouncy walk cycle. \n \nConclusion \nEmotion is a very complex subject, and there are many academic theories that attempt to \ndescribe how emotions work and how they might be classified. Video games are unlikely to \never completely simulate the entire spectrum of emotional responses, even if it were \ndesirable to do so. As game designers and developers, we can incorporate simple models of \nemotion in order to add some personality to individual agents. \nNon-verbal communication is an important part of human social interaction [Mehrabian72], \nand emotional values modify this communication. For instance, we tend to hold gaze on \nagents we are attracted to for a lot longer. A large part of the motivation for the \nincorporation of emotion into video games is that we can start to add non-verbal \ncommunication signals to our agents. This helps to create agents that feel more realistic \nand alive. Emotions can be used to modify things such as posture, gesture, gaze, gait, \nbehavior, and memory. Imagine a world where agents remember your actions and are \npositively brimming with joy when you come to visit them! Imagine a world where you can \nunderstand simply from the posture of an agent whether it is happy to see you or intends to \ndo you harm. To learn more about non-verbal communication and how it works in humans, \nsee [Argyle75]. \nAs more memory and processing time become available on newer platforms, we may begin \nto consider deeper models of agent emotion and memory, which in turn should lead to a \nmore effective display of an agent‘s emotional state. This emotional display should lead to \n",
      "page_number": 281
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 289-299)",
      "start_page": 289,
      "end_page": 299,
      "detection_method": "topic_boundary",
      "content": " \n \nmore engaging and believable characters—agents that can express themselves and their \nemotions non-verbally and engage the emotions of players at a deeper level. \n \nReferences \n[Argyle75] Argyle, M. Bodily Communication. Methuen & Co Ltd., 1975. \n[Bartneck02] Bartneck, C. ―Integrating the OCC Model of Emotions in Embodied \nCharacters.‖ Workshop on Virtual Conversational Characters, 2002. \n[Eckman04] Ekman, P. Emotions Revealed. UK: Phoenix Books, 2004. \n[Egges2004] Egges, Arjan, Sumedha Kshirsagar, and Nadia Magnenat-Thalmann. ―Generic \nPersonality and Emotion Simulation for Conversational Agents.‖ Computer Animation and \nVirtual Worlds 15.1 (2004): 1–13. \n[Eysenck65] Eysenck, H. J. Fact and Fiction in Psychology. Harmondsworth: Penguin, 1965. \n[Isla02] Isla, D., and B. Blumburg. ―Blackboard Architecture.‖ AI Game Programming \nWisdom 1. Boston: Charles River Media, 2002. 333–344. \n[McRae96] McCrae, R., and P. T. Costa Jr. ―Toward a New Generation of Personality \nTheories: Theoretical Contexts for the Five-Factor Model.‖ The Five-Factor Model of \nPersonality: Theoretical Perspectives. Guilford Press, 1996. 51–87. \n[Mehrabian72] Mehrabian, A. Non-Verbal Communication. Transaction Publishing, 1972. \n[Oatley06] Oatley K., D. Keltner, and J. Jenkins. Understanding Emotions (2nd edition). \nBlackwell Publishers Inc., 2006. \n[Ortony88] Ortony, A., G. Clore, and A. Collins. The Cognitive Structure of Emotions. \nCambridge University Press, 1988. \n[Reilly96] Reilly, W. S. ―Believable Social and Emotional Agents.‖ PhD Thesis. School of \nComputer Science, Carnegie Melon University, Pittsburgh. 1996. \n[Sowa92] Sowa, J. ―Semantic Networks.‖ n.d. John F. Sowa. 15 Sept. 2009. \n<http://www.jfsowa.com/pubs/semnet.htm>. \n \n3.10. Scalable Dialog Authoring \nBaylor Wetzel, Shikigami Games \nbaylorw@ShikigamiGames.com \nIt has been a goal of many a game to create a large city filled with people you can talk to. \nNot an inn or castle or a small town, but a city. A big city filled with hundreds (or \nthousands, or more!) of agents, each of which acts like an individual. But there‘s a reason \nwe fill shopping malls with zombies and countrysides with monsters but not cities with \npeople—creating hundreds of people, each with their own personality, takes a lot of time…a \ncost-prohibitively long time. There won‘t be games with large spaces truly filled with \nintelligent, conversational non-player characters (NPCs) until we find a way to create these \n\n\n \n \nagents more efficiently. Although the techniques in this gem don‘t try to tackle every \nproblem and bottleneck that you‘ll encounter in building a dialog system, hopefully they will \nhelp you create large groups of agents much faster. \nConversational Agents Today \nGames are filled with characters that talk. Although not every game and every character \nneeds to be able to answer questions or carry on a conversation, conversational ability is \nimportant to a wide variety of games. Dialog varies from the ―select a topic‖ approach used \nin the Elder Scroll series (where one scenario involves convincing a love-addled stalker to \ngive back the item he stole from a woman who would not go out with him), to the deep \nconversational trees of the original Fallout, to the multi-way conversations of Planescape: \nTorment, to the jury trials in Jade Empire. NPCs in these games are normally more complex \nthan NPCs in other games. They might refuse to discuss a given topic with someone they \ndon‘t know, ignore someone they previously argued with, insult someone from a rival \ngroup, or yell at someone trying to strike up a conversation in the ladies‘ bathroom. \nConversations can lead characters to give up their evil plans, join the player‘s team, or \nreveal the secret of their miniature giant space hamster. \nThe work described here is part of research done at Alelo, which makes ―serious games‖ \ndesigned to teach foreign languages and cultures. Many (though not all) of these games are \nused to train soldiers how to perform tasks overseas. These tasks range from manning \ncheckpoints and conducting house-to-house searches to negotiating with local leaders and \nhelping set up clinics. Success often depends on showing the proper level of politeness and \nprofessionalism, earning trust through culturally appropriate small talk and asking the \ncorrect questions in the correct way. In this gem, we‘ll use the example of a U.S. soldier in \nIraq to explain the techniques. \nTypical Methods for Building Conversational Agents \nThere are a few ways to make conversational agents, one of the more common (and \npainful) ways being to build them manually in script (if (1==option) \nbobDialog42() else…). An easier approach is to use a dialog editor to build a tree, \nwhere one node is what the NPC says, the nodes under that are things the player can say in \nresponse, the nodes under those are the NPC‘s response, and so on. Each node typically \ncontains the exact text the agent will say. You say, ―Do you like football?‖ and the NPC will \nreply ―Sure, who doesn‘t?‖ An NPC might have several responses based on whether they \nlike you, if you have fulfilled a quest for them, if you are both at a bar, and so on. In \nNeverwinter Nights, this is done by calling the TextAppearsWhen script, and in the Elder \nScrolls editors (including Fallout 3 ‘s G.E.C.K. editor), it‘s done by checking the \nConditions field, but the idea is the same—for every possible dialog option, the designer \nwrites the input text (player choice), the output text (from the NPC), and for each possible \noutput writes a script (either by hand in NWN or using a spreadsheet-like tool in Elder \nScrolls) to determine whether that particular output should be used. If not, the game \nchecks the next output in the list. The results can be very good, but it takes a lot of time, \nthought, and planning to build. \nThe Scalability Problem \nLet‘s start with a positive—the range of dialog that can be created by current techniques is \nessentially perfect. If you want an NPC‘s response to change based on the player‘s shoes, \nintelligence, the last enemy they fought, the health of the NPC‘s dog, and the phase of the \nmoon, you can do that. The problem is that it‘s going to take you a long time. \nThis isn‘t the only problem. Because of the sheer volume of data, designers face the \nproblem of covering the whole possibility space—you handled a lot of the possible variable \ncombinations, but did you get all of them? In Fallout, NPCs asked about quests that have \nlong since been completed. In Mass Effect, the person sitting next to you will calmly inform \n\n\n \n \nyou that they‘ve picked up a communications signal right rather than ask why you‘ve just \ndriven off a bridge into a bottomless chasm. In Neverwinter Nights, you can rescue a girl \nfrom a giant, go to the girl‘s farm, kill her family, then talk to her, and she‘ll thank you and \nask you to visit again soon. \nFinding and correcting problems like this isn‘t hard, it just takes time. How much time? To \nbuild a professional dialog tree, you not only have to decide which topics an agent can \ndiscuss, the words to use, and the flow of the conversation, you need to think about all the \nfactors (NPC personality, NPC culture, NPC state, world state, player state, conversational \nhistory, history with player, and so on) that should affect a conversation and make sure the \nNPC reacts appropriately. For one of the games I worked on in 2009, creating the full dialog \ntree for an important NPC took two to three weeks. Even then, the agent had the standard \nlapses in awareness and limited conversational ability that you find in any game. That \nparticular game was a bit more complex than most, but creating a decent conversational \nagent in any game is still measured in days and weeks, not minutes or hours. \nThe required effort influences how games are made. It would take a small team of designers \nworking on nothing but conversations a full year to make 100 individual (non-cloned) NPCs, \nand those NPCs would still suffer from limited conversational ability and situational \nawareness. Assuming one wishes to at least break even on their game, it is essentially \nimpossible to make large worlds filled with unique, believable agents using current \ntechniques. As a result, time and money force those worlds to be filled with a handful of \nhigh-quality NPCs (those that drive the plot) and dozens to hundreds of generic NPCs with \nno or almost no conversational abilities at all. \nUnique Personalities and Other Things We Might Want \nOur primary goal is to reduce the time it takes to create a conversational agent. The \nassociated goal is to reduce the cost to create a single agent, allowing us to reduce the cost \nof making the game or to create significantly more agents for the same cost. (This gem \nfocuses on the latter.) \nWe have already said we want agents with better conversational breadth and situational \nawareness. (In other words, responses are based on the NPC‘s personality, state, job, \nculture, feelings toward the player, and so on.) Another desirable trait is realistic \nuniqueness—characters in the game are roughly as diverse as people in the real world. \nWhich highlights the problems of a common technique—making a few high-quality NPCs (or \ndialog trees) and cloning them. Using templates (―Hi, my name is %this.name; I live here in \n%this.city‖), you could fill a world with hundreds of agents who knew some basic \ninformation about themselves but who all acted the same (or behaved like one of a handful \nof personality types). What we want are people who are realistically unique—based on who \nthey are, two agents will give different answers when it makes sense and the same answer \nwhen it makes sense. \nAnother desirable feature (already present in some games) is for the player to be able to \nchange an agent‘s attitude and behavior toward them. Scenarios often require the player to \nearn the trust of an NPC. Likewise, bad behavior on the player‘s part should have \nconsequences. Being able to win an agent‘s trust is often the key to a mission, and being \nable to make someone hopping mad is simply fun. \nCulture describes how a group of people behave in certain circumstances. For example, it \nmight be considered rude to ask an Afghani man about his wife, refuse a cup of tea in Iraq, \nor ask a first-level character about their flying mount. If you‘re dealing with a large number \nof cultures (groups, roles, character types, and so on), the sheer volume of dialog data \nmakes it hard to verify that agents behave consistently or behave the way the lead designer \nrequested. For serious games, where the behavior often has to be evaluated by an \neducational expert and/or people from the culture being modeled, unless those people are \nalso game programmers, this is a serious problem. \n\n\n \n \nFormat is also a problem. All of the behavioral information can be captured in the standard \nscript and tree structure of most dialog systems, but if the knowledge is explicit (say, a \nspreadsheet that focuses on behavior rather than wording), it‘s easier for an expert to \nreview (and author) the information. It‘s much harder to bring in a group of people from \nthat culture and ask them to review the information if the information is scattered across \nhundreds of script files. So another desirable feature is the ability to explicitly describe a \nculture or group of people. \nA benefit of an explicit cultural representation is that it allows another desirable feature—\nplug-and-play cultures. If the culture of the NPCs could be swapped out with other cultures, \nmaking a new city filled with conversational agents would be as easy as cloning an existing \ncity and swapping the culture, which meets both the goal of being fast (and cheap) and the \ngoal of the NPCs being realistically different. \nEditing dialog must entail an easy-to-understand workflow (not requiring a Ph.D. to use it). \nIt needs to be data driven in a way that makes it quick to create easy-to-use tools, as well \nas quick to write unit tests for the system. \nA final thing is something we don‘t want—the tool should not preclude designers from being \nable to do things they can do now. An example is a tool that uses psychological data to \ngenerate realistic behavior but doesn‘t allow the designer to override that behavior. While \nrealism is often nice, it is more important that designers be able to achieve the behavior \nthey want. In entertainment games, realism must sometimes be sacrificed to fun or moving \nthe plot along. In educational games, characters must sometimes do things to further the \neducational goal, such as a character correcting rather than overlooking an error, or leading \nthe player to the correct behavior rather than harshly punishing them. \nWhat We Won’t Cover \nIt likely comes as no surprise that a gem of this length will not cover every aspect of \nconversational agents. The focus of this gem is on intention planning, which means deciding \nhow you want to respond to a topic. We‘ll use topics, concept trees, response types, trust \nlevels, rapport modifiers, temperament stats, explicitly modeled cultural groups, sets of \nsparse culture wrappers, and a bit of memory to help decide when we should answer a \nquestion, feign ignorance, or insult the speaker‘s mother. \nWhat this gem doesn‘t cover is realization, the actual words that come out of the NPC‘s \nmouth. In the old days, this was a simple problem—if the designer decides the NPC should \ninsult the player, the response type Insult would map to one or more insults. If the NPC‘s \nintent is Answer, the response type and topic can be used to look up a specific answer, \nwhich might be specific to that character or used by the entire world. Using the techniques \npresented here, if the designer decides halfway through the project that all 300 guards in \nthe game need to be able to discuss bunnies or Pre-Raphaelite poetry with complete \nstrangers (but still act stuffy around people they actively dislike), the change can be made \nin a few minutes or less. \nBeing able to add entirely new responses or whole topics to hundreds of NPCs in a matter of \nminutes is a nice feature and offers all sorts of dreams of large, expansive dialog-filled \nworlds. Unfortunately, these days, things are a little more difficult. Most high-end games \nnow use voice actors, meaning each statement a designer adds to an NPC must be \nrecorded, a slow and expensive process. In these situations, voice recording, rather than \ndesigner creativity or predicting variable combinations, becomes the bottleneck. The topic of \nfaster, cheaper speech won‘t be covered in this gem, but it is worth noting that even when \nthe set of lines an NPC can say are fixed, considerable time must still be spent mapping the \nplayer‘s input to the NPC‘s output. The techniques presented here can help you more \nintelligently (and quickly) build those mappings. \n \n\n\n \n \nOverview \nThe goal of this gem is to describe a way to scale how one authors conversational agents. \nCurrent systems typically use hard-coded input-output mappings annotated with gateway \nscripts to decide which of the hard-coded responses to use. The system described here uses \na variety of techniques, but at the core, it tries to break the hard-coded links and replace \nthem with abstractions. Rather than linking the player‘s input directly to the NPC‘s output, \nwe use the player‘s input to determine the NPC‘s intention and then use the intention to \nselect the NPC‘s behavior. \nAll inputs are mapped to a Topic. The Topic is checked against the NPC‘s \nCulturalGroup and current level of Trust toward the player to determine a \nResponseType. Topics belong to a topic hierarchy, so if there is no match on the Topic \nin the NPC‘s dialog specification, the system moves up a level and checks for a \nResponseType to the parent Topic. \nCulturalGroup is a sparse set of {Trust-Topic-ResponseType} mappings. \nCulture represents not just nationality, but any group membership that affects how the \nagent will respond to a topic. An agent can (and almost certainly will) belong to multiple \ngroups. Groups are prioritized, and conflicts are resolved by selecting the highest-priority \nmatching group. \nThroughout this gem, we‘ll use the example of a U.S. soldier (the player) in an Iraqi city. \nTable 3.10.1 lists seven NPCs the player might interact with—a typical civilian, a policeman, \na policeman secretly working for the insurgents, another U.S. soldier, an insurgent \n(although the player doesn‘t know this), and two doctors. \nTable 3.10.1. NPCs in an Iraqi City \nName: Nori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nBio: \nIraqi Man Policeman Policeman U.S. \nSoldier \nInsurgent Doctor \nDoctor \nRoles: Iraqi \nPoliceman Zuhair \nScott \nSoccerFan AidWorker \nGovRep \n  \nSoccerFan GovRep \nGovRep \nSoldier \nInsurgent GovRep \nAidWorker \n  \nPerson \nIraqi \nInsurgent USCitizen \nIraqi \nIraqiFemale IraqiFemale \n  \n  \nPerson \nPoliceman Person \nPerson \nIraqi \nIraqi \n  \n  \n  \nPerson \n  \n  \nPerson \nPerson \nTrust: 0 \n0 \n–2 \n2 \n–6 \n0 \n0 \n \n \n \nIntention Modeling \n\n\n \n \nImagine a game in which there are 100 NPCs, and the player can insult any of them. How \nwill they react? Most will return the insult in a dozen different ways, based on their \npersonality, intelligence, culture, and preferred insult. Many will ignore the player. A few will \nattack. There are potentially 100 actual actions or phrases that might be used, but (in our \nexample) there are only three intentions (insult, ignore, attack). In most games, the \nplayer‘s action (the input) is hard-coded directly to the NPC‘s behavior (the output). The \nresult is that the designer must write thousands of pairings, such as {input:Hear(―Do \nyou like films about gladiators?‖), output:Say(―Get away from \nme, weirdo.‖)}. Much of this work is redundant—the same output is used for multiple \ninputs, and the same pairings are used in dozens of NPCs. If it is decided that this behavior \nis no longer desired (say, if designers decide late in the process that elves, unlike dwarves \nand humans, never insult others), the dialog pairings must be tracked down across dozens \nor hundreds of scripts or dialog files and changed. \nDoing duplicate work is not only inefficient and hard to maintain, it‘s not fun. To make the \ndesigner‘s life easier, we‘ll have them map inputs to intentions (a much easier task) and \nseparately map intentions to behaviors. \nTopics, Response Types, and Trust \nIn our approach, for a given culture (which we‘ll discuss a little later), a Topic and Trust \nlevel are used to select ResponseType. Topic is the subject the player is asking about \n(swords, rumors, dragons, and so on). In this gem, we‘ll assume that Topic is the only \ninput. While this is sufficient for most current video games, more demanding games will \nlikely use a more complex input consisting of an action (asking a question, demanding, \ngreeting, complimenting, insulting, and so on), a topic, and possibly some metadata (vigor, \npoliteness, and so on). The type of input is irrelevant to the rest of the system, so we‘ll \nkeep things simple and assume Topic is the sole input. \nThe ResponseType represents the responding NPC‘s intention. Possible values include \nAnswer (give the player the information they‘re looking for, if possible), Refuse, Evade, \nChangeTopic, VagueAnswer, Lie, Ignore, Insult, Threaten, Correct (if the \nplayer has made a mistake in what they asked for; this is more useful in educational \ngames), PositiveLie (say something is great, regardless of whether it is), \nNegativeLie, and Custom (e.g., attack). More complicated (and academically \nrespectable) schemes exist, but this works well for our purposes. \nThe actual words spoken by the NPC are based on the ResponseType. Suppose the player \nhas been told that a bomb has been placed in the market. The player stops a person on the \nstreet and asks for directions. Table 3.10.2 shows how the NPCs defined in Table 3.10.1 \nmight react. Five of the seven will attempt to answer the question (although one, Scott, \ndoes not know the answer). Shakir, an insurgent, will insult the player, while Zuhair, a \ncorrupt cop, will lie to the player, sending him away from the market. (Note that the \npoliceman does not necessarily know there is a bomb and wants it to explode, he merely \ndislikes the player; if the policeman knew about the bomb, he could be made to respond \ndifferently using a context modifier, but that is outside of the scope of this gem.) \nTable 3.10.2. What an NPC Says Depends on Their Intention \nPlayer asks: Where is the market? \nlying \nName: Nori \nAnwar \nZuhair Scott \nShakir \nSuha \nHalema \nRole: \nIraqi \nIraqi \nIraqi \nUSCitizen Insurgent Iraqi \nIraqi \n\n\n \n \nTable 3.10.2. What an NPC Says Depends on Their Intention \nPlayer asks: Where is the market? \nlying \nIntent: Answer Answer Lie \nAnswer \nInsult \nAnswer Answer \nSay: \nLeft \nLeft \nRight Not sure Pig \nLeft \nLeft \n \nTrust is the amount of trust the NPC has in the player. It‘s what ties the Topic to the \nResponseType for that cultural group. This attribute does not have to be Trust— it \ncould be rapport or some combination of other attributes, although trust as a single value \nworks well in most instances. The important thing is that there is an attitudinal value that \nunambiguously ties an input to a ResponseType. \nUsing the example in Table 3.10.1, let‘s assume that the player has insulted Anwar the \npoliceman. Let‘s measure trust from –10 (distrust) to 10 (full trust). Topic=insult, \nCulture=Police, and Trust=0. We have the rules (evaluated in order): \n{Trust >=  5, Ignore} \n{Trust >=  0, Insult} \n{Trust >= -7, Threaten} \n{Trust <  -7, Custom:Attack} \n \nWhen the player insults Anwar, Anwar will decide to insult the player. Assuming an insult \nlowers Trust by 1, if the player insults Anwar again, Anwar will threaten him. If the player \nkeeps it up, Anwar will eventually attack. \nKnowing that an NPC will insult the player does not automatically determine what the NPC \nwill do. The behavior generation system might be as simple as mapping \nResponseType=Insult to Say(―Oh yeah, your momma.‖). The intent is mapped \nat the group level (all policemen), but the behavior could be different for each individual \npoliceman. Each NPC could have his own favorite insult. Insults could be chosen based on \nthat NPC‘s intelligence. They could be based on the player‘s class, how they‘re dressed, or \ntheir location (sports arena, store, and so on). This decision is made independent of the \nintention system. \nSeparating intention from behavior has several important implications. First, separate \ndesigners can be assigned to intent (say, someone familiar with personality or social \npsychology) and realization (for example, a writer). Second, because it‘s a smaller set of \ndata and explicit in its goals, it is easier for one person to view and correct the data \n(important when striving for consistency across agents and designers). Third, the smaller \nset of options (which presumably will be chosen from a list rather than entered as free text) \nmeans the intent portion can be built faster and more easily. By removing duplicate data (in \nthe behavior system, you only have to map behaviors to a small set of intents, not the \nmuch larger set of inputs), the overall amount of work should be reduced. Fourth, it makes \nit easier for designers to tweak dialog later in the development process without editing (and \npossibly adding bugs to) individual NPC dialog trees. Fifth, having a separate intention layer \nmakes it easier to write unit tests, in part because there‘s less data to test and in part \nbecause the tests aren‘t dependent on free text (important both because text is often \nchanged and because of internationalization). Sixth, the explicit ResponseType and \nTrust levels help designers remember which conditions they need to handle. (Note that \n\n\n \n \nthis is not required: One can create a group Person that returns Answer for all topics at \nall Trust levels.) \nA final, and important, reason why separating intention from behavior is important is \nbecause it allows for design by composition, as seen in the next section. \nConcept Hierarchies \nTo enhance conversational breadth, topics belong to a topic hierarchy. If the player asked \nabout murders and the NPC‘s group didn‘t have an entry for Murder, the system would \ncheck the parent topic (say, Problems). If that was missing, it would check the next level \nuntil it had reached the root topic. There are several advantages to this, but two are worth \nmentioning. First, by placing a ResponseType on the root node, the agent has a default \nanswer to anything the player asks. This helps cover errors when a mapping has been \nforgotten. Second, it allows a designer to add new topics through extension, which is \nnormally lower risk than edits. If the designer decides that some characters need special \nbehaviors when discussing Pre-Raphaelite poetry, they can add it to the topic hierarchy. The \ngroups configured to discuss obscure Victorian poetry will do so, and everyone else will \nrespond to the general topic of poetry, writing, or something more general. \nIt should also be noted that the {Topic-Trust-ResponseType} mappings do not have \nto be completely specified. An NPC can be set up to talk about local crime when Trust is \neight or better and have no other mapping for that topic. If Trust was below that value, \nthe system would then use the parent topic. This comes in handy when a person belongs to \nmultiple groups, as described in the next section. \n \nCultural Wrappers \nWhen designers specify intent, they do so at the group level, not for individual NPCs. We \nwill refer to these groups as CulturalGroups. A CulturalGroup can represent race, \nnationality, occupation, political affiliation, or any group membership that affects how one \nreacts to something. Examples include Thai, Rural, Soldier, FootballFan, \nPunkRocker, AngryLoner, and Parent. Designers can also use the cultural group \nconcept to model personality traits, such as Paranoid and Bully. \nTable 3.10.3. Questions about Football Can Be Answered as Questions about Either \nFootball or Sports \nDo you like football? rapport building, concept abstraction \n  \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nTopic: \nFootball \nSports \nSports \nSports \nFootball \nSports \nSports \nRole: \nSoccerFan \nIraqi \nIraqi \nUSCitizen \nSoccerFan \nIraqi \nIraqi \nIntent: \nAnswer \nAnswer \nAnswer \nAnswer \nEvasive \nAnswer \nAnswer \nSay: \nYes \nYes \nYes \nNo \nMaybe \nNot really Yes \n\n\n \n \nTable 3.10.3. Questions about Football Can Be Answered as Questions about Either \nFootball or Sports \nDo you like football? rapport building, concept abstraction \n  \nTrust: \n+1 \n+1 \n+1 \n+1 \n+1 \n+1 \n+1 \n \nTable 3.10.4. U.S. and Iraqi Cultures Differ in Their Willingness to \nDiscuss Their Spouse with a Stranger \nTell me about your spouse. \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nTopic: \nSpouse \nSpouse \nSpouse \nSpouse \nSpouse \nSpouse Spouse \nRole: \nIraqi \nIraqi \nIraqi \nUSCitizen \nIraqi \nIraqi \nIraqi \nIntent: \nRefuse \nRefuse \nRefuse \nAnswer \nInsult \nRefuse Refuse \nSay: \nHow rude! \nI refuse \nNo \nChevy‘s nice \nYou‘re a \npig! \nNo \nNo \nTrust: \n–1 \n–1 \n–1 \n+1 \n–1 \n–1 \n–1 \n \nA cultural group contains one or more {Topic-Trust-ResponseType} mapping. These \nmappings are typically sparse—most doctors have a predictable reaction to medical \nquestions but not to questions about books, movies, or enchanted swords. \nAgents belong to one or more groups. Typically, one of those groups will be Person, which \nwill contain default mappings. Other groups specialize the agent. Some, such as Iraqi, will \nbe fairly broad and dense, containing a lot of mappings, while others, such as \nFootballFan, will be small and focused. The agent can have an unlimited number of \ngroups. \nWhen designing agents, two design principles are used: design by composition and design \nby exception. \nDesign by composition says that the designer should build the agent by selecting pieces \n(cultural groups) rather than writing the agent from scratch. The process is simple and \nfast—Agent A is a Doctor, GovernmentRepresentative, Iraqi, IraqiFemale, \nand Person, and Agent B is a GovernmentRepresentative, Insurgent, \nPoliceman, Iraqi, and Person. It takes only a few seconds to select the groups from a \nlist, and assigning the groups fully specifies how the agents will react to any dialog option in \nthe game. (Note that if per-agent behavior is used, that work will still need to be done, \nalthough it should still be less work than in a traditional system.) Design by composition \nspeeds up the authoring process for a single agent. \n\n\n \n \nDesign by exception speeds up the group authoring process. Following the principle of \ndesign by exception, default values should be set up in the base group (in our example, \nPerson), and only those values that differ from the default should be placed in new \ngroups. For example, you could have the group LittleGirl love to talk to complete \nstrangers about any type of animal, the group DemonEnthusiast love to discuss \ndemons, and the group RabbitPhobe be too terrified to discuss rabbits with anyone but \ntheir closest friends. Assigning those three groups to an agent produces an agent that will \ngladly talk about animals and demons yet refuse to discuss bunnies. The group \nRabbitPhobe does not contain mappings for any Topic other than Rabbit, making it \nfast to create, and the designer is not forced to create hundreds of combination groups, \nsuch as PeopleWhoLoveAnimalsAndDemonsButNotRabbits. \nAlthough it won‘t be frequent, conflicts between CulturalGroups can occur. Consider a \ndoctor who runs a government clinic in a war zone. Although the clinic is trying its best, \nthere are still rampant health problems in the area. If you ask the agent about the \nproblems, the Doctor in her wants to complain that they aren‘t doing enough, while the \nGovernmentRepresentative in her wants to say that the clinic is doing just fine. \nThe dialog system we describe here works more generally for any problem of determining \nan agent‘s reaction to an event. How one handles the cultural group conflicts depends on \nthe domain. For the dialog system, we decided to use a first-chance event handler with a \nprioritized group list (referred to as Cultural Wrappers). When setting up the agent, the \ndesigner must select the order of the groups. In Table 3.10.1, Suha has Doctor prioritized \nover GovernmentRepresentative, while Halema has the same groups but in a \ndifferent order. When asked about problems (Table 3.10.5), Suha complains about \nhealthcare, while Halema says there are no problems. \nTable 3.10.5. An NPC’s Answer Is Based on the Order (Prioritization) of Their \nGroups \nAre there any problems here? \n  \n  \n  \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nRole: \nIraqi \nIraqi \nGovRep \nUSCitizen \nInsurgent \nIraqi \nGovRep \nIntent: Answer Answer Deny \nAnswer \nInsult \nAnswer \nDeny \nSay: \nYes \nCrime \nIt‘s very safe I don‘t know You! \nHealthcare \nNo \n \nIn a very small number of cases, there is no acceptable ordering of groups—sometimes \nGroup A supersedes Group B, and other times B supersedes A. As an example, in Table \n3.10.1, Zuhair is both a policeman and (secretly) an insurgent. He wants to help the \nterrorists, but not at the risk of blowing his cover. He might voice support for the terrorists \naround people he trusts (as a terrorist) but be polite to the player (as a policeman). In \nthese instances, a simple solution is to create a new group that contains only those Topics \nand Trust levels needed to resolve conflicts. This new group might be an actual general-\npurpose group, such as Undercover-Insurgent, but it can also represent that specific \nindividual (in this case, Zuhair). Individuals have their personal quirks that can‘t be \ncaptured by any group, so modeling the things that are truly specific to an individual is \nokay, but the ―individual group‖ should only contain the exceptions. Unless the agent is \n\n\n \n \ntruly, eccentrically unique, most of his responses should be specified in the more general \ngroups. \nEarlier it was mentioned that Topics belong to a topic hierarchy. When determining the \nagent‘s intent, if a match on the Topic isn‘t found, the parent Topic is used, moving up \nthe tree until a match is found. When multiple groups are used, preference is given to \nTopic specificity. Consider the Topic TableTennis, child of Sports, and the ordered \ngroup list [USCitizen, PingPongFan]. USCitizen does not have a mapping for \nTableTennis but does for Sports, while PingPongFan matches on TableTennis. \nAssuming a Trust level of 0, the program first checks for a match on {USCitizen, \nTableTennis, 0} and fails to find a match. It then checks {PingPongFan, \nTableTennis, 0}, where it finds a match. Had it failed, it would have then checked \n{USCitizen, Sports, 0} and then {PingPongFan, Sports, 0}. This gives \ngreater freedom in arranging groups and allows for a greater number of groups to be used. \nIf the system moved up the topic heirarchy before checking the next group, any group with \na value high in the tree (for example, at the root node, which matches everything) would \nprevent any other group from having an influence. \n \nCreating Unique Individuals \nEach combination (and ordering) of groups results in an individual who is unique. It does \nnot mean that he will behave differently than every other agent under all circumstances at \nall times. We could design them to do so, but the agents wouldn‘t appear realistic, they‘d \nappear insane. It doesn‘t matter whether someone loves kittens, is a doctor, or grew up in a \nsmall town; if you ask him whether a particular neighborhood is dangerous or whether a \ngiven restaurant is good, there are only a limited number of responses you should get. \nResponding to the question ―where is the train station‖ by juggling cats might be unique, \nbut it isn‘t helpful. That said, there‘s nothing preventing the designer from adding that \nreaction. \nThe number of unique individuals you can create by combining groups grows quickly with \nthe number of groups. With three groups, 15 unique individuals can be made. With five \ngroups, the number is 325. With 10 groups, the number is 9,864,100, a number larger than \nmost cities in the world. Add one more group, and you can cover all cities and most \ncountries. \nThe process of assigning and ordering groups is not the only way to create unique NPCs. \nIntent can be tweaked at the NPC level using the TrustModifier property. This \nrepresents how trusting someone is and how quick they are to change their trust level. It is \nmultiplied against Trust to produce a modified trust score used in Trust checks. The \ndefault TrustModifier is 1. An agent with a TrustModifier of 1.5 is 50 percent \nmore trusting than normal—the agent needs only a Trust of four to trigger responses that \nother agents with the same group list require a six for. \nCircumstantial modifiers can be used to modify Trust scores. For example, if the agent \nhas been arrested or has a gun pointed at him, the threshold for giving an answer could be \nlowered. It seems likely that this would need to vary by NPC somehow (perhaps by a \nwillpower property). \nAlthough beyond the scope of this article, uniqueness can also be created in the behavior \ngeneration system. A single {Topic, ResponseType} pairing (where Topic can be a \nwildcard when topic is irrelevant, such as when insulting or ignoring the player) can map to \na set of realizations, one of which is selected at random (preferably using an intelligent \nrandom system that filters out long repeated sequences). Behaviors can also be chosen \n",
      "page_number": 289
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 300-308)",
      "start_page": 300,
      "end_page": 308,
      "detection_method": "topic_boundary",
      "content": " \n \nbased on attributes of the agent. For example, if the ResponseType was Compliment, \nan agent with a high intelligence or charisma might say something clever, while someone \nwith low intelligence might stumble badly and say something offensive. \n \nConclusion \nOne of the biggest obstacles to creating games filled with hundreds of intelligent, \nconversational agents is the sheer amount of work (and therefore cost) required to create \nthem. It‘s not that it‘s hard work (although designing interesting characters and dialog can \ncertainly be difficult); it‘s that any kind of work done several hundred times is a lot of work. \nAnd sometimes, quantity is as important as quality—you can‘t make a living, breathing, \nrealistic city with just three characters. One of the keys to improving AI is to improve its \nauthoring scalability; there needs to be processes and tools that make it easier to populate \nvirtual worlds. Hopefully, the ideas presented in this gem will be a big step toward helping \nyou fill your own worlds with intelligent, interesting, unique characters. \n \n3.11. Graph-Based Data Mining for Player Trace Analysis in \nMMORPGs \nNikhil S. Ketkar and G. Michael Youngblood \nIn this gem we will present techniques for analyzing player trace data in massively \nmultiplayer role-playing games (MMORPGs). As MMORPGs become increasingly popular, \nwith the number of subscribers going into the millions, an MMORPG provider is faced with a \nnumber of technical and business questions. For instance, how do you place an \nadvertisement in MMORPGs, or how do you detect cheating in the form of bots and gold \nfarmers? We observe that these and a lot of other such questions can be answered by \nanalyzing player traces (graph representations of the player‘s movement in the world), but \nmost traditional approaches in machine learning and data mining fall short when applied to \nthe task due to the inherent structural nature of player trace data. We claim that the \napplication of techniques in the area of graph-based data mining that are designed to work \nwith structured data are most suitable for the analysis of player traces. \nData Logging and Preprocessing \nTypically, player movements in the world can be logged as the location of the player in 3D \nspace at discrete intervals in time. Thus, the logged data for a single player is a sequence of \nthe form {(x0, y0, z0, t0),(x2, y2, z2, t2)...}. The player spawns in the world at time t0 (which \nis 0) and (x0, y0, z0) refers to the position where the player spawns. Subsequently, the \nplayer moves in the world, and assuming that we are logging data at every second, (x1, y1, \nz1, t1) refers to the position the player is at t1 = 1. Similarly, we have a number of positions \nwith the corresponding time for the player movements until the player exits the world. \nWe refer to this sequence as a walk. Our overall dataset consists of a set of such walks, \nwhere each walk corresponds to one session of a single player in the game. Figure 3.11.1 \npresents a visual representation of three walks. This data was collected in a world shown in \nFigures 3.11.2 and 3.11.3. This world is a part of the Urban MMO Testbed (UMMOT). \nUMMOT is an experimental environment designed to study human interactions in virtual \nworlds and is an extension of the Urban Combat Testbed [Cook07, Youngblood08]. \nFigure 3.11.1. A visual representation of three walks in a world. \n\n\n \n \n \n \nFigure 3.11.2. Urban MMO Testbed: Birds-eye view. \n \n \nFigure 3.11.3. Urban MMO Testbed: Screenshot. \n\n\n \n \n \n \nWhile data in such raw form is easy to log, for the purposes of analysis it needs to be \npreprocessed to a more suitable form. Dealing with data at this level of granularity \n(locations in 3D coordinate space) is computationally intensive and leads to poor results. \nHence, we convert the 3D locations to a discrete form by superimposing a grid on the world. \nFigure 3.11.4 illustrates this process. Once such a grid is superimposed, all points inside one \ngrid cube are assigned to the same discrete location. Once data is preprocessed in this \nmanner, it consists of a set of walks of the form W = {(l1, t1), (l2, t2)...}, which is a \nsequence of discrete grid locations for the corresponding time instance. \nFigure 3.11.4. Superimposing a grid to get discrete locations. \n \nSelecting a proper granularity for the grid (the size of a single cube) is important. Too small \na cube size will lead to too many locations, and too large will lead to too few locations. It is \nrecommended that the cube size be equal to, or a small multiple of, the bounding box of the \ncharacter model. \n \nAdvertisement Placement in MMORPGs \n\n\n \n \nPlacing advertisements in MMORPGs can provide host companies with an additional source \nof revenue. However, in order to capitalize on this business opportunity, companies hosting \nthe MMORPGs need to provide coverage guarantees to their clients who would pay for \nplacing advertisements. The notion of advertisement coverage is central to all marketing \nand is loosely defined as the estimated number of prospects reached by an advertisement. \nA lot of value is placed on advertisement coverage, as the clients placing the advertisement \nare solely interested in reaching as many prospects as possible and would be willing to pay \na higher amount of money for higher coverage. An advertisement in the New York Times \ncosts significantly more than an advertisement in the Charlotte Observer precisely because \nan advertisement in the New York Times will achieve higher coverage. \nIn the case of MMORPGs, estimating and providing guarantees on coverage is challenging \nfor a number of reasons. Players typically spawn in different locations, moving around the \nworld performing tasks, and this is quite different from a reader reading a newspaper or \nvisiting a webpage. Advertisements can be placed in different locations in the world, but \nwhere should they be placed? Furthermore, can some guarantees on coverage be provided? \nGiven a set of preprocessed walks, as described in the previous section, advertisements \ncould be placed at any of the locations in the walks. We define coverage for a set of \nlocations as the number of walks that contain the specified locations divided by the total \nnumber of walks. Intuitively, coverage captures the number of players that will get quite \nclose to and most likely see an advertisement. Note that it is quite possible, although \nunlikely, that a player will get close to a location but not see the advertisement because he \nor she is looking in a different direction. In our setting, we do not explicitly model where the \nplayer is looking. Such an approach has been tried before and has been found to be quite \ncomputationally expensive [Dixit08]. \nFigure 3.11.5 illustrates examples of how the coverage is computed. Note that there are \nfour distinct walks, as illustrated in the large graph on the left. Subfigures (a),(b), (c), and \n(d) illustrate four different location selections for this graph. For location selection as \nillustrated in Subfigure (a), two locations are selected, which cover Walks 2, 3, and 4. \nBecause there are a total of four walks, this amounts to coverage of 75 percent. For (b), \ntwo locations are selected, which covers all four walks, which amounts to coverage of 100 \npercent. Similarly for (c) and (d) we have 50-percent and 100-percent coverage, \nrespectively. \nFigure 3.11.5. Coverage computation. \n \nIn such a setting, our task is to maximize the coverage while minimizing the number of \nadvertisements placed. This task is equivalent to the set-cover problem, which is NP-\n\n\n \n \nComplete. Thus, optimal solutions are not feasible, and it is necessary to develop \napproaches that can produce near-optimal solutions at moderate computation cost. \nAn important, additional dimension in the task of advertisement placement is that of \ngeneralization to future player behavior (in terms of walks). Given a certain amount of \ntraining data, suppose that we select a set of locations that maximize coverage on this data. \nThen, our selected set of locations should achieve the required level of coverage on future \nwalks. Assuming that training data is a good reflection of the entire population, maximizing \ncoverage on training data will most likely achieve high coverage on unseen data. The \nimportant question here is how large of a sample is required to get a good generalization on \nunseen data. \nAnother factor to consider is the cost of collecting and logging data. Clearly, in order to have \ntraining sets on which to base advertisement placement, some data needs to be collected. \nThe most expensive case is where actual positions (3D locations) are logged. The space \nrequired for such logging grows linearly with the number of walks and may not be feasible. \nAnother approach is to simply log the number of players that visit a particular location, \nwhich is constant in the number of walks. There is an inherent tradeoff in the amount of \nlogged data and the quality of the solution. \nWe now present a set of approaches for these tasks and discuss their strengths and \nweaknesses. \nFrequency Maximizing Approach \nFrequency-based placement is a relatively simple approach that selects locations based on \nthe number of walks passing through a particular location. Walks in the training data are \nprocessed sequentially to count the number of walks passing through each location, and \nthese counts are used to select the topmost locations. Note that this approach only requires \nthe logging of frequencies of visits to each position, which requires space constant with \nrespect to the number of walks. Another important thing to note is that this approach might \nproduce suboptimal solutions in many cases, because it does not consider the overlap \nbetween walks. Figure 3.11.6 illustrates an example of such a case. Assuming that Position \nB has already been selected, the frequency maximizing approach will select Location A over \nLocation C. This is because, individually, A covers four walks and C covers two. This is \nclearly suboptimal, as there is an overlap of three walks between A and B (Walks 1, 2, and \n3). \nFigure 3.11.6. Suboptimal selection by frequency maximizing approach. \n \n \nMarkov Steady-State Probability-Based Approach \nThe Markov steady-state probability-based placement approach is based on computing the \nprobability of a player visiting a particular location based on the transition probabilities. The \nfirst step in this approach is to process the walks to generate a transition probability matrix. \nThe transition probability matrix stores probabilities of transitions from a location to any \n\n\n \n \nother location and is computed by counting the transitions and dividing by the total number \nof outgoing transitions. Once this matrix is generated, which we will refer to as M, the \nsteady-state probabilities can be computed by solving xM = x. There are several exact and \niterative approaches to solve such a system of linear equations. Based on the steady-state \nprobabilities, advertisements can be placed by selecting locations with the highest \nprobabilities. \nFigure 3.11.7 illustrates an example of this. Note that this approach requires the logging of \ntransitions and requires space constant with respect to the number of walks. \nFigure 3.11.7. Computation of steady-state probabilities. \n \n \nGreedy, Marginal Gain Maximizing Approach \nThis approach is based on selecting locations in a greedy manner, maximizing the marginal \ngain with each added location. Initially, the location with the highest frequency (or \ncoverage) is selected. This is followed by considering each location (not already selected) \nand evaluating the coverage of the newly formed set of locations (previously selected \nlocations with the current location). After evaluating each location in this manner, the \nlocation that maximizes the coverage is added to the set. \nThis is illustrated in Figure 3.11.8. Note that this approach considers the marginal coverage \nwhile considering a new location to add and therefore would produce solutions that are \nmore optimal as compared to the frequency-based approach. Another important point is \nthat this approach requires logging the actual walks—that is, the space required to log the \nnecessary data grows linearly with the length of the walk as well as the number of walks. \nFigure 3.11.8. Computing the marginal coverage. \n \n \nExperimental Comparison \n\n\n \n \nWe experimentally compared the three approaches to advertisement placement on a \ndataset of 2,436 walks. More details on the dataset can be found in [Cook07] and \n[Youngblood08]. Experiments were conducted for various sizes of training sets, ranging \nfrom 0.25 percent to 50 percent of the entire dataset, while the remaining data was used \nfor testing. (Advertisement placements were selected based on the walks in the training set, \nand these placements were evaluated on the test set.) \nFigure 3.11.9 shows the coverage achieved by each of the three approaches for different \nnumbers of advertisements, with 5 percent of the data used for training, on the training set. \nFigure 3.11.10 shows coverage on the test set. As a baseline, we also include a random \nplacement approach in the experimentation. Each result is an average over five runs of \ndifferent samples of training and test sets. Results indicate that the greedy marginal gain \nmaximizing approach significantly outperforms both the frequency-based and the Markov \nsteady-state probability-based approaches. The frequency-based approach is comparable to \nthe Markov steady-state probability-based approach. Similar results are observed on \nvarious other training sizes greater than 5 percent. An interesting observation is that we get \ndiminishing returns with an increased number of advertisements placed. That is, a lot of \ncoverage is achieved due to the initial advertisements, but after about seven or eight \nadvertisements are placed, there is very little improvement in coverage. \nFigure 3.11.9. Comparison of approaches to advertisement placement. Five \npercent of the data used for training, coverage on the training set. \n \n \nFigure 3.11.10. Comparison of approaches to advertisement placement. Five \npercent of the data used for training, coverage on the test set. \n\n\n \n \n \n \nThe closeness between the coverage on the training sets and the coverage on the test sets \nimplies that our advertisement placement generalizes well to unseen data. However, this is \nnot the case for very small training sizes. Figure 3.11.11 shows the coverage achieved by \neach of the three approaches for various budgets on the number of advertisements placed \nwith 0.25 percent of the data used for training, on the training set. Figure 3.11.12 shows \ncoverage on the test set. For such a small size of training set, 100-percent coverage is \nachieved for very few advertisements, but these results do not carry over to the unseen \ndata. For very small training sizes, we see that the performance of each of the three \napproaches is no better than random. The important point to take home is to have a \nsufficiently large sample size. Unfortunately, our present work does not include a theoretical \nbound on the sample size, and we advise users to partition their datasets (into training and \ntesting sets) to determine the appropriate size experimentally. In general, the size of the \ntraining set depends on the size of the world and the variability in the walks, and we are \nworking toward proving an upper bound on the training set size. \nFigure 3.11.11. Comparison of approaches to advertisement placement. 0.25 \npercent of the data used for training, coverage on the training set. \n\n\n \n \n \n \nFigure 3.11.12. Comparison of approaches to advertisement placement. 0.25 \npercent of the data used for training, coverage on the test set. \n \n \nEach of the three approaches generalizes well to future data given a sufficient size of the \ntraining data. In our experimentation, the greedy marginal gain maximizing approach \n",
      "page_number": 300
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 309-316)",
      "start_page": 309,
      "end_page": 316,
      "detection_method": "topic_boundary",
      "content": " \n \nsignificantly outperforms the frequency-based and Markov steady-state probability-based \napproaches. However, it should be noted that for a large-scale implementation, the greedy \nmarginal gain maximizing approach requires the logging of the actual walks, which can \nconsume space that grows linearly with the number of walks as well as the length of walks. \nThe other approaches require the logging of visits to particular locations or transition \nprobabilities, which is constant with respect to the number of walks. \n \nBuilding Player Profiles with Clustering \nThe idea behind clustering is partitioning a given set of examples into subsets (referred to \nas clusters) such that examples in each subset are similar to other examples in the subset \nby some measure. Cluster analysis is an unsupervised learning technique that allows us to \ncategorize data such that trends in the data are identified. A good introduction to cluster \nanalysis can be found in [Jain99]. \nIn the case of player trace analysis, we are interested in building player profiles that group \nplayers into categories such that players in a group have similar behaviors. Figure 3.11.13 \nshows player traces for six different players. Although visualizing such information can lead \nto important insight, this is only possible for small datasets. When player traces become \nlong or there are too many player traces, analyzing them visually becomes a tedious \nprocess. Clustering player traces serves as an important step in analyzing traces because it \nreduces the data from individual player traces to groups of player traces. Since there are far \nfewer groups than individual traces, it becomes possible to perform a visual analysis of \nthese groups instead of individual traces. \nFigure 3.11.13. Visualizing player traces in 3D space. \n \nThe key challenge in applying clustering algorithms to player trace data is that typically \nclustering algorithms are designed for attribute-valued data (data represented as a single \ntable), and player trace data is structured and cannot be represented as a single table \nwithout losing important information. A simple example of such data can be a table \n\n\n \n \nrepresenting information about customers where each row represents a customer and each \ncolumn represents a specific attribute, such as age or yearly income. \nTo produce such a grouping, a similarity measure between two examples (in this case, \ncustomers) is required. There are several distances measured for attribute-valued data—for \ninstance, Euclidean distance, which can be used to achieve good results. Contrast this \ncustomer data (represented as a table) with player trace data introduced earlier, which \ncannot be represented as a table. The notion of Euclidean distance cannot be used to \nmeasure the similarity between two player traces. This is because Euclidean distance can \nonly be used on data points represented as n-dimensional vectors of equal length, and \nwalks in the world are a sequence of points with variable length. \nTo address this difficulty, we introduce a similarity measure between two player walks in the \nworld. Using this similarity measure, any of the standard clustering algorithms can be \napplied to clustering player trace data. \nDistance Measure \nThe largest common subsequence (LCS) is used to measure the similarity between two \nwalks. An illustration of longest common subsequence can be found in Figure 3.11.14. Using \nLCS accounts for fragments of similarity between two walks. For example, suppose that \nsome walks consist of an important set of behaviors that are sequentially repeated in each \nof the walks. However, these repeating behaviors are interlaced with other actions that are \nnot common to all the walks. \nFigure 3.11.14. Longest common subsequence. \n \n \nSuch a case is illustrated in Figure 3.11.15. Here we have four walks, where Walks 1 and 2 \nhave two behaviors in common. Walks 3 and 4 also have two behaviors in common. The \nuncommon actions represent the variability or the noise in the data and should be ignored. \nWhat should be considered are the sequentially repeating, common aspects of the walks, \nwhich are in fact captured by the LCS. If we group the walks in Figure 3.11.15 based on \nLCS, we will have two groups—the first with Walks 1 and 2 and the second with Walks 3 and \n4. However, the LCS by itself does not take into account what fraction of two traces is \nsimilar. For example, in Figure 3.11.15, Walks 3 and 4 have much longer chunks of portions \nin common (with respect to the length of the entire walk) as compared to Walks 1 and 2. \nHence, Walks 3 and 4 are much more similar to each other than Walks 1 and 2. \nFigure 3.11.15. Longest common subsequence captures common behavior. \n\n\n \n \n \n \nTo take this into consideration, we define the similarity measure as: \n \nwhere A and B are the two walks under consideration. Note that this similarity measure is \ntypically a number between 0 and 1. Identical walks will have a similarity measure of 1, \nwhile completely dissimilar walks will have a similarity measure of 0. \nThe LCS problem is quite well studied in literature, with numerous applications in \nbioinformatics. An O(mn) time algorithm (where m and n are the lengths of the input \nsequences) for LCS can be found in [Wagner74]. \nApplying Standard Clustering Algorithms \nUsing the distance measure for player trace walks specified earlier, it is now possible to \nextend any of the standard clustering algorithms for the task of clustering player traces. The \ngeneral idea is to replace the distance computation (which is typically Euclidian distance) by \nthe distance measure based on LCS. Alternatively, for a given set of walks, we can \nprecompute a similarity matrix, which is basically a triangular matrix where each entry \nindicates the similarity between the example in the row and the example in the column \n(illustrated in Figure 3.11.16). Many clustering algorithms can operate on such a similarity \nmatrix to produce clusters that can be used for subsequent analysis. \nFigure 3.11.16. Similarity matrix. \n\n\n \n \n \n \nA common way to visualize clustering results is a dendrogram, which is a treelike structure \nthat depicts the similarity between the examples. We illustrate a dendrogram on the first \nfive walks (due to space limitations) in our datasets in Figure 3.11.17. \nFigure 3.11.17. Results of clustering (on a very small subset). \n \nThe overall procedure for cluster analysis is to first generate a dendrogram (source code for \ngenerating a dendrogram has been provided on CD, and more details can be found in \n[Jain99]) for the entire dataset, as depicted in Figure 3.11.17, and then focus on specific \nclusters to understand their common elements. The dendrogram is an extremely effective \ntool for visual data analysis because it allows the user to focus on specific samples in the \ndata rather than the entire dataset. The LCS procedure can also be used to produce the \ncommon subsequence thereby identifying such common elements. For example, in Figure \n3.11.17, the common element between Walks 1 and 3 is the walk up the stairs. Following \nsuch a procedure (looking at clusters and identifying common elements) can allow the \nidentification of several important behaviors. \n \nDetecting Bots and Gold Farmers with Classification Models \n\n\n \n \nThe basic task underlying the detection of bots and gold farmers is that of learning a binary \nclassification model. This task is quite well studied in the field of machine learning and is \ncommonly referred to as supervised learning. In supervised learning, we are given a set of \nexamples labeled as positive or negative (positive and negative are the two classes or \ncategories), and a supervised learning algorithm induces a function that can classify unseen \nexamples into these two categories. A good introduction to supervised learning can be \nfound in [Mitchell97]. While the supervised learning problem is quite well studied, most \nalgorithms for supervised learning only deal with attribute valued data. As mentioned \nearlier, player traces cannot be represented as attribute valued data, and hence applying \nexisting algorithms to the task can be quite challenging. \nAn important class of supervised learning algorithms is support vector machines (SVMs), \nwhich have been successfully applied to many application domains. A good introduction to \nSVMs can be found in [Cristianini00]. While SVMs also typically deal with attribute valued \ndata, they can be extended to operate with structured data by specifying a kernel function. \nA kernel function basically computes a similarity measure between two examples. The LCS-\nbased similarity measure used for clustering can also be used as a kernel function, allowing \nus to apply SVMs to classify player traces. \nUsing an LCS-Based Similarity Measure with K-NN \nWe begin by discussing the use of the LCS-based similarity measure with the K-Nearest \nNeighbor (K-NN) algorithm, which is a relatively simple classification algorithm and would \nallow the reader to develop an intuition for the task of player trace classification. The K-NN \nalgorithm is a simple lazy algorithm that stores all the input examples, and when a \nprediction is to be made on an unseen example, it first computes the K nearest neighbors \nusing some measure of similarity and predicts the class of the unseen example as the \nmajority of its neighbors. Typically, in the case of attribute valued data, Euclidian distance is \nused to measure the similarity between two examples. Although simple, the K-NN algorithm \ncan produce good classification models. \nTo extend the K-NN algorithm to operate on player trace data, we use the LCS-based \nmeasure to calculate neighbor distance. To predict whether a given player trace is a bot \ntrace or a human trace, we detect K nearest neighbors of the trace under consideration and \npredict its class (bot or human) based on the majority of the neighbors. \nTo see why the LCS similarity measure serves the purpose of distinguishing between bots \nand humans, consider the following observations. First, bots (or gold farmers) constantly \nrepeat a set of actions. While these actions may be interlaced by random movements, in \norder to achieve their objective (for example, killing boars in World of Warcraft (WoW) to \ngain experience points), they have to repeat some sequence of actions. Second, the areas \non the world where these actions can be performed are specific. (For example, there are \nparticular locations in WoW that are intended for neophyte players to kill boars and gain \nexperience points.) A set of player traces that represent bots (or gold farmers) will have \nspecific repeating locations easily captured by the LCS. \nWhile K-NN is conceptually simple, it is computationally unfeasible for the task of player \ntrace analysis on large datasets. This is because in order to identify the K nearest \nneighbors, we have to compute the LCS similarity measure of the unseen examples with all \nthe other examples in the training set. The LCS-similarity measure can be computed in \nO(mn). (m and n are lengths of the input sequences.) This is sufficiently fast for batch \nprocessing (offline classification and analysis of player traces); however, when bot detection \nneeds to be performed in real time, this is too slow. To address this issue, we need a more \nsophisticated technique, namely SVMs. \nUsing an LCS-Based Similarity Measure with SVMs \n\n\n \n \nThe LCS-based similarity measure can also be used in conjunction with SVMs. SVMs \ntypically operate on attribute valued data and, given a set of training examples (categorized \ninto two categories), produce a hyperplane (a higher dimensional plane) that separates \nexamples into the two categories. In order to classify an unseen example, its distance and \norientation with the hyperplane are computed, and based on this, we can make a prediction \nabout its category. Figure 3.11.18 illustrates this process. \nFigure 3.11.18. Support vector machines. \n \n \nThe hyperplane, more correctly referred to as the maximum margin hyperplane, is a plane \nthat puts the maximum distance between the positive and negative examples. The \nmaximum margin hyperplane is defined by the examples, which, in a sense, lie on the \nboundary of the positive and negative regions and are referred to as the support vectors. \nThe key point to note here is that in order to classify an unseen example, the LCS measure \nonly needs to be computed against the support vectors, and not the entire set of examples. \nThis significantly speeds up the process of prediction. \n \nConclusion \nWe presented a number of techniques to analyze data in MMORPGs, dealing with specific \nproblems such as advertisement placement, profile building, and bot detection. In \nconclusion, the most important point we would like to convey to the community is the \nadded value of logging player data. Understanding interaction and behavior in virtual worlds \ncan help us design better virtual worlds, and this is only possible through the collection and \nanalysis of such data. In most cases, the cost of collecting such data is a small price to pay \ncompared to the insight received by analyzing the data. \n \nReferences \n\n\n \n \n[Cook07] Cook, D. J., L. B. Holder, and G. M. Youngblood. ―Graph-Based Analysis of Human \nTransfer Learning Using a Game Testbed.‖ IEEE Transactions on Knowledge and Data \nEngineering 19.11 (2007): 1465–478. \n[Cristianini00] Cristianini, N. and J. Shawe-Taylor. An Introduction to Support Vector \nMachines and Other Kernel-based Learning Methods. Cambridge University Press, 2000. \n[Dixit08] Dixit, Priyesh N., and G. Michael Youngblood. ―Understanding Information \nObservation in Interactive 3D Environments.‖ Sandbox ‗08: Proceedings of the 2008 ACM \nSIGGRAPH Symposium on Video Games. 2008. 163–170. \n[Jain99] Jain, A. K., M. N. Murty, and P. J. Flynn. ―Data Clustering: A Review.‖ ACM \nComputing Surveys 31.3 (1999): 264–323. \n[Mitchell97] Mitchell, T. Machine Learning. WCB McGraw Hill, 1997. \n[Wagner74] Wagner, R. A., and M. J. Fischer. ―The String-to-String Correction Problem.‖ \nJournal of the ACM (JACM) 21.1 (1974): 168–173. \n[Youngblood08] Youngblood, G. M. and P. N. Dixit. ―Understanding Intelligence in Games \nUsing Player Traces and Interactive Player Graphs.‖ Game Programming Gems 7. Boston: \nCharles River Media, 2008. 265–280. \n \nSection 4: General Programming \nIntroduction \nFast-IsA \nRegistered Variables \nEfficient and Scalable Multi-Core Programming \nGame Optimization through the Lens of Memory and Data Access \nStack Allocation \nDesign and Implementation of an In-Game Memory Profiler \nA More Informative Error Log Generator \nCode Coverage for QA \nDomain-Specific Languages in Game Engines \nA Flexible User Interface Layout System for Divergent Environments \nRoad Creation for Projectable Terrain Meshes \nDeveloping for Digital Drawing Tablets \nCreating a Multi-Threaded Actor-Based Architecture Using Intel® Threading Building Blocks \n\n\n \n \nIntroduction \nDoug Binks, Intel Semiconductors AG \ndoug.binks@googlemail.com \nGame programming, like many disciplines, is becoming increasingly specialized. The steady \ntrend of technical innovation, along with the broadening requirements of game \ndevelopment, force us to focus our finite mental resources on an ever-narrowing section of \nthe field. Yet the very basis of the programming endeavor is the ability to coerce the \ncomputational architecture into performing to our will. Most of the gems in this section deal \nwith this—the fundamental art of game programming. \nPerformance is a critical aspect of most game software, and so several gems deal with this \nissue, either directly, by showcasing solutions for common tasks with enhanced \nperformance, or indirectly, by providing a better understanding of some aspect of \nperformance programming. Multi-threading is an increasingly important area for \nprogrammers looking for more cycles to execute their instructions, and suitably, a pair of \narticles targets this. Several articles deal with memory issues, from allocation to \noptimization and profiling. \nA good part of getting a system to do what we want is ensuring that it actually does. In this \nvein, a couple of articles deal with error logging and enabling the QA process. The solutions \npresented require minimal effort to implement, so they stand a good chance of being widely \nused if included in a code base. \nOther articles deal with functionality. It‘s here that the gems cover the widest area, partly \nthrough addressing general approaches to adding functionality and partly through \ndescribing specific but different types of functionality. Continuing the trend of many \nprevious editions, there‘s a bias toward tools—rightly so, as tools play an ever-important \nrole in game development. \nWhether you‘re a jack of all trades, a master of one, or new to game programming, you‘ll \nfind a good deal of useful innovation, information, and experience within this section. \n \n4.1. Fast-IsA \nJoshua Grass, PhD \njoshua.grass@gmail.com \nMany advanced scripting languages have notions of class hierarchies similar to those in \nprogramming languages such as C++, C#, or Java. Scripts written in these languages often \nneed to perform safe casts or IsA checks on objects. In our game, This Is Vegas, we found \nthat the amount of time spent performing the IsA check was not insignificant. This gem \ndescribes a method for processing class hierarchy data to change the IsA operation from \nO(N) to O(1). In our case, this resulted in a performance improvement of more than one \npercent for the cost of adding one DWORD for each class (depending on your platform; if the \nplatform does not have the BitScanReverse operator, you will need to store the location \nof the most significant bit along with the index). The algorithm is also an interesting study \nin combining several well-known data structures that we often see in school but rarely get \nto use to achieve some tangible results. \nProblem Definition \n",
      "page_number": 309
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 317-324)",
      "start_page": 317,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": " \n \nGiven a class hierarchy, we need to be able to determine whether Class A is a subclass of \nClass B. A typical class hierarchy might look like Figure 4.1.1. \nFigure 4.1.1. An example class hierarchy. \n \nOur first implementation of an IsA() function would be as follows: \nbool IsA(Class *pA, Class *pB) \n{ \n     while (*pA != NULL) \n     { \n          if (pA == pB) \n          { \n             return true; \n          } \n          pA = pA->GetParentClass(); \n     } \n     return false; \n} \n \nThis algorithm has two major problems: The worst-case scenario requires a traversal from \nleaf to root of the class tree, which can be very expensive if you are frequently doing IsA \ntests on leaf nodes (for example, we have an array of humans, and we want to process only \nthieves). The second problem is one of cache issues. The class metadata may be loaded \nanywhere in memory, and if one of the classes we traverse is not currently in the cache, \nthis operation can result in cache thrashing and low performance. \n \nBalanced Class Hierarchies \nWhile a graph with a variable number of branches at each node gives the system a huge \namount of flexibility, it also means that there is no regular way in which we can store or \naccess the hierarchy. Let‘s imagine that we were incredibly lucky in our class hierarchy, and \nat the very end of the project, we had a uniform graph in which each node branched exactly \ntwice, such as in the class hierarchy displayed in Figure 4.1.2. \nFigure 4.1.2. A balanced binary class hierarchy. \n\n\n \n \n \nThis graph has many useful properties, the main advantage being that there is a simple way \nof laying out the classes such that they can fit in one contiguous array of memory. \nProgrammers writing A* algorithms use this structure (a heap) all the time because it \neliminates the need for storing pointers and it makes memory management of an open list \nextremely easy. \nNULL \nObject Human Weapon Warrior Mage \nSword \nBow \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \nLevel 0 Level 1 Level 2 Level 2 \nLevel 3 Level 3 Level 3 Level 3 \n \nEach level we add to the tree adds 2(N-1) new nodes to the storage array, where N is the \nnew level. So if we were to add Level 4 in our aforementioned example, we would need to \nadd eight additional items to our array. Adding Level 5 would add 16 new nodes, and so on. \nWe start our table at Entry 1 instead of Entry 0 for reasons that will be discussed later in \nthe gem. \nWhat we‘re really interested in here is the index of the nodes and their relationship to their \nparents. I will refer to this index as the class index for the rest of the gem. In the above \ntable, the class index is the second row. It is important to note that we do not actually \ncreate a heap or store the classes in it. We use the heap structure purely to create a useful \nordering for the classes. \nThe function for getting the parent‘s class index of a node is trivial: \nint parentIndex(int nIndex) \n{ \n     return nIndex >> 1; \n} \n \nIf we take the class index of Sword (6) and right shift it by 1, we get the result 3, which is \nthe class index of the parent node, Weapon. We can do this again and determine that the \nparent node of Weapon (3) is, in fact, Object (1). \nGiven this perfectly balanced tree, we can easily rewrite our IsA function to use the class \nindices in the storage array to determine whether a class is a subclass of another. \n\n\n \n \nbool IsA_Balanced2Tree(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     while (nAIndex != 0) \n     { \n          if (nAIndex == nBIndex) \n          { \n               return true; \n          } \n          nAIndex = nAIndex >> 1; \n     } \n     return false; \n} \n \nSo while this function doesn‘t look that much better initially, it does have one huge \nadvantage over our previous algorithm. It doesn‘t depend on any information from the \nparent classes of A. It only retrieves information from Class A and from Class B, which are \nvery likely to already be in the cache. So we have eliminated the possibility of any \nunnecessary cache misses for intermediary classes between A or B or, in the worst-case \nscenario where A is not a child of B, all of the parent classes of A. \nWe can further improve this algorithm by realizing that once the index for a parent of A is \nless than the index for B, there is no way that they are ever going to be equal. \nbool IsA_Balanced2Tree_V2(Class   *pA, Class   *pB) \n{ \n      int   nAIndex =  pA->GetClassIndex(); \n      int   nBIndex =  pB->GetClassIndex(); \n \n      while (nAIndex >= nBIndex) \n      { \n           if   (nAIndex == nBIndex) \n           { \n                 return   true; \n           } \n           nAIndex = nAIndex >> 1; \n      } \n      return false; \n} \n \nThis has just reduced our worst-case scenario drastically. In the case where we are testing \na list of Humans to see whether they are Mages, we can halt immediately if they are \nWarriors, because the index of Warrior (4) is less than the index for Mage (5). Even in the \ncase where we were searching for Warriors, we would only need to do one right shift before \nwe could halt the function and return false. \n \nEliminating the Tree Traversal \n\n\n \n \nThe class indices have an additional property that allows us to remove the while loop from \nour function. Here is the child function for our nodes in our class tree: \nint childIndex(int nIndex, bool bRight) \n{ \n     if (bRight) \n          return (nIndex << 1) + 1; \n     else \n          return (nIndex << 1); \n} \n \nAny child of Node A has an index equal to the index Node A left-shifted a number of times \nplus a number defining the child‘s position in the sub-tree. The usefulness of this \nobservation becomes much more apparent if we write out the indices in binary (see Figure \n4.1.3). \nThis observation allows us to make the following rule: \nIf Class A is a child of Class B, then the lefimost N bits of B will match A \nwhere N is the highest bit set in A. \nThis works because we started our class hierarchy at Index 1, so we know that all indices \nare 1 followed by an arbitrary number of bits. \nFigure 4.1.3. Binary representation of indices of a node and its children. \n \n \nUsing this rule we can write our IsA function one more time without the use of the while \nloop. (Non-constant bit-shift operators are emulated on the PS3, but this is implemented in \nmicrocode, so it is still much faster than a while loop.) \nbool IsA_Balanced2Tree_V3(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     if (nAIndex <= nBIndex) \n          return nAIndex == nBIndex; \n \n     nAIndex = nAIndex >> \n               (BSR(nAIndex) – BSR(nBIndex)); \n \n\n\n \n \n     return nAIndex == nBIndex; \n} \n \nThe BSR function in our case is a wrapper for an inline assembly function that uses the BSR \nassembly instruction (BitScanReverse). This instruction returns the index of the \nleftmost set bit/most significant bit, which is exactly what we need for this algorithm. If a \nplatform does not have the BSR assembly instruction, we can easily pre-calculate this value \nand store it in the Class object along with the array index (GetArrayIndexMSB()). \nThis was our first implementation of the function before we found out about the BSR \ninstruction. \nFinally, we can take advantage of one further property of the right-shift operator. If the \namount to shift is negative, then the result is 0. And since we start our class hierarchy with \nan index of 1, no class will match 0. This leads to our final implementation of Fast-Isa. \nbool FastIsA(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     return nBIndex == \n          (nAIndex >> (BSR(nAIndex) – BSR(nBIndex)); \n} \n \n \n \nBuilding a Balanced Tree \nAll of the previous work has been built upon the notion that our class hierarchy is a perfectly \nbalanced binary tree. In practice, this is rarely the case. Luckily, what we want out of the \nIsA function isn‘t any notion of depth between nodes, but only if they are in fact ancestors. \nBecause of this, there is no reason why we cannot insert phantom classes to balance our \ntree. Figure 4.1.4 displays an example of the transformation from an unbalanced to a \nbalanced hierarchy. \nFigure 4.1.4. Converting a three-child node into a binary hierarchy. \n \nIn the case of these two class trees, every possible IsA relationship is maintained. \n\n\n \n \nIn any situation where we have more than two direct children of a class, we \ncan insert a number of phantom class nodes between the parent and the \nchildren to ensure that the IsA tree is a balanced binary tree. \nIt is important to realize that while we are using the notion of a heap to generate the \nindices, we are not actually storing anything in this structure. It is purely virtual, so adding \nlarge numbers of phantom nodes to balance the tree does nothing except use up our index \nspace. For most games, a 32-bit DWORD will contain more than enough space for the class \nhierarchy. \nThe simplest implementation for building the class tree is the following algorithm. I \nrecommend implementing this and determining whether you are close to running out of \nindex space before moving to a more complicated algorithm. \nvoid BuildTree(Class *pA) \n{ \n     int nCurrentClassIndex = pA->GetClassIndex(); \n     int nNumChildClasses = pA->GetNumberOfChildClasses(); \n     int nNumLevels = BSR(nNumChildClasses) + 1; \n     int nChildIndexStart = \n               nCurrentClassIndex << nNumLevels; \n     for (int i = 0; i < nNumChildClasses; ++i) \n     { \n           Class *pChild = pA->GetChildClass(i); \n           pChild->SetClassIndex(nChildIndexStart + i); \n           BuildTree(pChild); \n      } \n} \n \nThe heart of implementing a more complicated class tree construction algorithm is realizing \nthat in most cases we have a fair amount of play in how we actually lay out the phantom \nnodes. Take the four-child case (see Figure 4.1.5). \nFigure 4.1.5. Alternate ways of decomposing a four-child node into a binary \nhierarchy. \n\n\n \n \n \n \nBoth of these class trees have the exact same IsA relationship, so to the Fast-IsA algorithm \nthere is no distinction. If our class tree was very complex (or deep), we could balance the \ntree based on the number of subclasses or the maximum depth of any subclass of a class. \nThis leads to an algorithm similar to Huffman encoding to optimize the class tree such that \nthe maximum depth of any leaf is minimized. With more than 3,700 script classes in This Is \nVegas, we never encountered a problem with our hierarchy depth. \nIf your class tree is greater than 32 levels deep, there is no reason why you cannot simply \nchange your class index from a DWORD to a QWORD. The memory costs are minimal (since \nwe only pay per class and not per instance), and 64-bit processors will be able to perform \nthese operations at the same speed. \n \nConclusion \nMoving to the Fast-IsA algorithm had a large impact on the performance of our game for \nthe cost of only one additional DWORD per class. In our case, it was a performance \nimprovement of more than one percent for the entire game and far greater in specific \nportions of the code. Given how simple the implementation is and how little it costs, this \nwas an easy optimization win for us. We also found the Fast-IsA function to impact \nperformance in our pipeline‘s baking process, which uses a large number of IsA checks. \n \n4.2. Registered Variables \nPeter Dalton, Smart Bomb Interactive \npdalton@madprog.com \n\n\n \n \nInter-system communication is a critical consideration in a game engine, often dictating the \nbroad architecture of the code base. In practice, sacrifices are often made to allow one \nsystem to know about the internal workings of another in order to accommodate better \ncommunication. Although sometimes this can be appropriate, it often results in a loss of \nmodularity. These compromised systems lose their ―black box‖ characteristics and become \nharder to maintain and replace. This gem will present a solution to this problem by \ndemonstrating a technique for linking up shared variables across disparate systems. This \nallows systems to define a set of inputs or control variables that can be seamlessly linked to \nvariables in other systems. \nIt is important to recognize that this is not a messaging system, nor is it meant to replace \none. Rather, it is a system that allows a programmer to control communication across \nvarious systems without requiring blind casts, global variables, or a flat class hierarchy. This \ntechnique allows for basic variable types, such as integers and floats, as well as complex \ndata types, such as arrays, classes, and other user-defined types. This technique has been \nsuccessfully utilized to facilitate the necessary communication required to control animation \nsystems, user interface parameters, display shader parameters, and various other systems \nwhere variable manipulation is required. \nGetting Started \nThe basic idea is to create a wrapper for a variable and then allow these wrapped variables \nto be linked together. The code that is dependent upon the variables can be implemented \nwithout any special considerations. Rather than just accessing the value that has been \nwrapped, the registered variable will walk the chain of linked variables and provide access \nto the appropriate variable. When building registered variables, there are several key goals \nto keep in mind. \n \nKeep the registered variable seamless. To make a registered variable truly \nuseful, it needs to be easy to work with. The goal is to make it transparent to the \nprogrammer whether they are using a regular integer or our newly created integer \nregistered variable. Operator overloading will be the key here. \n \nAllow one registered variable to be linked to another. We are going to allow \nregistered variables to set redirectors, or in other words, allow registered variables \nto be chained together. \n \nTracking of a ―dirty state.‖ To enhance the usefulness of a registered variable, we \nwill include a dirty state in the variable. This provides users with knowledge of when \nthe variable has actually changed, which is useful for run-time optimizations. \n \nCustom run-time type information. This will become necessary when we start \nregistering variables together. It allows us to confidently cast to a specific type \nwithout the need for blind casts. \n \nProvide a way to link registered variables directly. We will provide an initial, \nexplicit method for linking variables together. This method is important when dealing \nwith specific situations where the control variables are well defined. \n \nProvide a way to link registered variables indirectly. As our systems grow in \ncomplexity, we want to allow for variables to be generically linked together without \neither system knowing the internal details of the other. This indirect method will \nbecome the key to dealing with complex situations where all control variables are not \nwell defined or are ambiguous. \n \nAssumptions \nThe code we will present is taken from a commercial Xbox 360 engine. It utilizes several \nroutines and data structures provided by the base engine that are beyond the scope of this \ngem. These dependencies are minimal; however, we need to explicitly mention them in \n",
      "page_number": 317
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 325-332)",
      "start_page": 325,
      "end_page": 332,
      "detection_method": "topic_boundary",
      "content": " \n \norder to avoid confusion. A basic implementation of these data structures has been included \non the accompanying CD-ROM. \nTArrays \nThe TArray class is a templated array holder. It is used to hold links to other registered \nvariables. \nFNames \nThis class implements a string token system. It is a holder for all of the strings that exist \nwithin the game engine. Each string is assigned a unique identifier by which it can then be \nreferenced and compared against other FNames with a constant cost of O(1). This \nfunctionality is the key to implementing the required run-time type information and the \nmeans by which registered variables are given unique names for linking. \n \nThe Base Class: RegisteredVar \nThe base class from which all registered variables will be derived is the RegisteredVar \nclass. This class provides all of the support for linking registered variables together and \ntracking the dirty state. Here only key portions of the RegisteredVar class are shown; a \ncomplete implementation can be found on the accompanying CD-ROM. \nclass RegisteredVar \n{ \npublic: \n    // Provides IsA<>() and GetClassType() routines, described \nlater. \n    DECLARE_BASEREGISTERED_VARIABLE( RegisteredVar ); \n \n    RegisteredVar() : m_bDirty(false), m_pRedirector(null) {} \n    virtual ~RegisteredVar() \n    { \n        if (m_pRedirector) \n            m_pRedirector->m_References.RemoveItem( this ); \n        while (m_References.Num()) \n            m_References[0]->SetRedirector( null, false ); \n    } \n    void SetRedirector( RegisteredVar* InRedir ) \n    { \n        if (InRedir!=this && (!InRedir || (InRedir->IsA( \nGetClassType() ) \n            && !InRedir->IsRegistered( this )))) \n        { \n            if (m_pRedirector) \n                m_pRedirector->m_References.RemoveItem( this ); \n            m_pRedirector = InRedir; \n            if (m_pRedirector) \n                m_pRedirector->m_References.AddItem( this ); \n        } \n    } \n    void SetDirty( bool InDirty, bool InRecurse=false ); \n\n\n \n \n    bool IsDirty() const; \n    void SetFName( FName InName ) { m_Name = InName; } \n    FName GetFName() const { return m_Name; } \nprotected: \n    template<class T> T* GetBaseVariable() const \n    { \n        return m_pRedirector ? \n            m_pRedirector->GetBaseVariable<T>() : (T*)this; \n    } \n    FName m_Name; \n    bool m_bDirty; \n    RegisteredVar* m_pRedirector; \n    TArray<RegisteredVar*> m_References; \n}; \n \nThere are two key elements to getting this class correct. The first is preventing dangling \npointers in the destructor. The important consideration here is that since we are going to be \nlinking up registered variables blindly between systems, we do not want to end up pointing \nto a registered variable that has been deleted. This scenario would result in dangling \npointers to invalid memory addresses and severe headaches. To prevent this, we create a \nlink back to the referencing registered variable so that we can clean it up when the \nreferenced registered variable is deleted. This would normally create a doubly linked list; \nhowever, in our case it is common for multiple registered variables to redirect to a single \nregistered variable, thus creating the need for an array of pointers as illustrated in Figure \n4.2.1 \nFigure 4.2.1. This diagram illustrates how registered variables will be linked \ntogether and how we will also be tracking referencing registered variables to \navoid dangling pointers. \n \n \nThe second key to keep in mind is that anytime you access a registered variable, you need \nto ask yourself an important question: Should I be working with ―this‖ copy of the \nregistered variable or should I forward the request to the redirected registered variable? If \nyou decide that the correct answer is to work on the redirected registered variable, the \nGetBaseVariable() routine will retrieve the base registered variable that should be \nused. \n \n\n\n \n \nSingle Variable Values versus Array Variable Values \nThe next step is to divide all the variables into two distinct classifications: single value types \nand arrays. The first classification, single value types, encompasses integers, floats, user-\ndefined types, and so on and will be the focus of the examples provided. The second, array \ntypes, will encompass arrays of integers, floats, user-defined types, and so on. The \nimplementation of array types is very similar to single values types with just a few minor \nalterations. The implementation of array types has been provided on the accompanying CD-\nROM. Having made this distinction, we will now create a templated base class that provides \n99 percent of the functionality required by any variable type. \ntemplate<class T, class RegVar> \nclass RegisteredVarType : public RegisteredVar \n{ \npublic: \n    RegisteredVarType(); \n \n    T Get() { return GetBaseVariable<RegVar>()->m_Value; } \n    const T& Get() const { return GetBaseVariable<RegVar>()-\n>m_Value; } \n    void Set( const T& InV ) \n    { GetBaseVariable<RegVar>()->SetDirectly( InV ); } \n    operator T() const { return GetBaseVariable<RegVar>()-\n>m_Value; } \n    operator T&() { return GetBaseVariable<RegVar>()->m_Value; } \n \n    void operator=( const RegVar& InV ) { Set( InV.Get() ); } \n \n    // Implement comparison operators >,<,>=,<=,==,!=, see CD-\nROM. \n    bool operator>( const T& InV ) { return Get() > InV; } \n \n    // Implement mathematic operators /,*,+,-, see CD-ROM. \n    T operator/( const T& InV ) const { return Get() / InV; } \n \n    // Implement assignment operators /=,*=,+=,-=, see CD-ROM. \n    RegVar& operator/=( const T& InV ) \n    { Set( Get() / InV ); return *(RegVar*)this; } \n \nprotected: \n    void SetDirectly( const T& InValue ) \n    { \n        if (m_Value != InValue) \n        { \n            m_Value = InValue; \n            SetDirty( true ); \n        } \n        for (int ii = 0; ii < m_Parents.Num(); ++ii) \n            ((RegVar*)m_References[ii])->SetDirectly( InValue ); \n    } \n    T m_Value; \n}; \n \n\n\n \n \nExamining the code should illustrate the emphasis placed on providing the appropriate \noverloaded operators to allow the programmer to seamlessly use registered variables. The \nprogrammer should not have to change the code whether they are using a standard variable \nor a registered variable. This ensures that the registered variable is used correctly and \nseamlessly. It also makes it easy to add and remove registered variables from a system \nsince only the variable definition and linking code needs to be updated. \nAnother important consideration is the SetDirectly() routine used by the Set() \nmethod. The SetDirectly() routine first determines whether the value is actually \ndifferent than the current value and sets the dirty flag if appropriate. This dirty flag allows \nthe owner of the variable to effectively track when the state of the variable has truly \nchanged, thus allowing for run-time optimizations. \nA common optimization, when dealing with shader parameter blocks within DirectX, is to \nprevent the blocks from being invalidated and rebuilt unless absolutely necessary. Thus, if \nyou have a variable controlling the state of a shader, you will want to make sure that the \nvariable has actually changed before processing it. You should also notice that there is no \nautomatic means by which the dirty flag is cleared. To clear the flag, the owner of the \nvariable will need to explicitly call the SetDirty( false ) routine when the owner is \ndone dealing with the change. Since the dirty flag is stored in each variable, the owner of \nthe variable can deal with the flag in its own way. In the case of a variable controlling a \nshader parameter, we would not want to handle the variable change and rebuild the state \nblock until the material is required by the renderer. However, another variable might also be \nlinked to this state and want to handle the change immediately. It is also safe for the owner \nto choose to ignore the dirty flag if it isn‘t required. \nThe SetDirectly() routine also has the task of copying the value to the entire chain of \nlinked registered variables. This feature is important to retain the most recent value in the \nevent that a registered variable clears its redirector either explicitly or if the redirector is \ndeleted. If the value was not copied, we would see a pop from the old value to whatever \nvalue is currently stored. While this might not be a critical issue, it can cause undesired \nbehavior, as the variable might appear un-initialized. Copying the value is also useful when \ndebugging, allowing for the value to be easily shown in the watch window without digging \nthrough a list of linked variables. \n \nType-Specific Registered Variable \nAt this point we have built all the base classes required, and creating registered variables is \nnow straightforward. \nclass RegisteredVarBOOL \n    : public RegisteredVarType<bool, RegisteredVarBOOL> \n{ \nDECLARE_REGISTERED_VARIABLE( RegisteredVarBOOL, \nRegisteredVarType ); \n    RegisteredVarBOOL& operator=( const bool& InValue ) \n    { Set( InValue ); return *this; } \n}; \n \nclass RegisteredVarFLOAT \n    : public RegisteredVarType<float, RegisteredVarFLOAT> \n{ \nDECLARE_REGISTERED_VARIABLE(RegisteredVarFLOAT, \nRegisteredVarType ); \n\n\n \n \n    RegisteredVarFLOAT & operator=( const float& InValue ) \n    { Set( InValue ); return *this; } \n}; \n \nThe listings above add support for both the standard Boolean and float types. Implementing \nadditional types is as simple as duplicating the provided code and updating the names and \ntypes appropriately. Note that the operator=() was not specified within the templated \nbase class RegisteredVarType in order to resolve conflicts when using the Visual Studio \n2008 C++ compiler. \n \nSetting a Registered Variable Directly \nWe‘ll now look at a simple example to illustrate what registered variables can do. We have a \nweapon class attached to a vehicle class, and the vehicle needs to tell the weapon when to \nfire. If we create a Boolean registered variable within the vehicle and link it to the weapon, \nwe can then just manipulate the variable within the vehicle and control the state of the \nweapon. Also, if we have multiple components that need to know about the weapon firing, \nsuch as AI logic, user interfaces, or game code, we now only have one variable that needs \nto be updated to keep everyone in sync. In contrast, without using registered variables we \nwould need to create a Fire() function within the weapon and call it to start and stop \nfiring. We would also need to manually notify all other systems that the weapon is firing. \nThe registered variable approach has the advantage that once the variables are correctly \nregistered, it is much easier to control communication. \nclass Weapon \n{ \n    void SetFireRegVar( RegisteredVar* InVar ) \n    { \n        m_Fire.SetRedirector( InVar ); \n    } \n    void HeartBeat( float InDeltaTime ) \n    { \n        if (m_Fire) FireWeapon(); \n    } \n    RegisteredVarBOOL m_Fire; \n}; \nclass Vehicle \n{ \n    void Initialize() \n    { \n        m_MyWeapon.SetFireRegVar( &m_FireWeapon ); \n    } \n    void HeartBeat( float InDeltaTime ) \n    { \n        m_FireWeapon = DoWeWantToFire(); \n    } \n    bool DoWeWantToFire(); \n    RegisteredVarBOOL m_FireWeapon; \n    Weapon m_MyWeapon; \n}; \n\n\n \n \n \n \n \nIsA Functionality \nThe DECLARE_REGISTERED_VARIABLE macro requires further explanation to assist in \nunderstanding the implementation. The purpose of this macro is to provide type information \nfor the registered variable. It ensures that we do not link two registered variables together \nthat are not of the same basic type. It also allows us to determine the type of register \nvariable that we have given only a pointer to the base class RegisteredVar. \n#define DECLARE_REGISTERED_VARIABLE( InClass, InBaseClass )            \n\\ \n  protected:                                                           \n\\ \n       typedef InClass ThisClass;                                      \n\\ \n       typedef InBaseClass Super;                                      \n\\ \n  public:                                                              \n\\ \n       virtual FName GetSuperClassType( FName InComponentType ) \nconst     \\ \n       {                                                               \n\\ \n           FName SuperType = NAME_None;                                \n\\ \n           if (InClass::StaticGetClassType() == InComponentType)       \n\\ \n               SuperType = Super::StaticGetClassType();                \n\\ \n           else if (InComponentType ! = NAME_None)                     \n\\ \n               SuperType = Super::GetSuperClassType ( \nInComponentType );  \\ \n           return SuperType == InComponentType ? NAME_None : \nSuperType;   \\ \n       }                                                               \n\\ \n       virtual FName GetClassType() const                              \n\\ \n       {                                                               \n\\ \n           return InClass::StaticGetClassType();                       \n\\ \n       }                                                               \n\\ \n       static FName StaticGetClassType()                               \n\\ \n       {                                                               \n\\ \n\n\n \n \n           static FName TypeName = FName( STRING( InClass));           \n\\ \n           return TypeName;                                            \n\\ \n       } \n#define DECLARE_BASEREGISTERED_VARIABLE( InClass )                     \n\\ \n   DECLARE_REGISTERED_VARIABLE( InClass, InClass )                     \n\\ \n   template<class T> bool IsA() const                                  \n\\ \n       {                                                               \n\\ \n           return IsA( T::StaticGetClassType() );                      \n\\ \n       }                                                               \n\\ \n       bool IsA( const FName& InTypeName ) const                       \n\\ \n       {                                                               \n\\ \n           for (FName Type = GetClassType(); Type != NAME_None;        \n\\ \n               Type = GetSuperClassType( Type ))                       \n\\ \n           {                                                           \n\\ \n               if (Type == InTypeName)                                 \n\\ \n                   return true;                                        \n\\ \n           }                                                           \n\\ \n           return false;                                               \n\\ \n       } \n \nWhile this code is being utilized here to provide IsA functionality for registered variables, it \nis generic in nature and can be used to provide RTTI functionality to any class or structure. \nThe code is written in the form of a macro to prevent the code from being duplicated due to \nit being required at every level of the inheritance chain. An important consideration is to \nrecognize that this implementation does not support multiple inheritance but could be \nextended to do so. \n \nSetting a Registered Variable Indirectly \nNow that we have basic RTTI information, we can safely link registered variables together \nwithout knowing the internals of other systems. Let‘s examine another example. \nSuppose we have a material used for rendering that has a parameter we can adjust to \nchange its damage state. The damage state is defined within the material and used to \n\n\n \n \ndetermine how the material is rendered. In this example we would like to create a generic \nsystem in which a high-level object can register a variable with another object and have it \ncorrectly link to a control variable. We want a vehicle class to provide a variable to control \nthe damage state of the material, and then the vehicle can drive the material‘s control \nvariable by simply modifying its own variable. In this example, adding a function or \nparameter to the Material class would not be desirable because it would lead to bloat \nand would not be applicable to all materials. \nclass RegisterVariableHolder \n{ \n    virtual void RegisterVariable( RegisterVar& InVar ) {} \n}; \nclass BaseClass : public RegisteredVariableHolder \n{ \n    virtual void RegisterVariables(RegisterVariableHolder& \nInHolder) {} \n}; \nclass Material : public BaseClass \n{}; \nclass DamageStateMaterial : public Material \n{ \n    void Initialize() \n    { \n        m_DamageState.SetFName( \"DamageState\" ); \n    } \n    virtual void RegisterVariable( RegisteredVar& InVar ) \n    { \n        if (InVar.IsA<RegisteredVarFLOAT>() && \n            InVar.GetFName() == m_Trans.GetFName()) \n        { \n            m_DamageState.SetRedirector( &InVar ); \n        } \n    } \n    RegisteredVarFLOAT m_DamageState; \n}; \nclass Vehicle : public BaseClass \n{ \n    void Initialize() \n    { \n        m_VehicleDamageState.SetFName( \"DamageState\" ); \n        m_Fire.SetFName( \"Fire\" ); \n        RegisterVariables( *m_pMaterial ); \n    } \n    virtual void RegisterVariables( RegisterVariableHolder& \nInHolder ) \n    { \n        InHolder.RegisterVariable( m_VehicleDamageState ); \n        InHolder.RegisterVariable( m_Fire ); \n    } \n    RegisteredVarFLOAT m_VehicleDamageState; \n    RegisteredVarBOOL m_Fire; \n    Material* m_pMaterial; \n}; \n \n",
      "page_number": 325
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 333-342)",
      "start_page": 333,
      "end_page": 342,
      "detection_method": "topic_boundary",
      "content": " \n \nNow, whenever the vehicle changes m_VehicleDamageState, the material class‘s \nm_DamageState variable will be automatically updated without the material being \nrequired to provide accessor routines or the vehicle knowing the type of material it has been \nassigned. The vehicle can also ignore the material since the only thing it needs to do is \nupdate its own registered variable. While the example is fairly simple, the principle can be \napplied to solve many more problems. \n \nConclusion \nWithin our game engine we have found registered variables to be an essential part of inter-\nsystem communication because they abstract the communication layer and minimize \nsystem dependencies. Registered variables are utilized to control the state of animation flow \nsystems, expose data to user interfaces, such as hit points and ammo counts, and control \nmaterial parameters, such as damage states and special rendering stages. We provide tools \nwithin the game‘s editor to allow artists and level designers to specify exactly which \nregistered variables should be linked together within the game. Systems have been \ndesigned to allow users to dynamically create new registered variables within the game \neditor and link them to any other appropriate registered variable. For us, this has opened \nthe door for content builders to access any set of data within the game engine and gives \nthem the necessary controls to manipulate gameplay. \nWe hope that you will have fun experimenting with the concept of registered variables and \nthat you will find them useful in improving your code. You will find an implementation of the \ntechniques presented on the CD-ROM. \n \n4.3. Efficient and Scalable Multi-Core Programming \nJean-François Dubé, Ubisoft Montreal \njfdube75@gmail.com \nNowadays, multi-core computers (and game consoles such as the Microsoft Xbox 360 and \nSony PlayStation 3) are very common. Programmers are faced with the challenges of \nwriting multi-threaded code: data sharing, synchronization, deadlocks, efficiency, and \nscalability. Adding multi-threading to an existing game engine is an enormous effort and \nmight give an initial speed gain, but will it run twice as fast if you double the number of \ncores? What if you run it on a 16+ cores system, such as Intel‘s Larrabee architecture? Will \nit be able to run on platforms with co-processors, such as Sony‘s PlayStation 3? \nIn this gem, we‘ll see how to write efficient and scalable multi-threaded code. The first \nsection will deal with the ―efficiency‖ part of the problem, while the second part will deal \nwith the ―scalability‖ part. \nEfficient Multi-Threaded Programming \nMulti-threaded programming introduces a variety of issues the programmer must be aware \nof in order to produce code that performs the required operations correctly. Additionally, \ncertain operations can lead to additional overheads in a multi-threaded program. In this \nsection we‘ll look at high-performance methods for resolving these issues. \nShared Data \n\n\n \n \nThe main problem with multi-threaded programming is concurrent access to the same \nmemory locations. Consider this simple example: \nuint32 IncrementCount() \n{ \n    static uint32 Count=0; \n    return Count++; \n} \n \nThis is commonly translated into three operations: a load, an addition, and a store. Now, if \ntwo threads execute this function slightly at the same time, what will happen? Here‘s an \nexample: \nThread 1 read Count and store it into register R1 \nThread 1 increment R1 \nThread 2 read Count and store it into register R1 \nThread 2 increment R1 \nThread 1 store the value of R1 into Count \nThread 2 store the value of R1 into Count \n \nIf Count was originally 5 before this sequence of events, what will be the result afterward? \nWhile the expected value is 7, the resulting value would be 6, because each thread has a \ncopy of Count in its register R1 before it was actually updated to memory. This example is \nvery simple, but with more complex interactions this could lead to data corruption or invalid \nobject states. This can be fixed by using atomic operations or by using synchronization \nprimitives. \nAtomic Operations \nAtomic operations are special instructions that perform operations on a memory location in \nan atomic manner; that is, when executed by more than one core on the same memory \nlocation, it is guaranteed to be done atomically. For example, the \nInterlockedIncrement function could be used in the previous example to make it \nthread-safe and lock-free. \nA very useful atomic operation is the Compare And Swap (CAS) function, implemented as \nInterlockedCompareExchange on Windows. Essentially, it compares a value with \nanother and exchanges it with a third value based on the outcome of the comparison, \natomically. It then returns the original value before the swap. Here‘s how it can be \nrepresented in pseudocode: \nuint32 CAS(uint32* Ptr,   uint32 Value, uint32   Comperand) \n{ \n    if(*Ptr   == Comperand) \n    { \n        *Ptr = Value; \n        return Comperand; \n    } \n    return *Ptr; \n} \n \n\n\n \n \nThis atomic operation is very powerful when used correctly and can be used to perform \nalmost any type of operation atomically. Here‘s an example of its usage: \nuint32 AtomicAND(volatile   uint32* Value, uint32   Op) \n{ \n    while(1) \n    { \n        uint32 CurValue = *Value; \n        uint32 NewValue = (CurValue & Op); \n        if(CAS(Value, NewValue, CurValue) == CurValue) \n        { \n            return NewValue; \n        } \n    } \n} \n \nIn this example, we read the current value and try to exchange it with the new value. If the \nresult of the CAS returns the old value, we know that it wasn‘t changed during the operation \nand that the operation succeeded. (It was swapped with the new value.) On the other hand, \nif the result is not equal to the old value, we must retry, since it was changed by another \nthread during the operation. This is the basic operation on which almost all lock-free \nalgorithms are based. \nSynchronization Primitives \nSometimes, atomic operations are not enough, and we need to be able to ensure that only a \nsingle thread is executing a certain piece of code. The most common synchronization \nprimitives are mutexes, semaphores, and critical sections. Although in essence they all do \nthe same thing—prevent execution of code from multiple threads—their performance varies \nsignificantly. They are kernel objects, which means that the operating system is aware \nwhen they are locked. Therefore, they will generate a costly context switch if already locked \nby another thread. On the other hand, most operating systems will make sure the thread \nthat has a critical section locked will not be preempted by another thread while it holds the \nlock. So using those primitives depends on a lot of factors: the time span of the lock, the \nfrequency of locking, and so on. \nWhen locking is required very frequently and for a very low amount of time, we want to \navoid the overhead from operating system process rescheduling or context switching. This \ncan be achieved by using a spin lock. A spin lock is simply a lock that will actively wait for a \nresource to be freed, as seen in this simplified implementation: \nwhile(CAS(&Lock, 1, 0)) {} \n \nWhat it does is simple: It uses the CAS function to try to gain access to the lock variable. \nWhen the function returns 0, it means we acquired the lock. Releasing the lock is simply \nassigning 0 to the Lock variable. \nA real implementation of a spin lock usually should contain another waiting loop that doesn‘t \nuse atomic functions to reduce inter-CPU bus traffic. Also, on some architectures, memory \nbarriers are required to make sure that the state of the Lock variable is not reordered in \nsome ways, as seen in the next section. The complete implementation is available on the \nCD. \n\n\n \n \nOn architectures that don‘t change a thread‘s affinity (in other words, that don‘t reschedule \nthreads on different processors, such as Xbox 360), running several threads on the same \ncore competing for a shared resource using a spin lock is a very bad idea. If the lock is held \nby a thread when it gets interrupted by the operating system scheduler, other threads will \nbe left spinning trying to acquire the lock, while the thread holding it is not making progress \ntoward releasing it. This results in worse performance than using a critical section, as shown \nin Figures 4.3.1 and 4.3.2. \nFigure 4.3.1. Wasting cycles when using spin locks for a long time. \n \n \nFigure 4.3.2. Efficient locking when using critical sections. \n \n \n \nMemory Ordering \nMemory can be read and written in a different order than written in your code for two main \nreasons: compiler optimizations and hardware CPU reordering. The latter differs a lot \ndepending on the hardware, so knowing your architecture is important. See [McKenney07] \nfor a detailed look at this problem. \n\n\n \n \nConsider the following pieces of code running simultaneously: \nThread 1 \nThread 2 \nGlobalValue = 50; \nwhile(!ValueIsReady) {} \nValueIsReady = true; LocalValue = GlobalValue; \n \nWhile this looks completely fine and will almost always work when running on a single core \ndepending on the compiler optimizations, it will probably fail when running on different \ncores. Why? First, the compiler will most likely optimize the while loop and keep \nValueIsReady in a register; therefore, declaring it as volatile should fix the problem on \nmost compilers. Second, due to out-of-order memory accesses, ValueIsReady might get \nwritten first; therefore, Thread 2 can read GlobalValue before it is actually written by \nThread 1. Debugging this kind of bug without knowing that memory reordering exists can \nbe long and painful. \nThree types of memory barriers exist: read, write, and full barriers. A read memory barrier \nforces reads from memory to complete, while a write memory barrier forces writes from \nmemory to complete, so other threads can access the memory safely. A full memory barrier \nis simply forcing both reads and writes to complete. Some compilers will also correctly \nhandle reads and writes of volatile variables by treating them as memory barriers (Visual \nStudio 2005 for instance, as described in [MSDN]). Also, some of the Interlocked functions \ncome with the Acquire and Release semantics, which behave like read and write barriers, \nrespectfully. \nThe previous example can be solved by using a write memory barrier before setting \nValueIsReady to ensure that the write to GlobalValue actually gets written before \nValueIsReady, ensuring that other threads will see the new GlobalValue before \nValueIsReady is written. \nFalse Sharing \nMost multi-core architectures have a per-core cache, which is generally organized as an \narray of memory blocks, each with a power of two size (128 bytes, for example), called \ncache lines. When a core performs a memory access, the whole line is copied into the cache \nto hide the memory latency and maximize speed. False sharing occurs when two cores \noperate on different data that resides in the same cache line. In order to keep memory \ncoherency, the system has to transfer the whole cache line across the bus for every write, \nwasting bandwidth and memory cycles. The solution to this problem is to make sure that \nthe data is structured in a way that avoids this problem. \nMemory Allocator Contention \nMemory allocators can rapidly become a bottleneck in multi-threaded applications. Standard \nmemory allocator implementations provided in the C run time (malloc/free) or even \noptimized and widely used allocators, such as Doug Lea‘s dlmalloc [Lea00], aren‘t designed \nto be used concurrently; they need a global lock to protect all calls, which inherently leads \nto false sharing. The design of a multi-processor‘s optimized memory allocator is beyond the \nscope of this gem, but a good comparison of available multi-processor allocators can be \nfound in [Intel07]. \nIdle Thread State \n\n\n \n \nWhen a thread is idle, it is important that it doesn‘t waste CPU cycles. Here‘s some code we \noften see from a person writing multi-threaded code for the first time: \nwhile(!HasWork()) \n{ \n    Sleep(0); \n} \n \nThe Sleep(0) will make the thread give up the remainder of its time slice for other \nthreads, which is fine. But, when it gets rescheduled, it will loop back, unnecessarily wasting \nCPU time (and multiple context switches) until it has work to do. A better solution is to put \nthe thread in sleep mode, waiting for an event. This is achieved by using the \nCreateEvent and WaitForSingleObject functions. Waiting for an event essentially \ntells the operating system scheduler that the thread is waiting and shouldn‘t get any CPU \ntime until the event is triggered. \nThread Local Storage \nIt is possible to declare per-thread global variables using Thread Local Storage (TLS). The \ndeclaration differs from compiler to compiler, but here‘s how it works under Visual Studio: \n__declspec(thread) int GlobalVar; \n \nEach thread now has its own GlobalVar variable copy. This can be especially useful for \nper-thread debugging information (such as the thread name, its profiling stats, and so on) \nor for custom memory allocators that can operate on a per-thread basis, effectively \nremoving the need for locking. \nLock-Free Algorithms \nA good introduction to such things is discussed in detail in [Jones05] and [Herlihy08]. \nEssentially, these algorithms are pieces of code that can be executed by multiple threads \nwithout locking. This can lead to enormous speed gain for some algorithms, such as \nmemory allocators and data containers. For example, a lock-free queue could need to be \nsafe when multiple threads push data into it, while multiple threads also pop data at the \nsame time. This is normally implemented using CAS functions. A complete implementation \nis available on the CD. \n \nScalable Multi-Threaded Programming \nThe most common way to rapidly thread an existing application is to take large and \nindependent parts of the code and run them in their own thread (for example, rendering or \nartificial intelligence). While this leads to an immediate speed gain and lots of \nsynchronization problems, it is not scalable. For example, if we run an application using \nthree threads on an eight-core system, then five cores will sit idle. On the other hand, if the \napplication is designed from the start to use small and independent tasks, then perfect \nscalability can be achieved. To accomplish this, several options already exist, such as the \nCilk language [CILK], which is a multi-threaded parallel programming language based on \nANSI C, or Intel‘s Threading Building Blocks [TBB]. An implementation of a simple task \nscheduler is presented next. \nTask Scheduler Requirements \n\n\n \n \nThe required properties of our scheduler are: \n1. Handle task dependencies. \n2. Keep worker threads‘ idle time at a minimum. \n3. Keep CPU usage low for internal task scheduling. \n4. Have extensibility to allow executing tasks remotely and on co-processors. \nThe scheduler is lock-free, which means that it will never block the worker threads or the \nthreads that push tasks to it. This is achieved by using fixed-size lock-free queues and a \ncustom spin lock and by never allocating memory for its internal execution. \nTasks \nA task is the base unit of the scheduler; this is what gets scheduled and executed. In order \nto achieve good performance and scalability, the tasks need to be small and independent. \nThe (simplified) interface looks like this: \nclass Task \n{ \n    volatile sint* ExecCounter; \n    volatile sint SyncCounter; \npublic: \n    virtual void Execute()=0; \n    virtual sint GetDependencies(Task**& Deps); \n    virtual void OnExecuted() {} \n} \n \nA task needs to implement the Execute function, which is what gets called when it is \nready to be executed (in other words, no more dependencies). \nTo expose dependencies to the scheduler, the GetDependencies() function can be \noverloaded to return the addresses and the number of dependent tasks. A base \nimplementation that returns no dependencies is implemented by default. \nA task is considered as fully executed when its Execute function has been called and when \nits internal SyncCounter becomes zero. The ExecCounter is an optional counter that \ngets atomically decremented when the Execute function is called. For tasks that spawn \nsub-tasks, setting the sub-task‘s ExecCounter pointer to the parent‘s SyncCounter \nensures that their OnExecuted functions will only be called when all sub-tasks have been \nexecuted. \nWorker Threads \nThe scheduler automatically creates one worker thread per logical core. Figure 4.3.3 \nillustrates how the worker threads behave: Each worker thread is initially pushed in a lock-\nfree queue of idle threads and waits for a wakeup event. When a new task is assigned to a \nworker thread by the scheduler, it wakes up and executes the task. Once the task is done, \nthe worker thread does several things. First, it tries to execute a scheduling slice (which will \nbe explained in the next section) by checking whether the scheduler lock is already acquired \nby another thread. Then, it tries to pop a waiting task from the scheduler. If a task is \navailable, it executes it, and the cycle restarts. On the other hand, if no tasks are available, \nit pushes itself in the lock-free queue of idle threads and waits for the wakeup event again. \nFigure 4.3.3. Worker thread logic. \n\n\n \n \n \n \n \nScheduler \nThe scheduler is responsible for assigning tasks to the worker threads and performing \ntidying of its internal task queues. To schedule a task, any thread simply needs to call a \nfunction that will push the task pointer in the lock-free queue of unscheduled tasks. This \ntriggers a scheduling slice, which is explained next. \nScheduling Slice \nAt this point, pending tasks are simply queued in a lock-free queue, waiting to be scheduled \nby the scheduler. This is done in the scheduling slice, which does the following, as described \nin Figure 4.3.4: \n1. Register pending tasks. \n2. Schedule ready tasks. \n3. Delete executed tasks. \nFigure 4.3.4. Scheduler slice logic. \n\n\n \n \n \n \n \nRegistering Pending Tasks \nDuring this phase, the scheduler pops the pending tasks and registers them internally. \nThen, it needs to handle dependencies. If a task is dependent on previously scheduled \ntasks, it goes through the dependencies and checks to see whether they have been \nexecuted. If that‘s the case, the task is ready to execute and is marked as ready. \nOtherwise, the task is kept as pending until the next scheduling slice. \nScheduling Ready Tasks \nThe second phase of the scheduling slice is to assign tasks that are ready to be executed to \nworker threads. The scheduler first tries to pop an idle thread from a lock-free queue of idle \nthreads. If it succeeds, the task is directly assigned to that thread, and the thread waiting \n\n\n \n \nevent is signaled. If all threads are working, the task is queued in the lock-free queue of \nwaiting tasks. The scheduler then repeats the process until there are no more tasks to \nassign. \nDeleting Executed Tasks \nThe last phase of the scheduling slice handles the tasks that are considered fully executed \nby calling the OnExecuted function on them and by deleting them if they are marked as \nauto-destroy; some tasks may need to be manually deleted by their owner, as they might \ncontain results, and so on. \n \nFuture Work \nSince tasks are usually independent from each other, the scheduler can be extended to \nsupport execution of tasks on remote computers or on architectures with co-processors. To \nachieve this, each task would have a way to package all the data it needs to execute. Then, \nthe scheduler would dispatch the task to other computers or co-processors though a simple \nprotocol. Compilation of the Execute function might be required to target the co-\nprocessor‘s architecture. When a completed task message arrives, the task would then \nreceive and unpackage the resulting data. This could be used as a distributed system (such \nas static lighting/normal map computations, distribution of the load of a game server to \nclients, and so on) or as a way to distribute work on the PlayStation 3 SPUs automatically. \n \nOptimizations \nThe single task queue could become a bottleneck on systems with a large number of cores \nor ones that have no shared cache between all cores. Here, using per-worker thread task \nqueues along with task stealing could prove advantageous. Furthermore, the scheduling \ncould be made more cache friendly through a policy of inserting newly created tasks into \nthe front of the worker thread‘s queue that generated the task, as these are likely to be \nconsuming the data generated by the task that created them. All of these optimizations \ndepend on the target architecture details. \n \nConclusion \nAs we have just seen, the scheduler is completely lock-free; its internal CPU usage is kept \nto a minimum, the worker threads are either executing a task or waiting for one, and most \nof all, it scales with the number of cores. The complete source code on the CD comes with \nseveral sample tasks that have been tested on a Intel Core 2 Quad CPU running at 2.83 \nGHz, with the following results (also see Figure 4.3.5): \nFigure 4.3.5. Test results. \n",
      "page_number": 333
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 343-350)",
      "start_page": 343,
      "end_page": 350,
      "detection_method": "topic_boundary",
      "content": " \n \n \n  \nFibonacci Sequence Perlin Noise QuickSort \n1 thread \n1.05ms \n9.06s \n9.30s \n2 threads 0.29ms \n4.55s \n4.84s \n3 threads 0.22ms \n3.03s \n3.44s \n4 threads 0.15ms \n2.29s \n2.74s \n5 threads 0.14ms \n2.21s \n2.81s \n6 threads 0.12ms \n2.31s \n2.86s \n \nThe Fibonacci sequence test has been created to see how dependencies are handled. The \nPerlin noise test, on the other hand, has been implemented to see how performance and \nscalability can be achieved; the test consists of computing a 2048 × 2048 Perlin noise \ngrayscale image with 16 octaves. The QuickSort test consists of sorting 65 million random \nintegers. As we can see, the expected scalability is achieved for all of these tests. \n \nReferences \n[CILK] ―The Cilk Project.‖ Massachusetts Institute of Technology. n.d. \n<http://supertech.csail.mit.edu/cilk>. \n[Lea00] Lea, Doug. ―A Memory Allocator.‖ 2000. State University of New York, Oswego. n.d. \n<http://gee.cs.oswego.edu/dl/html/malloc.html>. \n\n\n \n \n[Herlihy08] Herlihy, Maurice and Nir Shavit. The Art of Multiprocessor Programming. Morgan \nKaufmann Publisher, 2008. \n[Intel07] ―The Foundations for Scalable Multi-Core Software in Intel® Threading Building \nBlocks.‖ 2007. Intel. n.d. <http://download.intel.com/technology/itj/2007/v11i4/5-\nfoundations/5-Foundations_for_Scalable_Multi-core_Software.pdf>. \n[Jones05] Jones, Toby. ―Lock-Free Algorithms.‖ Game Programming Gems 6. Ed. Mark \nDeloura. Boston: Charles River Media, 2005. \n[McKenney07] MeKenney, Paul E. ―Memory Ordering in Modern Microprocessors.‖ 2007. \nRaindrop Laboratories. n.d. \n<http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf>. \n[MSDN] ―Synchronization and Multiprocessor Issues.‖ n.d. Microsoft. n.d. \n<http://msdn.microsoft.com/en-us/library/ms686355(VS.85).aspx>. \n[TBB] ―Intel Threading Building Blocks.‖ n.d. Intel. n.d. \n<http://www.threadingbuildingblocks.org>. \n \n4.4. Game Optimization through the Lens of Memory and Data \nAccess \nSteve Rabin, Nintendo of America Inc. \nsteve.rabin@gmail.com \nAs modern processors have become faster and faster, memory access has failed to keep \npace. This is such a pressing issue on current console hardware that experts advise \ndevelopers to treat memory as if it were as slow as hard drive access [Isensee06]. As a \nresult of this vast disparity between CPU and memory speed, a great deal of horsepower \ngoes to waste as CPUs wait for data to work on. Thus, a key aspect to optimizing games is \nkeeping the CPU well fed with data. \nBut isn‘t the cache supposed to alleviate this problem? While the cache is indispensible to \nany complex computer architecture, it isn‘t a panacea. To its benefit, cache is brilliantly \ntransparent to the code, but if we want to truly optimize our game, we‘ll have to pull back \nthe curtains and understand how the cache works in order to help it out. \nOnce we better understand the cache and the memory architecture, it will become apparent \nthat we need to respect the cache by shrinking the size of data and keeping it better \norganized. These will be our two guiding principles for most of our optimizations. \nUnderstand the Cache \nMain memory is extremely slow relative to the CPU. It‘s slow because it‘s typically far away \nfrom the CPU (on another chip) and made with fewer transistors per bit (which requires the \nbits to be refreshed so they don‘t fade away—which takes extra time). However, it‘s \npossible to make more expensive memory that is closer to the CPU (directly on the same \nchip) and faster by using more chip real estate per bit (so it doesn‘t need to be refreshed). \nHowever, this more expensive memory can‘t be as large as main memory because it simply \nwon‘t fit on the silicon die next to the CPU. So what is a computer architect to do? \n\n\n \n \nThe solution is to keep copies of the most recently used main memory bits in the ultra-fast \ncache memory that sits on the CPU die. In fact, this strategy works so well that modern \ncomputer architectures have multiple levels of cache, usually referred to as level 1, level 2, \nand level 3 (L1, L2 and L3, respectively). \nL1 cache is the smallest and fastest. Because of different access patterns between data and \ninstructions, it‘s commonly split into an L1 data cache and an L1 instruction cache (typically \n32 KB each). L2 cache is slightly slower and is usually shared between data and instructions \n(typically 256 KB to 6 MB, larger if shared among cores). L3 cache is a relatively new \ndevelopment in consumer hardware and appears on Intel‘s i7. On the i7, each core has a \ndedicated L1 and L2, but the 8-MB L3 is shared among the four cores. Table 4.4.1 shows \ncache sizes for various platforms [AMD09, Bell08, Lanterman07, Shimpi09, Wikipedia09]. \nTable 4.4.1. Cache Sizes for Various Platforms \nPlatform \nL1 Instruction/Data \nCache \nL2 Cache \nL3 \nCache \nCache Line \nSize \niPhone 3GS \n32 KB/32 KB \n256 KB \nN/A \n64 bytes \nWii \n32 KB/32 KB \n256 KB \nN/A \n32 bytes \nPS3 \n32 KB/32 KB \n512 KB \nN/A \n128 bytes \nXbox 360 \n32 KB/32KB \n1 MB \nN/A \n128 bytes \nAMD Athlon \nX2 \n64 KB/64 KB \n512 KB to 1 \nMB \nN/A \n64 bytes \nIntel Core 2 \n32 KB/32 KB \n1 MB to 6 MB \nN/A \n64 bytes \nIntel i7 \n32 KB/32 KB \n256 KB \n8 MB \n64 bytes \n \nBut how do these multiple levels of cache work? If the CPU needs to load a word from \nmemory, it will first ask the L1 cache. If the data is there, it‘s a hit in the L1 cache, and the \ndata will be delivered very quickly to the CPU. If the data isn‘t in the L1 cache, then it is a \nmiss in the L1 cache, and then the L2 cache is checked. If the data is in the L2 cache, it is a \nhit, and the data is delivered to the L1 cache and the CPU. If it is a miss in the L2 cache, \nthen the next level must be checked. If the next level is main memory, then the requested \ndata will be delivered to the L2, the L1, and the CPU. \nHowever, there is one more twist to how cache works. Memory isn‘t just copied to different \nlevels of cache in bytes or words. It‘s copied in chunks known as cache lines, which are \naligned sequential pieces of memory (for example, 128 bytes on the PS3 and Xbox 360, \nwhich is 32 words of 4 bytes each). So when the CPU asks for a 4-byte word and it misses \nin each cache, the entire cache line is copied from main memory into each cache level. This \ncache line copy is the reason why spatial coherency is so important for the working data \nset. Table 4.4.1 shows cache line sizes for various platforms. \nKnowing that main memory is slow and the cache is fast, our goal will be to keep data and \ninstructions in the cache as long as possible. The worst thing that can happen is cache \nthrashing, where the working set of data and instructions can‘t fit inside the cache at the \n\n\n \n \nsame time. In this case, memory is brought into the cache only to be thrown out because \nthere isn‘t enough room for the next needed piece of data. Imagine a tight loop that \ncontinually operates on 512 KB of data when the L2 cache size is only 256 KB. \nWith the limited size of cache and the importance of our data being in the cache, it becomes \nclear why we must keep data small and well organized. \n \nPinpoint Problem Areas \nIf we were to optimize every system in our game, we would waste a lot of time and effort. \nFor example, if we double the speed of a function that only takes 0.01 percent of the frame \ntime, then we have effectively done nothing toward speeding up our game. It‘s a hard truth \nto face since doubling the speed of a function sounds so satisfying, but the overall \nimprovement is too small to make a difference. \nProfiling our game is the only true way to pinpoint which areas are worth spending time to \nimprove. Unless you can prove that a system or section of code is a bottleneck, you \nshouldn‘t waste your effort even thinking about optimizing it. \nOnce you identify code that takes a significant amount of time, how can you tell whether \nthe CPU is waiting for data? The answer is performance counters. Performance counters are \nmeasurement tools built directly into the CPU that can count events and help identify \nproblems during code execution. Many profilers can record performance counters, or you \ncan usually turn them on and record the results directly in your code. \nUsing the performance counters, there are two key metrics to measure in order to identify \nwhen the CPU is spinning, waiting for data. The first is instructions per cycle (IPC), which \ngives a rough measure of how much work gets done per CPU cycle. If you measure an IPC \nof 0.8, then on average 0.8 instructions are completing per CPU cycle. The second key \nperformance counter is the percentage of loads or stores that resulted in an L2 cache miss \n(which means a load from main memory). If the IPC is low and the percentage of L2 cache \nmisses is high, then the cache isn‘t working well for this piece of code, perhaps because of \ncache thrashing. \nOnce you know which code or data needs to be optimized, you can go to work with the \nfollowing suggestions. \n \nAvoid Waste \nThe key to being efficient is avoiding waste. One source of wasted CPU cycles comes from \nreading memory that you don‘t use. Since the cache copies memory in cache line chunks, \nit‘s critical to use everything you read, in order to be efficient. For example, if you have an \narray of structs and only need to operate on one element in each struct, then you might be \nwasting a majority of the data you‘re bringing in from main memory. For example, if you \nonly operate on 30 percent of the struct, then as much as 70 percent of the data you‘re \nbringing in from main memory is wasted. Solutions to this problem are presented in the \n―Organize the Data‖ section. \nAnother source of waste comes from reading or writing memory that is not contiguous. \nAgain, since memory is copied in cache line chunks, it is important to read and write \nsequentially to avoid waste. \n\n\n \n \nLastly, waste can arise from redundancy. It would be wasteful to read the same data \nmultiple times a frame. Instead, batch operations together and try to optimize for a single \npass through the data per frame. \n \nShrink the Data \nOur first main strategy is simple: If your data is smaller, more of it will fit in the cache. This \nis as easy as carefully managing your data type sizes. If you‘re working with integers that \nwon‘t get very large, consider using a short (2-byte integer) or just a single byte to \nrepresent the value. Instead of using 64-bit doubles, consider using 32-bit floats. However, \none of the biggest wins is with Booleans. Typically, a Boolean takes up 4 bytes, but this is a \nhuge waste of space since ideally it should be represented as a single bit. \nPacked Structures \nAn easy way to manage your data type sizes is by defining them inside a structure. The \nfollowing code is an example of an inefficient structure for a billboard particle, followed by \nan efficiently packed one. Notice the use of reduced ranges, bitfields, and indices, as well as \nhow types are reordered by size to reduce padding. The result is a 1/3 savings in space, or \nabout 7 K saved for 500 particles. \nstruct InefficientParticle //total size 44 bytes \n{ \n   bool visible;           //31 bits of padding \n   Texture *texture;       //pointer to texture \n   int alpha;              //only needs 0 to 255 \n   float  rotation;        //too much precision \n   int type;               //enumeration - 4 possible types \n   Vec3 position; \n   Vec3 velocity; \n}; \n \nstruct EfficientParticle   //total size 30 bytes \n{ \n   Vec3 position; \n   Vec3 velocity; \n   unsigned char alpha;    //saved 3 bytes (0-255) \n   unsigned char rotation; //saved 3 bytes (0-255 degrees) \n   unsigned texture:4;     //saved 28 bits (texture index) \n   unsigned type:2;        //saved 29 bits (enumeration) \n   unsigned visible:1;     //saved 31 bits (single bit) \n}; \n \nTo emphasize the importance of ordering struct variables from largest to smallest (which \nreduces padding and results in smaller structs), consider the following two examples of the \nsame data: \nstruct WastedPadding       //20 bytes total \n{ \n   char      var1;         //1 byte \n   float     var2;         //4 bytes \n\n\n \n \n   char      var3;         //1 byte \n   int       var4;         //4 bytes \n   char      var5;         //1 byte \n}; \n \nstruct OptimalPadding      //12 bytes total \n{ \n   float    var2;          //4 bytes \n   int      var4;          //4 bytes \n   char     var1;          //1 byte \n   char     var3;          //1 byte \n   char     var5;          //1 byte \n}; \n \nThe following is a struct packing checklist for quick reference: \n \nAre numbers represented as the smallest reasonable data type? \no \nUse a float instead of a double? \no \nUse an 8-bit or 16-bit integer instead of an int? \no \nUse an 8-bit or 16-bit integer instead of a float (lose some precision)? \no \nUse the minimal bits necessary to represent the highest number? \n \nCan a char array be converted to a pointer and the string stored elsewhere? \n \nCan a pointer be converted to an index? \n \nAre all Booleans converted to a single bit? \n \nAre all enumerations converted to the range of numbers needed? \n \nAre the data types ordered from largest to smallest to reduce padding? \nCompile for Size \nCode is data! The smaller your code is, the more code will persist in the L1 instruction cache \nand in the shared instruction/data L2 cache. Since both code and data compete for L2 \ncache, smaller code also helps keep data in the cache. \nAll compilers offer the ability to optimize the compiled code for either speed or size. By \ncompiling for speed, small functions will be inlined, and loops will be unrolled (which will \nbloat the size of the code). If you compile for size, then compact code will be favored. Since \nit‘s not clear which option will make your game run the fastest, you‘ll need to profile each \noption. It also may be the case that you want to compile some parts for speed and others \nfor size. \n \nOrganize the Data \nThe second main strategy is to better organize your data to be cache-conscious. This means \ncreating contiguous data structures (rather than scattered around memory) and grouping \nfrequently used data together, away from infrequently used data. \nPrefer Compact Contiguous Containers \nNode-based containers, such as linked lists, hurt cache performance in two ways. The first \nis that they waste space by storing pointers to the next node, thus bloating the data \nstructure. The second is that they allow the container nodes to be scattered around \nmemory, hurting spatial locality [Isensee06]. A much more efficient data structure is either \nan array, an STL vector, or an STL deque, where the data is stored sequentially without the \n\n\n \n \nuse of pointers. If you are using STL node–based containers, be sure to allocate new nodes \nfrom a dedicated heap to maintain spatial locality. \nSeparate Hot and Cold Data \nWhile object-oriented programming and encapsulation lead to effective organization and \ncomprehension, they can also lead to inefficient cache use. Within each struct or class, \nsome members are referenced often by the code, and others are referenced infrequently. \nIdeally, all of the hot data (data that is used often) is placed together and is separate from \nthe cold data (data that is seldom used) [Ericson03, Kaeli01, Franz98]. This would result in \nbetter cache utilization since the hot data is adjacent and more likely to be in the cache. \nHowever, it‘s not enough to just separate hot and cold data within a struct or class. \nConsider what happens with an array of structs that have both hot and cold data. The \nproblem that arises is that the cold data causes gaps between the groups of hot data, as in \nthe left side of Figure 4.4.1. \nFigure 4.4.1. On the left, an array of structs containing a mix of hot and cold data. \nThe middle image shows splitting the hot and cold data for each struct, with a \npointer linking data from the same struct. The right image is slightly more efficient \nsince the pointers are eliminated and struct correspondence is implicit in the array \nindex. \n \nClearly, we need to further distill the hot and cold data even between structs or classes. In \norder to preserve encapsulation, one solution is to keep the hot data within the data \nstructure, but then reference the cold data with a pointer (with the cold data living in some \nother part of memory). This is shown in the middle image of Figure 4.4.1. \nThe most cache-efficient solution would be to further weaken encapsulation and eliminate \nthe extra pointer. In this scheme, there are two corresponding arrays: one holding the hot \ndata and another holding the cold data. The link between the hot and cold data is \nmaintained implicitly by the array index. For example, if a particular struct was originally \nstored in buffer[2], then the hot data is now stored in hot[2], and the cold data is stored in \ncold[2]. This is shown in the right image of Figure 4.4.1. \n \nManipulate the Cache \nSo far the optimizations have been centered around being cache-conscious and playing nice \nwith the cache. The following two optimizations are more direct and attempt to directly \nmanipulate the cache to our benefit. \n\n\n \n \nPrefetch Data \nSince the CPU will spin while waiting for data, it can be advantageous to prefetch data so \nthat it‘s in the cache when the CPU is ready to use it. Some CPUs have specific prefetch \ninstructions, but if these aren‘t available, you might have to cleverly prefetch the data \nyourself. The following code performs software prefetching for the next four array elements \nin a loop [Ericson03]. \nfor (int i = 0; i < 4 * n; i += 4) \n{ \n   Touch(array[i + 4]);   //Forces prefetch of memory \n   Process(array[i + 0]); \n   Process(array[i + 1]); \n   Process(array[i + 2]); \n   Process(array[i + 3]); \n} \n \nWhen prefetching data, timing is of the essence. You must not get the data too early, since \nit could be evicted from the cache before it‘s ever used. Conversely, you must not do it too \nlate, since it might not be ready in time. Verifying with a profiler is the only surefire way to \nconfirm that prefetching is having an effect. \nLock the Cache \nTo maximally use the cache, some consoles, such as the Wii, allow a portion of the cache to \nbe locked and directly managed by the game. (Data must be manually moved in and out.) \nThis can be extremely effective, since you are guaranteed to have particular data in the \ncache. However, this is console dependent, so check with your target platform to see \nwhether this is available. General-purpose computers, such as PCs, do not allow the cache \nto be locked, because this would interfere with other processes. \n \nConclusion \nOptimization in games is about eliminating wasted cycles. With CPUs chronically waiting on \nmemory access, waste comes in several forms: from waiting for bloated data structures \nfrom main memory, not using everything that is read into the cache, and redundantly \npulling the same data into the cache. These things can be avoided by shrinking your data \nstructures and organizing them better to be cache-conscious. Lastly, you can manipulate \nthe cache through prefetching and potentially locking the cache. \nHowever, as with any optimization work, ensure that you‘re spending time improving actual \nbottlenecks. Only a profiler and performance counters can give you a good idea of where to \nconcentrate your efforts. Finally, measure and compare improvements with a profiler to \nensure that you‘re making a tangible difference, resulting in the entire game running faster \nand not just the code you modified. \n \nReferences \n[AMD09] AMD. ―Key Architectural Features AMD Athlon™ X2 Dual-Core Processors.‖ 2009. \nAdvanced Micro Devices. n.d. \n",
      "page_number": 343
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 351-358)",
      "start_page": 351,
      "end_page": 358,
      "detection_method": "topic_boundary",
      "content": " \n \n<http://www.amd.com/us/products/desktop/processors/athlon-x2/Pages/amd-athlon-x2-\ndual-core-processors-key-architectural-features.aspx>. \n[Bell08] Bell, Brandon. ―Intel Core i7 (Nehalem) Performance Preview.‖ 2008. Firing Squad. \nn.d. \n<http://www.firingsquad.com/hardware/intel_core_i7_nehalem_performance_preview/>. \n[Ericson03] Ericson, Christer. ―Memory Optimization.‖ Game Developers Conference. 2003. \nSony Computer Entertainment. n.d. \n<http://www.research.scea.com/research/pdfs/GDC2003_Memory_Optimization_18Mar03.p\ndf>. \n[Franz98] Franz, Michael and Thomas Kister. ―Splitting Data Objects to Increase Cache \nUtilization.‖ 1998. Technical Report – University of California-Department of Information \nand Computer Science. n.d. <http://www.ics.uci.edu/~franz/Site/pubs-pdf/ICS-TR-98-\n34.pdf>. \n[Isensee06] Isensee, Pete. ―C++ on Next-Gen Consoles: Effective Code for New \nArchitectures.‖ 2006. Game Developers Conference. n.d. \n<https://www.cmpevents.com/sessions/GD/S1549i1.ppt>. \n[Kaeli01] Kaeli, David. ―Profile-Guided Instruction and Data Memory Layout.‖ 2001. \nNortheastern University Computer Architecture Research Laboratory. n.d. \n<http://www.ece.neu.edu/groups/nucar/publications/Tufts.pdf>. \n[Lanterman07] Lanterman, Aaron. ―Architectural Comparison: Xbox 360 vs. PlayStation 3.‖ \n2007. Georgia Institute of Technology. n.d. \n<http://users.ece.gatech.edu/~lanterma/mpg/ece4893_xbox360_vs_ps3_4up.pdf>. \n[Shimpi09] Shimpi, Anand. ―The iPhone 3GS Hardware Exposed & Analyzed.‖ 2009. \nAnandTech. n.d. <http://www.anandtech.com/gadgets/showdoc.aspx?i=3579>. \n[Wikipedia09] Wikipedia. ―Broadway (microprocessor).‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Broadway_microprocessor>. \n \n4.5. Stack Allocation \nMichael Dailly \nmike@dailly.org \nPerformance in games has always been important—and fun! Indeed, it‘s what brings many \npeople into our profession in the first place, and it‘s always fun looking for simple new ways \nof speeding up our code. One of the best places to optimize has always been in the center \nof a tight loop, where every cycle counts. Although this doesn‘t happen as much as it used \nto, if you‘re dealing with anything that allocates, speed is almost always important. This \ngem describes a method of allocation that allows you to shave valuable cycles off of your \nallocator, yet is frighteningly simple to follow and implement. \nOverview \nThe standard way of doing rapid allocation is to use a linked list of sorts, either single or \ndoubly linked. This allows you to get the next free element quickly, and all you need to do is \nmaintain the links. However, when dealing with linked lists (particularly doubly linked lists), \n\n\n \n \nyou can end up touching more memory than you want, and thus incur cache misses and \ndelays. The code is also longer than we‘d all like, and in these days of 64-bit address \npointers, you can end up burning more memory than you really need to. So what‘s the \nanswer? Well, you could use indices instead of pointers to do your linked list with, but they \ncan be even slower if you‘re not careful. \nSo, in steps the stack allocator. This system uses a pre-allocated list of items, with \narbitrary-sized indices—be that pointers, INTs, WORDs, BYTEs, or even BITs! But unlike a \nnormal list, we use a simple stack concept and position the stack pointer (SP) at the end. \nWe can then pop the next free item off the stack and return it with minimal fuss or code, \nwhile we push items onto the list to free them. \nFigure 4.5.1 illustrates a typical sequence where several objects are allocated and freed \n(where SP is the current stack pointer). \nFigure 4.5.1. Object allocation in action. \n \nThe ability to easily use varying sizes of indices, bits, pointers, or even POD types directly \nwithout having to really change the implementation is very powerful. But before looking at \nsome examples, let‘s look at how we would implement the code. \n \nExample Implementation \nWe‘ll start with a simple example implementation using a stack. But unlike a normal stack, \nyou pre-fill it with your object‘s pointers, indexes (INTs, SHORTs, BYTEs, or even bits), or \nany other kind of object handle. First, we need to create and initialize the array. In this \nexample we‘ll use basic pointer allocation. \n#define      MAX_OBJECT          10000 \n \nParticle*    Stack[MAX_OBJECT];            // Our object stack \nint          SP;                           // The Stack Pointer \nParticle     ParticleArray[MAX_OBJECT];    // The object pool \n \n// \n################################################################\n#### \n// Inititialise the stack with object indexes from 0 to \nMAX_OBJECT-1 \n// \n################################################################\n#### \n\n\n \n \nvoid InitStack( void ) \n{ \n      // Create our object list \n      // Pre-Fill stack with indexes (or pointers) \n      for(int i=0;i<MAX_OBJECT;i++){ \n            Stack[i] = &ParticleArray[i]; \n       } \n \n       // Initialise the stack pointer \n       SP = MAX_OBJECT; \n} \n \nAs you can see, creation is simply a matter of filling the array with object pointers and then \nsetting up a stack pointer. Next, allocation and releasing of a particle object: \n// \n################################################################\n###### \n//  Popa particle from the free  pool. \n//  -1 is returned if there are no free particles left. \n// \n################################################################\n###### \nParticle *  Pop(void ) \n{ \n      ASSERT( SP>=0, \"Error: Stack pointer has gone negative!\"); \n      if( SP==0 ) return -1; \n      return pStack[—SP]; \n} \n \n// \n################################################################\n### \n// Push a used particle back onto the free pool \n// \n################################################################\n### \nvoid Push(Particle* _pParticle ) \n{ \n      ASSERT( SP>=0, \"Error: Stack pointer has gone negative\"); \n      ASSERT( SP!= MAX_OBJECT, \"Error: Not enough space in \nstack. Object \nfreed twice?\"); \n      pStack[SP++] = _pParticle; \n} \n \nAllocating a particle is simply a matter of calling Pop(), while releasing it only requires you \nto call Push(). Again, not rocket science. Of course, these functions could just as well be \ncalled Alloc() and Free(), but for the sake of clarity when dealing with a stack method, \nwe‘ll stick with Push() and Pop(). \n\n\n \n \nYou can find code for a template based on this method on the accompanying CD \n(TemplateExample_V1). \n \nIndex-Based Implementation \nLet‘s now look at another couple of possible uses. The most obvious one is to allocate an \nindex as a handle to an object rather than a pointer to the object. In this example, we have \n256 sprites that we wish to allocate from, and rather than allocating an object and returning \na whole pointer, we‘ll simply allocate and return the BYTE index, thereby saving memory. \n#define           MAX_SPRITES        256 \n \nunsigned char     Stack[MAX_SPRITES]; \nSprite            SpritePool[MAX_SPRITES]; \n \n// \n################################################################\n#### \n// Inititialise the stack with object indexes from 0 to \nMAX_OBJECT-1 \n// \n################################################################\n#### \nvoid InitStack( void ) \n{ \n       // Pre-Fill stack with ready to use particles. \n       for(int i=0;i<MAX_SPRITES;i++){ \n             Stack[i] = (unsigned char) i; \n       } \n \n       // Initialise the stack pointer \n       SP = MAX_ SPRITES; \n} \n \nHere you can see that by simply replacing the Particle* with a sprite index, we‘ve \nchanged the allocation type. This is important because it opens up all manner of interesting \ntricks. In the above example, we knew that we were only ever going to have 256 sprites, so \nwe were able to allocate an index and keep our memory footprint down. The calling class \ncan then store the index and use it as a handle directly, or at the very least take the \naddress of &SpritePool[index] just before using it. This obviously saves more storage \nwith only minimal effort on the coder‘s part. \nThe CD holds code for another example (TemplateExample_V2), based on the index \nmethod. \nHow about taking an extreme case of 16 million objects (a full 24-bit number). This would \nnormally be hard to allocate, as you would normally either opt for a full INT index or use a \n1D linked list. However, using the stack method, you can easily store 3 bytes per entry, \neither by working out the byte offset yourself or by storing two tables of a byte and a short, \nthereby saving you 16 MB of data. So instead of 64 MB of memory required for rapid \nallocation, you can quickly and easily reduce it down to 48 MB. \n\n\n \n \nTaking this further, there‘s nothing to stop you from doing bit allocation. That is allocation \nof (say) 18 bits of information. Normally, you would again have to round this up to an INT \nso that you can easily deal with the number, but in this case you can easily deal with \nmultiple tables, one holding a short and another holding the leftover bits. Due to the nature \nof the Push/Pop, bit allocation can be processed fairly simply, either by masking and shifting \ndown or by assuming the next 2 bits are the lowest two in the INT and shifting them off as \nneeded. \nLet‘s see how this could be achieved. First, we need to allocate the arrays and create the \nindex list. \nconst   int                             MAX_OBJ = 0x40000; \nunsigned   short                        Stack_short[MAX_OBJ]; \nunsigned   int                          Stack_bits[MAX_OBJ/16]; \nint                                     SP; \n \n//  \n################################################################\n###### \n///  Function:<summary> \n///              Initialise  the free list \n///           </summary> \n//  \n################################################################\n###### \nvoid   Init( void ) \n{ \n       // Create and fill our 18bit table. \n       // Use Push to build the table as it's easier in this \ncase. \n       SP = 0; \n       for(int i=0;i<MAX_OBJ; i++){ \n             Push(i); \n       } \n} \n \nIn this case, we‘ll use Push to actually create the buffer, as it‘s far easier. This will create a \nlist of indices from 0 right up to 0x3ffff (262,143). \nNext we need to create the Push command itself. \n// \n################################################################\n#### \n/// Function:<summary> \n///             Push an 18bit index onto the object stack \n///          </summary> \n/// \n/// In:      <param name=\"_value\">18Bit number to store</param> \n// \n################################################################\n#### \nvoid Push( int _value) \n\n\n \n \n{ \n      assert(_value>=0); \n      assert(_value<0x40000); \n      assert(SP<MAX_OBJ); \n \n      Stack_short[SP] = (unsigned short)_value&0xffff; \n \n      // Clear the bits we're about to OR into. \n      int shift = (SP&0xf)<<1; \n      int index = SP>>4; \n      Stack_bits[index] &= ~(3<<shift); \n      Stack_bits[index] |= ((_value>>16)&0x3) << shift; \n      SP++; \n} \n \nWhile much slower than a linked list or using the stack with a straight array of pointers, it‘s \nobviously written for storage efficiency, in this case saving around 458 K compared to \nstoring the whole 32-bit pointer. Next, allocation... \n//  \n################################################################\n###### \n///  Function:<summary> \n///                Return the next free index (an 18bit number) \n///           </summary> \n/// \n///  Out :    <returns> \n///                Return the next free 18bit index, or -1 for \nan error. \n///           </returns> \n// \n################################################################\n###### \nint   Pop( void ) \n{ \n      // If none left, then return an error \n      if( SP==0) return -1; \n \n      // Get the main 16bits \n      int val = Stack_short[—SP]; \n \n      // Now OR in the extra bits we need to make up the 18bit \nindex. \n      val |= ( (Stack_bits[SP>>4]>>((SP&0xf)<<1)) &0x3)<<16; \n \n      return val; \n} \n \nThe ease and speed at which you can adapt the stack allocation system to your needs is a \nreal strength, and the ability to rapidly allocate not only pointers, BYTEs, SHORTs, and INTs, \nbut also groups of bits efficiently is always going to be a winner. \n\n\n \n \nUsing this system, you can compress your data right down into as few bits as required \nwithout sacrificing all your speed. There are always better ways to compress data down, but \nthis method often allows you to strike a middle ground in a matter of minutes, not days. \nThese are obviously extreme cases, but when trying to save memory, these types of options \naren‘t usually open to you, so you usually end up either not bothering or wasting valuable \ntime on far more complex methods; after all, how else could you easily allocate using 18-bit \nindices? It‘s normally not such a straightforward problem. \nThe example 18 BitAlloc can also be found on the CD. \n \nPOD Types \nLastly, we‘ll show how to use POD types for more complex indexing requirements. For those \nwho don‘t know, POD stands for Plain Old Data. POD types can store whole structures as \nlong as they fit inside a standard native type, such as an INT, SHORT, or BYTE. Here‘s an \nexample of a simple POD type. \nstruct STileCoordinate \n{ \n      unsigned char    X;    // 64x64 X tile coordinate \n      unsigned char    Y;    // 64x64 Y tile coordinate \n      short            page; // Texture page to use. (also \npadding) \n}; \n \nThis POD stores several pieces of pre-generated information—in this case, a 64× 64 tile \ncoordinate inside multiple 4096× 4096 texture pages. First, let‘s see how you would \nnormally deal with them. \nclass TileCoordinate \n{ \n      STileCoordinate* m_pNext;   // Next in the list \n      unsigned char    m_X;       // 64x64 X tile coordinate \n      unsigned char    m_Y;       // 64x64 Y tile coordinate \n      short            m_page;    // Texture page to use. \n}; \n \nIf we had 10 texture pages, this would mean 40,960 tiles we need to allocate from. At 8 \nbytes per tile (the m_page member is padded for better alignment), this makes 327,680 \nbytes. Calling a standard allocator would then return a pointer to this structure, allowing us \nto use it as an origin for copying tile data into the free space. \nSo how can we do better? First, let‘s remove the pointer, as we know the stack system \ndoesn‘t need that (or rather stores it outside the object itself). This leaves us with a 4-byte \nstructure, and while we could use the very first example to allocate using a pointer to the \nstructure (and saving no extra memory), we can do better. \nSince this data fits inside 4 bytes, we can actually allocate and return the whole structure‘s \ndata directly without the need for pointers. Taking the STileCoordinate type, we can \nactually make an array of these that is the same amount of memory as an array of INTs \n\n\n \n \n(since the whole structure is 4 bytes long). Now modern compilers will recognize this and \nallow us to pass around this data inside a register; that is, the whole structure is copied \naround without the need for pointers or expensive memcpy() operations. In fact, this is \ncopied around at the same speed as a standard integer number. \nNow our Pop() command doesn‘t return a pointer, but a structure, like so: \n// \n################################################################\n###### \n/// Function:<summary> \n///               Return the a free tile structure \n///          </summary> \n/// \n/// Out:     <returns> \n///               Return the next free tile structure, or an \nerror tile. \n///          </returns> \n// \n################################################################\n###### \nSTileCoordinate   Pop( void ) \n{ \n      // If none left, then return an error \n      if( SP==0) return EmptyTile; \n \n      // Get the main 16bits \n      return TileStack[—SP]; \n} \n \nWe need to pre-define what an empty tile is (probably just –1 for X,Y and page), but aside \nfrom that, we get the data passed to us directly. You can then store this as an \nSTileCoordinate (without the need for a pointer) and save yourself 163,840 bytes of \nmemory in the process. \nThe beauty of the stack allocator is that it doesn‘t care what it returns; it will return pretty \nmuch anything to you, and it‘s up to you to decide what that anything is. You could just as \neasily define the type as plain UNSIGNED INT and mask out bits of information as you need \nto, rather than relying on bytes or shorts to make things fit. \nFor example, we could have assigned 12 bits for the X and Y coordinates and 8 for the page \nnumber, which would allow us to specify an actual pixel coordinate inside the 4096× 4096 \ntexture if we really wanted to. The important thing to remember is that the stack system \nreturns data quickly, and that data doesn‘t have to be a pointer to some data or even a \nhandle to data! In fact, it can be an entire structure if you wanted it to be. POD types are \nbest because there‘s no special copying as they fit inside a register, but any type could be \nused. \nYou can find the POD example on the CD in the PODAlloc folder. \n \nKnown Issues \n",
      "page_number": 351
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 359-367)",
      "start_page": 359,
      "end_page": 367,
      "detection_method": "topic_boundary",
      "content": " \n \nOne of the few drawbacks to the stack allocation method is that you currently can‘t do it \natomically, and so it‘s not automatically thread-safe. This means you must manage \nmultiple-thread access yourself using all the standard lock methods normally at your \ndisposal or use per-thread stacks. \nYou also can‘t put any limits on the allocation; that is, you can‘t search through the whole \nlist and allocate one that better suits your needs—not without manually compacting the \nremain elements in the list, that is. If this is exceptional, then it might be worth occasionally \ncompressing the list, but chances are if you need to do this, then you shouldn‘t use this \nmethod. \nThe only other real disadvantage is that there‘s no easy way to keep a used list. In a doubly \nlinked list, you can move an item from the free to the used list, and this can help you debug \nthings because it‘s easy to see what‘s in use. In this system, it‘s very much a free-only list. \nThis doesn‘t mean you can‘t maintain debug tables or lists, but it does mean it‘s not a \nnatural part of the system. \n \nAdvantages and Disadvantages \nFirst, the advantages: \n1. The code is microscopic, so small it can easily be placed inline. The compiler will \noften do this anyway. \n2. Since all the objects‘ pointers or indices are gathered next to each other, they are \nvery cache-friendly. In fact, you‘ll get multiple items per cache line, reducing cache \nmisses to a bare minimum, particularly if you‘re allocating one after the other. \n3. You can allocate a number of any type and of any bit size, from only a few bits to \nlonger, more complex streams, and you can do so almost as easily as any other \nvalue or item. \n4. You can allocate POD types (or bit-packed data) directly and rapidly. This saves on \nhaving to wrap simple types or include pointers through them, thereby increasing \ntheir size. \n5. You don‘t have to modify any object to put links through it or have wrapper objects \nor templates to link things together. This simplifies your code. \n6. It requires either the same or less memory than a 1D linked list system. \n7. Allocation time is constant no matter how many elements you‘re allocating from. \n8. Block allocation is not only possible, but very simple and quick. Depending on use, \nyou could even return a pointer into the stack that holds the allocated block so you \ndon‘t even have to allocate an array to return. \n9. It‘s fast and easy to implement, even on severely restricted CPUs, and in any \nlanguage. It works just as well on an old 6502 as it does on a modern PC. \n10. It‘s very simple to debug, since you don‘t have to follow links all over the place. \nNext, the disadvantages: \n1. You can‘t easily remove items from the middle of the stack. This means it‘s hard to \nkeep an allocated list as well as a free list. \n2. It‘s not automatically thread-safe. \n3. You can‘t easily place any restrictions on allocation without severely affecting \nperformance. \n \nConclusion \n\n\n \n \nThis system has been used to allocate a single bullet from 20, sprites for rendering, \nscripting slots, blocks of memory, particles—you name it. The ease in setting up and using, \nnot to mention the fact that debugging is simply a matter of viewing the stack, makes it \nsimple for any level of ability to implement. Also, on modern hardware, with its relatively \nslow memory and cache lines, the cache coherency is an added bonus that‘s particularly \nuseful when dealing with thousands (or even tens of thousands) of allocations in a single \ngame cycle. Memory bandwidth is the ultimate enemy when dealing with large numbers of \nobjects in a game, and being able to reduce the size of an object handle to an index and \ngroup them into the same cache line can be a big win. Finally, it‘s fast, simple, easy to \nimplement, usually uses less memory than most other methods, and is very easy to adapt \nas your needs change! \n \n4.6. Design and Implementation of an In-Game Memory Profiler \nRicky Lung \nmtlung@gmail.com \nThis gem introduces an architecture and implementation for a low-overhead in-game \nmemory profiler with multi-threading support. With this profiler, one can examine the \nmemory allocation distribution of a running game in a call-stack-style table in real time. \nThese statistics can be extremely valuable for performing memory usage optimization and \nmemory leak tracking. \nIntroduction \nMemory consumption management is one of the cornerstones of technical quality for \ngames. Developers try their best to fit as much content as possible into a finite memory \nresource. We all know that choosing the right tool to do any kind of performance tuning is \ncrucial, as it provides solid data instead of guesswork. Strangely, there is only a very limited \nnumber of memory profiling tools available for C++ in both the open-source and \ncommercial worlds. This gem tries to narrow this gap by providing a lightweight memory \nprofiling library. Now, let us have a look at what it will provide. \nThe profiler can generate a call graph at a given time interval, as shown in Figure 4.6.1. \nEach column of the table reveals useful information: \n \nName. Mimics the program structure as a call stack, where the name of the function \ncomes from the user-supplied string literal. \n \nTCount. Total number of allocations currently made in the calling function and its \nchild call. \n \nSCount. Number of allocations currently made in the calling function without \ncounting its child call. \n \nTkBytes. Total amount of memory currently allocated in the calling function and its \nchild call, in units of kilobytes. \n \nSkBytes. Amount of memory currently allocated in the calling function. If you are \nlooking for the memory eater, this column might be the first place to look. \n \nSCount / F. Number of allocations performed by this function per frame. You might \nwant to reduce this number such that less run-time overhead of your game is spent \non memory allocation/deallocation. \n \nCall / F. Tells you how many times a function is invoked in a single rendering frame. \nFigure 4.6.1. A view of the memory profiling remote client. \n\n\n \n \n \n \nMemory Profiling Basics \nThe memory profiling mechanism can be divided into three main parts: collect memory \nallocation information, relate the collected information with the program structure, and, \nfinally, present the results. At first glance, the technique we use in CPU profiling can be \napplied: taking a measure at the beginning of a code block of interest and again at the end, \nminus their difference. But the challenge is yet to come, because memory allocated within a \ncode block can be deallocated somewhere else. Therefore, instead of collecting information \nfor a single code block at a time, we turn the problem into asking which corresponding code \nblock the current memory operation is related to by intercepting every memory allocation \noperation. \nThe remaining sections of this gem will discuss how to utilize function hooking to intercept \nallocation operations and how to get their corresponding call-stack information and \nstatistics. Of course, in this multi-core era, we will add support for multi-threaded \napplications. \n \nFunction Hooking \nOne simple way of intercepting memory allocation is defining your own new/delete operator \nin C++, but it has the limitation of not being able to intercept code that you don‘t have \nsource code access to. Instead, we use a more low-level approach called function hooking, \nwhich is extremely powerful. \nThe term ―hooking‖ in computer programming covers a range of techniques used to alter or \naugment the behavior of an application. Some applications, such as DirectX/OpenGL \ndebuggers and profilers, use such a technique. Although it can be very complicated to hook \nother processes‘ functions as those standalone profiling applications do, in this memory \n\n\n \n \nprofiler we only need to do some patching on the memory that a function is resident in. \nAfter the patching is done on a function—for example, the malloc function—whenever \nanywhere else in the program tries to invoke it, a proxy function myHookedMalloc is \ninvoked instead of the original one. The patching process needs a few assembly tricks, so \nlet‘s get our hands dirty and examine what the assembly of the malloc function looks like \nunder x86 first. \nvoid * __cdecl malloc(size_t size) { \n    78583D3F mov    edi,edi \n    78583D41 push   ebp \n    78583D42 mov    ebp,esp \n    78583D44 push   esi \n    ... \n \nThe above assembly shows the first four assembly instructions of the malloc function, \nwhich prepare the stack and register for use with that function, commonly called the \nfunction prologue. We will replace the first instruction with a jump instruction. \nvoid * __cdecl malloc(size_t size) { \n78583D3F jmp    myHookedMalloc \n78583D41 push   ebp \n78583D42 mov    ebp,esp \n78583D44 push   esi \n... \n \nWith the unconditional jump instruction added to the very beginning of the malloc \nfunction, all control flow will be directly transferred to our proxy function. \nvoid* myHookedMalloc(size_t size) { \n    void* p = (*originalMalloc)(size); \n    logMallocUsage(p, size); \n    return p; \n} \n \nWhat the proxy function does is simple: It invokes the original malloc function to perform \nthe actual allocation and log the memory size along with the allocated memory pointer for \nfurther processing. We then need to resolve the problem of how we can invoke the original \nmalloc function. To get back the original function, we need to back up the first assembly \ninstruction before we perform the patching and store it in an executable memory location. \n(Note that you can make a block of memory executable on the Windows platform using \nVirtualProtect.) Following that memory location, put a jump instruction pointing to the \nsecond instruction of the original function. \nmov        edi,edi \njmp        &malloc + sizeof(mov   edi,edi)   = 78583D41 \n \nWe give this block of executable memory a meaningful name, originalMalloc, with the \nsame function signature as malloc does. These backup instructions can also be utilized for \nrestoring the patched function when the memory profile gets shut down. One particular \nissue that we haven‘t addressed is how to get the size of a binary assembly instruction, \n\n\n \n \nwhich is necessary to perform the replace and copy operations correctly. A easy way to \ntackle this problem is to simply hardcode the instruction size, but this will create \nmaintenance problems because different C run times may have different compiled binary \ncode, even for the same function. A better solution is to calculate it at run time. Luckily, \nthere are free libraries that can do the task, such as libdasm. \nAll the aforementioned details of function hooking are encapsulated into a single class with \nonly three functions, which can be found on the CD-ROM. \nclass FunctionPatcher { \n    void* copyPrologue(void* func, int givenPrologueSize); \n    void patch(void* func, void* replacement); \n    void unPatchAll(); \n} \n \nAgain, using malloc as the example, the usage of the class is as follows. \nvoid (*origianlMalloc)(size_t); \nFunctionPatcher functionPatcher; \norigianlMalloc = functionPatcher.copyPrologue(&::malloc, 5); \nfuncitonPatcher.patch(&malloc, &myMalloc); \n \nAlthough all the source code and example here only work under x86, the same concept \nshould be able to be applied to other platforms, including game consoles. \n \nCall-Stack Generation \nIn the current generation of software architecture, most programs are built on a command \nand control scheme: One method calls another method and instructs it to perform some \naction. The place that stores the nested method call information is the call stack. From a \nlow-level point of view, it is just a simple memory block with some integer offsets giving the \nstack size; while at a higher-level point of view, we can use a graph structure to represent \nthe call stack. (The terms call graph and call stack can be used interchangeably.) \nTo collect the full call stack of a running program, a stack-walker [Gaurav08] with debug \nsymbol information can be used. However, generating a full call stack is not only time \nconsuming, but it also clutters the profiling results. Thus, a custom function call annotation \nscheme inspired by the previous Gems articles [Rabin00] and [Hjeistrom02] is used instead, \nwhich gives users a more flexible way to collect statistics for their most important functions. \nThose applications that already use similar profiling tools can easily integrate with this \nmemory profiler by combining the scope variable/macro. \nWe use a tree-like structure in C++ to mimic the actual call-graph structure. A tree is used \ninstead of a graph because we will accumulate the profiling data recursively instead of \nmaking more general node connections. This tree structure contains a number of nodes, \nwhile each of these nodes contains the necessary memory profiling statistic and the link to \nits sub-nodes. Obviously, the root node of the tree represents the entry point of the \nprogram, usually the main function. More nodes can be added to the tree on user‘s request, \nthrough declaring a scope variable. \nvoid createMesh() { \n\n\n \n \n    ProfilingScope scope(\"createMesh\"); \n... \n} \n \nSuppose Function A calls Function B, and both functions have a scope variable declared. \nThen a node named B will be inserted as a child of the node named A. The next time \nFunction A invokes B, a linear search for all the child nodes under A with the name B is \nperformed. This implementation assumes that the name argument of the scope variable \nmust be a string literal; therefore, we can use a simple pointer comparison instead of a full-\nblown string comparison during searching. Provided that the average number of child node \nis around the order of 10, the call-stack generation process is reasonably fast. \nFor the hooked memory allocation function to make use of the call-stack information, the \ncall tree will store a variable that points to the current call-stack node. Thus, a connection \nbetween the current allocation and the current call stack is made. \n \nCollecting Statistics \nOnce we have the call-stack data structure on hand, we can start to collect statistics. Only a \nhandful of statistics need to be stored in each node: \n \nInvocation count \n \nExclusive allocation count \n \nExclusive allocation size \nThe exclusive allocation count and size will be updated whenever the hooked functions are \ninvoked. Apart from simply updating the statistics, the hooked functions need to perform \nsome bookkeeping in order to retrieve the corresponding call-stack node for a particular \nallocated memory block during the deallocation operation. \nThe mapping between the two can be established using an STL map, but this results in a \nhigh run time and memory overhead; therefore, we use the approach of embedding the \ncall-stack node pointer with the allocated memory block. \nvoid* myHookedMalloc(size_t size) { \n    void* p = (*originalMalloc)(sizeof(Node*)+sizeof(int)+size); \n     (Node*)p = currentCallStackNode; \n    *(int*)((char*)p + sizeof(Node*)) = size; \n    currentCallStackNode->allocCount++; \n    currentCallStackNode->byte += size; \n    return (char*)p+sizeof(Node*)+sizeof(int); \n} \nvoid myHookedFree(void* p) { \n    int allocSize = *(int*)((char*)p-sizeof(int)); \n    p = ((char*)p-sizeof(Node*)-sizeof(int)); \n    Node* n = (Node*)p; \n    n->allocCount—; \n    n->byte -= allocSize; \n     (*originalFree)(p); \n} \n \n\n\n \n \nOther statistics that take into account the child function calls can be calculated on the fly \nduring profile report generation, because this step needs to traverse the call tree anyway. \n \nMulti-Thread Issues \nBy definition, each executing thread should have its own call stack. This means every call of \nmyHookMalloc can access the per-thread call-stack data structure freely without any \nproblem. Thread Local Storage (TLS) comes to assistance here. For each thread, we store \nthe pointer to its corresponding call-stack root node and another pointer to the current node \nusing the win32 API TlsSetValue. This data will be retrieved by myHookedMalloc \nusing TlsGetValue. Care has to be taken in implementing myHookedFree because it \nmay operate on a call-stack node that doesn‘t belong to the current thread. \nTo protect the deallocation routine from race conditions, each thread‘s root node is assigned \na critical section, and all its ancestor nodes will keep a reference to it. When \nmyHookedMalloc or myHookedFree is invoked, the critical section of that node will be \nlocked before any statistic information update. Such a design keeps the number of critical \nsections to a minimum while preserving a very low lock contention level; moreover, it does \nnot create a locking hierarchy, so it should be free from dead-lock. \n \nThe Source Code \nThe source code on the CD comes with Microsoft Visual Studio solutions that allow you to \ncompile and execute several demo programs related to the memory profiler. Although the \ncode snippets in this gem are simplified for illustration purposes, the code on the CD is \ncarefully crafted for maximum correctness and robustness. A client-to-server architecture is \nalso employed in one of the demos, which shows you how to make an external tool to \nmonitor the memory usage of your application. \nA CPU profiler based on the same code base as the memory profiler is also supplied as a \nreference. \n \nConclusion \nBy moving a step forward, our in-game CPU profiling techniques can be extended to \nmeasure memory usage as well. This gem also demonstrated how to modify the profiler to \ncope with the multi-core era. We hope you will utilize this profiler to develop a greater \nunderstanding of your program, locate memory hotspots, and improve your memory \nconsumption and performance. \n \nReferences \n[Gaurav08] Kumar, Gaurav. ―Authoring Stack Walker for X86.‖ 6 Jan. 2008. WinToolZone. \nn.d. <http://www.wintoolzone.com/PermaLink.aspx?ID=141>. \n[Rabin00] Rabin, Steve. Game Programming Gems. Boston: Charles River Media, 2000. \n\n\n \n \n[Hjeistrom02] Hjeistrom, Greg, and Byon Garrabrant. Game Programming Gems. Boston: \nCharles River Media, 2002. \n \n4.7. A More Informative Error Log Generator \nJ.L. Raza and Peter Iliev Jr. \njraza@versus-software.com, pete.iliev@gmail.com \nProgrammers commonly create error logging functions to aid in debugging during \ndevelopment. Usually, these functions print out error messages onscreen or to a log file. \nAlthough it is functional, the error message is only as good as the programmer‘s insight into \nhow the problem could have occurred. In other words, a one-line message is not always \nenough to help fix the bug. You may need to know which function passed in bad data or \nhow the program got into this bad state, and that is what this gem seeks to help with. \nIn this gem, we‘ll present a new error logging function that automatically generates useful \ninformation for the programmer and/or tester via what is known as retrieving the Run-Time \nStack Information (RTSI). \nDefinition of RTSI \nRetrieving the RTSI is a debugging tool conventionally used to aid in software development. \nDuring debugging, stack walking is the act of pausing the program and taking a glimpse into \nthe program‘s function stack at run time. For example, given the following code: \nvoid C() \n{ \n      int i = 0;//set a breakpoint here \n} \n \nvoid B() \n{ \n      C() ; \n} \n \nvoid A() \n{ \n        B() ; \n } \n \nint   main() \n{ \n        A() ; \n        ... \n        return 1 ; \n} \n \nWhen the compiler reaches the breakpoint in function C(), and the programmer requests \nthe program‘s run-time stack information, it would return something like this: \n\n\n \n \nC \nB \nA \nMain \n... \n \nThe bottom of the stack shows the program‘s entry point. For simplicity, the operating \nsystem functions called have been omitted from this example. In this case, the entry point \nis main(). It then details what function main() called, A(), and recursively what \nfunction A() called, and so on until it reaches the current function where the breakpoint \nwas set. As the flow of execution on the program continues, retrieving the RTSI again will \nreturn a different result based on where the program‘s execution is paused. \n \nPotential Uses \nRTSI is a normal component in many commercial and open-source compilers. It can be \nuseful for tracking stack-overflow bugs, as well as taking a snapshot of the program‘s \ncurrent execution status once it hits a breakpoint. \nMost often, programmers will only use the RTSI inside their compiler. But when working in \ngame development teams, there are more people around than just programmers. There are \nartists, level designers, and testers. All of these people will be playing the game. Yet, not \neveryone may have access to a compiler or a debug development kit, and anyone can run \ninto a crash bug. Having the program dump out as much info as possible during these \nsituations is vital, and dumping out the RTSI can be a big help. \nThe RTSI‘s usefulness extends beyond just the work environment, too. Online games, such \nas MMORPGs or RTS games, do beta testing where they release the game to a select few \npeople in their homes and let them play the game before it‘s out. The main intent is \nbalancing and getting feedback. Beta testing is also valuable time that can be used for \nbulletproofing the game. So as not to waste the testing time, whenever bad data is \nencountered or a crash occurs, the RTSI can be sent from the tester‘s machine to a \ndeveloper-owned server for use later. \nC/C++ does not have a standard function to call to print the program‘s RTSI, so it‘s up to \nthe operating system and hardware vendors to distribute a set of functions in an API to \naccomplish this job. In this gem, we‘ll explore the API functions on the Windows XP platform \nfor gathering the RTSI on an x86 processor. The example will also work on x64 processors, \nbut some of the assembly code will need to change to reference the correct registers. \n \nSetting Up the Code \nSo you have decided that the RTSI is a resource that‘s going to be used in your game. \nHenceforth, it needs to go through the three basic steps any resource goes through: \nloading, usage, and unloading. \nLoading \nSince the RTSI resource will probably be used in several different spots in the game code, \nwe have the issue of when to load it, when to unload it, and how to handle access to it. All \n",
      "page_number": 359
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 368-375)",
      "start_page": 368,
      "end_page": 375,
      "detection_method": "topic_boundary",
      "content": " \n \nthese issues can be solved by setting it up as a singleton resource. For reference on \nsingletons, see [Bilas00]. To load our RTSI API function calls, we use the following code: \n//load the dbghelp.dll which is the windows library used \n//for crawling up the stack \nm_dllHandle = LoadLibrary( \"dbghelp.dll\" ); \n \n//sets up which processes symbols \n//we are going to look at \nm_pSymInitialize = (SymInitialize) GetProcAddress( \n      m_dllHandle, \"SymInitialize\" ); \n \n//used to crawl up the stack \n//and look at each function call \nm_pStackWalk64 = (StackWalk64) GetProcAddress( \n      m_dllHandle, \"StackWalk64\" ); \n \n//used to retrieve the line number in each file the \n//function was called from \nm_pSymGetLineFromAddr64 = (SymGetLineFromAddr64) GetProcAddress( \nm_dllHandle, \n\"SymGetLineFromAddr64\" ); \n \n//used to get the name of each function in string form \nm_pSymGetSymFromAddr64 = \n      (SymGetSymFromAddr64)GetProcAddress( \n             m_dllHandle, \"SymGetSymFromAddr64\" ); \n \n//get and set for the options of what information we want \n//to retrieve. \n//this example only cares for line numbers and \n//function names. \nm_pSymGetOptions = (SymGetOptions) GetProcAddress( \n       m_dllHandle, \"SymGetOptions\" ); \n \nm_pSymSetOptions = (SymSetOptions) GetProcAddress( \n       m_dllHandle, \"SymSetOptions\" ); \n \n \nUsage \nWe need to set up an access point to the RTSI with the required function pointers. We only \nneed to activate it when something is about to go wrong; this is what the ASSERT() call is \nfor. For reference on how to use that and a few handy tricks, see [Rabin00]. \nThis access point can be created as such: \n#if DEBUG \n      #define ASSERT( expression ) \\ \n            if ( !expression ) \\ \n            { \\ \n                  printf( \"ASSERT[\" #expression \"]\\n\" ); \\ \n\n\n \n \n                  AssertWrapper::a.PrintStack(); \\ \n                  printf( \"\\n\\nPress enter to continue:\\n\" );\\ \n                  getchar(); \\ \n            } \n \n#else \n       //If we aren't in a debug configuration then just compile \nout \n       //Asserts \n       #define ASSERT( expression ) \n#endif \n \nNow, when we call the ASSERT() macro and the result is false, we‘ll print out the RTSI. \nNotice that it‘s the PrintStack function inside the Assert class that does the interesting \nwork. There are a couple of important pieces of code in that function. The first is how we fill \nthe CONTEXT data structure: \nCONTEXT context; \nmemset( &context, 0, sizeof(CONTEXT)); \ncontext.ContextFlags = CONTEXT_FULL; \n \n__asm    call x \n__asm    x: pop eax \n__asm    mov context.Eip,  eax \n__asm    mov context.Ebp,  ebp \n__asm    mov context.Esp,  esp \n \nHere we are copying the instruction, frame, and stack pointers into Context. This will let \nthe StackWalk function know where to look for all the debugging info we need. Normally, \nyou would copy the instruction pointer directly (EIP), but on x86 processors there is no \ndirect way to access that pointer, so we use a trick. This section of code is processor family–\nspecific, so the register will probably be named differently for the different companies. For \nexample, x64 (AMD) uses Rip, Rbp, and Rsp. \nThe other important aspect of the PrintStack function is the loop that displays the \nfunctions on the run-time stack. \nunsigned int i = 0; \ndo \n{ \n      // get the next stack frame info \n      this-<m_pStackWalk64( IMAGE_FILE_MACHINE_I386, \n                             process, \n                             thread, \n                             &frame, \n                             (void*)&context, \n                             NULL, \n                             SymFunctionTableAccess64 , \n                             SymGetModuleBase64, \n                             NULL ); \n \n\n\n \n \n      //now we try to print the current frame's info \n      if ( frame.AddrReturn.Offset != 0 && i != 0 ) \n      { \n            char line1[32]; \n            char line2[32]; \n            memset( line1, '\\0', 32 ); \n            memset( line2, '\\0', 32 ); \n            PrintLineNumber( process, frame.AddrPC.Offset, line2 \n); \n            PrintFuncName( process, frame.AddrPC.Offset, line1 \n); \n            printf( \"%-32s || %s\\n\", line1, line2); \n      } \n      i++; \n} while( frame.AddrReturn.Offset != 0 ); \n \nThus, if we want to integrate the RTSI to a game GUI, then that is where we would make \nour changes. \nUnloading \nTo unload it, we perform the reverse of the loading steps. \nFreeLibrary( m_dllHandle ); \n \nm_pSymInitialize = NULL; \nm_pStackWalk64 = NULL; \nm_pSymGetLineFromAddr64 = NULL; \nm_pSymGetSymFromAddr64 = NULL; \nm_pSymGetOptions = NULL; \nm_pSymSetOptions = NULL; \n \n \n \nError Logs Redundancy Problem versus Using RTSI \nHaving explained the RTSI and how to implement it in a non-compiler environment, we can \nnow begin to exemplify scenarios in which this feature could be quite useful. It is common \nfor programmers to create a set of functions that display an error message when something \ngoes wrong during testing. The problem with a generic error log function is that the \nprogrammer has to keep track of where he is in the code in order for the error message to \ncreate any useful information. Following is an example of this common scenario. \nint A() \n{ \n      if   ( did_an_error_occur()   ) \n      { \n              assert( ! \"Problem with function A()\" ) ; \n              return 0 ; \n      } \n      else \n\n\n \n \n      { \n              ... \n              return 1 ; // All went well \n      } \n} \n \n \nint B() \n{ \n      if   ( did_an_error_occur   () ) \n      { \n              assert( ! \"Problem with function B()\" ) ; \n              return 0 ; \n      } \n      else \n      { \n              ... \n              return 1 ; // All went well \n      } \n} \n \nint C() \n{ \n      if( A() && B() ) \n             return 1 ; \n \n      return 0 ; \n} \n \n... \n \nThe problem with this design is that as the software evolves, so will its function calls and \nthe context in which these functions are called. Keeping track of those error messages adds \nextra overhead to the development of the game. This could be solved if the error log \nfunction could ―know‖ its current context and report accordingly, which is exactly what RTSI \ncan do. \nWith the given code sample, the programmer could then bind the error log to the in-game \nGUI. This means that once an error log function gets triggered, a tester can report to the \nprogrammer a wider span of technical information (since the log is intrinsically related to \nthe game‘s code) as well as not have to resort to an error check table. \n \nConclusion \nIt‘s important for there to be a continuous flow of information among the staff on a game \ndevelopment team. Unfortunately, quantitative data regarding when a game hits an error in \na non-compiler environment is not an easy thing to capture. With tools like the RTSI, these \npotential problems can be smoothed out. One could even bind the RTSI entry point to a \nscript that fills a form in a bug database with this kind of information so that it could be \nlater analyzed by the programming team. \n\n\n \n \n \nReferences \n[Bilas00] Bilas, Scott. ―An Automatic Singleton Utility.‖ Game Programming Gems. Boston: \nCharles River Media, 2000. 36–40. \n[Rabin00] Rabin, Steve. ―Squeezing More Out of Assert.‖ Game Programming Gems. \nBoston: Charles River Media, 2000. 109–114. \n \n4.8. Code Coverage for QA \nMatthew Jack \nmtj22@cantab.net \nWhen working on a rapidly developing code base, a good QA team is a highly versatile \nresource that no automated testing can replace. However, actually playing the game is such \na high-level form of testing that it can be hard to relate it to low-level changes in the code. \nFirst, changes that a programmer has made may only be executed in certain situations \ndepending on the player‘s actions, which can be hard to give a procedure to re-create. For a \ngiven piece of low-level code, a programmer may have little idea where and when in the \ngames it is actually used! Further, even when we can give clear instructions for testing, we \nare rarely absolutely sure whether they worked and the code was actually executed; in \nsome cases there will be a clear visible indication, but in many others the effect of the code \nis subtle. Often, it is simplest in the end to throw up our hands, give no details, and just ask \nQA to ―be thorough‖—which is time-consuming for them and still provides no guarantees \nabout the result. \nThis gem describes a framework that combines code coverage analysis and conventional QA \nby using real-time feedback. It addresses these issues and improves the testing process for \nboth QA and programmers. It can bring rigor and repeatability to testing and yield valuable \ninsight to the low-level programmer about where the code is actually used. \nCommon Approaches \nBefore examining the approach of this gem, let‘s consider some of the established testing \nmethodologies. \n \nUnit testing. Especially for localized, pure refactoring, unit testing can be rigorous \nand fast. However, to be pragmatic, most games are developed without \ncomprehensive unit testing. One reason for this, at least for rapidly changing game \ncode, may be that unit testing is considered to add too much development overhead. \nIf unit tests are not present in the original code, then restructuring to add them is \nlikely to be an enormous task that could itself introduce bugs. \n \nAutomated functional testing. For instance, a repeatable play-through using an \ninput recorder. This is very convenient for detecting crashes; however, an \nexperienced eye is still required to watch for changes in behavior, and it in no way \naddresses the thoroughness of the original play-through. Furthermore, if the game \nyou are testing is currently under development, it may change so quickly that a \nrecording has a very short lifespan. \n \nQA testing. Plain old QA is essentially a manual form of functional testing. A good \nQA team can adapt rapidly to a changing specification, report changes in behavior, \n\n\n \n \nand benefit from human intuition when something looks wrong. These are valuable \nqualities to build upon. \nIn all of these approaches, we might ask: How complete is their testing? One way of \nassessing this is through code coverage analysis. This measures how rigorously a given test \nexercises source code, allowing this to be quantified and improved. Fundamental to this \ngem, we will examine it in detail. \n \nAn Analogy: Breakpoint Testing \nThere is a rough-and-ready version of this gem that I have often employed, and it turns out \nit is already available in your IDE. We might call it breakpoint testing. \nIn refactoring a function/class/file/blob, one systematic approach to testing on the \ndeveloper‘s machine is to place a breakpoint on every statement that represents a relevant \ncode path and then run the game, removing each breakpoint as it is hit. When all \nbreakpoints have been removed, without any crashes or unexpected behavior, testing is \ncomplete. \nThis can be a good minimal test of changes, as it ensures the developer has executed all the \ncode he changed and has observed the overall results. However, playing through takes \ntime, and it must be done on that computer, so this cannot easily be delegated. \nFurthermore, it is not repeatable: Future regression tests would require re-creating all of \nthe breakpoints. \nHowever, note that breakpoint testing works for new code as well as refactored code and \nthat it directly couples high-level functional tests to low-level details in the source. \nThis gem is a framework that formalizes and expands that ad hoc hybrid of code coverage \nand manual testing. \n \nCode Coverage \nConventional code coverage analysis is used to quantify how completely a testing procedure \n(usually automatic) exercises source code [Cornett96]. It works by monitoring at run time \nthe number of unique lines (or functions, branches, code paths, and so on) that have been \nexecuted by the test, compared to the number theoretically possible. This basic principle of \ndiscovering which code we have actually run is attractive, but there are several questions \nwe must address when applying this to games. \nThe first is what kind of testing procedure we should use, since code coverage is only useful \nin analyzing tests. Unit testing is the standard in the wider software industry, but for games \nwe propose QA testing of real levels. This is simple and flexible, and chances are your \ncompany is already doing it every day. It also better represents the end product, where \ncode and assets intertwine to determine final quality. \nNext, we have to consider granularity. Instrumenting every line will cause a significant \nslowdown that may make the game unplayable—especially on consoles—and the increased \nexecutable size may prevent it from loading at all. Furthermore, this is sheer data overload: \nSuch a glut of information would take further processing to make any sense to us. \nMonitoring every function is too arbitrary—some functions are trivial and/or speed-critical, \nwhile others may, for instance, contain large switch statements. Tracking all branches may \nagain generate too much data. \n\n\n \n \nAlso, we must consider the final output, which is related to the granularity. A list of all the \nnumbers of all the lines executed would be meaningless in itself—and rendered useless as \nsoon as the source code changes. Function names would be instantly recognizable, but what \nif we change them? \nThe result most commonly quoted from code coverage is a single figure: a percentage \nrepresenting the completeness of testing, where targets are commonly 90- to 100-percent \ncoverage [Cornett96, Obermeit06]. Note that if we use real levels as tests, in most games \nwe will see much lower coverage, as each level usually employs only a subset of the \nfeatures. If we accept that we will only test a fraction of our code, we have an ambiguity: \nWas a given feature left unexercised because testing was insufficient or because it is not \nused in this level? It may also beg the question: Much of the code may have been tested, \nbut was this piece of code tested? \nFinally, if we use human testers to perform the tests, our results may vary wildly with \ndifferent play-throughs. Can we guide them toward comprehensive, repeatable testing? \nThere are various code coverage packages already available, but without addressing these \nissues, their value in practical games development is limited. \n \nImplementation \nKey to our approach is that, rather than use any automatic instrumentation, the \nprogrammer places his own markers into the code using a simple macro. This allows us to \nsolve the problems of granularity and meaningful output and leads us to solutions for the \nremaining issues. \nBy using manual markers, we can keep them to a manageable number, applied only where \nrequired and omitted from performance-critical or trivial code. This—and careful \nimplementation of the marker code—avoids slowdown and code-bloat. \nWe use a unique string to label each marker. You could base these on the class and method \nname—for example, CCoverBehaviour_ThrowGrenade_A—or use a pure feature \ndescriptor—for example, ThrowGrenadeFromCover_A. The labels make the output \nmeaningful, while still allowing the programmer to add markers to individual if/else \nbranches, cases of a switch statement, loop bodies, or anywhere else he thinks they may be \nuseful. \nThe Markers \nThe following macro expands to code that constructs a static object with a trivial constructor \nand calls its Hit() function: \n#define CCMARKER( label ) \\ \n  { \\ \n    static CCodeCoverageMarker ccMarker_##label( #label ); \\ \n    ccMarker_##label.Hit(); \\ \n  } \n \nwhere Hit() registers the marker with a singleton [Alexandrescu01] for tracking the first \ntime it is executed. Keeping this method separate allows for a reset feature. \ninline void Hit() \n\n\n \n \n{ \n  if (!m_bHit) \n  { \n    m_bHit = true; \n    CCodeCoverageTracker::GetInstance().Register(this); \n  } \n} \n \nSome of the advantages of this approach include: \n \nMacros are easily compiled out, ensuring zero performance impact in the release. \n \nUsing a static object allows a fast check for whether this is the first time hit. \n \nFunctions can be renamed or moved into a different class or file, but while marker \nlabels are preserved, code coverage results remain undisrupted. This is helpful as we \nbuild up a history. (See the upcoming ―Collecting Results‖ section.) \n \nDescriptive labels immediately provide hints for real-time feedback to QA. When \nwondering how to hit that last one percent of markers, a tester will really appreciate \nthe onscreen text ―ThrowGrenadeFromCover_A‖. \nC++ compilers include predefined macros that could generate marker labels automatically, \nbased on function names, line numbers, and so on. Note, however, that for this saving of a \nfew moments, we lose much of the advantage of a manually defined label. \nIf labels are not unique, our results will of course be misleading. This would usually occur if \nsomeone copied and pasted code somewhere else, but it could also occur if a single marker \ndeclaration was instantiated multiple times in the binary. This could happen through use \nwithin macros or templates. Functions inlined as an optimization are not a problem, as the \nC++ standard defines that all inlined copies of a function must share one copy of any local \nstatic variables [ISO/IEC14882-98]. Note that some older compilers did not handle this \nproperly. \nThe best way to avoid the more common copy/paste problem would be to write a simple \nscript to scan the source code for duplicate labels. All types of duplicates can also be \ndetected by the framework itself at run time—the example implementation includes such a \ncheck. \nIn my own experience, there have been no problems with duplicates. \nExamples: Applying Markers \nMarkers are commonly embedded at the start of methods: \nvoid BrainClass::Update() \n{ \n  CCMARKER(\"Brain_Update\"); \n  UpdateVision(); \n  UpdateBlackboard(); \n  ... \n} \n \nwhile, if the method includes various early-out checks, it is most effective to place the \nmarker at the end, where the ―meat‖ of the code has already been used: \n",
      "page_number": 368
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 376-394)",
      "start_page": 376,
      "end_page": 394,
      "detection_method": "topic_boundary",
      "content": " \n \nvoid Brain::SendSignal( const char * signal, int targetID) \n{ \n  if (targetID == m_myTargetID) // Am I the target? \n    return; \n  if (GetDistanceToTarget(targetId ) > 20.0f ) // Is  it too \nfar? \n    return; \n  ... \n  CCMARKER(\"Brain_SendSignal\"); \n} \n \nHere I‘ve used a structured name, confident that sending signals will always be a \nresponsibility of my Brain class, or perhaps distinguishing it from other places where \nsignals are sent—but in many cases you may prefer a totally pure name, as below. Where \nyou only describe the feature, you can freely move the code and preserve the label without \nit becoming misleading. \nYou can also include multiple markers at different points in the same scope if desired, in \nloops (if not speed-critical): \nfor (int i=0; i<nSignalsToSend; ++i) \n{ \n  CCMARKER(SignalSent); \n  SendSignal(i); \n} \n \nor in branches of switch statements: \nCObject * pObj = NULL; \nswitch (n) \n{ \n   case 0: CCMARKER(FetchedLeader); \n          pObj = m_pLeader; \n          break; \n   case 1: CCMARKER(FetchedCompanion); \n          pObj = m_pCompanion; \n          break; \n   ... \n} \n \n \nThe Tracker \nAs earlier, each marker registers with the tracker the first time it is hit, which maintains the \nset of all markers hit thus far. In the example implementation, the tracker adds the marker \nto an STL vector and does nothing more; all other work is put off to help avoid disrupting \ncaches and skewing profiling results. \nThe markers are completely unknown to the program until they are hit, because their scope \nis local to function bodies. A simple script could extract the complete set from the source, \nbut usually we are only interested in the subset used by a given level. \n\n\n \n \nPer-Level Coverage Input File \nSince we have established that real levels only use a subset of features, we keep a separate \nlist of expected marker labels for each level. Once loaded, this defines the 100-percent \ncoverage that testers will aim for. After the testing process, these files are updated using \nthe results from code coverage—in particular, adding any ―unexpected‖ markers that were \nhit. \nWe can form the initial set of input files in the same way, employing ―blind‖ testing to find \nmost of the markers without guidance. We can then improve upon them in subsequent runs \nas testers find any remaining markers, gradually raising the bar for test quality. \nThis also smoothly assimilates new code: The markers will appear in results as \n―unexpected,‖ confirming to the programmer where they were used, and will be expected in \nthat level from then on. \nMaintenance of these per-level coverage files should obviously be automated by a \nprocessing script, but summaries of their changes are informative. (See the upcoming \n―Collecting Results‖ section.) \nThe Manager \nThe manager class loads the per-level input file containing the set of ―expected‖ marker \nlabels for the level being tested. It also monitors which of these have actually been hit, \nwhich are still remaining, and whether any ―unexpected‖ markers have been hit. The class \nexposes a simple interface to this data, allowing progress to be checked and results to be \noutput. \nThe same interface makes it easy to write a GUI for real-time feedback. We describe this in \nthe next section. \n \nQA Interface and Workflow \nGetting the most out of this approach requires real-time feedback, guiding testers toward \n100-percent coverage while making the smallest changes possible to workflow. One-\nhundred percent is often a realistic target. Markers have been deliberately placed and so \nshould be reachable. They must have been hit in this level before to be ―expected,‖ and we \nprovide guidance in reaching the last few percent. However, in branching levels it may be \nnecessary to combine multiple runs. (See the upcoming ―Collecting Results‖ section.) \nThe GUI \nThe implementation within CryENGINE, as shown in Figure 4.8.1, uses a simple onscreen \ndisplay comprising: \n \nProgress bar (0 to 100 percent ―expected‖ markers hit) \n \nIndication whenever a marker is hit for the first time \n \nList of the remaining marker labels as hints \nFigure 4.8.1. Code coverage GUI implementation within CryENGINE, including \nprogress bar, hit indicators, and a list of markers hit within the last few seconds. \n\n\n \n \n \nThe list of expected markers for each level is stored as a separate file in the level folder \nitself, loaded automatically when the code coverage display is activated. \nTo avoid clutter, it is best to only display marker labels when few remain. Normal testing \nshould easily hit 90 percent of markers reachable in a given level, but guidance on the last \nfew is important if high coverage scores are to be reliably reached. \nLabels as Hints \nIt would be a fair criticism that in some cases the ―meaningful‖ labels given by \nprogrammers may be useless when appearing out of context on a tester‘s screen. However, \nthey form the vocabulary of a common language between testers and programmers, \nallowing testers to find out how they might hit that specific marker and to communicate that \nknowledge to others. \nFor example, while ―Throw_Grenade‖ might be helpful, ―Reorganise_Brain_E‖ is less \nintuitive, but for the programmer still only a moment‘s string search away— hence, it can \nbe clarified by a quick email from tester to programmer. Also, should a tester discover how \nto hit this infamous marker through trial and error, he can easily relate that to others on the \nQA team. A Wiki or similar maintained by QA would be a good repository for details of any \ntricky markers. \nWe make it simple for programmers to help QA achieve high coverage, but we also give QA \nthe means to be self-sufficient in this regard, so in practice little or no programmer time \nshould be required. \nFigure 4.8.2. Code coverage within CryENGINE, when 93 percent of markers for \nthe level have already been hit and the remainder are displayed as hints to the \ntester. \n\n\n \n \n \n \nOutput of Results \nResults are output as a list of labels of the markers hit, either dumped to a log file or sent \nautomatically over the network. \nThe use of log files is simple and quite robust. Since it is common for testers to send logs \nwith their bug reports, it may be best for workflow to add them to the existing log file in an \neasily extracted format. This can be as the complete status of known labels in one block, or \ndumped periodically or manually, or you could also write individual labels as their markers \nare hit. The latter has the advantages of forming a record of when they were hit and losing \nno results should the game crash, although it does add clutter to the log. Log files must be \ncollected by testers and delivered to the programming team. \nSending results over the network is potentially much more elegant, with no impact on QA \nworkflow and coping easily with crashes. However, it requires more work to set up and \nsome thought in its deployment. (See the upcoming ―Possible Expansions‖ section.) \nQA Workflow \nFinal instructions to QA are simply: \n \nLoad the level, activate coverage, and test as normal. \n \nTry to reach 100-percent coverage, using the textual hints to help if need be. \n \nSend the logs as usual when you‘re finished (if you use a log-based approach). \nYou may also ask testers to quit and restart the game between levels—this is to reset the \ncoverage state and thus avoid mixing results. However, in the example code provided with \nthis gem, the state can be reset at any time and so chain-loading levels may be practical, \ndepending on how much state your game carries over. \nAs a final note on presentation: It may already have occurred to the reader that testing with \ncode coverage feedback is rather like playing a mini-game. An example of screen output \nwhile testing is shown in Color Plate 11. \n\n\n \n \n \nCollecting Results \nOutput from a code-coverage testing run should contain at least the following data: \n \nLevel tested \n \nMarkers hit (as text labels) \nIt may be useful to include: \n \nBuild identifier \n \nDate/time \n \nTester‘s name \n \nSpecifications of test platform \n \nSeparate list of unexpected markers hit \n \nExpected markers that were not hit \nSome of this may be found already in your log file and so, as above, you may choose to \nsimply add your code coverage results there. \nYou might find that on a small scale, this information is already enough. Your QA testing will \nbenefit from being guided to cover all markers, and by examining the logs you can now \ncheck that good coverage was achieved, check that specific code was tested, and look out \nfor code that should be run that isn‘t or code that needn‘t be run but is! \nHowever, you can get much more from the system by going further: \n \nMerge. Merge the results from multiple runs of the same level to reach a higher \ncombined coverage and to confirm unusual results. In many cases, branching player \nchoices may even make it impossible to reach 100-percent coverage in a single play-\nthrough. Further, merge the results of all your levels, looking for code that is not \nused anywhere. \n \nUpdate. Update your lists of expected checkpoints. After making changes to the \ncode and addressing any anomalies in the code coverage results, extract the list of \nmarkers hit to form a new list of expected markers for subsequent runs of the level. \n \nHistory. Build a history of results for each level over time. You can achieve this by \nsimply checking in the list of expected checkpoints for each level—which may in any \ncase be the best way to include them in the build. You can then leverage your \nexisting diff tools to analyze changes in results over time. For programmers, this can \nhelp with diagnosing bugs or tracking changes in levels under active development. \nWith a very large marker set, this could also be helpful in test planning to track when \na feature was last tested. \nAll of this can be done automatically. We use a simple text-processing tool that takes any \ncollection of log files and digests them into final results: sorting by level, merging separate \nruns, highlighting anything unusual, and outputting updated lists of expected checkpoints \nfor the next run. \n \nPossible Expansions \nThere is potential for getting more out of the framework: \n\n\n \n \n \nCollecting results over the network. Rather than using log files and extracting \ntheir results offline, it may be better to send results directly over the network to a \ndatabase server. This assumes that all builds tested can be reliably distinguished. If \nprogrammers often send their own local builds to QA, a simple build number may not \nbe enough. This approach simplifies the workflow for collecting and processing \nresults. \n \nAutomated testing. As described, the core testing method recommended for use \nwith this gem is a good QA team. However, the automated methods discussed, such \nas recorded play-throughs, are also compatible with this framework and would \ncomplement the QA process well. (Refer to the ―Common Approaches‖ section earlier \nin this gem.) In particular, code coverage reports from any automated tests \nperformed by your build machine would provide very fast feedback. \n \nCategorized markers. While the original implementation was applied to just AI \nsystem code, were it deployed more widely, it could make sense to divide the \nmarkers by module and/or feature. The ability to only consider a certain group of \nmarkers could then become useful, focusing testing on, say, the sound system, \ngame code, HUD features, and so on. \n \nPrioritized markers. Assigning priority levels to markers could help with test \nplanning, especially in times of fast turnaround. A simple high/medium/low scheme \nwould ensure the most important code is tested first. This would be most effective if \nintegrated into the GUI and if programmers had a simple way to promote/demote \nmarker priority in the builds. \n \nActivity display for markers. Flagging a group of markers to give a visual \nindication every time they are executed could be useful for better understanding the \nactivity of code or for guided stress-testing of a particular feature. \n \nHit counts. Rather than a hit flag, keeping a count for each marker could be useful \nas part of built-in performance tracking or monitoring the relative frequency of \nbranches, loop counts, and so on. \n \nRecentness tracking. Add a global counter that is advanced every time a marker is \nhit, writing the current counter value to that marker. This could be useful for tricky \ncrash bugs, to find out what code ran in the preceding moments. \n \nCustom levels for testing. This framework could be used to guide the creation of \nlevels for testing specific features, or indeed a single level that includes all features! \nThis offers the potential for very fast turnaround. \n \nA Post-Mortem \nThis framework was developed in order to safely make a particularly major refactor to our \nAI system, replacing a fundamental aspect of the system and involving around 25 percent \nof the code. The AI system is in constant use by developers, so full confidence in the result \nwas required before finally merging the branch. Milestones could not be disrupted, and no \none enjoys an endless bug-hunt. \nAs we have actively maintained our last major title as a test platform—which QA knows \ninside out—the levels of Crysis formed the subject of code coverage tests. In the process we \nformed a list of the features used—in other words, markers hit—in each level. Before this, \nvarious programmers and designers had different pieces of the puzzle, but no one had a \ncomplete, verified record of what was used and where—which is an invaluable reference for \nfuture maintenance. \nThe GUI proved useful to developers as well as QA testers, making it easy to observe when \nand where markers were first hit during their own testing, and with this information allowing \nQA time to be better targeted. By merging results from all levels, we were also able to find \nredundant code that would escape other forms of analysis— for example, minor features \nthat were not used in the released title. \n\n\n \n \nReception in the QA team was enthusiastic: They appreciated the feedback of the new tool \nand would like to see its use expanded. They also found no measurable impact on frame \nrate. \nHaving a text-processing tool to form the reports was essential in dealing with results from \nmultiple testers and many different levels. The tool processes entire folders of log files (one \nor more tests of each level of the game) in one pass, merging results, updating the label \ndata files in each level folder, and outputting a summary of changes to the console. After \nthat, it is easy to check the updated label files into source control. \nWhen investigating a bug, it was often useful to look through the history of the per-level \nlabel files and verify that this was the first test of that code, or to see that the code really \nhad been running successfully for some time, implying the problem must lie elsewhere. \nUse of the framework was focused on this refactor and testing in the following weeks. \nGreater automation would be instrumental in realizing its potential as an everyday \nmaintenance tool—in particular, a network-based collection of results to a central server, \nemailed summaries of results, and combination with automated tests. \nCode coverage helped inform the refactor, helped discover bugs quickly, and gave a new \nmeasure of confidence in the result. Some months after the final merge, we‘ve found only \ntwo bugs. \n \nConclusion \nThis gem presented an adaptation of the code coverage methodology specifically for \npractical games development, employing QA testing guided by real-time feedback. It can be \nadded to existing code and workflows to augment your existing testing procedures, \nimproving their rigor. It also provides programmers with more information about the usage \nof their code and greater confidence in their changes. \nIncluded on the CD is an efficient implementation of the framework, complete except for a \nGUI. This also forms a suitable basis for the various possible extensions of the framework \nthat have been discussed. \n \nReferences \n[Alexandrescu01] Alexandrescu, Andrei. ―Modern C++ Design.‖ 2001. \n[Cornett96] Cornett, Steve. ―Code Coverage Analysis.‖ 1996. Bullseye Testing Technology. \nn.d. <http://www.bullseye.com/coverage.html.> \n[ISO/IEC14882-98] ―Standard for Programming Language C++.‖ dcl.fct.spec/4. 1998. \n[Obermeit06] Obermeit, Tony. ―Code Coverage Lessons.‖ 2006. Mobile Me. n.d. \n<http://homepage.mac.com/hey.you/lessons.html>. \n \n4.9. Domain-Specific Languages in Game Engines \n\n\n \n \nGabriel Ware \ngabrielware@free.fr \nDomain-specific languages, DSLs for short, are computer languages used to solve problems \nwithin the explicit boundaries of the problem domain. The benefits of DSLs are multiple: \nThey help by separating domain-related code from application code; they let domain \nexperts solve problems using a language they understand; they can have multiple outputs, \nand users can easily shift from one to another; and last but not least, designing domain-\nspecific languages usually tighten relations between programmers and experts. This gem \nwill dig into domain-specific languages, answering the following questions: What is a DSL? \nWhen should I use a DSL? How do I build a DSL? \nDomain-Specific Languages in Depth \nIn this section we‘ll explore DSLs and their uses, concluding with some guidance on when to \nuse them. \nDomain-Specific Languages: Definitions and Examples \nSeveral definitions have been proposed for domain-specific languages. DSLs can be defined \nas artificial languages expressing instructions to a machine while working on a narrow field \nof expertise, a specific domain. These computer languages are sometimes referred to as \nlittle languages or micro-languages because of the limited expressivity of their syntaxes. \nTheir syntaxes are restricted to the problem domain they are modeling, including only what \nis relevant to the problems. Languages such as C, C++, or Java, which are labeled general \nprogramming languages, or GPLs, provide generic solutions to a broad range of problems \nand, as such, can be opposed to DSLs that provide more tailored solutions to a restricted \nset of problems. \nDomain-specific languages have existed for a long time, and their use in computer science \nis widespread. Successful examples include Lex and Yacc, programming languages intended \nto create lexers and parsers to help in building compilers; SQL, a computer language \ntargeted at relational databases; and LaTeX, a document markup language providing a \nhigh-level abstraction of TeX. DSLs have several characteristics emerging from their form \nand the process used to build them. The main characteristic of DSLs is their syntaxes, which \nprovide appropriate notations to the domain model and a very limited set of instructions. \nThis limits what problems users can solve but at the same time allows the language to be \nlearned quickly. DSLs are also usually declarative. They can sometimes be viewed as \nspecification languages, providing domain experts with the capability of writing \nspecifications that will become new tools, solve problems, and encode domain knowledge. \nBecause they encode domain knowledge as perceived by domain experts, DSLs are usually \nbuilt from a user perspective. Such a user-centric process tries not to take into account \nexternal factors, such as compiler capabilities, and prefers focusing on user experience. \nData mining is a domain that could be used as an example illustrating how domain-specific \nlanguages help. Code to search for persons matching certain criteria in tables can be written \nin any GPL, but it is much easier to write with a language that is appropriate to the domain. \nListings 4.9.1 and 4.9.2 show SQL and C++ code snippets that provide the same feature to \nan application. Even if C++ is more adequate on a general basis, it does not focus on the \ndomain and thus is harder to use in this specific case than SQL. \nListing 4.9.1. Mining a table for persons named Paul in SQL \nSelect users from table where name='Paul' \n \n\n\n \n \nListing 4.9.2. Mining a table for persons named Paul in C++—relying on the STL to \nhandle allocations and strings manipulation \nstd::list<CUser>::const_iterator const table_end = table.end(); \n    std::string SearchedName(\"Paul\"); \n    for(std::list<CUser>::iterator it = table.begin(); \n        it != table_end; \n         ++it) \n    { \n        if (it->Name == SearchedName) \n        { \n            users.push_back(*it); \n        } \n    } \n \nAnother interesting feature these two listings exhibit is the differences in term of interface. \nWhile GPLs provide a satisfying interface for programmers, some DSLs provide very light \nsyntaxes nearly exempt from notations that do not translate into everyday language, \ndropping parentheses, braces, and any other artifact as much as possible. These notations \nare referred to as language noise, and programming interfaces that minimize this noise are \ncalled fluent interfaces. Domain-specific languages do not always provide a fluent interface \nto their users, but this can be a useful feature to provide when end users do not have a \nprogramming background. \nThe Different Types of Domain-Specific Languages \nWhile GPLs are usually classified by their programming paradigm and the type of output \nproduced by their compilers, domain-specific languages are distinguished by the methods \nused to build them. As such, two main categories are emerging—internal DSLs, sometimes \nreferred to as embedded DSL, and external DSLs. \nWhen a DSL provides a custom syntax and relies on a custom-made lexer, parser, and \ncompiler, it is categorized as an external DSL. Building an external DSL is the same process \nas building a new general-purpose language: Programmers have to design the language and \nimplement it as well as any needed tools, such as editors, parsers, compilers, and \ndebuggers. On the other hand, internal DSLs are built from general programming languages \nthat offer syntaxes malleable enough to build new language from them. This greatly reduces \nthe amount of work needed to implement the language as programmers rely on the existing \ntool chain and language‘s features. On the down side, internal DSLs‘ syntaxes usually \ninclude language noise from their host language. \nIn addition to these two categories, DSLs are also often classified by the type of interface \nthey provide to end users. While some DSLs are created by programmers to be used by \nother programmers, others are designed to be used by domain experts who do not have \nprogramming experience. Thus, some DSLs provide textual interfaces, but other DSLs adopt \ngraphical front ends in order to ease programming. An example of a successful visual \ndomain-specific language is Unreal Engine‘s Kismet, which allows designers to control \nactions and handle events by connecting boxes using the graphical user interface provided \nby Unreal Editor. \nAdvantages of DSLs \nDomain experts may not be able to write code but are usually able to review code written \nusing domain-focused syntaxes. DSL code can sometimes even be written directly by \n\n\n \n \ndomain experts, achieving end user programming. DSLs concentrate on domain knowledge, \nand thus it is important that coders creating new DSLs deeply understand the problem \ndomain. As a side effect, this usually tightens relations between programmers and domain \nexperts, resulting in more accurate solutions. \nThe limited expressiveness of DSLs limits user input and, as such, can help to reduce user \nerrors. DSLs are easier to master, and as interfaces become more fluent, the code starts to \nbe self documenting. \nThrough these characteristics, DSLs are able to express important information while hiding \nimplementation details. Just like good APIs, DSLs provide users the ability to program at a \nhigher level of abstraction. This leads to a clear separation between domain knowledge and \nimplementation that allows for better conservation and reuse of this knowledge. \nFinally, DSLs provide new opportunities to do error checking, statistical analysis, or any \nother transformation of the domain knowledge. \nDisadvantages of DSLs \nThere are also several drawbacks to using domain-specific languages. The most problematic \nis the cost of building and maintaining a new language. While building external DSLs still \nrequires quite a bit of effort, new tools and new techniques have been used to reduce these \ncosts. Another alternative is to embed the DSL in a host language. As the language evolves \nand requirements change, language maintenance can become a burden. It can be very \ntempting to grow the problem domain by adding new keywords and notations, but this \nusually leads to building general-purpose languages with some domain-specific keywords. \nThis is a very costly approach and should be avoided unless it is desired. Another drawback \nof using multiple languages to build an application is that programmers need to learn more \nthan a few languages to control the whole pipeline, and thus they need to quickly learn and \nadapt. One last problematic aspect of using domain-specific languages is that it introduces \nan extra layer of complexity, which can slow the debugging process. \nRelations between DSL and Game Development \nGame development provides a wide variety of challenges in many different domains. To \ntake up these challenges, programmers usually use a few general-purpose languages and \nbuild frameworks that will help resolve domain-related issues. DSLs appear to be a good fit \nto this environment. Typical examples of problem domains related to video games are game \nlogic, navigation, animation, locomotion, data modeling, serialization, and transport. \nThinking in terms of modeling problem domains and user experience helps to define what \nsolutions are needed. \nWhen to Create a New Language \nCreating new languages is a difficult and time-consuming task, so deciding when to use a \nDSL is a very important process. The need for a domain-specific language usually arises \nwhen a common pattern is detected from several problems. Those patterns can occur at \ncode level, in programs, subroutines, or data, as well as at the application level, building \nsimilar tools or architecture several times. The problem domain can usually be identified \nfrom these patterns, and the boundaries of the domain can then be determined. Domain-\nspecific languages stress staying domain-focused, so it is important to deeply understand \nthe domain‘s definition. If the boundaries are blurry and users can‘t anticipate \nrequirements, then it may be impossible to design the language. It is important to carefully \nchoose the bounds because if they are too narrow, the DSL won‘t be used to encode enough \ndomain language, whereas if they are too broad, the language may lose its focus. \nBoundaries also influence the language interface by defining which variants are to be \nexposed to the user. Exposing too many variants will slow language learning, while not \n\n\n \n \nexposing enough will render it less usable. Domain experts, documentations, and \nspecifications can help determine such boundaries. \nLastly, because creating a new language is a difficult task, it is important to know whether \nsuch a language will be reused. If the problem domain is too narrow and domain knowledge \nshould be encoded only once, it may be better to build a framework over a general-purpose \nlanguage, but if domain knowledge needs to be encoded multiple times, solve multiple \nissues, or requires a lot of effort to be encoded using a GPL, then creating a new domain-\nspecific language may be a good option. \nFigure 4.9.1 depicts part of the decision process. \nFigure 4.9.1. DSL decision process. \n \n \n \n \nCreating a DSL \nThe process applied when creating a domain-specific language can be summed in six steps, \nas illustrated by Figure 4.9.2. \nFigure 4.9.2. DSL creation process. \n\n\n \n \n \n \nWe start with the problems that must be solved to meet our goals. As stressed earlier, it is \nimportant to be able to detect recurrent patterns coming from different problems because it \nwill lead to the identification of a problem domain. If patterns are detected soon enough, \nthe domain can be examined, and new problems may be anticipated. The second step is to \nacquire as much knowledge about the domain as possible. Documentation and domain \nexperts are the best sources of domain knowledge and will be able to explain what users \nexpect from the domain. This leads to a user-centric approach and designing the language \nfrom a user perspective. The last step before designing the language is to choose between \ninternal and external DSLs and the type of interface the language will provide to the end \nuser. Interfaces are usually driven by the domain model to represent end user ability to use \ngraphical and textual interfaces. In the language design phase, the specifications of the \nlanguage are laid down. Notations and keywords needed to model the domain are chosen, \nand variants—what the interface will show—and invariants—what assumptions about the \nmodel will be hidden in the implementation—are identified. Lastly, all tools required to \nimplement the language are created. \nChoosing between Types of DSLs \nDetermining the user interface of a new language is a decisive factor for its adoption as a \nnew tool. By understanding what users are expecting from the tool, programmers will be \nable to refine the required features. \nIf the domain can be modeled using text, then both internal and external DSLs can satisfy \nthe needs, and choosing between them is a matter of understanding their constraints along \nwith the programming proficiencies of the end user. Internal DSLs rely on the availability of \na host language that is malleable enough to let a DSL emerge from its own syntax. When \nsuch a language is available, its syntax and tool chain will influence the look and feel of the \nDSL. If those constraints are acceptable, then building an internal DSL is the quickest way \nto create a new textual DSL, as it will provide needed tools for the language. On the other \nhand, external DSLs do not rely on another language, allowing for a better customization of \ntheir syntaxes. But they also require that programmers build tools such as parsers, \n\n\n \n \ninterpreters, or compilers to support the new language. If no host language that satisfies \nsyntax needs is available, then external DSLs are the way to go. \nWhile some domains are very easy to model using words, other domains cannot be \nmodeled, or at least are difficult to model, using text. In this case, the domain-specific \nlanguage can rely on a graphical interface to help end users encode their knowledge. \nAnother factor for building a graphical front end to the DSLs is the programming \nbackground of their users. Some DSLs are intended for non-programmers, and if the \ndomain is too complex and thus requires exposing too many variants and keywords, it will \nthen probably be easier for the user to represent the domain knowledge using graphical \ntools. Although graphical DSLs are usually built from scratch, some tools do exist that help \nin creating them. \nFigure 4.9.3 synthesizes the whole process. \nFigure 4.9.3. DSL type decision process. \n \n \n \nCommon Programming Techniques for Building Internal DSLs \nAs domain-specific languages become more and more popular, several programming \npatterns used to build them emerge. Luckily, most programming tips used to build domain-\nspecific languages are easy to understand and use, but some may not be available from all \nhost languages. \nThe first, and probably oldest, technique is the use of macros. It has been widely used by C \nand C++ developers to pre-process source code. It uses the preprocessor capabilities to \nbuild fluent interfaces that generate complex code at pre-processing time. \nAnother old and widely available technique is called function sequencing. Domain knowledge \nis encoded using sequences of function calls. The implementation of this method relies \nheavily on the side effects of each function to affect the execution context of each \nsubsequent call. While this method provides a potentially acceptable solution in terms of \ninterface, relying on side effects can be dangerous and hard to debug as sequences become \nmore and more complex. Listing 4.9.3 shows an example of function sequencing. \nListing 4.9.3. Function sequencing \nanimation_engine(); \n    character_controller(); \n\n\n \n \n        playanimation(\"run_fast\"); \n            easing_in_using(LINEAR_EASE_FUNCTION); \n               during(10_MSEC); \n \nAn evolution of function sequencing is called method chaining. It uses objects to pass the \ncontext between calls without adding noise to the language. With this technique, each \nmethod call returns an object that provides a part of the language interface. This helps to \nfragment the interface across multiple types of objects. Rewriting the previous example \nusing method chaining leads to Listing 4.9.4. \nListing 4.9.4. Method chaining \nanimation_engine(). \n    character_controller(). \n        playanimation(\"run_fast\"). \n            easing_in_using(LINEAR_EASE_FUNCTION). \n                during(10_MSEC); \n \nNested functions are another way to call functions while removing language noise as much \nas possible. When using this method, all calls are nested, as presented in Listing 4.9.5. The \nmain characteristic of nested functions is the order in which functions get called. It can be \nvery useful when domain knowledge can be expressed as a sum of properties and \ncontainers. \nListing 4.9.5. Nested functions \ncharacter_controller( \n    playanimation( \n        \"run_fast\", \n        easing_in_using(LINEAR_EASE_FUNCTION), \n        during(10_MSEC) ) ) \n \nAnother frequently used technique for building domain-specific languages on top of an \nexisting framework is to separate the fluent interface from the existing API. Fluent \ninterfaces are usually created using assumptions about the calling context of their routines. \nAlthough this helps naming methods that chain efficiently and produce nearly fluent code, \nthis way of writing an API violates what is considered good programming practices. Thus, it \nmay be interesting to get the best of both worlds by adding a fluent interface on top of a \nmore standard framework. \nLambda functions, sometimes named blocks, anonymous methods, or closures, are a \nfeature that only recently became widespread in many mainstream languages. They have \nbeen successfully applied to creating DSLs because they offer the key characteristic of \nevaluating code, with minimal language noise, in a predetermined context. Lambda \nfunctions are very similar to a standard method definition but do not require the same \ntextual overhead as functions: They do not need to have names, complete parameter lists, \n\n\n \n \nand return types and are simply associated to standard variables that can be passed across \nfunctions. \nThe dynamic handling of missing methods is another widespread technique to create \ndomain-specific languages. It is a popular feature of languages such as Smalltalk and Ruby, \nwhere you can override doesNotUnderstand and method_missing, respectively. \nOther languages, such as Python, can provide similar features using other internal \nmechanisms. Handling missing keywords can be very convenient when the language has to \ndeal with unknown keywords and unknown function names. This technique allows the user \nto create keywords as needed. It allows creating new modeling languages with very little \nnoise very easily. \nListing 4.9.6. Animation DSL relying on method_missing, nested function, and \nclosures \n1:  animset   = define_animation_set( :terrestrial_locomotion) { \n2:    idle(from_file(\"tm_idle\")) \n3:    run_forward from_file \"tm_run_fwrd\" \n4:    walk_forward from_file \"tm_wlk_fwrd\" \n5:    turn_90degsLeft from_file \"tm_trn_90deg\" \n6: \n7:    jump_forward from_file \"tm_jmp_fwrd\" \n8:    jump_forward { can_blend_with \nall_from(\"terrestrial_locomotion\") } \n9:    jump_forward { can_blend_with \ntransitions_from(\"aerial_locomotion\") \n10: } \n \nListing 4.9.6 shows a domain-specific language where animation identifiers are keywords \nchosen by the user and thus impossible to predict. It demonstrates usage of blocks and \nnested functions using the Ruby programming language. The code block is written between \nbrackets and, in our example, it is given to the define_animation_set function. In this \ndomain-specific language, define_animation_set creates an animation set object and \nasks Ruby to evaluate the given block in the context of this object. The animation set object \ninterface will provide functions such as from_file, which is used to load an animation \nfrom a given filename. In order to reduce language noise, the language relies on the ability \nof Ruby to deduce parentheses placement. Lines 2 and 3 are similar and interpreted the \nsame way by the Ruby language, the only difference coming from parentheses‘ presence. \nLastly, Ruby handles function calls such as idle or run_forward as missing calls that \nare handled by our implementation to identify new animations. A sample demonstrating \nhow Listing 4.9.6 is implemented using the Ruby programming language is provided on the \naccompanying CD-ROM. \nAnother easy and very powerful technique to add meaning to code is called literals \nextension and is usually available from object-oriented languages. Extending literals helps \nreaders by allowing modifiers, which may or may not do anything useful, to be used on \nliterals in order to add fluency to the code. It requires the language to handle everything, \nincluding literals, as objects that can call methods. Literal extension also relies on the ability \nof the language to reopen and extend class definitions. \nListing 4.9.7. Literal extension \n\n\n \n \nrun if distance_to(nearest_enemy) < 10.meters \n \nOne last technique worth mentioning is abstract syntax tree and parse tree manipulations. \nIt is a rare feature allowing programmers to access the parse tree or the abstract syntax \ntree after the code has been parsed by the host-language parser. Ruby‘s ParseTree and C# \n3.0 both work in a similar function; a library call is used to parse a code fragment and \nreturn a data structure representing the code expressions. This feature is useful when \ntranslating code from one language to another or when the DSL needs to rely on a wider \nrange of expression than happens to be available in the host language and thus needs to be \ntransformed before use. \nTools Easing Language Construction \nExternal domain-specific languages have fewer constraints than internal ones but need \nslightly more effort to create because of the time required to build parsers and compilers. \nLuckily, tools have been created to ease this process and reduce this overhead. \nLexical analyzers and parser generators, such as Lex and Yacc, have been around for a long \ntime and are still a great help to build languages, but they tend to be replaced by new tools, \nsuch as ANTLRWorks or Microsoft‘s DSL tools, which are both providing powerful \ndevelopment environments focused on creating domain-specific languages. ANTLRWorks is \nan integrated development environment for creating languages using ANTLR V3 grammars. \nIt offers rapid iteration cycles by providing a full-featured editor, embedding an interpreter, \nand providing a debugger and a lot of other tools to ease the development process. While \nANTLRWorks uses textual grammars to create external textual DSLs, Microsoft‘s DSL tools \nprovide a way to create visual domain-specific languages to be integrated into Microsoft \nVisual Studio. The Microsoft DSL tools help design the language and its graphical interface \nby providing wizards and tools easing domain modeling, specifying classes and \nrelationships, and binding designers‘ shapes to the model concepts. Although Microsoft‘s \ntools for domain-specific languages can‘t be used for building run-time DSLs, they offer \nopportunities for integrating a custom visual domain-specific language inside Visual Studio. \n \nMulti-Language Game Engine Development \nThis section presents domain-specific languages for two domains related to low-level engine \nprogramming. Other examples of DSLs for game engines are shading and rendering passes, \nsound logic encoding emitters, occluders and propagation logic, behaviors of artificial \nagents, and locomotion rules for animation systems. \nThe first example of a domain-specific language in a game engine relates to data structure \nmodeling. Data management issues occur in many places, such as a pipeline‘s applications \nintercommunication, engine working set, multi-threading and performance issues, and \nnetwork replication. A common problem is the need to write data manipulation and \nserialization code multiple times. Thus, it may be interesting to encode as much knowledge \nabout data structures as possible using a domain-specific language that handles tasks such \nas generating code for serializing and accessing data in all languages used in the pipeline. \nSuch a DSL could also allow for statistical analysis of working sets, which would help profile \nthe engine‘s need in terms of data. \nAcquiring knowledge about the domain of data management is easy because programmers \nare the domain experts. This type of DSL should solve data-related problems by providing a \nsimple syntax to encode the data structures‘ layout. The language will encode structures \n\n\n \n \nand should provide end users with a way to control fields‘ identification, alignment \nproperties, and serialization requirements. Lastly, this domain-specific language will be used \nby programmers, and thus it is acceptable to use a textual interface with low language \nnoise. Listing 4.9.8 shows a sample of such data management DSL using Ruby as its host \nlanguage. You can find an implementation of this DSL on the CD-ROM. \nListing 4.9.8. Simple structure layout using a domain-specific language \nstruct (:PlayerInfos) { \n  required string :name, replicate_over_network! \n  required key :race \n  required boolean :is_male \n  required int32 :level \n  required int32 :exp_points \n  required vector3f :position, replicate_over_network!, \n16.bytes.alignment \n  required quaternion :orientation, replicate_over_network! \n  optional int32 :money \n  optional float :reputation \n} \n \nThe second use case for domain-specific languages targets the engine‘s threading model. \nScalability of the engine‘s performance over machine generations has become a very active \nfield of research. Console engines usually take advantage of running on fixed hardware with \nknown specifications, but good engines must allow evolution of hardware. A recurring \npattern when changing hardware is to rewrite the threading model to reflect new hardware \nand get better performance. Another pattern related to threading models happens during \ndevelopment when programmers try to offload heavy tasks from one processor to another, \nthus changing how tasks update. Again, a language focused on task dependencies and \nhardware specifications can help handling modifications of the threading model. Such a \nlanguage has to expose to the user variants such as number of cores, number of threads \nper core, or preferred number of software threads. It can also expose tasks that are run by \nthe engine and their dependencies in order to help scheduling given the hardware \nconstraints. The output of such a language can be either code or data that would drive \ncurrent engine‘s threading framework. Like the previous domain-specific language \npresented, this language is targeted at programmers, and an internal DSL‘s properties \nsatisfy our requirements. Listing 4.9.9 shows what such a DSL could look like, and its \nimplementation is provided on the accompanying CD-ROM. \nListing 4.9.9. Threading a domain-specific language \nhardware { \n      has 3.cores.each { |core| core.have 2.hardware_threads } \n} \n \nsoftware do \n      instanciate 6.software_threads \n      instanciate :camera.module \n      instanciate :player.module, :bots.module, :sound.module \n      instanciate :physics.module, :graphics.module \n \n      camera.depends_on(:player) \n\n\n \n \n      bots.depends_on(:player) \n      graphics.is_bound_to(thread(0)) \nend \n \n \nIntegrating DSLs into the Pipeline \nWe will now focus on how DSLs integrate in the production pipeline. \nEngine Integration through Embedding \nThe quickest and easiest way to integrate a domain-specific language into an engine is to \ndirectly embed it. As such, creating an internal DSL that relies on the engine‘s main \nlanguage seems to be an evident way to provide domain-specific languages from the game \nengine. But, with C++ being the preferred language for building game engines, it is difficult \nto provide a domain-specific language that allows for rapid iterations. C++ provides a very \nstrict syntax, and most of the advanced features used to build DSLs are difficult, if not \nimpossible, to use. Another problem of relying on C++ to build an internal DSL is the \ncompilation process that may disturb domain experts without any programming \nbackground. However, C++ provides macros, nested functions, method chaining, and \ntemplates that are powerful tools for building fluent interfaces. \nDevelopers who create DSLs using C++ as a host language must be careful about build \ntimes, ease of debugging, and code bloat, as many of the aforementioned techniques can \nlead to these problems if not used properly. \nEngine Integration through Code Generation \nIntegrating a DSL that relies on a language other than the one used by the engine is made \npossible by using DSLs as application generators. In this case, the domain-specific language \nis used to input domain knowledge and transform these high-level specifications to low-level \ncode that will be included in the engine. This approach provides the same advantages as \nany other code generation technique: End users can easily input data without worrying \nabout the implementation, programmers can modify an implementation without the user \nnoticing, and code need only be optimized once per code generator. \nAlthough this technique has the advantage of separating the domain-specific language from \nthe language used to implement the engine, it has the major drawback of increasing the \ncomplexity of the build process. \nAn example of DSL relying on code generation is Unreal Script, as it binds scripts to native \nclasses by generating C++ headers. Although this is very convenient for debugging and for \nvery tight integration into the engine, it requires script programmers to be really careful \nwhen modifying scripts, because it may trigger a full rebuild of the engine. When using this \ncode generation technique, developers must try to reduce compilation and link time as \nmuch as possible. \nEngine Integration through Interpretation \nDSLs can also be integrated into the engine by using a virtual machine that will read and \nexecute domain-specific code at run time. Embedding virtual machines for languages such \nas Lua or Python has been a popular method for years, and it is possible to build internal \nDSLs on top of such languages. Another path is to create an external DSL and embed its \nvirtual machine inside the engine, like [Gregory09] and [Sweeney06]. This integration \n\n\n \n \nmethod has the advantage of removing any constraint previously imposed by the engine‘s \nlanguage and also helping reduce iteration cycles, but it sacrifices run-time performances. \nIndependent of building your own virtual machine or using a preexisting one, it is crucial to \nprovide tools that will assist the debugging phase, since this new language will add an extra \nlayer of complexity. \nEngine Integration through a Hybrid Approach \nAn interesting way to integrate DSLs in an engine is a hybrid approach where DSL code can \nbe either compiled to machine code or interpreted by a virtual machine. Although such an \napproach requires substantial effort to write compilers and interpreters, it would provide the \nbest of both worlds, allowing for fast iteration during development and maximizing release \nbuild performance. \nTools are crucial to overcome debugging issues, but developers need to also care about the \nexecution environment of scripts, as it will change when going from interpreted to compiled. \nThe quickest way to set up this hybrid approach for game engines is to create an internal \nDSL using an already available interpreter, such as Lua, Python, Lisp, or Ruby, and bind it \nto the native engine‘s framework. Code generation routines should be written in the host \nlanguage and used to translate DSL code to native code relying on the engine‘s framework. \nPipeline Integration through Data Generation \nTools that help domain experts input their knowledge into the pipeline usually provide an \ninterface relying on domain-specific languages. Tools providing DSLs integrated into game \npipelines are very common. Unreal Engine provides Kismet for scripting game events \n[Unreal05], Crytek‘s CryENGINE offers Flow Graph—a visual editing system allowing \ndesigners to script game logic [Crytek09]. Other examples exist in the field of artificial \nintelligence [Borovikov08]. \nPipeline Integration through Centralization \nDSLs let users encode domain knowledge using custom syntax and usually help centralize \nthis knowledge. As a side effect, it can be very interesting to use DSLs not only to encode \ndomain knowledge, but also to distribute it to any application of the production pipeline, \neasing knowledge transfer across multiple languages and applications. For example, tools \nsuch as Google‘s protocol buffers or Facebook‘s thrift provide domain-specific languages \nthat ease data transfer across complex application architectures, which are very similar to \ngame pipelines. \n \nConclusion \nDomain-specific languages have been around for a long time and are successfully employed \nto solve a wide variety of problems throughout the software industry. They offer tailored \nsolutions, are easy to learn and manipulate, enable various opportunities to mine the \nknowledge they encode, and focus on end user experience. It is still difficult to reduce the \ncosts associated with creating and learning several languages, but because video game \ndevelopment addresses such a wide range of problem domains, it seems to be a perfect fit \nfor domain-specific languages. \n \nReferences \n",
      "page_number": 376
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 395-402)",
      "start_page": 395,
      "end_page": 402,
      "detection_method": "topic_boundary",
      "content": " \n \n[Borovikov08] Borovikov, Igor, and Aleksey Kadukin. ―Building a Behavior Editor for \nAbstract State Machines.‖ AI Game Programming Wisdom 4. Boston: Charles River Media, \n2008. \n[Crytek09] CryEngine Team, Crytek. ―CryEngine3 specifications.‖ 11 March 2009. \n[Gregory09] Gregory, Jason. ―State-Based Scripting in Uncharted2.‖ Game Developers \nConference. 2009. \n[Sweeney06] Sweeney, Tim. ―The Next Mainstream Programming Language: A Game \nDeveloper‘s Perspective.‖ Symposium on Principles of Programming Languages. 2006. \n[Unreal05] Unreal Engine Team, Epic. ―Unreal Kismet, the Visual Scripting System.‖ 2005-\n2008. <http://www.unrealtechnology.com/features.php?ref=kismet>. \n \n4.10. A Flexible User Interface Layout System for Divergent \nEnvironments \nGero Gerber, Electronic Arts (EA Phenomic) \nkontakt@gerogerber.de \nThe more people you want to address with your game, the more divergent system \nenvironments you have to support and take into account. The differences are not only \ndifferent CPUs and/or GPUs, but also displays. So what you want and need to develop is \nsoftware that scales with its environment and makes efficient use of it in all aspects. \nIn this gem, we highlight an approach to efficient resource usage, especially the available \nscreen size, from the perspective of the user interface (UI) layout. We show how you can \nkeep the UI layout system sufficiently flexible so that, once in place, you can achieve \noptimal layout results without the need to handle special cases in the source code. \nThe Problem \nThe more UI elements (widgets) you have in your game, and the smaller the screen size of \nyour minimum supported system requirements, the more important it is to have efficient \nand flexible UI layouts. In addition to the fact that there‘s a wide range of screen resolutions \nout there, you also have to take into account the fact that there are different aspect ratios. \nFor example, many current laptops make use of previously unusual aspect ratios. So when \ndesigning the UI layout, you may need to consider more than the widget‘s size, position, \nand font size. In most cases, where the difference between minimum supported system \nrequirements and high end is large, you also have to make use of different assets—for \nexample, textures, which you put on the widgets in order to have a crisply rendered widget. \nFinding an algorithmic solution for making optimal use of screen space and giving a useful \nlayout is difficult and may, if done in code, not always result in solutions UI designers or \nartists want. \n \nSome Cheap Solutions \nThere are some solutions that solve this problem in a cheap and sometimes acceptable way. \n\n\n \n \nThe first solution makes use of virtual screen coordinates (see Figure 4.10.1). Here the \nspace the UI works with is always the same (for example, 1024×768). All widgets are \npositioned in this virtual space. When the physical screen resolution differs from the virtual \nspace, all widgets are then scaled automatically. This effectively projects the virtual screen \ncoordinates onto the physical screen resolution. As long as the two spaces do not differ that \nmuch in size or aspect ratio, everything can look acceptable. But if the difference in size or \naspect ratio becomes too large, you get non-uniform-scaled widgets with blurred textures. \nFigure 4.10.1. Virtual screen coordinates. \n \n \nThe second solution works for designs with only a few widgets. Here, all widgets are bound \nto a combination of screen borders or are centered. So when, for example, you define a \nwidget that is bound to the lower and right screen borders, the widget will stick to the \nlower-right part of the screen when the screen resolution increases in width or height. You \ncan see border linking as a form of alignment (see Figure 4.10.2). (.Net users may know \nborder linking under the name of anchoring.) The difference is that with border linking, in \ncontrast to simple alignment, you can define multiple borders to link to, and you can define \na specific number of pixels the widget has to stay away from the borders. This way, you can \nmake use of a large number of screen resolutions without non-uniform-scaled widgets or \nblurred textures. The drawback here is that with increasing screen resolutions, the widgets \nmake use of a smaller part of the screen and so become harder to read. At the other end, \nbelow a given screen resolution the widgets may start overlapping each other, which may \nnot be the desired behavior. But for some designs, this may be the way to go. \nFigure 4.10.2. Border linking. \n\n\n \n \n \n \nHowever, both solutions may not work optimally when additional constraints exist. For \nexample, when you develop for consoles such as Xbox 360, you have to take into account \nthe fact that on some TVs, only 80 to 90 percent of the picture may be visible. This area is \ncalled the title-safe region. Parts of the display outside this region may not be visible and \nshould not be occupied by any widget or other important game elements. This case has to \nbe considered in the UI layout when running in this special environment. \nAnother solution would be to do multiple UI layouts and check inside the source code for \nwhich layout to use (see Figure 4.10.3). This results in significant extra work for the UI \nartist, and you have many duplicate layout definitions that need to be kept in sync. Adding, \nremoving, or editing existing widgets in multiple layouts can become complicated and error \nprone. \nFigure 4.10.3. Multi UI layout scenario. \n\n\n \n \n \n \nThings become worse for the software engineer in a multi-layout scenario when, at run \ntime, widgets have to be created (for example, adding new widgets to a list box), widget \ntextures have to be exchanged, or UI effects have to be started (for example, you want to \nuse different scaled UI effects for different screen resolutions). In these cases there would \nhave to be a check in the source code that decides which asset to load in order to fit into \nthe current UI layout. When, later during development, new layouts are added or removed, \nall these code sections have to be adjusted in order to make use of this new layout. \n \nA More Flexible Solution \nWidgets are defined by a set of properties (position, size, texture, font style, and so on) \nthat can be seen as key-value pairs. Many applications make use of XML to define widgets. \nUsing a binary format would result in faster loading times, but for the sake of readability, \nwe will stick to plain text XML in the examples. The following is a sample XML definition for \na widget that defines the widget‘s name, position, and size. \n<Widget \n   name = \"player_name\" \n   x = \"10\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n/> \n \nThis example is static and may not suit all environments. It could be better to have a larger \nwidget in larger screen resolutions. A more flexible solution we have already used in a \nsimpler form in our last project is the use of conditional modifiers (CM). A conditional \nmodifier is a node in a widget‘s XML definition that contains an additional set of widget \nproperties. You can attach a CM to any widget. A CM always contains a set of conditions \nconnected with condition operators, which, when evaluated to true, enable the properties \n\n\n \n \ndefined inside the CM. An example of a CM inside a widget‘s XML definition is presented \nhere: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n> \n   <CM \n      cm_name = \"xbox360\" \n      x = \"30\" \n      y = \"30\" \n      width = \"40\" \n      height = \"50\" \n   > \n      <Conditions> \n         <Condition_Platform \n            value = \"xbox360\" \n         /> \n      </Conditions> \n   </CM> \n</Widget> \n \nIn this example the widget is created with the properties defined in the Widget node. As a \nchild node of the widget, we added a CM node. As a child node of the CM, there is a \ncondition that evaluates to true when the current platform is equal to xbox360. The CM is \nloaded into memory when the corresponding condition is fulfilled. After the widget is fully \ncreated, we iterate over all loaded CMs attached to the widget (in the order defined in the \nXML). For each CM whose conditions evaluate to true, we apply the specified properties to \nthe outer widget. The property cm_name inside the CM node is just cosmetic and shall help \nto give the CM construct a meaning. So in the example above on a PC, the widget would be \nplaced at position (15, 15) with a size of (20, 30), and on the Xbox 360 its position would \nbe (30, 30) with a width of (40, 50). Of course, you do not have to re-specify all widget \nproperties inside a CM, but only those will be applied when the CM evaluates to true. So in \nthis example, you could, for example, only change the size of the widget if needed. \nIt is also possible to combine the previously discussed border-linking solution with the CM \nsystem. \n \nConcatenate Conditional Modifiers and Conditions \nUsing only a single CM or a single condition inside a CM is not that useful. So to further \nimprove the flexibility of CMs, you can chain together multiple conditions and CMs to form a \nmore complex set of conditions. In the following example you can see how this works: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n\n\n \n \n   width = \"20\" \n   height = \"30\" \n   texture = \"high_res_texture.tga\" \n> \n   <CM \n      cm_name = \"above_minspec_screen_size_pc\" \n      x = \"30\" \n      y = \"30\" \n   > \n      <Conditions> \n         <Condition_ScreenWidth \n            operator = \"greater_than\" \n            value = \"1024\" \n         /> \n         <Condition_Operator_And/> \n         <Condition_ScreenHeight \n            operator = \"greater_than\" \n            value = \"768\" \n         /> \n         <Condition_Operator_And/> \n         <Condition_ Platform \n            negate = \"true\" \n            value = \"xbox360\" \n         /> \n      </Conditions> \n   </CM> \n   <CM \n      cm_name = \"low_res_gfx\" \n      texture = \"low_res_texture.tga\" \n   > \n      <Condition_MinSpec/> \n   </CM> \n</Widget> \n \nHere we apply the properties inside the CM only in the case that screen width is greater \nthan 1024, screen height is greater than 768, and if the current platform is not Xbox 360. \nIn this case we change the widget‘s position. As you can see in this example, there‘s a \nsecond CM that changes the widget‘s texture to a low-resolution variant when \nCondition_MinSpec evaluates to true. Both CMs are independent from each other and \nchange different properties of the widget in a different environment. It is also possible for \ntwo different CMs to change the same widget property in different cases. The number of \ndifferent conditions only depends on the needs you have to obtain the desired UI layout for \na specific environment. \n \nImplementation Details of the CM System \nImplementing the CM system is straightforward. In order to keep the creation of CMs and \ntheir corresponding conditions in one place in the code, you can use the well-known Factory \nMethod Pattern [Gamma94] for creating conditions. Figure 4.10.4 shows the class diagram \nfor the CM system. \n\n\n \n \nFigure 4.10.4. UML class diagram for the conditional modifier system. \n \nWhat you can see from the UML diagram is that a widget can contain an arbitrary number of \nCMs and that each CM contains at least one condition. Two conditions with a condition \noperator in between form a condition pair that can be evaluated. Of course, you have to \nconsider operator precedence in these cases. For the sake of simplicity, we show only two \ncustom conditions in the diagram (ConditionScreenWidth and \nConditionScreenHeight). Additional condition types can be added, depending on your \nneeds. You can also add custom condition operators derived from Condition-\nOperatorBase. \n \nConditional Modifier Condition References \nAs you may have noticed, a non-trivial CM definition adds a good deal of potentially \nredundant data to each widget definition. This redundancy can be resolved by adding a CM-\nspecific property (cm_conditions_reference) to the CM definition that references an \nXML file containing all the relevant conditions and condition operators. The advantage of \nthis is that when you have to modify this set of conditions, you only need to touch one \nsingle file, and all referring CMs will work with the new definition. Here‘s an example: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n\n\n \n \n> \n  <CM \n      cm_name = \"xbox360\" \n      x = \"30\" \n      y = \"30\" \n       width = \"40\" \n       height = \"50\" \n       cm_conditions_reference = \"conditions_xbox360.xml\" \n   /> \n</Widget> \n \nFollowing are the corresponding definitions from conditions_xbox360.xml. \n<Conditions> \n   <Condition_ScreenWidth \n      operator = \"greater_than\" \n      value = \"1024\" \n   /> \n   <Condition_Operator_And/> \n   <Condition_ScreenHeight \n      operator = \"greater_than\" \n      value = \"768\" \n   /> \n   <Condition_Operator_And/> \n   <Condition_ Platform \n      value = \"xbox360\" \n   /> \n</Conditions> \n \nFigure 4.10.5. Conditional modifier condition references. \n \n \n \n \n",
      "page_number": 395
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 403-417)",
      "start_page": 403,
      "end_page": 417,
      "detection_method": "topic_boundary",
      "content": " \n \nWidget Templates \nWe can further improve the way we define widgets, especially if we make use of many \nsimilar widget definitions. A good example for this is the definition for a default button style \nin a game. Default buttons may share many properties that are equal in all instances. For \nexample, the default button‘s size, texture states, sounds, and so on would be the same for \neach instance of a default button. It would not make much sense if at each place where you \ndefine some sort of default button, you have to specify all the properties by which a default \nbutton is defined. For these cases you can make use of widget templates (see Figure \n4.10.6). \nFigure 4.10.6. Defining widgets with widget template files. \n \n \nA widget template defines a widget with a default set of properties. A widget template is an \nadditional XML widget definition file. A concrete widget definition can refer to widget \ntemplates via the additional property template_filename. The widget template can \ncontain CMs and conditions. From the implementation point of view, you create a widget \ninstance from the specified template XML definition, apply all properties found in the widget \ntemplate, then apply all CMs defined in the template, and at the end you set the instance-\nspecific properties for the concrete widget instance. If we stay with the default button \nexample instance, specific properties would be the text shown on the button or the event \nthat gets fired when clicking the button. This way you can save a lot of data inside the \nwidget definitions, and you can make changes quickly because you only have to change the \nwidget template itself. This feature is particularly useful for global styles. The following \nexample shows how you can make use of widget templates. \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   template_filename=\"default_widget.xml\" \n/> \n \nThe property template_filename refers to the widget template and contains all default \nproperties and CMs including conditions. So in this example, the only custom properties for \nthis widget instance are its position and its name. \n\n\n \n \n \nProxy Assets \nMany situations require assets such as textures or UI effects to be loaded at run time. One \nexample would be that you want to exchange a texture or show some UI effects to highlight \nsome widget. This requires the specific UI effects that fit the current widget shape and size \nto be loaded. At this point, when using CMs, you don‘t know which asset to load, because \non different widget sizes you may want to apply textures with different resolutions, and on \ndifferent widget shapes and widget sizes you have to use different UI effects. In order to \ndecouple these problems from the source code, you can make use of proxy assets. A proxy \nasset is an XML file that contains the asset information itself (for example, the path to some \ntexture or some UI effects file) and some CMs that control which asset to use in which \nenvironment. This is similar to the use of CMs in conjunction with widgets. Here is an \nexample of a proxy asset for a texture: \n<Texture \n   filename = \"high_res_texture.tga\" \n> \n   <CM \n      cm_name = \"minspec\" \n      filename = \"low_res_texture.tga\" \n   > \n      <Conditions> \n         <Condition_MinSpec/> \n      </Conditions> \n   </CM> \n</Texture> \n \nThis example by default uses the texture high_res_texture.tga. Only when \nCondition_MinSpec evaluates to true do we actually load low_res_texture.tga. \nSo in the source code you would reference the proxy asset‘s XML definition instead of a \nconcrete texture filename. This way, we have decoupled the asset from the source code. \nBecause we use the same CM system here as described in conjunction with widgets, we can \nmake use of the same factory as well. So when adding new conditions, you can use them \nautomatically for proxy assets, too. See Figure 4.10.7. \nFigure 4.10.7. Proxy assets decouple assets from the source code. \n \n \n \n \n\n\n \n \nPerformance \nOf course, all this parsing of CMs takes time. The conditions need only be evaluated once, \nso it would make sense to keep the conditions in memory and query the result when \nneeded. This is, of course, only valid in cases where you do not create conditions that can \nchange during run time. The same is true for widget templates. If your engine supports the \ncloning of widgets, you can keep a copy of the widget template in memory and clone it \nwhen you create a new widget instance of it. Proxy assets can be optimized by loading and \nevaluating them only once and reusing the results from memory. \n \nProblems \nWhen using CMs, the definition of widgets becomes a more complex task, and additional \nwork is required to configure all CMs and their corresponding conditions. We recommend \ntool support for adding, removing, and modifying CMs. In our last project, we had \neffectively three different UI layouts we used for all different screen resolutions. We defined \na minimum screen resolution of 1024×768 (4:3), a medium one of 1280×1024 (5:4), and a \nlarge one defined by 1680×1050 (16:10). Screen resolutions greater in width and height \nthan the ones defined use the large screen resolution layout. Screen resolutions in between \nuse the next smaller layout. UI elements positioned relative to a screen border keep the \ncorrect position via their border linking property. Because of this, it is possible to scale the \nUI layout to higher screen resolutions. This way, we covered the most common screen \nresolutions used by our customers. We also disabled some laptop screen resolutions in order \nto save time. Of course, when running the game with extremely large screen resolutions, \nyou run into the problem described earlier (widgets become small in relation to screen size). \nFortunately, screen sizes do not grow at such a pace that this should become a problem. \nThe UI Editor we used in house was capable of generating CMs recursively regarding the \nposition and size of the widgets. Therefore, we created our base UI layout for the smallest \nsupported screen resolution of 1024×768 and let the UI Editor create CMs with a \ncorresponding scale factor for the higher resolutions. \nAnother aspect you have to consider is the additional amount of testing you need when you \nhave different UI layouts. Therefore, you should plan early which UI layouts you need and \nby which conditions these layouts are being controlled. This way, your QA can check out the \ndifferent UI layouts with a defined checklist. \n \nConclusion \nThe CM system described in this gem gives you a good deal of flexibility in defining your UI \nlayout. It offers UI designers and UI artists many options to define the UI layout without \nrequiring corresponding source code changes. On the other hand, you have to invest some \nwork to integrate this system into your engine, and testing the UI layouts is more work \ncompared to just testing a single simple UI layout. But from experience with this system, \nthe advantages outweigh the additional amount of work. \n \nReferences \n[Gamma94] Gamma, E., et. al. Design Patterns: Elements of Reusable Object-Oriented \nSoftware. Addison-Wesley Professional, 1994. \n\n\n \n \n \n4.11. Road Creation for Projectable Terrain Meshes \nIgor Borovikov, Aleksey Kadukin \nigor.borovikov@gmail.com, akadukin@gmail.com \nRoads are becoming an increasingly important part of large environments in modern games. \nThey play a significant role for both the aesthetics and the functionality of the environment. \nIn this gem, we explore several techniques that are useful for modeling roads in a large \ngame environment like that of The Sims 3 by Electronic Arts [EA:Sims3]. The techniques \ndiscussed are general, and for simplicity we limit our discussion to the case of projectable \nterrain meshes. Some of our inspiration was drawn from earlier works, such as Digital \nElement World Builder [DEWB], a package for modeling and rendering natural scenes. \nWe start by building a trajectory for a road on a projectable mesh using elements of \nvariational calculus and describe an algorithm for creating optimal paths on the terrain \nsurface. Next, we move to building a mesh for the terrain while satisfying a number of \nnatural design requirements for the local road geometry. \nRoads as Geodesic Curves \nMany roads in the real world exhibit certain optimal properties in a sense that they were \nbuilt to minimize the cost of connecting one location to another. As a trivial example, \nconsider an ideal road between two points A and B on a horizontal plane, which is a straight \nline and is the shortest length curve connecting the points. \nWhen placing roads between two locations on a terrain model, we can use the same \nprinciple and require that a certain cost function is minimized on the curve representing the \nroad. Variational calculus provides a framework for describing such curves. An introduction \nto variational calculus can be found in any good book on differential geometry, for example \n[DNF91]. We will limit the formal part of our presentation here to the bare minimum, \nskipping details that are not important for the algorithm we propose. \nVariation Problem for Roads \nWe will represent a road between points A and B on a terrain with a smooth curve γ(t) \nparameterized with the natural parameter t ∊[0, L] (in other words, the curve is \nparameterized by the arc length), where γ(0)=A, γ(L)=B, and L is the length of the curve. \nThen we can define the cost of moving from A to B along such curve as the length of the \ncurve: \nEquation 1  \n \n \nwhere l(t) is the cost function: \nEquation 2  \n \n\n\n \n \n \nThis cost function is calculated based on a Euclidian metric in 3D space. The cost function \n(2) restricted to the terrain surface induces the so-called Riemannian metric on the terrain \n(which still directly corresponds to our intuitive geometric notions of distance). Shortest \nlength curves on such a Riemannian space are called geodesics. \nFor terrain modeled as a height map z = f(x,y), we have: \nEquation 3  \n \n \nGeodesics for such a cost function are similar to a rubber band stretched across the terrain \nalong the shortest trajectory between destination points while keeping contact with the \nsurface across their entire length. Such curves provide the shortest path but do not \ncorrespond to our desire to provide natural-looking roads, and in particular they ignore the \nrole of gravity. \nAn important observation is that actual roads also minimize variation of altitude along the \ntrajectory. This is true for both foot trails and automobile roads. Instead of going along the \nshortest path across a hill, roads tend to bend around while trying to maintain the same \naltitude. The altitude variation element along a path is: \nEquation 4  \n \n \nWe can define a new length element as follows: \nEquation 5  \n \n \nwith λ>=0. For larger values of λ, we expect the geodesics to be ―flatter‖ in the z direction. \nUsing in (1) the new cost function (5) will give us: \nEquation 6  \n \n \nwhich will provide a better approximation to the behavior of actual roads. The parameter λ \ncorresponds to the tradeoff between the desire to reach the destination point as soon as \npossible versus the desire to save on climbing up and down any slopes along the pathway. \n\n\n \n \nSolutions for the variational problem (1) can be found among the solutions of Euler-\nLagrange equations: \nEquation 7  \n \n \nWhen solving the equations (3), we need to also satisfy the boundary conditions γ(0) = A \nand γ(L) = B. However, solving the boundary condition problem for the equations (7) is not \na simple task. Instead, in the next section, we will obtain a numeric solution by directly \noptimizing a discrete version of (6). \nNumerical Solution for Geodesics \nHere we will look for the minimum of the cost functional (6) directly by approximating the \nsmooth curve γ with a piecewise linear curve on the surface. The following procedure \nconverges to the solution of the original continuous problem in a wide range of conditions; \nhowever, in the interest of brevity, we will leave out the proof. \nA piecewise linear representation of the curve γ(t) requires n+1 nodes: {γi, i=0,...,n} or, in \ncoordinates, γi=(xi, yi, f(xi, yi)). For such a curve, there will be n line cuts between nodes: \nci=[γi, γi+1]. The length of the line cut ci is denoted as |ci|. Obviously, \nEquation 8  \n \n \nThe discrete approximation of the cost along a curve for the variational problem (7) is the \nfollowing sum instead of an integral: \nEquation 9  \n \n \nFor a fixed number of nodes, the following iterative optimization process allows us to find \nlocal minimum for (9). \nG := calculate the cost using (9) \nCostChange := G \nWhile cost change is greater than threshold do: \nFor each node 0<i<n do: \n\n\n \n \n \nNewG := calculate using (9) \nCostChange: = G – NewG \nG := NewG \nwhere Mi is the line equidistant from the current nodes γi-1 and γi+ 1. In other words, for \neach three neighbor nodes, we optimize the location of the middle node by finding the \nminimum cost local to the two-segment part of the path by varying the location of the \nmiddle node on the middle line Mi between the two other nodes. See Figure 4.11.1. \nFigure 4.11.1. Optimization of the middle node. \n \n \nSuch a choice of the per-node optimization domain ensures that the spacing between nodes \nwill remain relatively uniform, thus adding to the method‘s stability. Also, single-parameter \noptimization is much cheaper computationally than the two-parameter optimization case, \nwhich searches for an optimal location in an area between two nodes. Another advantage is \nthat such an optimization is in direct correspondence with the variations used in the proof, \nsuch that we get the correct continuous limit for our procedure when the number of nodes \napproaches infinity. Note that we optimize the location of every node except the first and \nlast nodes of the curve. The iterations stop when the cumulative change of cost is less than \na given threshold, which is input as a parameter for the method. \nThe method can be relatively expensive for a detailed representation of a road curve with \nmany nodes. This can be addressed by starting with a less detailed representation with \nfewer nodes and subdividing it when the iterations on this curve stop. Subdivision stops \nwhen the length of the road segment reaches a minimal allowed length. This minimal length \ndepends on the design requirements but can be naturally set comparable to the distance \nbetween vertices for the regular mesh built from the height field. \n\n\n \n \nA Python-based version of the algorithm is provided on the CD for your experimentation. \nThis takes the file ridge.png as input and generates ridge_path.png as output. These are \nshown in the left and right sides, respectively, of Figure 4.11.2. Note that the Python code \nrequires Python 2.4 or later along with a matching version of the Python Imaging Library \n(PIL). \nA final touch for the created curve could be node resolution optimization. We can eliminate \nnodes where the angle between adjoined segments is sufficiently small. Again, a cost \nfunction and angle can provide criteria for optimization so that nodes are not removed too \naggressively. \nFigure 4.11.2. Sample paths built with the proposed algorithm. \n \nThe branching of roads requires a little extra work. The problem of connecting Point C to \none of the Points A or B in the presence of an already-built road from A to B can be reduced \nto the same variational problem with any movement along the existing road being negligibly \nsmall in comparison to the movement across terrain without roads. This simplification may \nbe used to reduce the problem to a similar variational problem but with a slightly different \nboundary condition: The end point D of the new curve will belong to the existing road rather \nthan being fixed to A or B. The required modification to the algorithm is very simple: We \ninclude D in the optimization by allowing it to move freely between the nodes of the existing \nroad. \n \nRoad Grading Techniques for The Sims 3 Worlds \nAfter creating a trajectory for a road, we need to deliver a road mesh that is consistent with \nthe terrain—this means the terrain mesh also needs to be locally modified around the road. \nThis produced a number of challenges during The Sims 3 [EA:Sims3] development. \nBlending Roads with Terrain in The Sims 3 \nThe Sims 3 worlds were designed without grid-based placement restrictions for roads. The \nroads could be placed anywhere with respect to the mesh, and the road segments were not \nlimited to straight lines and simple intersections. \n\n\n \n \nRoad segments were the spline-based curves connected to each other by connection ports. \nThe intersections have fixed (prebuilt) geometry. The world designer could place and edit \nroads in an in-house build tool. The Sims 3 world terrain is a 3D mesh generated from a 2D \nimage height map, which played a critical role in road network construction algorithms. \nThe road mesh is generated using a base spline and world terrain geometry that is covered \nby the road segment. This provides a relatively easy and intuitive way to lay down the road \nnetwork. However, as shown in Figure 4.11.3, the road surface generated in such way \nfollows the terrain surface exactly and may not be smooth enough. \nFigure 4.11.3. The road segment placed on the uneven terrain. \n \n \n \nRoad Grade and Road Slope \nThe road surface must satisfy a number of conditions enforced by the road-grading tool. \nThe most important characteristics controlled by this tool are road grade and road slope. A \nroad slope is defined as a road surface tilt (the gradient perpendicular to the road), and a \nroad grade is defined as steepness of the road (the gradient parallel to the road) [Wiest98]. \nThe tool allows designers to set up the road grade and slope limits and automatically modify \nthe road surface to create a realistic look and improve the routable functionality of roads. \nThe algorithm adjusts individual or connected road segments‘ tilt and steepness using \nheight map grid cells. We describe a technique that deals with the slope and grade limits \nseparately. \nFlattening Road Slope \nThe purpose of the road slope flattening algorithm is to set to zero the slope across a road \nsegment. The same technique can be applied to the connected road segments. \nThe actual road mesh modification is accomplished by a world terrain height map data \nmodification. The advantage of this method is a smooth coupling of road surface with the \nsurrounding terrain. The disadvantage is that the road surface geometry will become \nimprinted to the terrain geometry. Moving the graded road will leave a modified terrain \nbehind, and this may lead to visual artifacts. \n\n\n \n \nAlternatively, the road network surface could be managed by a separate height map, and \nterrain generation code should support multiple height map layers to combine the original \nterrain data and road network maps. \nThe road segment spline defines a road profile. At the first step, we divide a road segment \nby sub-dividers, as shown in Figure 4.11.4. \nFigure 4.11.4. The road segment divided by sub-dividers. \n \n \nThe distance between sub-dividers depends on the terrain height map resolution and the \nroad edges‘ lengths. For example, if each height map point represents a 1×1 unit in world \ncoordinates, then the sub-divider distance could be an average value between maximum \nand minimum road edge lengths. \nThe second step is determining a height for sub-divider end points. The height of sub-\ndivider end points should match the height of the sub-divider center point (a height at the \nintersection between the sub-divider and a road spline) to set zero road slope. \nThe third step is applying a new height to each height map point inside of a sub-quad \nformed by a pair of sub-dividers. Since there is no guarantee that the four points defining \nthe sub-quad corners belong to the same plane, we need to triangulate each sub-quad and \nflatten the inner height map points for each triangle, as shown in Figure 4.11.5. We use the \nplane equation to calculate the new height of height map points inside the triangle. \nFigure 4.11.5. Triangulated road segment. \n \n \nThe fourth and final step is to create a smooth road surface. The results of the previous step \ncan leave visual artifacts between road sub-quads. To fix the surface, we use a bilinear \n\n\n \n \ninterpolation between neighboring height map points across the entire road segment. In the \ncase of connected road segments, the bilinear interpolation should be performed across the \nentire road network. Figure 4.11.6 shows the result. \nFigure 4.11.6. Smoothed road segment. \n \n \n \nLimiting Road Grade Angle \nTo limit a road segment grade, we use a modified road slope flattening technique. After the \nroad segment subdivision, we calculate an inclination angle for each sub-quad. Then we \nadjust the angle by raising or lowering sub-quad dividers. The challenge here is to \ndetermine which road direction should be used for raising or lowering each end. Figure \n4.11.7 shows the difference. \nFigure 4.11.7. The road grade limit difference for opposed road directions. \n \n \n \nResults and Possible Generalizations \nThese algorithms were very useful during The Sims 3 development and saved a significant \namount of world designers‘ time by automating mesh modifications. The in-game routing \nsystem also benefited from flattened road slopes and reasonably limited road grades. \n\n\n \n \nThe same algorithms could also be applied to the flattening of water surfaces (such as \nrivers) or can be modified to achieve more complex road surfaces, such as road banking on \ncurves. However, using such an algorithm on non-height map–based terrain meshes may \nrequire different approaches for sub-quad height calculation, surface triangle flattening, and \nsurrounding geometry transitions. \n \nConclusion \nThese techniques cover a relatively complete set of road creation tasks for large \nenvironments. An abstract definition of a road as a geodesic curve may not be sufficient to \ndeliver fully automated road creation, but it can provide a good starting point for the world \ndesigners, from which they can tweak road layout to their liking. The road-grading tool \nproved to be so versatile that very few (if any) manual adjustments of the meshes were \nrequired after the road was created and imprinted into the terrain. \n \nReferences \n[EA:Sims3] The Sims 3, PC/Mac. Electronic Arts, 2009. \n[DEWB] ―Digital Element WorldBuilder, PC.‖ n.d. Digital Element Inc. n.d. <http://www.digi-\nelement.com/wb/index.htm>. \n[DNF91] Dubrovin, B. A., A. T. Fomenko, and S. P. Novikov. Modern Geometry—Methods \nand Applications: Part I: The Geometry of Surfaces, Transformation Groups, and Fields. \nSpringer, 1991. \n[Wiest98] Wiest, Richard L. ―A Landowner‘s Guide to Building Forest Access Roads.‖ July \n1998. National Forest Service. n.d. \n<http://www.na.fs.fed.us/SPFO/pubs/stewardship/accessroads/accessroads.htm>. \n \n4.12. Developing for Digital Drawing Tablets \nNeil Gower \nneilg@vertexblast.com \nA walk though the art department in almost any game studio will reveal a wide selection of \ndigital drawing tablets. In their most basic form, tablets replace the mouse with a stylus for \npointer input. However, treating a tablet as merely a mouse substitute greatly underutilizes \nits potential. Most tablets offer a rich set of inputs, including pressure and tilt sensitivity as \nwell as a variety of buttons. \nIn this gem, we look at ways to harness the full potential of the drawing tablet as an input \ndevice, first by surveying the research on pen-based interfaces and then by developing an \ninterface layer in C++ to conveniently access tablet functionality. \nEquipped with this knowledge, you will be able to make full use of tablet features directly in \nyour tools. Along the way, we also note best practices to follow when developing for tablets. \n\n\n \n \nBackground \nThe pen-based interface devices we find in game development are typically digitizer tablets, \ntablet PCs, or displays with built-in tablet functionality. Standard tablets provide a form of \nindirect interaction—the user moves the pen on a tablet on their desk to move the pointer \non the screen. Tablet PCs and tablet displays provide direct interaction by allowing the user \nto place the pen directly on the display. Direct interaction can be very empowering, \nalthough it is not yet commonplace. The majority of the pen-based devices we work with \ntoday are indirect tablet interfaces. \nBuilding a Better Mouse \nIf you and your users are already proficient with the mouse (usually a safe assumption in a \ngame studio), you might wonder why you should bother with tablets. \nLow-level studies, such as [MacKenzie91], have shown that pen tablets perform as well as \nthe mouse for pointing tasks and even slightly better for dragging tasks. Considering more \ncomplex tasks, such as drawing, we find that mouse users have trouble reproducing strokes \nmore complex than straight lines [Kurtenbach93]. This is an intuitive result when you \nconsider the difficulty of signing your name with a mouse versus with a pen. \nFigure 4.12.1. Tablet terminology. \n \n \nIn addition to pen position and touch detection, a modern tablet offers features a mouse \ncannot match, such as reporting the amount of pressure on the pen tip, the tilt and \norientation of the pen, and even which end of the pen is being used. These input channels \ncan be combined with keyboard modifiers to create a huge, expressive palette of inputs to \nyour application. Recently, tablets and touch-based devices have begun to offer the ability \nto detect multiple points of contact as well, which opens up opportunities for more advanced \ngestures and for interfaces that support multiple users working together on a single device. \nApplications \n\n\n \n \nTwo-dimensional drawing is one of the archetypal applications of pen tablets. Another class \nof application is digitizing, which involves precisely transcribing real-world data (such as \nschematics) into digital form. In most drawing and creative applications, precision is less \nimportant than expressiveness and convenience. Pressure is commonly used to vary brush \nparameters, such as width or opacity. Using keyboard shortcuts and extra buttons on the \ntablet, we can enable quick tool changes with minimal interruptions to the creative process. \nThe same principles from drawing can be transferred directly to game development \napplications. For example, terrain height-map editing—tip pressure can control height \nchanges or foliage density, and the eraser can be used to reverse the height changes or thin \nout foliage. Of course, with a modern tablet this is only the beginning. For example, \nadditional parameters, such as the normal direction of the foliage, can be taken from the \npen‘s tilt values. \nAnother important area of pen-based applications is sketch-based interfaces. This includes \nsketch-based modeling, which involves taking 2D pen strokes and transforming them into \n3D modeling operations, and sketch-based design tools. Sketch-based design tools can be \nused for everything from game logic state diagrams, to UI design and layout, to even UML \nor box-and-arrow software design. \nResearchers have found that pen-based interfaces are good for encouraging users to focus \non quick design iterations. (For example, see the discussion in [Kimura93].) This is in part \nbecause for most users, drawing with a pen is a familiar way to brainstorm. Other factors \ninclude the low level of detail and consequent low level of commitment to sketched content. \nKimura, et al. also discuss evidence that users of fully featured 3D programs get easily \ndistracted by style issues rather than focusing on substance during the design process, and \neven that tools that present content in a ―sketchy‖ style encourage faster iteration. By \nstripping our early design tools down to little more than the pen and the user, removing \ncomplex menus and tool palettes, we can potentially create situations where users just draw \nwhat they think, which would be the ideal design tool. \n \nTablet Theory \nBefore we jump into coding tablet-enabled tools, we will take a look at the research and \ntheory related to pen-based interfaces. The field of human-computer interaction (HCI) is \nrich with innovative ideas and empirical studies to help you make informed design decisions \nabout your tablet-enabled tools. \nSince most tools will not be designed exclusively for tablets, a comparison to the mouse is a \ngood place to start. Although low-level speed and accuracy are comparable, there are some \nsubtle differences between these devices. For example, users tend to produce larger \ngestures with the stylus than with the mouse [Moyle02], which may require you to scale the \nuser‘s input to compensate if you use a gesture-recognition system. There is also a \ntendency for right-handed users to err upward when moving the pointer to the right and \ndownward when moving left. The reverse is true of left-handed users. This horizontal error \neffect is less noticeable in mouse users; however, they tend to have more difficulty with \naccurate vertical motions compared to pen users. These differences imply that we should try \nto make decisions early on about whether to design our interfaces to favor one device type \nor to provide customized modes for each. \nAnother issue to consider is accuracy. When targets are only a few pixels wide, accurate \nselection can become an issue in pen-based systems. This is a common scenario in 3D \nediting applications—for example, when selecting vertices. Remember, too, that if the user \nmust hold the pen over a location before touching the tablet, he has to steady his hand \nwhile doing so. This is more difficult than with a mouse, which sits perfectly still on the desk \n\n\n \n \nwhen at rest. Direct-input setups, such as tablet PCs, introduce additional challenges for \nprecise selections, caused by the parallax between the display and its protective surface. \nOne solution to precision selection problems is presented in [Ramos07], which describes a \npressure-activated zoom lens. They found this interface effective at increasing both speed \nand accuracy, and it was well received by users. In pressure-activated systems, it is good \npractice to provide feedback to the user about how much pressure is required to activate \nthe system. In the case of the zoom lens, as the user approaches the pressure threshold, \nthe lens becomes more opaque to indicate its readiness. \nWorking in 3D \nManipulating 3D objects viewed with 2D displays using 2D input devices is always a \nchallenge. [Chen88] contains a good discussion of techniques for three-axis rotation \ntechniques that work with tablets. Their paper provides a good implementation-level \ndescription of their ―Virtual Sphere Controller.‖ \nThe virtual sphere essentially places a bounding sphere around the object to be rotated. \nDragging the stylus up and down or left and right rotates around X (forward) and Y (up), \nrespectively. To rotate around Z (out of page), the user moves the stylus around the \ncircumference of the sphere. \nFigure 4.12.2. 3D rotation controllers. \n \n \nAnother interesting stylus technique is the ―stirrer,‖ introduced by [Evan81]. For this tool, \nusers make small circular motions, analogous to stirring, to modify a single axis value. The \nstirrer has several nice properties. It can be used anywhere on the tablet, so it doesn‘t \nrequire the user to pay attention to pen location. Instead, users can focus on the object \nbeing manipulated. Users can also vary the rate of change by varying the size of the circles, \nwith larger circles changing the value more slowly. Evans uses the stirrer to create a virtual \n",
      "page_number": 403
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 418-425)",
      "start_page": 418,
      "end_page": 425,
      "detection_method": "topic_boundary",
      "content": " \n \nthree-axis trackball, which maps x and y stylus movement to rotation around X and Y and a \nstirrer to rotation around Z. \nShortcuts and Gestural Interfaces \nAs users become more proficient with software tools, they seek ways to optimize their \nworkflow and focus their efforts on the creative aspects of their work. A typical optimization \nis the use of shortcut keys to directly invoke commands. In practice, however, studies have \nshown that many users fail to make the transition to using shortcut keys. Gestural shortcuts \nhave been shown to be easier to learn while still providing the same level of performance to \nexpert users. In [Appert09], there is a good discussion of the performance and \nimplementation of a gesture-recognition system for this purpose, including references to a \nvariety of alterative approaches as well. \nA general principle of UI design is to make the functions of the system self-revealing, which \njust means that users should be exposed to the functions through normal interaction with \nthe system. For example, listing shortcut keys with their menu items makes the shortcuts \nself-revealing. You can similarly show gesture marks next to menu items or draw the \ngestures on the screen briefly when the user invokes the command. This concept of self-\nrevealing gestural shortcuts has also evolved into an elegant menu solution—marking \nmenus. \nGestures and Pie Menus: Marking Menus \nMarking menus evolved out of pie menus. Pie menus are menus arranged in a circular \nlayout, rather than the traditional linear box arrangement. The user touches the pen down \nto open the menu and then drags the pointer into the pie slice corresponding to the \ncommand he wants. Once the correct command is highlighted, the user lifts the pen to \ninvoke the command. Different menus can be triggered by opening the menu in different \nareas of the workspace, similar to right-click context menus in mouse interfaces. \nPie menus present several advantages to tablet users. The circular layout means different \nmenu options are accessed by moving the pointer in different directions, rather than just \nvarying the magnitude in a single direction. This requires less precision and effort to make \ncorrect menu selections. Another useful property is that due to the widening of the slices as \nthe pointer moves away from the center of the pie, the user is able to dynamically increase \nthe effective size of the menu items, simply by making larger gestures. \nThe circular layout can also be used to represent logical relationships between menu \noptions—for example, placing logically opposite commands such as cut and paste spatially \nopposite each other in the menu. This helps users remember where commands are in the \nmenus. Studies show that even numbers of pie slices facilitate the best performance, \nespecially 4, 8, and 12. There is also evidence that right-handed users access the upper-\nright and lower-left quadrants of the pie most quickly (see [Hancock04]), so you may want \nto place your most frequently used commands in these spots. Since the effect is related to \nthe handedness of the user, it‘s also a good idea to provide an option to flip the menu \nlayout for left-handed users. \nMarking menus take pie menus and combine them with gestural shortcuts to create a highly \nefficient interface [Kurtenbach93]. The basic marking menu implementation is very similar \nto a pie menu. The menu is pressure activated, and the user‘s pen motions are shown as a \ntrail on the screen. There is also a delay before the pie menu appears. The user can move \nthe pen in the anticipated direction of their selection and lift the pen to complete the \nselection during this delay before the menu appears. In this case, they have ―marked‖ their \nselection. See Figure 4.12.3. \nFigure 4.12.3. Marking menus. \n\n\n \n \n \n \nWhat makes marking menus so effective is that using the pie-menu interface is also a \nrehearsal of the shortcut marking gesture, so learning the shortcuts does not require any \nadditional effort from the user. Since the gestures created by the marking menus are all \nsequences of linear motions, marking menus can be combined with other gesture-\nrecognition systems that use non-linear gestures. This concept can be extended to \nhierarchical marking menus as well. \n \nTablet Programming \nA high-level point about adding tablet support to your tools is that it‘s highly advisable to \npackage your tablet code into a reusable toolkit. Your toolkit can include things such as a \ngesture-recognition system, custom UI widgets, and low-level tablet interface code. This \nhelps provide consistency across applications and makes it easier and therefore more likely \nthat other tool developers will add gesture support to their projects. \nFor Windows applications, Wintab is the industry-standard API for accessing tablet devices. \nFor Mac OS X, we have a choice between the older Carbon (C) API and the newer Cocoa \n(Objective C) API. On Linux, we use the XFree86 input extension and XFree86 event queue. \nOn the CD-ROM, you will find an example program that uses Wintab to read and display the \nvarious types of tablet data discussed in this gem. \nTablets generally have a wide variety of device capabilities, much like PC joysticks. It‘s \nimportant to query the capabilities of the device at run time and make the necessary \nadjustments in your code. A few of these capabilities can even change at run time, such as \nthe type of stylus being used. \nGetting Started with Wintab \nTo get things started, let‘s take a look at what‘s needed to initialize Wintab. \n#define PACKETDATA (PK_X | PK_Y | PK_BUTTONS) \n#define PACKETMODE PK_BUTTONS \n#include \"pktdefs.h\" \n \nif ( !WTInfo(0, 0, NULL) ) { \n    // Wintab is not available on this system \n    return -1; \n\n\n \n \n} \n \n// start with a default context... \nLOGCONTEXT contextParams; \nWTInfo( WTI_DEFCONTEXT,0, &contextParams ); \n \n// customize it for your application... \ncontextParams.lcOptions  |= CXO_MESSAGES | CXO_SYSTEM; \ncontextParams.lcPktData   = PACKETDATA; \ncontextParams.lcPktMode   = PACKETMODE; \ncontextParams.lcBtnUpMask = contextParams.lcBtnDnMask; \n \n// then get a context handle and we're ready to go! \nHCTX hContext = WTOpen( hWnd, &contextParams, TRUE ); \n \nAt the top, we have some macro magic that enables pktdefs.h to generate the PACKET \nstruct that matches the values you set in lcPktData further on. The second WTInfo() \ncall initializes our context parameters struct with reasonable defaults for most applications. \nThen we just tweak the parts we need before giving it to WTOpen() to initialize the tablet \ncontext and generate a handle for it. \nNow that we‘re up and running, let‘s see how to get position data from the tablet. With \nWintab, we get data from the tablet via packets in a message queue. When we configured \nthe context parameters, we had to set CXO_MESSAGES in lcOptions. This tells Wintab \nto send our application WT_PACKET messages through the standard Windows messaging \nsystem. WT_PACKET signals our code when the tablet has data queued up for the \napplication. In the handler for this message, we use WTPacket() to extract the data. \ncase WT_PACKET: \n    PACKET pkt; \n    WTPacket( hContext, wParam,&pkt ); \n    cursorX = pkt.pkX; \n    cursorY = pkt.pkY; \n    break; \n \nIt‘s important to stay on top of the packet queue, because if it overflows we‘ll stop getting \nWT_PACKET messages, and the tablet will stop sending state information. Practically \nspeaking, this means we‘ll usually use WTPacketsGet() to retrieve all of the packets \ncurrently in the queue and then process each one (demonstrated in the example code on \nthe CD-ROM). \nAs shown in the previous code listing, each packet contains the state information we \nrequested in PACKETDATA during initialization. In this case, pkX and pkY are the cursor \ncoordinates. \nNote that when we set up the context, we also set CXO_SYSTEM in lcOptions. This tells \nWintab to move the system cursor, rather than depending on our code to do it. This is the \nsimplest way to handle tablet versus mouse positioning. Clearing CXO_SYSTEM allows you \nto track mouse and tablet positions separately. This can get tricky and is potentially \nconfusing to the user. If you are rendering your own pointer, hiding the system cursor is a \ngood solution that still leverages Wintab to manage the pointer location. \n\n\n \n \nBy default, the tablet will deliver position information in tablet units (in other words, at the \nresolution of the tablet). With Wintab, you can specify a different range. For example, you \ncan have it scale positions to screen units by setting lcOutExtX/Y to the screen \ndimensions. Keep in mind that the origin of the default tablet coordinate system is in the \nlower-left corner of the tablet. You can change this by supplying negative extents to the \ncontext. If you also want to translate the tablet origin—for example, to match the origin of \nyour application‘s drawing area—you can do so with lcOutOrgX/Y. \nThe pen tip and eraser post standard Windows button up/down messages. This provides \ncompatibility with non-tablet-aware applications, since pen-down is usually equivalent to \nleft-click. Our code is tablet aware, though, so it can ignore the button messages and \ninstead process the tablet‘s button and pressure values directly from the packet queue. \ncase WT_PACKET: \n    PACKET pkt; \n    WTPacket( hContext , wParam, &pkt ); \n    DWORD buttonNum     = LOWORD( pkt.pkButtons ); \n    DWORD buttonChange  = HIWORD( pkt.pkButtons ); \n    // buttonChange is one of TBN_UP, TBN_DOWN, TBN_NONE \n \nThis code listing extracts the button number and button state from the packet. This is based \non the button reporting being in relative mode, which we configured by including \nPK_BUTTONS in our PACKETDATA and setting PK_BUTTONS in our context‘s \nlcPktMode. \nBeing a Good Neighbor \nRemember that more than one application can be running that uses the tablet for input. \nWith Wintab, tablet contexts are layered so that usually only the application with the top-\nmost context receives tablet events. It‘s up to you to make sure your context‘s stacking \nposition is in sync with the application‘s window position. When your window loses focus or \ngets minimized, you should push your tablet context to the bottom of the context stack with \nWTOverlap(). \nCursor Proximity \nWe have a basic level of functionality now, but really this only gets us to the same level as a \nmouse. Let‘s start adding some more advanced tablet features to the application. \nA key part of the tablet system is the cursor. The cursor is the physical device that the user \nuses to interact with the tablet surface. There are a variety of cursor types in addition to the \npen, including the eraser (usually the opposite end of the pen), airbrush, and puck. \nThe cursor does not have to be touching the tablet surface for it to be detected. Most \ntablets sense the cursor from about half a centimeter above the actual surface. Depending \non how general you want your application to be, you can request WT_CSR-CHANGE events \nwhen a cursor comes within proximity of the tablet by setting CXO_CSRMESSAGES in \nlcOptions. If you‘re not interested in the details of the cursor being used with the tablet, \nyou can just do everything with WT_PROXIMITY, which you‘ll get whenever any cursor \nenters or leaves the range of the tablet. WT_CSRCHANGE is an extra message that you get \nonly when the cursor enters proximity, which is the ideal time to update your cursor \ncapabilities and options. \ncase WT_PROXIMITY: \n\n\n \n \n    bIsInProximity = LOWORD(lParam); \n    break; \n \ncase WT_CSRCHANGE: \n    WTPacket( hContext, wParam, &pkt ); \n    // use pkt.pkCursor with WTInfo() to get properties of the \n    // active cursor... \n    break; \n \n \nAdditional Input Axes \nPressure is a common and very useful pen feature. This data is relatively straightforward to \naccess. When querying the device capabilities, you can get the parameters for the ―normal \npressure axis,‖ which tells you how many levels of pressure the device reports. This can \nthen be used to scale the pressure values you get from pkt.pkNormalPressure when \nyou have PK_NORMAL_PRESSURE set in your PACKET struct. Users can customize \nthresholds and response curves for pressure sensitivity at the system level, but as long as \nyou access the pressure data this way, this will all be transparent to your application. \nIf you‘re looking at the pressure information in the Wintab spec, you‘ll also see there is a \ntangent pressure property. This is pressure parallel to the tablet surface, which can come \nfrom special cursors like the airbrush, which has a finger wheel that produces tangent \npressure values. \nAnother useful feature is tilt sensing. This allows you to detect the angle that the stylus is \nbeing held at relative to the tablet surface. Wintab exposes this as pkt.pkOrientation, \nwhich represents the orientation as an azimuth and altitude angle (see Figure 4.12.4). Like \nthe pressure values, the orientation values should be scaled based on the device \ncapabilities. We can convert the orientation angles into a unit vector with a little \ntrigonometry: \nFigure 4.12.4. Azimuth and altitude representation of orientation. \n \n \nfloat azimuthRads = 2.0f * PI * pkt.pkOrientation.orAzimuth \n                        * azimuthScale; \nfloat altitudeRads = PI * pkt.pkOrientation.orAltitude \n                        * altitudeScale; \nfloat lengthXY = cos( altitudeRads ); \nfloat tiltX = sin( azimuthRads ) * lengthXY; \nfloat tiltY = cos( azimuthRads ) * lengthXY; \n\n\n \n \nfloat tiltZ = sin( altitudeRads ); \n \n \nButtons \nBesides the tip, most pens have buttons on the barrel as well. The states of these buttons \nare given to your application through pkt.pkButtons, just like the tip. Some tablets \nhave additional buttons on the tablet itself. These are not usually part of the main Wintab \nspecification, so they must be accessed through vendor-specific extensions. Vendors such \nas Wacom supply documentation and examples of how to access these extended features. \n \nConclusion \nWe‘ve looked at a variety of topics and techniques from the human-computer interaction \nresearch community and learned how to use the Wintab API to interact with tablets. The \nexample code on the CD-ROM can serve as a starting point for creating your own tablet-\nenabled software. The papers listed in the ―References‖ section (and the many papers they \nreference) provide much more detail than could be included here about the design and \nimplementation of the various pen-based solutions we touched on. They are an invaluable \nresource when you decide to tackle one of these projects yourself. \nWith the recent popularity of touch-based devices, touch- and pen-based computing has \nbecome an active and exciting area of research. Hopefully, this gem will inspire you to add \nrich and powerful tablet functionality to your next game tool project. \n \nReferences \n[Appert09] Appert, C., and S. Zhai. ―Using Strokes as Command Shortcuts: Cognitive \nBenefits and Toolkit Support.‖ Proceedings of the 27th International Conference on Human \nFactors in Computing Systems (2009): 2289–2298. \n[Chen88] Chen, M., S. Mountford, and A. Sellen. ―A Study in Interactive 3-D Rotation Using \n2-D Control Devices.‖ Proceedings of the 15th Annual Conference on Computer Graphics and \nInteractive Techniques (1988): 121–129. \n[Evans81] Evans, K. B., P. P. Tanner, and M. Wein. ―Tablet-Based Valuators That Provide \nOne, Two, or Three Degrees of Freedom.‖ Proceedings of the 8th Annual Conference on \nComputer Graphics and Interactive Techniques (1981): 91–97. \n[Hancock04] Hancock, M., and K. S. Booth. ―Improving Menu Placement Strategies for Pen \nInput.‖ Proceedings of Graphics Interface (2004): 221–230. \n[Kimura93] Kimura, T. D., W. Citrin, D. Halbert, C. Hewitt, N. Meyrowitz, and B. \nShneiderman. ―Potentials and Limitations of Pen-Based Computers.‖ Proceedings of the \n1993 ACM Conference on Computer Science (1993): 536–539. \n[Kurtenbach93] Kurtenbach, G., A. Sellen, and W. Buxton. ―An Empirical Evaluation of \nSome Articulatory and Cognitive Aspects of Marking Menus.‖ Journal of Human Computer \nInteraction 8 (1993): 1–23. \n\n\n \n \n[MacKenzie91] MacKenzie, S., A. Sellen, and W. Buxton. ―A Comparison of Input Devices in \nElemental Pointing and Dragging Tasks.‖ Proceedings of the CHI ‘91 Conference on Human \nFactors in Computing Systems (1991): 161–166. \n[Moyle02] Moyle, M., and A. Cockburn. ―Analysing Mouse and Pen Flick Gestures.‖ CHI‘02. \n(2002): 19–24. \n[Ramos07] Ramos, G., A. Cockburn, R. Balakrishnan, and M. Beaudouin-Lafon. ―Pointing \nLenses: Facilitating Stylus Input through Visual- and Motor-Space Magnification.‖ \nProceedings of the SIGCHI Conference on Human Factors in Computing Systems (2007): \n757–766. \n[Wacom09] ―Wacom Software Developer Support.‖ n.d. Wacom Technology. 1 Sept. 2009. \n<http://www.wacomeng.com/devsupport/index.html>. \n \n4.13. Creating a Multi-Threaded Actor-Based Architecture Using \nIntel® Threading Building Blocks \nRobert Jay Gould, Square-Enix \nrobert.jay.gould@gmail.com \nWith the next generation of consoles and personal computers having dozens of smaller \nprocessing cores, developers will have to redesign the architecture of their game engines to \ntake advantage of this processing power. In some areas, such as graphics and physics, \nvarious methods to achieve higher performance already exist. In general, there has been \nless success in adapting higher-level AI- and gameplay-related systems to massively multi-\ncored environments. \nThis gem presents one architecture and implementation that can be used for AI and \ngameplay systems capable of scaling through high concurrency. The proposed architecture \nis a multi-threaded message passing actor architecture, which uses engineering tradeoffs \nsimilar to those of the Erlang language in order to achieve its concurrency and scalability. \nThe implementation is in C++, using Intel‘s Threading Building Blocks (TBB) for high-\nperformance concurrency and Lua to create a friendly programming interface that hides the \ntypical complexity associated with message-passing systems so that designers and \ngameplay programmers can get their work done productively. \nIntroduction \nWhen striving for performance in multi-cored environments, the only solution is to increase \nthe concurrency (amount of work that can be done in parallel) to leverage the system‘s \nprocessing power. Thus, finding concurrency patterns in gameplay systems should be the \nfirst goal when designing scalable architectures. \nFinding Concurrency in Games \nGames are composed of several systems, and although each system might have its own \npeculiarities, they can be broadly grouped together by considering what kind of \nconcurrency, or parallel, programming patterns [Mattson04] best suit them. For example, \nsystems like graphics and physics that are almost entirely data-driven can be easily \nparallelized by applying data decomposition patterns and using implicit parallelization and \nreferential transparency techniques. On the other hand, gameplay systems are primarily \nevent-driven. If one looks at event handling as creating tasks to process events, task \ndecomposition patterns can be applied to gameplay systems to increase their concurrency. \n\n\n \n \nTask Decomposition \nIn simple terms, task decomposition is about breaking down the useful work of a system \ninto atomic tasks that can be processed concurrently, without compromising the state of the \nsystem. In games, we usually have no problem in dividing work into tasks, as this is \nsomething that is already part of most game architectures today. Instead, fulfilling the \n―without compromising the state‖ part is the tricky part in game development. So finding \nways in which state can be effectively divided and can be protected is the key to achieving \ntask decomposition in the event-driven parts of our game engines. \nState Protection Strategies \nPerhaps the most familiar way to protect the state of a system, for game programmers, is \nthrough lock-based synchronization. As most programmers also know, lock-based systems \ncan grow in complexity quite easily, and avoiding dead locks, live locks, data races, and \nconvoying can consume a large chunk of precious development time. Besides the \ncomplexity and instruction overhead, lock-based synchronization also forces a non-\nnegligible degree of serialization on processing, meaning that as the number of processing \ncores increases, scalability levels out, and it provides decreasing returns. Of course, the \nideal alternative to lock-based synchronization is to use lock-free algorithms, but besides \nimplementations being devilishly difficult even for algorithm experts, in many cases there \nare simply no known lock-free algorithms for some problems. \nOther common techniques involve attacking the problem by using memory policies as \nopposed to processing policies, as the previous two alternatives do. One of these \ntechniques, generally applicable to any sort of system, is to use immutable data like pure \nfunctional languages have decided to do. The benefit of immutable data is that state cannot \nbe compromised; instead, state changes are implemented through data copying and \ngarbage collection. Unfortunately, in systems that require lots of data and have rather tight \nmemory constraints, such as games, all this redundancy can become an issue in itself. One \nsolution to reduce this redundancy is called transactional memory. It consists of maintaining \nmetadata of the protected state, so it is possible to determine when conflicts arise and roll \nback and retry conflicting operations. \nThe strength of transactional memory is that unlike locks, it provides increasing returns as \nconcurrency increases, so one day transactional memory may be the easiest solution for \nhigh throughput systems, such as games [Sweeney08]. However, we are limited to software \ntransactional memory (STM), as there is no hardware support for these features. STM‘s \ncurrent downsides are mostly due to its experimental nature, which requires experimental \ncompiler extensions and proprietary run-time libraries. It also has a large memory and \ninstruction overhead due to the current bookkeeping algorithms, meaning that its \nperformance penalties typically outweigh its benefits when shared state contention levels \nare low [Cascaval08], as they typically are in games. \nFinally, one more solution—and the one used by the architecture presented in this gem—is \nthe shared nothing [Stonebraker86] approach, where state cannot be compromised because \nit is not shared. This is typically achieved by having each concurrent process be completely \nresponsible for modifying its own data through strong encapsulation. For this approach to \nscale on multi-cored environments, implementing lightweight processing units that can work \nconcurrently is the way to go, and these techniques can be found in actor-based \nprogramming. \n \nActor-Based Programming \nActor-based programming is a programming paradigm in which the basic programming and \nprocessing unit is the actor. Actors are similar to objects in that they encapsulate state, but \n",
      "page_number": 418
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 426-433)",
      "start_page": 426,
      "end_page": 433,
      "detection_method": "topic_boundary",
      "content": " \n \nunlike typical objects they are also capable of processing information on their own, sort of \nlike a cross between an object and a thread. Another difference between C++ style objects \nand actors is that instead of relying on method invocation for interaction, actors use \nmessage passing for communication. \nUnfortunately, many game programmers relate message passing to the typical centralized \nmessage-pump design commonly used in games (as if the entire game were a single actor). \nMessage passing is also associated with lots of the boilerplate code necessary to define and \nhandle all these messages, but these are just the artifacts of a poor interface design and \nC++‘s syntax. In fact, message-passing interfaces can be quite elegant without any tedious \nboilerplate code, with Smalltalk and Objective-C being good examples of elegant message-\npassing interfaces. \nThe actor-based architecture in this gem is in great part inspired by the Erlang language, \nwhich is the most successful actor-oriented language in use and also one of the most \nscalable languages in general. Erlang is a mature language that was designed by Joe \nArmstrong while working at Ericsson in 1986 to develop robust and scalable applications to \nhandle telecommunications systems [Armstrong07]. Nowadays, Erlang is used by many \ntelecoms around the world to power their telephony systems, and recently Erlang has been \ngaining ground in scalable commercial web applications, such as Amazon‘s EC2 and \nSimpleDB platforms and Facebook‘s chat system. Erlang‘s younger sibling and close \ncompetitor, Scala, which runs on the Java Virtual Machine, is used to provide the scalability \nbehind services such as Twitter, demonstrating the power of actor-based designs. \nOne critical diverging engineering point between Erlang and the architecture in this gem is \nthat Erlang is optimized for scaling in highly distributed environments, meaning that it has \nefficient IO handling as one of its central pillars. The actor-based architecture in this gem is \noptimized for running in highly multi-cored environments, focusing on efficient use of a \nsingle machine‘s multiple cores, foregoing IO-based considerations almost entirely. \n \nImplementing an Actor Engine \nThe first concern of implementing an actor engine is to fulfill the need to support the ability \nof each actor to process its own work individually and concurrently. The simplest, yet least \nscalable solution to this is to provide an individual physical thread (or even OS-level \nprocess) for each actor. Unfortunately, that means that as the number of actors increases, \nthe memory and processing overhead due to context switches increases, and scalability will \nlevel off and even decrease. The better solution is to use something lighter than threads, as \nErlang does, so we can support thousands of concurrent actors with near linear scalability. \nIn Erlang, this primitive processing unit is simply called a process, which is a lightweight \ncontext similar in nature to a green thread or a coroutine in implementation. The actor \nengine herein utilizes Intel‘s TBB library to provide the parallel processing needs of its \nactors, based on task-processing algorithms. \nIntel‘s TBB is an open-source threading library that provides a platform- and compiler-\nindependent production-ready foundation for building multi-threaded applications that only \nrequire standard C++ support from the compiler. It includes several high-level tools such as \nconcurrent algorithms like tbb::parallel_for and several thread-safe containers like \ntbb::concurrent_hash and tbb::concurrent_queue. There are also lower-level \ntools, such as scalable memory allocators, and a scalable task scheduler, \ntbb::task_scheduler, that is used to power all of TBB‘s higher-level concurrent \nalgorithms. It is these lower-level task scheduler components that are used to construct the \nprocessing workhorse for the actor engine of this gem. \nThe Task Scheduler \n\n\n \n \nThe task scheduler‘s main job is to effectively map its task-based workload onto the \nresources available to it in a way that avoids unnecessary context switching, thus providing \na scalable solution for task-based concurrency. Yet its performance benefits don‘t stop \nthere. Unlike most simple schedulers, the tbb::task_scheduler it isn‘t a fair scheduler \nthat uses a round-robin-like algorithm. Instead, it is implemented as a finely tuned work-\nstealing scheduler [Intel09] that schedules tasks in a fashion that decreases cache misses \nand memory thrashing. It does this by first working on tasks that are hot in the cache and \nthen working its way onto colder tasks as necessary. This cache-friendly scheduling also \nmeans that the task scheduler actually resembles a LIFO-like processor more than a typical \nFIFO-like processor. \nWhat this means to our architecture, besides high performance processing, is that it doesn‘t \nimpose any ordering restrictions on message processing. Although this might sound strange \nat first, allowing messages to be handled out of order is just what Erlang‘s message passing \nalgorithm does, because this is an important area that is best left open for scalability \noptimizations. \nThe Message Class \nWith the processing implementation decided, we‘ll quickly look at the messages that will be \nused to allow actors to interact. This, however, is actually just a simple implementation \ndetail, and the precise type of a message is probably best left for each system to define \nbased on the system‘s other requirements. For this reason, the message type that actors \nhandle is simply defined as a template argument in our implementation. \nThe Actor Class \nIn implementing the actor class, we can divide its functionality into four fundamental \nparts—its internal state, a message queue, a message processor, and the message \nhandling. The message queue and message processing are the critical core features that are \nin the domain of the system programmer to implement efficiently to provide good scalability \nwith little overhead, and as such, they are implemented in the base actor class. The internal \nstate and the message handling are the extensible features of the actor, so they are derived \nby subclasses of the base actor class. \nThe Message Queue \nAs one of the core features of the actor class, the message queue is important because it is \nthe only contention point in the actor‘s architecture where several threads may interact on \nthe same data. This implies that its performance directly affects the overall performance of \nthe actor and the scalability of system at large. The obvious candidate as a container for our \nmessage queue is tbb::concurrent_queue. It provides a thread-safe queue that \nallows several actors to add messages to it concurrently, with a relatively low overhead \nwhen compared to a std::queue with a big lock around it. \nOn this subject, an optimization not present in the sample code but likely of interest to \nsystem programmers is that the tbb::concurrent_queue is what is called a thread-\nsafe multi-producer multi-consumer (MPMC) queue, which is safe when several producers \nand consumers push and pop on the queue concurrently. However, our actors only need a \nmulti-producer single-consumer (MPSC) queue, because the only consumer of the queue is \nthe actor itself. So we are paying for more than we need by using \ntbb::concurrent_queue. While doing tests and benchmarks using an excellent lock-\nfree MPSC queue algorithm [V‘jukov08], the system‘s performance increased as much as 10 \npercent under a heavy load with high contention rates, showing just how important the \nperformance of the message queue is to the actor engine. \nActor Hierarchies \n\n\n \n \nIn this architecture, any number of root actors can be spawned, and all actors are allowed \nto spawn children, which become their responsibility. This means there can typically be \nmany trees of actors (a forest) running concurrently. This is useful because it allows any \nparticular actor hierarchies to be spawned or shut down independently, allowing particular \nsubsystems to be restarted without affecting others. As long as two actors can understand \neach other‘s messages, they can communicate, even if they belong to different hierarchies; \nhowever, actors within a single hierarchy will likely be processed in closer temporal \nproximity. \nListing 4.13.1. Actor construction \nActor::Actor( Actor* parent) :pParent(parent) \n{ \n    if (pParent) \n    { \n        //Get root processing unit/task \n        pRootTask = pParent->pRootTask; \n    } \n    else \n    { \n        //Become a new root   processing   unit/task \n        pRootTask = new   ( \n            tbb::task::allocate_root())tbb::empty_task(); \n        pRootTask->set_ref_count(++childrenCounter); \n    } \n} \n \nAs seen in Listing 4.13.1, root actors keep circular references to their own TBB processing \nroot task. This prevents the scheduler from cleaning up the task associated with each \nprocessing hierarchy when no work is left, allowing us to reuse the same root task for each \nactor tree indefinitely. Also of note is TBB‘s use of in-place object allocation using \nnew(tbb::task::allocate_root()). This idiom has the purpose of avoiding the \noverhead of creating tasks by recycling tasks from object pools behind the scene. Both \nfeatures serve to avoid memory management bottlenecks due to the large number of tasks \nthat will be spawned during the engine‘s lifetime. \nMessage Processing \nThe message processing cycle is the heart of the actor engine, and most of the important \nimplementation details are located in this section. Things will get a bit grittier from here on. \nAs in Erlang, message passing between actors in our design is totally asynchronous. The \ninteractions among actors are entirely one-way affairs, not requiring any sort of handshake \nbetween actors. This allows the sender to continue on with its own processing without \nhaving to wait on the recipient to handle the message sent. If we forced interactions to be \nsynchronous by requiring handshakes, message passing would not be scalable as Erlang‘s \nmessage passing is, and instead it would be similar to the message passing found in \nObjective-C or Smalltalk. \nBy decoupling actor interactions, the message handling can be decomposed into a great \nnumber of very finely grained discrete tasks. Theoretically, this allows the system to scale \nlinearly as long as actors outnumber the number of processing cores. However, in reality, \nthe creation and processing of tasks by the task scheduler has an overhead in itself. \n\n\n \n \nBecause of this, TBB‘s documentation recommends that the grain size of a task be between \n100 and 100,000 instructions for the best performance [Intel09]. So unless most message \nhandling is quite heavy, assigning one task per message can become quite wasteful. This \nissue requires optimizations to increase the total throughput of the system. \nWhen an actor places a message into another actor‘s inbox, the processing thread kick-\nstarts the recipient actor, as shown in Listing 4.13.2. In situations where the sender will be \nwaiting for a reply from the recipient, this optimization allows for reduced latency as the \nrecipient‘s task will be hot in the cache and favored by the task scheduler for being \nprocessed next. More importantly, this design also removes the need for having actors \nwastefully poll for work, making processing entirely event-driven. \nListing 4.13.2. Message passing \nvoid Actor::inbox(Actor*   sender,   const   MESSAGE_TYPE&  msg) \n{ \n     this->messageQueue..push(msg); \n     this->tryProcessing(sender->GetProcessingTask()); \n} \nvoid Actor::tryProcessing(tbb::task* processing_unit) \n{ \n     if( !messageQueue.empty() && \n         isProcessingNow.compare_and_swap(true,false)) \n     { \n         //use a continuation task \n         tbb::empty_task* continuation = new( \n             root->allocate_continuation())tbb::empty task; \n         this->pMessageProcessorTask = new( \n             continuation.allocate_child())MsgProcessor(this); \n         continuation.set_ref_count(1); \n         this->root->spawn( this->pMessageProcessorTask); \n     } \n} \n \nAlso in Listing 4.13.2, you can see that TBB continuations are utilized. This allows more \nfreedom to the scheduler for optimizing its own workflow by decoupling execution order. A \ndetailed explanation of continuation-style programming can be found in [Dybvig03]. \nAs mentioned earlier, to increase the workload of a single task over 100 instructions, an \nactor will consume the entirety of its message queue, as well as any messages that arrive \nwhile it is processing within the execution of a single task, as seen in Listing 4.13.3. This \ndesign dynamically eases the task-processing overhead as work increases. \nListing 4.13.3. Message consumption loop \nvoid Actor::processAllMessages() \n{ \n     isProcessingNow = true; \n     Msg_t msg; \n     while(messageQueue.trypop(msg)) \n     { \n\n\n \n \n         try \n         { \n              if(!this->receive(msg)) \n              { \n                  throw(messageHandlingError); \n               } \n         } \n         catch(tbb::exception e) \n         { \n             sendException(pParent, e, msg); \n         } \n \n     } \n \n     isProcessingNow = false; \n} \n \nAlso of note is the error handling logic employed for message processing. In an actor-based \narchitecture, when an actor encounters an exception or error it cannot handle, it doesn‘t \nmake sense for the error to bubble up through the call stack. Instead, it needs to bubble up \nthrough its actor hierarchy, forwarding the exception to its parent. Generally, when a parent \nreceives an error from a child, it has few options because it can‘t query or fix the child‘s \nstate directly. Typical error handling includes ignoring the message altogether, reporting the \nerror to another actor, or as is common in Erlang, creating a fresh new child actor to replace \nthe actor having problems. The functionality of the failing actor is thereby automatically \nreset to a clean state. \nMessage Handling \nLike actors in Erlang, our actors have a receive method, which can be seen in Listing \n4.13.3. The receive method is called whenever a message needs handling; the actual \nimplementation of this method is the responsibility of the derived actor classes. Typical \nimplementations of receive consist of matching message signatures to specific message \nhandling routines. This can be done in many ways, from something like a switch/case \nconstruct or a series of if/else statements to something more complex, such as \nrecognizing message signatures, regular expression matching, hierarchical state-machines, \nor even neural networks. A couple of implementation prototypes that might serve as \ninspiration can be found on the CD, but the one design that will be the focus for the rest of \nthis gem is that of a scripted actor, which matches message signatures directly to Lua \nfunctions registered to the actor. This straightforward design provides a simple and generic, \nboilerplate-free solution that can be easily extended by gameplay programmers and \ndesigners using Lua alone to build whatever class hierarchies or aggregations they need to \nget their work done, without having to touch any of the library code directly. \nAs promised earlier in this gem, message-passing APIs can be friendly and don‘t have to \ninvolve lots of boilerplate, as the Lua code in Listing 4.13.4 demonstrates. In fact, it looks \npretty much like any ordinary Lua API. \nListing 4.13.4. Lua-based actor API \nclass ―Player‖:inherit ―Character‖ \n{ \n\n\n \n \n    attack=function(target) \n        target:mod_hp(math.random(5,10)) \n    end, \n} \n–Example use case \nlocal player = getActor(―player1‖,‖Player‖) \nlocal enemy = getActor(―goblin01‖) \nplayer:attack(enemy) \n \nTo provide a clean, scriptable API, getActor takes an actor name and returns an actor \nproxy, not an actual actor reference. Using the proxy‘s metatable‘s index logic, it is capable \nof responding to any sort of function call. The proxy then converts these object-oriented \nfunction calls and its arguments into a message that gets sent to the C++ actor. Of course, \nsimply allowing game code to call any method on the proxy with no error checking could \nmake debugging quite complicated, but typically an unhandled message log combined with \nsome class-based checking makes most debugging reasonable. To add the class-based \nmessage checking feature to a proxy, an optional second argument is passed in—the class \nname of the class that should be used for verifying message calls, seen in Listing 4.13.5. \nListing 4.13.5. Lua actor proxies \nfunction getActor(address,classname) \n    local class = findClass(classname) \n    return    \nsetmetatable({__address=address,__class=class},mt_actor) \nend \n \nmt_actor.__index = function(self,message,...) \n    if rawget(self,―class‖) and not self.class[message] then \n        error((―class [%s] can’t handle message [%s]‖):format( \n               self._class.name,message)) \n    else \n        self._message = message \n        return send(self) → a C-function connected to the actor \nengine \n    end \nend \n \n \nMessage-Passing Patterns \nWhen working with asynchronous actors and message passing, some issues may appear. As \nwith other styles of programming, there are already several useful and tested programming \npatterns that can be applied to resolve most of these problems. \nQuerying for State and Promises \nAt times an actor may need to query the state of another actor, not just send it a message. \nBecause actor message passing is asynchronous, this can be an issue. One solution is to \n\n\n \n \nadd a way to force a synchronous message passing when required, but that adds coupling \nto the system that could compromise its concurrency. Another solution, and the one \ncommonly used by actor-based architectures, is to handle this scenario using a future or \npromise. Promises are implemented by sending along with the message the address of the \ncalling actor and a continuation context, to which the recipient will respond back to, \nresuming the context of the sender, or in this implementation by use of TBB continuations \n[Werth09]. In the proposed API, this is handled as seen in Listing 4.13.6, by calling the \npromise to block until the value is returned. \nListing 4.13.6. Using promises to query an actor for state \npromiseHp = enemy:get_hp() \nif promiseHp() > 50 then –-invoking the promise returns its \nvalue \n    player:run() \nend \n \nAnother type of promise is to obtain an actor reference from another actor, as in Listing \n4.13.7. This style of promise is accomplished by chaining proxy promises. \nListing 4.13.7. Using promises to obtain actors in second degree \nplayer.healClosest = function(self) \n    local map = getActor(―map‖) \n    local closestActor = \n        map:findClosestActor(self:get_position()) \n    closestActor:heal(100) \nend \n \nSequential Message Processing \nAnother special case that requires consideration because the actor-based system is \nasynchronous is how to handle messages when they only make sense in a certain order. For \nexample, when opening a door, it is required to first insert a key and then push it open. \nHowever, there is no assurance that the ―insert key‖ message will actually arrive before the \n―push door‖ message, even if they were sent in the correct order. The typical actor-based \nsolution is to use a sequencer actor that works in conjunction with the door. The job of the \nsequencer is to queue and sort messages according to some internal logic and then forward \nthem in an appropriate order as they become available. In our example, the sequencer \nwould not send the ―push door‖ message to the door before it has received the ―insert key‖ \nmessage. Although sequencers tend to be more effective than direct coupling, they do \nintroduce a degree of serialization to the code, so they should be used only where truly \nnecessary. \nMessage Epochs \nOne more common scenario is that it is possible for an actor to receive a message that, say, \nlowers its HP below 0, although a heal message had actually been sent before the damage \nmessage was issued. In most cases within a single atomic gameplay frame, the actual \nproduction order of messages should not be important, making this sort of issue actually \nmore like a quantum physics conundrum than a real gameplay issue. This means that \n\n\n \n \ngenerally, this sort of event can be ignored with no detriment to the game. Nevertheless, \nwhen disambiguation is required, an easy solution is to use epochs [Solworth92] or \ntimestamps to determine which message was sent first. \n \nConclusion \nThis gem reviewed the requirements and alternatives to build a highly scalable architecture \nand went into the details of implementing one viable alternative, based on the actor model \nand a shared nothing policy using Intel‘s Threading Building Blocks. On the CD there is a \nreference implementation in the form of an actor-based Lua console, along with sample \nscripts for experimentation with the concepts presented in this gem. \n \nReferences \n[Armstrong07] Armstrong, Joe. Programming Erlang: Software for a Concurrent World. \nPragmatic Bookshelf, 2007. \n[Cascaval08] Cascaval, Calin. ―Software Transactional Memory: Why Is It Only a Research \nToy?‖ ACM QUEUE (October 2008): n.p. \n[Dybvig03] Dybvig, Kent R. ―The Scheme Programming Language.‖ Sept. 2009. Cadence \nResearch Systems. n.d. <http://scheme.com/tspl3/further.html#./further:h4>. \n[Intel09] Intel. ―Intel(R) Threading Building Blocks, Reference Manual.‖ Sept. 2009. Intel. \nn.d. <http://www.threadingbuildingblocks.org/documentation.php>. \n[Mattson04] Mattson, Timothy G. Patterns for Parallel Programming. Addison-Wesley \nProfessional, 2004. \n[Solworth92] Solworth , Jon. ACM Transactions on Programming Languages and Systems \n14.1 (Jan. 1992): n.p. \n[Stonebraker86] Stonebraker, Michael. ―The Case for Shared Nothing Architecture.‖ \nDatabase Engineering 9.1 (1986): n.p. \n[Sweeney08] Sweeney, Tim. ―The End of the GPU Roadmap.‖ Sept. 2009. Williams College. \nn.d. <http://graphics.cs.williams.edu/archive/SweeneyHPG2009/TimHPG2009.pdf>. \n[V‘jukov08] V‘jukov, Dmitriy. ―Scalable Synchronization Algorithms, low-overhead mpsc \nqueue.‖ 13 May 2008. Google. n.d. <http://groups.google.com/group/lock-\nfree/browse_thread/thread/55df71b87acb8201>. \n[Werth09] Werth, Bradley. ―Sponsored Feature: Optimizing Game Architectures with Intel \nThreading Building Blocks.‖ 30 March 2009. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/3970/sponsored_feature_optimizing_game_.ph\n>. \n \nSection 5: Networking and Multiplayer \n",
      "page_number": 426
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 434-447)",
      "start_page": 434,
      "end_page": 447,
      "detection_method": "topic_boundary",
      "content": " \n \nIntroduction \nSecure Channel Communication \nSocial Networks in Games: Playing with Your Facebook Friends \nAsynchronous I/O for Scalable Game Servers \nIntroduction to 3D Streaming Technology in Massively Multiplayer Online Games \nIntroduction \nCraig Tiller and Adam Lake \nThe current generation of consoles all possess the capability to create a networked, \nmultiplayer experience. In effect, multiplayer networked gameplay has gone mainstream. \nOn the Xbox 360, there are 771,476 people playing Call of Duty: Modern Warfare 2 online \nthis very minute (4:37 p.m. on 11/28/2009). Several portable devices, such as the \nNintendo DS and Apple‘s iPhone, enable multiplayer networked experiences. Significant \nnumbers of men, women, and children now spend their entertainment dollars and time \nsocializing through online games. \nEach of the authors in this section addresses critical components in the networked \nmultiplayer architecture: security, scalability, social network harvesting, and streaming. \nFirst, in the gem ―Secure Channel Communication,‖ we discuss the issues related to creating \nand maintaining secure communication and the various attacks and responses posed in a \nnetworked gaming environment. Next, leveraging social network APIs to obtain player data \nis discussed in the gem ―Social Networks in Games: Playing with Your Facebook Friends.‖ \nThis allows a game developer, with the user‘s permission, to gain access to a player‘s \nFacebook friends list to be leveraged to create a multiplayer experience. The third gem, \n―Asynchronous I/O for Scalable Game Servers,‖ deals with the issues of scaling the I/O \nsystem architecture to handle the large number of requests generated in networked \nmultiplayer scenarios. Finally, the gem ―Introduction to 3D Streaming Technology in \nMassively Multiplayer Online Games,‖ was written by Kevin He at Blizzard and includes \nsource code to a terrain streaming application. This article is longer than a typical gem but \ncontains many details useful for those creating such a large system. \nIt is our hope that you will find these gems useful in your own applications and that you will \ncontribute your own innovations to this exciting and important area of game development. \n \n5.1. Secure Channel Communication \nChris Lomont \nchris@lomont.org \nThis gem is an overview of creating secure networking protocols. There is not enough space \nto detail all pieces, so instead a checklist of items is presented that covers necessary points. \nOnline games must prevent cheaters from using tools and hacks to their advantage, often \nto the detriment of other players‘ enjoyment. Cheating can be done through software add-\nons or changes, often making the cheater too powerful for other players or performing \ndenial-of-service attacks, making the game unresponsive for others‘ requests, such as gold, \nitems, services, and accounts. Since any code running on the client can be disassembled, \n\n\n \n \nstudied, and modified, security decisions must be made assuming the cheater has access to \ngame source code. Any system should be designed with the twin goals of making it difficult \nto cheat and making it easy to detect cheaters. \nThe main reason game and network security is often broken is that security designers must \nprotect against every possible avenue of attack, while a cheater only needs one hole for an \nexploit. This asymmetric warfare makes doing it right very hard and a continual arms race. \nThe goal of this gem is to supply a checklist of items to consider when designing and \nimplementing secure networking protocols. The ordering of topics is designed to be top \ndown, which is a good way to think through security design. Many related security features \nare mentioned, such as copy protection and code obfuscation, which, although not exactly \nnetworking, do play a role in an overall security of game networking by making it harder to \nchange assets. \nArchitecture \nThe most important decision when designing a gaming networking protocol is to decide how \nthe architecture is going to work before any programming is started. This architecture \nchoice has profound effects on later choices; making a significant change to the networking \ncomponent will have costly ripple effects throughout the rest of your game components. \nThree aspects of game engine design need up-front thought: multi-threading design, \nnetworking architecture, and code security. None of these can be bolted onto an existing \nengine without severe problems and bugs, resulting in poor quality for all components. So \nfix these three design decisions up front, document them as gospel, and build the rest of \nthe game around these choices. \nCode Security \nCode security is based on using secure coding practices. Without having this lowest layer \ndone well, it is impossible to get networking secure. Many games are written in C/C++. \nThree good references are [Seacord05, Howard03, and Graff03]. \nPeer to Peer \nThe main choice in architecture is whether to be peer to peer or client/server. In a peer-to-\npeer architecture, it is up to peers to detect cheaters, and of course such mechanisms can \nbe subverted by a cheating client. For example, if there is a ―feature‖ that allows a client to \nkick a suspected cheater off a network, then cheaters will subvert this to kick off legitimate \nplayers. In this case, a secure and anonymous voting protocol should be used, so numerous \nclients need to agree on a cheater before a ban occurs. \nClient/Server \nMost online games are a client/server architecture, where a central server is the authority \non game state, and clients send player input to and receive game state back from the \nserver. The main benefit of this architecture from a security point of view is the server can \nbe assumed to be a trusted, central authority on the game state. Unless your server is \ncompromised or there are fake servers for players to log onto, the server can be trusted to \ndetect cheaters and ban them and their accounts. \nThe Unreal Engine [Sweeny99] uses what Tim Sweeny calls generalized client-server, where \nthe server contains the definitive game state, and clients work on approximate and limited \nknowledge of the world. This information is synced at appropriate intervals. The limited \ngame state supports the security principle of least privilege, covered later. \nProtocol \n\n\n \n \nThe next big networking decision is selecting protocols to use and how to use them. A \ncommon tradeoff is between using slower TCP/IP for guaranteed delivery or faster UDP for \nspeed. Ensure selected security methods work with the protocol chosen. For example, if \nyour packets are encrypted in a manner requiring that all packets get delivered, then UDP \nwill cause you headaches. Good encryption needs an appropriate block-chaining method, as \ncovered later, but it will cause problems if some packets in a chain are not delivered. \nA recommendation is to use TCP/IP for login and authentication to guarantee \ncommunication, and then use UDP if needed for speed or bandwidth with symmetric key \nencryption during gameplay. \n \nAttacks \nThe level of attack sophistication against your game is directly proportional to popularity \nand longevity. Hence, more security is needed for a triple-A title than for a casual game. For \nexample, World of Warcraft (WoW) uses the Warden, a sophisticated kernel mode anti-\ncheat tool described in [Hoglund07, Messner09, and WikiWarden09]. \nReverse Engineering \nUsing tools such as IDA Pro and OllyDbg and a little skill, one can disassemble game \nexecutables into assembly code for any platform, and there are plug-ins that reverse the \ncode into C/C++ with good results. It only takes one cracker skilled in reverse engineering \nto remove or break weak security measures, and he then distributes the tool/crack to \neveryone. Assume crackers have access to your code and algorithms. \nKernel Mode \nKernel mode is the security layer that operating system code runs in (for the Windows PC \nand many other protected OSes), and most games run in user mode. WoW‘s Warden and \ncopy protection schemes such as SecuROM run as kernel mode processes, giving them \naccess to all processes and memory. However, even kernel mode software can be subverted \nby other kernel mode software. Using kernel mode tricks like those used in rootkits and \nother malware, a sophisticated cracker can run tools that hide under any snooping you \nmight do and can watch/record/modify run-time structures. This is done to circumvent \nWarden and many of the CD-ROM and DVD protection schemes. To detect and prevent \nkernel mode attacks on your code, you need kernel mode services, likely your own driver or \na commercial product, to do the work for you. \nLagging \nAlso known as tapping, this is when a player attaches a physical device called a lag switch \nto an Ethernet cable, slowing down communication to the server and slowing down the \ngame for all involved. However, the player with the lag switch can still run around and act, \nsending updates to the server. From the opponent‘s view, the player with the lag switch \nmay jump around, teleport, have super speed, and generally be able to kill opponents with \nease. \nIn peer-to-peer network architecture, this can also be achieved with packet flooding the \nopponents since each client sees other IP addresses. \nOther Attacks \n \nInternal misuse. A game must protect against employee cheating. For example, an \nonline poker site was implicated in using inside information for some individual to \n\n\n \n \nwin an online tournament using full knowledge of other player hands [Levitt07]. \nWhen these prizes reach tens or hundreds of thousands of dollars, there is a lot of \nincentive for employee cheating. \n \nClient hacking. Client hacks are changes made to a local executable, such as \nmaking wall textures transparent to see things a player should not (called \nwallhacking). \n \nPacket sniffing. To reverse network protocol, looking for weaknesses such as \nplayback attacks, DDoS, usernames/passwords, chat packet stealing. Some \nnetworks allow some users to see others‘ packets, such as colleges and others on \nthe same LAN, making packet sniffing attractive. \n \nBots. Numerous bots help by using auto aiming and auto firing and collecting goods \nsuch as gold and other items. Several years ago, this author created an (unnamed) \nword game-playing bot that was very successful, climbing to the top of the ranks \nduring the course of the experiment. \n \nAids. Aids assist a player, such as auto aiming, auto firing when aimed well, back-\ndoor communication, poker stats, poker playing games, and so on. Imagine how \neasy it is to cheat online for chess, checkers, Scrabble, and similar games where \ncomputers are better than humans. Other tools give multiple camera angles, better \ncamera angles, player highlighting, and so on. \n \nDesign flaws. Game design can be exploited. For example, on a game with scoring, \nit would be a design flaw if a player about to record a bad score can quit before the \nscore gets recorded and not be penalized. \n \nResponses \nIn response to all the attack methods, here are some methods to help defeat cheaters. \nCode Integrity \nFirst of all, write secure code. Then key code assets can be further hardened using methods \nfrom the malware community, such as code packing, encryption, and polymorphism. These \nall slow the code down but could still be used for infrequent actions, such as logging on or \nperiodic cheat detection. Further tools and methods along these lines should be researched \non the Internet, starting at sites such as www.openrce.com and www.rootkit.com. \nOne way to check code integrity is to integrate a small integrity scripting system and have \nthe server (or perhaps other clients in a peer-to-peer setting) send snippets of script code \nto execute. These snippets perform integrity checks such as hashing game assets, checking \ngame process memory for problems, and so on, returning the answer to the server for \nverification. The queries are generated randomly from a large space of possible code \nsnippets to send. To defeat this technique, a cheater has to answer each query correctly. \nThis requires keeping correct answers on hand, keeping a copy of modified game assets and \nremapping the scripting system, or something similar. Although doable, this adds another \nlevel of complexity for a cracker to work with since the script language is not built into \nexisting reversing tools. A variant of this takes it further and randomizes the language per \nrun and updates the client side as needed. \nA final method of code protection is integrated into commercial copy protection schemes, \nsuch as SecuROM, Steam, and PunkBuster. \nAn interesting attack on PunkBuster (which likely would work on other anti-cheat tools) was \nthe introduction of false positives getting players banned from games. The false positives \nwere caused by malicious users transmitting text fragments from known cheat programs \ninto popular IRC channels, and PunkBuster‘s aggressive memory scanning would see the \nfragments and [Punk08] ban players. This is likely to be fixed by the time this gem reaches \nprint. \n\n\n \n \nAgain, any code will eventually be reverse engineered given a determined cracker. A good \nplace to start reading is [Eliam05]. [Guilfanov09] shows some advanced code obfuscation \ntechniques from the creator of IDA Pro. \nKernel Mode Help \nAs mentioned earlier, this gives the highest level of computer control but also can crash a \ncomputer. An operating system may ban access to kernel mode code for gaming in the \nfuture, making kernel mode code a short-term solution. Kernel code mistakes often crash \nthe computer, not just the game process, so code must be extremely well tested before \nshipping. \nCheat Detection \nHaving the server detect cheaters through statistics is a powerful technique. Statistics from \nplayers should be kept and logged by username, including time online and game stats such \nas kills, deaths, scores, gold, character growth, kill rate, speed, and so on. An automated \nsystem or moderator should investigate any players with stats too many standard \ndeviations outside the norm. A rating system could be implemented behind the scenes like \nELO scores in chess, and players who suddenly show profound skills can be detected and \nwatched. \nContinued Vigilance \nEvery current solution requires vigilance from the game creators to patch games, update \ncheat lists, and evolve the game as cheats evolve. So far there is no one-shot method for \npreventing cheating in online games. However, following all the advice and reading deeper \ninto each topic will make protecting the game much easier by making cheats much harder \nto implement. Game creators should monitor common cheat sites, such as \nwww.gamexploits.com, and per-game forums looking for cheats and techniques. \nBackups/Restore \nTo prevent worst-case damage to a game environment, have a regularly scheduled backup \nin case of server hacking. \n \nDisciplinary Measures \nWhen a cheater is caught, the game has to have a well-defined punishment system. Most \ngames and anti-cheat systems currently ban accounts either temporarily, permanently, or \nwith some resolution process. \n \nExamples \nHere are two examples of current online game security features. \nWoW \nWorld of Warcraft uses a module called the Warden to ensure client integrity. From \n[Hoglund07, Messner09, and WikiWarden09], the following is found: \n \nIt checks the system once about every 15 seconds. \n\n\n \n \n \nIt dumps all DLLs to see what is running. \n \nIt reads the text of all Windows title bars. \n \nThe DLL names and title bars are hashed and compared to banned item hashes. \n \nIt hashes 10 to 20 bytes for each running process and compares these to known \ncheat program hashes, such as WoW Glider. \n \nIt looks for API hooks. \n \nIt looks for exploitative model edits. \n \nIt looks for known cheating drivers and rootkits. \nUnreal Tournament \nSweeny [Sweeny99] lists the following network cheats that have been seen in Unreal \nTournament: \n \nSpeedhack. Exploits the client‘s clock for movement updates. Fixed by verifying \nclient and server clock stay nearly synced. \n \nAimbots. UnrealScript and external versions. \n \nWall hacks and radars. UnrealScript and external versions. \n \nConclusion \nTo develop a secure networking protocol for gaming, securing all game assets from code to \nart and networking data is important. Performance versus security tradeoffs must be \ndesigned into encryption and message protocols from the beginning. \nSecuring an online game is a constantly evolving war, and whatever methods are used \ntoday may fail tomorrow. Developers must constantly monitor the servers and communities \nto detect, mitigate, and prevent cheating. This involves tools to update clients, protocols, \nservers, and assets as needed to provide an enjoyable, level playing field for all customers. \nFinally, throughout the game development process, keep a list of security checkpoints and \nfollow them religiously. \n \nReferences \n[Eliam05] Eliam, Eldad. Reversing: Secrets of Reverse Engineering. Wiley, 2005. \n[Ferguson03] Ferguson, Neils, and Bruce Schneier. Practical Cryptography. Wiley, 2003. \n[Graff03] Graff, Mark, and Kenneth Van Wyk. Secure Coding: Principles and Practices. \nO‘Reilly Media, 2003. \n[Guilfanov09] Guilfanov, Ilfak. ―IDA and Obfuscated Code.‖ 2009. Hex-Rays. n.d. \n<http://www.hex-rays.com/idapro/ppt/caro_obfuscation.ppt>. \n[Hoglund07] Hoglund, Greg. ―4.5 Million Copies of EULA-Compliant Spyware.‖ 2009. \nRootkit. n.d. <http://www.rootkit.com/blog.php?newsid=358>. \n[Howard03] Howard, Michael, and David LeBlanc. Writing Secure Code, 2nd Edition. \nMicrosoft Press, 2003. \n\n\n \n \n[Levitt07] Levitt, Steven D. ―The Absolute Poker Cheating Scandal Blown Wide Open.‖ 2007. \nThe New York Times. n.d. <http://freakonomics.blogs.nytimes.com/2007/10/17/the-\nabsolute-poker-cheating-scandal-blown-wide-open/>. \n[Messner09] Messner, James. ―Under the Surface of Azeroth: A Network Baseline and \nSecurity Analysis of Blizzard‘s World of Warcraft.‖ 2009. Network Uptime. n.d. \n<http://www.networkuptime.com/wow/>. \n[Punk08] ―netCoders vs. PunkBuster.‖ 26 March 2008. Bashandslash.com. n.d. \n<http://bashandslash.com/index.php?Itemid=78&id=297&option=com_content&task=view\n>. \n[Schneier96] Schneier, Bruce. Applied Cryptography: Protocols, Algorithms, and Source \nCode in C, 2nd Edition. Wiley, 1996. \n[Seacord05] Seacord, Robert. Secure Coding in C and C++. Addison-Wesley, 2005. \n[Sweeny99] Sweeny, Tim. ―Unreal Networking Architecture.‖ 2009. Epic Games, Inc. n.d. \n<http://udn.epicgames.com/Three/NetworkingOverview.html>. \n[Watte08] Watte, Jon. ―Authentication for Online Games.‖ Games Programming Gems 7 \nBoston: Charles River Media, 2008. \n[WikiCipher09] ―Block Cipher Modes of Operation.‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation>. \n[WikiWarden09] ―Warden (software).‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Warden_(software)>. \n \n5.2. Social Networks in Games: Playing with Your Facebook Friends \nClaus Höfele, Team Bondi \nclaus@claushoefele.com \nWhile multiplayer features are now commonplace, games often pit anonymous Internet \nusers against each other. This is a step backward from the enjoyment of playing split-screen \ngames with a friend sitting right next to you. In order to re-create this friendly atmosphere \nin online games, developers have to understand more about a player‘s ties with people, \nwhich is the domain of social networks such as Face-book, MySpace, and Twitter. \nThis gem describes how to access the web services of social networks from your game. As \nan example of how this might be put to use, the application developed in this gem will \ndemonstrate how your game can get access to a player‘s friends on Facebook. \nThe explanations in this gem describe Facebook integration from the point of view of a \nstandalone, desktop-style game as opposed to a game executed in a web browser. \nStandalone applications pose unique challenges because web services are primarily \ndesigned to be good web citizens but do not necessarily integrate well with desktop \napplications. \nRESTful Web Services \n\n\n \n \nRepresentational State Transfer (REST) is the predominant architecture for offering \nprogrammatic access to data stored on the web. \nA RESTful service is composed of a collection of resources, which are identified by a web \naddress, such as http://example.com/resource. Clients gain access to data through a set of \nwell-defined operations that can be used on these resources. Because RESTful services are \nbased on stateless operations (any state information is held in the client), a service can \nscale to a large number of clients—ideal for web services, which might have millions of \naccesses each day. \nREST does not demand any specific technologies in its implementation, which means every \nweb service has a similar but slightly different way of offering access to its resources. Also, \nweb services comply with pure RESTful design principles to varying degrees. \nIn practice, a RESTful service means that you‘ll send HTTP requests to send and receive \ndata. The most common HTTP operations are HTTP GET to retrieve data and HTTP POST \nto create new data on the server. \nRequesting Data \nAs an example of accessing data from a social network, consider the following request that \nuses Twitter‘s RESTful API [Twitter09]: \ncurl http://search.twitter.com/trends/current.json \n \ncURL [cURL09] is a tool that allows you to issue network requests on the command line. The \nprevious example sends an HTTP GET request to Twitter‘s servers to retrieve the most \npopular topics currently being discussed on Twitter. \nIn this simple example, you could have pasted the web address mentioned in the cURL \ncommand into the address field of your web browser. Because a web browser issues HTTP \nGET requests by default, you would have achieved the same result. When developing \naccess to web services, however, it‘s a good idea to learn how to use cURL because it has \nmany options that allow you to assemble more complex requests. For example, cURL also \nallows you to send HTTP POST requests and use HTTP‘s basic access authentication \nscheme. \nThe cURL command presented previously will result in a response similar to the following \noutput (formatted for easier reading): \n{\"trends\":{\"2009-08-23 04:00:47\":[ \n  {\"query\":\"\\\"Best love song?\\\"\",\"name\":\"Best love song?\"}, \n  {\"query\":\"#fact\",\"name\":\"#fact\"}, \n  {\"query\":\"#shoutout\",\"name\":\"#shoutout\"}, \n  {\"query\":\"#HappyBDayHowieD\",\"name\":\"#HappyBDayHowieD\"}, \n  {\"query\":\"\\\"District 9\\\"\",\"name\":\"District 9\"}, \n  {\"query\":\"\\\"Inglourious Basterds\\\"\",\"name\":\"Inglourious \nBasterds\"}, \n  {\"query\":\"\\\"Hurricane Bill\\\"\",\"name\":\"Hurricane Bill\"}, \n  {\"query\":\"#peacebetweenjbfans\",\"name\":\"#peacebetweenjbfans\"}, \n  {\"query\":\"#Nascar\",\"name\":\"#Nascar\"}, \n  {\"query\":\"Raiders\",\"name\":\"Raiders\"} \n]},\"as_of\":1251000047} \n \n\n\n \n \nHere, the output from the Twitter API is in JavaScript Object Notation (JSON) format \n[JSON09]. JSON is a lightweight data format that is becoming popular because it is less \nverbose and easier to parse than XML. \nDepending on the request, Twitter—like most web services—can be configured to produce \neither XML- or JSON-formatted output. I find that my applications often need an XML parser \nanyway because of other application requirements. For this reason, I tend to use XML more \noften because it is convenient to have a single data format in your application. \nA quick glance at the JSON data should give you a good idea of the information returned in \nthe request: Each line that starts with the word ―query‖ contains the name of a trending \ntopic as well as the search query that can be used to find all Twitter messages relating to \nthis topic. \nAuthenticating a User \nThe data received from the previous example represents public information that everyone \nhas access to. To gain access to private data, such as contact details and friends, you have \nto confirm a user‘s identity. \nPeople are understandably cautious to give applications access to their private data. For this \nreason, social networks have developed a variety of authentication mechanisms to cope \nwith different situations and technical limitations. Because these mechanisms vary wildly \nfrom service to service, authentication is often the most time-consuming request to \nimplement. \nThe most basic authentication mechanism requires users to enter a user name and \npassword, which your application sends to the web service. Entering authentication data \ninto your application requires users to trust your application not to collect passwords and \nabuse them for other purposes. This fear might stop users from trying out new applications \nbecause they don‘t want to risk their accounts being hijacked by malicious applications. \nApplications on the web have answered this need by offering authentication mechanisms \nbased on forwarding. The basic principle is that when logging in to a website, you are \nforwarded to the login page of the account provider and enter your user name and \npassword there. The application will never see your credentials, but will only receive a \nconfirmation of whether the login was successful. \nIdentifying Your Application \nApart from authenticating the user on whose behalf your application signs in to the service, \nmost websites also require a unique identifier that represents your application. Facebook, \nfor example, requires this. Twitter, on the other hand, doesn‘t use an application identifier. \nApplication identifiers allow for application-specific configurations on the service provider‘s \nwebsite but are also used to enforce service agreements between the developer and the \nservice provider. A social network might, for example, restrict what you are allowed to do \nwith the data received from the network. The service provider can choose to disable your \napplication if you violate the service agreement. \nDebugging RESTful Requests \nWhen developing my applications, I find it useful to see the data that is sent and received in \nthe requests to a web service. There are a number of good HTTP debug proxies [Fiddler09, \nCharles09] that act as middlemen between your application and a website. They often \ncontain special support to display and format XML and JSON data. \n\n\n \n \nHTTP proxies require a system-specific configuration so that the debugged application uses \nthe proxy instead of accessing the Internet directly. \nFor example: \ncurl —proxy localhost:8080 \nhttp://search.twitter.com/trends/current.json \n \nwill send the Twitter request from the previous example to a proxy on the local machine \n(localhost) using port 8080. An HTTP proxy installed on this port will then forward the \nrequest to the real server at search.twitter.com and record all data that goes back and forth \nbetween your computer and Twitter‘s server. \nAnother possibility is a network protocol analyzer, such as Wireshark [Wireshark09]. \nNetwork protocol analyzers work by listening to network packets going through your \nnetwork adapter. Because Wireshark works on a lower level than HTTP proxies, the \napplication is not aware of the fact that it is being debugged and thus doesn‘t need to \nchange its configuration. This is a more generic solution to monitor network traffic, but \nHTTP proxies are often easier to use because they specialize in HTTP traffic and \nautomatically filter out unnecessary information. \n \nThe Facebook API \nAs an example of how to integrate social networks into your game, this gem demonstrates \nhow to use Facebook‘s REST interfaces. \nSetting Up a Facebook Application \nBefore starting with your Facebook application, you have to register as a developer with \nFacebook. You do this by creating a Facebook user account and adding the Facebook \nDeveloper Application to your profile [Facebook09]. \nWithin the Developer Application, you‘ll find a link to set up your own Facebook application. \nFinishing this process will give you an API key that identifies your application when \nexchanging data with Facebook and a configuration page that contains your application‘s \nsetup. \nOne parameter you have to configure is the Canvas Callback URL, which determines from \nwhere Facebook pulls the content of your application if you were to display a page within \nFacebook. Since the demo application described in this gem is a desktop application, this \nURL is not used at all, but it is required nevertheless. \nMore importantly, you have to switch the Application Type from Web to Desktop. This \nchanges the authentication process when accessing Facebook‘s REST server to better suit \ndesktop applications. \nFacebook’s REST Server \nFacebook runs a RESTful service at the URL http://api.facebook.com/restserver.php. In \norder to exchange data with this server, you have to send an HTTP POST request with at \nleast the following parameters: \n\n\n \n \n \napi_key. This is the API key you get when registering your application with \nFacebook. \n \ncall_id. A number that increases with every request. \n \nsession_key. The session key obtained from the login process or empty if the \nrequest doesn‘t require a session. \n \nmethod. The API endpoint name that identifies the request. \n \nv. A version identifier, currently 1.0. \nSome requests have additional parameters, which are then appended to this list. \nFor Facebook‘s server to accept a request, you also have to send a signature that identifies \nyour application. To create the signature, you concatenate all input parameters to a string, \nappend a secret key, and build an MD5 hash out of this data. Since both Facebook and your \napplication know the secret key, Facebook‘s server can create the same signature and check \nthat the request is indeed coming from your application. \nThe secret key that‘s used for the signature is obtained by establishing a session. The secret \nis then called a session secret. Requests that are sent without a session context use the \napplication secret that you can look up in your application‘s configuration page on Facebook. \nThe handling of the secret key depends on the way you authenticate the user, so I‘ll have to \ntalk a bit more about authentication methods first. \nAuthenticating Facebook Users \nAs part of their terms of service—which must be agreed to in order to get an API key—\nFacebook forbids you to receive user names and passwords directly in your applications. \nInstead, users have to go through Facebook‘s website to log in. The reasoning is that users \nare more likely to trust the application because the same login screen is used that people \nare already familiar with from logging into Facebook on the web. In addition, it makes it less \nlikely, but not impossible, that applications will capture and hijack the user‘s password \nbecause the credentials are entered into a separate application (the browser). \nObviously, displaying a website for login purposes is easy for web applications. For desktop \napplications, on the other hand, you essentially have two choices: You can use the browser \nthat‘s installed on the user‘s system, or you can integrate a web browser, such as WebKit, \ninto your application. \nAuthentication with an External Browser \nLoading Facebook‘s login page in a browser separate from your application means that the \nuser has to leave your application until the Facebook login is complete. After the login, the \nuser returns to your application and confirms the authentication. Figure 5.2.1 illustrates this \nprocess. \nFigure 5.2.1. Authenticating a Facebook user through an external web browser. \n\n\n \n \n \n \nTo start with, your application has to request an authentication token from Face-book. (The \nmethod name of this request is auth.createToken.) Since you haven‘t established a \nsession yet, you use the application secret to sign this request. \nNext, you launch the external browser with a login page hosted by Facebook and pass the \ntoken to this website as part of the URL. The user can now log in to Facebook to establish a \nsession for your application. \nFinally, the user returns to your application and confirms the login process, whereupon your \napplication sends an auth.getSession request to Facebook. If the login was successful, \nyou will get a session key and a secret. The session key has to be used as part of the input \nparameters, and the session secret replaces the application secret in subsequent requests. \nThe demo application that comes on the CD for this gem contains an implementation of this \nlogin process, so you can see exactly what data needs to be sent to Facebook. \nAuthentication with an Application-Integrated Browser \nYou can achieve a better user experience by integrating a browser into your application. \nAgain, you have to load Facebook‘s login page to start the process. But this time, the web \npage is displayed as part of your application. Figure 5.2.2 shows the application flow. \nFigure 5.2.2. Authenticating a Facebook user through an application-integrated \nweb browser. \n\n\n \n \n \n \nWhen loading Facebook‘s login page, you pass in a parameter to configure a URL that gets \ndisplayed when the login is successful. This way, your application can figure out whether the \nuser has successfully logged in to Facebook by checking the URL that‘s currently being \ndisplayed in the browser. \nThis tight integration is only possible if you have complete control over the browser, for \nexample, by embedding WebKit [WebKit09] into your application. WebKit is an open-source \nweb browser layout engine that‘s also used as the basis of Apple‘s Safari and Google‘s \nChrome browsers. \nInstead of using WebKit directly, the demo for this gem uses the WebKit version that comes \nwith the Qt application framework [Qt09]. This makes it easy to display the browser \ncomponent as part of a dialog. For games, however, it might be better to render a web \npage‘s content to an image, which could then be displayed as a texture. (Have a look at the \nQWebFrame::render() API.) \nBecause the process starts off with Facebook‘s website when using an integrated browser, \nyour application never needs the application secret. This is a big advantage compared to the \nauthentication process with an external browser because it means the application secret can \nnever be compromised. \nPersisting a User Session \nBy default, a session is valid only for a limited time. To avoid going though the \nauthentication procedure every time the session expires, you can ask the user for offline \naccess. This permission can be requested during the login process by appending a \nparameter to the login URL (authentication with integrated browser only) or by displaying a \nwebsite with a permission form hosted by Facebook (works with both integrated and \nexternal browser authentication). \nTo skip the authentication, you store the session key and secret on the user‘s computer and \nuse these two values again the next time your application needs it. You have to check that \nthe permission is still valid because the user can revoke the authorization on Facebook‘s \nwebsite at any time. The demo application on the CD does this by sending a confirmation \nrequest to Facebook every time you start the application. \nApart from the session information, you should avoid storing data locally because it might \nbecome out of sync with the information on Facebook‘s website. \n\n\n \n \nRetrieving the Friends List \nOnce you have obtained a session, getting information about the user‘s friends is \nstraightforward: You send a request with the name friends.get, which will return a list \nof user IDs. Alternatively, you can query only those friends that already have used your \napplication by using friends.getAppUsers. \nYou could match these IDs to a database of high scores, for example, to realize a high score \ntable that only contains friends of the user. Also, if a friend hasn‘t played the game yet, \nyour application could send out invitations to try out the game. \nPosting Messages \nAnother often-used request is stream.publish, which posts a message to the user‘s \nFacebook page. Similar to the offline access, this requires that the user grant special \npermission to your application. \nPublishing messages could be used to post status updates about the user‘s progress in your \ngame. \n \nConclusion \nIn this gem, I have shown you how Facebook can provide social context to your game. By \nintegrating a player‘s friends network, you can make your games a more personal \nexperience, which avoids the anonymity often associated with multiplayer games played \nover the Internet. \nWhile the majority of this gem has focused on Facebook, the information in this article \nshould provide enough information to extend your games with features from other web \nservices. Here are some ideas you might want to try: \n \nRecord a player‘s gameplay as a video and upload it to YouTube so it can be viewed \nby others. \n \nSend status updates to Twitter when someone achieves a new high score. \n \nSend messages to a player‘s Facebook friends to invite them for a game. \n \nRecord the location of the player and create high score lists of people in your \nneighborhood. \n \nReferences \n[Charles09] ―Charles: Web Debugging Proxy Application.‖ n.d. Karl von Randow. n.d. \n<http://www.charlesproxy.com/>. \n[cURL09] ―cURL.‖ n.d. <http://curl.haxx.se/>. \n[Facebook09] Website of Facebook‘s developer application. \n<http://www.facebook.com/developers>. \n[Fiddler09] ―Fiddler Web Debugging Proxy.‖ n.d. Microsoft. n.d. \n<http://www.fiddler2.com/>. \n[JSON09] ―Introducing JSON.‖ n.d. JSON. n.d. <http://json.org/>. \n",
      "page_number": 434
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 448-460)",
      "start_page": 448,
      "end_page": 460,
      "detection_method": "topic_boundary",
      "content": " \n \n[Qt09] ―Qt Cross-Platform Application and UI Framework.‖ n.d. Nokia Corporation. n.d. \n<http://qt.nokia.com/>. \n[Twitter09] ―Twitter API Documentation.‖ n.d. Twitter. n.d. <http://apiwiki.twitter.com/>. \n[WebKit09] ―The WebKit Open Source Project.‖ n.d. WebKit. n.d. <http://webkit.org/>. \n[Wireshark09] ―Wireshark.‖ n.d. Wirehsark. n.d. <http://www.wireshark.org/>. \n \n5.3. Asynchronous I/O for Scalable Game Servers \nNeil Gower \nneilg@vertexblast.com \nScalability is a critical concern in today‘s online gaming market. The small-scale networking \nof 8- to 32-player LAN games has given way to massively multiplayer Internet games and \ncentralized game-hosting services. Even if a single game instance supports only a small \nnumber of players, the ability to run additional game instances on a single server has \nserious repercussions for the cost of operating a game service. Every physical server incurs \nongoing space, power, and cooling costs on top of the initial hardware purchases and \nsoftware license fees. \nThis gem explores asynchronous I/O as a technique for improving the scalability of \nmultiplayer game servers. We first review traditional synchronous I/O techniques and their \nimplications for server architecture. Then we take a look at the asynchronous alternatives \nfor Windows and POSIX and demonstrate their use in a sample game server. We conclude \nwith an analysis of asynchronous I/O and its applicability to building scalable game servers. \nBackground \nInput/output (I/O) operations have always posed a challenge to game developers and to \nnetwork programmers in particular. I/O operations are often among the most time \nconsuming and unpredictable functions that game code will use. In part, this is because the \nactual I/O systems reside deep in the operating system. Making system calls and crossing \nthe user-space to kernel-space boundary takes time, both in terms of CPU state changes \nand in terms of data transfers between user space and kernel space. Furthermore, I/O \noperations deal with physical devices that don‘t always behave the way they should in \nprinciple. Physical devices such as DVD drives have to cope with things like discs covered in \nreal-world fingerprints and scratches, and network cards have to contend with the real-\nworld tangle of wires and devices that makes up our LANs and the Internet. \nSome of this chaos is hidden from our code by OS buffers. For example, when we call a \nwrite function, the data is rarely written directly to the device. Instead, the OS places the \ndata in an internal buffer, which it will then use to write to the physical device when it is \nready. While this is usually effective at protecting user code from the underlying complexity \nof the device, buffers still get full under heavy loads and traffic spikes. When that occurs, \nthe real world ripples back up to the application code in the form of failed operations and \nunexpected delays. \nAside from the impact of these delays on the performance of our game code, the other \nproblem we encounter with I/O operations is that our process sits idle while synchronous \nI/O operations execute. When there is plenty of processing left to do, we can‘t afford to \nwaste CPU cycles doing nothing. \n\n\n \n \nBlocking and Non-Blocking Synchronous I/O \nMost standard I/O APIs operate synchronously. They keep us synchronized with the I/O \nsystem by blocking further execution of our code until the current I/O operation is \ncomplete. This is the model you‘ll encounter when using functions such as fread() and \nfwrite(), or send() and recv() for sockets. For applications whose main function is \nI/O, such as an FTP client, these APIs can be perfectly adequate. If there‘s nothing else for \nthe app to do, blocking on I/O may be fine. \nHowever, for real-time games, we generally have a game-world simulation running at 30 to \n60 updates per second. Some game architectures may be more event-driven on the server \nside, but to maintain responsiveness within any architecture, we can‘t waste time waiting on \nI/O. The biggest delay we want to avoid is making blocking I/O requests when the OS is not \nready to handle them. For example, this occurs if we call recv() when there is nothing to \nread or send() when the OS‘s send buffer is full. \nOne way to address this is the socket API‘s non-blocking mode. However non-blocking \nsockets are not quite as great as they sound. In non-blocking mode, when the operation is \nnot possible (for example, the send buffer is full), the function will return immediately with \nan error code telling us to try again later. This ―try again later‖ cycle is polling, and we have \nto be careful that it doesn‘t get out of hand. It is easy with non-blocking sockets to waste \nmany CPU cycles polling for I/O readiness. Non-blocking sockets help us avoid our initial \nobstacle of getting hung up on I/O calls when the OS is not ready, but they don‘t \nnecessarily improve the execution efficiency of our program overall. \nHandling Multiple Clients \nA game server must handle multiple connections, and a scalable server must handle many \nconnections. One simple server design is to iterate over the set of all client connections and \nperform non-blocking I/O operations for each client in turn. However, this can involve a lot \nof polling, especially when the majority of the connections are idle, as is often the case \nwhen reading data from clients. As the number of connections increases, so does the length \nof the iterations, which in turn degrades the server‘s responsiveness to each client. \nRather than polling one socket at a time, an alternative approach is to use an I/O \nmultiplexer, such as select() or WaitForMultipleObjects() (or one of the many \nplatform-specific variants of these functions). This allows our code to block on a whole set \nof sockets and unblock with a set of sockets ready for I/O, which we can then process \nsequentially. \nThis addresses two issues. First, we don‘t have to worry about getting blocked on sockets \nthat aren‘t ready, because in principle the multiplexer only returns ready sockets—although \nin practice a socket‘s state can still change between when the multiplexer returns and our \ncode performs I/O with it. Second, the OS monitors the whole set of sockets for readiness, \nso we don‘t have to explicitly iterate over them in our code, making numerous system calls \non sockets whose state hasn‘t changed since the last iteration. \nHowever, we can‘t block the main game loop on a multiplexer call, because even with a long \nlist of clients, it‘s still impossible to know how long we‘ll have to wait. Putting a timeout on \nthe call is one way around this, but now we‘re turning the I/O multiplexer into a (potentially \nvery) expensive polling system. We need to decouple the I/O processing from the rest of \nthe code. \nThread Carefully \n\n\n \n \nTo allow the main game loop to execute while I/O operations are being processed and to \ntake advantage of the OS‘s ability to handle many parallel I/O streams, we could introduce \nadditional threads to the server. \nThe obvious one-thread-per-client approach, while relatively easy to implement, scales very \npoorly. This is due to several factors. For one, the cost of synchronization primitives such as \nmutexes generally scales relative to the number of threads involved. Threads also consume \nlimited OS resources and waste cycles as they get swapped in and out of execution focus. \nThe ideal number of threads to minimize context switching is proportional to the number of \ninstruction streams the CPU can process in parallel. On present-day hardware, this means \nwe should really only have a small number of threads, so for any significant number of \nclients it is simply not an option to spawn a thread for each. \nA more practical approach is to dedicate one thread (or a pool of threads) to I/O operations, \nallowing the main thread to proceed while the new thread deals with the I/O operations. \nBy introducing more than one thread into our application, we have also introduced \nsynchronization overhead (mutexes, critical sections, and so on) and some overhead for the \nthread-safe run-time environment and libraries. On the upside, this approach also has a \nwell-defined interface point between threads—the queues used to store I/O requests and \nresults. The synchronization code still requires some careful programming to avoid deadlock \nand race conditions, but this is a textbook application of multi-threading. \nFigure 5.3.1. (a) Thread per-client versus (b) I/O thread with multiplexing. \n \n \nThe solution we‘ve devised so far is very common in network server implementations. It \nessentially emulates asynchronous I/O—the main thread (or threads) submit I/O requests \nto a queue for the I/O thread, which processes the requests and then notifies the callers of \nthe results via an event queue or similar mechanism. \n\n\n \n \nAsynchronous I/O uses native OS services to deliver similar functionality without crossing \nthe system call boundary as often and without introducing additional I/O threads. It also \nenables the OS to take maximum advantage of its own internal scheduling systems to \noptimize I/O. \n \nAsynchronous I/O APIs \nThe two main APIs for asynchronous I/O are Windows Overlapped I/O and the AIO API in \nthe POSIX real-time extensions. Overlapped I/O is supported on Windows 2000 and later. \nPOSIX AIO is available on most UNIX-like operating systems, though the capabilities of the \nimplementations can vary considerably, so look before you leap. \nWorking with either the Windows or the POSIX API is quite similar. AIO requests are \nassociated with a control struct when they are made, which the OS uses to track the \nrequest and return the results to our code. These structs contain hidden internal fields for \nthe OS, so they have to be explicitly zeroed out before each request. Once a request has \nbeen made, the struct is off limits to our code until we have been notified that the operation \nis complete. It is important to realize that the data you pass into the asynchronous API \nmust be valid for at least as long as the asynchronous operation and also must be \nexclusively accessible to the OS during that time. In particular, this means we can‘t pass \nlocal variables into AIO calls, and we can‘t read from or write to the buffers until the \noperation completes. \nAfter the AIO operation has been initiated, there are several ways to get completion \nnotifications. The simplest option is usually to use a callback function. Both POSIX and the \nWindows API provide mechanisms to include an application-defined pointer in the callback‘s \ncontext. This pointer can be used to refer to our own data structures (such as the object \nthat initiated the request), containing any additional information required from the game \ncode‘s perspective to handle the completion event. \nAs you would expect, there are also functions for cancelling active I/O operations and for \nquerying their current status. One thing to watch out for when cancelling asynchronous \noperations is that they are not guaranteed to be done when the cancel function returns—the \ncancel operation itself is asynchronous! This means we have to be a little bit careful in the \nshutdown process of our game, so as not to free any control structs or buffers in use by \nactive operations. \nImplementation \nThe code accompanying this gem on the CD-ROM includes a sample asynchronous game \nserver implemented with both POSIX AIO and Windows Overlapped I/O. The code models a \nsimple game server that runs multiple GameInstance objects, each with a collection of \nSocket objects for the clients connected to that instance. The Socket class is a wrapper \naround a SocketImpl object, which provides access to the platform‘s socket API. \nFor portability, the server uses synchronous I/O to accept incoming connections. Once the \npreconfigured number of connections is made, the server begins running each session‘s \nmain game loop. \nIn GameInstance::tick(), the server looks for input from the clients, as seen in Figure \n5.3.2. Using an asynchronous read call, tick() keeps a read operation open for each \nclient. The SocketImpl code has some logic in it to ignore new read requests if one is \nalready in progress. This is an improvement over a non-blocking read, because the check \nlooks at a simple Boolean flag rather than making any calls into the actual I/O system. \n\n\n \n \nFigure 5.3.2. Asynchronous I/O server overview. \n \n \nThe socket code also contains logic for the case when we send or receive less than the \nexpected amount of data in a single I/O call. The SocketImpl initiates additional \nasynchronous reads and writes until the expected data arrives. The synchronous \nimplementation of this functionality requires polling and some additional bookkeeping code. \nAfter the game sessions are updated, the server calls GameInstance::update-\nClients() on up to one game instance per iteration. This models game instances that \nsend regular state updates to the clients at a rate that is less than the game‘s main loop \nfrequency. By updating sessions in a round-robin fashion, the load is spread out to avoid \nI/O spikes caused by all of the sessions updating all of their clients at once. \nAs mentioned earlier, we have to be careful to make sure that the AIO control structs are \nvalid for the entire duration of the I/O operations. In the sample code, the ReadRequest \nand WriteRequest structs are used for this purpose. They contain the control struct, \nbuffers, and other I/O request-related information and are stored in the SocketImpl \ninstance, so they will be valid at least as long as the socket handle. \nSocketImpl::close() contains logic for ensuring that outstanding operations are \ncomplete before the socket is destroyed. \n \nResults and Analysis \nRunning the sample server on a supported platform, such as Windows XP or Open-Solaris, \nwe find that as the amount of ―would block‖ time on the sockets increases, the efficiency (in \nterms of time spent executing application code) of asynchronous I/O over synchronous \n\n\n \n \nsockets grows. Small transfers, combined with clients that read and write continuously as \nfast as the server, give synchronous I/O an advantage. However, these are not conditions \nthat are found in real-world applications. There will always be times when clients have no \ndata to send or are not ready to receive more data from the server, and this is where \nasynchronous I/O excels. \nTo summarize, the main advantages of asynchronous I/O for game server design are: \n \nIt eliminates idleness due to blocking. \n \nIt eliminates system call overhead due to polling. \n \nIt eliminates the need for multi-threading to handle I/O. \n \nIt leverages the OS for tricky subsystems that handle concurrent I/O processing and \nnotification dispatching. \n \nIt creates opportunities for internal I/O optimizations in the OS kernel. \nThe main disadvantages of asynchronous I/O are: \n \nIt has greater overhead per system call. \n \nI/O-related code may be harder to understand and debug. \n \nAsynchronous I/O capabilities can vary across platforms. \nThe greater overhead of asynchronous system calls is a consequence of their more complex \nfunctionality. In addition to the functionality equivalent to their synchronous peers, they \nmust also register the operations for asynchronous processing and configure the pending \nnotifications. Due to this overhead, it is best to avoid many small requests and make fewer, \nlarger I/O requests when working with asynchronous I/O. \nAsynchronous I/O code can be more difficult to understand, particularly when the \nunderlying logic is naturally I/O driven. For example, consider a protocol exchange like the \nfollowing pseudocode: \nrecv( playerName ) \nsend( loginChallenge ) \nrecv( loginResponse ) \nif ( loginResponse is valid ) startGameLoop() \nelse close() \n \nImplemented using synchronous I/O, the code for this exchange could read almost exactly \nlike the pseudocode (with suitable error checking added, of course). However, as shown in \nFigure 5.3.3, when implemented using asynchronous I/O, it is necessary to store various \npieces of state information so that we can resume the process at the appropriate stage after \neach operation completes. \nFigure 5.3.3. A simple network protocol state machine. \n\n\n \n \n \n \nWhether we represent this information with ad hoc flags and status variables or with a \nformal state machine, we now need a state machine to manage the protocol, instead of \nhaving the protocol state maintained implicitly by the game‘s execution stack. How \napplicable this is to real-world game servers is highly design-dependent. In the case of the \nmodel server code, asynchronous I/O actually simplifies the implementation, because the \nmain protocol between the clients and the server is stateless. \nAsynchronous code is often more difficult to debug because of the flow issues just described \nand because of the non-deterministic behavior of the I/O notifications. On the other hand, a \nmulti-threaded server will contain comparable levels of complexity in both the code and \ndebugging techniques required. Which approach is more difficult to work with may be \nlargely a matter of preference. \nThe variability in platforms‘ asynchronous I/O support is a concern for the long-term \nmaintainability of a code base. If we want to port our game server to a new platform, we \nmay get markedly different results, or we may be stopped altogether by lack of support for \nasynchronous I/O. Subtle differences in behavior can also complicate porting efforts. \n \nConclusion \nAsynchronous I/O is a tool for network programmers to use to improve scalability. It is best \nplanned for from the outset when building a game server, but some server architectures \nemulate similar functionality already using threads, so asynchronous I/O can be a \nreasonable retrofit in some code bases. The sample code included on the CD-ROM can serve \nas a starting point for evaluating the applicability of asynchronous I/O to your project. \nWhether we choose to use it for network I/O or even for other server I/O tasks, such as \nwriting log files, asynchronous I/O can offer significant benefits to the scalability of our \nservers. By adopting this approach, we can also hope to see asynchronous I/O \nimplementations mature on game server platforms and someday perhaps become the de \nfacto standard for network server programming. \n\n\n \n \n \nReferences \n[Schmidt00] Schmidt, D., M. Stal, H. Rohnert, and F. Buschmann. Pattern-Oriented \nSoftware Architecture, Volume 2: Patterns for Concurrent and Networked Objects. John \nWiley & Sons, Ltd, 2000. \n[Schmidt02] Schmidt, D., and S. Huston. C++ Network Programming, Volume 1: Mastering \nComplexity with ACE and Patterns. Addison-Wesley, 2002. \n[Schmidt03] Schimdt, D., and S. Huston. C++ Network Programming, Volume 2: \nSystematic Reuse with ACE and Frameworks. Addison-Wesley, 2003. \n[Silberschatz02] Silberschatz, A., P. B. Galvin, and G. Gagne. Operating System Concepts. \nJohn Wiley and Sons, Inc., 2002. \n[Wright94] Wright, G., and R. Stevens. TCP/IP Illustrated, Volume 2: The Implementation. \nAddison-Wesley, 1994. \n \n5.4. Introduction to 3D Streaming Technology in Massively \nMultiplayer Online Games \nKevin Kaichuan He \nkhe@blizzard.com \nMassively multiplayer online games (MMOGs) have become very popular all over the world. \nWith millions of players and tens of gigabytes of game content, popular MMOGs such as \nWorld of Warcraft face challenges when trying to satisfy players‘ increasing demand for \ncontent. Delivering content efficiently and economically will have more and more impact on \nan MMO game‘s success. Today, game content is distributed primarily through retail DVDs \nor downloads, which are expensive and slow. \nIn the future, it should be possible to deliver game content disc-free and wait-free through \nstreaming technology. Game streaming will deliver the game world incrementally and on \ndemand to players. Any update of the game world on the developer side will be immediately \navailable to players. Sending only the portion of the game world that players are interacting \nwith will save us significant bandwidth. As a result, 3D game streaming will give MMOG \ndevelopers the edge of lower delivery cost, real-time content updates, and design for more \ndynamic gameplay. \nThis gem will give an introduction to the 3D game streaming technology and its challenges. \nIt will also dive into the key components of a 3D streaming engine, including the renderer, \nthe transport layer, the predictive loading algorithm, and client/server architecture. Various \ntechniques to partition, stream, and re-integrate the 3D world data, including the terrain \nheight map, alpha blending texture, shadow texture, and static objects, will be revealed. A \nreal implementation of a 3D terrain streaming engine will be provided to serve the purpose \nof an illustrative demo. Source code is available and written in Visual C++/DirectX. \nThe Problem \nDelivering a large amount of content from one point to the other over the network is not a \nnew problem. Since the inception of the Internet, various file transport tools such as FTP, \n\n\n \n \nHTTP, and BitTorrent have been designed to deliver content. We could argue that these \nprotocols are sufficient if all of the content we delivered could be perceived as an opaque, \nundividable, monolithic pile of binary numbers and if we had an infinite amount of \nbandwidth to ship this content from one place to another. In reality, we have only limited \nbandwidth, and latency matters. Also, it‘s not only the final result that we care to deliver for \ngames, it is the experience the end user receives at the other end of the network that \nmatters. There are two goals to reach in order to deliver a great gaming experience to the \nusers: \n \nLow wait time \n \nHigh-quality content \nUnfortunately, the two goals conflict with each other using traditional download technology \nbecause the higher the quality of the content, the larger the size of the content, and the \nlonger the delivery time. How do we solve this dilemma? \n \nThe Solution \nThe conflict between the goals of low wait time and high quality leads us to consider new \nrepresentations that enable intelligent partitioning of the data into smaller data units, \nsending the units in a continuous stream over the network, then re-integrating the units at \nthe receiving end. This is the fundamental process of 3D game streaming technology. To \nunderstand how to stream game content, let‘s quickly review how video is streamed over \nthe Internet. \nVideo Streaming \nVideo is the progressive representation of images. Streaming video is naturally represented \nby a sequence of frames. Loss cannot be tolerated within a frame, but each of these frames \nis an independent entity that allows video streaming to tolerate the loss of some frames. To \nleverage the temporal dependency among video frames, MPEG is designed to delta-encode \nframes within the same temporal group. MPEG divides the entire sequence of frames into \nmultiple GOFs (groups of frames) and for each GOF, it encodes the key frame (I frame) with \na JPEG algorithm and delta-encodes the B/P frames based on the I frames. At the client \nside, the GOFs can be rendered progressively as soon as the I frame is delivered. There are \nstrict playback deadlines associated with each frame. Delayed frames are supposed to be \ndropped; otherwise, the user would experience out-of-order display of the frames. \nThe RTP transport protocol is based on unreliable UDP and is designed to ship media \ncontent in the unit of frames, as well as being aware of time sensitivity of the video/audio \nstream and doing smart packet loss handling. \nTo meet the goal of low wait time, linear playback order of the video frames is leveraged. \nMost of today‘s video streaming clients and servers employ prefetching optimization at \nvarious stages of the streaming pipeline to load video frames seconds or even minutes \nahead of the time when they are rendered. This way, enough buffer is created for decoding \nand rendering the frames. As a result, users will enjoy a very smooth playback experience \nat the client side. \nGame Streaming \nMMORPG 3D content has a different nature when compared to video. First, it is not \nconsumed linearly unless we de-generate the 3D content to one dimension and force \nplayers to watch it from beginning to end—which defeats the purpose of creating a 3D \nenvironment in the first place. Second, unlike video, 3D content has no intrinsic temporal \n\n\n \n \nlocality. With video, the frame we play is directly tied to the clock on the wall. In 3D, an \nimmersed gamer can choose to navigate through the content in an unpredictable way. He \ncan park his avatar at a vista point to watch a magnificent view of a valley for minutes, and \nthere is no deadline that forces him to move. He can also move in an arbitrary direction at \nfull speed to explore the unseen. Thus, we cannot prefetch 3D content according to the time \nthe content is supposed to be played back because there is no such time associated with the \ncontent. On the other hand, just like wandering in the real world, avatars in the virtual \nworld tend to move continuously in the 3D space, and there is a continuity in the subset of \nthe content falling in the avatar‘s view frustum, thus we should leverage the spatial locality \ninstead of temporal locality when streaming 3D content. As a result, 3D world streaming \ngenerally involves the following steps: \n1.  Partition the world geographically into independently renderable pieces. \n2.  Prefetch the right pieces of world at the right time ahead of when the avatar will \ninteract with the pieces. \n3.  Send the pieces from server to client. \n4.  Re-integrate the pieces and render them at the client side. \nThroughout this gem, we will discuss these technologies in more detail, as well as how to \nintegrate them to build a fully functional 3D streaming demo, the 3DStreamer. The full \nsource code and data of 3DStreamer is included on the CD-ROM. Please make sure to check \nout the code and experiment with it to fully understand how 3D streaming works. \n \nThe World \nBefore we can stream a 3D world from an MMO content server to the clients, we have to \nbuild it. In this section we discuss the basic 3D terrain rendering components and how they \nare generated and prepared for streaming. \nWhat Constitutes a 3D World \nIn this gem we will focus on streaming the artistic content of a 3D world, because artistic \ncontent easily constitutes 90 percent of the entire content set of today‘s MMO, and it is the \npart being patched most aggressively to renew players‘ interest in the game. \nTypical artistic content of a MMORPG includes: \n \nTerrain \no \nMesh (height map, normal map) \no \nTextures (multiple layers) \no \nAlpha blending map (for blending textures) \no \nShadow map (for drawing shadows) \n \nTerrain objects (stationary objects) \no \nMesh/textures \n \nAnimated objects (characters, NPCs, creatures) \no \nMesh/textures/animations \n \nSound \n\n\n \n \nIt is beyond the scope of a single gem to cover them all, and we will focus on terrain and \nterrain objects in this gem because they form the foundation of a 3D world streaming \nengine. \nSlice the Land \nThis section discusses the details of breaking up the world into tiles for rendering and \nstreaming. \nPatches of Land \nI always wondered why my house was bought as Parcel #1234 in the grant deed until I ran \ninto the need of slicing 3D terrain data in a virtual world. Today‘s MMORPG has a huge \nworld that can easily overwhelm a top-of-the-line computer if we want to render it all at \nonce. Similarly, it takes forever to download the entire MMO terrain data. Thus, to prepare \nthe world for streaming, the first step is slicing it into pieces that we can progressively send \nover and render. \nAs shown in Figure 5.4.1, in 3DStreamer, we divide the world into 32 × 32 patches so that \neach patch can be stored, downloaded, and rendered independently. The terrain information \nfor each patch is stored in its own data file, Terrain_y_x.dat, including all the data needed \nto render the patch, such as the height map, normal map, alpha blending textures, shadow \nmap, terrain object information, and so on. \nFigure 5.4.1. World consisting of 32×32 patches. \n \n\n\n \n \n \n \nTiles and Height Map \nTo generate the mesh for each terrain patch using the height map, we need to further \ndivide each patch into tiles. As shown in Figure 5.4.2, each patch is divided into 32×32 tiles. \nA tile has four vertices, thus we have 33×33 vertices per patch. To render a terrain with \nvarying heights, we assign each vertex of a patch a height. Altogether, we will have 33×33 \nheight values, which constitute the height map of the patch. \nFigure 5.4.2. Patch consisting of 32×32 tiles. \n \n \nTo build a mesh for each patch, we simply render each tile with two triangles, as shown in \nFigure 5.4.3. \nFigure 5.4.3. Patch consisting of 32×32 tiles. \n\n\n \n \n \n \nTo build a 3D height map, we need to map the above 2D mesh to 3D space. Here is the \nmapping between the 2D coordinate (x‘, y‘) we used above and its 3D world coordinates (x, \ny, z): \n{x, y, z} ç {x‘, height, -y‘} \nAs shown in Figure 5.4.4, the x‘ axis of the 2D coordinates becomes the X-axis of the 3D \nworld coordinate. The opposite of y‘ axis of the 2D coordinates becomes the Z-axis of the \n3D world coordinate. The 3D y-coordinate is given by the height of the vertices from the \nheight map. \nFigure 5.4.4. A rendered patch consisting of 32×32 tiles. \n",
      "page_number": 448
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 461-471)",
      "start_page": 461,
      "end_page": 471,
      "detection_method": "topic_boundary",
      "content": " \n \n \n \nFigure5.4.4 shows a few terrain patches rendered by 3DStreamer. Note that the entire \nterrain (32×32 patches) is located between the X-axis (x‘ axis of 2D space) and –Z-axis (y‘ \naxis of 2D space). This makes the traversal of tiles and patches very easy: Both start from \nzero. \nTo stitch the patches together seamlessly, the 33rd column of vertices of the patch (x‘ , y‘) \nis replicated to the first column of vertices of the patch (x‘+1, y‘). Similarly, the 33rd row of \npatch (x‘, y‘) is replicated to the first row of patch (x‘, y‘+1). \nThe following constants define the scale of the terrain and reveal the relationship among \npatches, tiles, and vertices. \n#define TILES_PER_PATCH_X 32 \n#define TILES_PER_PATCH_Y 32 \n#define PATCHES_PER_TERRAIN_X 32 \n#define PATCHES_PER_TERRAIN_Y 32 \n#define TILES_PER_TERRAIN_X (TILES_PER_PATCH_X * \nPATCHES_PER_TERRAIN_X) \n#define TILES_PER_TERRAIN_Y (TILES_PER_PATCH_Y * \nPATCHES_PER_TERRAIN_Y) \n#define VERTICES_PER_TERRAIN_X (TILES_PER_TERRAIN_X + 1) \n#define VERTICES_PER_TERRAIN_Y (TILES_PER_TERRAIN_Y + 1) \n \n \nTerrain Generation \n3DStreamer has a random terrain generator that will generate random terrain data and \nstore it into two types of output files. \n\n\n \n \nTerrain_y_x.dat: The terrain patch (x,y) \nTerrain_BB.dat: The bounding boxes for all patches \n \nThe reason that we need Terrain_BB.dat is for collision detection before the patches are \nloaded. To keep the avatar on the ground, even for patches not loaded, we need to be able \nto perform a rough collision detection using the patch‘s bounding box (BB). Also, the BB \nenables us to perform a rough view frustum culling before the detailed mesh information of \nthe patches is streamed over. \nIn a commercial MMO, the terrain data is usually handcrafted by a designer and artist to \ncreate a visually appealing environment. For demo and research purposes, though, it is \nconvenient to generate an arbitrary large terrain procedurally and use it to stress the \nstreaming engine. \nHere is the process of how to generate random terrain data and deploy it using \n3DStreamer. \n1. Run 3DStreamer with ―-g‖ on the command line. Alternatively, copy the pre-\ngenerated data from the 3Dstreamer source folder on the CD-ROM to skip this step. \n2. Upload the terrain data to an HTTP server (such as Apache). \nNow we can run 3DStreamer in client mode to stream the above data from the HTTP server \nand render the terrain incrementally based on a user‘s input. In production projects, \nthough, you probably won‘t use procedurally generated terrain with streaming because it‘s \nmuch cheaper to send the seed parameters of the procedure instead of the output data of \nthe procedure. \n \nThe Rendering \nThis section introduces the basic rendering features of terrain and how they are integrated \nwith streaming. \nTerrain Mesh \nAs described earlier, we will have a per-patch height map. From the height map, we‘ll build \nthe mesh for each patch as in the following code. \nCreateMesh(int patch_x, int patch_y) \n{ \n TERRAINVertex* vertex = 0; \n  D3DXCreateMeshFVF ( nrTri, nrVert, D3DXMESH_MANAGED, \n                  TERRAINVertex::FVF, m_pDevice, &m_pMesh); \n  m_pMesh->LockVertexBuffer(0,(void**)&vertex); \n    for(int y = patch_y * TILES_PER_PATCH_Y, y0 = 0; \n           y <= (patch_y+1) * TILES_PER_PATCH_Y; \n           y++, y0++) \n  for(int  x = patch_x * TILES_PER_PATCH_X, x0 = 0; \n           x<= (patch_x+1) * TILES_PER_PATCH_X; \n           x++, x0++) \n \n   { \n     int height = GetHeight(x, y); \n\n\n \n \n     D3DXVECTOR3 pos = D3DXVECTOR3(x, height, -y); \n     // Alpha UV stretches across the entire terrain in range \n[0, 1) \n     D3DXVECTOR2 alphaUV = D3DXVECTOR2(x / \nVERTICES_PER_TERRAIN_X, \n                                       y / \nVERTICES_PER_TERRAIN_Y); \n \n     // Color UV repeats every 10 tiles \n     D3DXVECTOR2 colorUV = alphaUV * TILES_PER_TERRAIN_X / \n10.0f; \n     vertex[z0 * (width + 1) + x0] = \n            TERRAINVertex(pos, GetNormal(x, z), alphaUV, \ncolorUV); \n     // Update BBox of the patch \n } \nm_pMesh->UnlockVertexBuffer(); \n… \n \n} \n \nFirst, we create a D3D mesh using the D3DXCreateMeshFVF API. Then we lock the mesh‘s \nvertex buffer and fill it up with vertex information. For each tile (x,y) of the patch \n(patch_x, patch_y), we retrieve its height from the height map and build the position \nvector as (x, height, -y). \nNext, we set up two sets of texture coordinates—colorUV for sampling texels from the \nterrain textures and alphaUV for sampling the alpha blending values of the terrain \ntextures. We‘ll discuss more about alpha blending in the next section. Then we calculate the \nnormals of each vertex based on the height map. Note that we don‘t want to only store the \nposition vectors at the server and re-generate normals at the client, because normals on \nthe patch border depend on the height from multiple patches. Since we need each patch to \nbe independently renderable, it‘s better that we store pre-computed normals with the patch \ndata itself. During the process of adding vertices, we update the BBox of the patch and \nstore it in the patch as well. The total size of vertex data of our demo terrain is about 40 \nbytes * (32 * 32) ^ 2 = 40 MB. \nMulti-Texture and Alpha Blending \nWith the mesh set up for each patch, we‘ll be able to render a wireframe terrain as shown in \nFigure 5.4.4. To give the terrain a more realistic look, we need to draw multiple types of \ntextures on top of the mesh to represent different terrain features (for example, dirt, grass, \nand stone). Three common methods exist for multi-texturing terrain. \nSingle Texture \nThe simplest way to texture the terrain is to manually bake multiple textures into one huge \nimage that covers the entire terrain. This method gives us total control over the texture \ndetails, but the cost of doing so can be prohibitive for large terrains. \nPer-Tile Texturing \nA more common method is to create multiple small textures at sizes from 128×128 to \n512×512 and then tile them across the terrain. For example, we can create a dirt texture, a \n\n\n \n \ngrass texture, and a stone texture, and then based on the height of each tile, we can \nprocedurally assign one of the textures to the tile. In a D3D environment, we can divide the \nmesh of a patch into three subsets, assign each subset a unique texture, and render the \nthree subsets in three iterations. However, this method has drawbacks. First, it‘s slow \nbecause it needs to render each mesh in three iterations for three subsets. Second, the \nborder between two areas with different textures will have a zigzag-shaped contour and \nabrupt change in tone of color. This is because our tile is square-shaped, and we can only \ndraw the entire tile using the same texture. Unless we apply special treatment to the \ntextures at the borders, the zigzag-shaped texture divider looks ugly. \nAlpha Blending Multi-Layer Textures \nA better way to do this is to assign each texture to its own layer, create an alpha channel \nfor each layer, and use a pixel shader to blend the textures. For example, we have three \ntexture layers (dirt, grass, stone) in 3DStreamer and one alpha texture with three channels \ncorresponding to the three layers. For each vertex, we can independently control whether \nit‘s dirt, grass, or stone. This way, we will not have the obvious zigzag border between \ndifferent textures because textures are assigned at the vertex level, not at the tile level. \nWith per-tile texturing, two triangles of the same tiles are always assigned to the same \ntexture, thus we‘ll have the shape of the tile show up at the border between different \ntexture areas. With alpha-blended multi-textures, the triangle that crosses the border of \ndifferent textures will have different textures assigned to different vertices. Thus, the color \nof pixels inside the triangle will be interpolated based on samples from different textures. \nAs a result, we will have a tile-wide blending zone between different textures, which \neliminates the clear zigzag border line. To further smooth the texture transition, we can \nbroaden the blending zone by assigning partial textures to the vertex at the border. For \nexample, instead of assigning a vertex with entirely dirt, grass, or stone, we can assign a \nvertex with 50 percent dirt and 50 percent grass if it is at the border. \nThe following code shows how we create a single alpha texture with three channels used to \nblend three layers of textures. We assign each vertex a texture type (dirt, grass, or stone) \nprocedurally. We also apply a smoothing filter to the alpha texture so that alpha at the \nborder transitions slower. \nD3DXCreateTexture(pDevice, VERTICES_PER_TERRAIN_X, \nVERTICES_PER_TERRAIN_Y, \n1, D3DUSAGE_DYNAMIC, D3DFMT_A8R8G8B8, D3DPOOL_DEFAULT, \npAlphaMap); \n \n D3DLOCKED_RECT sRect; \n(*pAlphaMap)->LockRect(0, &sRect, NULL, NULL); \nBYTE *bytes = (BYTE*)sRect.pBits; \n \nfor(int i = 0;i < numOfTextures; i++) \n  for(int y = 0; y < VERTICES_PER_TERRAIN_Y; y++) \n  { \n    for(int x = 0; x < VERTICES_PER_TERRAIN_X; x++) \n    { \n        TerrainTile *tile = GetTile(x,y); \n       // Apply a filter to smooth the border among different \ntile types \n        int intensity = 0; \n        // tile->m_type has procedually generated texture types \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x - 1, y); \n\n\n \n \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x , y - 1); \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x + 1, y); \n        if(tile->m_type == i) ++intensity; \n         tile = GetTile(x , y + 1); \n         if(tile->m_type == i) ++intensity; \n         bytes[y * sRect.Pitch + x * 4 + i] = 255 * intensity / \n5; \n       } \n    } \n(*pAlphaMap)->UnlockRect(0); \n \nFigure 5.4.5 shows the effect of alpha blending of three different textures (dirt, grass, \nstone) rendered by 3DStreamer. As you can see, the transition from one to another is \nsmooth. The total size of our terrain‘s alpha-blending data is about 3 bytes * (32 * 32) ^ 2 \n= 3 MB. \nFigure 5.4.5. Multi-texturing. \n \n \n \nStatic Shadow \nTo create a dynamic 3D terrain, we need to draw shadows of the mountains. We can either \ncalculate the shadow dynamically based on the direction of the light source or pre-calculate \na shadow map based on a preset light source. The latter approach is much faster because it \ndoes not require CPU cycles at run time. Basically, we build a per-vertex shadow texture \nthat covers the entire terrain. Each shadow texel represents whether the vertex is in \nshadow. We can determine whether a vertex is in shadow by creating a ray from the terrain \nvertex to the light source and test whether the ray intersects with the terrain mesh. If there \nis intersection, the vertex is in shadow, and it will have a texel value of 128 in the shadow \nmap. Otherwise, it is outside shadow and has a texel value 255. We can then use a pixel \nshader to blend in the shadow by multiplying the shadow texel with the original pixel. \n\n\n \n \nFigure 5.4.6 shows the effect of static shadow when the light source is preset at the left-\nhand side. The cost of storing the shadow texture of our terrain is not much—only 1 byte * \n(32 * 32) ^ 2 = 1 MB for the entire terrain. \nFigure 5.4.6. Shadow. \n \n \n \nTerrain Objects \nWithout any objects, the terrain looks boring. Thus, 3DStreamer adds two types of terrain \nobjects (stones and trees) to the terrain. The mesh of each terrain object is stored in a .X \nfile and loaded during startup. We are not streaming the .X files in 3DStreamer because \nthere are only two of them. In a game where a lot of unique terrain objects are used, we \nshould stream the model files of terrain objects as well. \nTo place terrain objects on top of terrain, we can use the terrain generator to randomly pick \none of the object types and place it at a random tile with random orientation with a random \nsize. We need to save the terrain objects‘ placement information with the per-patch terrain \ndata in order to redraw the objects at the client side. The following code fragment is an \nexample of writing terrain object placement information to the disk for each tile during \nterrain generation. \nOBJECT *object = tile->m_pObject; \nIf (object) \n{ \n  out.write((char*)&object->m_type, sizeof(object->m_type)); \n  out.write((char*)&object->m_meshInstance.m_pos, \nsizeof(object->m_meshInstance.m_pos)); \n  out.write((char*)&object->m_meshInstance.m_rot, \nsizeof(object->m_meshInstance.m_rot)); \n  out.write((char*)&object->m_meshInstance.m_sca, \nsizeof(object->m_meshInstance.m_sca)); \n} else \n{ \n      OBJECTTYPE otype = OBJ_NONE; \n\n\n \n \n      out.write((char*)&otype, sizeof(otype) \n} \n \nAssuming 20 percent of tiles have objects on them, the disk space taken by terrain objects‘ \nplacement information is about (4 B+12 B * 3) * (32 * 32)^2 * 20% = 8 MB. \nDivide and Conquer \nBased on discussion in previous sections, our experiment terrain used in 3DStreamer \nconsists of 32×32 patches and about one million tiles. Altogether, this takes about 60 MB of \ndisk space to store. Here is a rough breakdown of the sizes of various components of the \nterrain data. \nComponent \nData Size \nTerrain mesh \n40 MB \nTerrain object \n8 MB \nAlpha blending 3 MB \nShadow map \n1 MB \nOther \n8 MB \n \nAs shown in the Figure 5.4.7, it is a big terrain that takes a broadband user of 1-Mbps \nbandwidth 480 seconds (8 minutes) to download the complete data set. Thus, without \nstreaming we cannot start rendering the terrain for eight minutes! With streaming we can \nstart rendering the terrain in just a few seconds, and we will continuously stream the terrain \npatches the avatar interacts with over the network to the client. \nFigure 5.4.7. Big terrain. \n\n\n \n \n \n \n \n \nThe Transport \nTo this point we have generated our terrain, partitioned it into patches, and stored the \npatches in Terrain_y_x.dat and the bounding boxes of all the patches in Terrain_BB.dat. We \nalso know how to render the terrain based on these patches of data. The question left is \nhow to store the streaming data and send it over to the client from its data source. \nData Source and File Object \nStreaming is a progressive data transfer and rendering technology that enables a ―short \nwait‖ and ―high-quality‖ content experience. The 3D streaming discussed here targets \nstreaming over the network. However, the basic concepts and technique works for \nstreaming from the disk as well. Disk streaming can be very convenient for debugging or \nresearch purposes (for example, if you don‘t have an HTTP server set up, you can run \n3DStreamer with data stored on a local disk, too). We want to build a data storage \nabstraction layer that allows us to source 3D terrain data from both the local disk and a \nremote file server. 3DStreamer defined a FileObject class for this purpose. \n// Asynchronous File Read Interface for local disk read and \nremote HTTP read \nclass FileObject \n{ \npublic: \n      FileObject(const char* path, int bufSize); \n      ~FileObject(); \n      // Schedule the file object to be loaded \n      void Enqueue(FileQueue::QueueType priority); \n\n\n \n \n      // Wait until the file object is loaded \n      void Wait(); \n      // Read data sequentially out of an object after it is \nloaded \n      void Read(char* buf, int bytesToRead); \n      virtual void Load(LeakyBucket* bucket) = 0; \n}; \n \nFileObject provides an asynchronous file-loading interface consisting of the following \npublic methods: \n \nEnqueue. Schedule a file object to be loaded according to a specified priority. \n \nWait. Wait for a file object to be completely loaded. \n \nRead. Stream data out of the file after it is loaded to memory. \nThis is the main interface between the game‘s prefetching algorithm and the underlying \nmulti-queue asynchronous file read engine. The preloading algorithm will Enqueue() to \nqueue a file object for downloading at the specified priority. The render will call Wait() to \nwait for critical data if necessary. We should avoid blocking the render thread as much as \npossible to avoid visual lag. Currently 3DStreamer only calls Wait() for loading the BB \ndata at the beginning. The design goal of the prefetching algorithm is to minimize the wait \ntime for the render loop. Ideally, the prefetching algorithm should have requested the right \npiece of content in advance, and the renderer will always have the data it needs and never \nneed to block. When a file object is downloaded to the client, the render loop calls Read() \nto de-serialize the content from the file buffer to the 3D rendering buffers for rendering. \nBehind the scene, the FileObject will interact with the FileQueueManager to add \nitself to one of the four queues with different priorities, as shown in Figure 5.4.8. The \nFileQueueReadThread will continuously dequeue FileObjects from the \nFileQueues according to the priority of the queues and invoke the \nFileObject::Load() virtual method to perform the actual download from the data \nsource. We define the pure virtual method Load() as an interface to source specific \ndownloading algorithms. \nFigure 5.4.8. FileObject and FileQueue. \n\n\n \n \n \nBoth HTTPObject and DiskObject are derived from FileObject, and they \nencapsulate details of downloading a file object from the specific data source. They both \nimplement the FileObject::Load() interface. So when FileQueueThread invokes \nthe FileObject::Load(), the corresponding Load method of HTTPObject or \nDiskObject will take care of the data source–specific file downloading. Thus, \nFileObject hides the data source (protocol)–specific downloading details from the \nremainder of the system, which makes the asynchronous loading design agnostic to data \nsource. \nAsynchronous Loading with Multi-Priority Queues \nTo fulfill the ―low wait time‖ goal of game streaming, we need to achieve the following as \nmuch as possible: \nThe render loop does not block for streaming. \nThis translates into two requirements: \n \nWhen we request the loading of a file object, such as a terrain patch, the request \nneeds to be fulfilled asynchronously outside the render thread. \n \nWe only render the patches when they are available and skip the patches not loaded \nor being loaded. \nTo fulfill the ―high quality‖ goal for game streaming, we need to achieve the following \nrequirements: \n \nDynamically adjust the prefetching order of content in response to the player‘s input. \n \nOptimize the predictive loading algorithm so that the patches needed for rendering \nare always loaded in advance. \n\n\n \n \nWe will discuss the first three requirements here and leave the last requirement to later in \nthis gem. \nTo support asynchronous loading (the first requirement), the render thread only enqueues a \nloading request to one of the prefetching queues via FileQueueManager and never \nblocks on loading. The FileQueueReadThread is a dedicated file download thread, and \nit dequeues a request from one of the four queues and executes it. \nFileQueueReadThread follows a strict priority model when walking through the priority \nqueues. It starts with priority 0 and only moves to the next priority when the queue for the \ncurrent priority is empty. After it dequeues a request, it will invoke the transport protocol–\nspecific Load method to download the FileObject from the corresponding data source. \n(Refer to the ―Transport Protocol‖ section for transport details.) At the end of the Load \nfunction, when data is read into memory of the client, the FileObject::m_Loaded is \nmarked as true. \nThe third requirement is to react to players‘ input promptly. Our multi-priority asynchronous \nqueueing system supports on-the-fly cancel and requeue. At each frame, we will reevaluate \nthe player‘s area of interest, adjust the priorities of download requests, and move them \nacross queues if necessary. To support the second requirement, the render loop will do the \nfollowing: For patches in the viewing frustum and already submitted to DirectX (p-\n>m_loaded is TRUE), we render them directly. For patches not submitted yet but already \nloaded to the file buffer (p->m_fileObject->m_loaded is TRUE), we call \nPuntPatchToGPU() to fill vertex/index/texture buffers with the data and then render the \npatches. \nvoid TERRAIN::Render(CAMERA &camera) \n{ \n  … \n  for (int y = 0; y < m_numPatches.y; y++) \n   for (int x = 0; x < m_numPatches.x; x++) \n   { \n    PATCH* p = m_patches[y * m_numPatches.x + x]; \n    if(!camera.Cull(p->m_BBox)) \n     { \n       if (p->m_loaded) \n          p->Render(); \n       if (p->m_fileObject && p->m_fileObject->m_loaded) \n       { \n          PuntPatchToGPU(p); \n          p->Render(); \n       } \n     } \n   } \n … \n} \n \nWith the above multi-priority asynchronous queueing system, we can load patches \nasynchronously with differentiated priority. The dedicated FileQueueReadThread thread \ndecouples the file downloading from the video rendering in the main thread. As a result, \nvideo will never be frozen due to the lag in the streaming system. The worst case that could \nhappen here is we walk onto a patch that is still being downloaded. This should rarely \nhappen if our predictive loading algorithm works properly and we are within our tested and \nexpected bandwidth amount. We did have a safety net designed in this case, which is the \nper-patch bounding box data we loaded during startup. We will simply use the BB of the \n",
      "page_number": 461
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 472-480)",
      "start_page": 472,
      "end_page": 480,
      "detection_method": "topic_boundary",
      "content": " \n \npatch for collision detection between the avatar and the terrain. So even in the worst case, \nthe avatar will not fall under the terrain and die—phew! \nTransport Protocol \nWe will use HTTP 1.1 as the transport protocol that supports persistent connections. Thus, \nall the requests from the same 3DStreamer client will be sent to the server via the same \nTCP connection. This saves us connection establishment and teardown overhead for \ndownloading each patch. Also, HTTP gives us the following benefits: \n \nHTTP is a reliable protocol, necessary for 3D streaming. \n \nHTTP is a well-known protocol with stable support. So we can directly use a mature \nHTTP server, such as Apache, to serve our game content. \n \nHTTP is feature-rich. A lot of features useful to 3D streaming, such as caching, \ncompression, and encryption, come for free with HTTP. \n \nThe implementation is easy, since most platforms provides a ready-to-use HTTP \nlibrary. \nAs described in the ―Transport Protocol‖ section, the HTTP transport is supported via \nHTTPObject, whose primary work is to implement the FileObject::Load() interface. \nThe framework is very easy to extend to support other protocols as well, when such need \narises. \nHTTP Compression \nCompression is very useful to reduce bandwidth of streaming. With HTTP, we can enable the \ndeflate/zlib transport encoding for general compression. \nHTTP Caching \nWe will not discuss caching in detail. For what it‘s worth, we can easily enable client-side \nHTTP caching by not giving INTERNET_FLAG_NO_CACHE_WRITE to the \nHttpOpenRequest() in the HttpObject::Load() method. \nLeaky Bucket \nA leaky bucket–based bandwidth rate limiter becomes handy when we need to evaluate the \nperformance of a game streaming engine. Say we want to see how the terrain rendering \nworks at different bandwidth caps—2 Mbps, 1 Mbps, and 100 Kbps—and tune our predictive \nloading algorithms accordingly, or we want to use the local hard disk as a data source to \nsimulate a 1-Mbps connection, but the real hard disk runs at 300 Mbps—how do we do this? \nLeaky bucket is a widely used algorithm for implementing a rate limiter for any I/O channel, \nincluding disk and network. \nThe following function implements a simple leaky bucket model. m_fillRate is how fast \ndownload credits (1 byte per unit) are filled into the bucket and is essentially the bandwidth \ncap we want to enforce. The m_burstSize is the depth of the bucket and is essentially \nthe maximum burst size the bucket can tolerate. Every time the bucket is negative on \ncredits, it returns a positive value, which is how many milliseconds the caller needs to wait \nto regain the minimum credit level. \nint LeakyBucket::Update( int bytesRcvd ) \n{ \n   ULONGLONG tick = GetTickCount64(); \n int deltaMs = (int)(tick - m_tick); \n\n\n \n \n if (deltaMs > 0) \n { \n // Update the running average of the rate \n  m_rate = 0.5*m_rate + 0.5*bytesRcvd*8/1024/deltaMs; \n  m_tick = tick; \n } \n // Refill the bucket \n  m_credits += m_fillRate * deltaMs * 1024 * 1024 / 1000 / 8; \n  if (m_credits > m_burstSize) \n    m_credits = m_burstSize; \n   // Leak the bucket \n  m_credits -= bytesRcvd; \n if (m_credits >= 0) \n   return 0; \n else \n   return (-m_credits) * 8 * 1000 / (1024 * 1024) / m_fillRate; \n} \n \nThis is the HTTP downloading code that invokes the leaky bucket for rate limiting. \nvoid HttpObject::Load(LeakyBucket* bucket) \n{ \n  … \n  InternetReadFile(hRequest, buffer, m_bufferSize, &bytesRead); \n  int ms = bucket->Update(bytesRead); \n  if (ms) \n     Sleep(ms); \n  … \n} \n \nFigure 5.4.9 shows a scenario where we set the rate limiter to 1 Mbps to run a terrain \nwalking test in 3DStreamer. The bandwidth we displayed is the actual bandwidth the \n3DStreamer used, and it should be governed under 1 Mbps within the burst tolerance. Also, \nit shows how many file objects are in each priority queue. Since we are running fast with a \nrelatively low bandwidth cap, there are some patches being downloaded in the four queues, \nincluding one critical patch close to the camera. \nFigure 5.4.9. Bandwidth and prefetch queues. \n\n\n \n \n \n \n \n \nPredictive Loading \nThe predictive loading algorithm is at the heart of a 3D streaming engine because it impacts \nperformance and user experience directly. A poorly designed predictive loading algorithm \nwill prefetch the wrong data at the wrong time and result in severe rendering lag caused by \nlack of critical data. A well-designed predictive algorithm will load the right piece of content \nat the right time and generate a pleasant user experience. The following are general \nguidelines to design a good prefetching algorithm: \n \nDo not prefetch a piece of content too early (wasting memory and bandwidth). \n \nDo not prefetch a piece of content too late (causing game lag). \n \nUnderstand the dependency among data files and prefetch dependent data first. \n \nReact to user input promptly. (I turned away from the castle, stop loading it.) \n \nUtilize bandwidth effectively. (I have an 8-Mbps fat idle connection; use it all, even \nfor faraway terrain patches.) \n \nUse differentiated priority for different types of content. \nWe designed a very simple prefetching algorithm for 3DStreamer following the guidelines. \nTo understand how it works, let‘s take a look at the camera control first. \nCamera Control \nThe camera controls what the players see in the virtual world and how they see it. To \nminimize visual lag caused by streaming, it is crucial that the prefetching algorithm \nunderstands camera controls and synchronizes with the camera state. \nA camera in the 3D rendering pipeline is defined by three vectors: \n\n\n \n \n \nThe ―eye‖ vector that defines the position of the camera (a.k.a. ―eye‖). \n \nThe ―LookAt‖ vector that defines the direction the eye is looking. \n \nThe ―Up‖ or ―Right‖ vector that defines the ―up‖ or ―right‖ direction of the camera. \nSome common camera controls supported by 3D games are: \n \nShift. Forward/backward/left-shift/right-shift four-direction movement of the camera \nin the Z-O-X horizontal plane (no Y-axis movement). \n \nRotate. Rotate the ―LookAt‖ vector horizontally or vertically. \n \nTerrain following. Automatic update of the y-coordinate (height) of the eye. \n3DStreamer supports all three controls. And the way we control the camera impacts the \nimplementation of the predictive loading algorithm, as you can see next. \nDistance Function \nWhen do we need a patch of terrain to be loaded? When it is needed. We will present a few \nways to calculate when the patches are needed in the following sections and compare them. \nViewing Frustum-Based Preloading \nWe should preload a patch before it is needed for rendering. As we know, the camera‘s view \nfrustum is used by the terrain renderer to cull invisible patches, so it‘s natural to preload \nevery patch that falls in the viewing frustum. With some experiments, we can easily find the \ndynamic range of that viewing frustum is so huge that it‘s hardly a good measure of what to \nload and when. \nSometimes the scope is so small (for example, when we are looking directly at the ground) \nthat there is no patch except the current patch we are standing on in the frustum. Does this \nmean we need to preload nothing except the current patch in this case? What if we \nsuddenly raise our heads and see 10 patches ahead? \nSometimes the scope is too big (for example, when you are looking straight at the horizon \nand the line of sight is parallel to the ground) and there are hundreds of patches falling in \nthe frustum. Does this mean we should preload all of the patches up to the ones very far \naway, sitting on the edge of the horizon? What about the patches immediately to our left \nshoulder? We could turn to them anytime and only see blanks if we don‘t preload them. \nAlso, we don‘t really care if a small patch far away is rendered or not even if it may be in \nthe viewing frustum. \nDistance Function–Based Preloading \nTo answer the question of when a patch is needed more precisely, we need to define a \ndistance function: \nD(p) = distance of patch p to the camera \nIntuitively, the farther away the patch is from the camera, the less likely the avatar will \ninteract with the patch shortly. Thus, we can calculate the distance of each patch to the \navatar and prefetch the ones in the ascending order of their distances. The only thing left is \nto define exactly how the distance function is calculated. \nStraight Line Distance Function \nThe simplest distance function we can define is: \nD(p) = sqrt((x0 – x1) * (x0 – x1) + (z0 –z1) * (z0 – z1)) \n\n\n \n \n(x0, z0) are the coordinates of the center of the patch projected to the XZ plane, and (x1, \nz1) are the coordinates of the camera projected to the XZ plane. Then we can divide \ndistance into several ranges and preload the patches in the following order: \n \nCritical-priority queue: Prefetch D(p) < 1 * size of a patch \n \nHigh-priority queue: Prefetch D(p) < 2 * size of a patch \n \nMedium-priority queue: Prefetch D(p) < 4 * size of a patch \n \nLow-priority queue: Prefetch D(p) < 8 * size of a patch \nIn other words, we will divide the entire terrain into multiple circular bands and assign the \ncircular bands to different priority queues according to their distance from the camera. \nThis is a prefetching algorithm commonly used in many games. However, this algorithm \ndoes not take into consideration the orientation of the avatar. The patch immediately in \nfront of the avatar and the patch immediately behind the avatar are treated the same as \nlong as they have equal distances to the avatar. In reality, the character has a higher \nprobability moving forward than moving backward, and most games give slower backward-\nmoving speed than forward moving, thus it‘s unfair to prefetch the patch in front of a \ncamera at the same priority as the patch behind it. \nProbability-Based Distance Function \nAn avatar can move from its current location to the destination patch in different ways. For \nexample: \n1. Walk forward one patch and left shift one patch. \n2. Turn left 45 degrees and walk forward 1.4 patch. \n3. Turn right 45 degrees and left-shift 1.4 patch. \n4. Take a portal connected to the patch directly. \n5. Take a mount, turn left 45 degrees, and ride forward 1.4 patch. \nI intentionally chose the word ―way‖ instead of ―path.‖ In order to use a length of the path \nas a distance function, the avatar must walk to the destination on the terrain at a constant \nspeed. In reality, there are different ways and different speeds for the avatar to get to the \npatch, and they may or may not involve walking on the terrain. Also, the distance cannot be \nmeasured by the physical length of the route the avatar takes to get to the patch in some \ncases (such as teleporting). A more universal unit to measure the distance of different ways \nis the time it takes the avatar to get there. With the above example, the time it takes the \navatar to get to the destination is: \n1. Assuming forward speed is 0.2 patch/second and left-shift speed is 0.1 \npatch/second, it takes the avatar 5 + 10 = 15 seconds to get there. \n2. Assuming the turning speed is 45 degrees/second, it takes the avatar 1 + 7 = 8 \nseconds to get there. \n3. It takes 1 + 14 = 15 seconds to get there. \n4. Assuming the portal takes 5 seconds to start and 5 seconds in transit, it takes 10 \nseconds to get there. \n5. Assuming the mount turns and moves two times as fast, it takes 8 / 2 = 4 seconds \nto get there. \nLet‘s say we know that the probabilities of the avatar to use the aforementioned ways to get \nto patch p are: 0.2, 0.6, 0.0 (it‘s kind of brain-dead to do 3), 0.1, and 0.1, respectively. \nThe probability-based distance D(p) will then be given by: 15 * 0.2 + 8 * 0.6 + 10 * 0.1 + \n4 * 0.1 = 3 + 4.8 + 1 + 0.4 = 9.2 seconds. \nThus, the probability-based distance function can be written as: \n\n\n \n \n \nwhere p(i) is the probability of the avatar taking way i to get to patch p, and t(i) is the time \nit takes to get to p using way i. \nAs you can see, the power of this probability-based distance function is that it can be \nexpanded to an extent as sophisticated as we want depending on how many factors we \nwant to include to guide the predictive loading algorithm. For a game where we want to \nconsider all kinds of combinations of moving options for an avatar, we can model the \nmoving behavior of the avatar statistically in real time and feed statistics back to the above \nformula to have very accurate predictive loading. At the same time, we can simplify the \nabove formula as much as we can to have a simple but still reasonably accurate predictive \nloading algorithm. In the 3DStreamer demo, we simplify the distance function as the \nfollowing. \nFirst, 3DStreamer does not support teleporting or mount, so options 4 and 5 are out of \nconsideration. Second, as in most games, 3DStreamer defines the speeds for left/right \nshifting and moving backward at a much lower value than the speed of moving forward. \nFrom the user experience point of view, it‘s more intuitive for the player to move forward \nanyway. So in 3DStreamer, we assume that the probability of an avatar using the ―turn and \ngo forward‖ way is 100 percent. With this assumption, the distance function is reduced to: \nD(p) = rotation time + moving time= alpha / w + d / v. \nAlpha is the horizontal angle the camera needs to rotate to look at the center of p directly. \nw is the angular velocity for rotating the camera. d is the straight-line distance between the \ncenter of p and the camera. v is the forward-moving speed of the avatar. \nThe following code fragment shows the implementation of the predictive loading in \n3DStreamer based on the simplified distance function. \nD3DXVECTOR2 patchPos(((float)mr.left + mr.right) / 2, \n((float)mr.top + \nmr.bottom) / 2); \nD3DXVECTOR2 eyePos(camera.Eye().x, - camera.Eye().z); \nD3DXVECTOR2 eyeToPatch = patchPos - eyePos; \nfloat patchAngle = atan2f(eyeToPatch.y, eyeToPatch.x); // [-Pi, \n+Pi] \n \n// Calculate rotation distance and rotation time \nfloat angleDelta = abs(patchAngle - camera.Alpha()); \nif (angleDelta > D3DX_PI) \n    angleDelta = 2 * D3DX_PI - angleDelta; \n \nfloat rotationTime = angleDelta / camera.AngularVelocity(); \n \n// Calculate linear distance and movement time \nfloat distance = D3DXVec2Length(&eyeToPatch); \nfloat linearTime = distance / camera.Velocity(); \n \nfloat totalTime = rotationTime + linearTime; \nfloat patchTraverseTime = TILES_PER_PATCH_X / camera.Velocity(); \nif (totalTime < 2 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_CRITICAL); \n\n\n \n \nelse if (totalTime < 4 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_HIGH); \nelse if (totalTime < 6 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_MEDIUM); \nelse if (totalTime < 8 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_LOW); \n  else \n  { \n        If (patch->m_loaded) \n             patch->Unload(); \n        else if (patch->m_fileObject->GetQueue() != \nFileQueue::QUEUE_NONE) \n           CancelTerrainPatch(patch); \n} \n \nFor each frame, we reevaluate the distance function of each patch and move the patch to \nthe proper priority queue accordingly. The ability to dynamically move file prefetching \nrequests across priorities is important. This will handle the case where an avatar makes a \nsharp turn and the high-priority patch in front of the avatar suddenly becomes less \nimportant. In an extreme case, a patch earlier in one of the priority queues could be \ndowngraded so much that it‘s canceled from the prefetching queues entirely. \nNote that in the last case, where D(p) >= 8 * patchTraverseTime, we will process it \ndifferently depending on its current state. \n \nAlready loaded. Unload the patch and free memory for mesh and terrain objects. \n \nAlready in queue. Cancel it from the queue. \nWith this predictive loading algorithm, 3DStreamer can render a terrain of one million tiles \nat 800×600 resolution under 1-Mbps bandwidth pretty decently. At a speed of 15 \ntiles/second, we still get enough time to prefetch most nearby patches in the avatar‘s \nviewing frustum, even though we make sharp turns. \nFigure 5.4.10 shows what it looks like when we run the avatar around at 15 tiles/s with only \n1-Mbps bandwidth to the HTTP streaming server. The mini-map shows which patches of the \nentire terrain have been preloaded, and the white box in the mini-map is the intersection of \nthe viewing frustum with the ground. As you can see, the avatar is moving toward the \nupper-left corner of the map. As shown by ―Prefetch Queues,‖ all the critical- and high-\npriority patches are loaded already, which corresponds to all the nearby patches in the \nviewing frustum plus patches to the side of and behind the player. The medium- and low-\npriority queues have 80 patches to download, which are mostly the faraway patches in front \nof the player. \nFigure 5.4.10. Predictive loading under 1-Mbps bandwidth at 15 tiles/s. \n\n\n \n \n \nAlso note that the non-black area in the mini-map shows the current memory footprint of \nthe terrain data. It is important that when we move the avatar around, we unload faraway \npatches from memory to create space for new patches. This way, we will maintain a \nconstant memory footprint regardless of the size of the entire terrain, which is a \ntremendously important feature of a 3D streaming engine in order to scale up to an \ninfinitely large virtual world. \n \n3DStreamer: Putting Everything Together \n3DStreamer is a demo 3D terrain walker that implements most of the concepts and \ntechniques discussed in this gem. It implements a user-controllable first-person camera that \nfollows the multi-textured, shadow-mapped large terrain surface. A mini-map is rendered in \nreal time to show the current active area of interest and the viewing frustum. Various \nkeyboard controls are provided for navigation through the terrain as if in the game. (See \nthe onscreen menu for the key bindings.) Here is the one-page manual to set it up. \nCompile the Source \n3DStreamer.sln can be loaded and compiled with Visual Studio 2008 with DirectX November \n2008 or newer. The data is staged to the Debug folder for debug build by default. In order \nto run a release build, you need to manually copy the data (data, models, shaders, \ntextures) from the Debug folder to the Release folder. \nTerrain Generation \n\n\n \n \nRunning the 3DStreamer with the following command will generate a random 32 × 32-patch \nterrain and save the data in the executable‘s folder. It will take a while to complete. \nAlternatively, just use the pre-generated data in the Debug\\Data folder on the CD-ROM. \n3DStreamer -g \n \n \nStaging the Data \nFor HTTP streaming, upload the data (terrain_XX_YY.dat and terrain_BB.dat) to your HTTP \nserver and use the -s= command line argument to specify the server‘s host name and the \n-p= argument to specify the location of the data on the server. \nFor DISK streaming, simply give the data‘s path as the -p= argument. \nRun \nTo run it in VS, please make sure to add $(OutDir) to the Working Directory of the \nproject properties. Alternatively, you can run the executable from the executable folder \ndirectly. By default, 3DStreamer runs in DISK streaming mode with a default data path \npointing to the executable folder. \nTo run it in HTTP streaming mode, you need to give the -s argument for the host name. \nFor example, to stream the data from a server URL \nhttp://192.168.11.11/3dstreamer/32x32, just run it with the following command line: \n3DStreamer -h -s=192.168.11.11 -p=3dstreamer\\32x32\\ \n \nNote that the trailing slash \\ in the path is important. Otherwise, the client cannot construct \nthe proper server URL. \nYou can also adjust the bandwidth cap (the default is 1 Mbps) with the -b= argument. For \nexample, to run it in DISK streaming mode simulating a 2-Mbps link, just enter: \n3DStreamer –b=2 \n \n \n \nConclusion \nIn this gem we described a fundamental problem—delivering game content in a short time \ninterval at high quality. We then discussed a 3D streaming solution. We presented a three-\nstep method to build a 3D streaming engine: Decompose the world into independent \ncomponents at the server, transfer the components with guidance from a predictive loading \nalgorithm over the network, and reintegrate the components and render the world at the \nclient. In the process, we defined the distance function–based predictive loading algorithm \nthat is the heart of a 3D streaming engine. Finally, we integrated all the components to \nbuild a 3DStreamer demo that streams a large terrain of a million tiles that has multiple \nlayers of textures blended to the client in real time with a remote HTTP server hosting the \ndata. Now, the reader of this gem has everything he or she needs to apply 3D streaming \ntechnology to a next-generation MMO design! \n",
      "page_number": 472
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 481-489)",
      "start_page": 481,
      "end_page": 489,
      "detection_method": "topic_boundary",
      "content": " \n \n \nSection 6: Audio \nIntroduction \nA Practical DSP Radio Effect \nEmpowering Your Audio Team with a Great Engine \nReal-Time Sound Synthesis for Rigid Bodies \nIntroduction \nBrian Schmidt, Founder and Executive Director, GameSoundCon; President, Brian \nSchmidt Studios brian@brianschmidtstudios.com \nFor a good deal of its lifetime, advances in game audio have been focused on creating more \nadvanced audio chips and synthesis capabilities using dedicated pieces of hardware for \naudio processing. From the SID and NES chips through Yamaha chips, Sony SPUs, and the \noriginal Xbox audio chip, the trend has been toward more voices, higher fidelity, and \ngreater audio signal processing capabilities in a never-ending goal to create realistic video \ngame sounds. With the current generation and the movement toward a more easily C-\nprogrammable audio system, the emphasis has expanded somewhat—rather than focusing \nsolely on driving more mathematics into the audio signal path, an effort has been made to \nmake those (quite simple) technologies that are available easier to use and manipulate by \nthe composer and the sound designer. \nGame audio development, from a programming perspective, has therefore focused on two \nquite different areas: the very high and the very low level. \nAt the very high level, game audio development was radicalized by the introduction of high-\nlevel authoring tools and matching game audio engines. Prior to the creation of these tools, \nit was clear that one of the largest obstacles to great game audio was the ability for sound \ndesigners and composers (the people with the ―ears‖) to bring their vision to fruition, rather \nthan a lack of cutting-edge sound-processing technology. Often the ideas composers and \nsound designers had were well within the existing capabilities of common custom or third-\nparty game audio engines. Indeed, creating cooler game audio technology was of limited \nvalue, because even if new technologies were available, it was difficult (and programmer \nintensive) to effectively use those technologies. Composers and sound designers weren‘t \neven able to use simple existing technologies effectively, because even the simplest creative \nnotion required creating a detailed description of what they wanted and then going to the \nprogrammer in the hopes that it could be programmed without too much trouble. Such \ncode-driven workflow was not conducive to creativity, because it relied both on programmer \ntime (scarce in any project) and on the ability for the sound designer/composer to \nadequately describe what he or she wanted in a language the programmers could \nunderstand. \nHigh-level tools were introduced broadly around 2002, with the introduction of XACT (Xbox \nAudio Creation Tool) for Xbox and SCREAM for Sony platforms. In these tools—and later in \nplatform-independent tools, such as WWise and FMOD Designer—the sound designer or \ncomposer could use a graphical interface to create game audio content that was then \nplayable by the matching high-level game audio engine. The key element of these content-\ndriven tools was that they allowed, for the first time, highly desirable features to be used in \na way that the composer or sound designer could try, modify, tweak, and otherwise \nexperiment without needing to bother the programmer. \n\n\n \n \nSo, ironically, audio quality dramatically increased not in response to cool, cutting-edge low-\nlevel audio capabilities, but simply by packaging up the existing technologies into a \nformat/tool chain that could be easily used by the creative audio professional. In essence, \nbetter game audio came from programming better tools with better work-flows and better \nUIs. \nThe gem by Mat Noguchi of Bungie describes the content-driven system used in the Halo \nseries of games across Xbox and Xbox 360. Note the UIs created for the sound designer. \nAlso, pay particular attention to the mixing portion; game audio mixing and post-production \nrepresent some of the biggest issues in current-day game development. \nThe emphasis on high-level tools and content-driven audio systems notwithstanding, the \ncutting edge of game programming has no shortage of low-level problems to be solved. \nAudio signal processing down at the sample level still provides many challenges for great \ngame audio. Growing in popularity in game development is the use of customized digital \nsignal processing (DSP) algorithms to achieve specific effects. \nAudio DSP in games is sometimes compared to pixel shaders in graphics; in fact, the term \n―sound shader‖ is occasionally used. Although game engines have for some time supported \nsomewhat fixed-function DSP in the form of hard-coded resonant filters, \nocclusion/obstruction filtering, and environmental reverb, the advent of CPU-based audio \nsystems has greatly lowered the bar for custom-written DSP tailored for a specific audio \neffect. The DSP effects are sometimes used to take a sound and modify it for a certain \nenvironment/circumstance (such as a reverb or voice effect), or it may be integral to \ncreation of the sound itself (as in a pitch shifter used to create a low dinosaur roar from a \nrecording of an elephant). Audio DSP is also used to process the final output of the audio \nengine using special DSP effects called mastering effects, which are used in virtually every \nother audio/visual medium to put final polish on the sound. The gem by Ian Lewis on \ncreating a run-time radioization effect describes the creation of custom DSP for a particular \nvoice effect used in a popular Xbox title. \nIn addition to audio DSP designed to process existing audio data, further low-level audio \nprogramming challenges lie in creating audio from pure mathematics. Physical modeling is \nsuch a system, where the vibrations that result from the collision of objects are modeled \nand treated as audio data and fed into the audio mix. Further, savvy game programmers \nrecognize the treasure trove of data within the physics simulation code that can be used to \neither create audio or drive parameters of a sophisticated audio engine. The gem by Zhimin \nRen describes such a system, deriving modal synthesis control parameters from the physics \nengine to create tightly correlated audio matching the visual images for impacting and \nrolling interactions. \nSo there remains no shortage of high-level and low-level challenges for game audio \nprogrammers. Better tools that enable composers and sound designers to work more \nefficiently and take advantage of existing technologies in unique and creative ways are \ncontinually undergoing development and improvement. Low-level signal processing in the \nform of off-the-shelf or custom-written DSP provides greater variety and tighter interaction \nwith the visuals. And physical modeling, together with other procedurally generated audio, \nis beginning to show promise in real-world applications. In John Carmack‘s GDC 2004 \nkeynote address, he postulated that, but for a bit more CPU, game audio was ―basically \ndone.‖ We would challenge that assertion as… premature. \n \n6.1. A Practical DSP Radio Effect \nIan Ni-Lewis \nSinglemalt71@gmail.com \n\n\n \n \nLet‘s say you‘re making a squad-oriented first-person shooter and you want it to have great \nimmersive audio. So you carefully place all of the sound effects in the world, making sure \nthey have realistic distance-based roll-off curves. You calculate the geometry of the game \nmap and make sure each sound gets filtered for obstruction and occlusion. And you set up \nzones for reverberation and get them all carefully cross-faded. Everything sounds realistic. \nBut there‘s a problem, says your lead designer: The gameplay design requires that you be \nable to hear messages from your squadmates. If the squad gets separated, that dialog gets \nrolled off or occluded, and the player can‘t hear what‘s going on. You explain that this is \nrealistic, that people can‘t hear each other when they‘re hundreds of yards away or \nseparated by thick concrete walls, and that changing that would break the immersiveness of \nthe game. The designer is unmoved—and your dialog guy is on his side for once. The dialog \nneeds to be audible everywhere. \nAssuming your FPS isn‘t set too far in the past, a great way to solve this problem is with a \nradio effect. Keep all of the work you did on distance, obstruction, and occlusion for each \ndialog source, because that still sounds great when the source is nearby. But as the direct \nsound from the source gets fainter, you cross-fade in a version of the dialog that is band-\nlimited and distorted (see Figure 6.1.1). \nFigure 6.1.1. Cross-fading of original and distorted dialog based on distance. \n \n \nThis is a great effect—when the source is close by, you get the full environmental audio; but \nwhen it‘s far away, it sounds like it‘s coming to you over the airwaves. \nThe next question, and the topic of this gem, is: How do we get the distorted dialog? One \neasy solution is to just apply a radio effect offline in your favorite audio editing application. \nBut taking the easy way out in this case is going to double your storage budget for audio, \nnot to mention doubling the bandwidth required to render each line of dialog. Plus, it‘s no \nfun. It‘d be a lot more interesting if we could apply the effect in real time. Fortunately, it‘s \nnot too hard to do. \nThe Effect \nI‘m not going to try to explain exactly how signals are affected by radio transmission, \nbecause I can‘t. But we can come up with a pretty convincing approximation by making the \nsignal sound tinny and distorted, with maybe a little static thrown in for good measure. \nCranking Up the Distortion \nDistortion is the most interesting part of the effect from the programmer‘s point of view. We \nwant to emulate clipping, which happens when the signal volume is too high for the output \ndevice to handle. This is insanely easy to do if you don‘t care about quality—just raise the \nvolume and saturate. That lops off the top and bottom of each wave, as you see in Figure \n6.1.2. \n\n\n \n \nFigure 6.1.2. Distortion of a sine wave due to digital saturation. \n \n \nWith just a little extra gain, this doesn‘t sound all that bad—but it doesn‘t sound right, \neither. It‘s a little harsher and more ―digital‖ than what we really want. We‘re getting noise \nand distortion, but it‘s not the good kind. It‘s the bad kind that makes you think maybe you \nneed to go back and change all your sound levels. This is unsurprising, since all we‘ve done \nso far is digital clipping—the same thing your digital-to-analog converter is going to end up \ndoing if your levels are too hot. \nWhat we really want is something that sounds grungy, but warm—something that won‘t \nirritate our ears or make us think something is wrong with the game. Something that \nsounds nice and warm and analog…. How do we do that? \nTo understand how we go about making nice-sounding distortion, let‘s start by taking a look \nat why our naïve distortion technique sounds so bad. Figure 6.1.3 shows a spectral plot of a \nclean, full-range sine wave with a frequency of just under 1/8 Nyquist. \nFigure 6.1.3. Spectral (frequency) plot of a clean sine wave. \n \n \nFigure 6.1.4 shows the same sine wave with the gain turned up to 1.1. Notice the little \nbumps that start appearing to the right of the original frequency? There‘s the grunge we‘re \nhearing. It‘s harmonic distortion—―distortion‖ because we‘re getting additional frequencies \nthat weren‘t in the source, and ―harmonic‖ because the new frequencies happen to be \nmultiples of the frequency of the original sine wave. Normally, harmonic distortion is exactly \n\n\n \n \nwhat we want. But there‘s a problem here. It‘s hard to see in the previous graph, but watch \nwhat happens when we crank the gain up to 11 (see Figure 6.1.5). \nFigure 6.1.4. Spectral (frequency) plot of a clipped sine wave with gain = 1.1. \n \n \nFigure 6.1.5. Spectral (frequency) plot of a clipped sine wave with gain = 11. \n \n \nIt‘s a classic aliasing pattern: New frequencies are generated above Nyquist, so they wrap \nback over the top of the original frequencies. In the previous graph, only three of the \nfrequencies—the original sine and the largest two harmonics—are below the Nyquist \nfrequency. These three get smaller as you go from left to right. The next longest bars in the \ngraph are between 2× and 3× Nyquist, so they‘re reflected. You can see that they get \nsmaller from right to left. After that, the magnitudes get pretty small and hard to see, but \nat the resolution of this graph, there are still a couple of harmonics that bounced off zero \nand started decreasing from right to left again. \nSo there‘s where the harshness is coming from. The distortion is producing frequencies that \naren‘t band-limited, and that‘s turning into aliasing, and it sounds awful. Let‘s see if we can \nfix that and add some warmth while we‘re at it. \n\n\n \n \nWhat we‘ve been talking about so far is hard clipping, which boils down to just throwing \naway any sample with a magnitude larger than some threshold value and replacing it with a \nsample at that threshold value (or a negative threshold value). If we plotted a graph of \ninput sample values versus output values for a hard-clipping algorithm, it would look \nsomething like the graph in Figure 6.1.6, with input values on the X-axis and corresponding \noutputs on the Y-axis. This graph is called the transfer function. \nFigure 6.1.6. Transfer function for hard-clipping algorithm. \n \n \nOne easy way to improve on this is to gently reduce the values, rather than chopping them \noff entirely. We‘ll start by choosing a lower threshold than the one we used for hard \nclipping, so we have some headroom to work with. Our new soft-clipping code looks more \nlike this: \nif( input > threshold ) \n      output = ( ( input – threshold ) * ratio ) + threshold; \nelse \n      output = input; \n \nOr, simplifying some: \noffset = ( 1.0f – ratio ) * threshold; \nif( input > threshold ) \n      output = ( input * ratio ) + offset; \nelse \n      output = input; \n \nGraphing the transfer function of the soft clipper gives us something like what you see in \nFigure 6.1.7. \nFigure 6.1.7. Transfer function for softened clipping algorithm. \n\n\n \n \n \n \nIf you‘re familiar with studio production tools, you might recognize this as the transfer \nfunction for a hard-knee compressor. It‘s a common tool used to implement soft clipping. \nAnd it works, as you can see from the spectrum of our turned-to-11 sine wave run through \nthe soft clipper (see Figure 6.1.8). \nFigure 6.1.8. Spectral plot of hard-knee compressed sine, gain = 11, t = 0.2, r= \n0.4. \n \n \nIt almost works too well, in fact—the aliasing has almost disappeared, but so has much of \nthe harmonic distortion we were after in the first place. Well, maybe we can combine these \ntwo techniques. We could start out by implementing a hard-knee compressor, but instead of \nkeeping the ratio constant, we could make the ratio also vary with the magnitude of the \ninput sample. But now we‘re starting to do some expensive math. If we‘re going to go that \nfar, let‘s not play around. Let‘s go straight for the power tools and use a higher-order \npolynomial. \n\n\n \n \nPolynomials show up in games occasionally. They‘re sometimes used for audio sample rate \nconversion, for instance. In graphics and animation, they show up frequently in the guise of \nparametric splines. The polynomial we need is very similar to a spline, and we‘ll derive it in \nmuch the same way. \nUnfortunately for us, the splines used in graphics are all parameterized on the single \nvariable t, which is usually interpreted as a distance along the spline from the starting point. \nSo we‘ll do this the old-fashioned way, by solving a system of linear equations based on a \nset of constraints. Here‘s what we want our function to look like: \n1. The value of the function at x = 0 is 0, and the slope of the function is 1 to begin \nwith. Things don‘t get interesting until x is greater than or equal to a threshold \nvalue, which we‘ll call t. That means that the value of the function at x = t is t. \n2. The slope at t is still 1. \n3. At x = 1, the slope of the function is the compression ratio, which we‘ll call r. \n4. We probably want the slope to reach r before x = 1. Let‘s define a point k (for knee) \nwhere the slope of the function is r. \nThat‘s four constraints, which means we need a fourth-order polynomial to satisfy them. So \nlet‘s start by defining that function: \nf = ax4 + bx3 + cs2 + dx \nThat‘s the function we‘ll run on each input value (x) to get the output value we‘re looking \nfor. The function itself is pretty simple. The complicated question is, what are the values of \nthe coefficients a, b, c, and d? We get those by solving a system of equations that represent \nour constraints. So let‘s restate those constraints as linear equations in a, b, c, and d, by \nsubstituting x, r, t, and k into our basic equation f(x). The system of equations looks like \nthis: \nAt this point, we‘ll stop pretending to be good at math and turn things over to a symbolic \nsolver (in this case, Maplesoft‘s Maple 13) and ask it to solve for a, b, c, and d in terms of t, \nk, and r. The results are: \n \nYes, it really is that ugly. Fortunately, we only have to calculate the coefficients when t, k, \nor r actually changes, which shouldn‘t be too often. \nOne last thing before we call the distortion finished. It turns out that if we set the \nparameters so that the function sounds nice for normal-range audio, it starts to behave \n\n\n \n \nbadly when the amplitude is less than t or greater than k. We‘ll solve this by switching to a \nnormal hard-knee compressor function when the input is higher than t + k and ignoring the \ndistortion function completely when the input is less than t. \nNow we can start to play with this function—or rather, the piecewise function defined by our \nspline, the compressor ratio, and the inflection points t and k: \n \nIn Figure 6.1.9, notice how as the ratio decreases, the tops of the waves begin to look more \nsquared off—but instead of being completely flat, they get rounded off to the point of \nturning inside out. Visually, this effect seems a little odd. But in the audio domain, where a \nwave‘s frequency content is far more important than its shape, it sounds much better. Why? \nTake a look at the spectrum of our turned-up-to-11 sine wave, once we‘ve run its output \nthrough our new function (see Figure 6.1.10). \nFigure 6.1.9a. Transfer function with t = 0.2, k = 0.4, r varying from 0.9 to 0.4. \n \n \nFigure 6.1.9b. Transfer function with t = 0.2, k = 0.4, [0.9,0.4] applied to full-scale \nsine wave. \n \n",
      "page_number": 481
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 490-498)",
      "start_page": 490,
      "end_page": 498,
      "detection_method": "topic_boundary",
      "content": " \n \n \nFigure 6.1.10. Spectral plot of distorted sine, gain = 11, t = 0.2, k = 0.4, r = 0.4. \n \n \nThe new harmonics are loud and strong, but the aliased frequencies have all but \ndisappeared. (The aliased frequencies are in fact still present, but at such a low volume that \nthey don‘t show up at the resolution of this graph. For our purposes, they‘ve ceased to \nexist.) \n \nAutomatic Gain Control \nSo we‘ve got a decent-sounding distortion, but it has one major downside: The quality of \nthe distortion is heavily dependent on the volume of the input. That might not seem like a \ndrawback at first—after all, that‘s how distortion works in the real world. But it‘s a \nsignificant problem for game dialog. It‘s not always possible to normalize the volume across \nall dialog samples, so some lines of dialog will sound more distorted than others. Sadly, the \nloudest lines of dialog are invariably the most important or emotional lines—exactly the \nones you don‘t want to get lost in distortion. \nThere are a few different ways of dealing with this problem, but the one I‘ve had the most \nsuccess with is to fiddle with the volume before and after distortion. Decide on an ideal \naverage volume of samples to feed into the distortion effect. Then measure the actual \nvolume (or RMS, which we‘ll discuss in a moment) of your incoming samples. If the \nincoming RMS is lower than the ideal, then crank up the volume. If the incoming RMS is \nhigher, then turn it down. On the way out of the distortion effect, just apply the reverse of \nwhatever you did on the way in, so that the output volume stays about the same. \nFiddling with the volume like this is called automatic gain control, or AGC. It‘s a simple \neffect that‘s very popular in consumer recording devices. It‘s easy to implement. \nFirst, we calculate the root mean square (RMS) of the incoming samples. In theory, this is \nthe square root of the average of the square of all samples, or \n. In \npractice, we don‘t average all samples, because that would give too much weight to past \nvalues and not enough to more recent ones. Instead, we calculate a windowed RMS, which \n\n\n \n \nis the root mean square of a subset of samples, from the current sample n back to some \nprevious sample \n. Of course, we don‘t need to calculate \nthe complete sum for every sample—we just keep a running total. The C code for each \nsample looks something like this: \nfloat* rms, *y; \n \nconst int m; // window length \nfloat window[m]; \n \nfloat MS; // mean square \n \nfloat current = (y[n]* y[n]) / m; \nMS += current;            // add current value \nMS -= window[n % m];      // subtract oldest previous value \nwindow[n % m] = current;  // replace oldest value with current \nvalue \n \nrms[n] = sqrt(MS); // voila, root-mean-square \n \nThis gives an accurate RMS measurement, but if you‘re in a hurry, you can leave out the \nsquare and square root (thus calculating a windowed mean rather than a windowed RMS) \nand still get decent results. The important part is the moving average, which is effectively a \nlow-pass filter on our AGC inputs. \nOnce you‘ve calculated the input RMS, the rest of the AGC is simple. Let T be the target \nvolume. The AGC output is \n. In other words, we calculate the RMS as a \npercentage of the target and multiply each incoming sample by that percentage. \nOn the way out of the distortion effect, we want to readjust the volume to where it was \nbefore. We could just reverse our previous calculation and multiply by \n, but that \nturns out to be a bad idea. The problem is that the distortion effect has a large and not \nalways predictable effect on output volume. The volume of the output is a nonlinear function \nof the inputs. In non-mathematical terms, that means the output volume will be different in \nways that won‘t be easy to understand or correct. And in practical terms, that means your \nsound guy will not be happy with you. \nFortunately, we already have the tools we need to fix this problem. All we need is yet \nanother AGC. This time, instead of using a constant value for the target, we‘ll use the \nincoming RMS that we calculated before. The block diagram looks like Figure 6.1.11. \nFigure 6.1.11. Volume compensation by pre- and post-distortion AGC. \n \n\n\n \n \n \nOne last note on the AGC: As with anything involving division, there are numerical \ninstabilities in this formula. In particular, there‘s a singularity when the input volume is near \nzero. I‘ve dealt with this in two ways: either always adding an epsilon or clamping the input \nvolume to an arbitrary minimum. Both methods work equally well, in my opinion. The \nclamping method gives a little more headroom, so that‘s what I‘ve used in the code that \naccompanies this article. \n \nAdding Static \nA little bit of snap, crackle, and pop is the finishing touch on this effect. Static should be \nsubtle, so you can take some shortcuts here. Blending in a prerecorded noise sound is fine, \nbut unless it‘s a long wave, the loop points will become obvious. Depending on exactly what \nyour sound designer wants, it can be cheaper and more effective just to create your own. \nOne technique that works well: Take your floating-point input value, cast it to an int, \ninvert the bits, and divide by INT_MAX, like so: \nfloat noise = (float)(~(*(int*)& input)) / (float)INT_MAX; \n \nDrop that down to about –24 dB below the main outputs, and it sounds like a mixture of \nstatic and crosstalk. I personally love this effect, but it‘s not for everyone. It has a definite \ndigital-age sound to it, so I wouldn‘t suggest it for games set much earlier than 1990. \nMaking It Tinny \nThe finishing touch that really sells this effect is band-pass filtering. The sound has to be \nthin, like it‘s coming from a very small and not very well-made speaker. It‘s a simple effect \nto achieve. You‘ll want to make the exact parameters configurable, but a bandpass filter \nwith a 24-dB/octave roll off set at about 1 kHz makes a good starting point. \nDesigning this sort of filter is actually quite difficult. Fortunately, the heavy lifting has all \nbeen done long in the past. There is any number of excellent digital filter designs on the \nInternet, free for the taking. I recommend Robert Bristow-Johnson‘s excellent design, which \nis available at www.musicdsp.org/files/EQ-Coefficients.pdf. Try Bristow-Johnson‘s band-pass \nEQ, set to a center frequency of 1 kHz and a Q of 4.5. \n \nPutting It All Together \nFigure 6.1.12 shows the block diagram as implemented in the code accompanying this \narticle. \nFigure 6.1.12. Complete block diagram of radio effect. \n\n\n \n \n \nThis configuration puts the band-pass filter at the end, so that the sound is easier to fit into \na busy mix. You may want to try other configurations. For instance, if the radio is the main \nor the only sound playing, you‘ll get a fuller sound by putting the filter directly after the \ndistortion instead of at the end. Or you can double the filter for that extra-tinny sound. \nFinally, don‘t forget to add a limiter at the end of your mix. \n \nParameter Animation \nThe best part about applying a radio effect in-game, rather than baking it into your audio \nbeforehand, is that it gives you an opportunity to animate the parameters. Varying the \nsettings of your radio effect, either cyclically over time or dynamically in response to in-\ngame parameters, makes the audio much more organic and unpredictable. For instance: \n \nIncrease the distortion AGC’s gain target as the sound source gets further \nfrom the receiver. The effect is to add another distance/occlusion cue to the sound. \n \nLink the center frequency of the band-pass filter to a low-frequency \noscillator. This is a cheap way to get a phasing effect similar to an out-of-tune AM \nradio station. \n \nAnimate the ratio and knee of the distortion effect. I love this technique \nbecause it adds motion to the sound in a subtle and non-obvious way. Be careful, \nthough: A little of this goes a long way. \nSinusoidal low-frequency oscillators—LFOs—are extremely cheap to run. They require only \ntwo fused multiply-adds per sample and have no real storage needs, which means they can \nbe easily interleaved with other processing. The technique takes advantage of the fact that \nthe cosine and sine functions are derivatives of each other: \nsin(x)\n = cos(x) and cos(x)\n = – sin (x). As long as the frequency is low enough, you \ncan just: \n1. Scale the previous frame‘s sine and cosine values by the per-frame step (2π \n*frequency). \n2. Increment the sine by the scaled cosine value. \n3. Decrement the cosine by the scaled sine value. \nThat‘s all there is to it. This method falls apart at audio frequencies, but for LFOs it‘s \nremarkably stable. \n \nConclusion \nThe effect presented here isn‘t an accurate simulation of real-world electronics. But it‘s \npractical, relatively low cost, and effective. Most important, it‘s configurable and easy to \n\n\n \n \nuse. The polynomial waveshaper gives it a unique sound, and the dual AGCs make it easy to \ndrop into the mix. It‘s shipped in two triple-A titles that I know of, and I hope to see it ship \nin many more. \n \n6.2. Empowering Your Audio Team with a Great Engine \nMat Noguchi, Bungie \nmatthewn@bungie.com \nMaking award-winning game audio at Bungie isn‘t just about the using best technology or \nhaving the best composers (although that doesn‘t hurt). The best technology will ring flat \ngiven poor audio, and the best music will sound out of place given poor technology. If you \nreally want your game to sing, you need to put audio in the control of your audio team. \nFor the past nine years, with a core set of principles, a lot of code, and even more content, \nBungie has empowered its audio team to make masterpieces. This gem will explore the \naudio engine that drives Halo, from the basic building blocks the sound designers use to the \ninteresting ways the rest of the game interacts with audio. We will also take a peek at the \npost-production process to see how everything comes together. \nAudio Code Building Blocks \nThe sound engine starts with the s_sound_source. \n   enum e_sound_spatialization_mode \n   { \n       _sound_spatialization_mode_none, \n       _sound_spatialization_mode_absolute, \n       _sound_spatialization_mode_relative \n   }; \n \n   struct s_sound_source \n   { \n       e_sound_spatialization_mode spatialization_mode; \n       float scale; \n \n       // only valid if spatialization_mode is absolute. \n       point3d position; \n       quaternion orientation; \n       vector3d translational_velocity; \n   }; \n \nThis structure encompasses all the code-driven behavior of sound. You have your typical \npositional audio parameters, a fade on top of the default volume, some stereo parameters, \nand a single value called scale. What is scale? \nThe scale value is used to parameterize data from the game engine to the audio engine. It \nis normalized to lie within [0, 1], making it simple to use as an input into a function or linear \nrange. Everything that can play a sound in our game exports at least one scale value, if not \nmore. As a simple example, sounds that get generated from particle impacts receive a scale \nnormalized between 0.5 and 1.5 world units/second. A more complex example would be the \n\n\n \n \nsounds that play when a Banshee banks sharply and forms contrails at the wing. The actual \nscale that gets exported is shown in Figure 6.2.1 in our object editor. \nFigure 6.2.1. An object function from the Warthog for the engine sound. \n \nThis is an example of an object function; it takes various properties exported by an object \nand combines them into a single value that can be sent to the sound system. Incidentally, \nwe drive our shaders in a similar way, although shaders can use more than one input. \nIn general, simple things such as impacts and effects export a single scale. Objects such as \nthe Warthog and Brute can export a combination of multiple scales. \nParameterizing audio with a single value may seem a bit simplistic. However, as we‘ll \nexplore later, we tend to parameterize only a few properties of a sound based on scale, and \nin almost all cases it makes sense to parameterize multiple properties in a coupled fashion. \nFor spatialized audio, we have a separate distance envelope that we‘ll describe in the next \nsection. \n \nSound Parameterization \n\n\n \n \nGiven that we can send the audio engine interesting data from the game, we need to author \ncontent to use this data (that is, the scale and distance). The audio designers export .AIFF \nfiles, which get converted into the native platform format (XBADPCM for Xbox and XMA2 for \nXbox 360), and they attach in-game metadata through our custom game content files called \ntags. Sound content breaks down into one of two categories: impulse sounds and looping \nsounds. \nImpulse Sounds \nFor impulse sounds, such as impacts, gunshots, and footsteps, we allow the audio designers \nto adjust gain and pitch with the scale shown in Figure 6.2.2. \nFigure 6.2.2. Scale parameter editor. \n \n(Side note: Having your data use units that the audio team understands goes a long way to \nmaking them feel at home with the data they have to work with!) \nFor spatialized audio, we also can specify a distance envelope, as shown in Figure 6.2.3. \nFigure 6.2.3. Distance envelope editor. \n \nFrom the sound source origin to the ―don‘t play distance,‖ the sound is silent. From ―don‘t \nplay‖ to ―attack distance,‖ the sound scales from silence to full volume. Between ―attack \ndistance‖ and ―minimum distance,‖ the sound plays at full volume. And from ―minimum \ndistance‖ to ―maximum distance,‖ the sound scales from full volume back to silence. \nThe audio designers use the attack distance primarily for sound LODs. You can hear this for \nyourself in any Halo 3 level: A sniper rifle firing far away sounds like a muffled echo, while \nthe sniper rifle firing up close has the crisp report of a death machine. See Figure 6.2.4. \nFigure 6.2.4. Distance envelopes for the sniper rifle gunshot. \n\n\n \n \n \n \nImpulse sounds can also be parameterized based on the total number of instances of that \nsound playing. For example, when glass breaks, it can form a few or a lot of broken glass \nparticles. A lot of glass hitting a concrete floor sounds much different than a little; \nattempting to replicate that sound by playing a lot of the same glass impact sound does not \nwork without a prohibitively large variety of sounds. \nTo combat this, we allow sounds to ―cascade‖ into other sounds as the total number of \nsounds hits a certain threshold. For glass, the sound tag can specify a set of promotion rules \n(see Figure 6.2.5). \nFigure 6.2.5. Broken glass particle promotion rules. \n \n\n\n \n \nThese promotion rules are defined in the order that they should play at run time. For each \nrule, you can specify which kind of sound to play (for example, few glass pieces, many glass \npieces) as well as how many instances of that kind can play before you start the next rule. \nEach rule can also contain a timeout to suppress all sounds from previous rules. \nUsing the rules from Figure 6.2.5, if we played five glass sounds at once, we would play four \ninstances of the breakable_glasspieces_single sounds. When the fifth sound \nplayed, we would play a breakable_glass_few sound and stop the previous four \nbreakable_glasspieces_single sounds. If we then managed to play four more \nbreakable_glass_few sounds in the same way (such that they were all playing at \nonce), we would play a breakable_glass_many sound, stop the previous \nbreakable_glass_few sounds, and then suppress any future glass sound for two \nseconds. \nCascading sounds allow us to have an expansive soundscape for particle impacts without \nplaying a prohibitive number of sounds at once. \nLooping Sounds \nA sound that does not have a fixed lifetime (such as engine sounds, dynamic music, or \nambience) is created using looping sounds. Because looping sounds are dynamic, we allow \ntheir playback to be controlled with a set of events: start, stop, enter alternate state, and \nexit alternate state. (More on alternate state in a bit.) Since these events are really just \nstate transitions, we need just two more bits for playing looping sounds: one bit for whether \nthe loop should be playing and one bit for whether it should be in the alternate state. For \neach event, as well as the steady state of normal playing and alternate playing, the audio \ndesigners can specify a sound. In the steady state when a looping sound is playing, we \nsimply keep playing the loop sound. It‘s usually authored such that it can play forever \nwithout popping. For transition events (start, stop, enter alternate, exit alternate, and stop \nduring alternate), those sounds either can be queued up to play after the loop or can play \non top of the currently playing loop. \nFigure 6.2.6. Looping sound state diagram. \n \n \n(―Alternate‖ is really a way of saying ―cool.‖ During the development of Halo 1, the audio \ndirector Marty O‘Donnell asked for a way to have a cool track for music, so we added the \nalternate loop.) \n",
      "page_number": 490
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 499-510)",
      "start_page": 499,
      "end_page": 510,
      "detection_method": "topic_boundary",
      "content": " \n \nDynamic music is implemented with just a few script commands: start, stop, and set \nalternate <on/off>. \nVehicle engines are implemented with looping sounds; however, in order to capture more \nintricacies with how engines sound under various loads (in other words, cruising at a low \nspeed sounds much different than flooring the accelerator), we use something similar to the \ncascade system to select different sounds to play based on scale: the pitch range. (In fact, \nas an implementation note, cascades are implemented referencing pitch ranges.) \nAs the name implies, a pitch range specifies a certain range of pitches to play in (for \nexample, only play this pitch range when the sound is playing from –1200 cents to 1200 \ncents). There are many playback parameters for that pitch range, such as distance \nenvelopes and relative bend. Relative bend is the bend applied to the permutation playing \nfrom this pitch range based on a reference pitch. In the example in Figure 6.2.7, if we were \nplaying the sound with a scale-based pitch of 55 cents, the idle pitch range sounds would \nplay with an actual pitch of –110 cents (pitch – reference pitch). The ―playback bends \nbounds‖ simply clamps the pitch to those bounds before calculating the actual pitch. \nFigure 6.2.7. Pitch range editor. \n \nThis is probably more complicated than it needs to be, since we are basically parameterizing \nthe pitch, then using that to select a pitch range, then converting that back into a relative \nbend to play sounds from that pitch range. But that‘s more a historical artifact than \nanything else, and now the audio designers are used to it. \nAt run time, you can have multiple pitch ranges from a single loop playing at once (see \nFigure 6.2.8). \nFigure 6.2.8. Warthog pitch ranges. Actual gain is displayed as power (gain2). \n\n\n \n \n \n \nThis allows for smooth cross-fading between multiple pitch ranges based on the input scale. \nThe looping sound system has been powerful enough to add novel uses of sound without \nadditional modifications. For example, in Halo 2, we added support for continuous collisions \n(for example, the sound a boulder makes rolling or sliding down a hill) from Havok by \ngenerating a looping sound at run time whenever we registered an object rolling or sliding; \nwe mapped the normal loop to rolling and the alternate loop to sliding so that a single \nobject transitioning between rolling and sliding would have a smooth audio transition \nbetween those states. \nThis kind of flexibility makes it very easy for the audio designers to collaborate with other \nprogrammers without necessarily having to involve an audio programmer. If you can export \na scale value, you can easily add either an impulse or a looping sound to whatever it may \nbe. \n \nMixing \nOne powerful aspect of Bungie‘s audio engine is how well it is integrated into the overall \ngame engine; everything that should make a sound can make a sound, from weapons firing, \nto objects rolling and bouncing, to the various sounds in the HUD based on in-game events. \nOne daunting aspect of the Halo audio engine is that almost everything makes a sound in \nsome way, which means the audio designers have to make a lot of audio content. \nTo make it easier to manage sound across the entirety of a game, we assign every sound a \nsound class, essentially a label we use to define a set of default sound properties, such as \ndistance envelope, Doppler effect multiplier, and so on. The properties in the sound class \nwill be applied to all sounds with that sound class by default, so the audio designers only \nhave to tweak a few sounds here and there. \nListing 6.2.1. A non-exhaustive listing of sound classes \nprojectile_impact \nprojectile_detonation \nprojectile_flyby \n\n\n \n \nprojectile_detonation_lod \n \nweapon_fire \nweapon_ready \nweapon_reload \nweapon_empty \n \nobject_impacts \nparticle_impacts \nweapon_fire_lod \n \nunit_footsteps \nunit_dialog \nunit_animation \n \nvehicle_collision \nvehicle_engine \nvehicle_animation \nvehicle_engine_lod \n \nmusic \nambient_nature \nambient_machinery \nambient_stationary \nhuge_ass \n \nmission_dialog \ncinematic_dialog \nscripted_cinematic_foley \n \nWe also use sound classes to control the mix dynamically at run time. For each sound class, \nwe store an additional attenuation to apply at run time—essentially, a sound class mix. \nThese values can be script-driven; for example, during cinematics, we always turn down the \nambience sound classes to silence with the following script call: \n  (sound_class_set_gain \"amb\" 0 0) \n \nWe use a simple LISP-like scripting language. With the sound_class script commands, \nwe use the string as a sound class substring match, so this script command would affect the \ngain (in amplitude) for all sounds that have ―amb‖ in them. If we had a sound class called \n―lambchop‖ it would also affect that, but we don‘t. \nIn addition to manually setting the mix, under certain gameplay conditions, we can activate \na predefined sound class mix. For example, if we have Cortana saying something important \nto you over the radio, we‘ll activate the spoken dialog mix. These mixes, which are \nautomatically activated, fade in and out over time so that the change in volume doesn‘t \npop. The scripted mix and dynamic mix are cumulative; it‘s simple, but that tends to match \nthe expected behavior anyway. \n \n\n\n \n \nPost-Production \nThe bulk of audio production is spent in creating and refining audio content. This is a lot of \nwork, but it‘s fairly straightforward: Create some sound in [insert sound application here], \nplay it in game, tweak it, repeat. However, as the project gets closer to finishing, the audio \nteam has two major tasks left: scoring the game and finalizing the overall mix. \nScoring any particular level is a collaborative process between the composer and the level \ndesigner. The composer works with the level designer to determine music triggers based on \ngameplay, progression, or anything else that can be scripted. (The endgame driving \nsequence of Halo 3 has three triggers: one when you start driving on the collapsing ring, \none after Cortana says ―Charging to 50 percent!‖, and one when you make the final \nWarthog jump into the ship.) Each trigger can specify what looping sound to play, whether \nto use the regular or alternate loop, and when to start and stop. The composer can then \nwork alone to determine the appropriate music to use for the entire level. This collaborative \neffort allows the composer to remain in creative control of the overall score for a level while \nallowing the level designer to provide the necessary hooks in his script to help create a \ndynamic musical score. \nThere is also a chunk of time set aside at the end of production for the audio team to work \nwith finalized content. At this point all the graphics, cinematics, scripts, levels, animations, \nand so on, are locked down; this allows the audio team to polish without needing to worry \nabout further content changes invalidating their work. Once all the sound is finally in place, \nthe audio team then plays through the entire game in a reference 5.1 studio to adjust the \nfinal mix and make sure everything sounds great. \n \nConclusion \nBungie‘s audio engine isn‘t just a powerful engine; it‘s a powerful engine that has continued \nto evolve over time. Many of the concepts and features presented in this gem have been \naround since the first Halo game. Having a mature audio engine means that the entire audio \nteam can iterate the process of making game audio instead of having to reinvent technology \nfrom scratch. Many of the innovations in Bungie‘s audio engine have come from the audio \ndesigners, not just the programmers. In Halo 2, they came up with coupling the \nenvironment ambience loops with the state of the weather so that when a level transitioned \nto rain, so would the ambience. In Halo 3, they suggested the attack portion of the distance \nenvelope to support sound LODs. \nIn other words, Bungie‘s audio engine is not just about technology; it‘s about enabling \neveryone who works on audio to do great things. Any programmer who wants to add sound \nto their feature just needs to use the s_sound_source. Any audio designer can custom \ntailor the playback of any sound with a huge amount of flexibility and functionality. And with \nour mature and proven audio engine, an audio programmer has the framework to add \nfunctionality that can be used right away, in infinite variety. \nThe trifecta of lots of content, a fully integrated sound engine, and an effective audio \nproduction process, combined with Bungie‘s talented audio team, forms an award-winning \ngame audio experience. The numerous accolades Bungie has received for audio for the \nentire Halo series show that our approach to game audio works—and works well. \n \n6.3. Real-Time Sound Synthesis for Rigid Bodies \nZhimin Ren and Ming Lin \n\n\n \n \nzren@cs.unc.edu \nIn recent 3D games, complex interactions among rigid bodies are ubiquitous. Objects collide \nwith one another, slide on various surfaces, bounce, and roll. The rigid-body dynamic \nsimulation considerably increases the engagement and excitation levels of games. Such \nexamples are shown in Figure 6.3.1. However, without sound induced by these interactions, \nthe synthesized interaction and virtual world are not as realistic, immersive, or convincing \nas they could be. \nFigure 6.3.1. Complicated interactions among rigid bodies are shown in the two \nscenes above. In this gem, we introduce how to automatically synthesize sound \nthat closely correlates to these interactions: impact, rolling, and sliding. \n \n \nAlthough automatically playing back pre-recorded audio is an effective way for developers \nto add realistic sound that corresponds well to some specified interactions (for example, \ncollision), it is not practical to pre-record sounds for all the potential complicated \ninteractions that are controlled by players and triggered at run time. \nSound that is synthesized in real time and based on the ongoing physics simulation can \nprovide a much richer variety of audio effects that correspond much more closely to \ncomplex interactions. \nIn this gem, we explore an approach to synthesize contact sounds induced by different \ntypes of interactions among rigid bodies in real time. We take advantage of common \ncontent resources in games, such as triangle meshes and normal maps, to generate sound \nthat is coherent and consistent with the visual simulation. During the pre-processing stage, \nfor each arbitrary triangle mesh of any sounding object given as an input, we use a modal \nanalysis technique to pre-compute the vibration modes of each object. At run time, we \nclassify contact events reported from physics engines and transform them into an impulse \nor sequence of impulses, which act as excitation to the modal model we obtained during \npre-processing. The impulse generation process takes into consideration visual cues \nretrieved from normal maps. As a result, sound that closely corresponds to the visual \nrendering is automatically generated as the audio hardware mixes the impulse responses of \nthe important modes. \nModal Analysis and Impulse Responses \nIn this section, we give a brief overview on the core sound synthesis processes: modal \nanalysis and modal synthesis. Both of them have been covered in previous Game \nProgramming Gems. More details on modal analysis can be found in Game Programming \nGems 4 [O‘Brien04], and the impulse response to sound calculation (modal synthesis) is \ndescribed in Game Programming Gems 6 [Singer06]. \n\n\n \n \nFigure 6.3.2 shows the pipeline of the sound synthesis module. \nFigure 6.3.2. In pre-processing, a triangle mesh is converted into a spring-mass \nsystem. Then modal analysis is performed to obtain a bank of vibration modes for \nthe spring-mass system. During run time, impulses are fed to excite the modal \nbank, and sound is generated as a linear combination of the modes. This is the \nmodal synthesis process. \n \n \n \nSpring-Mass System Construction \nIn the content creation process for games, triangle meshes are often used to represent the \n3D objects. In pre-processing, we take these triangle meshes and convert them into spring-\nmass representations that are used for sound synthesis. We consider each vertex of the \ntriangle mesh as a mass particle and the edge between any two vertices as a damped \nspring. The physical properties of the sounding objects are expressed in the spring \nconstants of the edges and the masses of the particles. This conversion is shown in \nEquation (1), where k is the spring constant, Y is the Young‘s Modulus that indicates the \nelasticity of the material, t is the thickness of the object, mi is the mass of particle i, p is the \ndensity, and ai is the area covered by particle i. \nEquation 1  \n \n \nFor more details on the spring-mass system construction, we refer our readers to \n[Raghuvanshi07]. \nSince this spring-mass representation is only used in audio rendering and not graphics \nrendering, the triangle meshes used in this conversion do not necessarily have to be the \nsame as the ones used for graphics rendering. For example, when a large plane can be \nrepresented with two triangles for visual rendering, these two triangles do not carry detailed \ninformation for approximating the sound of a large plane. (This will be explained later.) In \nthis case, we can subdivide this triangle mesh before the spring-mass conversion and use \nthe detailed mesh for further sound computation. On the contrary, sometimes high-\n\n\n \n \ncomplexity triangle meshes are required to represent some visual details, but we are not \nnecessarily able to hear them. In this scenario, we can simplify the meshes first and then \ncontinue with sound synthesis–related computation. \nModal Analysis \nNow that we have a discretized spring-mass representation for an arbitrary triangle mesh, \nwe can perform modal analysis on this representation and pre-compute the vibration modes \nfor this mesh. Vibration of the spring-mass system created from the input mesh can be \ndescribed with an ordinary differential equation (ODE) system as in Equation (2). \nEquation 2  \n \n \nwhere M, C, and K are the mass, damping, and stiffness matrix, respectively. If there are N \nvertices in the triangle mesh, r in Equation (1) is a vector of dimension N, and it represents \nthe displacement of each mass particle from its rest position. Each diagonal element in M \nrepresents the mass of each particle. In our implementation, C adopts Rayleigh damping \napproximation, so it is a linear combination of M and K. The element at row i and column j \nin K represents the spring constant between particle iand particle j. f is the external force \nvector. The resulting ODE system turns into Equation (3). \nEquation 3  \n \n \nwhere M is diagonal and K is real symmetric. Therefore, Equation (3) can be simplified into \na decoupled system after diagonalizing K with K = GDG–1, where D is a diagonal matrix \ncontaining the Eigenvalues of K. \nThe diagonal ODE system that we eventually need to solve is Equation (4). \nEquation 4  \n \n \nwhere Z = G–1r, a linear combination of the original vertex displacement. The general \nsolution to Equation (4) is Equation (5). \nEquation 5  \n \n \n\n\n \n \nwhere λ is the i‘th Eigenvalue of D. With particular initial conditions, we can solve for the \ncoefficient ci and its complex conjugate, \n. The absolute value of ωi‘s imaginary part is the \nfrequency of that mode. Therefore, the vibration of the original triangle mesh is now \napproximated with the linear combination of the mode shapes zi. This linear combination is \ndirectly played out as the synthesized sound. Since only frequencies between 20 Hz to 20 \nkHz are audible to human beings, we discard the modes that are outside this frequency \nrange. \nImpulse Response Calculation \nWhen an object experiences a sudden external force f that lasts for a small duration of time, \nΔt, we say that there is an impulse fΔt applied to the object. f is a vector that contains \nforces on each particle of the spring-mass system. This impulse either causes a resting \nobject to oscillate or changes the way it oscillates; we say that the impulse excites the \noscillation. Mathematically, since the right-hand side of Equation (4) changes, the solution \nof coefficients ci and \nalso changes in response. This is called the impulse response of the \nmodel. \nThe impulse response, or the update rule of ci and \n, for an impulse fΔt, follows the rule \nexpressed in Equation (6): \nEquation 6  \n \n \nwhere gi is the i‘th element in vector G–1f. Whenever an impulse acts on an object, we can \nquickly compute the summation of weighted mode shapes of the sounding object at any \ntime instance onward by plugging Equation (6) into Equation (5). This linear combination is \nwhat we hear directly. With this approach, we generate sound that depends on the sounding \nobjects‘ shape and material, and also the contact position. \nIn conclusion, we can synthesize sound caused by applying impulses on preprocessed 3D \nobjects. In the following section, we show how to convert different contact events in physics \nsimulation into a sequence of impulses that can be used as the excitation for our modal \nmodel. \n \nFrom Physics Engine to Contact Sounds \nWhen any two rigid bodies come into contact during a physics simulation, the physics \nengine is able to detect the collision and provide developers with information pertaining to \nthe contact events. However, directly applying this information as excitation to a sound \nsynthesis module does not generate good-quality sound. We describe a simple yet effective \nscheme that integrates the contact information and data from normal maps to generate \nimpulses that produce sound that closely correlates with visual rendering in games. \nEvent Classification: Transient or Lasting Contacts? \nWe can imagine that transient contacts can be easily approximated with single impulses, \nwhile lasting contacts are more difficult to represent with impulses. Therefore, we handle \n\n\n \n \nthem differently, and the very first step is to distinguish between a transient and a lasting \ncontact. The way we distinguish the two is very similar to the one covered in [Sreng07]. \nTwo objects are said to be contacting if their models overlap in space at a certain point p, \nand if vp • np <, 0, where vp and np are their relative velocity and contact normal at point \np. Two contacting objects are said to be in lasting contact if vt ≠ 0, where vt is their relative \ntangential velocity. Otherwise, they are in transient contact. \nFrom Transient Contacts to Impulses \nWhen our classifier detects that there are only transient contacts, corresponding single \ninstances of impulses are added to the modal model. The impulse magnitude is scaled with \nthe contact force at the contact point. Developers can specify the scale to control transient \ncontact sound‘s volume. The direction of the impulse is the same as that of the contact \nforce, and it is applied to the nearest neighboring mass particle (in other words, vertex of \nthe original triangle mesh) of the contact point. We keep a k-d tree for fast nearest \nneighbor searches. \nThe following pseudocode gives some more details of this process. \n    IF a transient contact takes place THEN \n          COMPUTE the local coordinates of the contact point \n          FIND the nearest neighbor of the contact in local \nframe \n          CREATE impulse = contactForce * scale \n          CLEAR the buffer that stores impulse information \n          ADD the new impulse to the nearest neighbor vertex \n          UPDATE the coefficients of mode shapes \n    ENDIF \n \n \nFrom Lasting Contacts to Impulses: Using Normal Maps \nWhen our classfier sees a lasting contact (for example, sliding, scraping, and so on), the \ncontact reports from common real-time physics engines fall short for directly providing us \nwith the contact forces for simulating the sliding sound. If we directly add impulses \nmodulated with the contact force, we would not hear continuous sound that corresponds to \nthe continuous sliding motion. Instead, we would ―hear‖ all the discrete impulses that are \napplied to the object to maintain the sliding. One of the problems is the significant gap \nbetween physics simulation rate (about 100 Hz) and audio sampling rate (44,100 Hz). \nWe solve this problem with the use of normal maps, which exist in the majority of 3D \ngames. We generate audio from this commonly used image representation for sound \nrendering as well as by noting the following observations: \n \nNormal maps can add very detailed visual rendering to games, while the underlying \ngeometries are usually simple. However, common physics engines do not see any \ninformation from normal maps for simulation. Therefore, by only using contact \ninformation from physics engines, we miss all the important visual cues that are \noften apparent to players. \n \nNormal maps give us per-pixel normal perturbation data. With normal maps of \ncommon resolutions, a pixel is usually a lot smaller than a triangle of the 3D mesh. \nThis pixel-level information allows us to generate impulses that correspond closely to \nvisual rendering at a much higher sampling rate. This effect would have been \n\n\n \n \nimpossible to achieve with common 3D meshes even if the physics engine were to \ncalculate the contact force faithfully. \n \nNormal maps are easy to create and exist in almost every 3D game. \nFigure 6.3.3 shows a pen scraping against three normal-mapped flat surfaces. The flat \nplanes no longer sound flat after we take the normal map information as input to our \nimpulse generation for sound synthesis. \nFigure 6.3.3. A pen scraping on three different materials: brick, porcelain, and \nwood. The underlying geometries of the surfaces are all flat planes composed of \ntriangles. With normal maps applied on the surface, the plane looks bumpy. And it \nalso sounds bumpy when we scrape the pen on the surfaces! \n \n \nWe take the normal maps and calculate impulses in the following way. \nImagine an object in sliding contact with another object, whose surface F is shown in Figure \n6.3.4; the contact point traverses the path P within a time step. We look up the normal map \nassociated to F and collect those normals around P. The normals suggest that the high-\nresolution surface looks like f in Figure 6.3.4 and that the contact point is expected to \ntraverse a path P on f. Therefore, besides the momentum along the tangential direction of \nF, the object must also have a time-varying momentum along the normal direction of F, \nnamely pN, where N is the normal vector of F. From simple geometry, we compute its value \nwith Equation (7). \nEquation 7  \n \n \nwhere m is the object‘s mass and vT is the tangential velocity of the object relative to F. The \nimpulse along the normal direction JN that applies on the object is just the change of its \nnormal momentum expressed in Equation (8) \nEquation 8  \n \n \nwhen the object moves from pixel i to pixel j on the normal map. With this formulation, the \nimpulses actually model the force applied by the normal variation on the surface of one \nobject to another, generating sound that naturally correlates with the visual appearance of \nbumps from textures. \n\n\n \n \nFigure 6.3.4. Impulse calculation. (a) The path P traced by an object sliding \nagainst another object within one physics simulation time step. Each square is a \npixel of the normal map bound to the surface. ni are the normals stored in the \nnormal map around the path. The path lies on the surface F, which is represented \ncoarsely with a low-resolution mesh (here a flat plane). (b) The normal map \nsuggests that the high-resolution surface looks like f, and the object is expected to \ntraverse the path P. (c) The impulse along the normal direction can be recovered \nfrom the geometry configuration of n, N, and VT. \n \n \nAt the end of each time step, we collect impulses that should have taken place in this time \nstep by tracing back the path the object took. We make up these impulses in the immediate \nfollowing time step by adding them into our impulse queue at the end of this time step. The \nfunction for doing this is shown below. \n/** \n* \\param[in] endX the x coordinate of ending position \n* \\param[in] endY the y coordinate of ending position \n* \\param[in] endTime time stamp of the end of this time step \n* \\param[in] elapsedTime time elapsed from last time step \n* \\param[in] tangentialVelocity tagential velocity of the object \n* \\param impulseEventQueue gets updated with the new impulses \n*/ \nvoid CollectImpulses(float endX, float endY, \nfloat endTime, float elapsedTime, Vec3f tangentialVelocity, \nImpulseEventQueue & impulseEventQueue) \n{ \n      float speed = tangentialVelocity.Length(); \n      float traveledDistance = elapsedTime * speed; \n \n// dx is the width of each pixel \n// We assume pixels are square shaped, so dx = dy \n// +1 to ensure nGridTraveled >= 1 \n      int nGridTraveled = int(traveledDistance / dx) + 1; \n \n// Approximate the time used for going through one pixel \n// Divided by 2 to be conservative on not missing a pixel \n      // This can be loosen if performance is an issue \n      float dtBump = ( elapsedTime / nGridTraveled ) / 2; \n      float vX = tangentialVelocity.x, vY = \ntangentialVelocity.y; \n\n\n \n \n \n// Trace back to the starting point \n      float x = endX - vX * elapsedTime, y = endY - vY * \nelapsedTime; \n      float startTime = endTime - elapsedTime; \n      float dxTraveled = vX * dtBump, dyTraveled = vY * dtBump; \n \n// Sample along the line segment traveled in the elapsedTime \n      for(float t = startTime; t <= endTime; t += dtBump) \n{ \n            ImpulseEvent impulseEvent; \n \n            // Compute the impulse from the normal map value \n           impulseEvent.impulse = \nGetImpulseAt(x, y, tangentialVelocity); \n           if (impulseEvent.Impulse.Length() == 0) \n                  continue; \n \n            // Update impulseEvent and \n// add this impulse to impulse queue \n            impulseEvent.Time = t; \n            impulseEvent.x = x; \nimpulseEvent.y = y; \n            impulseEventQueue.push_back(impulseEvent); \n            x += dxTraveled; \ny += dyTraveled; \n      } \n} \n \nNotice that we are assuming the object has constant velocity in one time step when we \ntrace back its path. The impulses in the impulse queue updated here are taken out of the \nqueue and added sequentially to the modal model in the next time step. When we ―play \nback‖ these impulses, we look at their time stamps (the variable shown in the code as \nimpulseEvent.Time) and add the excitation exactly the same time in the next time \nstep. Although there will be a delay of one time step, humans are not able to detect it \naccording to [Guski03]. However, humans are able to detect the impression of all the \nimpulses played at different time in one time step. This approach gives a finer \napproximation of the sound responses to the visual cues. \nWe refer our readers to [Ren10] for more details on contact sound synthesis for normal-\nmapped models. \n \nPutting It Together \nThe complete pipeline (as shown in Figure 6.3.5) is composed of the interaction handling \nand sound synthesis modules. The sound synthesis module consists of the modal analysis \nand modal synthesis processes and the interaction handling, which converts a contact event \ninto impulses. The sound synthesis module calculates the mode shapes at audio sampling \nrate (44.1 kHz), while interaction handling runs at the same rate as the physics engine \n(about 100 Hz). However, the impulses generated from normal maps are played back at a \nhigher frequency than the physics engine. \n",
      "page_number": 499
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 511-519)",
      "start_page": 511,
      "end_page": 519,
      "detection_method": "topic_boundary",
      "content": " \n \nFigure 6.3.5. The complete pipeline for the sound synthesis algorithm discussed in \nthis gem. \n \n \n \n \nConclusion \nThis gem describes a pipeline for automatically synthesizing sounds that correspond well to \nthe visual rendering in games. This method utilizes graphics resources such as triangle \nmeshes and normal maps to produce sound for different kinds of interactions. It renders \naudio effects that are tightly related to the visual cues, which otherwise cannot be captured \nat all. The method is general, works well with most game engines, and runs in real time. \n \nReferences \n[Guski03] Guski, Troje. ―Audiovisual Phenomenal Causality.‖ Perception and Psychophysics \n65.5 (2003): 789–800. \n[O‘Brien04] O‘Brien, James F. ―Modal Analysis for Fast, Stable Deformation.‖ Game \nProgramming Gems 4. Boston: Charles River Media, 2004. 287-298. \n[Raghuvanshi07] Raghuvanshi, Lin. ―Physically Based Sound Synthesis for Large-Scale \nVirtual Environments.‖ IEEE Computer Graphics and Applications 27.1 (2007): 14–18. \n[Ren10] Ren, Z., H. Yeh, and Lin. ―Synthesizing Contact Sounds Between Textured Models.‖ \nIEEE Virtual Reality. 2010. \n[Singer06] Singer, Marq. ―Real-Time Sound Generation from Deformable Meshes.‖ Game \nProgramming Gems 6. Boston: Charles River Media, 2006. 541–547. \n[Sreng07] Sreng, J., F. Bergez, J. Legarrec, A. L ecuyer, and C. Andriot. ―Using an Event-\nBased Approach to Improve the Multimodal Rendering of 6dof Virtual Contact.‖ Proceedings \nof the 2007 ACM Symposium on Virtual Reality Software and Technology. 165–173. \n \nSection 7: General Purpose Computing on GPUs \nIntroduction \n\n\n \n \nUsing Heterogeneous Parallel Architectures with OpenCL \nPhysX GPU Rigid Bodies in Batman: Arkham Asylum \nFast GPU Fluid Simulation in PhysX \nIntroduction \nAdam Lake, Sr. Graphics Software Architect, Advanced Visual Computing, Intel \nadam_t_lake@yahoo.com \nThis is a new chapter for the Game Programming Gems series. For the past decade we‘ve \nseen academic literature filled with papers about using GPUs for various non-graphics tasks, \nin particular those that have a high ratio of compute-to-memory-bandwidth density. I \nbelieve we are at a new stage of this development, a time when this work migrates from the \nacademic literature to use in production software. This hypothesis is validated by the \nexciting developments in both hardware and industry APIs supporting development on \nGPUs. The new gold rush is throughput computing, and GPUs will be at the forefront. \nGraphics and gaming will play a critical role in putting this computational horsepower into \nthe hands of researchers, developers, and consumers at a cost that will create new \nindustries, ecosystems, and value for society as a whole. \nThe first article in this section discusses OpenCL, an industry standard for harnessing the \ncomputational horsepower of both CPUs and GPUs in today‘s heterogeneous computing \nenvironment. OpenCL contains APIs and a programming model that allows developers to \nexpress both task and data parallelism in their applications in a cross-platform API. The \nauthor summarizes the basic principles of OpenCL and uses examples to discuss how to use \nOpenCL in a performant way. I‘d expect to see this standard evolve over the next few years \nand to be leveraged for areas of both the game development process and in the game \nengines we ship in the not-too-distant future. \nWe‘ve also included two articles on PhysX, one focused on rigid bodies and the other on \nfluid simulation used in the game Batman: Arkham Asylum. Fluid simulation was used to \nadd smoke and fog, and the rigid body simulation was used to destroy the wall panels in the \nScarecrow levels. The fluid simulation was done by leveraging the CPU to implement a \nparticle system and simulate the fluid with Smoothed Particle Hydrodynamics (SPH). Rigid \nbody simulation is performed using a rigid body simulation implemented on the GPU. \nI hope these gems inspire game developers to think about other interesting ways to exploit \nthe computational resources available in the coming decade of computing! \n \n7.1. Using Heterogeneous Parallel Architectures with OpenCL \nUdeepta Bordoloi, Benedict R. Gaster, and Marc Romankewicz, Advanced Micro \nDevices \nOpen Compute Language (OpenCL) [Munshi09] is an open standard for data-and task-\nparallel programming on CPUs and GPUs. It is developed by the Khronos Group, which is \npart of the Compute Working Group and includes members from AMD, Apple, Electronic \nArts, IBM, Intel, NVIDIA, Qualcomm, and others. OpenCL is a standard to enable parallel \nprogramming on heterogeneous compute platforms. It can provide access to all the \nmachine‘s compute resources, including CPUs and GPUs. \n\n\n \n \nIn this gem, we give a primer on OpenCL and then introduce a set of general techniques for \nusing it to optimize games. Two examples are used: convolution and histogram. The former \nhighlights many of the techniques for optimizing nested loop data parallelism; the latter \nshowcases methods for accessing memory effectively. \nConvolution is a technique in the field of image processing; it calculates an output pixel as a \nfunction of its corresponding input pixel and the values of the neighboring pixels. This \ntechnique, however, is expensive to compute. Its run time is proportional to the width and \nheight of the image. If W and H are the width and height, respectively, the run time is O(W2 \nH2). The highly data-parallel nature of convolution makes it an excellent candidate for \nleveraging OpenCL optimizations. \nIn game programming, histograms improve the dynamic range of rendered frames through \ntone mapping. Fast on-GPU histogram computation is essential, since the image data \noriginates on the GPU. Historically, histogram computation has been difficult to do on GPU, \nas typical algorithms become limited by the scatter (random write-access) performance. \nHowever, with the advent of large on-chip, user-controlled memories and related atomic \noperations, performance can be dramatically improved. \nEfficient implementations for convolution and histogram calculations broaden the set of \npossibilities for game developers to exploit emerging heterogeneous parallel architectures; \nboth PCs and game consoles have many-core CPUs and GPUs. They also highlight the \ndifficulties that must be overcome to achieve anything like peak performance. \nIn this gem, we discuss a step-by-step optimization toolkit for game developers using \nOpenCL. These optimizations can be used together or individually, but their overall effect \ndepends on the application; there is no magic wand for an algorithm with little parallelism \nor a dataset so small that there is little chance to amortize the associated run-time cost. \nOpenCL Primer \nOpenCL is based on the notion of a host API, which consists of a platform and run-time \nlayer and a C-like language (OpenCL C) for programming compute devices; these devices \ncan range from CPUs to GPUs and other kinds of accelerators. Figure 7.1.1 illustrates this \nmodel, with queues of commands, reading/writing data, and executing kernels for specific \ndevices. The overall system is called a platform. There can be any number of platforms from \ndifferent vendors within a particular system, but only devices within a single platform can \nshare data within what OpenCL calls a context. \nFigure 7.1.1. OpenCL host/device architecture. \n \n\n\n \n \n \nThe devices are capable of running data- and task-parallel work; a kernel can be executed \nas a function of multidimensional domains of indices. Each element is called a work-item; \nthe total number of indices is defined as the global work-size. The global work-size can be \ndivided into sub-domains, called work-groups, and individual work-items within a group can \ncommunicate through global or locally shared memory. Work-items are synchronized \nthrough barrier or fence operations. Figure 7.1.1 is a representation of the host/device \narchitecture with a single platform, consisting of a GPU and a CPU. \nGiven an enumeration of platforms, we choose one, select a device or devices to create a \ncontext, allocate memory, create device-specific command queues used to submit work to a \nspecific device, and perform computations. Essentially, the platform layer is the gateway to \naccessing specific devices. Given these devices and a corresponding context, the application \nis unlikely to have to refer to the platform layer again. It is the context that drives \ncommunication with, and between, specific devices. A context generally is created from one \nor more devices. A context allows us to: \n \nCreate a command queue. \n \nCreate programs to run on one or more associated devices. \n \nCreate kernels within those programs. \n \nAllocate memory buffers or images, either on the host or on the device(s). (Memory \ncan be copied between the host and device.) \n \nWrite data to the device. \n \nSubmit the kernel (with appropriate arguments) to the command queue for \nexecution. \n \nRead data back to the host from the device. \nThe relationship between context(s), device(s), buffer(s), program(s), kernel(s), and \ncommand queue(s) is best seen by looking at sample code. The following program adds the \nelements of two input buffers, a and b, and stores the results in a single output buffer, o. \nWhile trivial in its application, the sample program is the foundational pattern of all OpenCL \nprograms. \n  #include <CL/cl.hpp> \n  #include <iostream> \n  static char kernelSourceCode[] = \n  \"__kernel void\\n\" \n  \"hello(__global int * inA, __global int * inB, __global int * \nout)\\n\" \n  \"{\\n\" \n  \"    size_t i = get_global_id(0);\\n\" \n  \"    out[i] = inA[i] + inB[i];\\n\" \n  \"}\\n\"; \n  int main(void) { \n      int a[10] = {1,2,3,4,5,6,7,8,9,10}; \n      int b[10] = {1,2,3,4,5,6,7,8,9,10}; \n      int o[10] = {0,0,0,0,0,0,0,0,0,0}; \n      cl::Context context(CL_DEVICE_TYPE_ALL); \n      std::vector<cl::Device> devices = \ncontext.getInfo<CL_CONTEXT_DEVICES>(); \n      cl::CommandQueue queue(context, devices[0]); \n      cl::Program::Sources sources(1, \nstd::make_pair(kernelSourceCode, 0)); \n      cl::Program program(context, sources); \n      program.build(devices); \n\n\n \n \n      cl::Kernel kernel(program, \"hello\"); \n      cl::Buffer inA(context,CL_MEM_READ_ONLY,10 * sizeof(int)); \n      cl::Buffer inB(context,CL_MEM_READ_ONLY,10 * sizeof(int)); \n      cl::Buffer out(context,CL_MEM_WRITE_ONLY,10 * \nsizeof(int)); \n      queue.enqueueWriteBuffer(inA,CL_TRUE,0,10 * \nsizeof(int),a); \n      queue.enqueueWriteBuffer(inB,CL_TRUE,0,10 * \nsizeof(int),b); \n      kernel.setArg(0,inA); kernel.setArg(1,inB); \nkernel.setArg(2,out); \n      queue.enqueueNDRangeKernel( \n      kernel, cl::NullRange, cl::NDRange(10), cl::NDRange(2) \n  ); \n  queue.enqueueReadBuffer(out,CL_TRUE,0,10 * sizeof(int),o); \n  std::cout << \"{\" ; \n  for (int i = 0; i < 10; i++) { \n       std::cout << o[i] << (i!=9 ? \", \" : \"\") ; \n  } \n  std::cout << \"}\" << std::endl; \n \n} \n \nBecause it is not possible to provide a full introduction to OpenCL in the limited amount of \nspace allocated to this gem, we advise readers new to OpenCL to read through the \nreferences given at the end of the article. For example, many introductory samples are \nshipped with particular implementations, and readers might also like to work though a \ntutorial such as ―hello world‖ [Gaster09]. \n \nTips for Optimizing OpenCL C Kernels \nTwo-dimensional convolution is used to illustrate techniques that can be used to optimize \nOpenCL kernels. \nConvolution Kernel \nThe OpenCL C kernel for convolution is given below. It is almost a replica of a corresponding \nC code for convolution; the only difference is that the C code uses two for loops that \niterate over the output image to initialize the variables xOut and yOut, instead of using \nthe get_global_id call. The output image dimensions are width by height, the input \nimage width is inWidth (equals width+filterWidth-1), and the input height equals \n(height+filterWidth-1). \n__kernel void Convolve(__global float * input, \n                      __constant float * filter, __global float \n* output, \n                      int inWidth, int width, int height, int \nfilterWidth) \n{ \n\n\n \n \n    int yOut = get_global_id(1);//for (int yOut = 0; yOut < \nheight; yOut++) \n    int xOut = get_global_id(0);//for (int xOut = 0; xOut < \nwidth; xOut++) \n    int xInTopLeft = xOut; int yInTopLeft = yOut; \n    float sum = 0; \n    for (int r = 0; r < filterWidth; r++) { \n        int idxFtmp = r * filterWidth; \n        int yIn = yInTopLeft + r; \n        int idxIntmp = yIn * inWidth + xInTopLeft; \n        for (int c = 0; c < filterWidth; c++) \n            sum += filter[idxFtmp+c]*input[idxIntmp+c]; \n    } //for (int r = 0... \n    int idxOut = yOut * width + xOut; \n    output[idxOut] = sum; \n} \n \nFigure 7.1.2 shows the computation time for an output image of size 8192×8192. The test \ncomputer is an AMD Phenom X4 9950 Black Edition with 8 GB RAM. For a filter width of 2, \nthe input image size is 8193×8193; for a filter of width 32, the input image is 8223×8223. \nFor each pixel, the loop runs for (filterWidth)2 times. The computation time increases \napproximately as a function of the square of the filter width. It takes about 14.54 seconds \nfor a 20×20 filter and 3.73 seconds for a 10×10 filter. We first consider unrolling loops to \nimprove performance of this workload. \nFigure 7.1.2. Computation time for various filter widths for an 8192×8192 output \nimage \n \n \n \nLoop Unrolling \nHaving loops in the code comes at a performance cost. For example, consider a 32×32 \nfilter. For each pixel in the output image, the statements in the innermost loop are run \n32×32 = 1024 times. This cost is negligible for small filters, but it becomes significant as \nthe filter width increases. The solution is to reduce the loop count. \n\n\n \n \nThe following kernel has four iterations of the innermost loop unrolled. A second loop is \nadded to handle the remainder of iterations when filter width is not an even multiple of four. \n__kernel void ConvolveUnroll(...) \n{ \n \n    ... \n    for (int r = 0; r < filterWidth; r++){ \n        ... \n        int c = 0; \n        while (c <= filterWidth-4) { \n            sum += filter[idxFtmp+c] *input[idxIntmp+c]; \n            sum += filter[idxFtmp+c+1]*input[idxIntmp+c+1]; \n            sum += filter[idxFtmp+c+2]*input[idxIntmp+c+2]; \n            sum += filter[idxFtmp+c+3]*input[idxIntmp+c+3]; \n            c += 4; \n        } \n        for (int c1 = c; c1 < filterWidth; c1++) \n            sum += filter[idxFtmp+c1]*input[idxIntmp+c1]; \n    } //for (int r = 0... \n    ... \n} \n \nFigure 7.1.3 shows the results of the unrolled kernel. For larger filters, unrolling helps \nimprove speed by as much as 20 percent. The sawtooth kind of behavior that we see in the \ngraph is due to the iterations that are left over after unrolling and is easily handled by \nunrolling the second inner loop (and substituting it with an if-else statement). \nFigure 7.1.3. Loop unrolling. Unrolling the inner loop (Unroll) and the second \ninner loop (Unroll_If). \n \n \n \nInvariants \nFor the aforementioned kernels, filter widths were passed as an argument. Consider an \napplication where the filter size is constant (for example, 5×5). In this case, the inner loop \ncan be unrolled five times, and the loop condition can be removed. \n\n\n \n \n__kernel void DefConvolve(...) \n{ \n    ... \n    for (int r = 0; r < FILTER_WIDTH; r++) { \n        int idxFtmp = r * FILTER_WIDTH; \n        ... \n        for (int c = 0; c < FILTER_WIDTH; c++) \n    ... \n} \n \nAs the filter width is static, FILTER_WIDTH, it can be defined when building the OpenCL \nprogram. The following code shows how to pass in the value of the invariant. \nstd::string sourceStr = FileToString(kernelFileName); \ncl::Program::Sources sources(1, \nstd::make_pair(sourceStr.c_str(), \n                                               \nsourceStr.length())); \nprogram = cl::Program(context, sources); \nchar options[128]; \nsprintf(options, ―-DFILTER_WIDTH=%d‖, param.filterWidth); \nprogram.build(devices, options); \ncl::Kernel kernel = cl::Kernel(program, kernelName.c_str()); \n \nFigure 7.1.4 shows that defining the filter width as an invariant helps the DefConvolve \nkernel gain about 20 percent performance over the Convolve kernel, particularly for small \nkernel sizes. \nFigure 7.1.4. Using the filter width as an invariant in the Convolve and the \nUnroll_If kernels. \n \n \n \nVectorization \nSince the inner loop of the unrolled kernel has four products and four additions, it is \npossible to use one vector (SSE or GPU) packed-multiply and one packed-add to achieve the \nsame results. But how do we use vectors in an OpenCL kernel? AMD‘s OpenCL \nimplementation will try to use ―packed-arithmetic‖ instructions whenever it encounters a \n\n\n \n \nvector data type in the kernel. The following kernel body uses the vector type float4. \nNote the additional loop at the end to handle any remaining iterations when filter width is \nnot an even multiple of four. \n__kernel void ConvolveFloat4(...) { \n    ... \n    float4 sum4 = 0; \n    for (int r = 0; r < filterWidth; r++) { \n        ... \n        int c = 0; int c4 = 0; \n        while (c <= filterWidth-4) { \n            float4 filter4 = vload4(c4, filter+idxFtmp); \n            float4 in4     = vload4(c4, input +idxIntmp); \n            sum4 += in4 * filter4; \n            c += 4; c4++; \n        } \n        for (int c1 = c; c1 < filterWidth; c1++) \n            sum4.x += filter[idxFtmp+c1]*input[idxIntmp+c1]; \n    } //for (int r = 0... \n    int idxOut = yOut * width + xOut; \n    output[idxOut] = sum4.x + sum4.y + sum4.z + sum4.w; \n} \n \nThe second inner loop can be unrolled (kernel ConvolveFloat4_If), and invariants can \nbe used for further speedups (kernels DefFloat4 and DefFloat4_If). Figure 7.1.5 \nshows the results for the vectorized kernel. \nFigure 7.1.5. The effect of vectorization. ConvolveFloat4 is the float4 version \nof Convolve; ConvolveFloat4_If has the second inner loop unrolled. \nDefFloat4 and DefFloat4_If use the filter width as an invariant. \n \n \nOptimizing Memory-Bound OpenCL Kernels \nSo far, our optimizations have focused on how to maximize ALU throughput. In the \nfollowing section, we use a histogram calculation to address techniques for optimizing \nkernels limited by memory performance. \nFor simplicity, we assume that a histogram consists of 32-bit-per-pixel images with 256 32-\nbit bins. The algorithm parallelizes the computation over a number of workgroups, each of \nwhich uses a number of sub-histograms stored in on-chip __local memory. \n",
      "page_number": 511
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 520-528)",
      "start_page": 520,
      "end_page": 528,
      "detection_method": "topic_boundary",
      "content": " \n \nNumber of Work-Groups \nWhen choosing the optimal number of work-items and work-groups, some constraints follow \nautomatically from the algorithm, the device architecture, and the size of __local \nmemory. \n__local memory is shared within a single work-group. For the histogram algorithm, it is \ndesirable to have fewer, larger work-groups. This allows many successive threads to \ncontribute to the same __local memory area. Conversely, using more and smaller work-\ngroups would require costly flushing of __local sub-histograms to __global memory \nmore frequently, since the lifetime of a work-group is shorter. \nTo maximize the use of __local memory, the number of work-groups should be as close \nas possible to the number of compute units. Each compute unit typically has a dedicated \nblock of __local memory, and using as many of them concurrently as possible is \nadvantageous. \nThe actual number of compute units, typically in the tens per GPU, can be queried at run \ntime using: \nclGetDeviceInfo( ..., CL_DEVICE_MAX_COMPUTE_UNITS, ... ); \n \n \nWork-Group Size \nA work-group should be at least as big as the basic hardware unit for scheduling. On AMD \nGPUs, a wavefront is a hardware unit of work-items executing concurrently on a given \ncompute unit. OpenCL work-groups are executed as a collection of wavefronts. Wavefront \nsize cannot be queried via OpenCL—on current high-end AMD GPUs, it is 64 work-items. \nAlso, a total of two wavefronts can be scheduled to run on a single compute unit at a time. \nThis sets an absolute minimum for the total work-item number: Multiply two times the \nwavefront size with the number of work-groups. An integer multiple of that number is often \nadvisable, as it will allow more flexibility for run-time scheduling—hiding memory latency, \nallocating register use, and so on. On the other hand, the upper limit of the number of \nthreads is given by the maximum work-group size times the chosen number of work-\ngroups. The maximum work-group size can be queried using: \nclGetDeviceInfo( ..., CL_DEVICE_MAX_WORK_GROUP_SIZE, ... ); \n \nWithin this range, fine-tuning remains an algorithm- and kernel-specific matter. Kernels can \nrequest a particular set of resources. For example, they can request the number of \nregisters, the size of the __local buffer, and the number of instructions. These \nparameters influence how the kernel is scheduled on the GPU at run time. Possible tuning \nknobs include the number of groups, the group size, and the number of work-items. The \nupcoming section on optimal read patterns gives an example. \n__global Read/Write Access Patterns \nFor any implementation of either a parallel or a serial histogram, very few arithmetic \noperations are required. This implies that the first limiting factor for histogram performance \nis the read bandwidth from __global memory, as the input image originates from there. \nFortunately, because the histogram read is order-independent, it is possible to optimize \neach work-item‘s read pattern to that which is best supported by the GPU or CPU. \n\n\n \n \nThe CPU/GPU memory subsystem and the compiler tool chain are optimized for 128-bit \nquantities—the common shader pixel size of four 32-bit floats. On the GPU, multiples of \nthese quantities are advantageous to allow the memory subsystem to take advantage of the \nwide (typically hundreds of bits) data path to memory. Ideally, all simultaneously executing \nwork-items read adjacent 128-bit quantities, so that these can be combined to larger \nquantities. \nThis leads to the following access pattern: Each thread reads unsigned integer quantities \npacked into four-wide vectors (uint4) quantities, starting at its global work-item index and \ncontinuing with a stride equal to the number of threads, until it reaches the end. For a total \nwork-item number of NT, the resulting pattern is: \nuint4 addr 0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \nThread \n0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \nuint4 addr NT NT+1 NT+2 NT+3 ... 2NT-4 2NT-3 2NT-2 2NT-1 \nThread \n0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \n \nAs many of the work-items execute this step at the same time, it results in a large number \nof simultaneous, adjacent read requests, which can be combined into optimal hardware \nunits by the memory subsystem. For example, for NT = 8192, the addresses sent to the \nmemory subsystem result in a tightly aligned pattern: \nThread \n0 1 2 3 4 ... 8189 8190 8192 \nuint4 addr 0 1 2 3 4 ... 8189 8190 8192 \n \nNote that the straightforward choice—a serial access pattern from within each work-item—\nresults in widely dispersed concurrent accesses when issued across all active threads; this is \na worst-case scenario for the GPU. Here, NI is the number of uint4 items per thread: \nuint4 addr 0 1 \n2 \n3 \n... NI-4 \nNI-3 \nNI-2 \nNI-1 \nThread \n0 0 \n0 \n0 \n... 0 \n0 \n0 \n0 \nuint4 addr NI NI+1 NI+2 NI+3 ... 2NI-4 2NI-3 2NI-2 2NI-1 \nThread \n1 1 \n1 \n1 \n... 1 \n1 \n1 \n1 \n \nAs an example, for NT = 8192 and NI = 128, the addresses sent to the memory subsystem \nare widely dispersed: \n\n\n \n \nThread \n0 1 \n2 \n3 \n4 \n5 \n6 \n7 \n... \nuint4 addr 0 128 256 320 512 640 768 896 ... \n \nOn the CPU, this pattern is nearly ideal, particularly when running only a few CPU work-\nitems with large numbers of NI. Each work-item reads sequentially through its own portion \nof the dataset; this allows prefetching and per-core cache line reads to be most effective. As \na rule of thumb, it is desirable to use per-thread column-major access on the GPU and per-\nthread row-major access on the CPU. Row-major and column-major access can be \nimplemented in a single kernel through different strides—a kernel can switch at run time \nwithout cost. \nBoth patterns are shown in the following code example, where nItemsPerThread is the \noverall number of uint4 elements in the input buffer, divided by the number of threads: \n__kernel void singleBinHistogram (__global uint4 *Image, \n                                  __global uint *Histogram, \n                                  uint nItemsPerThread ) \n{ \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; uint bin = 0; \n    uint val = SOME_PIXEL_VAL; \n#ifdef GPU_PEAK_READ_PERF \n    // with stride, fast \n    for( i=0, idx=get_global_id(0); i<nItemsPerThread; i++, \nidx+=nWItems) { \n#else \n    // serial, slow on GPU, fast on CPU \n    for( i=0, idx=0; i<nItemsPerThread; i++, idx+=1 ) { \n#endif \n        if( Image[idx].x == val ) bin++; \n        if( Image[idx].y == val ) bin++; \n        if( Image[idx].z == val ) bin++; \n        if( Image[idx].w == val ) bin++; \n   } \n \n   Histogram[id] = bin; \n} \n \nWhen executed with at least 8192 work-items and a work-group size of 64, this kernel \nreaches near hardware peak performance (on AMD GPUs), as shown in Figure 7.1.6. \nFigure 7.1.6. Bandwidth per number of work-items. \n\n\n \n \n \n \n \nGetting around the Scatter Bottleneck \nOn the CPU, reads and randomly indexed writes can be performed at roughly the same \nspeed. The GPU, on the other hand, excels when massively parallel writes can be coalesced \ninto large blocks, just as was shown for reads in the previous section. Also, the total read-\nmodify-write latency to __global memory can be a contributor. For a straightforward \nparallel histogram implementation, where each thread creates a sub-histogram in \n__global memory, as in the following code example, scatter turns out to be a bottleneck. \n__kernel void multiBinHistogram ( __global uint4 *Image, \n                                  __global uint *Histogram, \n                                  uint nItemsPerThread ) { \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; \n \n    for( i=0, idx=get_global_id(0); i<nItemsPerThread; i++, \nidx+=nWItems ) \n    { \n        Histogram[ id * NBINS + Image[idx].x ]++; \n        Histogram[ id * NBINS + Image[idx].y ]++; \n        Histogram[ id * NBINS + Image[idx].z ]++; \n        Histogram[ id * NBINS + Image[idx].w ]++; \n    } \n   ... \n} \n \n \nOptimizing __local Memory Access \nA powerful way to address the GPU scatter bottleneck is to direct scatter into __local \nmemory. This memory is available to all work-items in a work-group, and its lifetime is that \nof the group. \n\n\n \n \nIt is fast for the following reasons: \n \nIt typically is on-chip (cf. clGetDeviceInfo(CL_DEVICE_LOCAL_MEM_TYPE)) \nand very low latency. In essence, it is a user-manageable cache. \n \nIt supports a large number of concurrent data paths: one for each compute unit and \nmore for multiple banks on each compute unit, resulting in an aggregate bandwidth \nup to an order of magnitude higher than __global memory. \n \nIt allows the use of hardware atomics, so that random access RMW cycles can be \nperformed without having to worry about expensive, explicit synchronization \nbetween the global set of threads. \nThe __local memory size per work-group (and by extension, per compute unit) is given \nby: \nclGetDeviceInfo( ..., CL_DEVICE_LOCAL_MEM_SIZE, ... ); \n \nThis gives a total __local size per GPU device as CL_MAX_COMPUTE_UNITS * \nCL_DEVICE_LOCAL_MEM_SIZE. \nA straightforward implementation for the histogram algorithm would be to simply have a \nsub-histogram instance per __local memory and to let all threads in that group \ncontribute to that single instance, as shown in the following example code: \n__kernel void histogramLocal( __global uint4 *Image, \n                              __global uint *Histogram, \n                              __local uint *subhists, \n                              uint nItemsPerThread ) { \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; uint4 temp; \n    // initialize __local memory \n    ... \n    // scatter loop \n    for( i=0, idx=tid; i<nItemsPerThread; i++, idx+=nWItems ) { \n       temp = Image[idx]; \n \n       atom_inc( subhists + temp.x ); \n       atom_inc( subhists + temp.y ); \n       atom_inc( subhists + temp.z ); \n       atom_inc( subhists + temp.w ); \n    } \n    barrier( CLK_LOCAL_MEM_FENCE ); \n    // reduce sub-histogram and write out to __global for \nfurther reduction \n    ... \n} \n \nThis approach performs very well for randomized input data, since the collective scatter \noperations from all threads are distributed over the __local banks. Unfortunately, real \napplication images can consist of large swaths of identical pixel values—a black background, \nfor example. The resulting simultaneous atomic read-modify-write access by many threads \ninto a single bin quickly leads to severe contention and loss of performance. \n\n\n \n \nThis is where __local banks become effective as the hardware provides multiple banks \nthat can be accessed at the same time. On AMD GPUs, addresses of adjacent 32-bit words \nmap to adjacent banks. This pattern repeats every NB addresses, where NB is the number of \navailable banks. \nFor NB=32, the __local bank layout looks like this: \nuint addr 0 \n1 \n2 \n3 \n... 28 29 30 31 \nbank \n0 \n1 \n2 \n3 \n... 28 29 30 31 \nuint addr 32 33 34 34 ... 60 61 62 63 \nbank \n0 \n1 \n2 \n3 \n... 28 29 30 31 \n \nThe goal is to map the set of running threads to as many banks as possible. In the \nhistogram case, an easy way to do this is to maintain NB copies for each bin and use the \nlocal work-item ID to steer simultaneous writes to the same logical bin, but into separate \nbanks. \nThe resulting bins are then combined after the read-scatter phase of the kernel. The optimal \nnumber of copies for each bin can be experimentally determined, but it is less than or equal \nto NB. The value of NB cannot be queried via OpenCL and is obtained from the hardware \nspec. For recent AMD GPUs, it is 32. \n#define NBANKS 32 \n// subhists is of size: (number of histogram bins) * NBANKS \n__kernel void histogramKernel( __global uint4 *Image, \n                               __global uint *Histogram, \n                               __local uint *subhists, \n                               uint nItemsPerWI ) { \n     uint id = get_global_id(0); \n     uint ltid = get_local_id(0); \n     uint nWItems = get_global_size(0); \n \n     uint i, idx; uint4 temp; \n \n     uint4 spread = (uint4) (NBANKS, NBANKS, NBANKS, NBANKS); \n     uint4 offset = (uint4)(ltid, ltid, ltid, ltid); \n     offset &= (uint4)(NBANKS-1, NBANKS-1, NBANKS-1, NBANKS-1); \n \n     // initialize __local memory \n     ... \n     // scatter loop \n     for( i=0, idx=tid; i<nItemsPerWI; i++, idx+=nWItems ) { \n        temp = Image[idx] * spread + offset; \n        atom_inc( subhists + temp.x ); \n        atom_inc( subhists + temp.y ); \n        atom_inc( subhists + temp.z ); \n        atom_inc( subhists + temp.w ); \n     } \n\n\n \n \n     barrier( CLK_LOCAL_MEM_FENCE ); \n     // reduce sub-histograms, and write out to __global for \nfurther reduction \n     ... \n} \n \nBy spreading out read-modify-write accesses in this manner, contention is greatly reduced, \nif not removed. The resulting performance gain for a test image consisting of identical pixels \nis an order of magnitude. \n \nOpenCL Command Profiling \nIn the previous sections, a selection of techniques for optimizing OpenCL programs have \nbeen described. But how should we time changes made for performance? In general, \nWindows‘ performance counters work well, and for more detailed profiles, tools such as \nAMD‘s CodeAnalyst Performance Analyzer or GPU PerfStudio help. In addition to these, \nOpenCL provides the ability to query profiling information for specific commands executed \nvia a command queue on a device. \nBy default profiling is not enabled, but it can be explicitly requested at the creation of a \ncommand queue, with the following flag: \nCL_QUEUE_PROFILING_ENABLE \n \nAlternatively, profiling can be enabled for a command queue with the following API call: \nqueue.setProperty(CL_QUEUE_PROFILING_ENABLE, CL_TRUE); \n \nGenerally, commands are enqueued into a queue asynchronously, and the developer must \nuse events to keep track of a command‘s status, as well as to enforce dependencies. Events \nprovide a gateway to a command‘s history: They contain information detailing when the \ncorresponding command was placed in the queue, when it was submitted to the device, and \nwhen it started and ended execution. Access to an event‘s profiling information is through \nthe following API call: \nevent.getProfilingInfo<cl_profiling_info>(); \n \nValid values of the enumeration cl_profiling_info are CL_PROFILING_COMMAND_ \nQUEUED, CL_PROFILING_COMMAND_SUBMIT, CL_PROFILING_COMMAND_START, \nCL_PROFILING_ COMMAND_END. \n \nConclusion \nOpenCL is a technology for describing data- and task-parallel programs that can use all \ncompute resources, including CPUs and GPUs. While it is straightforward to get applications \nto run using OpenCL, it is not always simple to get the expected performance. In this \n\n\n \n \narticle, we have outlined a number of techniques that can be used to optimize OpenCL \nprograms. These optimizations are not a guarantee for performance; instead, they should \nbe seen as a toolkit to be considered when optimizing an application. \n \nReferences \n[Gaster09] Gaster, Benedict R. ―Introductory Tutorial to OpenCL.‖ August 2009. ATi Stream \nTechnical publications. n.d. \n<http://developer.amd.com/gpu/ATIStreamSDK/pages/TutorialOpenCL.aspx>. \n[Munshi09] Munshi, Aaftab, ed. ―The OpenCL Specification.‖ Version 1.0. Khronos OpenCL \nWorking Group, August 2009. \n \n7.2. PhysX GPU Rigid Bodies in Batman: Arkham Asylum \nRichard Tonge, NVIDIA Corporation, rtonge@nvidia.com \nBen Wyatt and Ben Nicholson, Rocksteady Studios \nOne of the goals of the PC version of Batman: Arkham Asylum was to improve the realism \nand dynamism of the game environment by adding GPU-based physics effects, including \ncloth, fluids, and rigid body–based destruction. With fluids, we were able to add volumetric \nsmoke and fog, which moves around players and other characters as they progress through \nit. With cloth, we were able to bring to life the piles of paper littering the admin areas of \nArkham Asylum. These piles appear as static triangle meshes on other platforms. For rigid \nbody dynamics, we wanted to provide large-scale destruction—for example, to demolish \nlarge concrete walls in the outdoor areas of the game. To believably break up a large wall \nrequires the simulation of thousands of debris pieces colliding with the high triangle count \nenvironment of Batman. To achieve this in addition to running all the gameplay physics, \ngraphics, and other game code on the CPU at high frame rates, it was clear that the effects \nwould have to be run on the GPU. \nFor GPU acceleration of cloth and fluids, we were able to use NVIDIA PhysX, which already \nincludes CUDA implementations of those features. For rigid body simulation, NVIDIA \ndeveloped a GPU rigid body engine (GRB) implementing the subset of PhysX features we \nneeded for Batman. Using GRB, physics calculations run up to three times faster than on \nthe CPU, enabling the large-scale destruction required. This gem describes the design and \nimplementation of GRB and its use in Batman. \nBatman made use of NVIDIA‘s APEX destruction module to author and simulate destructible \ngame objects. APEX is a layer above PhysX that allows specific effects to be authored and \ninserted into games easily. APEX destruction allows the artist to specify how an object \nbreaks, with APEX doing the dirty work of creating, splitting, and destroying PhysX rigid \nbodies when the object is shot at, blown up, or swept away by a tornado. \nIn Batman‘s Scarecrow levels, large wall panels needed to be destroyed, with the debris \nswept up into a tornado centered on the Scarecrow. Wall panels were imported into the \nAPEX destruction tool, where they were broken into pieces and placed into the game using \nUnreal Editor. With the content pipeline in place, our task was to make a GPU rigid body \nengine to accelerate the subset of PhysX rigid body features used by APEX destruction. \nFigure 7.2.1. Destructible wall section being placed in Unreal Editor. \n\n\n \n \n \n \nRequirements \nWe identified the following requirements: \n \nPerformance. The system must be able to support thousands of rigid bodies \ncolliding and interacting with the game environment in real time. \n \nParallelism. The system must be designed to use thousands of GPU threads to get \ngood GPU performance and allow scaling to future GPUs. \n \nAPI. The API should be similar to PhysX rigid body and support all features and API \ncalls made by APEX destruction. \n \nInteraction with PhysX. The game will use PhysX (CPU) rigid bodies for game-play \nobjects (for example, Batman). GRB should support one-way interaction with PhysX \nbodies. (PhysX bodies should move GRBs but not vice versa.) \n \nShapes. GRB should support the PhysX shapes used in Batman—that is, triangle \nmeshes, convex meshes, spheres, capsules, and boxes. \n \nMulti-threading. The simulation should occur asynchronously to the game thread to \nallow unused GPU resources to be utilized whenever they occur in the frame and \navoid having the game thread wait for GRB to complete. \n \nBuffering. The game should be free to change the scene while the simulation is \nrunning without disrupting the simulation. \n \nStreaming. Batman uses a single scene for the whole game and streams in adjacent \nlevels as the player moves around. This means that additions to the scene have to \nbe fast. \n \nAuthoring. It should be easy to author and tune content. Artists should be able to \nspecify which game objects collide with GRBs and which don‘t. \n \nShape Representation \n",
      "page_number": 520
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 529-536)",
      "start_page": 529,
      "end_page": 536,
      "detection_method": "topic_boundary",
      "content": " \n \nThe static environment is represented by the original geometry used in the game—triangle \nmeshes, convex meshes, boxes, and so on. Dynamic objects have a sphere-based \nrepresentation made by voxelizing the object geometry. Color Plates 13 and 14 show how \nshapes in Batman are represented by voxel-generated spheres in GRB. Although spheres \nare used to represent the collision geometry of dynamic objects, the dynamics are still done \non rigid body coordinates. \n \nDynamics Algorithm \nOne of the most important requirements was that the algorithm be highly data parallel to \nallow efficient GPU implementation. Initially, the dynamics method from [Harada07] seemed \nlike a good choice. In this method, spring and damper forces are applied at contact points \nto prevent penetration. The forces are integrated explicitly, which means that they can be \ncalculated in parallel. In a sense, this method is maximally parallel, because there are as \nmany threads as contacts. \nUnfortunately, like other penalty methods, this method is hard to tune. In this method, the \nspring stiffness must be set high enough so that penetration is avoided for all incoming \nvelocities. Penalizing penetration with springs injects unwanted energy into the system, so a \ndamping parameter has to be tuned to dissipate the extra energy. Although tuning these \nparameters is reasonable for simple collisions—for example, between two spheres—it \nquickly becomes intractable for multiple simultaneous collisions each having multiple \ncontacts. Also, because the method is an explicit method, the stiffness dictates a maximum \ntime-step size for which the simulation is stable. [Harmon09] addresses some of these \nproblems, but the method is far from real time. \nTo avoid these problems, we decided to use an implicit constraint-based method similar to \nthat used in PhysX. In this method, instead of producing a spring and damper per contact \npoint, the collision detection code produces a set of rules called constraints. These \nconstraints are fed into a constraint solver, which iteratively finds a new set of velocities for \nthe bodies. Its goal is to find velocities that won‘t move the bodies into penetration (violate \nthe constraints) in the current time step. Compared to [Harada07], implementing this \nmethod has the additional complexity of parallelizing a constraint solver. The parallelization \nwill be described in a later section. \n \nPipeline \nThe following is a high-level description of the pipeline. \n \nTransform spheres into world space using body transformation matrices. \n \nFind sphere-sphere overlaps, producing contacts. \n \nFind sphere-mesh overlaps, producing contacts. \n \nEvaluate force fields, such as wind, explosions, tornados, and so on. \n \nUpdate body velocities by applying force fields and gravity. \n \nGenerate constraints from contacts. \n \nSolve constraints producing modified velocities and position corrections. \n \nUpdate body positions using position corrections. \n \nTransform Spheres into World Space \n\n\n \n \nThe spheres are rigidly attached to the rigid body reference frame. For this reason, in each \nframe the spheres have to be transformed from their local frame to world space. For a \nparticle with position r belonging to a rigid body with transformation matrix M, we calculate \nthe world space position as follows: \nrworld = Mr \n \nSphere-Sphere Collision Detection \nThe dynamic objects are represented by grids of spheres, so we use sphere-sphere tests to \ndetermine the points of contact between them. We use the same size spheres for all the \ndynamic objects in the scene, so the sphere radius is a global parameter ζ. A pair of \nspheres separated by vector rij are overlapping if |rij| ≤2ζ. In this case, the contact normal \nnc, position pc, and separation θc are defined as follows. \n \nIn the case, where the two spheres are in almost the same position (|rij| ≤ ε, where ε is a \nsmall positive number), we use an arbitrary contact normal nc = (1,0,0). Note that when \nthe spheres are penetrating, the ―separation‖ is negative. \n \nSphere-Mesh Collision Detection \nThe static environment is mainly composed of non-convex triangle meshes, so we also have \nto test for sphere-triangle contact. For each sphere and potentially colliding triangle, we first \nneed to check whether the center of the sphere is already behind the triangle. Ignoring \ncontact in this case helps avoid the situation where some of a body‘s spheres are trapped \non one side of a triangle and some on the other, and it also simplifies the computation of \nthe separation. Given sphere center ps, triangle vertices (v0, v1, v2), and triangle normal nt, \nthe sphere center is behind the triangle if: \nnt.ps < nt.v0 \nIf a sphere and triangle survive this test, then we next need to calculate world coordinates \npt of the closest point on the triangle to the center of the sphere ps. For details on how to \ndo this, see [Ericson05]. We generate a contact for a sphere and triangle if |ps – pt| ≤ ζ. \nThe contact normal, position, and separation are given by the following: \nnc = (ps – pt)/|ps – pt| \npc = pt \nθc = |ps – pt| – ζ \nIn the case that the sphere center is on the triangle |ps – pt| ≤ ε, we set the contact normal \nnc to the triangle normal instead. \n\n\n \n \nWe use a uniform grid data structure to avoid having to test every sphere against every \ntriangle in the mesh. This is described later in this gem. \n \nEvaluate Force Fields \nForce fields are used in Batman to blow around debris and implement tornado effects. GRB \nsupports the same interface as PhysX for specifying and evaluating force fields. See \n[Nvidia08] for more details. \n \nCalculate Unconstrained Velocities \nFor a body with velocity v, we step the acceleration due to force fields af and gravity ag \nthrough time step h to get the unconstrained velocity v*. We also apply a global damping \nparameter η. \nv* = (1 – ηh)[v + h(af + ag)] \nEach body‘s inertia matrix I is calculated in its local frame, so we need to transform it to the \nworld frame as follows: \n \nwhere R is the orientation matrix of the body. \n \nGenerate Constraints \nFor each contact, the constraint solver needs a coordinate frame [b0 b1 b2] in which to \napply the constraint forces. We‘ll refer to these three vectors as constraint directions. The \nX-axis is aligned with the contact normal b0 = nc, and the other two axes are arbitrarily \nchosen orthogonal axes in which the friction forces will be applied. \nConsider a contact with position pc, normal nc, and separation θc. Let the position of the \ncontact relative to the center of masses of body a and body b be ra and rb, their final \n(constrained) velocities be \nand \n, their inertia be Ia and Ib, and their \nmasses be ma and mb. \nEach non-penetration constraint has the following form, known as the velocity Signorini \ncondition: \n \nHere, v0 is the component of relative velocity of the two bodies at the contact point along \nthe contact normal, and f0 is the magnitude of the impulse calculated by the constraint \nsolver to satisfy the constraint. The condition ensures that the bodies are either separating \nor resting. Additionally, if the bodies are separating, then the solver must not apply any \n\n\n \n \nimpulse; and if the bodies are resting, then the solver must apply a non-adhesive impulse. \nThe calculation of the friction impulses will be described later. \nThe constraint solver solves each constraint independently. To do this, it needs to calculate \n, the magnitude of the impulse to apply along direction bj needed to zero the \nrelative velocity along that direction, given initial relative velocity vj. The constant a is the \nreciprocal of the effective mass and is precomputed as follows: \n \nSolver \nThe task of the solver is as follows: Given a set of bodies with unconstrained velocities v* \nand a set of constraints, find impulses to apply to the bodies such that the resulting \nvelocities will (approximately) satisfy the constraints. As with most game physics engines, \nGRB uses an iterative projected Gauss-Seidel solver. Given an unlimited number of \niterations, the constraints will be solved exactly. In Batman, we terminate after four \niterations, so although the velocities may slightly violate the constraints, we can bound the \namount of time the solver will take. \nLet the number of constraints be m. If friction is used, then m is the number of contacts \nmultiplied by three. In this section, we‘ll assume that the constraint directions and all the \nother constraint parameters are concatenated into arrays of length m. The solver iterates \nover the constraints, solving each in isolation. \nSolver algorithm: \nZero f, the impulse accumulator for each constraint \nFor each iteration \nFor i = 0 to m \nCalculate the impulse required to prevent relative motion along \nconstraint direction i \nClamp the accumulated impulse to satisfy the Signorini or \nfriction constraint \nAdd impulse to impulse accumulator \nApply impulse to update the velocities of the constrained bodies \nCalculate Impulse Required to Prevent Relative Motion Along Constraint Direction i \nGiven \nand \n, the linear and angular velocities of the two bodies constrained \nby constraint i, and ra and rb, the position of the contact in the frames of body a and body \nb, the relative velocity along the constraint direction bi is given by the following: \n \n\n\n \n \nIn other words, vrel is the velocity of the contact point on body a minus the velocity of the \ncontact point on body b projected onto the direction bi. We can now multiply by the inverse \neffective mass ai to get fd, the additional impulse required to satisfy the constraint in \nisolation: \nfd = aivrel \nClamp the Accumulated Impulse to Satisfy the Signorini Constraint \nConstraint i is either a non-penetration constraint or a friction constraint. For non-\npenetration constraints, we clamp to prevent adhesion as follows: \nfnew = max(0,fi + fd) \nFor friction constraints, we approximate the Coulomb friction cone with a pyramid aligned to \nthe friction constraint directions. This allows us to calculate the friction impulses separately. \nFor a friction constraint with friction coefficient μ, we look up fj, the impulse applied at the \ncorresponding non-penetration constraint, and then clamp to the friction pyramid as \nfollows: \nfnew = max( –μfj, min(fi + fd, μfj)) \nApply Impulse to Update the Velocities of the Constrained Bodies \nWe now apply the clamped impulse to the two bodies involved in the constraint. \n \n \nUpdate Positions \nThe solver calculates a new velocity for each body. To apply velocity (vlin, vang) to a body \nwith position p, orientation quaternion q, and time step h, we use the following: \n \nwhere a = vang/|vang| and θ = |hvang|. \n \nGPU Implementation \nAmdahl‘s law dictates that if you parallelize less than 100 percent of your code, your \nspeedup will be at best the reciprocal of the proportion not parallelized. For example, if 80 \n\n\n \n \npercent of the code is parallelized, the speedup will be no more than 5×, even though the \nGPU might be significantly more than five times faster than the CPU. So our initial aim was \nto port all stages of the GRB pipeline to the GPU. In the end, we ported all pipeline stages \nexcept for force field calculation to the GPU. The reason that we didn‘t port force field \ncalculation is that the PhysX API allows the user to write custom force evaluation kernels in \nC. Linking artist-generated kernels into the GRB CUDA code was not supported with the \navailable tool chain. An advantage to running almost all of the pipeline on the GPU is that \nmost of the data can be kept in the GPU‘s memory. This is an advantage because data \ntransfer between CPU memory and GPU memory introduces latency and unnecessary \nsynchronization with the CPU. \nThe pipeline stages can be divided into those that are easily parallelized and those that are \nnot. The stages in the first category are transform particles, find sphere-sphere contacts, \nfind sphere-mesh contacts, update velocities, generate constraints, and update body \npositions. For example, in transform particles, each GPU thread is assigned a single particle. \nThe calculation of each particle is independent, and as there are typically more than 10,000 \nparticles in Batman, the GPU is well utilized. Similarly, each thread is assigned one particle \nin the collision detection stages. In the sphere-sphere stage, each thread is responsible for \nfinding nearby particles; and in the sphere-triangle stage, each thread looks at nearby \ntriangles. Again, each thread can do this without synchronizing with any other. In order to \nconcatenate the contacts produced by each thread, we allocate memory for a fixed number \nof contacts per thread. We used four contacts per sphere in Batman. Once this fixed number \nof contacts has been generated, further contacts are discarded. Discarding contacts could \nresult in unwanted penetration, but in Batman this is rarely visible. Another way to do this \nwould be to use a scan (prefix sum) operation [Harris07] to allocate space for the contacts. \nHowever, this requires running the collision kernels twice—once to calculate the number of \ncontacts and again to fill in the contact data. For the easily parallelized stages, the CUDA \nport was very quick. In most cases it was as simple as removing an outer for loop, adding \nthe__device__ keyword, and compiling the body of the for loop with the CUDA \ncompiler. Most of the loop bodies compiled with little or no changes. \nThe only pipeline stage that is not easily parallelized is the constraint solver, and that will be \ndiscussed in one of the following sections. \n \nCUDA Voxelizer \nVoxelization is the conversion of an object into a volume representation, stored in a three-\ndimensional array of voxels. A voxel is the three-dimensional equivalent of a pixel, \nrepresenting a small cubical volume. Since GRB represents objects as a collection of \nspheres, we use voxelization to generate an approximate sphere representation of the \nconvex objects generated by APEX destruction. Each covered voxel generates a single \nsphere. \nThe voxelizer isn‘t strictly part of the pipeline because it is only run when dynamic objects \nare first loaded. However, Batman levels are streamed, so object loading has to be as fast \nas possible. For this reason we decided to use a GPU voxelizer. \nInitially, we implemented an OpenGL voxelizer, similar to that described in [Karabassi99]. \nHowever, this required creating a separate OpenGL context, and there was significant \noverhead in transferring data between OpenGL and CUDA, so we decided to write a CUDA \nimplementation instead. \nSince the shapes generated by APEX destruction were all convex meshes, this was \nconsiderably simpler than writing a general-purpose triangle mesh voxelizer. The basic \nalgorithm used was to convert each triangle in the convex mesh into plane equation form. \nThis way, it is simple to determine the distance of a point from any plane by performing a \n\n\n \n \nsimple dot product. The plane data is sent to the GPU using CUDA constant memory. We \nthen launch a kernel with one thread per voxel. Each thread loops over all the planes, \ncalculating the distance to the plane and keeping track of the minimum distance and sign. If \nthe point is inside all the planes, then we know it is inside the object. By discarding points \nmore than a maximum distance away from the nearest plane, we can modify the thickness \nof the voxelized skin of the object. \nAlthough this algorithm was efficient for large volumes, for GRB we typically needed only a \ncoarse voxelization (perhaps only 83 voxels), which didn‘t generate enough threads to fill \nthe GPU. For this reason, we developed a batched voxelizer that voxelized a whole collection \nof convex shapes in a single kernel invocation. \nOne issue was that the original voxelizer would sometimes generate overlapping spheres for \nthe initially tightly packed groups of convexes generated by APEX destruction, causing the \nobjects to fly away prematurely. This was due to performing the voxelization in the local \nspace of the convex object. By modifying the voxelization to be performed in world space, \nwe ensured that the spheres generated by voxelization fitted together exactly with no \noverlaps. \n \nSpatial Data Structures \nTo avoid having to test each sphere for collision against every other sphere and each sphere \nagainst every triangle in each frame, we use a spatial data structure to cull out all but \nnearby spheres and triangles. Because the spheres are all the same size, we use a uniform \ngrid whose cell size is set to the sphere diameter. So for each sphere, we can find \npotentially overlapping spheres by querying only the 33 cells surrounding it. The triangles \nare also put into a uniform grid, but a separate grid is used to allow the cell size to be tuned \nseparately. When the triangle mesh is loaded, a voxelization process is used to identify \nwhich cells the triangle passes through. Each intersection cell is given a pointer to the \ntriangle. In Batman, triangle meshes are streamed in as the user progresses through the \nlevels. Initially, we updated the triangle grid incrementally each time a new mesh was \nloaded, but we found out that regenerating the whole grid from scratch at each mesh load \nwas more efficient. As the spheres are used to represent moving objects, the sphere grid \nhas to be rebuilt each frame. Like [LeGrand07, Green07] we use a fast radix sort [Satish09] \nto do this. The grid size used for both grids is 643. A simple modulo 64 hash function is used \nso that the world can be bigger than 643 cells. \n \nSolver \nUnlike the other stages, the projected Gauss Seidel solver isn‘t easy to parallelize. The \nreason for this is that each constraint reads and writes the velocities of two bodies, and \neach body can be affected by more than one constraint. For example, consider a stack of \nthree spheres resting on the ground. Each body is affected by two constraints, except for \nthe top body. If the constraints were executed in parallel by separate threads, then each \nbody (except the top body) would be overwritten by two threads. The final value of the \nvelocity of each body would depend on the exact order in which the constraints accessed \nthe body, and the effect of one constraint would not be taken into account in the calculation \nof the other. \nTo get around this problem, the solver has two phases, both of which run on the GPU. The \nfirst divides the constraint list into batches of constraints that can be executed in parallel. In \neach batch, all bodies are modified by just one constraint. The second executes the batches \nin sequence, using one thread for each constraint in each batch. \n\n\n \n \nWe make the assumption that any system can be solved in 32 or fewer batches. The goal is \nthat each body be modified only once in each batch and that the number of batches used be \nminimal. Each body is given an unsigned integer called a batch bitmap. Bit i set means that \nthe body is modified in batch i. \nInitially, all the batch bitmaps are set to zero. In the first phase, each constraint is \nprocessed by a thread in parallel. Each thread calculates the logical or (disjunction) of the \nbatch bitmaps of its two bodies. If a bit in the result is zero, then it means that batch \ndoesn‘t modify either body, so the constraint can be processed in this batch. So, the thread \nfinds the first zero bit in the result and attempts to atomically set it in the two bodies. If the \natomic set fails (because another thread has allocated the body to the batch in the \nmeantime), then it starts again to find a new bit that is not set for both bodies. If all the \nbits are set in the disjunction of the two bitmaps, then more than 32 batches are required, \nand the constraint is discarded. \n \nConclusion \nTo test the speedup of GRB, we ran the second Scarecrow level from Batman with and \nwithout GRB. When the level is run without GRB, both the gameplay and destruction rigid \nbodies are run on the CPU, and when the level is run with GRB, the game-play bodies are \nsimulated using PhysX on the CPU, but the 1,600 debris chunks are simulated on the GPU \nusing GRB. The following timings are for the 1,600 debris chunks in both cases. In both \ncases, the debris rigid bodies collide with each other, the gameplay rigid bodies, and the \ntens of thousands of triangles that make up the static environment. Because force fields are \nnot simulated on the GPU in either PhysX or GRB, the force field calculation is excluded in \nboth cases. \nThe test was run for the first 900 frames starting at the second-to-last checkpoint of the \nlevel, where a large concrete wall is destroyed and carried away by a force field. The \ntimings are from a representative frame captured after all the chunks have been loosened \nfrom the wall. Because the timings are of the physics pipeline only, they don‘t contain \ngraphics processing time or any other game code. The timings were taken on an Intel Core \n2 Quad running at 2.66 GHz with the graphics running on a GTX260 and the physics running \non a second GTX260. \nGRB physics frame time (1,600 bodies) \n16.5ms \nPhysX physics frame time (1,600 bodies) 49.5ms \nSpeedup \n3.0x \n \nIn a real game, GRB running on a GPU can achieve a threefold performance increase over \nPhysX running on a CPU. \n \nAcknowledgements \nGRB was written by Richard Tonge, Simon Green, Steve Borho, Lihua Zhang, and Bryan \nGaldrikian. We‘d like to thank Adam Moravanszky, Pierre Terdiman, Dilip Sequeira, Mark \nHarris, and all the other people who indirectly contributed to GRB. The GRB Batman \n",
      "page_number": 529
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 537-549)",
      "start_page": 537,
      "end_page": 549,
      "detection_method": "topic_boundary",
      "content": " \n \ndestruction content was designed by Dane Johnston and Johnny Costello, and the code was \nintegrated into the game by James Dolan, David Sullins, and Steve Borho. \n \nReferences \n[Ericson05] Ericson, Christer. Real-Time Collision Detection. Morgan Kaufmann, 2005. \n[Erleben04] Erleben, Kenny. ―Stable, Robust, and Versatile Multibody Dynamics Animation.‖ \nPhD thesis. Department of Computer Science, University of Copenhagen, 2004. \n[Green07] Green, Simon. ―CUDA Particles.‖ NVIDIA Corporation. Whitepaper, 2007. \n[Harada07] Harada, Takahiro. ―Real Time Rigid Body Physics on GPUs.‖ GPU Gems 3. \nAddison Wesley, 2007. \n[Harmon09] Harmon, David, et al. ―Asynchronous Contact Mechanics.‖ SIGGRAPH ‘09: ACM \nSIGGRAPH 2009 papers (2009): 1–12. \n[Harris07] Harris, Mark. ―Scan Primitives for GPU Computing.‖ ACM SIGGRAPH/Eurographics \nConference on Graphics Hardware (2007): n.p. \n[Karabassi99] Karabassi, Evaggelia-Aggeliki, et al. ―A Fast Depth-Buffer-Based Voxelization \nAlgorithm.‖ Journal of Graphics Tools 4.4 (1999): n.p. \n[LeGrand07] LeGrand, Scott. ―Broad-Phase Collision Detection with CUDA.‖ GPU Gems 3. \nAddison Wesley, 2007. \n[Nvidia08] ―NVIDIA PhysX SDK 2.8.1 Documentation.‖ NVIDIA Corporation, 2008. \n[Satish09] Satish, Nadathur, et al. ―Designing Efficient Sorting Algorithms for Manycore \nGPUs.‖ Proceedings of the 23rd IEEE International Parallel & Distributed Processing \nSymposium (2009): n.p. \n \n7.3. Fast GPU Fluid Simulation in PhysX \nSimon Schirm and Mark Harris, NVIDIA Corporation \nsschirm@nvidia.com, harrism@nvidia.com \nOver the past several years, physically based simulation has become an important feature \nof games. Effects based on physical simulation of phenomena such as liquid, smoke, \nparticulate debris, cloth, and rigid body dynamics greatly increase the level of realism and \ndetail in a game. These effects can be expensive to simulate using traditional sequential \napproaches, especially in a complex game engine where the CPU is already handling many \nexpensive tasks. On the other hand, there is a lot of data parallelism in many physical \nsimulations, and the GPUs in most modern PCs and consoles are powerful parallel \nprocessors that can be applied to this task. Simulating fluids and particles on the GPU \nenables games to incorporate complex fluids and particle effects with tens of thousands of \nparticles that interact with the player and the game scenery. \nNVIDIA PhysX is a real-time physics engine for games that uses the GPU in concert with the \nCPU to accelerate physically based simulation of particle systems, fluids, cloth, and rigid \n\n\n \n \nbody dynamics [NVIDIA09a]. The GPU-accelerated portions of the engine are implemented \nusing the C for CUDA language and the CUDA run-time API. The standard C and C++ \nlanguage support in the CUDA compiler makes programming, debugging, and maintaining \nthe complex algorithms used in PhysX much easier and ensures that they can be integrated \nefficiently with the CPU portions of the engine. For more information on CUDA, see \n[NVIDIA09b]. \nIn this gem, we present the algorithms and implementation details for the particle and fluid \nsystems in PhysX. We describe the features of each system and then present an overview of \nthe algorithm and details of the implementation in CUDA. Figure 7.3.1 and Color Plate 15 \nare examples of the results of the techniques described in this gem. \nFigure 7.3.1. Real-time GPU-accelerated Smoothed Particle Hydrodynamics (SPH) \nwater simulation in PhysX. There are more than 60,000 particles in the simulation. \nThe rendering is performed using a screen-space splatting technique. \n \n \n \nParticle Systems \nParticle systems are ubiquitously used in games to create a variety of effects. Particles can \nbe used to represent media ranging from highly granular materials, such as leaves, gravel, \nand sparks, to more continuous media, such as smoke or steam. Particle effects in games \nare typically tailored to interact as lightly as possible with the scene in order to reduce \ncomputational overhead. This is done, for example, by applying pre-computed forces to the \nparticles in a way that doesn‘t require any intersection tests or collision detection. The \nrange of effects that can be achieved is therefore significantly limited. One goal of PhysX is \nto relax these restrictions by moving expensive but highly parallel calculations to the GPU, \nenabling particles to interact with geometrically rich and dynamic scenes. \n \nFluids \nReal-time simulation of fluids can dramatically increase the realism and interactivity of \nsmoke, steam, and liquid effects in games. The PhysX SDK supports the simulation of \ngaseous and liquid fluids by adding support for particle-particle interactions to its particle \n\n\n \n \nsystems. Inter-particle forces are defined so that the volume the particles occupy is \napproximately conserved over time. \nPhysX fluid simulation is built on top of the particle system component, sharing the same \ncollision algorithm and spatial partitioning data structure. Like particle systems, fluid motion \nis not practically limited to a box domain; particles and fluids can spread throughout a game \nlevel. \n \nRobust Particle Collisions \nPhysX particles are allowed to interact with arbitrary triangle meshes in a game scene, and \ntherefore we need an algorithm that can robustly handle particle collisions with these \nmeshes. This is particularly challenging in the presence of inter-particle forces, which tend \nto literally push particles into all the edge and corner cases. Our algorithm has the following \nproperties. \n \nPenetration and artificial energy loss are minimized so that particles don‘t leak or \nstick as shown in Figure 7.3.2: Left. \nFigure 7.3.2. Left: Particles don’t stick or leak in edge cases. \nRight: Particles don’t fall through static meshes when pushed by dynamic objects. \n \n \nA configurable collision radius is provided. \n \nHigh-velocity collisions are handled. (The collision radius is usually small compared \nto the distance a fast particle travels per time step.) \n \nParticles on thin static objects cannot be pushed through by dynamic objects \npenetrating the static geometry, as shown in Figure 7.3.2: Right. \nThe PhysX particle collision algorithm is based on a mixed approach that makes use of both \ncontinuous and discrete collision detection, as illustrated in Figure 7.3.3. \nFigure 7.3.3. Left: Continuous collision detection (CCD) with multiple shapes. \nRight: A discrete collision detection test between a particle and geometry. \n \n \nThe linear path of the particle motion is tested for intersection with any dynamic or static \nobjects in order to find the first impact location. For dynamic objects, the intersection test \n\n\n \n \nmust be handled with extra care. Each particle‘s original position p is transformed into the \nlocal space of the moving object according to its original pose. The target position q is \ncorrespondingly transformed according to the target pose of the object, as shown in Figure \n7.3.4. Now the linear path defined by the transformed local positions p‘ and q‘ is tested \nagainst the object in its local space. This is just an approximation, since the motion path of \nthe particle observed from the moving dynamic geometry would generally be curved. \nNonetheless, this approximation has the nice property that the test is topologically correct \nwhen multiple compound geometries are involved. Practically, this means that a particle-\nfilled hollow box doesn‘t leak particles when shaken. \nFigure 7.3.4. Left: Continuous collision detection with a dynamic shape. \nRight: The same situation as on the left, but relative to a moving shape. Instead of \nintersecting the curved particle path, a linear approximation is used. \n \n \nIn addition to the continuous collision test, the sphere defined by the target particle position \nand radius is tested for intersections with nearby geometry, as shown in Figure 7.3.3: Right. \nBoth results are combined to calculate an appropriate collision response for the particle. \nThe collision algorithm must gracefully handle cases in which particles could get trapped \nbetween penetrating scene objects, as shown in Figure 7.3.2: Right. In practice, it makes \nsense to give collisions with static objects precedence over collisions with dynamic objects. \nThis is achieved by dividing the collision algorithm into a first pass handling dynamic objects \nand a second pass handling static objects. \nParticles are often continuously pushed against scene geometry by gravity or fluid forces. In \nthese cases, the particle path intersects close to the particle‘s current position. Correct \ncontinuous collision detection will prevent a particle from sliding along object boundaries. \nOne way to overcome this is to correct the velocity of the particle and check its new path for \npotential collisions again for the rest of the time step. This can be repeated until the end of \nthe time step is reached and the final position of the particle is found. For a particle in a \ncorner, this iteration might take forever. To solve this problem, our algorithm remembers \nthe surfaces a particle collided with in the previous time step and uses them in the current \ntime step to correct the particle velocity in a pre-process to collision detection. In practice, it \nturns out that storing two planes— called surface constraints—per particle is enough to \nprovide good results in most cases. \n \nFluid Simulation with Smoothed Particle Hydrodynamics \nTo simulate a fluid, one must compute the mass (and other field values) carried by the fluid \nthrough space over time. There are two fundamental approaches to fluid simulation: \n \nEulerian simulations track fluid field values at fixed locations in space—for example, \non a grid. \n\n\n \n \n \nLagrangian simulations track fluid field values at locations moving with the flow—for \nexample, by using particles. \nIn PhysX, using particles has several advantages. \n \nParticles are already used in games, which eases the integration. \n \nParticles enable seamless collisions with static geometry and rigid bodies with no \nboundary resampling. \n \nMass conservation is trivially achieved by associating a constant mass with each \nparticle. \nThe approach we use in PhysX is called Smoothed Particle Hydrodynamics (SPH), which is a \nLagrangian simulation technique that was originally developed to simulate the motion of \ngalaxies [Monaghan92] and later applied to free-surface fluid flow simulation. SPH is an \ninterpolation method that allows field values that are defined only at particle positions to be \nevaluated anywhere in space [Müller03]. The method is based on the Navier-Stokes \nequations, which describe fluid motion by relating continuous velocity, density, and pressure \nfields over time. The two equations enforce conservation of mass and momentum. Since \nmass is conserved by using discrete particles, we only use the momentum equation. In \nLagrangian form, it reads as follows: \nEquation 1  \n \n \nAll terms in Equation (1) are derived from local field properties at the location xi of particle i. \nThe mass of each particle is constant over time. \nrepresents the acceleration of the \nparticle, and the three terms on the right-hand side describe contributions to the \nacceleration. \n1. External acceleration g, including gravity and collision impulses. \n2. Force due to the fluid pressure gradient ▽p|xi. \n3. Force due to the viscosity of the fluid, given by the Laplacian of the velocity field \n▽2v|xi weighted by the scalar viscosity of the fluid μ. \nThe pressure and viscosity terms each represent a so-called ―body force,‖ which is a force \nper unit volume. To convert a body force into acceleration using Newton‘s second law, it \nmust be normalized by the mass density ρi at the location of the particle, as shown in \nEquation (1). \nThe particles act as sample points for the continuous physical fields. Radially symmetric \nkernel functions are used to reconstruct smooth fields from the discrete particle samples, \nhence the name Smoothed Particle Hydrodynamics. \nTo evaluate a field value at a certain location, the contributions of the overlapping kernel \nfunctions at each nearby particle are summed up, as shown in Figure 7.3.5. \nCorrespondingly, the pressure gradient and the velocity Laplacian can be derived using the \nkernel functions, as described by [Müller03]. Choosing kernels with a finite non-zero domain \nis crucial to accelerating the search for nearby particles. \nFigure 7.3.5. A one-dimensional field reconstruction formed by the summation of \noverlapping kernels for each particle. \n\n\n \n \n \n \nTo compute the acceleration on each particle in the fluid, we need to evaluate Equation (1) \nat the particle position by summing up the kernel contributions from all overlapping \nparticles. In practice, this requires evaluating three physical fields: density, pressure, and \nviscosity. This can be done in two passes: one to evaluate the density field and another to \nevaluate pressure and viscosity, which both depend on density. After evaluating Equation \n(1) in this way, we have an acceleration that can be used to integrate the velocity and \nposition of the particle over the time step. \nFor further details on SPH and our application of the method, see [Müller03]. \n \nFluid Simulation Algorithm \nFigure 7.3.6 shows an overview of the fluid simulation pipeline. First, the spatial domain \nthat is covered by the particles is approximated with a set of bounds. These are inserted \ninto the broad phase along with the bounds of static and dynamic objects in the scene. In \nparallel with the broad phase, the SPH phase computes fluid forces and uses them to \nupdate the particle velocity. The particle collision phase uses the output of the broad phase \nand the SPH phase to compute collisions with dynamic and static objects and applies \nvelocity impulses to resolve any penetration. Finally, the positions of all particles are \nupdated using their newly computed velocities. \nFigure 7.3.6. Particle pipeline overview. \n\n\n \n \n \n \n \nParticle Bounds Generation and the Broad Phase \nTo support efficient collision detection between particles and static geometry and rigid \nbodies present in the scene, we need to avoid testing all possible particle-object pairs for \nintersections. The so-called broad phase collision detection algorithm compares the spatial \nbounds of scene objects to find all potentially colliding pairs. Checking the bounds of all \nindividual particles would overload the broad phase. Therefore, we generate bounds for \ngroups of spatially coherent particles. The particles are associated with a particular particle \ngroup based on a hash grid similar to the one used for finding neighbors in SPH but with \nmuch coarser grid spacing. The particle collision algorithm uses the output of the broad \nphase, a set of pairs of overlapping particle groups and scene objects, to find the \nintersections of individual particles with scene objects. The broad phase can run in parallel \nin a separate CPU thread while the GPU executes the SPH phase, since the result of the \nbroad phase is only needed before the collision phase starts. \nSPH Phase \nThe SPH phase computes the density and force at each particle position xi. The force \ncomputation evaluates the influence of all other particles in a neighborhood around xi. In \norder to find neighboring particles efficiently, we use a hashed regular grid to accelerate \nfinding nearby particles. A hashed grid implementation similar to the one used in PhysX is \ndemonstrated in the ―particles‖ sample in the NVIDIA CUDA SDK [Green09], as described \nby [Teschner03]. \nHashed Grid Data Structure \nBuilding the hashed grid data structure requires two steps. The first step computes a hash \nindex for each particle such that all particles in the same grid cell have the same hash \nindex. The second step sorts all particle data by hash index so that the data for all particles \nin the same hash cell is contiguous in memory. \n\n\n \n \nTo compute the hash index, each particle position (x, y, z) is discretized onto a virtual (and \ninfinite) 3D grid by dividing its coordinates by the grid cell size and rounding down to the \nnearest integer coordinates (i, j, k) = ([x/h], [y/h], [z/h]). These coordinates are then used \nas input to a hash function that computes a single integer hash index. Our hash function is \nsimple; it computes the integer coordinates modulo the grid size and linearizes the resulting \n3D coordinates, as shown in Listing 7.3.1. This simple hash function has the important \nproperty that given a hash index, computing the hash indices of neighboring cells is trivial, \nbecause neighboring cells in the virtual grid map to neighboring cells in the hash grid \n(modulo the grid size). \nListing 7.3.1. The virtual grid hash function. Note that GRID_SIZE must be a \npower of 2. \nint3 gridPosition(float3 pos, float cellSize) \n{ \n \n   int3 gridPos; \n   gridPos.x = floor(pos.x/cellSize) & (GRID_SIZE – 1); \n   gridPos.y = floor(pos.y/cellSize) & (GRID_SIZE – 1); \n   gridPos.z = floor(pos.z/cellSize) & (GRID_SIZE – 1); \n   return gridPos; \n} \nint hash(int3 gridPos) \n{ \n   return (gridPos.z * GRID_SIZE + gridPos.y) * GRID_SIZE + \ngridPos.x; \n} \n \nThe hash values for each particle are computed in a CUDA C kernel that outputs an array of \nhash indices and an array of corresponding particle indices. These are then used as arrays \nof keys and corresponding indices, which we sort using the fast CUDA radix sort of \n[Satish09]. For each hash grid cell, we also store the sorted indices of the first and last \nparticle in the cell, to enable constant-time queries of the particles in each hash cell. After \nthe key-index pairs are sorted, we use the sorted indices to reorder the particle position and \nvelocity arrays. Reordering the data arrays is important because it ensures efficient, \ncoalesced memory accesses in subsequent CUDA computations. \nUsing parallel sorting to build this ―list of lists‖ grid structure is more efficient than building \nit directly (in other words, by appending each particle to a list for each grid cell) in parallel, \nbecause we don‘t know a priori which cells are occupied and how many particles are in each \ncell. An advantage of using a radix sort is that multiple non-interacting fluids can be \ncombined within the same virtual grid at low additional cost, because doubling the number \nof fluids only adds one bit to the sort key length. We simply process the particles of all fluids \ntogether in parallel and append the fluid ID above the most significant bit of the hash index. \nThen when we sort the particles, they will be sorted first by fluid ID and then by hash index. \nThis enables us to do all subsequent steps in the simulation of all fluids in parallel. To \nreduce sorting cost, we sort using only as many bits as we need to represent our chosen \ngrid resolution, which currently is log2 Nf + 18 bits (64×64×64 grid), where Nf is the \nnumber of fluids. \nWith the use of a hash, we can handle an unlimited grid. Therefore, the fluid is not confined \nto a box but can flow anywhere in the scene. Naturally, in a widely dispersed fluid, particles \nfrom far-apart cells may hash to the same index. If many non-interacting particles hash to \nthe same index, we will perform unnecessary work to evaluate their Euclidean distance, but \n\n\n \n \nin practice this has not been a problem. For more on evaluating hash functions and hash \ntable size, see [Teschner03]. \nComputing Density and Force \nOnce the hashed grid is built, it can be used to quickly find all particles near to each \nparticle. Thus, we can evaluate the density and force by finding the cell each particle resides \nin and iterating over the 27 neighboring cells in 3D. At each neighboring cell, we perform a \nhash table lookup to find the list of particles in each cell. Using these particles, we can \ncompute the sum of the influences of each neighboring particle, as shown in Listing 7.3.2. \nListing 7.3.2. Pseudocode for evaluating a smoothing kernel at a particle’s position \nSumOfSupportFunctions(float3 pos, float cellSize) \n{ \n \n   int3 gridPos = gridPosition(pos, cellSize); \n   for (int i = -1; i <= 1; i++) \n      for (int j = -1; j <= 1; j++) \n         for (int k = -1; k <= 1; k++) { \n             gridPosition += int3(i, j, k); \n             particles = hashTableLookup(hash(gridPosition)) \n             foreach particle p in particles { \n                sum += supportFunction(pos, p.position); \n             } \n         } \n} \n \nThe procedure in Listing 7.3.2 can be applied to both of the steps required to compute the \nbody forces in Equation (1). First, it is used to compute density using the density kernel \nfunction. Then, the density is used to compute pressure and viscosity using the pressure \nand viscosity kernel functions. For details of these kernel functions, see [Müller03]. \nParallel Implementation in CUDA \nThe four SPH steps shown in Figure 7.3.6 are implemented using one CUDA C kernel each \nto compute the particle hash, the density, and the force at each particle, plus a radix sort, \nwhich is implemented using multiple CUDA C kernels as described in [Satish09], and a \nkernel that finds the starting and ending indices of particles in each cell and reorders the \nparticle position and velocity arrays using the sorted indices. \nThe particle hash kernel simply computes the hash index for each particle given its 3D \nposition using the function in Listing 7.3.1. We launch as many GPU threads as there are \nparticles in the scene and compute one hash per thread. After sorting the particles by hash \nindex using the radix sort, we launch another kernel, again with one thread per particle, to \nreorder the particle position and velocity arrays. This kernel also compares each particle‘s \nhash index against those before and after it in the sorted array. If it is different from the \nindex before it, then we know this is the first particle in a cell, so we store this particle‘s \nindex in the sorted array to a ―cell starts‖ array at the cell index. If it is different from the \nindex after it, then it is the end of a cell, so we store its index in a ―cell ends‖ array. \nThe density and force kernels also process one particle per thread. Each thread runs the \nprocedure given in Listing 7.3.2. The density kernel uses the density support function given \n\n\n \n \nin [Müller03] as its support function to compute a scalar density per particle. The force \nkernel computes two 3D force sums, one using the pressure gradient support function and \none using the viscosity Laplacian support function, both given in [Müller03]. The \nhashTableLookup function in Listing 7.3.2 simply reads the indices at the cell‘s index in \nthe cell start and cell end arrays to get a range of particle indices to read from the sorted \nparticle position and velocity arrays. \nWe make several optimizations to our CUDA C kernels to ensure best performance. First, we \npack together the data for all active particle systems or fluids into large batches so that the \nCUDA kernels can process all particles at once. This improves utilization of the GPU in the \npresence of multiple small particle systems. We add a small amount of padding between the \nend of one particle system‘s data and the beginning of the next to ensure that each particle \nsystem begins on an aligned boundary, and we use structures of arrays (SOA) rather than \narrays of structures for the particle data. This means that instead of having an array of \nparticle structures containing particle position, velocity, and other data, we have an array of \nall particle positions, an array of all velocities, and so on. These optimizations ensure that \nparallel thread accesses to the data arrays are coalesced by the GPU [NVIDIA09b]. Finally, \nthe density and force kernels bind a texture reference to the data arrays so that the \nspatially coherent but non-sequential access pattern of these kernels can benefit from the \ntexture cache hardware on the GPU [NVIDIA09b]. \nParticle Collision Phase \nThe collision phase uses the particle positions and updated velocities from the SPH phase \n(which includes external accelerations, such as gravity) to compute intersections with scene \ngeometry and rigid bodies and to prevent penetration. The collision algorithm is composed \nof the following sequence of stages: \n1. Surface constraint collision \n2. Collision detection with dynamic rigid body shapes \n3. Dynamic collision response \n4. Collision detection with static rigid body shapes \n5. Static collision response \n6. Update velocity and position \nWithin each stage we can treat particles independently, and therefore we typically use one \nCUDA thread per particle. The particle data as well as any temporary per-particle storage is \nlaid out as structures of arrays (SOA) for efficient coalesced memory access. As with the \nSPH stage, in order to improve the utilization of the GPU, each CUDA kernel processes \nmultiple particle systems together in one batch. \nSurface Constraint Collision \nThe first collision CUDA kernel loads up to two surface constraints from the previous time \nstep for each particle, if available. From the current particle position and velocity, we \ncompute a target position. The target position is projected out of the surface to get a \nmotion path that is not in conflict with the surface the particle is in contact with. The target \nposition is then passed on to the next stage. \nCollision Detection with Static and Dynamic Shapes \nIntersection tests for static and dynamic shapes can be handled with the same set of CUDA \nkernels, treating a static shape as a special case of a dynamic shape that doesn‘t move. The \ncollision detection described in this section is therefore executed twice— first for dynamic \nshapes and then for static shapes. \nFor each spatially coherent group of particles, we need to find all relevant intersections with \nall shapes that are provided by the broad phase for this group. To improve execution \n\n\n \n \ncoherence on the data-parallel GPU architecture, we run a different CUDA kernel for each \nshape type. Our collision algorithm handles collisions with capsules, convex meshes, and \ntriangle mesh shape types, so we have three shape-specific collision kernels. \nEach shape-specific kernel iterates over its shapes by looping over batches of shape data \n(capsule extents and radii, triangles, or planes depending on shape type) that are loaded by \nall threads cooperatively from global device memory into fast on-chip shared memory. \nThen, for each loaded batch, each thread tests one particle for intersection with all shapes \nin the batch and aggregates the contact information. \nIn the case of triangle meshes, an additional optimization improves GPU utilization when \nthere are more triangles than particles in the spatial group. Instead of using one thread per \nparticle, it is more efficient to parallelize across triangles (in other words, run one thread \nper triangle). However, since particles may collide with multiple triangles, we would need to \nstore contact information per particle-triangle pair, which is not memory efficient. Our \napproach is to perform parallel early-out tests for which the results (one bit per test) can be \nstored very efficiently. This is done as follows. For each triangle, all particles are \ninvestigated to find the ones that potentially collide, ruling out the others with a relatively \ncheap conservative intersection test. The per-particle contact generation can then skip \ntriangles for which the early-out test had a negative result. \nCollision Response and Particle Update \nAfter the dynamic shape collision detection, a CUDA kernel computes a collision response \nfrom the aggregated surface contact information and updates particle target positions \naccordingly. After the static shape collision detection, the collision response kernel is \nexecuted again, providing the final particle position. \n \nFluid Rendering \nDisplaying materials in games that are represented as particles requires very different \napproaches depending on the material. Granular materials, such as gravel, leaves, or \nsparks, can be rendered using instanced meshes or billboard techniques, such as impostors. \nGaseous fluids are typically displayed using a billboard per particle and various blending \nmethods. Adding environmental shadowing and self-shadowing improves the realism with \nsome added performance cost. The ―Smoke Particles‖ sample in the NVIDIA CUDA SDK \ndemonstrates this technique [Green09]. Rendering liquids based on particle simulation is \nparticularly challenging, since a smooth surface needs to be extracted. \nScreen-Space Fluid Rendering \nExtracting a surface from a set of points is typically done using the grid-based marching \ncubes algorithm [Lorensen87], which creates the surface by marching through voxels and \ngenerating triangles in cells through which an isosurface is found to pass. This method, \nhowever, doesn‘t provide real-time performance for the resolution and particle count we are \ntargeting. A real-time alternative is the approach described by [Müller07], which simplifies \nthe marching cubes to marching squares to generate a surface mesh in screen space. A \nfurther simplification is to avoid any polygonization of the surface and work entirely at the \npixel level. This is the approach used in the demo shown in Figure 7.3.1 [NVIDIA08]. \nOur screen-space rendering approach is as follows. \n1. Render the scene to a color texture and to the depth buffer. \n2. Render all particles to a depth texture as sphere point sprites. \n3. Render the particle thickness to a ―thickness‖ texture with additive blending. \n\n\n \n \n4. Smooth the depth texture to avoid a ―particle appearance.‖ \n5. Compute a surface normal texture from the smoothed depth using central \ndifferencing. \n6. Use the normal, depth, and thickness to render the fluid surface into the scene color \ntexture with proper depth testing, reflection, refraction, and Fresnel term. \nThese steps are all implemented in a standard graphics API using pixel and vertex shaders. \nThe smoothing step is very important; without it, the fluid will look like a collection of \ndiscrete particles. There are multiple ways to implement this smoothing. The basic approach \nused in Figure 7.3.1 is to smooth the depth texture using a separable Gaussian blur filter. \nThis is inexpensive but can lead to a somewhat ―blobby‖ appearance. A more sophisticated \napproach is to apply curvature flow, as described by [van der Laan09]. Their technique \nachieves much more smoothing in areas of low curvature, while maintaining sharp details. A \ncomparison of the two techniques is shown in Color Plate 16. \n \nPerformance \nWe measured the performance of the PhysX particle fluid demo shown in Figure 7.3.1 and \nColor Plate 16 on a PC with Intel Core 2 Quad CPU and an NVIDIA GeForce GTX 260 GPU. \nThe demo was run with a screen resolution of 1024×768 and an average of about 60,000 \nparticles. Figure 7.3.7 presents the results. The time taken for rendering and CUDA kernels \nin PhysX is about balanced; on the other hand, the PhysX SDK work executed on the CPU \ncontributes relatively strongly. This can be improved by moving all of the per-particle \nprocessing onto the GPU. The executions of the CUDA kernels take 8.2 ms on average. For \nSPH, the summation passes over particle neighbors that compute density and force take \nmost of the time. For collision, the largest portion is contributed from collisions with the \nstatic triangle mesh. \nFigure 7.3.7. Left: Overall frame run time. Middle, right: Cuda kernel run time for \nSPH and collision phases. The gaps display contributions from remaining kernels \nof the respective phase. ―Radix‖ refers to the radix sort kernels. \n \n \n \nAcknowledgements \nThe CUDA fluid simulation implementation and the ideas behind it are the fruits of the \nefforts of a large team. We would like to thank all the members of the NVIDIA PhysX team \nand others at NVIDIA for their contributions, especially Steve Borho, Curtis Davis, Stefan \nDuthaler, Isha Geigenfeind, Monier Maher, Simon Green, Matthias Müller-Fischer, Ashu \n\n\n \n \nRege, Miguel Sainz, Richard Tonge, Lihua Zhang, and Jiayuan Zhu. We would also like to \nthank Gareth Ramsey and Eidos Game Studios for the use of Color Plate 15. \n \nReferences \n[Green09] Green, S. ―Particles.‖ and ―Smoke Particles.‖ n.d. NVIDIA CUDA SDK version 2.3. \nn.d. <http://www.nvidia.com/object/cuda_get.html>. \n[Lorensen87] Lorensen, W. E., and H. E. Cline. ―Marching Cubes: A High Resolution 3D \nSurface Construction Algorithm.‖ Computer Graphics 21.4 (July 1987): n.p. \n[Monaghan92] Monaghan, J. J. ―Smoothed Particle Hydrodynamics.‖ Annual Review of \nAstronomy and Astrophysics 30 (1992): 543–574. \n[Müller03] Müller, M., et al. ―Particle-Based Fluid Simulation for Interactive Applications.‖ \nSymposium on Computer Animation (2003): 154–159. \n[Müller07] Müller, M., S. Schirm, and S. Duthaler. ―Screen-Space Meshes.‖ Symposium on \nComputer Animation (2007): 9–15. \n[NVIDIA08] NVIDIA Corporation. ―Fluids: Technology Demo.‖ 2008. NVIDIA. n.d. \n<http://www.nvidia.com/content/graphicsplus/us/download.asp>. \n[NVIDIA09a] NVIDIA Corporation. ―NVIDIA PhysX.‖ 2009. NVIDIA. n.d. \n<http://www.nvidia.com/object/physx_new.html>. \n[NVIDIA09b] NVIDIA Corporation. ―NVIDIA CUDA Programming Guide.‖ 2009. NVIDIA. n.d. \n<http://www.nvidia.com/cuda>. \n[Satish09] Satish, N., M. Harris, and M. Garland. ―Designing Efficient Sorting Algorithms for \nManycore GPUs.‖ Proceedings of IEEE International Parallel and Distributed Processing \nSymposium (2009): to appear. \n[Teschner03] Teschner, M. et al. ―Optimized Spatial Hashing for Collision Detection of \nDeformable Objects.‖ Proceedings of VMV (2003): 47–54. \n[van der Laan09] van der Laan, W., S. Green, and M. Sainz. ―Screen Space Fluid Rendering \nwith Curvature Flow.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames (2009): 91–98. \n \nColor Plate \nColor Plate 1: Final results of the SSAO technique presented in Gem 1.2. The top-left pane \nshows lighting without the ambient occlusion, while the top-right pane shows lighting with \nthe SSAO component mixed in. The final colored result is shown in the bottom image. Here \nthe SSAO samples are very wide, bathing the background area with an effect that would \notherwise only be obtained with a thorough global illumination algorithm. The SSAO term \nadds depth to the scene and helps anchor the characters within the environment. \n",
      "page_number": 537
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 550-557)",
      "start_page": 550,
      "end_page": 557,
      "detection_method": "topic_boundary",
      "content": " \n \n \nColor Plate 2: Comparison of large- versus small-area SSAO samples from Gem 1.2. The \nimages show the contrast between the large-area, low-contrast SSAO sampling component \non the bar surface and background in the image on the left and the tighter, higher-contrast \nSSAO samples apparent within the helmet and nooks and crannies found on the \ncharacter’s space suit in the middle image. Final output is on the right. \n \nColor Plate 3: Output of rendering stages and final image from the deferred rendering \ntechnique described in Gem 1.3. Position and material ID (top left), normal and depth G-\nbuffers (bottom left), and a resultant image (right). \n\n\n \n \n \nColor Plate 4: The deferred shading technique presented in Gem 1.2 has three steps: \ngeometry pass, multi-resolution rendering pass, and composite pass. \n \nColor Plate 5: A screenshot of Blur. The SPUs are used to compute position and normal \noffsets to deform the car as it takes damage. More details are in Gem 1.8. \n\n\n \n \n \nColor Plate 6: A screenshot of Blur, showing a final result of the SPU-based lighting system \npresented in Gem 1.8. \n \nColor Plate 7: Test face models while performing basic expressions from the anatomical \nhuman face model presented in Gem 2.1. From left to right: neutral, joy, sadness, surprise, \ndisgust, and fear. \n\n\n \n \n \nColor Plate 8: Segmentation results and piecewise convex approximations of different 3D \nmeshes presented in Gem 2.8. (T number of triangles of the original mesh, D length of the \ndiagonal of its bounding box, α = 0.03 × D and the K number of the ACD clusters.) \n\n\n \n \n \nColor Plates 9 and 10: From Gem 3.4, examples of characters in World of Zoo’s (WOZ’s) \nenvironment. Characters move through the environment in a spatially appropriate and \nconsistent manner when using the technique presented in this gem. \n\n\n \n \n \nColor Plate 11: Example output from Gem 4.8 demonstrating an approach to testing code \ncoverage. © Courtesy of Crytek GmbH and Electronic Arts, Inc. \n \nColor Plate 12: Destructible wall section being placed into Batman in Unreal Editor from \nGem 7.2. \n\n\n \n \n \nColor Plates 13 and 14: Destruction showing sphere representation of rigid bodies and \nrendered geometry from Gem 7.2. \n \nColor Plate 15: Batman: Arkham Asylum with interactive PhysX SPH ground smoke, as \ndescribed in Gem 7.3. \n\n\n \n \n \nColor Plate 16: A close-up of screen-space fluid rendering with depth smoothing via \nGaussian blur (left) and curvature flow (right) from Gem 7.3. The rendering on the right \nalso uses surface Perlin noise to add visual detail. \n \n \n \n",
      "page_number": 550
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": " \n \nCopyright \n \nGame Programming Gems 8 \nEdited by Adam Lake \n \nPublisher and General Manager, \nCourse Technology PTR: \nStacy L. Hiquet \n \nAssociate Director of Marketing: \nSarah Panella \n \nManager of Editorial Services: \nHeather Talbot \n \nMarketing Manager: \nJordan Castellani \n \nSenior Acquisitions Editor: \nEmi Smith \n \nProject and Copy Editor: \nCathleen D. Small \n \nInterior Layout: \nShawn Morningstar \n \nCover Designer: \nMike Tanamachi \n \nCD-ROM Producer: \nBrandon Penticuff \n \nIndexer: \nKatherine Stimson \n \nProofreader: \nHeather Urschel \n© 2011 Course Technology, a part of Cengage Learning. \nALL RIGHTS RESERVED. No part of this work covered by the copyright herein may be \nreproduced, transmitted, stored, or used in any form or by any means graphic, electronic, \nor mechanical, including but not limited to photocopying, recording, scanning, digitizing, \ntaping, Web distribution, information networks, or information storage and retrieval \nsystems, except as permitted under Section 107 or 108 of the 1976 United States Copyright \nAct, without the prior written permission of the publisher. \nFor product information and technology assistance, contact us at Cengage Learning \nCustomer & Sales Support, 1-800-354-9706 \nFor permission to use material from this text or product, submit all requests online at \ncengage.com/permissions Further permissions questions can be emailed to \n",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": " \n \nFor product information and technology assistance, contact us at Cengage Learning \nCustomer & Sales Support, 1-800-354-9706 \npermissionrequest@cengage.com \n \nAll trademarks are the property of their respective owners. \nCover image used courtesy of Valve Corporation. \nAll other images © Cengage Learning unless otherwise noted. \nLibrary of Congress Control Number: 2010920327 \nISBN-10: 1-58450-702-0 \neISBN-10: 1-43545-771-4 \nCourse Technology, a part of Cengage Learning \n20 Channel Center Street \nBoston, MA 02210 \nUSA \nCengage Learning is a leading provider of customized learning solutions with office locations \naround the globe, including Singapore, the U nited Kingdom, Australia, Mexico, Brazil, and \nJapan. Locate your local office at: international.cengage.com/region \nCengage Learning products are represented in Canada by Nelson Education, Ltd. \nFor your lifelong learning solutions, visit courseptr.com \nVisit our corporate website at cengage.com \nPrinted in the United States of America \n1 2 3 4 5 6 7 12 1 1 10 \n \nPreface \nWelcome to the eighth edition of the Game Programming Gems series, started by Mark \nDeLoura in 2000. The first edition was inspired by Andrew Glassner‘s popular Graphics \nGems series. Since then, other Gems series have started, including AI Gems and a new \nseries focused on the capabilities of programmable graphics, the ShaderX series. These \ntomes serve as an opportunity to share our experience and best practices with the rest of \nthe industry. \nMany readers think of the Game Programming Gems series as a collection of articles with \nsections that target specialists. For me, I‘ve read through them as a way to get exposure to \nthe diverse subsystems used to create games and stay abreast of the latest techniques. For \nexample, I may not be a specialist in networking, but reading this section will often \nenlighten and stimulate connections that I may not have made between areas in which I \nhave expertise and ones in which I do not. \n",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": " \n \nOne statement I‘ve heard recently regarding our industry is the idea that we now have all \nthe horsepower we need to create games, so innovations by hardware companies are not \nneeded. I believe this argument is flawed in many ways. First, there are continued \nadvancements in graphical realism in academia, in R&D labs, and in the film industry that \nhave yet to be incorporated into our real-time pipelines. As developers adopt these new \nfeatures, computational requirements of software will continue to increase. Second, and the \nmore important issue, is that this concept of play isn‘t entirely correct—the very notion of \nwhat gaming serves from an anthropological perspective. Play is fundamental, not just to \nthe human condition, but to the sentient condition. We invent interactive experiences on \nany platform, be it a deck of cards, a set of cardboard cutouts, or a next-gen PC platform \nwith multi-terabyte data and multi-threaded, multi-gigahertz, multi-processor \nenvironments. It‘s as natural as the pursuit of food. This play inspires real-world \napplications and pushes the next generation of platform requirements. It enables \naffordability of ever-increased computational horsepower in our computing platforms. \nThe extension of gaming into other arenas, mobile and netbook platforms, serves only to \nprove the point. While the same ideas and themes may be used in these environments, the \nexperience available to the player is different if the designer is to leverage the full \ncapabilities and differentiating features of the platform. \nThere is an often-chanted ―ever increasing cost of game development‖ quote for console \nand PC platforms. In the same breath, it‘s alluded that this spiral of cost cannot continue. I \nbelieve these issues are of short-term concern. If there is a community willing to play, our \neconomies will figure out a way to satisfy those needs. This will open up new opportunities \nfor venture capital and middleware to reduce those platform complexities and cross-industry \ndevelopment costs, fueling the next generation of interactive experiences. I do believe the \nprocess has changed and will continue to evolve, but game development will continue to \nthrive. Will there be 15 first-person military simulations on a single platform? Perhaps not, \nbut will there continue to be compelling multiplayer and single-player experiences? I believe \nso. The ingenuity of the game developer, when brought to the task of leveraging new \nincarnations of silicon, will continue to create enriching interactive experiences for ever-\nincreasing audiences. \nFinally, I‘d like to take a moment to address another issue often mentioned in the press. In \nNovember 2009, the Wall Street Journal ran an article by Jonathan V. Last from the Weekly \nStandard discussing the social implications of gaming. The majority of his article, \n―Videogames—Not Only for the Lonely,‖ was making this observation in the context of a \nholiday gathering of family members of many generations sharing experiences with their \nNintendo Wii. Near the end of the article, he refers to the fact that ―the shift to videogames \nmight be lamenting if it meant that people who would otherwise be playing mini-golf or \nMonopoly were sealing themselves off and playing Halo 3 death matches across the \nInternet.‖ Much to the contrary, I have personally spent many quality multiplayer hours \ninteracting socially with longtime friends when playing multiplayer games. A few days ago, I \nwas having a conversation with an acquaintance who was thrilled that she could maintain \nher relationship with her brother on the East Coast by playing World of Warcraft with him. \nUltimately, whether we are discussing our individual game experiences with others or \ninteracting directly while playing, games do what they have always done across generations \nand platforms—they bring us together with shared experiences, whether it be cardboard \ncutouts, a deck of cards, or multiplayer capture the flag. Despite the overall informed \nmessage of the article, the writer encouraged a myth I see repeated in the mainstream \npress by those out of touch with the multiplayer, socially interactive game experiences that \nare common today, including Halo 3. \nOverview of Content \nThe graphics section in this edition covers several topics of recent interest, leveraging new \nfeatures of graphics APIs such as Compute Shader, tessellation using DirectX 11, and two \ngems on the implementation details of Screen Space Ambient Occlusion (SSAO). In the \n",
      "content_length": 4537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": " \n \nphysics and animation section, we have selected a number of gems that advance beyond \nthe basics of the topics such as IK solvers or fluid simulation in general. Instead, these \ngems go deeper with improvements to existing published techniques based on real-world \nexperience with the current state of the art—for example, a simple, fast, and accurate IK \nsolver, leveraging swarm systems for animation, and modeling air and fluid resistance. \nArtificial intelligence, AI, is one of the hottest areas in game development these days. Game \nplayers want worlds that don‘t just look real, but that also feel and act real. The acting part \nis the responsibility of the AI programmer. Gems in the AI section are diverse, covering \nareas such as decision making, detailed character simulation, and player modeling to solve \nthe problem of gold farm detection. The innovations discussed are sure to influence future \ngems. \nIn the general programming section, we have a number of tools to help with the \ndevelopment, performance, and testing of our game engines. We include gems that deal \nwith multi-threading using Intel‘s Thread Building Blocks, an open-source multithreading \nlibrary, memory allocation and profiling, as well as a useful code coverage system used by \nthe developers at Crytek. The gems in the networking and multiplayer section cover \narchitecture, security, scalability, and the leveraging of social networking applications to \ncreate multiplayer experiences. \nThe audio section had fewer submissions than in past years. Why is this? Is the area of \naudio lacking in innovation? Has it matured to the point where developers are buying off-\nthe-shelf components? Regardless, we‘ve assembled a collection of gems for audio that we \nthink will be of interest. In one of the articles in the audio section, we discuss a relatively \nnew idea—the notion of real-time calculation of the audio signal based on the actual physics \ninstead of using the traditional technique of playing a pre-recorded processed sound. As \ngames become more interactive and physics driven, there will be a corresponding demand \nfor more realistic sound environments generated by such techniques enabled with the \nincreasing computational horsepower Moore‘s Law continues to deliver to game developers. \nI‘m excited to introduce a new section in this edition of Game Programming Gems 8 that I‘m \ncalling ―General Purpose Computing on GPUs.‖ This is a new area for the Gems series, and \nwe wanted to have a real-world case study of a game developer using the GPU for non-\ngraphics tasks. We‘ve collected three gems for this section. The first is about OpenCL, a \nnew open standard for programming heterogeneous platforms of today, and we also have \ntwo gems that leverage PhysX for collision detection and fluid simulation. The PhysX \ncomponents were used in Batman: Arkham Asylum by Rocksteady Studios Ltd. As the \ncomputing capabilities of the platform evolve, I expect game developers will face the \ndecision of what to compute, where to compute, and how to manage the data being \noperated upon. These articles serve as case studies in what others have done in their \ngames. I expect this to be an exciting area of future development. \nWhile we all have our areas of specialty, I think it‘s fair to say game developers are a \nhungry bunch, with a common desire to learn, develop, and challenge ourselves and our \nabilities. These gems are meant to insprire, enlighten, and evolve the industry. As always, \nwe look forward to the contributions and feedback developers have when putting these \ngems into practice. \nAdam Lake \nAdam_t_lake@yahoo.com \nAbout the Cover Image \n© Valve Corporation \n",
      "content_length": 3682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": " \n \n \nThe cover of Game Programming Gems 8 features the Engineer from Valve‘s Team Fortress \n2. With their follow-up to the original class-based multiplayer shooter Team Fortress, Valve \nchose to depart from the typical photorealistic military themes of the genre. Instead, they \nemployed an ―illustrative‖ non-photorealistic rendering style, reminiscent of American \ncommercial illustrators of the 1920s. This was motivated by the need for players to be able \nto quickly visually identify each other‘s team, class, and weapon choices in the game. The \nnovel art style and rendering techniques of Team Fortress 2 allowed Valve‘s designers to \nvisually separate the character classes from each other and from the game‘s environments \nthrough the use of strong silhouettes and strategic distribution of color value. \nCD-ROM Downloads \nIf you purchased an ebook version of this book, and the book had a companion CD-ROM, we \nwill mail you a copy of the disc. Please send ptrsupplements@cengage.com the title of the \nbook, the ISBN, your name, address, and phone number. Thank you. \n \nAcknowledgments \nI‘d like to take a moment to acknowledge the section editors that I worked with to create \nthis tome. They are the best and brightest in the industry. The quality of submissions and \ncontent in this book is a testament to this fact. They worked incredibly hard to bring this \nbook together, and I thank them for their time and expertise. Also, I appreciate the time \nand patience that Emi Smith and Cathleen Small at Cengage Learning have put into this \nfirst-time book editor. They were essential in taking care of all the details necessary for \npublication. Finally, I‘d like to acknowledge the artists at Valve who provided the cover \nimage for this edition of Game Programming Gems. \nI have been blessed to have had exposure to numerous inspirational individuals— friends \nwho refused to accept norms, parents who satiated my educational desires, teachers willing \nto spend a few extra minutes on a random tangent, instructors to teach not just what we \nknow about the world, but also to make me aware of the things we do not. Most \nimportantly, I want to acknowledge my wife, Stacey Lake, who remained supportive while I \ntoiled away in the evenings and weekends for the better part of a year on this book. \nI dedicate these efforts to my mother, Amanda Lake. I thank her for teaching me that \neducation is an enjoyable lifelong endeavor. \n \n",
      "content_length": 2445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": " \n \nContributors \nFull bios for those contributors who submitted one can be found at www.courseptr.com/ \ndownloads. Contributors to this book include: \nDr. Doug Binks, D.Phil. \nUdeepta Bordoloi \nIgor Borovikov \nCyril Brom \nEric Brown \nPhil Carlisle \nMichael Dailly \nPeter Dalton \nKevin Dill \nJean-Francois Dube \nDominic Filion \nMarco Fratarcangeli \nNico Galoppo \nBenedict R. Gaster \nGero Gerber \nRobert Jay Gould \nNeil Gower \nJoshua Grass, Ph.D. \nHunter Hale \nMark Harris \nThomas Hartley \nKevin He \nClaus Höfele \nAllen Hux \nPeter Iliev \n",
      "content_length": 537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": " \n \nMatthew Jack \nAleksey Kadukin \nNikhil S. Ketkar \nHyunwoo Ki \nAdam Lake \nMichael Lewin \nChris Lomont, Ph.D. \nRicky Lung \nKhaled Mamou \nDave Mark \nQuasim Mehdi \nKrzysztof Mieloszyk \nJason Mitchell \nBen Nicholson \nIan Ni-Lewis \nMat Noguchi \nBorut Pfeifer \nBrian Pickrell \nTomas Poch \nSteve Rabin \nMike Ramsey \nB. Charles Rasco, Ph.D. \nJoão Lucas G. Raza \nAurelio Reis \nZhimin Ren \nMarc Romankewicz \nDario Sancho \nRahul Sathe \n",
      "content_length": 427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": " \n \nSimon Schirm \nBrian Schmidt \nOndřej Šerý \nPhilip Taylor \nRichard Tonge \nSteven Tovey \nGabriel Ware \nBen Wyatt \nG. Michael Youngblood \nJason Zink \nRobert Zubek \n \nSection 1: Graphics \nIntroduction \nFast Font Rendering with Instancing \nPrinciples and Practice of Screen Space Ambient Occlusion \nMulti-Resolution Deferred Shading \nView Frustum Culling of Catmull-Clark Patches in DirectX 11 \nAmbient Occlusion Using DirectX Compute Shader \nEye-View Pixel Anti-Aliasing for Irregular Shadow Mapping \nOverlapped Execution on Programmable Graphics Hardware \nTechniques for Effective Vertex and Fragment Shading on the SPUs \nIntroduction \nJason Mitchell, Valve \njason@pixelmaven.com \nIn this edition of the Game Programming Gems series, we explore a wide range of \nimportant real-time graphics topics, from lynchpin systems such as font rendering to \ncutting-edge hardware architectures, such as Larrabee, PlayStation 3, and the DirectX 11 \ncompute shader. Developers in the trenches at top industry studios such as Blizzard, id, \n",
      "content_length": 1028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": " \n \nBizarre Creations, Nexon, and Intel‘s Advanced Visual Computing group share their insights \non optimally exploiting graphics hardware to create high-quality visuals for games. \nTo kick off this section, Aurelio Reis of id Software compares several methods for \naccelerating font rendering by exploiting GPU instancing, settling on a constant-buffer-\nbased method that achieves the best performance. \nWe then move on to two chapters discussing the popular image-space techniques of Screen \nSpace Ambient Occlusion (SSAO) and deferred shading. Dominic Filion of Blizzard \nEntertainment discusses the SSAO algorithms used in StarCraft II, including novel controls \nthat allowed Blizzard‘s artists to tune the look of the effect to suit their vision. Hyunwoo Ki \nof Nexon then describes a multi-resolution acceleration method for deferred shading that \ncomputes low-frequency lighting information at a lower spatial frequency and uses a novel \nmethod for handling high-frequency edge cases. \nFor the remainder of the section, we concentrate on techniques that take advantage of the \nvery latest graphics hardware, from DirectX 11‘s tessellator and compute shader to \nLarrabee and the PlayStation 3. Rahul Sathe of Intel presents a method for culling of Bezier \npatches in the context of the new DirectX 11 pipeline. Jason Zink then describes the new \nDirectX 11 compute shader architecture, using Screen Space Ambient Occlusion as a case \nstudy to illustrate the novel aspects of this new hardware architecture. In a pair of articles \nfrom Intel, Nico Galoppo and Allen Hux describe a method for integrating anti-aliasing into \nthe irregular shadow mapping algorithm as well as a software task system that allows highly \nprogrammable systems such as Larrabee to achieve maximum throughput on this type of \ntechnique. We conclude the section with Steven Tovey‘s look at the SPU units on the \nPlayStation 3 and techniques for achieving maximum performance in the vehicle damage \nand light pre-pass rendering systems in the racing game Blur from Bizarre Creations. \n \n1.1. Fast Font Rendering with Instancing \nAurelio Reis, id Software \nAurelioReis@gmail.com \nFont rendering is an essential component of almost all interactive applications, and while \ntechniques exist to allow for fully scalable vector-based font rendering using modern GPUs, \nthe so-called ―bitmap font‖ is still the most versatile, efficient, and easy-to-implement \nsolution. When implemented on typical graphics APIs, however, this technique uses run-\ntime updated vertex buffers to store per-glyph geometry, resulting in inefficient rendering \nperformance by potentially stalling the graphics pipeline. By leveraging efficient particle \nsystem rendering techniques that were developed previously, it is possible to render \nthousands of glyphs in a single batch without ever touching the vertex buffer. \nIn this article, I propose a simple and efficient method to render fonts utilizing modern \ngraphics hardware when compared to other similar methods. This technique is also useful in \nthat it can be generalized for use in rendering other 2D elements, such as sprites and \ngraphical user interface (GUI) elements. \nText-Rendering Basics \nThe most common font format is the vector-based TrueType format. This format represents \nfont glyphs (in other words, alphabetic characters and other symbols) as vector data, \nspecifically, quadratic Bezier curves and line segments. As a result, TrueType fonts are \ncompact, easy to author, and scale well with different display resolutions. The downside of a \nvector font, however, is that it is not straightforward to directly render this type of data on \n",
      "content_length": 3666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": " \n \ngraphics hardware. There are, however, a few different ways to map the vector \nrepresentation to a form that graphics hardware can render. \nOne way is to generate geometry directly from the vector curves, as shown in Figure 1.1.1. \nHowever, while modern GPUs are quite efficient at rendering large numbers of triangles, the \nnumber of polygons generated from converting a large number of complex vector curves to \na triangle mesh could number in the tens of thousands. This increase in triangle throughput \ncan greatly decrease application performance. Some optimizations to this way of rendering \nfonts have been introduced, such as the technique described by Loop and Blinn in which the \npolygonal mesh consists merely of the curve control points while the curve pixels are \ngenerated using a simple and efficient pixel shader [Loop05]. While this is a great \nimprovement over the naive triangulation approach, the number of polygons generated in \nthis approach is still prohibitively high on older graphics hardware (and that of the current \nconsole generation—the target of this article). \nFigure 1.1.1. Vector curves converted into polygonal geometry. \n \n \nBecause of these limitations, the most common approach relies on rasterizing vector \ngraphics into a bitmap and displaying each glyph as a rectangle composed of two triangles \n(from here on referred to as a quad), as shown in Figure 1.1.2. A font texture page is \ngenerated with an additional UV offset table that maps glyphs to a location in that texture \nvery similar to how a texture atlas is used [NVIDIA04]. The most obvious drawback is the \nresolution dependence caused by the font page being rasterized at a predefined resolution, \nwhich leads to distortion when rendering a font at a non-native resolution. Additional \ntechniques exist to supplement this approach with higher quality results while mitigating the \nresolution dependence that leads to blurry and aliased textures, such as the approach \ndescribed by [Green07]. Overall, the benefits of the raster approach outweigh the \ndrawbacks, because rendering bitmap fonts is incredibly easy and efficient. \nFigure 1.1.2. A font page and a glyph rendered on a quad. \n \n \nTo draw glyphs for a bitmap font, the program must bind the texture page matching the \nintended glyph set and draw a quad for each glyph, taking into account spacing for kerning \nor other character-related offsets. While this technique yields very good performance, it can \nstill be inefficient, as the buffers containing the geometry for each batch of glyphs must be \ncontinually updated. Constantly touching these buffers is a sure way to cause GPU stalls, \n",
      "content_length": 2658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": " \n \nresulting in decreased performance. For text- or GUI-heavy games, this can lead to an \nunacceptable overall performance hit. \n \nImproving Performance \nOne way to draw the glyphs for the GUI is to create a GUI model that maintains buffers on \nthe graphics card for drawing a predefined maximum number of indexed triangles as quads. \nWhenever a new glyph is to be drawn, its quad is inserted into a list, and the vertex buffer \nfor the model is eventually updated with the needed geometry at a convenient point in the \ngraphics pipeline. When the time comes to render the GUI model, assuming the same \ntexture page is used, only a single draw call is required. As previously mentioned, this \nbuffer must be updated each frame and for each draw batch that must be drawn. Ideally, as \nfew draw batches as possible are needed, as the font texture page should contain all the \nindividual glyphs that would need to be rendered, but on occasion (such as for high-\nresolution fonts or Asian fonts with many glyphs), it‘s not possible to fit them all on one \npage. In the situation where a font glyph must be rendered from a different page, the batch \nis broken and must be presented immediately so that a new one can be started with the \nnew texture. This holds true for any unique rendering states that a glyph may hold, such as \nblending modes or custom shaders. \nLock-Discard \nThe slowest part of the process is when the per-glyph geometry must be uploaded to the \ngraphics card. Placing the buffer memory as close to AGP memory as possible (using API \nhints) helps, but locking and unlocking vertex buffers can still be quite expensive. To \nalleviate the expense, it is possible to use a buffer that is marked to ―discard‖ its existing \nbuffer if the GPU is currently busy with it. By telling the API to discard the existing buffer, a \nnew one is created, which can be written to immediately. Eventually, the old buffer is \npurged by the API under the covers. This use of lock-discard prevents the CPU from waiting \non the GPU to finish consuming the buffer (for example, in the case where it was being \nrendered at the same time). You can specify this with the D3DLOCK_DISCARD flag in \nDirect3D or by passing a NULL pointer to glBufferDataARB and then calling \nglMapBufferARB(). Be aware that although this is quite an improvement, it is still not \nan ideal solution, as the entire buffer must be discarded. Essentially, this makes initiating a \nsmall update to the buffer impossible. \nVertex Compression \nAnother step in improving performance is reducing the amount of memory that needs to be \nsent to the video card. The vertex structure for sending a quad looks something like this \nand takes 28 bytes per vertex (and 112 bytes for each quad): \nstruct GPU_QUAD_VERTEX_POS_TC_COLOR \n{ \n      D3DXVECTOR4 Position; \n      D3DXVECTOR2 Texcoord; \n      D3DCOLOR Color; \n}; \n \nSince the bandwidth across the AGP bus to the video card is not infinite, it is important to \nbe aware of how much memory is being pushed through it. One way to reduce the memory \ncosts is to use an additional vertex stream to update only that information that has changed \non a per-frame basis. Unfortunately, the three essential quad attributes (position, texture \n",
      "content_length": 3242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": " \n \ndimensions, and color) could be in a state of constant flux, so there is little frame-to-frame \ncoherency we can exploit. \nThere is one very easy way to reduce at least some of the data that must be sent to the \nvideo card, however. Traditionally, each vertex represents a corner of a quad. This is not \nideal, because this data is relatively static. That is, the size and position of a quad changes, \nbut not the fact that it is a quad. Hicks describes a shader technique that allows for aligning \na billboarded quad toward the screen by storing a rightFactor and upFactor for each \ncorner of the billboard and projecting those vertices along the camera axes [Hicks03]. This \ntechnique is attractive, as it puts the computation of offsetting the vertices on the GPU and \npotentially limits the need for vertex buffer locks to update the quad positions. \nBy using a separate vertex stream that contains unique data, it is possible to represent the \nwidth and height of the quad corners as a 4D unsigned byte vector. (Technically, you could \ngo as small as a Bool if that was supported on modern hardware.) In the vertex declaration, \nit is possible to map the position information to specific vertex semantics, which can then be \naccessed directly in the vertex shader. The vertex structure would look something like this: \nstruct GPU_QUAD_VERTEX \n{ \n      BYTE OffsetXY[ 4 ]; \n}; \n \nAlthough this may seem like an improvement, it really isn‘t, since the same amount of \nmemory must be used to represent the quad attributes (more so since we‘re supplying a 4-\nbyte offset now). There is an easy way to supply this additional information without \nrequiring the redundancy of all those additional vertices. \n \nInstancing Quad Geometry \nIf you‘re lucky enough to support a Shader Model 3 profile, you have hardware support for \nsome form of geometry instancing. OpenGL 2.0 has support for instancing using pseudo-\ninstancing [GLSL04] and the EXT_draw_instanced [EXT06] extension, which uses the \nglDrawArraysInstancedEXT and glDrawElementsInstancedEXT routines to \nrender up to 1,024 instanced primitives that are referenced via an instance identifier in \nshader code. \nAs of DirectX 9, Direct3D also supports instancing, which can be utilized by creating a \nvertex buffer containing the instance geometry and an additional vertex buffer with the per-\ninstance data. By using instancing, we‘re able to completely eliminate our redundant quad \nvertices (and index buffer) at the cost of an additional but smaller buffer that holds only the \nper-instance data. This buffer is directly hooked up to the vertex shader via input semantics \nand can be easily accessed with almost no additional work to the previous method. While \nthis solution sounds ideal, we have found that instancing actually comes with quite a bit of \nper-batch overhead and also requires quite a bit of instanced data to become a win. As a \nresult, it should be noted that performance does not scale quite so well and in some \nsituations can be as poor as that of the original buffer approach (or worse on certain \nhardware)! This is likely attributed to the fact that the graphics hardware must still point to \nthis data in some way or another, and while space is saved, additional logic is required to \ncompute the proper vertex strides. \n \n",
      "content_length": 3311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": " \n \nConstant Array Instancing \nAnother way to achieve similar results with better performance is to perform shader \ninstancing using constant arrays. By creating a constant array for each of the separate quad \nattributes (in other words, position/size, texture coordinate position/size, color), it is \npossible to represent all the necessary information without the need for a heavyweight \nvertex structure. See Figure 1.1.3. \nFigure 1.1.3. A number of glyphs referencing their data from a constant array. \n \nSimilar to indexed vertex blending (a.k.a. matrix palette skinning), an index is assigned for \neach group of four vertices required to render a quad, as shown in Figure 1.1.4. To get the \nvalue for the current vertex, all that is needed is to index into the constant array using this \nvalue. Because the number of constants available is usually below 256 on pre–Shader Model \n4 hardware, this index can be packed directly as an additional element in the vertex offset \nvector (thus requiring no additional storage space). It‘s also possible to use geometry \ninstancing to just pass in the quad ID/index in order to bypass the need for a large buffer of \nfour vertices per quad. However, as mentioned previously, we have found that instancing \ncan be unreliable in practice. \nFigure 1.1.4. A quad referencing an element within the attribute constant array. \n",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": " \n \n \n \nThis technique yields fantastic performance but has the downside of only allowing a certain \nnumber of constants, depending on your shader profile. The vertex structure is incredibly \ncompact, weighing in at a mere 4 bytes (16 bytes per quad) with an additional channel still \navailable for use: \nstruct GPU_QUAD_VERTEX \n{ \n      BYTE OffsetXY_IndexZ[ 4 ]; \n}; \n \nGiven the three quad attributes presented above and with a limit of 256 constants, up to 85 \nquads can be rendered per batch. Despite this limitation, performance can still be quite a bit \nbetter than the other approaches, especially as the number of state changes increases \n(driving up the number of batches and driving down the number of quads per batch). \n \nAdditional Considerations \nI will now describe some small but important facets of font rendering, notably an efficient \nuse of clip-space position and a cheap but effective sorting method. Also, in the sample \ncode for this chapter on the book‘s CD, I have provided source code for a texture atlasing \nsolution that readers may find useful in their font rendering systems. \nSorting \n",
      "content_length": 1117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": " \n \nFonts are typically drawn in a back-to-front fashion, relying on the painter‘s algorithm to \nachieve correct occlusion. Although this is suitable for most applications, certain situations \nmay require that quads be layered in a different sort order than that in which they were \ndrawn. This is easily implemented by using the remaining available value in the vertex \nstructure offset/index vector as a z value for the quad, allowing for up to 256 layers. \nClip-Space Positions \nTo save a few instructions and the constant space for the world-view-projection matrix (the \nclip matrix), it‘s possible to specify the position directly in clip-space to forego having to \ntransform the vertices from perspective to orthographic space, as illustrated in Figure 1.1.5. \nClip-space positions range from –1 to 1 in the X and Y directions. To remap an absolute \nscreen-space coordinate to clip space, we can just use the equation [cx = –1 + x * (2 / \nscreen_width)], [cy = 1 – y * (2 / screen_height)], where x and y are the screen-space \ncoordinates up to a max of screen_width and screen_height, respectively. \nFigure 1.1.5. A quad/billboard being expanded. \n \n \n \nTexture Atlasing \nOn the book‘s CD, I have provided code for a simple virtual texture system that uses atlases \nto reduce batches. This system attempts to load an atlased version of a texture if possible \nand otherwise loads a texture directly from disk. There are some switches (documented in \nthe code) that demonstrate how to turn this system on and off to demonstrate how \nimportant it can be toward reducing the number of batches and maintaining a high level of \nperformance. \n \nFuture Work \n",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": " \n \nThe techniques demonstrated in this chapter were tailored to work on current console \ntechnology, which is limited to Shader Model 3. In the future, I would like to extend these \ntechniques to take advantage of new hardware features, such as Geometry Shaders and \nStreamOut, to further increase performance, image fidelity, and ease of use. \n \nDemo \nOn the accompanying disc, you‘ll find a Direct3D sample application that demonstrates each \nof the discussed techniques in a text- and GUI-rich presentation. Two scenes are presented: \nOne displays a cityscape for a typical 2D tile-based game, and the other displays a Strange \nAttractor simulation. In addition, there is an option to go overboard with the text rendering. \nFeel free to play around with the code until you get a feel for the strengths and weaknesses \nof the different approaches. \nThe main shader file (Font.fx) contains the shaders of interest as well as some additional \nfunctionality (such as font anti-aliasing/filtering). Please note that certain aspects (such as \nquad expansion) were made for optimum efficiency and not necessarily readability. In \ngeneral, most of the code was meant to be very accessible, and it will be helpful to \nperiodically cross-reference the files GuiModel.cpp and Font.fx. \n \nConclusion \nIn this gem, I demonstrated a way to render font and GUI elements easily and efficiently by \ntaking advantage of readily available hardware features, such as instancing, multiple stream \nsupport, and constant array indexing. As a takeaway item, you should be able to easily \nincorporate such a system into your technology base or improve an existing system with \nonly minor changes. \n \nReferences \n[EXT06] ―EXT_draw_instanced.‖ 2006. Open GL. n.d. \n<http://www.opengl.org/registry/specs/EXT/draw_instanced.txt>. \n[GLSL04] ―GLSL Pseudo-Instancing.‖ 17 Nov. 2004. NVIDIA. n.d. \n<http://http.download.nvidia.com/developer/SDK/Individual_Samples/DEMOS/OpenGL/src/\nglsl_pseudo_instancing/docs/glsl_pseudo_instancing.pdf>. \n[Green07] Green, Chris. ―Improved Alpha-Tested Magnification for Vector Textures and \nSpecial Effects.‖ Course on Advanced Real-Time Rendering in 3D Graphics and Games. \nSIGGRAPH 2007. San Diego Convention Center, San Diego, CA. 8 August 2007. \n[Hicks03] Hicks, O‘Dell. ―Screen-aligned Particles with Minimal VertexBuffer Locking.‖ \nShaderX2: Shader Programming Tips and Tricks with DirectX 9.0. Ed. Wolfgang F. Engel. \nPlano, TX: Wordware Publishing, Inc., 2004. 107–112. \n[Loop05] Loop, Charles and Jim Blinn. ―Resolution Independent Curve Rendering Using \nProgrammable Graphics Hardware.‖ 2005. Microsoft. n.d. \n<http://research.microsoft.com/en-us/um/people/cloop/loopblinn05.pdf>. \n",
      "content_length": 2698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": " \n \n[NVIDIA04] ―Improve Batching Using Texture Atlases.‖ 2004. NVIDIA. n.d. \n<http://http.download.nvidia.com/developer/NVTextureSuite/Atlas_Tools/Texture_Atlas_W\nhitepaper.pdf>. \n \n1.2. Principles and Practice of Screen Space Ambient Occlusion \nDominic Filion, Blizzard Entertainment \ndfilion@blizzard.com \nSimulation of direct lighting in modern video games is a well-understood concept, as \nvirtually all of real-time graphics has standardized on the Lambertian and Blinn models for \nsimulating direct lighting. However, indirect lighting (also referred to as global illumination) \nis still an active area of research with a variety of approaches being explored. Moreover, \nalthough some simulation of indirect lighting is possible in real time, full simulation of all its \neffects in real time is very challenging, even on the latest hardware. \nGlobal illumination is based on simulating the effects of light bouncing around a scene \nmultiple times as light is reflected on light surfaces. Computational methods such as \nradiosity attempt to directly model this physical process by modeling the interactions of \nlights and surfaces in an environment, including the bouncing of light off of surfaces. \nAlthough highly realistic, sophisticated global illumination methods are typically too \ncomputationally intensive to perform in real time, especially for games, and thus to achieve \nthe complex shadowing and bounced lighting effects in games, one has to look for \nsimplifications to achieve a comparable result. \nOne possible simplification is to focus on the visual effects of global illumination instead of \nthe physical process and furthermore to aim at a particular subset of effects that global \nillumination achieves. Ambient occlusion is one such subset. Ambient occlusion simplifies \nthe problem space by assuming all indirect light is equally distributed throughout the scene. \nWith this assumption, the amount of indirect light hitting a point on a surface will be directly \nproportional to how much that point is exposed to the scene around it. A point on a plane \nsurface can receive light from a full 180-degree hemisphere around that point and above \nthe plane. In another example, a point in a room‘s corner, as shown in Figure 1.2.1, could \nreceive a smaller amount of light than a point in the middle of the floor, since a greater \namount of its ―upper hemisphere‖ is occluded by the nearby walls. The resulting effect is a \ncrude approximation of global illumination that enhances depth in the scene by shrouding \ncorners, nooks, and crannies in a scene. Artistically, the effect can be controlled by varying \nthe size of the hemisphere within which other objects are considered to occlude neighboring \npoints; large hemisphere ranges will extend the shadow shroud outward from corners and \nrecesses. \nFigure 1.2.1. Ambient occlusion relies on finding how much of the hemisphere \naround the sampling point is blocked by the environment. \n",
      "content_length": 2961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": " \n \n \n \nAlthough the global illumination problem has been vastly simplified through this approach, it \ncan still be prohibitively expensive to compute in real time. Every point on every scene \nsurface needs to cast many rays around it to test whether an occluding object might be \nblocking the light, and an ambient occlusion term is computed based on how many rays \nwere occluded from the total amount of rays emitted from that point. Performing arbitrary \nray intersections with the full scene is also difficult to implement on graphics hardware. We \nneed further simplification. \nScreen Space Ambient Occlusion \nWhat is needed is a way to structure the scene so that we can quickly and easily determine \nwhether a given surface point is occluded by nearby geometry. It turns out that the \nstandard depth buffer, which graphics engines already use to perform hidden surface \nremoval, can be used to approximate local occlusion [Shanmugam07, Mittring07]. By \ndefinition, the depth buffer contains the depth of every visible point in the scene. From \nthese depths, we can reconstruct the 3D positions of the visible surface points. Points that \ncan potentially occlude other points are located close to each other in both screen space and \nworld space, making the search for potential occluders straightforward. We need to align a \nhemisphere around each point‘s upper hemisphere as defined by its normal. We will thus \nneed a normal buffer that will encode the normal of every corresponding point in the depth \nbuffer in screen space. \nRather than doing a full ray intersection, we can simply inspect the depths of neighboring \npoints to establish the likelihood that each is occluding the current point. Any neighbor \nwhose 2D position does not fall within the 2D coverage of the hemisphere could not possibly \nbe an occluder. If it does lie within the hemisphere, then the closer the neighbor point‘s \ndepth is to the target point, the higher the odds it is an occluder. If the neighbor‘s depth is \nbehind the point being tested for occlusion, then no occlusion is assumed to occur. All of \nthese calculations can be performed using the screen space buffer of normals and depths, \nhence the name Screen Space Ambient Occlusion (SSAO). \nAt first glance, this may seem like a gross oversimplification. After all, the depth buffer \ndoesn‘t contain the whole scene, just the visible parts of it, and as such is only a partial \nreconstruction of the scene. For example, a point in the background could be occluded by an \nobject that is hidden behind another object in the foreground, which a depth buffer would \n",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": " \n \ncompletely miss. Thus, there would be pixels in the image that should have some amount of \nocclusion but don‘t due to the incomplete representation we have of the scene‘s geometry. \nFigure 1.2.2. SSAO Samples neighbor points to discover the likelihood of occlusion. \nLighter arrows are behind the center point and are considered occluded samples. \n \n \nIt turns out that these kinds of artifacts are not especially objectionable in practice. The eye \nfocuses first on cues from objects within the scene, and missing cues from objects hidden \nbehind one another are not as disturbing. Furthermore, ambient occlusion is a low-\nfrequency phenomenon; what matters more is the general effect rather than specific \ndetailed cues, and taking shortcuts to achieve a similar yet incorrect effect is a fine tradeoff \nin this case. Discovering where the artifacts lie should be more a process of rationalizing the \nerrors than of simply catching them with the untrained eye. \nFrom this brief overview, we can outline the steps we will take to implement Screen Space \nAmbient Occlusion. \n \nWe will first need to have a depth buffer and a normal buffer at our disposal from \nwhich we can extract information. \n \nFrom these screen space maps, we can derive our algorithm. Each pixel in screen \nspace will generate a corresponding ambient occlusion value for that pixel and store \nthat information in a separate render target. For each pixel in our depth buffer, we \nextract that point‘s position and sample n neighboring pixels within the hemisphere \naligned around the point‘s normal. \n \nThe ratio of occluding versus non-occluding points will be our ambient occlusion term \nresult. \n \nThe ambient occlusion render target can then be blended with the color output from \nthe scene generated afterward. \nI will now describe our Screen Space Ambient Occlusion algorithm in greater detail. \n \nGenerating the Source Data \nThe first step in setting up the SSAO algorithm is to prepare the necessary incoming data. \nDepending on how the final compositing is to be done, this can be accomplished in one of \ntwo ways. \nThe first method requires that the scene be rendered twice. The first pass will render the \ndepth and normal data only. The SSAO algorithm can then generate the ambient occlusion \noutput in an intermediate step, and the scene can be rendered again in full color. With this \n",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": " \n \napproach, the ambient occlusion map (in screen space) can be sampled by direct lights from \nthe scene to have their contribution modulated by the ambient occlusion term as well, which \ncan help make the contributions from direct and indirect lighting more coherent with each \nother. This approach is the most flexible but is somewhat less efficient because the \ngeometry has to be passed to the hardware twice, doubling the API batch count and, of \ncourse, the geometry processing load. \nA different approach is to render the scene only once, using multiple render targets bound \nas output to generate the depth and normal information as the scene is first rendered \nwithout an ambient lighting term. SSAO data is then generated as a post-step, and the \nambient lighting term can simply be added. This is a faster approach, but in practice artists \nlose the flexibility to decide which individual lights in the scene may or may not be affected \nby the ambient occlusion term, should they want to do so. Using a fully deferred renderer \nand pushing the entire scene lighting stage to a post-processing step can get around this \nlimitation to allow the entire lighting setup to be configurable to use ambient occlusion per \nlight. \nWhether to use the single-pass or dual-pass method will depend on the constraints that are \nmost important to a given graphics engine. In all cases, a suitable format must be chosen to \nstore the depth and normal information. When supported, a 16-bit floating-point format will \nbe the easiest to work with, storing the normal components in the red, green, and blue \ncomponents and storing depth as the alpha component. \nScreen Space Ambient Occlusion is very bandwidth intensive, and minimizing sampling \nbandwidth is necessary to achieve optimal performance. Moreover, if using the single-pass \nmulti-render target approach, all bound render targets typically need to be of the same bit \ndepth on the graphics hardware. If the main color output is 32-bit RGBA, then outputting to \na 16-bit floating-point buffer at the same time won‘t be possible. To minimize bandwidth \nand storage, the depth and normal can be encoded in as little as a single 32-bit RGBA color, \nstoring the x and y components of the normal in the 8-bit red and green channels while \nstoring a 16-bit depth value in the blue and alpha channels. The HLSL shader code for \nencoding and decoding the normal and depth values is shown in Listing 1.2.1. \nListing 1.2.1. HLSL code to decode the normal on subsequent passes as well as \nHLSL code used to encode and decode the 16-bit depth value \n// Normal encoding simply outputs x and y components in R and G \nin \n// the range 0.. 1 \nfloat3 DecodeNormal( float2 cInput ) { \n     float3 vNormal.xy = 2.0f * cInput.rg - 1.0f; \n     vNormal.z = sqrt(max(0, 1 - dot(vNormal.xy, vNormal.xy))); \n     return vNormal; \n} \n \n// Encode depth to B and A \nfloat2 DepthEncode( float fDepth ) { \n    float2 vResult; \n    // Input depth must be mapped to 0..1 range \n    fDepth = fDepth / p_fScalingFactor; \n \n    // R = Basis = 8 bits = 256 possible values \n    // G = fractional part with each 1/256th slice \n    vResult.ba = frac( float2( fDepth, fDepth * 256.0f )); \n    return vResult; \n",
      "content_length": 3226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": " \n \n} \n \nfloat3 DecodeDepth( float4 cInput ) { \n    return dot ( cInput.ba, float2( 1.0f, 1.0f / 256.0f ) * \n                 p_fScalingFactor; \n} \n \n \nSampling Process \nWith the input data in hand, we can begin the ambient occlusion generation process itself. \nAt any visible point on a surface on the screen, we need to explore neighboring points to \ndetermine whether they could occlude our current point. Multiple samples are thus taken \nfrom neighboring points in the scene using a filtering process described by the HLSL shader \ncode in Listing 1.2.2. \nListing 1.2.2. Screen Space Ambient Occlusion filter described in HLSL code \n// i_VPOS is screen pixel coordinate as given by HLSL VPOS \ninterpolant. \n// p_vSSAOSamplePoints is a distribution of sample offsets for \neach sample. \nfloat4 PostProcessSSAO( float 3 i_VPOS ) \n{ \n   float2 vScreenUV;       //←This will become useful later. \n   float3 vViewPos = 2DToViewPos( i_VPOS, vScreenUV); \n \n   half fAccumBlock = 0.Of; \n   for (inti = 0; i < iSampleCount; i++ ) \n   { \n       float3 vSamplePointDelta = p_vSSAOSamplePoints[i]; \n       float fBlock = TestOcclusion( \n                          vViewPos, \n                             vSamplePointDelta, \n                          p_fOcclusionRadius, \n                          p_fFullOcclusionThreshold, \n                          p_fNoOcclusionThreshold, \n                          p_fOcclusionPower ) ) \n       fAccumBlock += fBlock; \n   } \n   fAccumBlock / = iSampleCount; \n \n   return 1.Of - fAccumBlock; \n} \n \nWe start with the current point, p, whose occlusion we are computing. We have the point‘s \n2D coordinate in screen space. Sampling the depth buffer at the corresponding UV \ncoordinates, we can retrieve that point‘s depth. From these three pieces of information, the \n",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": " \n \n3D position of the point within can be reconstructed using the shader code shown in Listing \n1.2.3. \nListing 1.2.3. HLSL shader code used to map a pixel from screen space to view \nspace \n// vRecipDepthBufferSize = 1.0 / depth buffer width and height \nin pixels. \n// p_vCameraFrustrumSize = Full width and height of camera \nfrustum at the \n// camera's near plane in world space. \nfloat2 p_vRecipDepthBufferSize; \nfloat2 p_vCameraFrustrumSize; \nfloat3 2DPosToViewPos( float3 i_VPOS, out float2 vScreenUV ) \n{ \n    float2 vViewSpaceUV = i_VPOS * p_vRecipDepthBufferSize; \n    vScreenUV    = vViewSpaceUV; \n \n    // From 0..1 to to 0..2 \n    vViewSpaceUV = vViewSpaceUV * float2( 2.Of, -2.Of ); \n    // From 0..2 to-1..1 \n    vViewSpaceUV = vViewSpaceUV + float2( -1.0f , 1.Of ); \n    vViewSpaceUV = vViewSpaceUV * p_vCameraFrustrumSize * 0.5f; \n \n    return float3( vViewSpaceUV.x, vViewSpaceUV.y, 1.0f ) * \n                   tex2D( p_sDepthBuffer, vScreenUV ).r; \n} \n \nWe will need to sample the surrounding area of the point p along multiple offsets from its \nposition, giving us n neighbor positions qi. Sampling the normal buffer will give us the \nnormal around which we can align our set of offset vectors, ensuring that all sample offsets \nfall within point p‘s upper hemisphere. Transforming each offset vector by a matrix can be \nexpensive, and one alternative is to perform a dot product between the offset vector and \nthe normal vector at that point and to flip the offset vector if the dot product is negative, as \nshown in Figure 1.2.3. This is a cheaper way to solve for the offset vectors without doing a \nfull matrix transform, but it has the drawback of using fewer samples when samples are \nrejected due to falling behind the plane of the surface of the point p. \nFigure 1.2.3. Samples behind the hemisphere are flipped over to stay within the \nhemisphere. \n",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": " \n \n \n \nEach neighbor‘s 3D position can then be transformed back to screen space in 2D, and the \ndepth of the neighbor point can be sampled from the depth buffer. From this neighboring \ndepth value, we can establish whether an object likely occupies that space at the neighbor \npoint. Listing 1.2.4 shows shader code to test for this occlusion. \nListing 1.2.4. HLSL code used to test occlusion by a neighboring pixel \nfloat TestOcclusion( float3 vViewPos, \n                     float3 vSamplePointDelta, \n               float fOcclusionRadius, \n                     float fFullOcclusionThreshold, \n                     float fNoOcclusionThreshold, \n   float fOcclusionPower ) \n{ \n    float3 vSamplePoint = vViewPos + fOcclusionRadius * \nvSamplePointDelta; \n    float2 vSamplePointUV; \n    vSamplePointUV = vSamplePoint.xy / vSamplePoint.z; \n    vSamplePointUV = vSamplePointUV / p_vCameraSize / 0.5f; \n    vSamplePointUV = vSamplePointUV + float2( 1.0f, -1.0f ); \n    vSamplePointUV = vSamplePointUV * float2( 0.5f, -0.5f ); \n \n    float fSampleDepth = tex2D( p_sDepthBuffer, vSamplePointUV \n).r; \n    float fDistance = vSamplePoint.z - fSampleDepth; \n    return OcclusionFunction( fDistance, \nfFullOcclusionThreshold, \n                              fNoOcclusionThreshold, \nfOcclusionPower ); \n} \n \n",
      "content_length": 1299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": " \n \nWe now have the 3D positions of both our point p and the neighboring points qi. We also \nhave the depth di of the frontmost object along the ray that connects the eye to each \nneighboring point. How do we determine ambient occlusion? \nThe depth di gives us some hints as to whether a solid object occupies the space at each of \nthe sampled neighboring points. Clearly, if the depth di is behind the sampled point‘s depth, \nit cannot occupy the space at the sampled point. The depth buffer does not give us the \nthickness of the object along the ray from the viewer; thus, if the depth of the object is \nanywhere in front of p, it may occupy the space, though without thickness information, we \ncan‘t know for sure. We can devise some reasonable heuristics with the information we do \nhave and use a probabilistic method. \nThe further in front of the sample point the depth is, the less likely it is to occupy that \nspace. Also, the greater the distance between the point p and the neighbor point, the lesser \nthe occlusion, as the object covers a smaller part of the hemisphere. Thus, we can derive \nsome occlusion heuristics based on: \n \nThe difference between the sampled depth di and the depth of the point qi \n \nThe distance between p and qi \nFor the first relationship, we can formulate an occlusion function to map the depth deltas to \nocclusion values. \nIf the aim is to be physically correct, then the occlusion function should be quadratic. In our \ncase we are more concerned about being able to let our artists adjust the occlusion \nfunction, and thus the occlusion function can be arbitrary. Really, the occlusion function can \nbe any function that adheres to the following criteria: \n \nNegative depth deltas should give zero occlusion. (The occluding surface is behind \nthe sample point.) \n \nSmaller depth deltas should give higher occlusion values. \n \nThe occlusion value needs to fall to zero again beyond a certain depth delta value, as \nthe object is too far away to occlude. \nFor our implementation, we simply chose a linearly stepped function that is entirely \ncontrolled by the artist. A graph of our occlusion function is shown in Figure 1.2.4. There is \na full-occlusion threshold where every positive depth delta smaller than this value gets \ncomplete occlusion of one, and a no-occlusion threshold beyond which no occlusion occurs. \nDepth deltas between these two extremes fall off linearly from one to zero, and the value is \nexponentially raised to a specified occlusion power value. If a more complex occlusion \nfunction is required, it can be pre-computed in a small ID texture to be looked up on \ndemand. \nFigure 1.2.4. SSAO blocker function. \n",
      "content_length": 2676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": " \n \n \n \nListing 1.2.5. HLSL code used to implement occlusion function \nfloat OcclusionFunction( float fDistance, \n                         float fNoOcclusionThreshold, \nfloat fFullOcclusionThreshold, \nfloat fOcclusionPower ) \n{ \n    const c_occlusionEpsilon = 0.01f; \n \n    if ( fDistance > c_ occlusionEpsilon ) \n    { \n        // Past this distance there is no occlusion. \n        float fNoOcclusionRange = fNoOcclusionThreshold - \n                                  fFullOcclusionThreshold; \n        if ( fDistance < fFullOcclusionThreshold ) \n            return 1.0f; \n        else return max(1.0f – pow(( ( fDistance – \n             fFullOcclusionThreshold ) / fNoOcclusionRange, \n             fOcclusionPower ) ) ,0.0f ); \n    } else return 0.0f; \n} \n \nOnce we have gathered an occlusion value for each sample point, we can take the average \nof these, weighted by the distance of each sample point to p, and the average will be our \nambient occlusion value for that pixel. \n \nSampling Randomization \n",
      "content_length": 1005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": " \n \nSampling neighboring pixels at regular vector offsets will produce glaring artifacts to the \neye, as shown in Figure 1.2.5. \nFigure 1.2.5. SSAO without random sampling. \n \n \nTo smooth out the results of the SSAO lookups, the offset vectors can be randomized. A \ngood approach is to generate a 2D texture of random normal vectors and perform a lookup \non this texture in screen space, thus fetching a unique random vector per pixel on the \nscreen, as illustrated in Figure 1.2.6 [Mittring07]. We have n neighbors we must sample, \nand thus we will need to generate a set of n unique vectors per pixel on the screen. These \nwill be generated by passing a set of offset vectors in the pixel shader constant registers \nand reflecting these vectors through the sampled random vector, resulting in a semi-\nrandom set of vectors at each pixel, as illustrated by Listing 1.2.6. The set of vectors passed \nin as registers is not normalized—having varying lengths helps to smooth out the noise \npattern and produces a more even distribution of the samples inside the occlusion \nhemisphere. The offset vectors must not be too short to avoid clustering samples too close \nto the source point p. In general, varying the offset vectors from half to full length of the \nocclusion hemisphere radius produces good results. The size of the occlusion hemisphere \nbecomes a parameter controllable by the artist that determines the size of the sampling \narea. \nFigure 1.2.6. Randomized sampling process. \n",
      "content_length": 1487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": " \n \n \nListing 1.2.6. HLSL code used to generate a set of semi-random 3D vectors at each \npixel \nfloat3 reflect( float 3 vSample, float3 vNormal ) \n{ \n      return normalize ( vSample – 2.0f * dot( vSample, vNormal \n) * vNormal ); \n} \n \nfloat3x3 MakeRotation( float fAngle, float3 vAxis ) \n{ \n    float fS; \n    float fC; \n    sincos( fAngle, fS, fC ); \n    float fXX       = vAxis.x * vAxis.x; \n    float fYY       = vAxis.y * vAxis.y; \n    float fZZ       = vAxis.z * vAxis.z; \n    float fXY       = vAxis.x * vAxis.y; \n    float fYZ       = vAxis.y * vAxis.z; \n    float fZX       = vAxis.z * vAxis.x; \n    float fXS       = vAxis.x * fS; \n    float fYS       = vAxis.y * fS; \n    float fZS       = vAxis.z * fS; \n    float fOneC      = 1.0f - fC; \n \n    float3x3 result = float3x3( \n        fOneC * fXX +  fC,  fOneC * fXY + fZS, fOneC * fZX - \nfYS, \n        fOneC * fXY - fZS,  fOneC * fYY +  fC, fOneC * fYZ + \nfXS, \n        fOneC * fZX + fYS,  fOneC * fYZ - fXS, fOneC * fZZ +  fC \n    ); \n    return result; \n} \n \n",
      "content_length": 1021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": " \n \nfloat4 PostProcessSSAO( float3 i_VPOS ) \n{ \n \n      ... \n \n      const float c_scalingConstant = 256.0f; \n \n \n \n \n \n \nfloat3 vRandomNormal = ( \nnormalize( tex2D( p_sSSAONoise, vScreenUV * \n \n \n \n \n \n \np_vSrcImageSize / \nc_scalingConstant ).xyz * 2.0f \n \n \n \n \n \n \n– 1.0f ) ); \n \n \n \n \n \n \nfloat3x3 rotMatrix = \nMakeRotation( 1.0f,vNormal ); \n \n      half fAccumBlock = 0.0f; \n      for ( int i = 0; i < iSampleCount; i++ ) { \n          float3 vSamplePointDelta = reflect( \np_vSSAOSamplePoints[i], \n \n \n \n \n \n \nvRandomNormal ); \n          float fBlock = TestOcclusion( \n                             vViewPos, \n      vSamplePointDelta, \n                             p_fOcclusionRadius, \n                             p_fFullOcclusionThreshold, \n                             p_fNoOcclusionThreshold, \n                             p_fOcclusionPower ) ) { \n       fAccumBlock += fBlock; \n      } \n \n     ... \n \n} \n \n \nAmbient Occlusion Post-Processing \nAs shown in Figure 1.2.7, the previous step helps to break up the noise pattern, producing a \nfiner-grained pattern that is less objectionable. With wider sampling areas, however, a \nfurther blurring of the ambient occlusion result becomes necessary. The ambient occlusion \nresults are low frequency, and losing some of the high-frequency detail due to blurring is \ngenerally preferable to the noisy result obtained by the previous steps. \nFigure 1.2.7. SSAO term after random sampling applied. Applying blur passes will \nfurther reduce the noise to achieve the final look. \n",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": " \n \n \n \nTo smooth out the noise, a separable Gaussian blur can be applied to the ambient occlusion \nbuffer. However, the ambient occlusion must not bleed through edges to objects that are \nphysically separate within the scene. A form of bilateral filtering is used. This filter samples \nthe nearby pixels as a regular Gaussian blur shader would, yet the normal and depth for \neach of the Gaussian samples are sampled as well. (Encoding the normal and depth in the \nsame render targets presents significant advantages here.) If the depth from the Gaussian \nsample differs from the center tap by more than a certain threshold, or the dot product of \nthe Gaussian sample and the center tap normal is less than a certain threshold value, then \nthe Gaussian weight is reduced to zero. The sum of the Gaussian samples is then \nrenormalized to account for the missing samples. \nListing 1.2.7. HLSL code used to blur the ambient occlusion image \n// i_UV : UV of center tap \n// p_fBlurWeights Array of gaussian weights \n// i_GaussianBlurSample: Array of interpolants, with each \ninterpolants \n// packing 2 gaussian sample positions. \nfloat4 PostProcessGaussianBlur( VertexTransport vertOut ) \n{ \n     float2 vCenterTap    = i_UV.xy; \n     float4 cValue        = tex2D( p_sSrcMap, vCenterTap.xy ); \n     float4 cResult       = cValue * p_fBlurWeights[0]; \n     float fTotalWeight   = p_fBlurWeights[0]; \n \n     // Sample normal & depth for center tap \n     float4 vNormalDepth = tex2D( p_sNormalDepthMap, \nvCenterTap.xy ).a; \n     for ( int i = 0; i < b_iSampleInterpolantCount; i++ ) \n     { \n         half4 cValue = tex2D( p_sSrcMap, \n                               i_GaussianBlurSample[i].xy ); \n         half fWeight = p_fBlurWeights[i * 2 + 1]; \n \n",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": " \n \n         float4 vSampleNormalDepth = tex2D( p_sNormalDepthMap, \n                                     i_GaussianBlurSample[i].xy \n); \n         if ( dot( vSampleNormalDepth.rgb, vNormalDepth.rgb) < \n0.9f || \n              abs( vSampleNormalDepth.a – vNormalDepth.a ) > \n0.01f ) \n             fWeight = 0.0f; \n \n         cResult += cValue * fWeight; \n         fTotalWeight += fWeight; \n \n         cValue = tex2D( p_sSeparateBlurMap, \n                         INTERPOLANT_GaussianBlurSample[i].zw ) \n; \n         fWeight = p_fBlurWeights[i * 2 + 2]; \n         vSampleNormalDepth = tex2D( p_sSrcMap, \n                              \nINTERPOLANT_GaussianBlurSample[i].zw ) ; \n         if ( dot( vSampleNormalDepth.rgb, vNormalDepth .rgb < \n0.9f ) || \n                   abs( vSampleNormalDepth.a – vNormalDepth.a ) \n> 0.01f ) \n              fWeight = 0.0f; \n         cResult += cValue * fWeight; \n         fTotalWeight += fWeight; \n     } \n \n     // Rescale result according to number of discarded samples. \n     cResult *= 1.0f / fTotalWeight; \n \n     return cResult; \n} \n \nSeveral blur passes can thus be applied to the ambient occlusion output to completely \neliminate the noisy pattern, trading off some higher-frequency detail in exchange. \nFigure 1.2.8. Result of Gaussian blur. \n",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": " \n \n \n \n \n \nHandling Edge Cases \nThe offset vectors are in view space, not screen space, and thus the length of the offset \nvectors will vary depending on how far away they are from the viewer. This can result in \nusing an insufficient number of samples at close-up pixels, resulting in a noisier result for \nthese pixels. Of course, samples can also go outside the 2D bounds of the screen. Naturally, \ndepth information outside of the screen is not available. In our implementation, we ensure \nthat samples outside the screen return a large depth value, ensuring they would never \nocclude any neighboring pixels. This can be achieved through the ―border color‖ texture \nwrapping state, setting the border color to a suitably high depth value. \nTo prevent unacceptable breakdown of the SSAO quality in extreme close-ups, the number \nof samples can be increased dynamically in the shader based on the distance of the point p \nto the viewer. This can improve the quality of the visual results but can result in erratic \nperformance. Alternatively, the 2D offset vector lengths can be artificially capped to some \nthreshold value regardless of distance from viewer. In effect, if the camera is very close to \nan object and the SSAO samples end up being too wide, the SSAO area consistency \nconstraint is violated so that the noise pattern doesn‘t become too noticeable. \n \nOptimizing Performance \nScreen Space Ambient Occlusion can have a significant payoff in terms of mood and visual \nquality of the image, but it can be quite an expensive effect. The main bottleneck of the \nalgorithm is the sampling itself. The semi-random nature of the sampling, which is \nnecessary to minimize banding, wreaks havoc with the GPU‘s texture cache system and can \nbecome a problem if not managed. The performance of the texture cache will also be very \ndependent on the sampling area size, with wider areas straining the cache more and \nyielding poorer performance. Our artists quickly got in the habit of using SSAO to achieve a \n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": " \n \nfaked global illumination look that suited their purposes. This required more samples and \nwider sampling areas, so extensive optimization became necessary for us. \nOne method to bring SSAO to an acceptable performance level relies on the fact that \nambient occlusion is a low-frequency phenomenon. Thus, there is generally no need for the \ndepth buffer sampled by the SSAO algorithm to be at full-screen resolution. The initial depth \nbuffer can be generated at screen resolution, since the depth information is generally \nreused for other effects, and it potentially has to fit the size of other render targets, but it \ncan thereafter be downsampled to a smaller depth buffer that is a quarter size of the \noriginal on each side. The downsampling itself does have some cost, but the payback in \nimproved throughput is very significant. Downsampling the depth buffer also makes it \npossible to convert it from a wide 16-bit floating-point format to a more bandwidth-friendly \n32-bit packed format. \n \nFake Global Illumination and Artistic Styling \nIf the ambient occlusion hemisphere is large enough, the SSAO algorithm eventually starts \nto mimic behavior seen from general global illumination; a character relatively far away \nfrom a wall could cause the wall to catch some of the subtle shadowing cues a global \nillumination algorithm would detect. If the sampling area of the SSAO is wide enough, the \nlook of the scene changes from darkness in nooks and crannies to a softer, ambient feel. \nThis can pull the art direction in two somewhat conflicting directions: on the one hand, the \nneed for tighter, high-contrast occluded zones in deeper recesses, and on the other hand, \nthe desire for the larger, softer, ambient look of the wide-area sampling. \nOne approach is to split the SSAO samples between two different sets of SSAO parameters: \nSome samples are concentrated in a small area with a rapidly increasing occlusion function \n(generally a quarter of all samples), while the remaining samples use a wide sampling area \nwith a gentler function slope. The two sets are then averaged independently, and the final \nresult uses the value from the set that produces the most (darkest) occlusion. This is the \napproach that was used in StarCraft II. \nFigure 1.2.9. SSAO with different sampling-area radii. \n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": " \n \n \n \nThe edge-enhancing component of the ambient occlusion does not require as many samples \nas the global illumination one, thus a quarter of the samples can be assigned to crease \nenhancement while the remainder are assigned for the larger area threshold. \nThough SSAO provides for important lighting cues to enhance the depth of the scene, there \nwas still a demand from our artist for more accurate control that was only feasible through \nthe use of some painted-in ambient occlusion. The creases from SSAO in particular cannot \nreach the accuracy that using a simple texture can without using an enormous amount of \nsamples. Thus the usage of SSAO does not preclude the need for some static ambient \nocclusion maps to be blended in with the final ambient occlusion result, which we have done \nhere. \nFigure 1.2.10. Combined small- and large-area SSAO result. \n \n",
      "content_length": 870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": " \n \n \nFor our project, complaints about image noise, balanced with concerns about performance, \nwere the main issues to deal with for the technique to gain acceptance among our artists. \nIncreasing SSAO samples helps improve the noise, yet it takes an ever-increasing number \nof samples to get ever smaller gains in image quality. Past 16 samples, we‘ve found it‘s \nmore effective to use additional blur passes to smooth away the noise pattern, at the \nexpense of some loss of definition around depth discontinuities in the image. \n \nTransparency \nIt should be noted the depth buffer can only contain one depth value per pixel, and thus \ntransparencies cannot be fully supported. This is generally a problem with all algorithms \nthat rely on screen space depth information. There is no easy solution to this, and the SSAO \nprocess itself is intensive enough that dealing with edge cases can push the algorithm \noutside of the real-time realm. In practice, for the vast majority of scenes, correct ambient \nocclusion for transparencies is a luxury that can be skimped on. Very transparent objects \nwill typically be barely visible either way. For transparent objects that are nearly opaque, \nthe choice can be given to the artist to allow some transparencies to write to the depth \nbuffer input to the SSAO algorithm (not the z-buffer used for hidden surface removal), \noverriding opaque objects behind them. \n \nFinal Results \nColor Plate 1 shows some results portraying what the algorithm contributes in its final form. \nThe top-left pane shows lighting without the ambient occlusion, while the top-right pane \nshows lighting with the SSAO component mixed in. The final colored result is shown in the \nbottom pane. Here the SSAO samples are very wide, bathing the background area with an \neffect that would otherwise only be obtained with a full global illumination algorithm. The \nSSAO term adds depth to the scene and helps anchor the characters within the \nenvironment. \nColor Plate 2 shows the contrast between the large-area, low-contrast SSAO sampling \ncomponent on the bar surface and background and the tighter, higher-contrast SSAO \nsamples apparent within the helmet, nooks, and crannies found on the character‘s \nspacesuit. \n \nConclusion \nThis gem has described the Screen Space Ambient Occlusion technique used at Blizzard and \npresented various problems and solutions that arise. Screen Space Ambient Occlusion offers \na different perspective in achieving results that closely resemble what the eye expects from \nambient occlusion. The technique is reasonably simple to implement and amenable to \nartistic tweaks in real time to make it ideal to fit an artistic vision. \n \nReferences \n",
      "content_length": 2698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": " \n \n[Bavoil] Bavoil, Louis and Miguel Sainz. ―Image-Space Horizon-Based Ambient Occlusion.‖ \nShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River \nMedia, 2009. Section 6.2. \n[Bavoil09] Bavoil, Louis and Miguel Sainz. ―Multi-Layer Dual-Resolution Screen-Space \nAmbient Occlusion.‖ 2009. NVIDIA. n.d. \n<http://developer.download.nvidia.com/presentations/2009/SIGGRAPH/Bavoil_MultiLayerD\nualResolutionSSAO.pdf>. \n[Bavoil08] Bavoil, Louis and Miguel Sainz. ―Screen Space Ambient Occlusion.‖ Sept. 2008. \nNVIDIA. n.d. \n<http://developer.download.nvidia.com/SDK/10.5/direct3d/Source/ScreenSpaceAO/doc/Scr\neenSpaceAO.pdf>. \n[Fox08] Fox, Megan. ―Ambient Occlusive Crease Shading.‖ Game Developer. March 2008. \n[Kajalin] Kajalin, Vladimir. ―Screen Space Ambient Occlusion.‖ ShaderX7: Advanced \nRendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. Section \n6.1. \n[Lajzer] Lajzer, Brett and Dan Nottingham. ―Combining Screen-Space Ambient Occlusion \nand Cartoon Rendering on Graphics Hardware.‖ n.d. Brett Lajzer. \nn.d.<http://brettlajzer.com/pub/graphics/final/nprssao_final_presentation.pdf>. \n[Luft06] Luft, Thomas, Carsten Colditz, and Oliver Deussen. ―Image Enhancement by \nUnsharp Masking the Depth Buffer.‖ Course on Non-Photorealistic Rendering. SIGGRAPH \n2006. Boston Convention and Exhibition Center, Boston, MA. 3 August 2006. \n[Mittring07] Mittring, Martin. ―Finding Next Gen—CryEngine 2.0.‖ Course on Advanced Real-\nTime Rendering in 3D Graphics and Games. SIGGRAPH 2007. San Diego Convention Center, \nSan Diego, CA. 8 August 2007. \n[Pesce] Pesce, Angelo. ―Variance Methods for Screen-Space Ambient Occlusion.‖ ShaderX7: \nAdvanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. \nSection 6.7. \n[Ritschel09] Ritschel, Tobias, Thorsten Grosch, and Hans-Peter Seidel. ―Approximating \nDynamic Global Illumination in Image Space.‖ 2009. Max Planck Institut Informatik. n.d. \n<http://www.mpi-inf.mpg.de/~ritschel/Papers/SSDO.pdf>. \n[Sains08] Sains, Miguel. ―Real-Time Depth Buffer Based Ambient Occlusion.‖ Game \nDevelopers Conference. Moscone Center, San Francisco, CA. 18–22 February 2008. \n[Shamugan07] Shanmugam, Perumaal and Okan Arikan. ―Hardware Accelerated Ambient \nOcclusion Techniques on GPUs.‖ 2007. Google Sites. \nn.d.<http://perumaal.googlepages.com/ao.pdf>. \n[Sloan07] Sloan, Peter-Pike, Naga K. Govindaraju, Derek Nowrouzezahrai, and John Snyder. \n―Image-Based Proxy Accumulation for Real-Time Soft Global Illumination.‖ Pacific Graphics \nConference. The Royal Lahaina Resort, Maui, Hawaii. 29 October 2007. \n[Tomasi98] Tomasi, Carlo and Roberto Manduchi. ―Bilateral Filtering for Gray and Color \nImages.‖ IEEE International Conference on Computer Vision. Homi Bhabha Auditorium, \nBombay, India. 7 January 1998. \n \n",
      "content_length": 2818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": " \n \n1.3. Multi-Resolution Deferred Shading \nHyunwoo Ki, INNOACE Co., Ltd \npsykinu@gmail.com \nRecently, deferred shading has become a popular rendering technique for real-time games. \nDeferred shading enables game engines to handle many local lights without repeated \ngeometry processing because it replaces geometry processing with pixel processing \n[Saito90, Shishkovtsov05, Valient07, Koonce07, Engel09, Kircher09]. In other words, \nshading costs are independent of geometric complexity, which is important as the CPU cost \nof scene-graph traversal and the GPU cost of geometry processing grows with scene \ncomplexity. Despite this decoupling of shading cost from geometric complexity, we still seek \nto optimize the pixel processing necessary to handle many local lights, soft shadows, and \nother per-pixel effects. In this gem, we present a technique that we call multi-resolution \ndeferred shading, which provides adaptive sub-sampling using a hierarchical approach to \nshading by exploiting spatial coherence of the scene. Multi-resolution deferred shading \nefficiently reduces pixel shading costs as compared to traditional deferred shading without \nnoticeable aliasing. As shown in Figure 1.3.1, our technique allows us to achieve a \nsignificant improvement in performance with negligible visual degradation relative to a more \nexpensive full-resolution deferred shading approach. \nFigure 1.3.1. Deferred shading (left: 20 fps), multi-resolution deferred shading \n(center: 38 fps), and their difference image (right). There are 40 spot lights, \nincluding fuzzy shadows (1024×1024 pixels with 24 shadow samples per pixel). \n \n \n \nDeferred Shading \nUnlike traditional forward rendering approaches, deferred shading costs are independent of \nscene complexity. This is because deferred shading techniques store geometry information \nin textures, often called G-buffers, replacing geometry processing with pixel processing \n[Saito90, Shishkovtsov05, Valient07, Koonce07]. \nDeferred shading techniques start by rendering the scene into a G-buffer, which is typically \nimplemented using multiple render targets to store geometry information, such as positions, \nnormals, and other quantities instead of final shading results. Next, deferred shading \nsystems render a screen-aligned quad to invoke a pixel shader at all pixels in the output \nimage. The pixel shader retrieves the geometry information from the G-buffer and performs \nshading operations as a post process. Naturally, one must carefully choose the data formats \nand precise quantities to store in a G-buffer in order to make the best possible use of both \nmemory and memory bandwidth. For example, the game Killzone 2 utilizes four buffers \ncontaining lighting accumulation and intensity, normal XY in 16-bit floating-point format, \nmotion vector XY, specular and diffuse albedo, and sun occlusion [Valient07]. The Z \ncomponent of the normal is computed from normal XY, and position is computed from depth \n",
      "content_length": 2971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": " \n \nand pixel coordinates. These types of encodings are a tradeoff between decode/encode cost \nand the memory and memory bandwidth consumed by the G-buffer. As shown in Color Plate \n3, we simply use two four-channel buffers of 16-bit floating-point precision per channel \nwithout any advanced encoding schemes for ease of description and implementation. The \nfirst of our buffers contains view-space position in the RGB channels and a material ID in \nthe alpha channel. The other buffer contains view-space normal in the RGB channels and \ndepth in the alpha channel. \nWe could also use material buffers that store diffuse reflectance, specular reflectance, \nshininess, and so on. However, material buffers are not necessary if we separate lighting \nand material phases from the shading phase using light pre-pass rendering [Engel09]. \nUnlike traditional deferred shading, light pre-pass rendering first computes lighting results \ninstead of full shading. This method can then incorporate material properties in an \nadditional material phase with forward rendering. Although this technique requires a second \ngeometry rendering pass, such separation of lighting and material phases gives added \nflexibility during material shading and is compatible with hardware multi-sample anti-\naliasing. A related technique, inferred lighting, stores lighting results in a single low-\nresolution buffer instead of the full-resolution buffer [Kircher09]. To avoid discontinuity \nproblems, this technique filters edges using depth and object ID comparison in the material \nphase. As we will describe in the next section, our technique is similar to inferred lighting, \nbut our method finds discontinuous areas based on spatial proximity and then solves the \ndiscontinuity problems using a multi-resolution approach during the lighting (or shading) \nphase. \n \nMulti-Resolution Deferred Shading \nAlthough deferred shading improves lighting efficiency, computing illumination for every \npixel is still expensive, despite the fact that it is often fairly low frequency. We have \ndeveloped a multi-resolution deferred shading approach to exploit the low-frequency nature \nof illumination. We perform lighting in a lower-resolution buffer for spatially coherent areas \nand then interpolate results into a higher-resolution buffer. This key concept is based upon \nour prior work [Ki07a]. Here, we generalize this work and improve upon it to reduce \naliasing. \nThe algorithm has three steps, as shown in Color Plate 4: geometry pass, multi-resolution \nrendering pass, and composite pass. The geometry pass populates the G-buffers. Our \ntechnique is compatible with any sort of G-buffer organization, but for ease of explanation, \nwe will stick with the 8-channel G-buffer layout described previously. \nThe next step is multi-resolution rendering, which consists of resolution selection (non-edge \ndetection), shading (lighting), and interpolation (up-sampling). We allocate buffers to store \nrendering results at various resolutions. We call these buffers R-buffers, where the ―R‖ \nstands for ―Result‖ or ―Resolution.‖ In this chapter, we will use three R-buffers: full \nresolution, quarter resolution, and 1/16th resolution (for example, 1280×1024, 640×512, \nand 320×256). If the full-resolution image is especially high, we could choose to decrease \nthe resolutions of the R-buffers even more drastically than just one-quarter resolution in \neach step. \nMulti-resolution rendering uses rendering iterations from lower-resolution to higher-\nresolution R-buffers. We prevent repeated pixel processing by exploiting early-Z culling to \nskip pixels processed in earlier iterations using lower-resolution R-buffers [Mitchell04]. To \nstart shading our R-buffers, we set the lowest-resolution R-buffer as the current render \ntarget and clear its depth buffer with one depth (farthest). Next, we determine pixels being \nrendered in this resolution by rendering a screen-aligned quad with Zi = 1.0 – i * 0.1, \nwhere i is the current iteration, writing only depth. During this pass, the pixel shader reads \ngeometry information from mip-mapped versions of our G-buffers and estimates spatial \n",
      "content_length": 4162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": " \n \nproximity for non-edge detection. To estimate spatial proximity, we first compare the \ncurrent pixel‘s material ID with the material IDs of neighboring pixels. Then, we compare \nthe difference of normal and depth values using tunable thresholds. If spatial proximity is \nlow for the current pixel, we should use a higher-resolution R-buffer for better quality, and \nthus we discard the current pixel in the shader to skip writing Z. After this pass, pixels \nwhose spatial proximity is high (in other words, non-edge) in the current resolution contain \nmeaningful Z values because they were not discarded. The pixels whose spatial proximity is \nlow (in other words, edges) still have farthest Z values left over from the initial clear. \nWe then perform shading (or lighting) by rendering a screen-aligned quad with Zi = 1.0 – i \n* 0.1 again, but the Z function is changed to Equal. This means that only spatially coherent \npixels in this resolution will pass the Z-test, as illustrated in Color Plate 4. In the pixel \nshader, we read geometric data from G-buffers and compute illumination as in light pre-\npass rendering. On a textured surface, such as wall and floor, although spatial proximity \nbetween neighboring pixels is high, these pixel colors are often different. Such cases can \ncause serious aliasing in the resulting images. To solve this problem, we store only lighting \nresults instead of full shading results into R-buffers, and we handle material properties with \nstored illumination in R-buffers in the composite pass. \nAfter shading, we copy the current shading/lighting results and depth to the next higher-\nresolution R-buffer, allowing the hardware‘s bilinear units to do a simple interpolation as we \nup-sample. We have found that bilinear filtering is adequate, though we could use bi-cubic \nfiltering or other higher-order filtering for better quality. \nWe repeat the process described above at the next higher resolution, estimating spatial \nproximity and writing Z and computing illumination until we reach the full-resolution R-\nbuffer. A full-screen quad is drawn three times per iteration. If a given pixel was shaded on \na prior iteration in a lower-resolution R-buffer, that pixel is not shaded again at the higher \nresolution due to early-Z culling. In this way, we are able to perform our screen-space \nshading operations at the appropriate resolution for different regions of the screen. In \nFigure 1.3.2, we visualize the distribution of pixels shaded at each level of our hierarchy. \nFigure 1.3.2. Visualization of hierarchical pixel processing. \nNon-black pixels were shaded in the first pass at 1/16th resolution as in the image on the \nleft. The middle image shows the pixels shaded in the second iteration at one-quarter \nresolution, and only the pixels in the image on the right were shaded at full image \nresolution. \n \n \nBecause this approach exploits image scaling from low resolution to high resolution with \ninterpolation, discontinuity artifacts can appear at boundaries of lighting or shadows. We \naddress this issue during the multi-resolution rendering phase. We write 1.0 to the alpha \nchannel of R-buffer pixels that are lit; otherwise, we write zero. If pixels are lit by the same \nlights (or the same number of lights), their neighbors‘ alpha values will be equal. Therefore, \nwe interpolate these pixels to a higher-resolution buffer. Otherwise, we consider these \npixels within the boundary, and thus we discard them in the interpolation pass (see Figure \n1.3.3). We can handle shadow boundaries similarly. \n",
      "content_length": 3567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": " \n \nFigure 1.3.3. A boundary-check algorithm. If a pixel is lit by a light, we add one \nalpha for this pixel in the lighting phase. In the interpolation pass, we consider \npixels that are in boundaries whose neighbor pixels’ alpha values are different to \nothers, and thus we use a higher-resolution buffer without interpolation. \n \n \nIf shadow color is neither zero nor one (in other words, penumbra), we also set a pixel \nalpha to zero and thus discard it in the interpolation work. \nIn the composite pass, we render a screen-aligned quad, reading shading results from the \nfull-resolution R-buffer and material properties such as albedo to compute the final shading \nresult. We could draw scene geometry instead of drawing a screen quad for MSAA, similar \nto light pre-pass rendering. \nIn contrast to traditional deferred shading and light pre-pass rendering, multi-resolution \ndeferred shading reduces rendering costs for low-frequency pixels. Our multi-resolution \ndeferred shading is also more efficient than inferred lighting due to the hierarchical \napproach. Multi-resolution deferred shading can also be used for other rendering \ntechniques, such as the GPU-based light clustering technique for diffuse interreflection and \nsubsurface light diffusion called Light Pyramids [Ki08]. The Light Pyramids technique stores \nfirst-bounced lights in shadow maps and groups them by considering their angular and \nspatial similarity. Although such light clustering dramatically reduces the number of lights, it \nstill requires hundreds of lights for each pixel. Figure 1.3.4 shows an example of a \ncombination of Light Pyramids and multi-resolution deferred shading. Thanks to our pixel \nclustering, we achieved a performance improvement of approximately 1.5 to 2.0 times \nwithout noticeable quality loss. As pixel processing increases in complexity—for example, \nusing higher resolution or using more lights—the relative performance improvement also \nincreases. \nFigure 1.3.4. Indirect illumination using Light Pyramids [Ki08] based on \ntraditional deferred shading (left) and multi-resolution deferred shading (right: \n1.7 times faster). \n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": " \n \n \n \n \n \nConclusion and Future Work \nWe have presented a multi-resolution deferred shading technique that performs lighting and \nshading computations at appropriate screen-space frequency in order to improve the \nefficiency of deferred shading without aliasing. In the future, we would also like to develop \neven more efficient resolution-selection algorithms, and we also seek to handle a wider \nvariety of surface reflection models. We also hope to integrate transparent rendering of \ninferred lighting into our method. We believe that our method could be applied for not only \nlighting but also other rendering operations with high per-pixel overhead, such as per-pixel \ndisplacement mapping [Ki07b]. \n \nReferences \n[Engel09] Engel, Wolfgang. ―Designing a Renderer for Multiple Lights: The Light Pre-Pass \nRenderer.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: \nCharles River Media, 2009. 655-666. \n[Ki08] Ki, Hyunwoo. ―A GPU-Based Light Hierarchy for Real-Time Approximate Illumination.‖ \nThe Visual Computer 24.7–9 (July 2008): 649–658. \n[Ki07a] Ki, Hyunwoo. ―Hierarchical Rendering Techniques for Real-Time Approximate \nIllumination on Programmable Graphics Hardware.‖ Master‘s Thesis. Soongsil University, \n2007. \n[Ki07b] Ki, Hyunwoo and Kyoungsu Oh. ―Accurate Per-Pixel Displacement Mapping using a \nPyramid Structure.‖ 2007. Hyunwoo Ki. n.d. <http://ki-h.com/article/ipdm.html>. \n[Kircher09] Kircher, Scott and Alan Lawrance. ―Inferred Lighting: Fast Dynamic Lighting and \nShadows for Opaque and Translucent Objects.‖ Course on 3D and the Cinematic in Games. \nSIGGRAPH 2009. Ernest N. Morial Convention Center, New Orleans, LA. 6 August 2009. \n[Koonce07] Koonce, Rusty. ―Deferred Shading in Tabula Rasa.‖ GPU Gems 3. Ed. Hurbert \nNguyen. Kendallville, KY: Addison-Wesley, 2007. 429–458. \n",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": " \n \n[Mitchell04] Mitchell, Jason and Pedro Sander. ―Applications of Explicit Early-Z Culling.‖ \nCourse on Real-Time Shading. SIGGRAPH 2004. Los Angeles Convention Center, Los \nAngeles, CA. 8 August 2004. \n[Saito90] Saito, Takafumi and Tokiichiro Takahashi. ―Comprehensible Rendering of 3-D \nShapes.‖ ACM SIGGRAPH Computer Graphics 24.4 (August 1990): 197–206. \n[Shishkovtsov05] Shishkovtsov, Oles. ―Deferred Shading in S.T.A.L.K.E.R.‖ GPU Gems 2: \nProgramming Techniques for High-Performance Graphics and General Purpose Computation. \nEd. Matt Pharr. Kendallville, Ky: Addison-Wesley, 2005. 143–166. \n[Valient07] Valient, Michal. ―Deferred Rendering in Killzone 2.‖ Develop Conference 2007. \nBrighton Hilton Metropole, Brighton, England, UK. 25 July 2007. \n \n1.4. View Frustum Culling of Catmull-Clark Patches in DirectX 11 \nRahul P. Sathe, Advanced Visual Computing, Intel Corp \nrahul.p.sathe@intel.com \nDirectX 11 has introduced hardware tessellation in order to enable high geometric detail \nwithout increasing memory usage or memory bandwidth demands. Higher-order surface \npatches with displacements are of prime interest to game developers, and we would like to \nrender them as efficiently as possible. For example, we would like to cull subdivision surface \npatches (instead of the resulting triangles) that will not affect the final image. Culling a \ngiven patch avoids higher-order surface evaluation of domain points in that patch as well as \nprocessing of the triangles generated for the patch. The nature of higher-order surface \npatches coupled with displacements and animation make the process of culling them non-\ntrivial, since the exact geometric bounds are not known until well after the opportunity to \ncull a given patch. In this chapter, we will present an algorithm that evaluates conservative \nbounding boxes for displaced approximate Catmull-Clark subdivision surface patches at run \ntime, allowing us to perform view frustum culling on the patches. With this method, we \nachieve performance improvement with minimal overhead. \nBackground \nBefore describing our culling strategy, we must review the fundamentals of Catmull-Clark \nsubdivision surfaces, displacement mapping, and the methods that are currently in use to \napproximate Catmull-Clark subdivision surfaces on DirectX 11. \nDisplaced Subdivision Surfaces and Catmull-Clark Surfaces \nCatmull-Clark subdivision surfaces have become an increasingly popular modeling primitive \nand have been extensively used in offline rendering [DeRose98]. In general, subdivision \nsurfaces can be described as recursive refinement of a polygonal mesh. Starting with a \ncoarse polygonal mesh M0, one can introduce new vertices along the edges and faces and \nupdate the connectivity to get a mesh M1, and repeat this process to get meshes M2, M3, \nand so on. In the limit, this process approaches a smooth surface S. This smooth surface S \nis called the subdivision limit surface, and the original mesh M0 is often referred to as the \ncontrol mesh. \nThe control mesh consists of vertices connected to each other to form edges and faces. The \nnumber of other vertices that a given vertex is connected to directly by shared edges is \ncalled the valence of a vertex. In the realm of Catmull-Clark subdivision surfaces, a vertex \nis called a regular or ordinary vertex if it has a valence of four. If the valences of all of the \n",
      "content_length": 3388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": " \n \nvertices of a given quad are four, then that quad is called an ordinary quad or an ordinary \npatch. The faces that have at least one vertex that is not valence four are called \nextraordinary faces (or patches). \nApproximate Catmull-Clark Subdivision Surfaces \nRecently, Loop and Schaefer introduced a hardware-friendly method of rendering \nApproximate Catmull Clark (ACC) subdivision surfaces, which maps very naturally to the \nDirectX 11 pipeline [Loop08]. At its core, the ACC scheme maps each quadrilateral from the \noriginal control mesh to a bi-cubic Bezier patch. Loop and Schaefer show that, for ordinary \npatches, the bi-cubic Bezier corresponds exactly to the Catmull-Clark limit surface. \nExtraordinary patches do not correspond exactly to the limit surface, but Loop and Schaefer \ndecouple the patch description for position attributes and normal attributes in order to \nreduce the visual impact of the resulting discontinuities. To do this, for extraordinary \npatches, ACC generates separate normal and bi-tangent patches in order to impose GN \ncontinuity at patch boundaries. The word ―approximate‖ in ACC has its roots in the fact that \nthese extraordinary patches are GN continuous, and this GN continuity only guarantees the \nsame direction of partial derivatives but not the magnitudes across the patch boundaries. \nThe ACC scheme describes the normals and bi-tangents using additional Bezier patches, \nwhich results in a continuous normal field even across edges of extraordinary patches. \nDisplacement \nAlthough it is very empowering to be able to generate smooth surfaces from polygonal \nmeshes procedurally, such smooth surfaces are rarely encountered in real life and lack \nrealism without additional high-frequency geometric detail. This is where displacement \nmaps come into the picture. Displacement maps are simply textures that can be used to \nstore geometric perturbations from a smooth surface. Although normal maps and \ndisplacement maps have the similar effect of adding high-frequency detail, the difference is \nnotable around the silhouettes of objects. A normal mapped object‘s silhouette lacks \ngeometric detail because only per-pixel normals are perturbed and not the underlying \ngeometry, as illustrated in Figure 1.4.1. To add this high-frequency detail, displacement \nmaps can be applied to subdivision surfaces. \nFigure 1.4.1. Normal mapping versus displacement mapping. \n \n \n \nDirectX 11 Pipeline \nDirectX 11 has introduced three new stages to the graphics pipeline to enable dynamic on \nchip tessellation, as shown in Figure 1.4.4. The two new programmable pipeline stages are \nthe hull shader and the domain shader. Between these two programmable stages lies a new \nfixed function stage, the tessellator. Fortunately for us, ACC and Direct3D 11 were designed \nwith each other in mind, and there is a natural mapping of the ACC algorithm onto the \nDirect3D 11 pipeline. \n",
      "content_length": 2917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": " \n \nHull Shader \nAs illustrated in Figure 1.4.1, the new hull shader stage follows the traditional vertex \nshader. In a typical implementation of ACC on Direct3D 11, the vertex shader is responsible \nfor performing animation of the control mesh vertices. In the hull shader, each \nquadrilateral‘s four vertices and its one-ring neighborhood are gathered from the output of \nthe vertex shader. These vertices are used to define the control points of a bi-cubic Bezier \npatch. This basis conversion process that generates the Bezier patch control points is SIMD \nfriendly, and every output control point can be calculated independently of others. In order \nto exploit this opportunity for parallelism, this control point phase of the hull shader is \ninvoked once per control point. In the case of ACC, the basis conversion process depends on \nthe topology of the incoming patch, but the output control points are always a 4×4 Bezier \ncontrol mesh. Please refer to the sample code on the CD. \nFigure 1.4.2. Basis conversion for an irregular patch. \n \n \nIn addition to the computation of the Bezier control points, the hull shader can optionally \ncalculate edge tessellation factors in order to manage level of detail. One can assign \narbitrary tessellation factors to the edges of a patch (within some constraints, defined by \nthe DirectX 11 tessellator specifications). Because the hull shader is programmable, one can \nchoose any metric to calculate edge tessellation factors. Typical metrics may include screen \nspace projection, proximity to silhouette, luminosity reaching the patch, and so on. The \ncalculation of each edge tessellation factor is typically independent of the others, and hence \nthe edge tessellation factors can also be computed in parallel in a separate phase of the hull \nshader called the fork phase. The final stage of hull shader is called the join phase (or patch \nconstant phase) and is a phase in which the shader can efficiently compute data that is \nconstant for the entire patch. This stage is of most interest to us in this chapter. \nTessellator \nThe tessellator accepts edge LODs of a patch and other tessellator-specific states that \ncontrol how it generates domain locations and connectivity. Some of these states include \npatch topology (quad, tri, or isoline), inside reduction function (how to calculate inner \ntessellation factor(s) using outer tessellation factors), one-axis versus two-axis reduction \n(whether to reduce only one inner tessellation factor or two—once per each domain axis), \nand scale (how much to scale inner LOD). The tessellator feeds domain values to the \ndomain shader and connectivity information to the rest of the pipeline via the geometry \nshader. \nDomain Shader \nIn the case of quadrilateral patch rendering, the domain shader is invoked at domain values \n(u,v) determined by the tessellator. (In the case of triangular patches, the barycentric \n",
      "content_length": 2914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": " \n \ncoordinates (u,v,w); w = 1 – u – v are used.) Naturally, the domain shader has access to \noutput control points from the hull shader. Typically, the domain shader evaluates a higher-\norder surface at these domain locations using the control points provided by the hull shader \nas the basis. After evaluating the surface, the domain shader can perform arbitrary \noperations on the surface position, such as displacing the geometry using a displacement \nmap. \nIn ACC, we evaluate position using bi-cubic polynomials for a given (u,v). Our domain \nshader interpolates texture coordinates (s,t) from the four vertices using bilinear \ninterpolation to generate the texture coordinates for the given (u,v). We also optionally \nsample a displacement map at these interpolated texture coordinates. As mentioned earlier, \nnormal calculation is different for ordinary and extraordinary patches. For ordinary patches, \nwe just calculate d/du and d/dv of the position and take the cross-product. For \nextraordinary patches, we evaluate tangent and bi-tangent patches separately and take \ntheir cross-product. \n \nCulling \nThe mapping of ACC to the DirectX 11 pipeline that we have described allows us to render \nsmooth surfaces with adaptive tessellation and displacement mapping, resulting in a \ncompelling visual quality improvement while maintaining a modest memory footprint. At the \nend of the day, however, we are still rendering triangles, and the remaining stages of the \ngraphics pipeline are largely unchanged, including the hardware stages that perform \ntriangle setup and culling. This means that we perform vertex shading, domain shading, \ntessellation, and hull shading of all patches submitted to the graphics pipeline, including \nthose patches that are completely outside of the view frustum. Clearly, this provides an \nopportunity for optimization. The main contribution of this chapter is a method for frustum \nculling patches early in the pipeline in order to avoid unnecessary computations. Of course, \nwe must account for mesh animation and displacement, both of which deform a given patch \nin a way that complicates culling. An elegant generalized solution to surface patch culling \nhas been proposed by Hasselgren et al. that generates culling shaders, looking at domain \nshaders using Taylor Arithmetic [Hasselgren09]. This article proposes a simplified version of \nideas discussed in their work to cull the approximate Catmull-Clark patches against view \nfrustum. \n \nPre-Processing Step \nWe perform a pre-processing step on a given control mesh and displacement map in order \nto find the maximum displacement for each patch. Please note, although the positions are \nevaluated as bi-cubic polynomials using the new basis, the texture coordinates for those \npoints are the result of bilinear interpolation of texture coordinates of the corners. This is \ndue to the fact that the local (per-patch) uv-parameterization used to describe the Catmull-\nClark surface and the global uv-parameterization done while creating the displacement map \nare linearly dependent on each other. Figure 1.4.3 shows one such patch. This linear \ndependence means that straight lines u = 0, v = 0, u = 1, and v = 1 in the patch \nparameterization are also straight lines in the global parameterization. Due to this linear \nrelationship, we know the exact area in the displacement map from which the displacements \nwill be sampled in the domain shader for that patch. The maximum displacement in the \ngiven patch can be found by calculating the maximum displacement in the region confined \nby patch boundaries in the displacement map. Even if the displacement map stores vector-\nvalued displacements, the mapping is still linear, so we can still find the magnitude of the \nmaximum displacement for a given patch. Based on this, we can create a buffer for the \nentire mesh that stores this maximum displacement per patch. \n",
      "content_length": 3911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": " \n \nFigure 1.4.3. Mapping between global (s-t) and local (u-v) parameterization is \nlinear. The figure on the left shows (u,v) parameterization that is used for patch \nevaluation. The figure on the right shows the global parameterization (s,t) that \nwas used while unwrapping original mesh. Bold lines correspond to u=0, v=0, \nu=1, and v=1 lines in the figure on the left. \n \n \n \nRun-Time Step \nAt run time, the patch vertices of the control mesh go through the vertex shader, which \nanimates the control mesh. The hull shader then operates on each quad patch, performing \nthe basis transformation to Bezier control points. One convenient property of Bezier patches \nis that they always stay within the convex hull of the control mesh defining the patch. Using \nthe maximum displacement computed previously, we can move the convex hull planes of a \ngiven patch outward by the maximum displacement, resulting in conservative bounds \nsuitable for culling a given patch. Although moving the convex hull planes out by the max \ndisplacement may give tighter bounds compared to an axis-aligned bounding box (AABB) \nfor the control mesh, calculating the corner points can be tricky because it requires \ncalculation of plane intersections. It is simpler and more efficient to compute an AABB of the \ncontrol mesh and offset the AABB planes by the maximum displacement. \nIn Figure 1.4.5, we show a 2D representation of this process for illustration. Dotted black \nlines represent the basis-converted Bezier control mesh. The actual Bezier curve is shown in \nbold black, displacements along the curve normal (scalar valued displacements) are shown \nin solid gray, and the maximum displacement for this curve segment is denoted as d. An \nAABB for the Bezier curve is shown in dashed lines (the inner bounding box), and the \nconservative AABB that takes displacements into account is shown in dashed and dotted \nlines (the outer bounding box). \nFigure 1.4.4. The DirectX11 pipeline. Normally, triangles get culled after primitive \nassembly, just before rasterization. The proposed scheme culls the patches in the \n",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": " \n \nhull shader, and all the associated triangles from that patch get culled as a result, \nfreeing up compute resources. \n \n \nFigure 1.4.5. Conservative AABB for a displaced Bezier curve. The Bezier curve is \nshown in bold black, the control mesh in dotted lines, and displacements in solid \ngray lines. AABB for the Bezier curve without displacements is shown in dashed \nlines (inner bounding box), and conservative AABB for the displaced Bezier curve \nis shown in dashed and dotted lines (outer bounding box). \n \n \nAs you can see, the corners of inner and outer enclosures are more than d distance apart, \nso we are being more conservative than we need to be for the ease and speed of \ncomputation. \n",
      "content_length": 702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": " \n \nAt this point, we have a conservative patch AABB that takes displacements into account. If \nthe AABB for a patch is outside the view frustum, we know that the entire patch is outside \nthe view frustum and can be safely culled. If we make the view frustum‘s plane equations \navailable as shader constants, then our shader can test the AABB using in-out tests for view \nfrustum. Alternatively, one can transform the AABB into normalized device coordinates \n(NDC), and the in-out tests can be done in NDC space. In-out tests in NDC space are easier \nthan world space tests because they involve comparing only with +1 or –1. If the AABB is \noutside the view frustum, we set the edge LODs for that patch to be negative, which \nindicates to the graphics hardware that the patch should be culled. We perform the culling \ntest during the join phase (a.k.a. patch constant phase) of the hull shader because this \noperation only needs to be performed once per patch. \n \nPerformance \nFor each culled patch, we eliminate unnecessary tessellator and domain shader work for \nthat patch. All patches, whether or not they‘re culled, take on the additional computational \nburden of computing the conservative AABB and testing against the view frustum. When \nmost of the character is visible on the screen (for example, Figure 1.4.9 (a)), culling \noverhead is at its worst. Figure 1.4.6 shows that, even in this case, culling overhead is \nminimal and is seen only at very low levels of tessellation. At LOD=3, the gains due to \nculling a very small number of patches (around the character‘s feet) start offsetting the \ncycles spent on culling tests. \nFigure 1.4.6. Culling overhead is the worst when nothing gets culled. Culling \noverhead is minimal except at very low levels of tessellation. ―NO CULL‖ indicates \nthe fps measured when no culling code was running. ―CULL Overhead‖ shows the \nfps measured when culling code was running in the patch constant phase of \nshaders. \n \nWhen about half of the patches in our test model are outside of the view frustum (see \nFigure 1.4.9 (b)), the overhead of the AABB computations is offset by the gains from culling \nthe offscreen patches. The gains from culling patches are more noticeable at higher levels of \ntessellation. This is shown graphically in Figures 1.4.7 and 1.4.8. Figure 1.4.7 shows how \n",
      "content_length": 2333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": " \n \nfps changes with the edge tessellation factor (edge LOD) when about half of the patches are \nculled. As you can see, at moderate levels of tessellation, we strike the balance between \nbenefits of the proposed algorithm at increased level of detail. Figure 1.4.8 shows the same \ndata as percentage speed-up. \nFigure 1.4.7. Culling benefits go up with the level of tessellation, except at the \nsuper-high levels of tessellation where culling patches doesn’t help. At moderate \nlevels of tessellation, we get benefits of the proposed algorithm and still see high \ngeometric details. \n \nFigure 1.4.8. Culling benefits shown as percentage increase in fps against edge \nLODs (edge tessellation factor). \n \n",
      "content_length": 704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": " \n \nFigure 1.4.9. Screenshots showing our algorithm in action. We saw about 8.9 fps \nfor the view on the left and 15.1 fps for the view on the right on the ATI Radeon \n5870. Increase in the frame rate was due to view frustum culling patches. \n \n \nWe performed all our tests on the ATI Radeon 5870 card, with 1 GB GDDR. The benefits of \nthis algorithm increase with domain shader complexity and tessellation level, whereas the \nper-patch overhead of the culling tests remains constant. It is easy to imagine an \napplication strategy that first tests an object‘s bounding box against the frustum to \ndetermine whether patch culling should be performed at all for a given object, thus avoiding \nthe culling overhead for objects that are known to be mostly onscreen. \n \nConclusion \nWe have presented a method for culling Catmull-Clark patches against the view frustum \nusing the DirectX 11 pipeline. Applications will benefit the most from this algorithm at \nmoderate to high levels of tessellation. In the future, we would like to extend this technique \nto account for occluded and back-facing patches with displacements. \n \nReferences \n[DeRose98] DeRose, Tony, Michael Kass, and Tien Truong. ―Subdivision Surfaces in \nCharacter Animation.‖ Proceedings of the 25th Annual Conference on Computer Graphics \nand Interactive Techniques. 1998. ACM SIGGRAPH. n.d. \n<http://doi.acm.org/10.1145/280814.280826>. \n[Hasselgren09] Hasselgren, Jon, Jacob Munkberg, and Tomas Akenine-Möller. ―Automatic \nPre-Tessellation Culling.‖ ACM Transactions on Graphics 28.2 (April 2009): n.p. ACM Portal. \n[Loop08] Loop, Charles and Scott Schaefer. ―Approximating Catmull-Clark Subdivision \nSurfaces with Bicubic Patches.‖ ACM Transactions on Graphics 27.1 (March 2008): n.p. ACM \nPortal. \n[Microsoft09] Microsoft Corporation. DirectX SDK. August 2009. \n[Reif95] Reif, Ulrich. ―A Unified Approach to Subdivision Algorithms Near Extraordinary \nVertices.‖ Computer Aided Geometric Design 12.2 (March 1995): 153–174. ACM Portal. \n[Stam98] Stam, Jos. ―Exact Evaluation of Catmull-Clark Subdivision Surfaces at Arbitrary \nParameter Values.‖ Proceedings of the 25th Annual Conference on Computer Graphics and \nInteractive Techniques (1998): 395–404. ACM Portal. \n",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": " \n \n[Zorin2000] Zorin, Dennis and Peter Schroder. ―Subdivision for Modeling and Animation.‖ \nSIGGRAPH. 2000. 85–94. \n \n1.5. Ambient Occlusion Using DirectX Compute Shader \nJason Zink \njzink_1@yahoo.com \nMicrosoft has recently released DirectX 11, which brings with it significant changes in \nseveral of its APIs. Among these new and updated APIs is the latest version of Direct3D. \nDirect3D 11 provides the ability to perform multi-threaded rendering calls, a shader \ninterface system for providing an abstraction layer to shader code, and the addition of \nseveral new programmable shader stages. One of these new shader stages is the compute \nshader, which provides a significantly more flexible processing paradigm than was available \nin previous iterations of the Direct3D API. Specifically, the compute shader allows for a \ncontrollable threading model, sharing memory between processing threads, synchronization \nof primitive functions, and several new resource types to allow read/write access to \nresources. \nThis gem will provide an introduction to the compute shader and its new features. In \naddition, we will take an in-depth look at a Screen Space Ambient Occlusion (SSAO) \nalgorithm implemented on the compute shader to show how to take advantage of this new \nprocessing paradigm. We will examine the SSAO algorithm in detail and provide a sample \nimplementation to demonstrate how the compute shader can work together with the \ntraditional rendering pipeline. Finally, we will wrap up with a discussion of our results and \nfuture work. \nThe Compute Shader \nBefore we begin to apply the compute shader‘s capabilities to a particular problem domain, \nlet‘s take a closer look at the compute shader itself and the general concepts needed to \nprogram it. \nOverview \nThe compute shader is a new programmable shader stage that is actually not simply \ninserted into the traditional rendering pipeline like some of the other new DirectX 11 \npipeline stages discussed in Sathe‘s Gem 1.4. Rather, the compute shader is conceptually a \nstandalone processing element that has access to the majority of the functionality available \nin the common shader core, but with some important additional functionality. The two most \nimportant new mechanics are fine-grained control over how each thread is used in a given \nshader invocation and new synchronization primitives that allow threads to synchronize. The \nthreads also have read/write access to a common memory pool, which provides the \nopportunity for threads to share intermediate calculations with one another. These new \ncapabilities are the basic building blocks for advanced algorithms that have yet to be \ndeveloped, while at the same time allowing for traditional algorithms to be implemented in \ndifferent ways in order to achieve performance improvements. \nCompute Shader Threading Model \nTo use the compute shader, we need to understand its threading model. The main concept \nis that of a Thread Group. A Thread Group defines the number of threads that will be \nexecuting in parallel that will have the ability to communicate with one another. The threads \nwithin the Thread Group are conceptually organized in a 3D grid layout, as shown in Figure \n",
      "content_length": 3211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": " \n \n1.5.1, with the sizes along each axis of the grid determined by the developer. The choice of \nthe layout provides a simple addressing scheme used in the compute shader code to have \neach thread perform an operation on a particular portion of the input resources. When a \nparticular thread is running, it executes the compute shader code and has access to several \nsystem value input attributes that uniquely identify the given thread. \nFigure 1.5.1. Thread Groups visualized as a 3D volume. \n \n \nTo actually execute the compute shader, we tell the API to execute a given number of \nThread Groups via the Dispatch method, as illustrated in Figure 1.5.2. \nFigure 1.5.2. Visualization of the Dispatch method. \n \n",
      "content_length": 713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": " \n \n \nWith these two layout definitions in mind, we can look at how they affect the addressing \nscheme of the compute shader. The following list of system values is available to the \ncompute shader: \n \nSV_GroupID. This system value identifies the Thread Group that a thread belongs \nto with a 3-tuple of zero-based indices. \n \nSV_GroupThreadID. This system value identifies the thread index within the \ncurrent Thread Group with a 3-tuple of zero-based indices. \n \nSV_DispatchThreadID. This system value identifies the current thread identifier \nover a complete Dispatch call with a 3-tuple of zero-based indices. \n \nSV_GroupIndex. This system value is a single integer value representing a flat \nindex of the current thread within the group. \nThe individual threads running the compute shader have access to these system values and \ncan use the values to determine, for example, which portions of input to use or which \noutput resources to compute. For example, if we wanted a compute shader to perform an \noperation on each pixel of an input texture, we would define the thread group to be of size \n(x, y, 1) and call the Dispatch method with a size of (m, n, 1) where x*m is the width of the \nimage and y*n is the height of the image. In this case, the shader code would use the \nSV_DispatchThreadID system value to determine the location in the input image from \nwhich to load data and where the result should be stored in the output image. \nFigure 1.5.3 illustrates one way in which a 2D workload might be partitioned using this \nmethod. In this example, we have an image with a size of 32×32 pixels. If we wanted to \nprocess the image with a total of 4×4 (m = 4, n = 4) Thread Groups as shown, then we \nwould need to define the Thread Groups to each have 8×8 (x = 8 and y = 8) threads. This \ngives us the total number of threads needed to process all 32×32 (x*m and y*n) pixels of \nthe input image. \n",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": " \n \nFigure 1.5.3. Visualization of Thread Group distribution for a 2D workload, where \nthe number of Thread Groups (m = 4, n = 4) and the number of threads (x = 8, y = \n \n \n \nCompute Shader Thread Interactions \nIn addition to providing an easy-to-use thread addressing scheme, the compute shader also \nallows each Thread Group to declare a block of Group Shared Memory (GSM). This memory \nis basically defined as an array of variables that are accessible to all of the threads in the \nThread Group. The array itself can be composed of any native data types as well as \nstructures, allowing for flexible grouping of data. In practice, the Group Shared Memory is \nexpected to be on-chip register-based memory that should be significantly faster to access \nthan general texture memory, which can have unpredictable performance depending on \naccess patterns. \nSimilar to CPU-based multi-threaded programming, when you have multiple threads reading \nand writing to the same area of memory there is the potential that the same memory can \n",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": " \n \nbe accessed simultaneously by more than one thread. To provide some form of control over \nthe sequences of access, the compute shader introduces several atomic functions for thread \nsynchronization. For example, there is an atomic function for adding called \nInterlockedAdd. This can be used to have all threads perform a test sequence and then \nuse the InterlockedAdd function to increment a variable in the Group Shared Memory \nto tabulate an overall number of test sequences that produce a particular result. \nAnother atomic function is the InterlockedCompareExchange function, which \ncompares a shared variable with one argument and sets the variable to a second argument \nif the variable has the same value as the first argument. This provides the basic building \nblocks of creating a mutex system in the compute shader, where a shared variable serves \nas the mutex. Each thread can call this function on the mutex variable and only take action \nif it is able to update the variable to its own identifier. Since the compute shader is intended \nto provide massively parallel execution, a mutex is not really a preferred choice, but in \nsome situations it may be a desirable avenue to follow, such as when a single resource \nmust be shared across many threads. The Direct3D 11 documentation can be referenced for \na complete list of these atomic functions and how they can be used. \nAlso similar to CPU-based multi-threaded programming is the fact that it is more efficient to \ndesign your algorithms to operate in parallel while minimizing the number of times that they \nmust synchronize data with one another. The fastest synchronization operation is the one \nthat you don‘t have to perform! \nCompute Shader Resources \nNew resource types introduced in Direct3D 11 include Structured Buffers, Byte Address \nBuffers, and Append/Consume Buffers. Structured Buffers provide what they sound like—ID \nbuffers of structures available in your shader code. The Byte Address Buffers are similar, \nexcept that they are a general block of 32-bit memory elements. \nThe Append/Consume Buffers allow for stack/queue-like access to a resource, allowing the \nshader to consume the elements of a buffer one at a time and append results to an output \nbuffer one at a time. This should also provide some simplified processing paradigms in \nwhich the absolute position of an element is less important than the relative order in which \nit was added to the buffer. \nTo further facilitate the compute shader‘s parallel-processing capabilities, Direct3D 11 \nprovides a new resource view called an Unordered Access View (UAV). This type of view \nallows the compute shader (as well as the pixel shader) to have read and write access to a \nresource, where any thread can access any portion of the resource. This is a big departure \nfrom the traditional shader resource access paradigm; typically, a shader can only read \nfrom or write to a given resource during a shader invocation, but not both. The UAV can be \nused to provide random access to both the new and existing resource types, which provides \nsignificant freedom in designing the input and output structure of compute shader–based \nalgorithms. \nWith a general understanding of the new capabilities of the compute shader, we can now \ntake a look at a concrete example in order to better understand the details. We will discuss \nthe general concepts of the SSAO algorithm and then describe how we can use the compute \nshader‘s features to build an efficient implementation of the technique. \n \nScreen Space Ambient Occlusion \nScreen Space Ambient Occlusion is a relatively recently developed technique for \napproximating global illumination in the ambient lighting term based solely on the \n",
      "content_length": 3731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": " \n \ninformation present in a given frame‘s depth buffer [Mittring07]. As described in detail in \nGem 1.2 by Filion, an approximate amount of ambient light that reaches a given pixel can \nbe computed by sampling the area around the pixel in screen space. This technique \nprovides a convincing approximation to global illumination and performs at a usable speed \nfor high-end applications. \nThe quality of the algorithm depends on the number of samples and subsequent calculations \nthat are performed for each pixel. In the past few years, a variety of techniques have been \nproposed to modify the general SSAO algorithm with varying levels of quality versus \nperformance tradeoffs, such as HBAO [Bavoil09a] and SSDO [Ritschel09]. While these new \nvariants of the original algorithm provide improvements in image quality or performance, \nthe basic underlying concepts are shared across all implementations, and hence the \ncompute shader should be applicable in general. \nWe will now review some of these recent SSAO techniques and discuss several areas of the \nunderlying algorithm that can benefit from the compute shader‘s new capabilities. Then we \nwill look at an implementation that takes advantage of some of these possible \nimprovements. \nSSAO Algorithm \nAmbient occlusion techniques have been around for some time and have found uses \nprimarily in offline rendering applications [Landis02]. The concept behind these techniques \nis to utilize the geometric shape of a model to calculate which portions of the model would \nbe more likely to be occluded than others. If a given point on a model is located on a flat \nsurface, it will be less occluded than another point that is located at a fold in the surface. \nThis relationship is based on the following integral for the reflected radiance: \n \nIn this integral, Lin is the incident radiation from direction ω, and the surface normal vector \nis n. This integral indicates that the amount of light reflected at a given surface point is a \nfunction of the incident radiance and the angle at which it reaches that point. If there is \nnearby geometry blocking some portion of the surface surrounding the surface point, then \nwe can generally conclude that less radiant energy will reach the surface. With this in mind, \nthe ambient lighting term can be modulated by an occlusion factor to approximately \nrepresent this geometric relationship. \nOne way to perform this geometric calculation is to project a series of rays from each \nsurface point being tested. The amount of occlusion is then calculated depending on the \nnumber of rays that intersect another part of the model within a given radius from the \nsurface point. This effectively determines how much ―background‖ light can reach that point \nby performing the inverse operation of the radiance integral described previously. Instead of \nintegrating the incident radiance coming into that point over the surface of a hemisphere, \nwe shoot rays out from the surface point over the hemisphere to test for occlusion within \nthe immediate area. The overall occlusion factor is then calculated by accumulating the ray \ntest results and finding the ratio of occluded rays versus non-occluded rays. Once it is \ncalculated, this occlusion factor is then stored either per vertex or per pixel in a texture map \nand is used to modulate the ambient lighting term of that object when rendered. This \nproduces a rough approximation of global illumination. Figure 1.5.4 demonstrates this ray \ncasting technique. \nFigure 1.5.4. Side profile of a ray casting technique for approximating occlusion. \n",
      "content_length": 3593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": " \n \n \n \nThis technique works quite well for static scenes or individual static geometric models, but \nthe pre-computation requirements are not practical for dynamic geometry, such as skinned \nmeshes. Several alternative techniques have been suggested to allow for dynamic ambient \nocclusion calculations, such as [Bunnell05], which generalizes the geometric object into \ndisks to reduce the computational complexity of the occlusion calculations. This allows real-\ntime operation of the algorithm, but it still requires some pre-processing of the models \nbeing rendered to determine where to place the disks in the approximated models. In \naddition, the cost of performing the occlusion calculation scales with increased scene \ncomplexity. \nThe Screen Space Ambient Occlusion algorithm provides an interesting alternative technique \nfor determining an approximate occlusion value. Instead of computing an occlusion value \nbased on the geometric representation of a scene by performing ray casting, the occlusion \ncalculation is delayed until after the scene has been rasterized. Once the scene has been \nrasterized, an approximated amount of occlusion is determined by inspecting the contents \nof the scene‘s depth buffer only—the geometric queries are carried out on the depth buffer \ninstead of on the geometric models. This effectively moves the operation from an object \nspace operation to a screen space operation—which is one of the major benefits of this \nalgorithm. Since it operates at the screen space level, the algorithm‘s performance is less \nsensitive to the amount of geometry being rendered and is more sensitive to the resolution \nof the buffers being used. \nThe scene‘s depth buffer can be obtained by utilizing the actual Z-buffer used during \nrendering, by performing a separate rendering pass that writes the linear depth to a render \ntarget, or by using the depth information from a deferred rendering G-buffer. Regardless of \nhow the buffer is generated, the algorithm performs a processing pass that uses the depth \nbuffer as an input and generates an output texture that holds the occlusion values for the \nentire visible scene. Each pixel of the output is calculated using the depth information within \na given radius of its local area, which can be considered an approximation to ambient \nocclusion. I will refer to this output in the remainder of this document as the occlusion \nbuffer. When the final scene rendering is performed, the occlusion buffer is sampled based \non screen space location and used to modulate the ambient term of each object in the final \nscene. \nSSAO Algorithm Details \nScreen Space Ambient Occlusion has provided a significant improvement over previous \nambient occlusion algorithms. Due to the fact that the algorithm runs after a scene is \nrendered, it focuses the processing time on only the portion of the scene that is visible for \n",
      "content_length": 2891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": " \n \nthe current frame, saving a significant amount of computation and allowing the algorithm to \nbe run in real-time applications without pre-computation. However, the use of the depth \nbuffer also introduces a few obstacles to overcome. \nThere is the potential that some occluders will not be visible in the depth buffer if there is \nanother object in front of it. Since the depth buffer only records one depth sample per pixel, \nthere is no additional information about the occluders behind the foreground object. This is \ntypically handled by defaulting to zero occlusion if the depth sample read from the depth \nbuffer is too far away from the current pixel being processed. If a more accurate solution is \nneeded, depth peeling can be used to perform multiple occlusion queries, as described in \n[Bavoil09b]. \nAdditionally, if an object is offscreen but is still occluding an object that is visible onscreen, \nthen the occlusion is not taken into account. This leads to some incorrect occlusion \ncalculations around the outer edge of the image, but solutions have also been proposed to \nminimize or eliminate these issues. One possibility is to render the depth buffer with a \nlarger field of view than the final rendering to allow objects to be visible to the algorithm \naround the perimeter of the view port [Bavoil09a]. \nAnother issue with the algorithm is that a relatively large number of samples needs to be \ntaken in order to generate a complete representation of the geometry around each pixel. If \nperformance were not a concern, we could sample the entire area around the pixel P in a \nregular sampling pattern, but in real-time applications this quickly becomes impractical. \nInstead of a regular sampling pattern, a common solution is to use a sparse sampling kernel \nto choose sampling points around the current pixel. This roughly approximates the \nsurrounding area, but the decreased sampling rate may miss some detail. \nTo compensate for the decreased sampling, it is common to use a stochastic sampling \ntechnique instead. By varying the sampling kernel shape and/or orientation for each pixel \nand then sharing the results between neighboring pixels, an approximation to the more \nexpensive regular sampling pattern can be achieved. Since a typical 3D scene is composed \nof groups of connected triangles, the majority of the contents of the depth buffer will \ncontain roughly similar depth values in neighborhoods of pixels except at geometric \nsilhouette edges. The variation of the sampling kernel between pixels in combination with \nthis spatial coherence of the depth buffer allows us to share a combined larger number of \nsample results per pixel while reducing the overall number of calculations that need to be \nperformed. \nThis helps to effectively widen the sampling kernel, but it also introduces some additional \nhigh-frequency noise into the occlusion buffer. To compensate for this effect, it is common \nto perform a filtering pass over the entire occlusion buffer that blurs the occlusion values \nwithout bleeding across object boundaries. This type of a filter is referred to as a bilateral \nfilter, which takes into account both the spatial distance between pixels and the intensity \nvalues stored in neighboring pixels when calculating the weights to apply to a sample \n[Tomasi98]. This allows the filter to remove high-frequency noise and at the same time \npreserve the edges that are present in the occlusion buffer. In addition, the randomization \nprocess can be repeated over a small range to facilitate easier filtering later on. Figures \n1.5.5 and 1.5.6 show ambient occlusion results before and after bilateral filtering. \nFigure 1.5.5. A sample scene rendered without bilateral filtering. \n",
      "content_length": 3738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": " \n \n \n \nFigure 1.5.6. A sample scene after bilateral filtering. \n \n \nAs mentioned before, the algorithm is performed after rasterization, meaning that its \nperformance is directly related to the screen resolution being used. In fact, this dependency \non screen resolution has been exploited to speed up the algorithm as described in Gem 1.3. \nThe depth buffer and/or the occlusion buffer can be generated at a decreased resolution. If \nthe screen resolution is decreased by a factor of 2 in the x and y directions, there is an \noverall factor of 4 reduction in the number of occlusion pixels that need to be calculated. \nThen the occlusion buffer can either be upsampled with a bilateral filter or just directly used \nat the lower resolution. \nThis strategy can still lead to fairly pleasing results, since the contents of the occlusion \nbuffer are relatively low frequency. \nSSAO Meets the Compute Shader \nWhen looking at the block diagram of the SSAO algorithm in Figure 1.5.7, we can begin to \ncompare these high-level operations with the new capabilities of the compute shader to see \nhow we can build a more efficient implementation. We will now go over the steps of the \nalgorithm and discuss potential strategies for mapping to the compute shader. \nFigure 1.5.7. Block diagram of the SSAO algorithm. \n",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": " \n \n \n \n \nCalculation Setup \nThe first step shown in the block diagram is to initialize the computations for the current \npixel. This entails sampling the depth buffer to obtain the pixel‘s depth. One of the benefits \nof having a Group Shared Memory that can be shared by all threads in a Thread Group is \nthe possibility to share texture samples among the entire Thread Group. Because the shared \nmemory is supposed to be significantly faster than a direct texture sample, if each thread \nrequests a depth sample to initialize its own calculations, then it can also write that depth \nvalue to the shared memory for use later on by other threads. The net effect of every \nthread in a Thread Group doing this is to have a copy of the complete local depth data in \nthe Group Shared Memory. Later, as each thread begins calculating the relative occlusion \nagainst the local area, it can read the needed depth values from the Group Shared Memory \ninstead of directly loading from texture memory. Figure 1.5.8 shows this process. \nFigure 1.5.8. Comparison of directly sampling versus using the Group Shared \nMemory for cached sampling. \n",
      "content_length": 1132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": " \n \n \n \nThere are a few additional notes to consider on this topic, however. There is some overhead \nassociated with reading the depth values and then storing them to the Group Shared \nMemory. In addition, the texture cache can often provide very fast results from memory \nsample requests if the result was in the cache. Thus, depending on the hardware being run \nand the patterns and frequency of memory access, it may or may not provide a speed \nincrease to use the Group Shared Memory in practice. \nRandomize Sampling Kernel \nThe next step in the SSAO block diagram is to somehow randomize the sampling kernel that \nwill be used to later look up the surrounding area. This is typically done by acquiring a \nrandom vector and then performing a ―reflect‖ operation on each of the sampling kernel \nvectors around the random vector. Probably the most common way to acquire this vector is \nto build a small texture with randomized normal vectors inside. The shader can load a single \nnormalized reflection vector based on the screen space position of the pixel being processed \n[Kajalin09]. This makes removing the ―salt-and-pepper‖ noise easier in the filtering stage of \nthe algorithm. \nIn the past, SSAO was performed in the pixel shader, which means that the pixel shader \nrequired a screen space position as a fragment attribute to be passed by the vertex or \ngeometry shader. The compute shader can help to simplify this operation somewhat. By \nutilizing the Dispatch ID system value, we can automatically receive the integer ID of each \npixel being processed in our compute shader code. To create our repeating pattern of \nreflection vectors in screen space, we can simply perform a bitwise AND operation on the \nleast significant bits of the dispatch ID—in other words, if we wanted to repeat every 4×4 \nblock of pixels, we would mask off all but the two least significant bits of the ID. \nIn fact, we can even store the randomized vectors as an array of constants in our shader. \nThis eliminates the need for a texture sample and the repeating texture of normalized \nreflection vectors altogether. Of course this is predicated on the fact that we don‘t use too \nmany vectors, but we could always use the standard approach if that is needed. \nAcquire Depth Data \n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": " \n \nOnce the sampling kernel has been randomized, we can acquire each individual depth \nsample. In a traditional SSAO algorithm, this is done with a sampler that uses the x and y \ncoordinates of the current sampling kernel vector to offset from the current pixel location. \nSince the sampling kernel has been pseudo-randomized, there is a potential for reduced \ntexture cache efficiency if the sampling kernel width is large enough. \nIf we utilize the Group Shared Memory as described previously, then the depth values that \nwe need to acquire could already be available in the GSM. However, there are several points \nto consider before embarking on this strategy as well. Since the Thread Group will only be \noperating on one block of the depth data at a time—for example, a 16×16 block—then we \nneed to consider what happens at the edges of that block. The pixels along the outer edges \nof the block will need access to the depth samples within our sampling radius, and they \nwould not already be pre-loaded. This provides a choice—we could either pre-load a larger \nportion of the depth buffer to include the surrounding area or we could dynamically check to \nsee whether the data has been loaded to the GSM yet, and, if not, then directly get it from \nthe depth buffer. \nBoth options could have performance penalties. Pre-loading large bands of depth data \naround each block may end up increasing the number of depth samples to the point that it \nwould be just as efficient to perform the sampling in the traditional manner. If we \ndynamically decide whether or not to fetch data from the depth buffer, then we could \nperform a large number of dynamic branches in the shader, which could also be detrimental \nto performance. These factors need to be weighed against the increased access speed \nprovided by using the GSM instead of direct sampling. With the texture cache providing \nsimilar fast access for at least a portion of the texture samples, it is altogether possible that \nthe standard approach would be faster. Of course, any discussion of texture cache \nperformance depends on the hardware that the algorithm is running on, so this should be \ntested against your target platform to see which would be a better choice. \nThe other point to consider with using the GSM is that there is no native support for bilinear \nfiltering of the GSM data. If you wanted to filter the depth values for each depth sample \nbased on the floating-point values of the kernel offset vector, then you would need to \nimplement this functionality in the shader code itself. However, since the depth buffer \ncontains relatively low-frequency data, this is not likely to affect image quality in this case. \nPerform Partial Occlusion Calculation (per Sample) \nOnce we have obtained a depth sample to compare to our current pixel depth, we can move \nto the partial occlusion calculations. In this step, we determine whether our sample depth \ncauses any occlusion at the current pixel. There are many different varieties of calculations \navailable to perform here, from a binary test of the sample point being above or below the \nkernel offset vector [Kajalin09] all the way up to a piecewise defined function read from a \ntexture [Filion08]. \nRegardless of how the calculation is performed, there is an interesting possibility that the \ncompute shader introduces if the calculation is only a function of the depth delta—sharing \nocclusion calculations between pixels. If we call our current pixel point P and our current \nsample point S, then the occlusion caused at point P by point S is inherently related to the \ninverse occlusion at point S by point P. Since the compute shader can perform scatter \noperations, a single thread can calculate the occlusion for one pair of locations and then \nwrite the result to point P and the inverse of the calculation to point S. \nThis would save the number of required calculations by nearly a factor of 2, but it would \nalso introduce the need for some type of communication mechanism to get the values to \nboth occlusion buffer values. Since there is the possibility that multiple pixels would be \ntrying to write a result to the same pixel, we could attempt to use the atomic operations for \nupdating the values, but this could lead to a large number of synchronization events \nbetween threads. At the same time, these occlusion values can be accumulated in the GSM \n",
      "content_length": 4397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": " \n \nfor fast access by each thread. Again, the cost of the synchronization events will likely vary \nacross hardware, so further testing would be needed to see how much of a benefit could \ncome from this implementation. \nPerform Complete Occlusion Calculation \nThe final step in this process is to calculate the final occlusion value that will end up in the \nocclusion buffer for use in the final rendering. This is normally done by performing a simple \naverage of all of the partial occlusion calculations. In this way, we can scale the number of \nsamples used to calculate the occlusion according to the performance level of the target \nhardware. \nAs described earlier, there is typically some form of a bilateral filter applied to the occlusion \nbuffer after all pixels have a final occlusion value calculated. In general, filtering is one area \nthat could potentially see huge benefits from compute shader implementations. Since \nfiltering generally has an exact predetermined access pattern for the input image, the Group \nShared Memory can directly be used to pre-load the exact texture data needed. This is \nespecially beneficial when implementing 2D separable filters due to the ability to perform \nthe filtering pass in one direction, store the result into the GSM, then perform the second \nfiltering pass in the other direction over the values in the GSM without ever writing the \nresults back to the output buffer in between steps. Even though the bilateral filter is non-\nseparable, it has been shown that a decent approximation of it can be achieved with a \nseparable implementation [Pham05]. \n \nCompute Shader Implementation Details \nAfter reviewing some of the new features available in the compute shader and how they can \nbe used with the SSAO algorithm, we can now look at a sample implementation. Since the \ncompute shader techniques are relatively new, the focus of this implementation will be to \ndemonstrate some of its new features and draw some conclusions about appropriate-use \ncases for them. These features are described briefly here, with additional detail provided in \nthe following sections. \nThis implementation will utilize two different-size thread groups, 16×16 and 32×32, to \ngenerate the occlusion buffer. Using two different sizes will allow us to see whether the \nThread Group size has any effect on the performance of the algorithm. We will also \ndemonstrate the use of the GSM as a cache for the depth values and compare how well this \ntactic performs relative to directly loading samples from the depth buffer. In addition to \nusing the GSM, we also utilize the Gather sampling function for filling the GSM with depth \nvalues to see whether there is any impact on overall performance. The randomization \nsystem will utilize one of the new thread addressing system values to select a reflection \nvector, eliminating the need for a randomization texture. After the occlusion buffer has been \ngenerated, we will utilize a separable version of the bilateral filter to demonstrate the ability \nof the compute shader to efficiently perform filtering operations. \nImplementation Overview \nThe process is started by rendering a linear depth buffer at full-screen resolution with the \ntraditional rendering pipeline. Stored along with the depth value is the view space normal \nvector, which will be used during the occlusion calculations. This depth/normal buffer serves \nas the primary input to the compute shader to calculate a raw, unfiltered occlusion buffer. \nFinally, we use the depth/normal buffer and the raw occlusion buffer to perform separable \nbilateral filtering to produce a final occlusion buffer suitable for rendering the scene with the \nstandard rendering pipeline. \n",
      "content_length": 3717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": " \n \nDepth/Normal Buffer Generation \nThe depth/normal buffer will consist of a four-component floating-point texture, and each of \nthe occlusion buffers will consist of a single floating-point component. The depth/normal \nvectors are generated by rendering the linear view space depth and view space normal \nvectors into the depth/normal buffer. The depth value is calculated by simply scaling the \nview space depth by the distance to the far clipping plane. This ensures an output in the \nrange of [0,1]. The normal vector is calculated by transforming the normal vector into view \nspace and then scaling and biasing the vector components. Listing 1.5.1 shows the code for \ndoing so. \nListing 1.5.1. Generation of the view space depth and normal vector buffer \noutput.position = \nmul( float4( v.position, 1.0f ) ,WorldViewProjMatrix ); \n \nfloat3 ViewSpaceNormals = \nmul( float4( v.normal, 0.0f ) ,WorldViewMatrix ).xyz; \n \noutput.depth.xyz = ViewSpaceNormals * 0.5f + 0.5f; \noutput.depth.w = output.position.w / 50.0f; \n \nDepending on the depth precision required for your scene, you can choose an appropriate \nimage format—either 16 or 32 bits. This sample implementation utilizes 16-bit formats. \nRaw Occlusion Buffer Generation \nNext, we generate the raw occlusion buffer in the compute shader. This represents the \nheart of the SSAO algorithm. As mentioned earlier, we will utilize two different Thread \nGroup sizes. The occlusion calculations will be performed in Thread Groups of size 16×16×1 \nand 32×32×1. Since we can adjust the number of Thread Groups executed in the \napplication‘s Dispatch call, either Thread Group size can be used to generate the raw \nocclusion buffer. However, if there is any performance difference between the two Thread \nGroup sizes, this will provide some insight into the proper usage of the compute shader. \nRegardless of the size of the Thread Groups, each one will generate one portion of the raw \nocclusion buffer equivalent to its size. Each thread will calculate a single pixel of the raw \nocclusion buffer that corresponds to the thread‘s Dispatch thread ID system value. This \nDispatch thread ID is also used to determine the appropriate location in the depth/normal \nbuffer to load. The depth value and normal vector are loaded from the texture and \nconverted back into their original formats for use later. \nDepth Value Cache with the GSM \nWe will also set up the compute shader to cache local depth values in the GSM. Once the \ndepth values of the surrounding area are loaded into the GSM, all subsequent depth \nsampling can be performed on the GSM instead of loading directly from texture memory. \nBefore we discuss how to set up and use the GSM, we need to consider the desired layout \nfor the data. Since we are utilizing two different Thread Group sizes, we will specify a \ndifferent layout for each. Each of the Thread Groups requires the corresponding depth \nregion that it represents to be present in the GSM. In addition, the area surrounding the \nThread Group‘s boundary is also needed to allow the occlusion calculations for the border \npixels to be carried out correctly. This requires each thread to sample not only its own \ndepth/normal vector, but also some additional depth values to properly load the GSM for \n",
      "content_length": 3273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": " \n \nuse later. If we stipulate that each thread will load four depth values into the GSM, then our \n16×16 thread group will provide a 32×32 overall region in the GSM (the original 16×16 \nblock with an 8-pixel boundary). The 32×32 Thread Group size will provide a 64×64 region \n(the original 32×32 block with a 16-pixel boundary). \nFortunately, the Gather instruction can be utilized to increase the number of depth values \nthat are sampled for each thread. The Gather instruction returns the four point-sampled \nsingle component texture samples that would normally have been used for bilinear \ninterpolation—which is perfect for pre-loading the GSM since we are using only single \ncomponent depth values. This effectively increases the number of depth samples per \ntexture instruction by a factor of 4. If we use each thread to perform a single Gather \ninstruction, then we can easily fill the required areas of 32×32 and 64×64. The required \nsamples are obtained by having each thread perform the Gather instruction and store the \nresults in the GSM for all other threads within the group to utilize. This is demonstrated in \nListing 1.5.2. \nListing 1.5.2. Declaring and populating the Group Shared Memory with depth data \n#define USE_GSM \n \n#ifdef USE_GSM \n    // Declare enough shared memory for the padded thread group \nsize \n    groupshared float LoadedDepths[padded_x][padded_y]; \n#endif \n \n    int3 OffsetLocation = \n        int3( GroupID.x*size_x - kernel_x, GroupID.y*size_y - \nkernel_y, 0 ); \n    int3 ThreadLocation = GroupThreadID * 2; \n \n    float2 fGatherSample; \n    fGatherSample.x = ((float)GroupID.x * (float)size_x – \n        (float)kernel_x + (float)GroupThreadID.x * 2.0f ) / \nxres; \n    fGatherSample.y = ((float)GroupID.y * (float)size_y – \n        (float)kernel_y + (float)GroupThreadID.y * 2.0f ) / \nyres; \n \n    float4 fDepths = DepthMap.GatherAlpha( DepthSampler, \nfGatherSample + \n        float2( 0.5f / (float)xres, 0.5f / (float)yres ) ) * zf; \n \n    LoadedDepths[ThreadLocation.x][ThreadLocation.y] = \nfDepths.w; \n    LoadedDepths[ThreadLocation.x+1][ThreadLocation.y] = \nfDepths.z; \n    LoadedDepths[ThreadLocation.x+1][ThreadLocation.y+1] = \nfDepths.y; \n    LoadedDepths[ThreadLocation.x][ThreadLocation.y+1] = \nfDepths.x; \n \n    GroupMemoryBarrierWithGroupSync(); \n",
      "content_length": 2299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": " \n \n \nThe number of depth values loaded into the GSM can be increased as needed by having \neach thread perform additional Gather instructions. The Group Shared Memory is defined as \na 2D array corresponding to the size of the area that will be loaded and cached. After all of \nthe depth values have been loaded, we introduce a synchronization among threads in the \nThread Group with the GroupMemoryBarrierWithGroupSync() intrinsic function. \nThis function ensures that all threads have finished writing to the GSM up to this point in \nthe compute shader before continuing execution. \nA compile-time switch is provided in the sample code to allow switching between filling the \nGSM to use the cached depth values or to directly access the depth texture. Since the GSM \nhas the potential to improve the sampling performance depending on the access pattern, \nthis will allow an easy switch between techniques for a clear efficiency comparison. \nNext, we initialize the randomization of the sampling kernel with the lowest four bits of the \nDispatch thread ID x and y coordinates, as shown in Listing 1.5.3. The lowest four bits in \neach direction are used to select a reflection vector from a 2D array of rotation vectors, \nwhich are predefined and stored in a constant array. This eliminates the need for a separate \ntexture and range expansion calculations, but it requires a relatively large array to be \nloaded when the compute shader is loaded. After it is selected, the reflection vector is then \nused to modify the orientation of the sampling kernel by reflecting each of the kernel \nvectors about the reflection vector. This provides a different sampling kernel for each \nconsecutive pixel in the occlusion buffer. \nListing 1.5.3. Definition of the sampling kernel and selection of the randomization \nvector \nconst float3 kernel[8] = \n{ \n    normalize( float3(  1, 1, 1 )), \n    normalize( float3( -1,-1,-1 )), \n    normalize( float3( -1,-1, 1 )), \n    normalize( float3( -1, 1,-1 )), \n    normalize( float3( -1, 1, 1 )), \n    normalize( float3(  1,-1,-1 )), \n    normalize( float3(  1,-1, 1 )), \n    normalize( float3(  1, 1,-1 )) \n}; \n \nconst float3 rotation[16][16] = \n{ \n    { {...},{...},{...},{...}, ... } \n}; \n \nint rotx = DispatchThreadID.x & 0xF; \nint roty = DispatchThreadID.y & 0xF; \nfloat3 reflection = rotation[rotx][roty]; \n \nWith a random reflection vector selected, we can begin the iteration process by sampling a \ndepth value at the location determined by the randomized sampling kernel offsets. The \n",
      "content_length": 2524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": " \n \nsample location is found by determining the current pixel‘s view space 3D position and then \nadding the reoriented sampling kernel vectors as offsets from the pixel‘s location. This new \nview space position is then converted back to screen space, producing an (x, y) coordinate \npair that can then be used to select the depth sample from either the GSM or the \ndepth/normal texture. This is shown in Listing 1.5.4. \nListing 1.5.4. Sampling location flipping and re-projection from view space to \nscreen space \nfloat3 vRotatedOffset = \n        reflect( kernel[y], rotation[rotx][roty] ); \n \n    float fSign = dot( fPixelNormal, vRotatedOffset ); \n    if ( fSign < 0.0f ) \n        vFlippedOffset = -vFlippedOffset; \n \n    float3 Sample3D = PixelPosVS + vFlippedOffset * scale; \n    int3 newoffset = ViewPosToScreenPos( Sample3D ); \n \n#ifndef USE_GSM \n    float fSample = DepthMap.Load( iNewOffset ).w * zf; \n#else \n    float fSample = LoadDepth( iNewOffset - OffsetLocation ); \n#endif \n \nThe pixel‘s view space normal vector is used to determine whether the kernel offset vector \npoints away from the current pixel. If so, then the direction of the offset vector is negated \nto provide an additional sample that is more relevant for determining occlusion. This \nprovides additional samples in the visible hemisphere of the pixel, which increases the \nusable sample density for the pixel. The final screen space sample location is then used to \nlook up the depth sample either directly from the texture or from the GSM by calling the \nLoadDepth() function. \nAfter the depth has been loaded, the occlusion at the current pixel from this sample is \ncalculated. The calculation that is used is similar to the one presented in [Filion08] and \n[Lake10], using a linear occlusion falloff function raised to a power. This produces a smooth \ngradual falloff from full occlusion to zero occlusion and provides easy-to-use parameters for \nadjusting the occlusion values. \nThe partial occlusion calculation is repeated for a given number of samples, implemented as \na multiple of the number of elements in the sampling kernel. In this implementation, the \nnumber of samples can be chosen in multiples of eight. All of these individual occlusion \nvalues are averaged and then stored in the raw occlusion buffer for further processing. \nSeparable Bilateral Filter \nThe final step in our occlusion value generation is to perform the bilateral blur. As described \nearlier, we are able to use a separable version of the filter, even though it is not perfectly \naccurate to do so. The bilateral filter passes are implemented in the compute shader, with \neach of the separable passes being performed in an individual dispatch call. Since we are \nonly processing one direction at a time, we will first use one Thread Group for each row of \nthe image and then process the resulting image with one Thread Group for each column of \nthe image. In this arrangement, we can load the entire contents of a Thread Group‘s row or \n",
      "content_length": 3003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": " \n \ncolumn into the GSM, and then each thread can directly read its neighbor values from it. \nThis should minimize the cost of sampling a texture for filtering and allow larger filter sizes \nto be used. This implementation uses 7×7 bilateral filters, but this can easily be increased \nor decreased as needed. Listing 1.5.5 shows how the separable filter pass loads its data into \nthe GSM. \nListing 1.5.5. Loading and storing the depth and occlusion values into the GSM for \nthe horizontal portion of a separable bilateral filter \n// Declare enough shared memory for the padded group size \ngroupshared float2 horizontalpoints[totalsize_x]; \n... \n \nint textureindex = DispatchThreadID.x + DispatchThreadID.y * \ntotalsize_x; \n \n// Each thread will load its own depth/occlusion values \nfloat fCenterDepth = DepthMap.Load(DispatchThreadID).w; \nfloat fCenterOcclusion = AmbientOcclusionTarget[textureindex].x; \n \n// Then store them in the GSM for everyone touse \nhorizontalpoints[GroupIndex].x = fCenterDepth; \nhorizontalpoints[GroupIndex].y = fCenterOcclusion; \n \n// Synchronize all threads \nGroupMemoryBarrierWithGroupSync(); \n \nOne thread is declared for each pixel of the row/column, and each thread loads a single \nvalue out of the raw occlusion buffer and stores that value in the GSM. Once the value has \nbeen stored, a synchronization point is used to ensure that all of the memory accesses have \ncompleted and that the values that have been stored can be safely read by other threads. \nThe bilateral filter weights consist of two components: a spatially based weighting and a \nrange-based weighting. The spatial weights utilize a fixed Gaussian kernel with a size of 7 \ntaps in each direction. A separate Gaussian weighting value is calculated based on the \ndifference between the center pixel and each of the samples to determine the weighting to \napply to that sample. Modifying the sigma values used in the range-based Gaussian allows \nfor easy adjustment of the range-filtering properties of the bilateral filter. Listing 1.5.6 \nshows how this calculation is performed. \nListing 1.5.6. Horizontal portion of a separable bilateral filter in the compute \nshader \nconst float avKernel7 = \n{ 0.004431f, 0.05402f, 0.2420f, 0.3990f, 0.2420f, 0.05402f, \n0.004431f }; \n \nconst float rsigma = 0.0051f; \nfloat fBilateral = 0.0f; \nfloat fWeight = 0.0f; \n \nfor ( int x = -3; x <= 3; x++ ) \n",
      "content_length": 2385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": " \n \n{ \n    int location = GroupIndex + x; \n    float fSampleDepth = horizontalpoints[location].x; \n    float fSampleOcclusion = horizontalpoints[location].y; \n \n    float fDelta = fCenterDepth - fSampleDepth; \n    float fRange = \n        exp( ( -1.0f * fDelta * fDelta ) / ( 2.0f * rsigma * \nrsigma ) ); \n \n    fBilateral += fSampleOcclusion * fRange * avKernel[x+3]; \n    fWeight += fRange * avKernel[x+3]; \n} \n \nAmbientOcclusionTarget[textureindex] = fBilateral / fWeight; \n \nFinally, once both passes of the bilateral filter are performed, the values written to the final \noutput buffer can be used to supply the ambient lighting term of an output image. The \nvalue stored in the occlusion buffer represents visibility, and thus can be directly used as \nthe ambient lighting term without modification. The sample implementation provides its \noutput using only the occlusion value—no other lighting is applied to the scene. \n \nResults \nFigure 1.5.9 shows the end result of our compute shader–based implementation of the \nalgorithm. All images and performance numbers were generated on an AMD 57xx series \nGPU. The following image was generated using 32 depth samples for each occlusion pixel \nand with the 7×7 separable bilateral filter applied twice. \nFigure 1.5.9. Final results of the compute shader SSAO implementation. \n \n \nTo gain some insight into how well the new implementation techniques perform, we can \nreview the overall frame time for each of our optional configurations. Table 1.5.1 provides \n",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": " \n \nthe performance metrics for the two Thread Group sizes for varying numbers of samples \nused in the occlusion calculation. These frames-per-second figures were generated with the \ndirect sampling technique on a 640×480 render target with no bilateral filtering applied. \nTable 1.5.1.  \nNO GSM 8 \n16 \n24 \n32 \n40 \n48 \n56 \n64 \n16×16 \n947 608 437 349 288 241 212 188 \n32×32 \n961 610 445 350 288 245 214 187 \n \nThe two Thread Group sizes produce nearly identical performance numbers. This indicates \nthat the Thread Group size does not have a significant impact on this type of iterative \nalgorithm. When considering the actual frame time for each test (the inverse of the fps), we \nsee a linear increase in frame time for each of the additional sets of samples. In \ncomparison, Table 1.5.2 provides the same metrics generated with the GSM caching \ntechnique. \nTable 1.5.2.  \nW/GSM 8 \n16 \n24 \n32 \n40 \n48 \n56 \n64 \n16×16 811 699 531 426 357 306 269 240 \n32×32 764 625 485 391 330 286 252 225 \n \nTable 1.5.2 shows a different performance characteristic. When compared to the direct \nloading technique, we see the GSM technique performs slower at the 8-sample level. \nHowever, for all sample levels above this, the GSM technique significantly outperforms the \ndirect sampling method. For all of these higher sampling levels, we see a similar linear \nincrease in frame time but a smaller slope than the direct sampling method. Figure 1.5.10 \nshows the frame times for the four different cases. \nFigure 1.5.10. Comparison of frame times with and without the GSM as a cache. \n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": " \n \n \nThe slower performance with the GSM at lower sampling rates can be attributed to the \noverhead of loading and storing all of the additional depth data. However, there is a clear \nperformance gain for each additional sample used in the occlusion calculation. \nWith this performance advantage also comes some limitations. In both Thread Group sizes, \nwe defined a fixed border size. In some cases, when a pixel is close to the viewer, the offset \nvector can produce a screen space offset much larger than this border size. This can be \novercome either by scaling the size of the sampling kernel according to the distance from \nthe camera or by dynamically determining whether the sample location is available in the \nGSM and directly loading the sample if needed. \n \nConclusion and Future Work \nIn this chapter, we have applied the compute shader to the Screen Space Ambient \nOcclusion algorithm and discussed the implications of various implementation choices. This \nimplementation provides a basic framework upon which further proposed extensions can be \nimplemented relatively easily. Additional research can be directed at sharing partial \nocclusion values between neighboring pixels for each occlusion calculation, which is now \npossible due to the scatter capabilities of the compute shader. In addition, further \nexploration on the use of the GSM as a caching mechanism for regional depth averages \ncould be investigated. Finally, there have been several recent findings using multi-resolution \nrendering solutions for SSAO, which should also benefit from the compute shader \nimplementations. \n \nReferences \n[Bavoil09a] Bavoil, Louis and Miguel Sainz. ―Image-Space Horizon-Based Ambient \nOcclusion.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang F. Engel. Boston: \nCharles River Media, 2009. Section 6.2. \n",
      "content_length": 1825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": " \n \n[Bavoil09b] Bavoil, Louis and Miguel Sainz. ―Multi-Layer Dual-Resolution Screen-Space \nAmbient Occlusion.‖ 2009. SlideShare. n.d. <http://www.slideshare.net/NVIDIA/multilayer-\ndualresolution-screenspace-ambient-occlusion>. \n[Bunnell05] Bunnell, Michael. ―Dynamic Ambient Occlusion and Indirect Lighting.‖ 2005. \nGamedev.net. n.d. <http://downloads.gamedev.net/pdf/Pharr_ch14.pdf>. \n[Filion08] Filion, Dominic and Rob McNaughton. ―Effects and Techniques.‖ Course on \nAdvances in Real-Time Rendering in 3D Graphics and Games Course. SIGGRAPH 2008. Los \nAngeles Convention Center, Los Angeles, CA. 11 August 2008. \n[Kajalin09] Kajalin, Vladimir. ―Screen Space Ambient Occlusion.‖ ShaderX7: Advanced \nRendering Techniques. Ed. Wolfgang F. Engel. Boston: Charles River Media, 2009. Section \n6.1. \n[Lake10] Lake, Adam, ed. Game Programming Gems 8. Boston: Charles River Media, 2010. \n[Landis02] Landis, Hayden. ―Production-Ready Global Illumination.‖ Course notes on \nRenderMan in Production. SIGGRAPH 2002. Henry B. Gonzalez Convention Center, San \nAntonio, TX. 21 July 2002. Chapter 5. \n[Mittring07] Mittring, Martin. ―Finding Next Gen – CryEngine 2.0.‖ Course notes on \nAdvanced Real-Time Rendering in 3D Graphics and Games. SIGGRAPH 2007. San Diego \nConvention Center, San Diego, CA. 8 August 2007. 97–121. \n[Pham05] Pham, T.Q. and L.J. van Vliet. ―Separable Bilateral Filtering for Fast Video \nProcessing.‖ IEEE International Conference on Multimedia and Expo. Amsterdam, The \nNetherlands. July 2005. \n[Ritschel09] Ritschel, Tobias, Thorsten Grosch, and Hans-Peter Seidel. ―Approximating \nDynamic Global Illumination in Image Space.‖ Proceedings of the 2009 Symposium on \nInteractive 3D Graphics and Games (2009): 75–82. \n[Tomasi98] Tomasi, Carlo and Roberto Manduchi. ―Bilateral Filtering for Gray and Color \nImages.‖ Proceedings of the Sixth International Conference on Computer Vision. (1998): \n839–846. \n \n1.6. Eye-View Pixel Anti-Aliasing for Irregular Shadow Mapping \nNico Galoppo, Intel Advanced Visual Computing (AVC) \nnico.galoppo@intel.com \nThe irregular shadows algorithm (also known as Irregular Z-Buffer shadows) combines the \nimage quality and sampling characteristics of ray-traced shadows with the performance \nadvantages of depth buffer–based hardware pipelines [Johnson04]. Irregular shadows are \nfree from aliasing from the perspective of the light source because the occlusion of each \neye-view sample is evaluated at sub-pixel precision in the light view. However, irregular \nshadow mapping suffers from pixel aliasing in the final shadowed image due to the fact that \nshadow edges and high-frequency shadows are not correctly captured by the resolution of \nthe eye-view image. Brute-force super-sampling of eye-view pixels decreases shadow \naliasing overall but incurs impractical memory and computational requirements. \nIn this gem, we present an efficient algorithm to compute anti-aliased occlusion values. \nRather than brute-force super-sampling all pixels, we propose adaptively adding shadow \n",
      "content_length": 3024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": " \n \nevaluation samples for a small fraction of potentially aliased pixels. We construct a \nconservative estimate of eye-view pixels that are not fully lit and not fully occluded. Multiple \nshadow samples are then inserted into the irregular Z-buffer based on the footprint of the \nlight-view projection of potentially aliased pixels. Finally, the individual shadow sample \nocclusion values are combined into fractional and properly anti-aliased occlusion values. Our \nalgorithm requires minimal additional storage and shadow evaluation cost but results in \nsignificantly better image quality of shadow edges and improved temporal behavior of high-\nfrequency shadow content. \nPreviously, architectural constraints of traditional GPUs have inhibited per-frame \nconstruction and traversal of irregular data structures in terms of both performance and \nprogrammer flexibility. Our implementation of anti-aliased irregular shadow mapping \nexploits many strengths of the Larrabee architecture, one of which is the ability to write to \nrun-time computed addresses in global memory space. Additionally, we were able to do so \nusing the conventional C programming model and incorporate the adaptive nature of our \ntechnique with little effort. In comparison, traditional GPU architectures do not offer \nprogramming semantics for such global scatter operations, or they do so at extremely low \nperformance due to their highly specialized but constrained (localized) memory hierarchy \n(for example, the CUDA programming model), in the worst case falling back to main \nmemory writes [Sintorn08, Baumann05]. \nBackground and Problem: Shadow Edge Aliasing \nIn this section, we‘ll describe the characteristics of various popular shadow generation \nalgorithms and how they cope with different forms of aliasing, and we‘ll describe the \nproblem of screen-space shadow edge aliasing, which affects many current algorithms. \nPixel-Perfect Shadows with the Irregular Z-Buffer \nConventional shadow mapping renders the scene from the eye and the light, and in the final \ncompositing pass, the two views are compared to identify points that are in shadow \n[Williams78]. Light-view aliasing results from misalignment of these two views, as shown in \nFigure 1.6.1(a). There are several variants of shadow mapping that reduce but do not \neliminate sampling and self-shadowing artifacts [Fernando01, Stamminger02, Sen03, \nLloyd08, Lefohn07], because none of them resolves the fundamental mismatch in sampling \npatterns between the eye and light views, which is the root cause of most shadow mapping \nartifacts. \nFigure 1.6.1. Conventional versus irregular shadow mapping. In conventional \nshadow mapping (left), both the eye-view and light-view images are rendered \nwith the classic Z-buffer, leading to a mismatch between the desired and actual \nsample locations in the shadow map. Irregular shadow mapping (right) avoids this \nmismatch by rendering the light-view image with the irregular Z-buffer. \n",
      "content_length": 2974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": " \n \n \nIrregular shadow mapping addresses the root cause of visual artifacts in conventional \nshadow mapping by basing the light-view sampling pattern on the positions of pixels in the \neye-view raster and their corresponding depth values, therefore perfectly aligning the \ncompared occluder surface point with the projection of the shadow sample, as illustrated in \nFigure 1.6.1(b) [Johnson04, Johnson05]. The density of shadow samples varies significantly \nacross the image plane (as seen in Figure 1.6.2), which illustrates the need for an irregular \ndata structure during the light pass. \nFigure 1.6.2. The classic Z-buffer (a) samples a scene at regularly spaced points \non the light image plane. The irregular Z-buffer (b) samples a scene at arbitrary \npoints on the light image plane. Irregular shadow mapping (d) eliminates aliasing \nartifacts typically associated with conventional shadow mapping (c). \n \nIrregular shadow mapping utilizes the irregular Z-buffer in this context. This data structure \nexplicitly stores all of the sample locations in a two-dimensional spatial data structure rather \nthan implicitly representing them with a regular pattern. The data structure can be any \nspatial data structure that supports efficient range queries, such as a k-d tree or a grid. Just \nas in conventional shadow mapping, irregular shadow mapping projects triangles onto the \nlight-view image plane one at a time and then determines which samples lie inside a \ntriangle. Unlike conventional shadow mapping, this determination is made by querying the \nirregular Z-buffer. Finally, for each sample inside a triangle, irregular shadow mapping \nperforms the standard depth comparison and updates the sample‘s occlusion value. \n",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": " \n \nNote that when a conventional rasterizer is used during light-view projection of occluder \ntriangles, it is necessary to scan-convert expanded triangles to ensure fragments will be \ngenerated for any cell touched by the unexpanded triangle (also known as conservative \nrasterization [Akenine-Möller05]), since irregular Z-buffer samples may lie anywhere within \nthe cell bounds, as illustrated in Figure 1.6.3. For reference, [Hasselgren05] describes a \nshader implementation with example code. On the other hand, the advantage of a software \nrasterizer (for example, on Larrabee) is that a special rasterization path can be \nimplemented to apply custom rasterization rules that enable conservative rasterization \ndirectly without triangle expansion. \nFigure 1.6.3. Conservative rasterization versus conventional rasterization. Scan-\nconverted triangles have to be expanded during light-view projection to ensure \nfragments will be generated for any cell touched by the unexpanded triangle \n(shaded cells), since irregular Z-buffer samples (circles) may lie anywhere within \nthe pixel bounds. \n \n \n \nEye-View Aliasing \nWhile irregular shadow mapping is free of light-view aliasing, it still suffers from eye-view \naliasing of pixels, as illustrated in Figure 1.6.4. Such aliasing is a common problem in \ncomputer graphics. For example, it is also encountered in ray casting with a single eye ray \nper pixel. The problem is that thin geometry (high-frequency screen content) cannot be \ncaptured by a single ray, because the rays of two neighboring pixels may miss some \ngeometry even though the geometry projects to part of those pixels. Similarly, in the case \nof shadows in a rasterizer, it is possible that a surface point projected to the center of an \neye-view pixel is lit, but the entire area of the pixel is not lit. This phenomenon, known as \neye-view shadow aliasing, is caused by the fact that a single bit occlusion value is not \nsufficient to represent the occlusion value of aliased pixels. Anti-aliased occlusion values are \nfractional values that represent the fraction of the total pixel area that is lit. Recently, a few \nnovel shadow mapping techniques [Brabec01, Lauritzen06, Salvi08] have addressed this \nproblem and provide good solutions for eye-view aliasing but still expose light-view aliasing. \nFigure 1.6.4. The thin geometry in the tower causes eye-view aliasing of the \nprojected shadow. Note that some of the tower’s connected features are \ndisconnected in the shadow. \n",
      "content_length": 2504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": " \n \n \n \nThe most obvious approach to produce anti-aliased shadows with irregular shadow mapping \nis super-sampling of the entire screen by generating and evaluating multiple shadow \nsamples for each eye-view pixel. The anti-aliased occlusion value for a pixel is then simply \nthe average of the individual sample occlusion values. While this brute-force approach \ncertainly works, as illustrated in Figure 1.6.5, the computational and storage costs quickly \nbecome impractical. Data structure construction, traversal times, and storage requirements \nof the irregular Z-buffer are proportional to the number of shadow samples, making real-\ntime performance impossible on current hardware for even as little as four shadow samples \nper pixel. \nFigure 1.6.5. A four-times super-sampled irregular shadow mapping result image \nof the tower scene. \n \n \nThe recent method by [Robison09] provides a solution to compute anti-aliased shadows \nfrom the (aliased) output of irregular shadow mapping, but in essence it is also a brute-\nforce approach in screen space that does not exploit the irregular Z-buffer acceleration \nstructure and is therefore at a computational disadvantage compared to our approach. \n \n",
      "content_length": 1201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": " \n \nSolution: Adaptive Multi-Sampling of Irregular Shadows \nWe observed in Figure 1.6.5 that accumulating shadow evaluation results of multiple \nsamples per eye-view pixels provides a nice anti-aliased shadow and that potentially \nshadow-aliased pixels are those pixels that lie on a projected shadow edge. Therefore, we \npropose an efficient algorithm for anti-aliased irregular shadow mapping by adaptive multi-\nsampling of only those pixels that potentially lie on a shadow edge. Since only a marginal \nfraction of all screen pixels are shadow-edge pixels, this approach results in substantial \ngains in computational and storage costs compared to the brute-force approach. \nEssentially, our method is an extension of the original irregular shadow mapping algorithm, \nwhere the irregular Z-buffer acceleration structure remains a light space–oriented \nacceleration structure for the projected eye-view shadow samples. However, during \nirregular Z-buffer construction, potential shadow edge pixels are detected using a \nconservative shadow edge stencil buffer. Such pixels generate multiple shadow samples \ndistributed over the pixel‘s extent and are inserted in the irregular Z-buffer (shadow sample \nsplatting). Non-shadow-edge pixels are treated just as in the original irregular shadow \nmapping algorithm—a single shadow sample is sufficient to detect the occlusion value of the \nentire pixel. In the final shadow evaluation step, shadow occlusion values are averaged over \neach eye-view pixel‘s sample, resulting in a properly anti-aliased fractional occlusion value. \nThis value approximates the fraction of the pixel‘s area that is occluded, and it goes toward \nthe true value in the limit as the number of samples per pixel increases. \nAlgorithm: Anti-Aliased Irregular Shadow Mapping \nWe will now give an overview of the complete algorithm to provide structure to the \nremainder of the algorithm description in this section. Then we describe how to determine \nwhich pixels are potentially aliased by constructing a conservative shadow edge stencil \nbuffer and how to splat multiple samples into the irregular Z-buffer efficiently. Finally, we \nput it all together and present the complete algorithm in practice. \nWe can formulate our approach in the following top-level description of our algorithm: \n1.  Render the scene conservatively from the light‘s point of view to a variance shadow \nmap. \n2.  Render the scene from the eye point to a conventional Z-buffer—depth values only \n(gives points P0). \n3.  Construct a conservative shadow edge stencil buffer using a variance shadow map and \nlight-space projection of P0. \n4.  Using the stencil in Step 3, generate N extra eye-view samples Pi for potential shadow \nedge pixels only. \n5.  Transform eye-view samples Pi to light space P‘i (shadow sample splatting). \n6.  Insert all samples P‘i in the irregular Z-buffer. \n7.  Render the scene from the light‘s point of view while testing against samples in the \nirregular Z-buffer, tagging occluded samples. \n8.  Render the scene from the eye point, using the result from Step 7 and the conservative \nshadow edge stencil buffer. Multi-sampled eye-view pixels accumulate shadow sample \n",
      "content_length": 3193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": " \n \nvalues into a fractional (anti-aliased) shadow value. \nConservative Shadow Edge Stencil Buffer \nTo adaptively add shadow samples at shadow-edge pixels, we construct a special stencil \nbuffer that answers the following question: Is there any chance that this eye-space pixel is \npartially occluded by geometry in this light-space texel? We call this stencil buffer the \nconservative shadow edge stencil buffer. Giving an exact answer to the aforementioned \nquestion is impossible because it is essentially solving the shadowing problem. However, we \ncan use a probabilistic technique to answer the question conservatively with sufficient \nconfidence. A conservative answer is sufficient for our purpose, since multi-sampling of non-\nshadow-edge pixels does not alter the correctness of the result—it only adds some extra \ncost. Obviously, we strive to make the stencil buffer only as conservative as necessary. \nWe employ a technique called Variance Shadow Mapping [Lauritzen06]. Variance shadow \nmaps encode a distribution of depths at each light-space texel by determining the mean and \nvariance of depth (the first two moments of the depth distribution). These moments are \nconstructed through mip-mapping of the variance shadow map. When querying the variance \nshadow map, we use these moments to compute an upper bound on the fraction of the \ndistribution that is more distant than the surface being shaded, and therefore this bound \ncan be used to cull eye-view pixels that have very little probability to be in shadow. \nIn particular, the cumulative distribution function F(t) = P(x ≥ t) can be used as a measure \nof the fraction of the eye-view fragment that is lit, where t is the distance of the eye-view \nsample to the light, x is the occluder depth distribution, and P stands for the probability \nfunction. While we cannot compute this function F(t) exactly, Chebyshev‘s inequality gives \nan upper bound: \n \nThe upper bound Pmax (t) and the true probability Plit (t) are depicted in Figure 1.6.6. Thus, \nwe can determine that it is almost certain that a projected eye-view sample with light depth \nt is in shadow (for example, with 99-percent certainty) by comparing Pmax to 1% (Pmax < \nimplies Plit (t) < 0.01). \nFigure 1.6.6. Pin shadow (t) and Plit (t), in addition to their conservative upper bounds \nPmax (t) and p′max (t). \n \n",
      "content_length": 2347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": " \n \nConversely, we can use the same distribution to construct a bound to cull eye-view pixels \nthat have very high probability of being lit: \n \nIn summary, the conservative shadow edge stencil buffer can be constructed in the \nfollowing steps: \n1.  Render the scene from the light‘s point of view, writing out depth x and depth squared \nx2 to a variance shadow map texture (VSM). \n2.  Mip-map the resulting texture, effectively computing E(x) and E(x2), the mean and \nvariance of the depth distribution. \n3.  Render the scene from the eye point, computing for each sample: \na. The light depth t by projection of the sample to light space. \nb. E(x) and E(x2) by texture-sampling the mip-mapped VSM with the appropriate \nfilter width, determined by the extent of the light projection of the pixel area. \nc. μ = E(x), ζ2 = E(x2) – E(x) and Pmax (t), p′max (t) \n4.  Compare Pmax (t) and p′max (t) to a chosen threshold (for example, 1 percent). Set the \nstencil buffer bit if either one is smaller than the threshold. \nThese steps can be implemented in HLSL shader pseudocode, as shown in Listing 1.6.1. \nListing 1.6.1. Conservative shadow edge stencil buffer construction HLSL shader \nfloat2 ComputeMoments(float Depth) \n{ \n    // Compute first few moments of depth \n    float2 Moments; \n    Moments.x = Depth; \n    Moments.y = Depth * Depth; \n    return Moments; \n} \n \nfloat ChebyshevUpperBound( \n  float2 moments, float mean, float minVariance) \n{ \n    // Compute variance \n    float variance = max( \n               minVariance, \n               moments.y - (moments.x * moments.x)); \n    float d        = mean - moments.x; \n    float pMax     = variance / (variance + (d * d)); \n \n    // One-tailed Chebyshev's Inequality \n",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": " \n \n    return (mean <= moments.x ? 1.0f : pMax); \n} \n \nbool IsPotentialShadowEdge(float2 texCoord, \n                           float2 texCoordDX, \n                           float2 texCoordDY, \n                           float  depth) \n{ \n    float4 occluderData; \n    // Variance Shadow Map mip-mapped LOD tex lookup \n    occluderData = texShadowMap.SampleGrad( \n      sampShadowMap, \n      texCoord, texCoordDX, texCoordDY); \n \n    float2 posMoments = occluderData.xy; \n \n    // Minimum variance to take account for variance \n    // across entire pixel \n    float gMinVariance = 0.000001f; \n \n    float pMaxLit = ChebyshevUpperBound( \n        posMoments, depth, gMinVariance); \n    float pMaxShadow = ChebyshevLowerBound( \n        posMoments, depth, gMinVariance); \n \n    if (PMaxLit < .01 || PMaxShadow < .01) { \n      return true; \n    } \n \n    return false; \n} \n \nNote that while conventional rasterization of the scene from the light‘s point of view in Step \n1 earlier is sufficient for generating a conventional variance shadow map, it is not sufficient \nfor generating our conservative stencil buffer. Conventional rasterization does not guarantee \nthat a primitive‘s depth contributes to the minimum depth of each light-view texel that it \ntouches. Hence, the conservativeness of the stencil buffer would not be preserved as \nillustrated in Figure 1.6.7, which depicts potential shadow-edge pixels in overlay but misses \nquite a few due to the low resolution of the variance shadow map. \nFigure 1.6.7. Conservative shadow edge stencil map with regular rasterization. \nMany potential shadow-edge pixels (overlay) are missed due to low resolution of \nthe variance shadow map. \n",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": " \n \n \n \nTo preserve conservativeness, it is required to perform conservative rasterization in the \nlight-view render of Step 1, just as we do during the light-view render of irregular shadow \nmapping illustrated in Figure 1.6.3. Figure 1.6.8 depicts correctly detected potential \nshadow-edge pixels in overlay, regardless of the variance shadow map resolution. \nFigure 1.6.8. Conservative shadow edge stencil map with conservative \nrasterization. All potential shadow-edge pixels are detected (overlay), regardless \nof the variance shadow map resolution. \n \n \n \nShadow Sample Splatting \nIn the irregular Z-buffer construction phase, when the time comes to generate additional \nsamples for potentially aliased pixels, as defined by the conservative shadow edge stencil \nbuffer, we insert the eye-view samples in each light-view grid cell that is touched by the \npixel samples. We call this process shadow sample splatting, because we conceptually splat \nthe projection of the pixel footprint into the light space grid data structure. This process is \nas follows: \n1. In addition to light view coordinates of the eye-view pixel center, also generate \nmultiple samples per eye-view pixel. We have achieved very good results with \n",
      "content_length": 1227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": " \n \nrotated grid 4× multi-sampling, but higher sample rates and even jittered sampling \nstrategies can be used to increase the quality of the anti-aliasing. \n2. Project all samples into light space as in the original irregular shadow algorithm. \n3. Insert all samples into the irregular Z-buffer as in the conventional irregular \nshadowing algorithm. Potentially multiple light grid cells are touched by the set of \nsamples of a pixel. \n \nResults and Discussion \nWe will now show the results of our algorithm for two different scenes. The first scene \nconsists of a tower construction with fine geometry casting high-frequency shadows onto \nthe rest of the scene. The second scene is the view of a fan at the end of a tunnel, viewed \nfrom the inside. The fan geometry casts high-frequency shadows on the inside walls of the \ntunnel. The tunnel walls are almost parallel to the eye and light directions, a setup that is \nparticularly hard for many shadow mapping algorithms. Irregular shadow mapping shows its \nstrength in the tunnel scene because no shadow map resolution management is required to \navoid light-view aliasing. However, severe eye-view aliasing artifacts are present for the \nsingle-sample irregular shadow algorithm (see Figures 1.6.10(a) and 1.6.11(a)). Figure \n1.6.9 illustrates the result of computing the conservative shadow edge stencil buffer on both \nscenes: Potential shadow edge pixels are rendered with an overlay. Figure 1.6.10 compares \nsingle-sample irregular shadows with 4× rotated grid multi-sampling on potential shadow \nedge pixels only. Note the significant improvement in the tower shadow, where many \ndisconnected features in the shadow are now correctly connected in the improved \nalgorithm. Figure 1.6.11 illustrates the same comparison for the tunnel scene. There is a \ngreat improvement in shadow quality toward the far end of the tunnel, where high-\nfrequency shadows cause significant aliasing when using only a single eye-view shadow \nsample. \nFigure 1.6.9. Result of the conservative shadow edge stencil buffer on the tower \n(a) and tunnel (b) scenes. Potential shadow edge pixels are rendered with an \noverlay. \n \n \nFigure 1.6.10. Tower scene: (a) Single-sample irregular shadows and (b) 4× \nrotated grid multi-sampling on potential shadow edge pixels only. Note the \nsignificant improvement in the tower shadow, where many disconnected features \nin the shadow are now correctly connected in the improved algorithm. \n",
      "content_length": 2463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": " \n \n \n \nFigure 1.6.11. Tunnel scene: (a) Single-sample irregular shadows and (b) 4× \nrotated grid multi-sampling on potential shadow edge pixels only. There is a great \nimprovement in shadow quality toward the end of the tunnel, where high-\nfrequency shadows caused significant anti-aliasing when using only a single eye-\nview shadow sample. \n \n \n \nImplementation Details \nOn Larrabee, we have implemented Steps 2 and 3 of our algorithm in an efficient post-\nprocess over all eye-view pixels in parallel. However, Step 3 is identical to the conventional \nirregular shadowing algorithm; therefore, it could be implemented as in [Arvo07] as well. \nConceptually, we use a grid-of-lists representation for the irregular Z-buffer. This \nrepresentation is well-suited to parallel and streaming computer architectures and produces \nhigh-quality shadows in real time in game scenes [Johnson05]. The following chapter of this \nbook [Hux10], in particular Figure 1.7.1, explains our grid-of-lists representation and its \nconstruction in more detail. \nFinally, our solution was implemented in a deferred renderer, but it could also be \nimplemented in a forward renderer with a few modifications. \nCompute Requirements \nSince only a marginal fraction of all screen pixels are shadow-edge pixels, this approach \nresults in substantial gains in computational and storage costs compared to the brute-force \n",
      "content_length": 1392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": " \n \napproach. Compared to the single-sample irregular shadow maps, the additional \ncomputational cost is relatively small. For example, let‘s assume the number of potential \nshadow-edge pixels is ~10 percent of all eye-view pixels, and that we generate N additional \nsamples per potential shadow-edge pixel. Since data structure construction and traversal \ntimes are proportional to the number of shadow map samples, this means an additional cost \nof 10N percent for anti-aliasing. \nAdditionally, there is an extra cost associated with creating the conservative shadow edge \nstencil buffer. In our implementation inside a deferred rendering, much of the required \ninformation was already computed—therefore, that extra cost is small. However, our \nalgorithm does require an extra light-space pass, per light, to capture the depth distribution \ninto the variance shadow map. \nStorage Requirements \nThe storage cost is the same as the standard irregular Z-buffer, proportional to the number \nof samples. Again, for 10-percent extra samples, 10N-percent extra storage is required, \ndepending on the implementation. Storage of the stencil buffer requires only 1 bit per eye-\nview pixel and can easily be packed into one of the existing eye-view buffers of the irregular \nshadowing algorithm. \nFuture Work \nGoing forward, we would like to investigate the benefits of merging our algorithm that \nadaptively samples potential shadow edge pixels multiple times with conventional multi-\nsampling techniques that adaptively sample the geometry silhouette pixels multiple times—\nfor best performance, preferably through the use of common data structures and shared \nrendering passes. Additionally, it should be fairly straightforward to extend our approach to \nsoft irregular shadow mapping, where the concept of anti-aliasing is implicit, as both the \nsoft and hard irregular shadow mapping algorithms share the same algorithmic framework \n[Johnson09]. For soft shadows, one may envision extending the conservative stencil to \nshadow penumbra detection. \n \nConclusion \nThe main advantage of irregular shadow maps with respect to conventional shadow maps is \nthat they bear no light-view aliasing. However, irregular shadow maps are affected by eye-\nview aliasing of the shadow result. Recent pre-filterable shadow mapping algorithms and \nbrute-force eye-view techniques have provided solutions for anti-aliased shadows, but none \nof them exploits the irregular Z-buffer acceleration structure directly. The method in this \nchapter is an extension of irregular shadow mapping, exploits the same irregular data \nstructure, and is therefore the first algorithm to produce anti-aliased shadows by means of \nadaptive multi-sampling of irregular shadow maps, while keeping all its other positive \ncharacteristics, such as pixel-perfect ray-traced quality shadows and the complete lack of \nlight-view aliasing. \n \nAcknowledgements \nWe‘d like to thank the people at the 3D Graphics and Advanced Rendering teams at the \nIntel Visual Computing group for their continued input while developing the methods \ndescribed here. Many thanks go out in particular to Jeffery A. Williams for providing the art \nassets in the tower and tunnel scenes and to David Bookout for persistent support and \nfeedback. \n",
      "content_length": 3279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": " \n \n \nReferences \n[Akenine-Möller05] Akenine-Möller, Tomas and Timo Aila. ―Conservative and Tiled \nRasterization Using a Modified Triangle Setup.‖ Journal of Graphics, GPU, and Game Tools \n10.3 (2005): 1–8. \n[Arvo07] Arvo, Jukka. ―Alias-Free Shadow Maps using Graphics Hardware.‖ Journal of \nGraphics, GPU, and Game Tools 12.1 (2007): 47–59. \n[Baumann05] Baumann, Dave. ―ATI Xenos: Xbox 360 Graphics Demystified.‖ 13 June 2005. \nBeyond 3D. n.d. http://www.beyond3d.com/content/articles/4/>. \n[Brabec01] Brabec, Stefan and Hans-Peter Seidel. ―Hardware-Accelerated Rendering of \nAntialiased Shadows with Shadow Maps.‖ Proceedings of the International Conference on \nComputer Graphics (July 2001): 209. ACM Portal. \n[Fernando01] Fernando, Randima, Sebastian Fernandez, Kavita Bala, and Donald P. \nGreenberg. ―Adaptive Shadow Maps.‖ Proceedings of the 28th Annual Conference on \nComputer Graphics and interactive Techniques (2001): 387–390. ACM Portal. \n[Hasselgren05] Hasselgren, Jon, Tomas Akenine-Möller, and Lennart Ohlsson. ―Conservative \nRasterization.‖ GPU Gems 2. 2005. NVIDIA. n.d. \n<http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter42.html>. \n[Hux10] Hux, Allen. ―Overlapped Execution on Programmable Graphics Hardware.‖ Game \nProgramming Gems 8. Ed. Adam Lake. Boston: Charles River Media, 2010. \n[Johnson04] Johnson, Gregory S., William R. Mark, and Christopher A. Burns. ―The Irregular \nZ-Buffer and its Application to Shadow Mapping.‖ April 2004. The University of Texas at \nAustin. n.d. <http://www.cs.utexas.edu/ftp/pub/techreports/tr04-09.pdf>. \n[Johnson05] Johnson, Gregory S., Juhyun Lee, Christopher A. Burns, and William R. Mark. \n―The Irregular Z-Buffer: Hardware Acceleration for Irregular Data Structures.‖ ACM \nTransactions on Graphics 24.4 (October 2005): 1462–1482. ACM Portal. \n[Johnson09] Johnson, Gregory S., Allen Hux, Christopher A. Burns, Warren A. Hunt, William \nR. Mark, and Stephen Junkins. ―Soft Irregular Shadow Mapping: Fast, High-Quality, and \nRobust Soft Shadows.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames. (2009): 57–66. ACM Portal. \n[Lauritzen06] Lauritzen, Andrew and William Donnelly. ―Variance Shadow Maps.‖ \nProceedings of the 2006 Symposium on Interactive 3D Graphics and Games. (2006): 161–\n165. ACM Portal. \n[Lefohn07] Lefohn, Aaron E., Shubhabrata Sengupta, and John D. Owens. ―Resolution-\nMatched Shadow Maps.‖ ACM Transactions on Graphics 26.4 (Oct. 2007): 20. ACM Portal. \n[Lloyd08] Lloyd, D. Brandon, Naga K. Govindaraju, Cory Quammen, Steven E. Molnar, and \nDinesh Manocha. ―Logarithmic Perspective Shadow Maps.‖ ACM Transactions on Graphics \n27.4 (Oct. 2008): 1–32. ACM Portal. \n[Robison09] Robison, Austin, and Peter Shirley. ―Image Space Gathering.‖ Proceedings of \nthe Conference on High Performance Graphics 2009 (2009): 91–98. ACM Portal. \n",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": " \n \n[Salvi08] Salvi, Marco. ―Rendering Filtered Shadows with Exponential Shadow Maps.‖ \nShaderX6: Advanced Rendering Techniques. Ed. Wolfgang Engel. Boston: Charles River \nMedia, 2008. 257–274. \n[Sen03] Sen, Pradeep, Mike Cammarano, and Pat Hanrahan. ―Shadow Silhouette Maps.‖ \nACM Transactions on Graphics 22.3 (July 2003): 521–526. ACM Portal. \n[Sintorn08] Sintorn, Erik, Elmar Eisemann, and Ulf Assarsson. ―Sample Based Visibility for \nSoft Shadows using Alias-free Shadow Maps.‖ Computer Graphics Forum: Proceedings of \nthe Eurographics Symposium on Rendering 2008 27.4 (June 2008): 1285–1292. \n[Stamminger02] Stamminger, Marc and George Drettakis. ―Perspective Shadow Maps.‖ \nProceedings of the 29th Annual Conference on Computer Graphics and Interactive \nTechniques (2002): 557–562. ACM Portal. \n[Williams78] Williams, Lance. ―Casting Curved Shadows on Curved Surfaces.‖ ACM \nSIGGRAPH Computer Graphics 12.3 (Aug. 1978): 270–274. ACM Portal. \n \n1.7. Overlapped Execution on Programmable Graphics Hardware \nAllen Hux, Intel Advanced Visual Computing (AVC) \nallen.hux@intel.com \nSome graphics algorithms require data structure construction and traversal steps that do \nnot map well to constrained graphics pipelines. Additionally, because of the dependencies \nbetween rendering and non-rendering passes, much (or all) of the compute power of the \ndevice may go idle between steps of a given algorithm. In this gem, we examine techniques \nfor executing non-rendering algorithms concurrently with traditional rendering on \nprogrammable graphics hardware, such as Larrabee. Such programmable graphics devices \nenable fine-grained signaling and event graphs, allowing algorithmic stages to ―overlap.‖ As \na working model, we present an implementation of Irregular Z-Buffer (IZB) shadows—an \nalgorithm requiring both standard rendering passes (for example, depth-only pre-pass) and \nparallelized data structure construction. Identification of rendering and non-rendering work \nthat is not dependent reveals opportunities to remove the stalls that currently occur when \nswitching between the two types of workloads. The APIs discussed in this article are \nexamples and not necessarily representative of APIs provided with a particular product. \nIntroduction to Irregular Z-Buffer Shadows \nThe simplest shadow mapping algorithm requires two passes. First, render the scene from \nthe light view to get a light-view depth buffer (at one resolution). Second, render the scene \nfrom the eye view (at the same or different resolution). For each point, test the visibility of \nthat point by projecting it into the light view and comparing its depth to the one found in \nthe first pass. If the eye-view depth is greater, then something must be between the light \nand the point being tested, and therefore it must be in shadow. This algorithm fits entirely \nwithin conventional rendering pipelines [Williams78]. \nThe problem with this approach is aliasing: The resolution of the light-view plane will never \nprecisely match the sampling frequency when projecting from the eye view, resulting in \nvisual artifacts. As described in the previous gem, an irregular Z-buffer stores precise eye-\nview positions in the light-view grid, enabling accurate shadow determination from the eye \nview [Johnson04, Johnson09, Galoppo10]. Shadow mapping with an irregular Z-buffer is a \nmulti-step process involving three rendering passes interleaved with two non-rendering \nsteps. \n",
      "content_length": 3455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": " \n \n1.  Render the scene from the eye view, depth only. \n2.  Transform the eye-view points to light view. \na. For each point, atomic increment the corresponding pixel in the light-view plane. \n(A bigger plane improves parallelism at the cost of memory.) \nb. Parallel prefix sum the indices, resulting in a mapping from each eye-view \nreceiver to a light-view pixel. \nc. Scatter the light-view values into a light-view-friendly 1D structure. Indices into \nthe 1D structure are stored in the light-view plane. \n3.  Render the scene from the light view. Instead of executing a traditional pixel shader, \ntest the triangle bounds against the points in the data structure (referring to the indices \nfrom Step 2c). Points that are inside and behind the triangle are in shadow. Set a bit in \nthe data structure marking this point as in shadow (occluded). \n4.  Create a standard shadow map by traversing the data structure and scattering out the \nocclusion value to a traditional 2D image. We call this the ―deswizzle‖ step. \n5.  Render the scene from the eye view again, using the shadow map. \nFor our purposes, this serves as an example of an algorithm that has some rasterization \nsteps intermingled with some algorithmic work one might normally implement in C++. The \nsecond step in particular can result in quite a bit of idle hardware, because it requires a \ndependency chain of parallel and serial workloads. \nAssuming we have a graphics system capable of performing the compute steps of IZB \n(shown in Figure 1.7.1) and executing a pixel shader capable of traversing the IZB data \nstructure, we get the simple dependency graph of tasks shown in Figure 1.7.2. Each stage \nin the graph cannot start until the last thread completes the last task of the prior stage. \nFigure 1.7.1. Building the IZB data structure, effectively a grid of lists. Regular \npoints from the eye view (top left) are transformed into the light view (top \nmiddle). A count of the number of pixels is kept in the light grid (bottom left). A \nprefix sum of the counters in the light grid results in offsets (bottom right) into \nthe 1D data structure (top right). Point data is scattered to the 1D data structure, \nincluding position and occlusion state (initially 0). The number of points in a light-\nview pixel can be determined by subtracting the current pixel value from the value \nto the right. The offset table combined with the 1D data structure forms the grid-\nof-lists IZB data structure. \n",
      "content_length": 2467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": " \n \n \n \nFigure 1.7.2. IZB dependency graph. (Tasks are completed in the order they are \nsubmitted.) \n \nBecause there is almost always a ―long pole‖ that determines the duration of a stage in the \nalgorithm, nearly every thread in the system experiences some amount of idle time. This is \nsuggested by the activity bars in Figure 1.7.2. (Imagine these correspond to the execution \ntime on a system with four threads.) In the remainder of the gem, we will describe how we \ncan reclaim some of those lost cycles on a programmable architecture, such as Larrabee. \n \nOverview of Programmable Graphics Hardware \nAs graphics devices become more general, they can be viewed as many-core compute \ndevices with threads that can communicate amongst themselves (for example, via global \nvariables and atomic instructions). Consider a representative programmable graphics \narchitecture, Larrabee. Larrabee consists of many in-order cores on a single chip, each \nexecuting four threads in round-robin fashion, with an extended instruction set supporting \n16-element vectors [Seiler08]. The cores are connected by a very high-bandwidth ring that \nmaintains cache coherency. Several hardware texture samplers are distributed around the \nring, as well as connections to GDDR. Traditional graphics APIs (DirectX, OpenGL) could be \nimplemented on Larrabee as a typical process running within a relatively conventional \noperating system [Abrash09a]. \n",
      "content_length": 1431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": " \n \nA Larrabee architecture device could be an add-in adapter discrete from the traditional host \nCPU, it could be on-chip or on-die with the host CPU, or it could be the only processor in the \nsystem. For the purposes of this gem, we ignore the transport mechanism that enables our \nprograms to run on the device, but it is important to realize that the techniques and code \nthat follow are designed to execute directly on an architecture such as Larrabee. \nImplementing an efficient rasterizer within a many-threaded, 16-wide SIMD platform is \nbeyond the scope of this chapter, but we can summarize it here. Maximizing parallelism is \nthe main design goal. (Keeping the cores busy will be a recurring theme.) The approach \ndescribed by Abrash [Abrash09b] is to use a binning architecture where the bin dimensions \nare chosen such that the data accessed by each core (the depth buffer format and the pixel \nformat) does not exceed the cache size of the core (256 KB L2, 32 KB L1 in the current \nLarrabee architecture). Done properly, operations such as depth tests would require no off-\ncore bandwidth (neither ring nor GDDR accesses)—unless, of course, something wants to \naccess that depth buffer later, which merely requires a one-time write at the end. \nRemember that textures are accessed via hardware texture samplers, which have their own \ncaches. Despite the attention to bandwidth, the primary motivation for binning is to produce \na lot of independent work that can be executed in parallel. \nA programmable graphics device, such as a software rasterizer, would build a dependency \ngraph of rendering tasks to complete (which we will call a render graph). We can expect \nrendering tasks to be roughly divided between front end (for example, vertex \nprocessing/binning) and back end (rasterization/pixel shading). A dependency graph \nenables independent tasks to run concurrently; for example, if a core completes pixel \nshading within a bin, it can work on a different bin or begin vertex shading for the next \nframe. Dependencies can be defined in terms of resources—for example, a render task may \nsignal that it has completed writing to a render target resource (write dependency) or wait \nto start until a shadow map resource (texture) is ready for use (read dependency). The task \naffected (front end or back end) is a function of whether the resource is bound to the vertex \nshader or pixel shader. \nCommands can be inserted within the graph to be executed when a resource is ready—for \nexample, CopySubResource of a render target. We can also create nodes in the graph \nthat are signaled by outside events, such as direct memory access (DMA) transfer \ncompletions. We will discuss dependency graphs in more detail in the subsequent sections. \n \nOverview of Task-Based Parallelism \nNon-rendering algorithms hoping to effectively use many-core architectures require an \nefficient tasking system, such as Cilk [Blumofe95]. Such a task system leverages a thread \npool (where the number of software threads is less than or equal to the number of hardware \nthreads) to avoid operating system overhead from switching between threads. A good \ntasking system also provides the following features: \n \nThe ability to create individual tasks or sets of tasks (task sets). A task set calls the \nsame task function a user-defined number of times (in parallel). \n \nTasks and task sets may depend upon each other. Specifically, a task or task set will \nnot start until the tasks or task sets it depends on have completed. \n \nTasks and task sets may also depend upon user-defined events. Events can be \nsignaled by simply calling a NotifyEvent API. \n \nAn efficient work-stealing scheduler to automatically spawn and load-balance among \nindependent tasks. \nFollowing is an example of what such a task API might look like. Tasks call a user function \nwith user data. Task sets call a user function with user data and a number indicating which \ninstance this is (0..numTasks-1). \n",
      "content_length": 3977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": " \n \n// create an individual work item \ntypedef void (* const TaskFunction) \n    (void* in_pTaskFunctionArg); \nSyncObject CreateTask( \n    TaskFunction in_pTaskFunc, void* in_pData, \n    SyncObject* in_pDependencies, \n    int in_numDependencies); \n \n// create work items that do a subset of work \ntypedef void (* const TaskSetFunction) \n    (void* in_pTaskSetFunctionArg, \n    int in_taskIndex, int in_taskSetSize); \nSyncObject CreateTaskSet( \n    int in_numTasksInSet, \n    TaskSetFunction in_pTaskFunc, void* in_pData, \n    SyncObject* in_pDependencies, \n    int in_numDependencies); \n \nSyncObject CreateEvent(bool initialState); \n \nSince the task system itself is software, common-sense performance heuristics apply: The \namount of work done in the task should be sufficient to compensate for the overhead of the \ntasking system, which includes the function call into the task as well as some amount of \ncommunication to synchronize with internal task queues. For example, to perform an \noperation across a 1024×1024 image, don‘t create one million tasks. Instead, create a \nmultiple of the number of hardware threads in the system. For a system with 100 hardware \nthreads, 400 or 500 equally sized tasks would give the task system some opportunity to \ngreedily load balance, while each task of approximately 2,000 pixels would give good \nopportunities for prefetching and loop unrolling. (A Larrabee optimized routine would \noperate on 16 pixels at a time, hence a loop of only about 100 iterations.) Our experiments \non desktop x86 machines show that tasks of a few thousand clocks each achieve 90 percent \nor better overall efficiency. \nEfficient graphics processing requires thread affinity knowledge. That is, the rasterizer \nassigns threads to cores with the expectation that those threads will share render target \ndata in their cache. Non-rendering tasks typically function more opportunistically, executing \nwhenever a thread becomes idle. Hence, we design our tasks to be independent of the \nthreads they may execute on. Since the hardware is programmable, finer control is possible \nbut adds complexity. \nEven with equal-sized tasks, in a machine with that many threads, contention for resources \n(caches and GDDR bandwidth) will cause tasks to have variable durations. For very irregular \nworkloads, such as irregular Z-buffer, optimal performance may require thousands of \ntasks—it all depends on the algorithm. \n \nCombining Render Graphs and Task Graphs \nThe ability to create non-rendering task sets that depend on, or are dependencies of, \nrendering tasks is the key to achieving maximum hardware utilization for these mixed-\nusage algorithms. To do this, we need a way to interact with the rendering dependency \ngraph from user code. Following is a method to inject a non-rendering task into the render \ngraph, referring to the current render context (analogous to a DirectX or OpenGL context), \n",
      "content_length": 2915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": " \n \na function to call when the dependencies are met, user data to pass to the function, and a \nlist of all the resource dependencies. \nvoid CreateRenderTask(in_pRenderContext, \n    in_pUserFunc, in_pUserData, \n    in_pReadDependencies, in_numReadDependencies, \n    in_pWriteDependencies, in_numWriteDependencies, \n    out_pRenderTask); \n \nWe then need a way to notify the render system that the render task and its read and write \ndependencies, as declared above, are complete and available. \nvoid NotifyRenderTaskComplete(in_pRenderTask); \n \nNow we have a way for tasks created using our task system to define dependencies with \nrendering tasks and for rendering tasks to very finely interact with our task system. For \nexample, if a render pass has pixel shaders bound to a resource declared as a write \ndependency of a user task, the front end of the render pass can start (transform/ bin), but \nthe back end cannot (rasterize/pixel shade). \nWe need a little helper glue to efficiently communicate between ―render‖ work and the tasks \ncreated for our ―client‖ work. Following, we show how an event that waits on a task set can \ncall NotifyRenderTaskComplete() to enable dependent render work. We also show \nhow render work can cause a callback declared in CreateRenderTask() to signal an \nevent, thereby starting dependent client work. \nOn the left side of Figure 1.7.3, we show how a render pass can be made to wait on client \nwork. First, create a task set that does some client work, such as builds a data structure. \nNext, create a render task with the data structure (resource) written to by the task set as a \nwrite dependency, no read dependencies, and no callback. Then, create a render pass with \nthe data structure resource as a read dependency. Finally, create a task that depends on \nthe task set that will call NotifyRenderTaskComplete(). The render pass cannot start \nuntil the task set is complete. \nFigure 1.7.3. Detail of connecting client task sets to render passes via render \ntasks. \n",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": " \n \n \n \nOn the right side of Figure 1.7.3, we show how client work can be made dependent on a \nrender pass. First, create an event that we will signal and a task set that depends on the \nevent. (It would do work on the render target.) Next, create the render pass, which in this \ncase writes to a render target (write dependency). Finally, create a render task with the \nrender target as a read dependency and a callback that will set the event. The task set \ncannot start until the render pass is complete. \n \nCombined Dependency Graph \nTo reduce the idle time, we need to build a complete dependency graph including both \nrendering and non-rendering tasks. Below, we work with the following constraints: \n \nTasks or task sets must have their dependencies described at creation time. \n \nTasks or tasks sets can depend on tasks, task sets, or events. \nThese constraints force us to work from the end of the algorithm backwards. Since a task \nwill start immediately if it has no dependencies, we create events in a not-signaled state to \nact as gates for the task sets. \n1. Create a build data structure event (not signaled). \n",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": " \n \n2. Create a build data structure task set that depends on event (1). \n3. Create a deswizzle event (not signaled). \n4. Create a deswizzle task set that depends on event (3). \n5. Create a light-view render pass where: \na. Rasterization depends on resource from task set (2). \nb. Render target resource complete signals event (3). \n6. Create a final eye-view render pass where rasterization depends on the shadow map \nresource from task set (4). \n7. Create a depth-only eye-view render pass that signals event (1) when its render \ntarget resource is complete. \nAs soon as we complete the seventh step, creating the depth-only eye-view render with no \ndependencies, the whole algorithm will fall into place. As shown in Figure 1.7.4, when the \ndepth-only pass (7) completes, it signals the build event (1), which enables the build task \nset (2) to start. Completion of the build task set (2) enables the light-view rasterization (5) \nto start. (Transform and binning should already be complete.) When the light-view render \n(5) completes, it signals the deswizzle event (3), which enables the deswizzle task set (4) to \nstart. When the deswizzle task set (4) completes, it enables the final eye-view rasterization \n(6) to start. (Transform and binning should already be complete.) \nFigure 1.7.4. Order of creation of events, task sets, and render graph nodes for \noverlapped execution of IZB. \n \n \n \n \n",
      "content_length": 1402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": " \n \nIdle-Free Irregular Z-Buffer Shadows \nFigure 1.7.5 shows the naïve linear dependency graph of the IZB algorithm discussed \nearlier, showing the render stages expanded into front end (transform + binning) and back \nend (rasterization + pixel shading) for a total of eight stages, or task sets. \nFigure 1.7.5. IZB dependency graph with render stages expanded into front and \nback end. \n \nFigure 1.7.6 shows how threads that would otherwise have become idle can instead work on \nnon-dependent front-end rendering tasks if we start all three render passes immediately. \nThis overlapped execution is especially helpful for improving the performance of our \nirregular shadow-mapping tasks, allowing us (in this example) to fully hide the cost of the \nfront-end rendering tasks. Another way to interpret this is that the compute part of irregular \nshadow mapping is essentially free when overlapped with rendering. \nFigure 1.7.6. IZB with dependency graph integrated with flexible rendering \npipeline. Xform/bin tasks can complete as threads become available from non-\nrendering tasks, filling in gaps in execution. \n \nThis demonstrates another advantage of programmable hardware: Maximum performance is \nachieved by enabling flexible hardware to execute whatever tasks are available, rather than \nby partitioning the hardware into dedicated islands of computation. Compare this to the \nearly days of graphics devices with dedicated pixel shader and vertex shader hardware: \nWhen there was more vertex work than pixel work, the idle pixel shading hardware could \nnot be reconfigured to help out. In modern architectures, graphics processors dynamically \nload balance across all execution units. On a programmable architecture such as Larrabee, \nthis load balancing can be controlled by the programmer, enabling the overlap of rendering \nand non-rendering tasks. \n \nConclusion \n",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": " \n \nMaximizing performance on modern, many-core platforms requires identifying independent \nwork to be executed in parallel. Non-graphics workloads leverage a system of dependent \ntasks and task sets to manage parallel computation. On programmable graphics devices \nsuch as Larrabee, a similar system of task dependencies can be used to identify \nindependent and dependent graphics work—for example, binning versus pixel shading. By \nconnecting these graphs, we can further exploit available execution units by exposing more \nopportunities for independent tasks to run concurrently. \n \nReferences \n[Abrash09a] Abrash, Michael. ―A First Look at the Larrabee New Instructions (LRBni).‖ Dr. \nDobb‘s. 1 April 2009. <http://www.ddj.com/hpc-high-performance-\ncomputing/216402188>. \n[Abrash09b] Abrash, Michael. ―Rasterization on Larrabee: A First Look at the Larrabee New \nInstructions (LRBni) in Action.‖ Intel. March 2009. <http://software.intel.com/file/15542>. \n[Blumofe95] Blumofe, Robert D., Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. \nLeiserson, Keith H. Randall, and Yuli Zhou. ―Cilk: An Efficient Multithreaded Runtime \nSystem.‖ Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of \nParallel Programming (July 1995): 207–216. \n[Galoppo10] Galoppo, Nico. ―Eye-view Pixel Anti-Aliasing for Irregular Shadow Mapping.‖ \nGame Programming Gems 8. Ed. Adam Lake. Boston: Charles River Media, 2010. \n[Johnson04] Johnson, Gregory S., William R. Mark, and Christopher A. Burns. ―The Irregular \nZ-Buffer and its Application to Shadow Mapping.‖ April 2004. The University of Texas at \nAustin. n.d. <http://www.cs.utexas.edu/ftp/pub/techreports/tr04-09.pdf>. \n[Johnson09] Johnson, Gregory S., Allen Hux, Christopher A. Burns, Warren A. Hunt, William \nR. Mark, and Stephen Junkins. ―Soft Irregular Shadow Mapping: Fast, High-Quality, and \nRobust Soft Shadows.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames. (2009): 57–66. ACM Portal. \n[Seiler08] Seiler, Larry, et al. ―Larrabee: A Many Core x86 Architecture for Visual \nComputing.‖ ACM Transactions on Graphics 27.3 (Aug. 2008): n.p. ACM Portal. \n[Williams78] Williams, Lance. ―Casting Curved Shadows on Curved Surfaces.‖ ACM \nSIGGRAPH Computer Graphics 12.3 (Aug. 1978): 270–274. ACM Portal. \n \n1.8. Techniques for Effective Vertex and Fragment Shading on the \nSPUs \nSteven Tovey, Bizarre Creations Ltd. \nsteven.tovey@bizarrecreations.com \nWhen the Cell Broadband Engine was designed, Sony and the other corporations in the STI \ncoalition always had one eye on the Cell‘s ability to support a GPU in its processing activities \n[Shippy09]. The Cell has been with us for three years now, and like any new piece of \nhardware, it has taken time for developers to understand the best ways of pushing the \nhardware to its limits. The likes of Mike Acton and the Insomniac Games Technology Team \n",
      "content_length": 2891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": " \n \nhave been instrumental in pushing general development and coding strategies for the Cell \nforward, but there has been little discussion about ways that the SPUs can support a GPU in \nits processing activities specifically. This chapter aims to introduce fundamental techniques \nthat can be employed when developing code for the CBE that will allow it to aid the GPU in \nperforming rendering tasks. \nThe CBE as Part of a Real-World System \nUnderstanding Cell‘s place in a real-world system is useful to our discussion, and, as such, \nwe will use Sony‘s PlayStation 3 as our case study. PlayStation 3 contains the Cell \nBroadband Engine, which was developed jointly by Sony Computer Entertainment, Toshiba \nInc., and IBM Corp. [Shippy09, Möller08, IBM08]. The Cell forms part of the overall \narchitecture of the console along with the Reality Synthesizer, RSX, and two types of \nmemory. Figure 1.8.1 shows a high-level view of the architecture. \nFigure 1.8.1. The PlayStation 3 architecture (illustration modeled after [Möller08, \nPerthuis06]). \n \n \nThe Cell contains two distinctly different types of processor: the PowerPC Processing \nElement (PPE) and the Synergistic Processing Element (SPE). The PPE is essentially the \nbrains of the chip [Shippy09] and is capable of running an operating system in addition to \ncoordinating the processing activities of its counterpart processing elements, the SPEs. \nInside PlayStation 3, there are eight SPEs. However, to increase chip yield, one is locked \nout, and Sony reserves another for their operating system, leaving a total of six SPEs \navailable for application programmers. All processing elements in the Cell are connected by \na token-ring bus, as shown in Figure 1.8.2. \nFigure 1.8.2. The Cell Broadband Engine (modeled after [IBM08]). \n \n \nBecause the SPEs are the main focus of this chapter, they are discussed in much greater \ndetail in the forthcoming sections. \n",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": " \n \n \nThe SPEs \nEach Synergistic Processing Element is composed of two major components: the Synergistic \nProcessing Unit (SPU) and the Memory Flow Controller (MFC). \nThe SPU \nDetailed knowledge of the SPU instruction set and internal execution model are critical to \nachieving peak performance on the PlayStation 3. In the following sections, we will highlight \nsome important facets of this unique processor. \nThe Synergistic Execution Unit and SPU ISA \nThe Synergistic Execution Unit (SXU), part of the SPU, is responsible for the execution of \ninstructions. Inside the SXU are two pipelines: the odd pipeline and the even pipeline. \nInstructions are issued to exactly one of these pipelines, depending on the group the issued \ninstruction falls into (see Table 1.8.1). The SXU supports the dual issue of instructions (one \nfrom each pipeline) if and only if a very strict set of requirements is met. We will discuss \nthese requirements in detail later. \nTable 1.8.1. A List of Instruction Groups Together with Their Associated Execution \nPipes and Latencies \nInstruction Group \nPipeline \nLatency \n(Cycles) \nIssue \n(Cycles) \nSingle precision floating-point operations \nEVEN \n6 \n1 \nDouble precision floating-point operations \nEVEN \n7 \n6 \nInteger multiplies, integer/float conversions, and \ninterpolation \nEVEN \n7 \n1 \nImmediate loads, logical operations, integer \naddition/subtraction, carry/borrow generate \nEVEN \n2 \n1 \nElement-wise rotates and shifts, special byte \noperations \nEVEN \n4 \n1 \nLoads and stores, branch hints, channel operations \nODD \n6 \n1 \nShuffle bytes, qword rotates and shifts, estimates, \ngather, selection mask formation and branches \nODD \n4 \n1 \n \nThe SPU has a particularly large register file to facilitate the execution of pipelined, unrolled \ncode without the need for excessive register spilling. Unlike its counterpart, the PPE, the \nregister file of the SPU is unified. That is, floating-point, integer, and vector operations act \non the same registers without having to move through memory. As the SPU is a vector \n",
      "content_length": 2044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": " \n \nprocessing unit at its heart, its Instruction Set Architecture (ISA) is designed specifically for \nvector processing [IBM08a]. All 128 of the SPU‘s registers are 16 bytes in size, allowing for \nup to four 32-bit floating-point values or eight 16-bit integers to be processed with each \ninstruction. \nWhile a full analysis of the SPU‘s ISA is beyond the scope of this gem, there are a number \nof instructions worth discussing in greater detail that are particularly important for efficient \nprogramming of the SPU. The first of these instructions is selb, or ―select bits.‖ The selb \ninstruction performs branchless selection on a bitwise basis and takes the form selb rt, \nra, rb, rm. For each bit of a quadword, this instruction uses the mask register (rm) to \ndetermine which bits of the source registers (ra and rb) should be placed in the \ncorresponding bits of the target register (rt). Comparison instructions all return a \nquadword selection mask that can be used with selb[1]. \n[1] The fsmbi instruction is also very useful for efficiently constructing a selection mask for use with \nselb. \nThe shuffle bytes instruction, shufb, is the key instruction in data manipulation on the \nSPU. The shufb instruction takes four operands, all of which are registers. The first \noperand, rt, is the target register. The next two operands, ra and rb, are the two \nquadwords that will be manipulated by the quadword pattern from the fourth operand, rp. \nThe manipulations controlled by this fourth operand, known as the shuffle pattern, are \nparticularly interesting. \nA shuffle pattern is a quadword value that works on a byte level. Each of the 16 bytes in the \nquadword controls the contents of the corresponding byte in the target register. For \nexample, the 0th byte of the pattern quadword controls the value that will ultimately be \nplaced into the 0th byte of the target register, the 1th byte controls the value of the 1th \nbyte placed into the target register, and so on, for all 16 bytes of the quad word. Listing \n1.8.1 provides an example shuffle pattern. \nListing 1.8.1. An example shuffle pattern \nconst vector unsigned char _example1 = \n{ 0x00, 0x11, 0x02, 0x13, \n  0x04, 0x15, 0x06, 0x17, \n  0x08, 0x19, 0x0a, 0x1b, \n  0x0c, 0x1d, 0x0e, 0x1f }; \n \nThe above pattern performs a perfect shuffle, but on a byte level. (The term ―perfect \nshuffle‖ typically refers to the interleaving of bits from two words.) The lower 4 bits of each \nbyte can essentially be thought of as an index into the bytes of the first or second operand \nquadword. Similarly, the upper 4 bits can be thought of as an index into the registers \nreferred to in the instruction‘s operands. Since there are only two, we need only concern \nourselves with the LSB of this 4-bit group—in other words, 0x0x (where x denotes some \nother value of the lower 4 bits of the byte) would index into the contents of the ra register, \nand 0x1x would access the second. It is worth noting that there are special case values that \ncan be used to load constants with shufb; an interested reader can refer to [IBM08a] for \ndetails. A further example in Listing 1.8.2 will aid us in our understanding. \nListing 1.8.2. An example of using shufb \n",
      "content_length": 3209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": " \n \nconst vector unsigned char _example2 = \n{ 0x00, 0x01, 0x02, 0x03, \n  0x14, 0x15, 0x16, 0x17, \n  0x08, 0x09, 0x0a, 0x0b, \n  0x1c, 0x1d, 0x1e, 0x1f }; \n \nqword pattern = (const qword)_example2; \nqword ra = si_ilhu(0x3f80); // ra contains: 1.0f, 1.0f, 1.0f, \n1.0f \nqword rb = si_ilhu(0x4000); // rb contains: 2.0f, 2.0f, 2.0f, \n2.0f \n \n// result contains: 1.0f, 2.0f, 1.0f, 2.0f \nqword result = si_shufb(ra, rb, pattern); \n \nIn many programs, simply inlining of shuffle patterns for data manipulation requirements \nwill suffice, but since the terminal operand to shufb is simply a register, there is nothing to \nstop you from computing the patterns dynamically in your program or from forming them \nwith the constant formation instructions (as should be preferred when lower latency can be \nachieved than the 6-cycle load from the local store). As it turns out, dynamic shuffle pattern \ncomputation is actually critical to performing unaligned loads from the local store in a \nvaguely efficient manner, as we shall see later. In-depth details of the SPU ISA can be \nfound in [IBM08a]. \nLocal Store and Memory Flow Controller \nAs previously mentioned, each of the SPUs in the Cell is individually endowed with its own \nmemory, known as its local store. The local store is (at least on current implementations of \nthe CBE) 256 KB in size and can essentially be thought of as an L1 cache for the Synergistic \nExecution Unit. Data can be copied into and out of the local store by way of the DMA engine \nin the MFC, which resides on each SPE and acts asynchronously of the SXU. Loads and \nstores to and from the local store are always 16-byte aligned and sized. Hence, processing \ndata smaller than 16 bytes requires use of a less-than-efficient load-modify-store pattern. \nAccesses to the local store are arbitrated by the SPU Store and Load unit (SLS) based on a \npriority; the DMA engine always has priority over the SXU for local store accesses. \nEach DMA is part of a programmer-specified tag group. This provides a mechanism for a \nprogrammer to poll the state of the MFC to find out if a specific DMA has completed. A tag \ngroup is able to contain multiple DMAs. The tag group is denoted by a 5-bit value internally, \nand, as such, the MFC supports 32 distinct tag groups [Bader07]. The DMA queue (DMAQ) \nis 16 entries deep in current implementations of the CBE. \n \nData Management \nIn many ways, the choice of data structure is more important than the efficiency of the \noperations that must be performed on it. In the following sections, we will describe a variety \nof data management strategies and their tradeoffs in the context of the SPU. \nMulti-Buffering \n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": " \n \nAll graphics programmers will be familiar with the concept of a double buffer. The multi-\nbuffer is simply a term that generalizes the concept to an arbitrary number of buffers. In \nmany cases two buffers will be sufficient, but sometimes a third buffer will be required to \neffectively hide the latency of transfers to and from the effective address space. Figure \n1.8.3 shows the concept of multi-buffering. \nFigure 1.8.3. Multi-buffering data to hide latency (modeled after [Bader07]). \n \n \nBader suggests that each buffer should use a separate tag group in order to prevent \nunnecessary stalling of the SPU waiting for data that will be processed sometime in the \nfuture. Barriers and fences should be used to order DMAs within a tag group and the DMA \nqueue, respectively [Bader07]. Multi-buffering can yield significant performance increases, \nbut it does have a downside. Because the buffers are resident in the local store, it does \nmean that SPE programs must be careful not to exceed the 256-KB limit. \nUsing a reasonable size for each of the buffers in your multi-buffer (about 16 KB) in order to \nallow the SPU to process several vertices or pixels before requiring more data from the main \naddress space is a fine strategy. However, the pointer wrangling can become a little \ncomplicated if one‘s goal is to support a list of arbitrarily sized (and hence aligned) vertex \nformats. Conversely, alignments do tend to be a little more favorable and can be easily \ncontrolled by carefully selecting a reasonably sized unit of work when processing pixels. \nStructure-of-Arrays versus Array-of-Structures \nThe design of data is paramount when hoping to write performant software for the SPU. \nSince the SPU is a SIMD vector processor, concepts familiar to those who have programmed \nwith other vector ISAs, such as SSE on Intel chips, Altivec on PowerPC chips, or even the \nVU on the PlayStation 2, are immediately transferable to the SPU. One such concept is \nparallel array data layout, better known as Structure-of-Arrays (SOA). By laying data out in \na format that is the transpose of its natural layout (Array-of-Structures), as can be seen in \nFigure 1.8.4, a programmer is often able to produce much more efficient code (most \nnotably in those cases where vectorized data is interacting with scalar data). \n",
      "content_length": 2325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": " \n \nFigure 1.8.4. An Array-of-Structures layout on the left is transposed into a \nStructure-of-Arrays layout (illustration modeled after [Tovey10]). \n \n \nThe benefits of using an SOA layout are substantial in a lot of common cases. Listing 1.8.3 \nillustrates this by way of computing the squared length of a vector. \nListing 1.8.3. Two versions of a function to calculate the squared length of a \nvector. The first assumes Array-of-Structures data layout, and the second \nStructure-of-Arrays layout. \n// Version 1: AOS mode - 1 vector, ~18 cycles. \n      qword dot_xx                  = si_fm(v, v); \n      qword dot_xx_r4               = si_rotqbyi(dot_xx, 4); \n            dot_xx                  = si_fa(dot_xx, dot_xx_r4); \n      qword dot_xx_r8               = si_rotqbyi(dot_xx, 8); \n            dot_xx                  = si_fa(dot_xx, dot_xx_r8); \n      return si_to_float(dot_xx); \n \n      // Version 2: SOA mode - 4 vectors, ~8 cycles. \n      qword dot_x                   = si_fm(x, x); \n      qword dot_y                   = si_fma(y, y, dot_x); \n      qword dot_z                   = si_fma(z, z, dot_y); \n      return dot_z; \n \nBranch-Free DMAs \nThe cost of badly predicted branches on the SPU is quite significant. Given that the SPU \ndoes not contain any dedicated branch prediction hardware[2], the burden of responsibility \nfalls squarely on the shoulders of the programmer (or in the majority of cases, the \ncompiler). There are built-in language extensions available in most SPU compilers that allow \nthe programmer to supply branch hints, but such things assume that you have sufficient \ntime in order to make the prediction (that is, more than 11 cycles) and that the branch is \nintrinsically predictable, which may not be the case. It is therefore recommended that \nprogrammers avoid branches entirely [Acton08]. Others have discussed this topic at length \n[Acton08, Kapoulkine09], so I will refrain from doing so here; however, I do wish to touch \nupon one common case where branch avoidance is not entirely obvious but is entirely \ntrivial. \n[2] The SXU adopts the default prediction strategy that all branches are not taken. \nIBM‘s SDK provides several MFC functions to initiate DMA without resorting to the manual \nwriting of registers[3]. An unfortunate side effect of such functions is that they seem to \nactively encourage code such as that presented in Listing 1.8.4. \n",
      "content_length": 2399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": " \n \n[3] SPU-initiated DMAs are performed by the writing of special-purpose registers in the MFC using the \nwrch instruction. There are six such registers that must be written in order to initiate a DMA. These \nmay be written arbitrarily as long as the command register is written terminally [IBM08]. \nListing 1.8.4. All-too-often encountered code to avoid issuing unwanted DMAs \nif(si_to_uint(counter)> 0) ) \n       mfc_put(si_to_uint(lsa), \n                     si_to_uint(ea), \n                     si_to_uint(size), \n                     si_to_uint(tag)); \n \nHowever, a little knowledge of the MFC can help avoid the branch in this case. The MFC \ncontains the DMA queue (DMAQ). This queue contains SPU-initiated commands to the MFC‘s \nDMA engine. Similar to a CPU or GPU, the MFC supports the concept of a NOP. A NOP is an \noperation that can be inserted into the DMAQ but doesn‘t result in any data being \ntransferred. A NOP for the MFC is denoted by any DMA command being written that has \nzero size. The resulting code looks something like Listing 1.8.5. \nListing 1.8.5. Branch-free issue of DMA \nqword cmp_mask    = si_cgti(counter, 0x0); \nqword cmp         = si_andi(cmp_mask, 0x1); // bottom bit only. \nqword dma_size    = si_mpy(size, cmp);    // size < 2^16 \n                    mfc_put(si_to_uint(lsa), \n                                  si_to_uint(ea), \n                                  si_to_uint(dma_size), \n                                  si_to_uint(tag)); \n \nUnfortunately, the hardware is not smart enough to discard zero-sized DMA commands \nimmediately upon the command register being written, and these commands are inserted \ninto the 16-entry DMAQ for processing. The entry into the queue is immediately discarded \nwhen the DMA engine attempts to process this element of the queue. However, this causes \na subtle downside to the employment of this technique for branch avoidance. SPE programs \nthat issue a lot of DMAs can quickly back up the DMAQ, and issuing a zero-sized DMA can \nstall the SPU while it flushes the entire DMAQ. Luckily, this state of affairs can be almost \nentirely mitigated by a well-designed SPE program, which issues fewer, but larger DMAs. \n \nVertex/Geometry Shading \nThe SPUs can also lend a hand in various vertex processing tasks and, because of their \ngeneral nature, can help overcome some of the shortcomings of the GPU programming \nmodel. In Blur, we were able to use the SPU to deal with awkward vertex sizes and to \noptimize the vehicle damage system. \nHandling Strange Alignments When Multi-Buffering \n",
      "content_length": 2561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": " \n \nVertex data comes in all shapes and sizes, and, as a result, multi-buffering this type of data \npresents some challenges. When vertex buffers are created, contiguous vertices are packed \ntightly together in the buffer to both save memory and improve the performance of the pre-\ntransform cache on the GPU. This presents an SPU programmer with a challenge when \nattempting to process buffers whose per-vertex alignment may not be a multiple of 16 \nbytes. This is a problem for two reasons. First, the DMA engine in the MFC transfers 1, 2, 4, \n8, or multiples of 16 bytes, meaning that we must be careful not to overwrite parts of the \nbuffer that we do not mean to modify. Second, loads and stores performed by the SXU itself \nare always 16-byte aligned [IBM08]. \nThere are a lot of cases where a single vertex will straddle the boundary of two multi-\nbuffers, due to vertex structures that have alignments that are sub-optimal from an SPU \nprocessing point of view. The best way of coding around this problem is to simply copy the \nend of a multi-buffer to its nearest 16-byte boundary into the start of the second multi-\nbuffer and offset the pointer to the element you are currently processing. This means that \nwhen the second multi-buffer is transferred back to the main address space, it will not \ncorrupt the vertices you had previously processed and transferred out of the first multi-\nbuffer, as shown in Figure 1.8.5. Listing 1.8.6 contains code demonstrating how to handle \nunaligned loads from the local store. \nFigure 1.8.5. Avoid buffer corruption by copying a small chunk from the end of one \nmulti-buffer into the start of another. \n \n \nCase Study: Car Damage in Blur \nThe car damage system in Blur works by manipulating a lattice of nodes that roughly \nrepresent the volume of the car. The GPU implementation makes use of a volume texture \ncontaining vectors representing the offset of these nodes‘ positions from their original \npositions. This is then sampled based on the position of a vertex being processed relative to \na volume that loosely represents the car in order to calculate position and normal offsets \n(see Figure 1.8.6). The texture is updated each time impacts are applied to the lattice, or \nwhen the car is repaired. \nFigure 1.8.6. Position and normal offsets are applied to each vertex based on \ndeltas stored in a volume texture. \n",
      "content_length": 2372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": " \n \n \n \nThe GPU performs the deformation every frame because the damage is stateless and a \nfunction of the volume texture and the undamaged vertex data. Given the amount of work \ninvolved and the additional performance hit from sampling textures in the vertex unit, the \nperformance of rendering cars in Blur was heavily vertex limited. This was something we \nwanted to tackle, and the SPUs were useful in doing so. Porting the entire vertex shader to \nthe SPU was not practical given the timeframe and memory budgets, so instead we focused \non moving just the damage calculations to the SPUs. This meant that the car damage vertex \nprocessing would only occur when damage needed to be inflicted on the car (instead of \nevery frame with the equivalent GPU implementation), and it would greatly reduce the \ncomplexity of the vertex shader running on the GPU. \nThe damage offsets are a function of the vertex‘s position and the state of the node lattice. \nGiven the need for original position, we must transfer the vertex data for the cars to the \nlocal store via DMA and read the position data corresponding to each vertex. This is done \nusing a multi-buffering strategy. Because different components of the car utilize different \nmaterials (and hence have different vertex formats), we were also forced to a variety of \nvertex alignments as described earlier. With the vertex data of the car in the SPU local \nstore, we are able to calculate a position and normal offset for each vertex and write these \nout to a separate vertex buffer. Each of these values is stored as a float4, which means \nthe additional vertex stream has a stride of 32 bytes per vertex. An astute GPU programmer \nwill notice the potential to pack this data into fewer bits to improve cache utilization. This is \nundesirable, however. The data in its 32-bytes-per-vertex form is ideal for the DMA engine \nbecause the MFC natively works in 16-byte chunks, meaning from the point of view of other \nprocessing elements (in our case, the GPU), a given vertex is either deformed or it is not. \nThis is one of the tradeoffs made to mitigate the use of a double buffer. Color Plate 5 has a \nscreen-shot of this technique. \nTo GPU Types and Back Again \nFor the most part, GPUs do their best to support common type formats found in CPUs. The \nIEEE754 floating-point format is (for better or worse) the de facto floating-point standard on \npretty much all modern hardware that supports floating point[4]. \n[4] Ironically, the SPUs do not offer full IEEE754 support, but it‘s very close. \nHowever, in addition to the IEEE754 standard 32-bit floats and 64-bit doubles, most \nshading languages offer a 16-bit counterpart known as half. The format of the half is not \ndefined by any standard, and, as such, chip designers are free to implement their own \nfloating-point formats for this data type on their GPUs. Fortunately, almost all GPU vendors \nhave adopted the half format formalized by Industrial Light & Magic for their OpenEXR HDR \nfile format [ILM09]. This format uses a single bit to denote the sign of the number, 5 bits \nfor the exponent, and the remaining 10 bits for its mantissa or significand. \nSince the half type is regrettably absent from the C98 and C++ standards, it falls to the \nprogrammer to write routines to convert to other data types. Acton has made available an \nentirely branch-free version of these conversion functions at [Acton06]. For the general \ncase, you would be hard-pressed to better Acton‘s code (assuming you don‘t have the \nmemory for a lookup table as in [ILM09]). However, in many constrained cases, we have \n",
      "content_length": 3613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": " \n \nknowledge about our data that allows us to omit support for floating-point special cases that \nrequire heavyweight conversion logic (NaNs and de-normalized numbers). Listing 1.8.6 \ncontains code to convert between an unaligned half4 and float4 but omits support for \nNaNs. This is an optimization that was employed in Blur‘s damage system. The inverse of \nthis function is left as an exercise for the reader. \nListing 1.8.6. Code to convert an unaligned half4 to a qword \nstatic inline const qword ld_float16_4(void * __restrict__ addr) \n{ \n     const vector unsigned char _loader = \n     { 0x80, 0x80, 0x00, 0x01, \n       0x80, 0x80, 0x02, 0x03, \n       0x80, 0x80, 0x04, 0x05, \n       0x80, 0x80, 0x06, 0x07 }; \n     const vector unsigned char _shft = \n     { 0x00, 0x01, 0x02, 0x03, \n       0x04, 0x05, 0x06, 0x07, \n       0x08, 0x09, 0x0a, 0x0b, \n       0x0c, 0x0d, 0x0e, 0x0f }; \n \n     qword target           = si_from_ptr(addr); \n     qword val_lo           = si_lqd(target, 0x00); \n     qword val_hi           = si_lqd(target, 0x10); \n     qword sign_bit_mask    = si_ilhu(0x0); \n           sign_bit_mask    = si_iohl(sign_bit_mask, 0x8000); \n     qword mant_bit_mask    = si_ilhu(0x0); \n           mant_bit_mask    = si_iohl(mant_bit_mask, 0x7fff); \n     qword expo_bias        = si_ilhu(0x3800); \n     qword loader           = (const qword)_loader; \n     qword shft             = (const qword)_shft; \n     qword offset           = si_andi(target, 0x0f); \n     qword lo_byte_pat      = si_ilh(0x0303); \n     qword offset_pat       = si_shufb(offset, offset, \nlo_byte_pat); \n     qword mod_shuf         = si_a(shft, offset_pat); \n     qword val              = si_shufb(val_lo, val_hi, \nmod_shuf); \n     qword result           = si_shufb(val, val, loader); // \naligned \n     qword sign_bit         = si_and(result, sign_bit_mask); \n           sign_bit         = si_shli(sign_bit, 0x10); \n     qword significand      = si_and(result, mant_bit_mask); \n           significand      = si_shli(significand, 0xd); \n     qword is_zero_mask     = si_cgti(significand, 0x0); \n           expo_bias        = si_and(is_zero_mask, expo_bias); \n     qword exponent_bias    = si_a(significand, expo_bias); \n     qword final_result     = si_or(exponent_bias, sign_bit); \n     return final_result; \n} \n \n",
      "content_length": 2297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": " \n \nBenefits versus Drawbacks \nProcessing vertex data on the SPUs has a number of advantages; one of the most \nsignificant is that the rigidity of the GPU‘s processing model is largely circumvented as you \nare performing processing on a general-purpose CPU. Access to mesh topology is \nsupported, but one must be careful that these accesses do not introduce unwanted stalls as \nthe data is fetched from the main address space. In addition, since we are using a CPU \ncapable of general-purpose program execution, we are able to employ higher-level \noptimization tactics, such as early outs or faster code paths, which would be tricky or \nimpossible under the rigid processing model adopted by GPUs. The ability to split workloads \nbetween the SPUs and the GPU is also useful in striking the ideal balance for a given \napplication. \nAs with most things in graphics programming, there are some tradeoffs to be made. Vertex \nprocessing on the SPU can in many cases require that vertex buffers are double buffered, \nmeaning a significantly increased memory footprint. The situation is only aggravated if \nthere is a requirement to support multiple instances of the same model. In this case, each \ninstance of the base model may also require a double buffer. This can be mitigated to some \nextent by carefully designing the vertex format to support atomic writes of individual \nelements by the DMA engine, but the practicality of this is highly application-specific and \ncertainly doesn‘t work in the case of instances. Clever use of a ring buffer can also solve this \nproblem to some extent, but it introduces additional problems with SPU/GPU inter-processor \ncommunication. \n \nFragment Shading \nFragment shading in the traditional sense is heavily tied to the output of the GPU‘s \nrasterizer. Arbitrarily ―hooking into‖ the graphics pipeline to have the SPUs perform general-\npurpose fragment shading with current generations of graphics hardware is effectively \nimpossible. However, performing the heavy lifting for certain types of fragment shading that \ndo not necessarily require the use of the rasterizer, or even helping out the GPU with some \npre-processing as in [Swoboda09], is certainly feasible and in our experience has yielded \nsignificant performance benefits in real-world applications [Tovey10]. This section discusses \nsome of the techniques that will help you get the most out of the SPUs when shading \nfragments. \nBatch! Batch! Batch! \nIt might be tempting with initial implementations of pixel processing code on the SPU to \nadopt the approach of video hardware, such as the RSX. RSX processes pixels in groups of \nfour, known as quads [Möller08]. For sufficiently interleavable program code—in other \nwords, program code that contains little dependency between operations that follow one \nanother—this may be a good approach. However, in our experience, larger batches can \nproduce better results with respect to pixel throughput because there is a greater volume of \ninterleavable operations. Too few pixels result in large stalls between dependant operations, \ntime that could be better spent performing pixel shading, while larger batches cause high \nregister pressure and ultimately spilling. Moreover, in many applications that have a fixed \nsetup cost for each processing batch, you are doing more work for little to no extra setup \noverhead. \nSo, what is the upper bound on the number of pixels to process in a single batch of work? \nCan we simply process the entire buffer at once? The answer to this is not obvious and \ndepends on a number of factors, including the complexity of your fragment program and the \nnumber of intermediate values that you have occupying registers at any one time. Typically, \nthe two are inextricably linked. \n",
      "content_length": 3764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": " \n \nAs mentioned earlier, the SXU contains 128 registers, each 16 bytes in size. It is the task of \nthe compiler to multiplex all live variables in your program onto a limited register file[5]. \nWhen there are more live variables than there are registers—in other words, when register \npressure is high—the contents of some or all of the registers (depending on the size of the \nregister file) have to be written back to main memory and restored later. This is known as \nspilling registers. The more pixels one attempts to process in a batch, the higher the \nregister pressure for that function will be, and the likelihood that the compiler will have to \nspill registers back to the stack becomes greater. Spilling registers can become very \nexpensive if done to excess. The optimum batch size is hence the largest number of pixels \nthat one can reasonably process without spilling any registers back to the local store and \nwithout adding expense to the setup code for the batch of pixels. \n[5] The process of mapping multiple live variables onto a limited register file is known as register \ncoloring. Register coloring is a topic in its own right, and we will not cover it in detail here. \nPipeline Balance Is Key! \nAn efficient, well-written program will be limited by the number of instructions issued to the \nprocessor. Those processors with dual-issue capabilities, such as the SPU, have the \npotential to dramatically decrease the number of cycles that a program consumes. Pipeline \nbalance between the odd and even execution pipelines is critical to achieving good \nperformance with SPU programs. We will now discuss the requirements for instruction dual-\nissue and touch briefly on techniques to maximize instruction issue (through dual-issue) for \nthose programmers writing in assembly. \nThe SPU can dual-issue instructions under a very specific set of circumstances. Instructions \nare fetched in pairs from two very small instruction buffers [Bader07], and the following \nmust all be true if dual-issue is to occur: \n \nThe instructions in the fetch group must be capable of dispatch to separate execution \npipelines. \n \nThe alignment of the instructions must be such that the even pipeline instruction \noccupies an even-aligned address in the fetch group, and the odd pipeline in the \nodd-aligned address. \n \nFinally, there must be no dependencies either between the two instructions in the \nfetch group or between any one of the instructions in the fetch group and another \ninstruction currently being executed in either of the pipelines. \nProgrammers writing code with intrinsics rarely need to worry about instruction alignment. \nThe addition of nops and lnops in intrinsic form does not typically help the compiler to \nbetter align your code for dual-issue, and, in many cases, the compiler will do a reasonable \njob of instruction balancing. However, if you‘re programming in assembly language, the use \nof nop (and its odd-pipeline equivalent, lnop) will be useful in ensuring that code is \ncorrectly aligned for dual-issue. Of course, care must be taken not to overdo it and actually \nmake the resulting code slower. A good rule of thumb is never to insert more than two \nnops/lnops. \nCase Study: Light Pre-Pass Rendering in Blur \nLight pre-pass rendering is a variant of deferred shading first introduced by Wolfgang Engel \non his blog [Engel08] and later in [Engel09, Engel09a] at the same time it was derived \nindependently by Balestra et al. for use in Uncharted: Drake‘s Fortune [Balestra08]. The \ntechniques behind light pre-pass rendering are well understood and are discussed elsewhere \n[Engel08, Balestra08, Engel09, Engel09a, Tovey10], so a brief summary will suffice here. \nAs with all deferred rendering, the shading of pixels is decoupled from scene complexity by \nrendering out ―fat‖ frame buffers for use in an image space pass [Deering88, Saito90]. Light \npre-pass rendering differs slightly from traditional deferred shading in that only the data \n",
      "content_length": 3984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": " \n \nrequired for lighting calculations is written to the frame buffer during an initial rendering \npass of the scene. This has several advantages, including a warm Z-buffer and a reduced \nimpact on bandwidth requirements, at the expense of rendering the scene geometry twice. \nBecause one of the main requirements for the new engine written for Blur was that it should \nbe equipped to handle a large number of dynamic lights, the light pre-pass renderer was a \nvery attractive option. After implementing a light pre-pass renderer for Blur (which ran on \nthe RSX), it became apparent that we could get significant performance gains from \noffloading the screen-space lighting pass to the SPUs[6]. \n[6] Coincidentally, it was around this time that Matt Swoboda presented his work in a similar area, in \nwhich he moved a fully deferred renderer to the SPUs in [Swoboda09]; Matt‘s work and willingness to \ncommunicate with us was useful in laying the ground work for our implementation in Blur. \nThe lighting calculations in Blur are performed on the SPU in parallel with other non-\ndependent parts of the frame. This means that as long as we have enough rendering work \nfor the RSX, the lighting has no impact on the latency of a frame. Processing of the lighting \nbuffer is done in tiles, the selection of which is managed through the use of the SPE‘s \natomic unit. When the tiles are processed, the RSX is free to access the lighting buffer \nduring the rendering of the main pass. The results of our technique are shown in Color Plate \n6 and discussed in greater detail in [Swoboda09, Tovey10]. \nBenefits versus Drawbacks \nThe SPUs are powerful enough to perform fragment processing. This has been \ndemonstrated by developers with deferred shading, post-processing, and so on \n[Swoboda09, van der Leeuw09, Tovey10]. While general-purpose fragment shading is not \npossible, it is possible to perform a plethora of image-space techniques on the SPUs, \nincluding motion blur, depth of field, shadowing, and lighting. Parallelization with other non-\nrelated rendering work on the GPU can provide an extra gain if one‘s goal is to minimize \nframe latency. Such gains can even be made without the expense of an increased memory \nfootprint. \nRasterization on the SPUs has been achieved by a number of studios with good results, but \nthe use cases for this technique are somewhat restricted, usually being reserved for \nocclusion culling and the like rather than general-purpose rendering. Rasterization aside, \nthe most serious drawback to performing fragment shading on the SPUs is the lack of \ndedicated texture-mapping hardware. Small textures may be feasible, as they will fit in the \nlimited local store, but for larger textures or multiple textures, software caching is currently \nconsidered to be the best approach [Swoboda09]. \n \nFurther Work \nDue to the highly flexible nature of the SPUs in augmenting the processing power of the \nGPU, it is hard to suggest avenues of further work with any certainty. However, there are a \nfew significant challenges that warrant additional research efforts in order to further \nimprove the feasibility of some graphics techniques on the SPUs. \nTexture mapping is one such avenue of research. Currently, the best that has been done is \nthe use of a good software cache [Swoboda09] to try and minimize the latency of texture \naccesses from the SPUs. Taking inspiration from other convergent architectures, namely \nIntel Larrabee [Seiler08], we believe that the employment of user-level threads on the SPUs \nas a mechanism for hiding latency could certainly go some way to helping the prohibitively \nslow texture access speeds currently endured by graphics programmers seeking to help the \nGPU along with the SPUs. Running two to four copies of the same SPU program (albeit with \noffline modifications to the program‘s byte code) could allow a programmer to trade space \n",
      "content_length": 3904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": " \n \nin the local store for processing speed. The idea is simple: Each time a DMA is initiated, the \nprogrammer performs a lightweight context-switch to another version of the program \nresiding in the local store, which can be done cheaply if the second copy does not make use \nof the same registers. The hope is that by the time we return the original copy, the data we \nrequested has arrived in the local store, allowing us to process it without delay. Such a \nscheme would impose some limitations but could be feasible for small-stream kernels, such \nas shaders. \n \nConclusion \nThe SPUs are fast enough to perform high-end vertex and fragment processing. While they \nare almost certainly not going to beat the GPU in a like-for-like race (in other words, the \nimplementation of a full graphics pipeline), they can be used in synergy with the GPU to \nsupplement processing activities traditionally associated with rendering. The option to split \nwork between the two processing elements makes them great tools for optimizing the \nrendering of specific objects in a scene. The deferred lighting and car damage systems in \nBlur demonstrate the potential of the SPUs to work harmoniously with the GPU to produce \nimpressive results. \nLooking to the future, the ever-growing popularity and prevalence of deferred rendering \ntechniques in current generations of hardware further empower the SPUs to deliver \nimpressive improvements to the latency of a frame and allow game developers to get closer \nto synthesizing reality than ever before. \n \nAcknowledgements \nI would like to thank the supremely talented individuals of the Bizarre Creations Core \nTechnologies Team for being such a great bunch to work with, with special thanks reserved \nfor Steve McAuley for being my partner in crime with our SPU lighting implementation. \nThanks also go to Andrew Newton and Neil Purvey at Juice Games for our numerous \ndiscussions about SPU coding, to Matt Swoboda of SCEE R&D for our useful discussions \nabout SPU-based image processing, and to Wade Brainerd of Activision Central Technology \nfor his helpful comments, corrections, and suggestions. Last but not least, thanks also to \nJason Mitchell of Valve for being an understanding and knowledgeable section editor! \n \nReferences \n[Acton06] Acton, Mike. ―Branch-Free Implementation of Half Precision Floating Point.‖ \nCellPerformance. 17 July 2006. Mike Acton. 2 July 2009. \n<http://cellperformance.beyond3d.com/articles/2006/07/update-19-july-06-added.html>. \n[Acton08] Acton, Mike and Eric Christensen. ―Insomniac SPU Best Practices.‖ Insomniac \nGames. 2008. Insomniac Games. 2 July 2009. \n<http://www.insomniacgames.com/tech/articles/0208/files/insomniac_spu_programming_g\ndc08.ppt>. \n[Bader07] Bader, David A. ―Cell Programming Tips & Techniques.‖ One-Day IBM Cell \nProgramming Workshop at Georgia Tech. 6 Feb. 2007. Georgia Tech College of Computing. \n2 July 2009. <http://www.cc.gatech.edu/~bader/CellProgramming.html>. \n",
      "content_length": 2968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": " \n \n[Balestra08] Balestra, Christophe and Pal-Kristian Engstad. ―The Technology of Uncharted: \nDrake‘s Fortune.‖ Game Developers Conference. 2008. Naughty Dog Inc. n.d. \n<http://www.naughtydog.com/docs/Naughty-Dog-GDC08-UNCHARTED-Tech.pdf>. \n[Deering88] Deering, Michael, et al. ―The Triangle Processor and Normal Vector Shader: A \nVLSI System for High Performance Graphics.‖ Proceedings of the 15th Annual Conference on \nComputer Graphics and Interactive Techniques (1988): 21–30. ACM Portal. \n[Engel08] Engel, Wolfgang. ―Light Pre-Pass Renderer.‖ Diary of a Graphics Programmer. 16 \nMarch 2008. Blogspot.com. 4 July 2009. \n<http://diaryofagraphicsprogrammer.blogspot.com/2008/03/light-pre-pass-renderer.html>. \n[Engel09] Engel, Wolfgang. ―Designing a Renderer for Multiple Lights: The Light Pre-Pass \nRenderer.‖ ShaderX7: Advanced Rendering Techniques. Ed. Wolfgang Engel. Boston: \nCharles River Media, 2009. 655–666. \n[Engel09a] Engel, Wolfgang. ―The Light Pre-Pass Renderer Mach III.‖ To appear in \nproceedings of ACM SIGGRAPH09, 2009. \n[IBM08] ―Cell Broadband Engine Programming Handbook.‖ IBM. 19 April 2006. IBM. n.d. \nhttps://www-\n01.ibm.com/chips/techlib/techlib.nsf/techdocs/7A77CCDF14FE70D5852575CA0074E8ED>. \n[IBM08a] ―Synergistic Processing Unit Instruction Set Architecture.‖ IBM. 27 Jan. 2007. \nIBM. n.d. <https://www-\n01.ibm.com/chips/techlib/techlib.nsf/techdocs/76CA6C7304210F3987257060006F2C44/$file\n/SPU_ISA_v1.2_27Jan2007_pub.pdf.> \n[IBM09] ―The Cell Project at IBM Research.‖ IBM. n.d. IBM. 4 July 2009. \n<http://researchweb.watson.ibm.com/cell/home.html>. \n[ILM09] ―OpenEXR.‖ OpenEXR. n.d. Lucas Digital Limited. 4 July 2009. \n<http://www.openexr.com>. \n[Kapoulkine09] Kapoulkine, Arseny. ―View frustum culling optimization—never let me \nbranch.‖ What Your Mother Never Told You About Graphics Development. 1 March 2009. \nBlogspot.com. 21 July 2009. <http://zeuxcg.blogspot.com/2009/03/view-frustum-culling-\noptimization-never.html>. \n[Möller08] Akenine-Möller, Thomas, Eric Haines, and Naty Hoffman. Real-Time Rendering, \n3rd Edition. Wellesley, MA: A.K. Peters, Ltd, 2008. \n[Perthuis06] Perthuis, Cedric. ―Introduction to the Graphics Pipeline of the PS3.‖ \nEurographics 2006. Austrian Academy of Sciences, Vienna, Austria. 6 Sept. 2006. \n[Saito90] Saito, Takafumi and Tokiichiro Takahashi. ―Comprehensible Rendering of 3-D \nShapes.‖ ACM SIGGRAPH Computer Graphics 24.4 (1990): 197-206. ACM Portal. \n[Seiler08] Seiler, Larry, et al. ―Larrabee: A Many Core x86 Architecture for Visual \nComputing.‖ ACM Transactions on Graphics 27.3 (Aug. 2008): n.p. ACM Portal. \n[Shippy09] Shippy, David and Mickie Phipps. The Race for a New Games Machine: Creating \nthe Chips Inside The New Xbox360 & The Playstation 3. New York: Citadel Press, 2009. \n[Swoboda09] Swoboda, Matt. ―Deferred Lighting and Post Processing on PLAYSTATION®3.‖ \nGame Developers Conference. 2009. Sony Computer Entertainment Eurpoe, Ltd. n.d. \n",
      "content_length": 2928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": " \n \n<http://www.technology.scee.net/files/presentations/gdc2009/DeferredLightingandPostProc\nessingonPS3.ppt>. \n[Tovey10] Tovey, Steven and Steven McAuley. ―Parallelized Light Pre-Pass Rendering with \nthe Cell Broadband Engine™.‖ GPU Pro: Advanced Rendering Techniques. Natick, MA: A K \nPeters Ltd., 2010. \n[van der Leeuw09] van der Leeuw, Michiel. ―The PLAYSTATION3‘s SPUs in the Real World—\nKILLZONE2 Case Study.‖ Game Developers Conference 2009. Moscone Center, San \nFrancisco, CA. 25 March 2009. \n \nSection 2: Physics and Animation \nIntroduction \nA Versatile and Interactive Anatomical Human Face Model \nCurved Paths for Seamless Character Animation \nNon-Iterative, Closed-Form, Inverse Kinematic Chain Solver (NCF IK) \nParticle Swarm Optimization for Game Programming \nImproved Numerical Integration with Analytical Techniques \nWhat a Drag: Modeling Realistic Three-Dimensional Air and Fluid Resistance \nApplication of Quasi-Fluid Dynamics for Arbitrary Closed Meshes \nApproximate Convex Decomposition for Real-Time Collision Detection \nIntroduction \nJeff Lander, Darwin 3D, LLC \njeffl@darwin3d.com \nGame creation as a business and an art has become much more mature, in years of \nexperience, in complexity, and in controversy of the material covered. For the most part, \nlong gone are the days when text adventures, simple frame flipping, and sprite-based \nanimation ruled the top-ten lists of gamers‘ hearts. Our games need to be much more real \nand complex to compete with the ever-increasing expectations of our audience. \nNowhere is this more evident than in the character performances and physical simulations \nof real (or imaginary) worlds. Players exposed to the amazing worlds that television and \nfilmmakers can create with visual effects rightly believe that their games should reflect \nthese advances and expectations as well. We need to bring our characters to life. We need \nto create worlds where the rules and systems that govern the reality have a basis in \nphysical realism and have a consistency that is at once familiar and exciting for our players. \nWe are past the point where players can be amazed by a simple animation clip of a \ncharacter running or watching a ball bounce on the ground. They have seen that all before. \nThey now expect a game‘s characters to react to the worlds around them in an intelligent \nway, as well as interact with the world in a way that models the physical interactions in our \nreal-life experiences. \n",
      "content_length": 2459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": " \n \nThe gems in this section represent the intersection of the physical interactions and animated \nperformances that we need in order to bring more of the illusion of life to our characters \nand worlds. In ―A Versatile and Interactive Anatomical Human Face Model,‖ Marco \nFratarcangeli discusses how to bring more realistic movement to facial animation to directly \nattack the problems with facial performance. This gem models the underlying facial \nanimation systems with physical simulation. In this same way, physical simulation is used in \nthe gems ―Application of Quasi-Fluid Dynamics for Arbitrary Closed Meshes‖ and ―What a \nDrag: Modeling Realistic Three-Dimensional Air and Fluid Resistance‖ to improve the realism \nin our interactive worlds. ―Particle Swarm Optimization for Game Programming‖ discusses \napplying easy-to-use particle simulation techniques to a variety of optimization problems. \nMuch of this is pretty advanced stuff. We will not be discussing how to play back an \nanimation on a hierarchical character or how to simulate and detect a collision between two \nobjects. You are expected to be masters of that kind of low-level system by now. \nWe are attacking larger problems now. For example, it is no longer sufficient for our \ncharacters to follow a simple piecewise linear path when moving across a space. That simply \nlooks too mechanical and robotic. In ―Curved Paths for Seamless Character Animation,‖ \nMichael Lewin discusses how it is necessary to smooth the results from our AI pathfinding \nsystems to create a movement path that follows a much more realistic curve, while still \navoiding all the obstacles that may be in the way. Adding to our character performance \nimprovements, in Philip Taylor‘s ―Non-Iterative, Closed-Form, Inverse Kinematic Chain \nSolver,‖ the existing iterative IK techniques are improved with an easy-to-understand, \nclosed-form solution. It is also not enough to take for granted little code snippets for \nnumerical integration we have seen posted on the net. It is important now for us to have a \ndeeper understanding of what is going on when we use something such as Euler integration \nfor a physical simulation and why it is important that we understand the error inherent in \nthese algorithms. ―Improved Numerical Integration with Analytical Techniques‖ looks \ndirectly at these issues and proposes methods to increase the accuracy in our simulations. \nAs we continue to create amazing new projects and push the envelope for what is possible \nto do in a game, I believe some of the most important steps forward will come at this \nintersection of animation and physics. As our animated characters become more physically \naware and grounded in our game environments, and our simulated worlds become inhabited \nby these responsive and emotional characters, our games will make huge leaps forward in \nconnecting to our audience. I hope these gems provoke some new ideas and encourage you \nall to inject just a little more life into your virtual creations. \n \n2.1. A Versatile and Interactive Anatomical Human Face Model \nMarco Fratarcangeli \nfrat@dis.uniroma1.it \nIn a compelling virtual world, virtual humans play an important role in improving the illusion \nof life and interacting with the user in a natural way. In particular, face motion is crucial to \nrepresent a talking person and convey emotive states. In a modern video game, \napproximately 5 to 10 percent of the frame cycle is devoted to the animation and rendering \nof virtual characters, including face, body, hair, cloth, and interaction with the surrounding \nvirtual environment and with the final user. Facial blend shapes are commonly adopted in \nthe industry to efficiently animate virtual faces. Each blend shape represents a key pose of \nthe face while performing an action (for example, a smiling face or raised eyebrows). By \ninterpolating the blend shapes with each other, as depicted in Figure 2.1.1, the artist can \nachieve a great number of facial expressions in real time at a negligible computational cost. \nFigure 2.1.1. Blend shapes interpolation. \n",
      "content_length": 4091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": " \n \n \n \nHowever, creation of the facial blend shapes is a difficult and time-consuming task, and, \nmore likely than not, it is still heavily dependant on the manual work of talented artists. \nIn this gem, I share principles and ideas for a tool that assists the artist in authoring \nhumanoid facial blend shapes. It is based on a physical simulation of the human head, \nwhich is able to mimic the dynamic behavior of the skull, passive tissues, muscles, and skin. \nThe idea is to let the artist design blend shapes by simply adjusting the contraction of the \nvirtual muscles and rotating the bony jaw. The anatomical simulation is fast enough to feed \nback the results in real time, allowing the artist to tune the anatomical parameters \ninteractively. \nOverview \nOur objective is to build a virtual model of the human head that simulates its anatomy and \ncan be adapted to simulate the dynamics of different facial meshes. The artist models a \nstatic face mesh, and then the anatomical simulation is used to generate its blend shapes. \nThe goal of the simulation is to create shapes that generate realistic motion. The exact \naccuracy of the individual muscles in a real anatomical model is not the goal. \nThe anatomical elements are simple yet expressive and able to capture the dynamics of a \nreal head. The anatomical simulation must also be fast enough to be interactive (in other \nwords, run at least at 30 fps) to allow the artist to quickly sketch, prototype, tune, and, \nwhere needed, discard facial poses. \nThe basic anatomical element is the skull. The anatomical model is not bound to the \npolygonal resolution or to the shape of the skull mesh; we require only that the skull mesh \nhas a movable jaw. On top of the skull, the artist may design several layers of muscles and \npassive tissue (such as the fat under the cheeks), the so-called muscle map. The skull and \nthe muscle map form the musculoskeletal structure that can be saved and reused for \ndifferent faces. The musculoskeletal structure is morphed to fit the shape of the target face. \nThen, the face is bound to the muscles and to the skull, and thus it is animated through a \nsimple and efficient numerical integration scheme. \n \nNumerical Simulation \nThe anatomical model is composed of different parts, most of which are deformable bodies, \nsuch as muscles, fat, and the skin. The dynamics algorithm must be stable enough to allow \ninteraction among these parts, it must be computationally cheap to carry out the \ncomputation at an interactive rate, and it must be controllable to allow precise tuning of the \nmuscles‘ contractions. To meet these requirements, we will use Position Based Dynamics \n(PBD), a method introduced in [Müller06] and recently embedded in the PhysX and Bullet \nengines. A less formal (although limited) description was introduced in [Jakobsen03] and \nemployed in the popular game Hitman: Codename 47. \n",
      "content_length": 2905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": " \n \nFor an easy-to-understand and complete formulation of PBD, as well as other useful \nknowledge about physics-based animation, you can review the publicly available SIGGRAPH \ncourse [Müller08]. In this gem, I describe PBD from an implementation point of view and \nfocus on the aspects needed for the anatomical simulation. \nIn most of the numerical methods used in games, the position of particles is computed \nstarting from the forces that are applied to the physical system. For each integration step at \na given time, we obtain velocities by integrating forces, and eventually we obtain the \nparticle‘s position by integrating velocities. This means that, in general, we can only \ninfluence a particle‘s position through forces. \nPBD works in a different way. It is based on a simple yet effective concept: The current \nposition of particles can be directly set according to a customizable set of geometrical \nconstraints \n, where \nis the set of particles involved in the simulation. Because \nthe constraints could cause the resulting position to change slightly, the velocity must be \nrecalculated using the new position and the position at the previous time step. As the \nposition is computed, the velocity is adjusted according to the current position and the \nposition at the previous time step. The integration steps are: \n(1) for each particle i do \n \n(2) for each particle i do \n \n(3) loop nbIterations times \n(4) solve \n \n(5) for each particle i \n(6) \n \nFor example, let us consider the simple case of two particles traveling in the space, which \nmust stick to a fixed distance d from each other. In this case, there is only one constraint C, \nwhich is: \nEquation 1 \n \n \nAssuming a particle mass m = 1kg, Step (3) in the Algorithm 1 is solved by: \n \n",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": " \n \nIn this example, if a force is applied to \n, it will gain acceleration in the direction of the \nforce, and it will move. Then, both \nand \nwill be displaced in Step (3) in order to \nmaintain the given distance d. \nGiven \n, finding the change in position Δpi for each particle is not difficult; however, it \nrequires some notions of vectorial calculus, which are outside the scope of this gem. The \nprocess is explained in [Müller06, Müller08] and partly extended in [Müller08b]. \nSteps (3) and (4) solve the set of constraints in an iterative way. That is, each constraint is \nsolved separately, one after the other. When the last constraint is solved, the iteration \nstarts again from the first one, and the loop is repeated nbIterations times, eventually \nconverging to a solution. Then, the velocity is accommodated in order to compensate the \nchange in position Δpi. In general, using a high value for nbIterations improves the \nprecision of the solution and the stiffness of the system, but it slows the computation. \nFurthermore, an exact solution is not always guaranteed because the solution of a \nconstraint may violate another constraint. \nThis issue is partly solved by simply multiplying the change in position Δpi by a scalar \nconstant K∊[0,..,1], the so-called constraint stiffness. For example, choosing k< for a \ndistance constraint leads to a dynamics behavior similar to the one of a soft spring. Using \nsoft constraints leads to soft dynamics and improves drastically the probability of finding an \nacceptable (and approximated) solution for the constraint set. For example, think about two \nrigid spheres that must be accommodated inside a cube with a diagonal length smaller than \nthe sum of the diameters of the two spheres: The spheres simply will not fit. However, if the \nspheres were soft enough, they would change shape and eventually find a steady \nconfiguration to fit in the cube. This is exactly how soft constraints work: If they are soft \nenough, they will adapt and converge toward a steady state. \nIn my experiments, I used a time step of 16.6 ms and found that one iteration is enough in \nmost of the cases to solve the set of constraints. Rarely did I use more iterations, and never \nbeyond four. \nSimilar to the distance constraint, other constraints can be formulated considering further \ngeometric entities, such as areas, angles, or volumes. The set of constraints defined over \nthe particles defines the dynamics of a deformable body represented by the triangulated \nmesh. I provide the source code for the distance, bending, triangular area, and volume \nconstraints on the companion CD-ROM. You are encouraged to experiment with PBD and \nbuild deformable bodies following the examples in the source code. \nPBD has several advantages: \n \nThe overshooting problem typical of force-driven techniques, such as mass-spring \nnetworks, is avoided. \n \nYou can control exactly the position of a subset of particles by applying proper \nconstraints; thus, the remaining particles will displace accordingly. \n \nPBD scales up very well with spatial dimensions because the constraint stiffness \nparameter is an adimensional number, without unit of measure. \n \nBuilding the Anatomical Model \nWe begin the process of building our virtual head by building up the low-level pieces that \nmake up the foundation of the physical motion. To do this, we take an anatomical approach. \nThe Skull \n",
      "content_length": 3418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": " \n \nThe skull is the basis upon which the entire computational model of the face is built. It is \nrepresented by a triangulated mesh chosen by the artist. Figure 2.1.2 illustrates an example \nof the mesh used in our prototype. The skull mesh is divided in two parts: the fixed upper \nskull and the movable mandible. The latter will move by applying the rigid transformation \ndepicted in Figure 2.1.2. \nFigure 2.1.2. (1) Example skull mesh, (2) jaw lateral slide, (3) jaw opening, (4) \njaw protruding. \n \n \n \nInteractive Sketching of the Muscles \nIn our model, muscles are represented by rectangular parallelepipeds, which are deformed \nto match the shape of the facial muscles. To define the shape of the muscles, we draw a \nclosed contour directly on the skull surface and the already-made muscles. Figure 2.1.3 \nshows an example of the definition of the initial shape of a muscle in rest state. The closed \ncontour is defined upon underlying anatomical structures—in this case, the skull. Then, a \nhexahedral mesh M is morphed to fit the contour. M is passively deformed as the skull \nmoves. \nFigure 2.1.3. Defining the shape of a muscle on the skull. \n \nThe closed contour is drawn through a simple ray-casting algorithm. The position of the \npointing device (I used a mouse) is projected into the 3D scene, and a ray is cast from the \nnear plane of the frustum to the far plane, as shown in Figure 2.1.4. \nFigure 2.1.4. Casting a ray to determine the intersection point. \n",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": " \n \n \n \nThe intersection points form the basis of the muscle geometry, the so-called action lines. \nAn action line is a piecewise linear curve lying on at least one mesh. The purpose of the \naction lines is twofold: (1) They define the bottom contour of the muscle geometry during \nthe simulation, and (2) they provide a mechanism to control the active contraction of the \nmuscle itself. A surface point is a point sp in S, where S is the surface represented by the \nmesh. A surface point sp is uniquely described by the homogeneous barycentric coordinates \n(t1, t2, t3) with respect to the vertices A1A2A3 of the triangular facet to which it belongs, as \nshown in Figure 2.1.5. \nFigure 2.1.5. (a) A surface point sp in A1A2A3 is defined by the homogeneous \nbarycentric coordinates (t1, t2, t3) with respect to the triangle vertices. (b) When \nthe triangle deforms, the triple (t1, t2, t3) does not change, and sp is updated to \nsp’. \n \nThe relevant attributes of a surface point are position and normal; both of them are \nobtained through the linear combination of the barycentric coordinates with the \ncorresponding attributes of the triangle vertices. When the latter displace due to a \ndeformation of the triangle, the new position and normal of sp are updated and use the new \nattributes of the vertices (Figure 2.1.5 (b)). Each linear segment of the action line is defined \nby two surface points; thus, an action line is completely described by the ordered list of its \nsurface points. Note that each single surface point may belong to a different surface S. So, \nfor example, an action line may start on a surface, continue on another surface, and finish \n",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": " \n \non a third surface. When the underlying surfaces deform, the surface points displace, and \nthe action line deforms accordingly. \nSoft Model for a Facial Muscle \nStarting from a triangulated, rectangular parallelepiped (or hexahedron), each vertex is \nconsidered as a particle with mass m = 1; particles are connected with each other to form a \nnetwork of distance constraints, as shown in Figure 2.1.6. \nFigure 2.1.6. Distance constraints used in the muscle model. \n \nThese constraints replicate some aspects of the dynamic behavior of a real face muscle, in \nparticular resistance to in-plane compression, shearing, and tension stresses. Note that \ndistance constraints are placed over the surface of the hexahedron, not internally. For \ncompleting the muscle model, we add bending constraints among the triangular faces of the \nmesh to conserve superficial tension. We also add a further volume constraint over all the \nparticles, which makes the muscle thicker due to compression and thinner due to \nelongation. \nThe Muscle Map \nUsing the interactive editor to sketch the shape of the soft tissues over the skull, we define \nthe structure made up of intertwined muscles, cartilage, and facial tissue, mostly fat. The \nmuscles are organized in layers. Each layer influences the deformation of the layers on top \nof it, but not those underlying it. See Figure 2.1.7. \nFigure 2.1.7. Different layers forming the muscle map used in the experiments. \n \n \n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": " \n \nThe muscle map comprises 25 linear muscles and one circular muscle. This map does not \nrepresent the real muscular structure of the human head; this is due to the simulated \nmuscle model, which has simplified dynamics compared to the real musculoskeletal system. \nHowever, even though there may be not a one-to-one mapping with the muscle map in a \nreal head, this virtual muscle map has been devised to mimic all the main expressive \nfunctionalities of the real one. \nFor instance, on the forehead area of a real head, there is a single large, flat sheet muscle, \nthe frontalis belly, which causes almost all the motion of the eyebrows. In the virtual model, \nthis has been represented by two separate groups of muscles, each one on a separate side \nof the forehead. Each group is formed by a flat linear muscle (the frontalis) and on top of it \ntwo additional muscles (named, for convenience, frontalis inner and frontalis outer). On top \nof them, there is the corrugator, which ends on the nasal region of the skull. Combining \nthese muscles, the dynamics of the real frontalis belly are reproduced with a satisfying \ndegree of visual realism, even though the single linear muscle models have simple dynamics \ncompared to the corresponding real ones. \nEach simulated muscle is linked to the underlying structures through position constraints \nfollowing the position of surface points. Thus, when an anatomical structure deforms, the \nentire set of surface points lying on it moves as well, which in turn influences the motion of \nthe above linked structures. For instance, when the jaw, which is part of the deepest layer, \nrotates, all the deformable tissues that totally or partially lie on it will be deformed as well, \nand so on, in a sort of chain reaction that eventually arrives at the skin. \nActive contraction of a muscle is achieved by simply moving the surface points along the \naction lines. Given that the bottom surface of the muscles is anchored to the surface points \nthrough position constraints, when the latter move, the muscle contracts or elongates, \ndepending on the direction of motion of the surface points. \nFigure 2.1.8. The example muscle map is deformed by rotating the jaw and \ncontracting the frontalis belly. Note how all the above muscles are properly \ndeformed. \n \n \n \nMorphing the Muscle Map into the Target Face Mesh \nOnce the skull and the muscle map are ready, they can be morphed to fit inside the target \nfacial mesh, which represents the external skin. The morphing is done through an \ninterpolation function, which relies on the so-called Radial Basis Functions [Fang96]. We \ndefine two sets of 3D points, P and Q. P is a set of points defined over the surface of the \nskull and the muscles, and Q is a set of points defined over the skin mesh. Each point of P \n",
      "content_length": 2810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": " \n \ncorresponds to one, and only one, point of Q. The position of the points in P and Q are \nillustrated in Figure 2.1.9 (a) and must be manually picked by the artist. These positions \nhave been proven to be effective for describing the shape of the skull and of the human face \nin the context of image-based coding [Pandzic and Forchheimer02]. Given P and Q, we find \nthe interpolation function G(p), which transforms a point pi in P and a point qi in Q. Once \nG(p) is defined, we apply it to all the vertices of the skull and muscle meshes, fitting them \nin the target mesh. \nFigure 2.1.9. (a) The set of points P and Q picked on the skull and on the face \nmesh, respectively. (b) The outcome of the morphing technique. \n \n \nFinding the interpolation function G(p) requires solving a system of linear independent \nequations. For each couple 〈pi,qi〉, where pi is in P and qi is the corresponding point in Q, we \nset an equation like: \n \nWhere n is the number of points in each set, di is the distance of pi from pj, ri is a positive \nnumber that controls the ―stiffness‖ of the morphing, and hi is the unknown. \nSolving the system leads to the values of hi, i = 1, .., n, and thus G(p): \n \nThis particular form of Radial Basis Functions is demonstrated to always have a solution, it \nis computationally cheap, and it is particularly effective when the number of points in P and \nQ is scarce. Figure 2.1.9 (b) shows an example of fitting the skull into a target skin mesh. \nSkin \n",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": " \n \nSkin is modeled as a deformable body; its properties are defined by geometrical constraints \nin a similar way to the muscles. The skin is built starting from the target face mesh provided \nin input. Each vertex in the mesh is handled as a particle with a mass, which is set to 1.0. \nAfter the skull and the muscle map are fitted onto the skin mesh, further constraints are \ndefined to bind the skin to the underlying musculoskeletal structure. For each particle p in \nthe skin mesh, a ray is cast along the normal that is toward the outer direction. In fact, \nafter the fitting, portions of some muscles may stay outside the skin. By projecting in the \nouter direction, the skin vertices are first bound to these muscles. If no intersection is \nfound, then another ray is cast in the opposite direction of the normal, toward the inner part \nof the head. The ray is tested against the muscles from the most superficial to the deepest \none. If the ray does not intersect any muscle, then the skull is tested. The normal of the \nskin particle is created by averaging the normals of the star of faces to which the particle \nbelongs. \nIf an intersection is found, then it is defined as a surface point sp on the intersected \ntriangular face in the position where the ray intersects the face. A particle q is added to the \nsystem, and it is bound through a position constraint to sp. A stretching constraint is placed \namong the particles p and q. When the skull and the muscles move, the position of the \nsurface points will change accordingly. The set of added particle q is updated as well \nbecause it is bound to the surface points through the corresponding position constraint and \nwill displace the skin particles, whose final motion will depend also on the other involved \nconstraints. \n \nConclusion \nAlthough not very accurate from the point of view of biomechanics, the presented \nanatomical model is able to simulate convincing facial poses, including macro-wrinkles, \nwhich can be used as blend shapes, as shown in Color Plate 7. The model is stable, robust, \nand controllable, which is critical in interactive tools for producing video game content. The \nanatomical model is adaptive enough to animate directly the face mesh provided by the \nartist, thus it does not lead to the artifacts associated with motion retargeting. It does not \nrequire expensive hardware, and it may run on a consumer-class PC while still providing \ninteractive feedback to the artist. \n \nReferences \n[Fang96] Fang, Shiaofen, Raghu Raghavan, and Joan T. Richtsmeier. ―Volume Morphing \nMethods for Landmark Based 3D Image Deformation.‖ International Symposium on Medical \nImaging. 2710 (1996): 404–415. \n[Fratarcangeli08]. Fratarcangeli, Marco. ―A Computational Musco-Skeletal Model for \nAnimating Virtual Faces.‖ Ph.D. thesis, Universitá degli Studi di Roma ―La Sapienza.‖ 2008. \n[Pandzic and Forchheimer02] Pandzic, Igor S. and Robert Forchheimer. MPEG-4 Facial \nAnimation—The Standard, Implementation and Applications, 1st ed. John Wiley & Sons, \n2002. \n[Jakobsen03] Jakobsen, T. ―Advanced Character Physics.‖ 2003. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/2904/advanced_character_physics.php, 2003>. \n[Müller06] Müller, M., B. Heidelberger, M. Hennix, and J. Ratcliff. ―Position Based \nDynamics.‖ J. Vis. Commun. 18.2 (2007): 109–118. \n",
      "content_length": 3341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": " \n \n[Müller08] Müller M., D. James, J. Stam, and N. Thuerey. ―Real Time Physics.‖ 2008. \nMatthias Muller. n.d. <http://www.matthiasmueller.info/realtimephysics/index.html>. \n[Müller08b] Müller, M. ―Hierarchical Position Based Dynamics.‖ Proceedings of Virtual Reality \nInteractions and Physical Simulations. Grenoble, 2008. \n \n2.2. Curved Paths for Seamless Character Animation \nMichael Lewin \nmikelewin@cantab.net \nThe latest generation of consoles brings with it the promise of almost lifelike graphics and \nanimation. Yet for all the beautiful artwork, we are still seeing artifacts, such as character \nsliding, that break the illusion of the virtual world. Sliding occurs when we apply any extra \nrotation or translation to an animation that does not correspond to the way the character‘s \nlimbs are moving. This causes movement in which the feet are not planted firmly on the \nground, such as running on the spot, shifting left or right while walking, rotating unnaturally \nwhile walking or standing, or sliding upwards or downwards while walking on stairs. \nA tension exists between our desire for realistic human animation and our need to maintain \nprecise control of the character‘s position, orientation, and velocity. In many projects, there \nis not sufficient synergy between the AI and animation layers to satisfy both requirements \nsimultaneously. This gem presents a general way to adapt pathfinding techniques to better \ninteract with animation selection. We developed a technique at Sony Computer \nEntertainment that uses cubic Bezier curves to allow the AI system to be more closely \ncoupled with dynamics and animation, considering the constraints of the character‘s \nmovement. \nRelated Work \n[Johnson06] describes a method for fitting Bezier curves inside the cells of a navigation \nmesh to create a smooth path. This gem describes in a more general way that Bezier curves \ncan be fitted to any piecewise linear path. \n \nFinding a Path \nAll characters need to move around their environment, which usually involves fast-paced \ndynamic changes and contains unpredicted obstacles. This problem is just as relevant to \nrobot motion planning in the real world as it is to character movement in simulations such \nas games, and there is a wealth of literature on the subject applied to both domains. \nThe vast majority of pathfinding solutions create a piecewise linear path first and then fit a \ncurved path to it. There are a few examples where this intermediate step is not needed, \nsuch as using potential fields [Ratering95], but they do not give the same level of control as \nother methods. I will not consider them further in this article. \nThere are many different choices of representation when constructing a piecewise linear \npath. The A* algorithm is usually applied because it is simple, fast, and guaranteed to be \noptimal. Where techniques for generating pathfinding solutions differ lies in how to \nrepresent the physical space. It can be represented as a set of connected cells or points. I \nbriefly explain some of the main techniques; a thorough review can be found in [Tozour03] \n",
      "content_length": 3112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": " \n \nor [Latombe91], and an edifying demo can be downloaded at [Tozour05]. Figure 2.2.1 \nshows an example that compares the results of using the various methods. \nFigure 2.2.1. The various methods of creating a piecewise linear path: (a) regular \ngrid, (b) quad tree, (c) navigation mesh, (d) Voronoi diagram, and (e) visibility \ngraph. \n \n \nThe simplest method is to divide up the space into a grid of regular rectangular cells with a \nresolution that is small compared to the size of obstacles. A slightly more sophisticated \n",
      "content_length": 527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": " \n \nversion of this is to use a quad tree instead of identical rectangles, so that areas of different \nresolution are defined as needed. \nNavigation meshes, as described in [Snook00, Tozour02], are an increasingly popular \nalternative to the grid solutions described previously. They partition the space using a \nvariety of irregular polygons so that each cell is either entirely filled by an obstacle or \nentirely empty. The partitioning may be performed by hand when the environment is \nauthored, or it can be generated automatically and then hand-edited later. [Axelrod08, \nMarden08] describe algorithms for dynamically recalculating the mesh to account for \nmoving obstacles. \nVoronoi diagrams are on first appearance similar to navigation meshes, but they yield quite \ndifferent paths. Obstacles are represented as a finite set of points, and the space is \npartitioned such that each point is inside a region, where that region is defined as the set of \npoints closer to that obstacle point than any other. The edges of the resulting diagram can \ndefine paths around the space provided that all edges that pass through an obstacle are \nremoved first. \nVisibility graphs are different from the previously described methods because they generate \na set of waypoints rather than partitioning the space into cells. The graph is defined by \npoints in the environment, in which pairs of points are connected by an edge if they can be \njoined by a straight line without intersecting any obstacles. The points are selected to be \nclose to the corners of obstacles (this is known as a corner graph), so that the character can \npass close to obstacles without colliding with them. A good demonstration of the technique \ncan be seen at [Paluszewski]. These methods are shown in Figure 2.2.1. \n \nSmoothing the Path \nThe AI programmer‘s challenge is to create a piecewise linear path through the environment \nso that the character appears as lifelike and natural as possible, which means it cannot \ninclude jagged edges. Piecewise polynomial parametric curves, more commonly known as \nsplines, are well suited to this task, and in particular cubic Bezier curves. This is a \nparametric curve of the form: \n \nAs Figure 2.2.2 illustrates, the control points P0 and P3 define the start and end points, \nrespectively, while the control points P1 and P2 define the curvature at the end points. The \nvectors P1-P0 and P3-P2 are referred to as the start and end velocity, respectively, because \nthey define the tangent and curvature at the two end points. In practical terms, this means \nwe can adjust their direction to control the character‘s initial and final direction of motion, \nand we can adjust their magnitude to control the curvature of the curve. Another useful \nproperty is that the curve will always lie within the boundary of the quadrilateral convex hull \ndefined by the four control points. \nFigure 2.2.2. Bezier curves are well suited to path fitting. The four control points \ndefine the curve’s position, shape, and curvature. The curve will always be \ncontained inside the convex hull defined by the control points. \n",
      "content_length": 3120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": " \n \n \n \nHaving constructed a piecewise linear path, we can fit cubic Bezier curves to it for a smooth \npath. By choosing the control points so that for each consecutive pair of curves the output \ndirection of the first matches the input direction of the second, we achieve continuity and \nsmoothness at the endpoints. Our objective is to adhere to the linear path only as much as \nnecessary. If the character‘s momentum is high and a large sweeping curve looks most \nnatural, and provided there are no obstacles in the way of that path, we want to select such \na path rather than sticking rigidly to the linear path. However, if the path is very close to \nobstacles and there is little room to maneuver, we want a path that is as curved as possible \nbut still avoids those obstacles. \nThe solution to this problem comes from an unlikely source: The open source graph plotting \nsoftware Graphviz [Graphviz] makes use of the same technique for drawing edges between \nits nodes [Dobkin97]. The inspiration comes from a 1990 article in the first Graphics Gems \nbook that was presented as a way to render fonts using vector graphics [Schneider90, \nSchneider90_2]. The basic principle is described by the following pseudocode: \nFitcurve(startPos, endPos, startVel, endVel) \n \ncreate a Bezier curve from startPos to endPos, \nusing startVel and endVel to define the control points \n \nwhile the curve intersects an obstacle: \n    reduce startVel and endVel \n    recalculate the Bezier curve \n    if startVel and endVel reach a user-defined minimum: \n        break \n \nif the curve still intersects an obstacle: \n    divide the path in two by choosing a point newPos along path \n    set newVel using the vector between two neighbors of newPos \n    Fitcurve(startPos, newPos, startVel, newVel) \n    Fitcurve(newPos, endPos, newVel, endVel) \n \nThe whole process of calculating the piecewise linear path and fitting the curved path to it is \nfast enough to run in a single frame. The path can therefore be recalculated whenever a \nnew obstacle renders the current path unfeasible. Note that the above recursive algorithm is \nguaranteed to terminate with a valid path because, in the limiting case, we are left with the \nsame piecewise linear path that we began with. \n",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": " \n \nFigure 2.2.3 shows an example. A piecewise linear path is created that avoids an obstacle. \nThen a single cubic Bezier curve is fitted to it, which takes into account the character‘s \nstarting momentum. But this curve intersects with another obstacle, so a new path is \ncreated, using two cubic Bezier curves, that passes through another point on the piecewise \nlinear path. \nFigure 2.2.3. Fitting a curve that accounts for character momentum. A single cubic \nBezier curve A (light dashed line) is fitted to the piecewise linear curve (light filled \nline). But this passes through another obstacle, so a new path B (heavy dashed \nline) is constructed from two cubic Bezier curves that passes through an \nintermediate point on the piecewise linear path. \n \n \nIn order for a character to walk along the Bezier path, we need to choose the maximum and \nminimum velocities for the curve. Consider the distance d from the start point to the end \npoint of a single Bezier curve. If the sum of the magnitudes of the start and end velocities \nexceeds d, the resulting curve can contain a loop. Usually we do not want this, so we can \nimpose d/2 as a maximum. A value of zero works perfectly well for a minimum, but we can \nalso use this as an opportunity to impose constraints based on the character‘s initial \nmomentum, so that sharp turns are not allowed if the character is moving fast. \nUnfortunately, the character‘s physical velocity is not identical to the concept of velocity in \nthe Bezier curve. This is because the latter is related to the curve‘s internal parametrization \nand therefore varies with curvature and curve length. Nonetheless, good results can be \nachieved by imposing a minimum kv/d, where v is the character‘s speed and k is a constant \ncalculated empirically. \n \nAnimation Selection \nThus we have created a smooth path to the end point that avoids any obstacles in the \nenvironment. All that remains is to select the appropriate animation for the character that \nwill take it along this path. There is a wealth of literature on this complicated topic, using \ntechniques such as motion graphs and inverse kinematics, which are beyond the scope of \nthis article. Instead, I will present a simple solution using animation blending that gives a \nsatisfactory degree of accuracy. \nTo test the system, I created a character with a family of hand-designed animations for \neach different gait (for example, walking, running, sprinting). Within each family, all the \nanimations had the same duration and path length but different amounts of rotation (for \nexample, 0, 22.5, 45, 90 degrees). In this way, we can generate motion with any amount of \nrotation by blending two of the animations together. Linear blending of two or more \nanimations simply means averaging the position and rotation of the skeleton joints. This is \ndone separately for each frame of the animation. The resulting output is therefore a mixture \nof the inputs. This will only look natural when the animations are quite similar to begin with, \nas is the case here. Figure 2.2.4 shows an example. \n",
      "content_length": 3083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": " \n \nFigure 2.2.4. A family of animations that can be blended together. Each has the \nsame path length but a different amount of rotation (0, 22.5, 45, and 90 degrees). \n \n \nWe can chain the animations together because each one begins and ends at the same point \nin the character‘s stride. We can transition seamlessly between different gaits using \ntransition animations, authored in the same way as a family of animations with different \ndegrees of rotation. \nAs one animation ends, a new one must be selected. It is possible at this moment that the \ncharacter‘s position and orientation do not perfectly match the curved path. We could \nchoose an animation that will return the character to the path, but if his orientation is \nwrong, this will result in a zigzagging motion as the character veers too far one way with \none step and then too far the other way with the next. It is better to ensure the character‘s \nfinal orientation matches the tangent to the path, even if this means there is some error in \nhis position. \nThe method described gave sufficiently accurate results with a wide range of walking and \nrunning gaits; Figure 2.2.5 shows an example. This technique is limited, however, as Figure \n2.2.6 shows. The animations we can create by blending in this way always describe a \ncircular arc. All we can do is set the curvature of this arc. Some portions of a Bezier curve, \nhowever, cannot be described by a circular arc, and this is what causes the character to \ndeviate from the path. For this reason, more sophisticated animation selection techniques \nwould work better. \nFigure 2.2.5. There is not much error between the dotted line, made of circular \narcs, and the full line, made of four cubic Bezier curves. Each dot represents the \nstart of a new arc. \n",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": " \n \n \n \nFigure 2.2.6. (a) A possible blended output: a circular arc with a rotation of 67.5 \ndegrees. (b) An impossible output: No single circular arc can fit it. \n \n \n \n \nConclusion \nIn conclusion, this gem has presented a simple and effective way to generate a curved path \nfrom any piecewise linear path that facilitates animation selection and promotes interaction \nbetween the AI and animation layers of a character. Future work should look at applying \nthis to common physical constraints, such as enforcing a run-up before jumping or picking \nup an object. An important future extension to this work is to address how best to manage \nfeatures of a three-dimensional terrain, such as stairs, gaps, and ledges. \n \nReferences \n",
      "content_length": 731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": " \n \n[Axelrod08] Axelrod, Ramon. ―Navigation Graph Generation in Highly Dynamic Worlds.‖ AI \nGame Programming Wisdom 4. Boston: Charles River Media, 2008. \n[Dobkin97] Dobkin, David P., et al. ―Implementing a General-Purpose Edge Router.‖ \nProceedings of Graph Drawing 1997: 262–271. \n[Graphviz] Graphviz ―Spline-o-matic.‖ n.d. Graphviz. n.d. \n<http://www.graphviz.org/Misc/spline-o-matic/>. \n[Johnson06] Johnson, Geraint. ―Smoothing a Navigation Mesh Path.‖ AI Game Programming \nWisdom 3. Boston: Charles River Media, 2006. \n[Latombe91] Latombe, Jean-Claude. Robot Motion Planning. Kluwer Academic Publishers, \n1991. \n[Marden08] Marden, Paul. ―Dynamically Updating a Navigation Mesh via Efficient Polygon \nSubdivision.‖ AI Game Programming Wisdom 4. Boston: Charles River Media, 2008. \n[Paluszewski] Paluszewski, Martin. ―Robot Motion Planning (applet).‖ n.d. University of \nCopenhagen. n.d. <http://people.binf.ku.dk/palu/robotmotion/index.html>. \n[Ratering95] Ratering, Steven and Maria Gini. ―Robot Navigation in a Known Environment \nwith Unknown Moving Obstacles.‖ Autonomous Robots. 1.1 (June 1995): n.p. \n[Schneider90] Schneider, Philip J. ―An Algorithm for Automatically Fitting Digitized Curves.‖ \nGraphics Gems. Academic Press Professional, Inc., 1990. \n[Schneider90_2] Schneider, Philip J. ―A Bezier Curve-Based Root-Finder.‖ Graphics Gems. \nAcademic Press Professional, Inc., 1990. \n[Snook00] Snook, Greg. ―Simplified 3D Movement and Pathfinding Using Navigation \nMeshes.‖ Game Programming Gems. Boston: Charles River Media, 2000. \n[Tozour02] Tozour, Paul. ―Building a Near-Optimal Navigation Mesh.‖ AI Game Programming \nWisdom. Boston: Charles River Media, 2002. \n[Tozour03] Tozour, Paul. ―Search Space Representations.‖ AI Game Programming Wisdom \n2. Boston: Charles River Media, 2003. \n[Tozour05] Tozour, Paul. ―Pathfinding Algorithms & Search Space Representations Demo.‖ \n16 July 2005. <http://www.ai-blog.net/archives/000091.html>. \n \n2.3. Non-Iterative, Closed-Form, Inverse Kinematic Chain Solver \n(NCF IK) \nPhilip Taylor \nptaylor@trapdoorinc.com \nInverse kinematics (IK) has many uses in games. A primary use is to control the limbs of \ncharacters—to fit the pose of the character to the terrain it is standing on or to pin a foot \nwhile walking to reduce foot sliding, as described in [Forsyth04]. For many characters, a \nsimple two-bone solver is all that is required because, as in the case of human characters, \nthere are only two major bones in a chain that need to be modified. \n",
      "content_length": 2504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": " \n \nThe problem of solving a two-bone chain is often reduced to a two-dimensional problem by \nconstraining the solution to lie on a plane, usually defined by the root of the chain, the IK \ngoal, and an up-vector position value. These two-bone solutions are considered ―closed \nform‖ because the solution can be found using trigonometry [Lander98]. When it comes to \nchains with more than two bones, there are several well-known algorithms used to solve \nthis problem. Coordinate Cyclic Descent (CCD), Jacobian Transpose, or Pseudo-Inverse are \ncommonly used. \nThese algorithms suffer from performance issues because multiple iterations are required to \nconverge on a solution. Without sufficient iterations, the chain may not reach the goal \nwithin acceptable limits, and the chain may exhibit irregular movements between frames, \ncausing visual artifacts. Additionally, they lack precise control over the resulting shape of \nthe chain. \nIn this gem, I present a new method for solving inverse kinematics on chains comprising \nany number of bones by solving each bone separately in two dimensions. This solution is \nnon-iterative in that only a single evaluation per bone is required, while guaranteeing a \ncorrect solution if the goal is within reach of the chain. The algorithm does not require extra \nparameters, such as up-vector position values or preferred angles. Instead, the algorithm \nattempts to preserve the initial shape of the chain while reaching for the goal, maintaining \nthe integrity of any pose or animation data that was present on the chain prior to the IK \nsolver‘s evaluation. \nContext \nConsider the context of the character within a game. We are usually not building a set of \nrotations to define the chain pose; rather, we are modifying a set of orientations to more \nclosely fit a world space constraint. \nDuring the evaluation of a game scene, the animation system is sampled to provide a local \nspace transform per bone, and these local space pose transforms get concatenated together \nto build a global space pose. The pose at this point in the engine‘s evaluation is artist \ndefined and part of what describes the style and personality of the character. \nMaintaining the integrity of the character‘s pose, or motion, while introducing new \nconstraints, such as foot planting or lever pulling, is desirable as significant investment has \nbeen made in defining these motions. Any modifications made to the pose should be \nminimized to avoid breaking the original intent of the motion. \nIn this gem, the pose defined by the animation system is referred to as the forward \nkinematic pose, or FK pose. Forward kinematic refers to the way that the pose was built, by \naccumulating local space transforms down through the hierarchy to generate global space \ntransforms. \n \nBone Definition \nFor the purpose of this gem, a bone is defined as containing a position and an orientation. \nThe position is represented as a vector triple, and the orientation is represented as a \nquaternion. As illustrated in Figure 2.3.1, we refer to the vector that runs along its length as \nthe bone length vector, and the length of a bone is defined as the length of this vector. By \nconvention, bones are aligned with their local X-axis. A chain is defined as a linear hierarchy \nof bones, usually lined end to end. \nFigure 2.3.1. Bone definition. \n",
      "content_length": 3354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": " \n \n \n \n \n \nIK Goal Position \nWithout considering hands, feet, or any other child bones of the chain, a solution is defined \nas solving the chain such that the tip of the last bone touches, or comes as close to \ntouching as possible, some predetermined IK goal position. The exact location of this IK \ngoal position is defined by the engine and is not within the scope of this gem. \nThe animation engine might be locking the foot to a plant position over the course of a \ncharacter‘s step, or the hand of a character could be constrained to the steering wheel of a \nvehicle. In both cases, we have a bone chain with animation applied and a desired goal \nposition, which is defined by the character‘s environment. \nOne-Bone IK \nThe simplest chain would only comprise one bone, and the closest valid solution would be to \nalign the bone with the goal. While this example may appear trivial, the rest of this gem \nbuilds upon this concept. \nThe vector between the bone and the goal is called the bone-to-goal vector, and we refer to \nthe desired angle between the bone length vector and the bone-to-goal vector as the IK \nangle. \nWhen solving a chain comprising one bone, the best that can be achieved is to adjust the \nbone‘s pose such that the bone length vector is aimed at the IK goal, as shown in Figure \n2.3.2. This is done by incrementing the rotation using the angle between the bone length \nvector and the bone-to-goal vector. The axis of rotation is perpendicular to both the bone \nlength vector and the bone-to-goal vector. This rotation is the shortest arc of rotation that \nwill transform the bone vector onto the bone-to-goal vector. Computing this quaternion \ndirectly is described in the article by [Melax06]. \nFigure 2.3.2. Aiming a bone at an IK goal position. \n",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": " \n \n \n \n \nTwo-Bone IK \nWhen an additional bone is added into the system, the mathematics becomes more \ncomplex. \nAligning the Chain to the IK Goal \nMaintaining the shape of the limb while solving IK requires that each bone retain the \nrespective transforms with bones prior to it and after it in the chain. For example, if the \nbones in the chain all lie on one plane prior to solving, then after solving the bones should \nall still lie on a single plane. \nThe first step to solving a chain is to offset the orientation of each bone by the shortest arc \nbetween the root of the chain and the chain tip, and the root of the chain and the IK goal \nposition, as shown in Figure 2.3.3. This step ensures that all further deformation will occur \non the same plane with respect to the overall chain shape, as the original chain pose \ndescribes. This step also has the effect of biasing most of the deformation to the first joint \nin the chain. For most characters‘ limbs this is desirable, but joint limits can be imposed to \nrestrict this deformation. See the ―Future Work‖ section. \nFigure 2.3.3. Aligning the chain to the IK goal. \n \nWe first apply the overall alignment to each bone and then solve using the appropriate \nmethod for that bone. \n",
      "content_length": 1242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": " \n \nCalculating Bone 0 in a Two-Bone Chain \nThe vector from a bone to the tip of the chain prior to solving is called the bone-to-chain-tip \nvector, and the angle between the bone length vector and the bone-to-chain tip is the FK \nangle, as shown in Figure 2.3.4. To calculate the IK angle for Bone 0, we use the law of \ncosines equation providing the parameters a, b, and c. \nEquation 1  \n \n \nFigure 2.3.4. Solving the first bone in a two-bone chain. \n \n \nOnce we have calculated the IK angle, we subtract this from the current angle between the \nbone length vector and the bone-to-goal vector to get a delta angle, which we will use to \nmodify the bone‘s pose. The axis around which we rotate the bone is the cross product of \nthe bone-to-goal vector and the bone-to-chain-tip vector. We then modify the bone‘s \norientation by incrementing the rotation using the axis and the delta angle. \nSolving the Last Bone \nBefore any child bone can be solved, its new global position must be calculated by \ntransforming the bone‘s length vector using the new orientation of its parent and adding it \nto the parent‘s position. Bone 1, or the last bone in any chain, is solved using the one-bone \nsolution described previously in the section ―One-Bone IK.‖ \nThree-Bone IK Solver \n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": " \n \nI have shown that a two-bone problem can be decomposed into two types of bones that are \neach solved using an appropriate method. The first thing to consider about a three-bone \nchain is that it encapsulates the two-bone chain described previously with one extra bone at \nthe start of the chain, as shown in Figure 2.3.5. Therefore, once a solution can be found for \nBone 0, then bones one and two can be derived using the previously described methods. \nFigure 2.3.5. Three-bone chain. \n \n \n \nMaximum Bone Angles \nConsider that the initial pose of the chain defines an ideal pose for the limb, and any \nmodification of the limb pose must be minimized. By analyzing the FK pose of the chain, for \nexample, in Figure 2.3.6, we can determine a value that describes how bent Bone 0 is with \nrespect to the rest of the chain. \nFigure 2.3.6. Calculating the bone angle and the maximum FK and IK bone angles. \n",
      "content_length": 907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": " \n \n \n \nWe can use the law of cosines to calculate a value that defines the angle of Bone 0 if the \nremaining chain were to be laid out in a straight line. The remaining chain length, the \ndistance to the FK chain tip, and the bone length are used to calculate the max bone angle. \nThis angle is referred to as the max FK bone angle. Comparing this max FK bone angle to \nthe actual FK bone angle gives us a value that defines our FK bone angle relative to the \ndistance to the FK chain tip. \nEquation 2  \n \n \nThe bone angle fraction is defined relative to the initial shape of the chain and is a \ncorrelation between the bone‘s orientation and the rest of the chain pose. Conversely, the \nremaining bone length can also be used to calculate the maximum possible angle that the \nbone can assume in IK. This maximum IK bone angle is multiplied by the bone angle \nfraction value to determine the new IK bone angle. \nFour-Bone IK Solver \nFigure 2.3.7 illustrates that with chains comprising three or more bones, the bones before \nthe last two bones are all solved using the method described previously. Considering only \nthe bone length, the remaining bone lengths, the distance to the FK chain tip, and the \ndistance to the IK goal, a bone angle can be calculated. Once the IK bone angle has been \ncalculated for Bone 0, then we simply continue down the chain, and Bone 0 can be solved in \nexactly the same way. Bone 2 can then be solved using trigonometry, and Bone 3 is simply \naligned with the target. \nFigure 2.3.7. Applying the method to a four-bone chain. \n",
      "content_length": 1560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": " \n \n \n \n \nN-Bone IK Solver \nAn N-bone chain consists of three categories of problems. Bones 0 to N–3 are all solved \nusing the method described previously of calculating maximum IK and FK bone angles to \nderive an IK bone angle. Bone N–2 can be solved using trigonometry, and Bone N–1 is \nsimply aligned with the goal. \nFunction SolveIKChain( chain ) \nbegin \n    calculate chain target alignment \n \n    for each bone in chain \n    begin \n        apply chain target alignment to bone \n \n        if bone is last bone \n            aim bone at target \n        else if bone is second last \n            use trigonometry to calculate bone angle \n        else \n        begin \n            determine FK bone angle \n            determine maximum FK bone angle \n            determine maximum IK bone angle \n \n",
      "content_length": 797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": " \n \n            IK bone angle = ( FK bone angle / maximum FK bone \nangle ) * \n                            maximum IK bone angle \n        end \n    end \nend \n \n \n \nHandling Extreme Deformation \nIn some cases, the remaining chain length is greater than the bone length plus the distance \nto the chain tip or IK goal. In these cases, the technique described previously of using \ntrigonometry to calculate the maximum FK and IK angles cannot generate a maximum \nangle greater than π. Furthermore, during animation there may be visual artifacts if the \nmax IK bone angles hit this limit. \nAs shown in Figure 2.3.8, if the distance between the bone and the chain tip is greater than \nthe remaining chain length, then trigonometry is used as described previously. Once the \ndistance to the IK goal is less than the remaining chain length, the maximum IK and FK \nbone angle values can be calculated. \nFigure 2.3.8. Handling chains with extreme curvature. \n \n \nThe following pseudocode describes how to calculate the maximum FK bone angle for a long \nchain. \nif( distToFkChainTip > remainingChainLength ) \n{ \n",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": " \n \n       use trigonometry to calculate maximum bone angle \n} \nelse \n{ \n       maxFkBoneAngle = acos( ( boneLength/2 ) / \nremainingChainLength ) \n       maxFkBoneAngle += ( remainingChainLength - \ndistToFkChainTip ) / \nboneLength; \n       } \n \nEffectively, the remaining chain length is applied in an arc around the one position, allowing \nus to define maximum angles greater than π. This technique gives a much greater range of \nangles for the maximum FK/IK bone angle values, while maintaining the important limits \nwhen the chain is extended. \n \nAdditional Details \nWe have only discussed the basic case in this gem. Here are some additional details that \nmay be helpful for specific cases. \nIK/FK Blending \nIn many cases, you will want to turn off IK because it may not be applicable. Simply \ndisabling IK evaluation will cause a visual pop in the pose of your character. A better \napproach is to interpolate your IK pose back to your original FK pose before disabling IK. To \ndo this, you need to keep an IK pose buffer separate from your FK pose buffer. \nAnimated Joint Offsets \nSo far in this gem, we have only considered chains with static local offsets along the local X-\naxis. This solver can readily be extended to support animated joint offsets. \nThe local position offset of a joint can be used as the bone length vector of the parent bone, \nas shown in Figure 2.3.9. The FK bone angle and max FK/IK bone angles are all calculated \nusing this vector and its length, as described in this gem. This ensures that the bones pivot \nin the same position relative to their parent bone and that local position offsets are still \napplied in parent space. Because we are required to know the total length of the chain \nbefore solving, the entire chain‘s length needs to be measured prior to solving for IK. \nFigure 2.3.9. Animated joint offsets. \n \n \n \n",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": " \n \nExtension Limit \nOne concern with applying inverse kinematics to a chain in a game is that often the IK \nsolvers will generate artifacts in the motion of the limb when the limb reaches the limit of its \nextension. The term hyperextension refers to the visual pop that happens when a chain \nreaches the limits of its reach. Fixing this issue is a minor addition to the IK solver \nalgorithm. In the code samples provided with the book, extension dampening has been \nimplemented to show how this can be achieved. \n \nFuture Work \nOur algorithm will not address all cases that you will come across. In the future, we plan to \nextend the system in several ways. \nSliding Joints \nThis IK solver will not currently calculate new local joint position offsets but only calculates \norientation changes for each bone in the hierarchy. For example, it will not extend a \nhydraulic joint to reach the goal. Sliding joints could be implemented using a similar \ntechnique as that used to calculate angles. By defining the FK bone‘s local position relative \nto a slide vector, a new local offset position could be defined by comparing the distance to \nthe FK chain tip and the actual distance to the IK goal. \nJoint Limits \nAside from limiting the overall extension of the chain, it may be required that certain bones \ndo not rotate beyond a certain limit or that they are not free to rotate on any plane. \nBecause this chain is evaluated from top to bottom, it is possible to start imposing limits on \nangles. A simple way to do this is to limit the angle generated for each bone. In this way, \nwe can limit the angle of a bone with respect to its parent. \nDOF Constraints \nCreature limbs are usually made up of a ball joint followed by a collection of hinge joints. In \nthis gem, however, no twisting is ever applied to the chain, meaning that the solver only \nmodifies the pose of the root bone on two axes, and every other joint rotates on only one \naxis. The axis used to modify the bone‘s orientation is assumed to be perpendicular to the \nbone vector and the bone-to-goal vector. Chains with limited degrees of freedom are not \nsupported in the algorithm presented in this gem. If an axis of motion were defined for a \nparticular bone, then solving would still be feasible by calculating the maximum bone angles \nprojected onto the plane defined by this axis. \nGeneral-Purpose Programming on Graphics Processors \nThe algorithm presented in this gem does not employ any recursion, or major branching, \nand has a fixed cost per evaluation. These features make it an ideal candidate for \nimplementations using some of the newer SIMT architectures, such as CUDA or OpenCL. \nPerhaps, as in the case of shaders, specific versions of the solver would be generated for \ncategories of chains and the loops unrolled. A two-, three-, and four-bone version could be \ngenerated and the loops unrolled at compile time. Depending on the bone count, a chain \nwould be solved in a batch with all other limbs of the same structure. This would avoid \ncostly synchronization points, which would slow all chain evaluations to the cost of the \nlongest chain. \n \n",
      "content_length": 3134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": " \n \nConclusion \nIn this gem, I have presented a new method for solving inverse kinematics on chains of \nbones that is simple, fast, and accurate. This is achieved by calculating bone angles directly \nusing values derived from the initial pose of the chain and the position of the IK goal. This \nmethod minimizes the modification of the original chain‘s pose, while ensuring the goal is \nalways reached if within range. There are many possibilities for further development to \nexpand this concept beyond a simple chain solver. \n \nReferences \n[Forsyth04] Forsyth, Tom. ―How to Walk.‖ 2004. Game Tech. n.d. <http://www.game-\ntech.com/Talks/HowToWalk.ppt>. \n[Lander98] Lander, Jeff. ―Oh My God, I Inverted Kine!‖ Game Developer. (September \n1998): 9–14. \n[Melax06] Melax, Stan. ―The Shortest Arc Quaternion.‖ Game Programming Gems. Boston: \nCharles River Media, 2006. 214–218. \n \n2.4. Particle Swarm Optimization for Game Programming \nDario L. Sancho-Pradel \ndariosancho@gmail.com \nThis gem presents the foundations of Particle Swarm Optimization (PSO), a simple yet \npowerful meta-heuristic optimization technique that can be applied to complex non-linear \nproblems, even in the absence of a precise analytical formulation of the system. As a result, \nPSO is used in a variety of engineering applications, such as artificial neural network \ntraining, mechanical design, and telecommunications. I have also applied this technique to \nrobotic systems. PSO is related to other population-based search strategies, such as Genetic \nAlgorithms (GA), and can solve similar problems. However, PSO works natively with real \nnumbers and tends to be more efficient than GA, often reaching a near-optimal solution in \nfewer function evaluations [Hassan05]. Next-generation consoles are pushing the \nboundaries of realism and complexity in game development. Often, games contain large \nsets of parameters that drive or influence the behavior of various systems, such as physics, \nanimation, or AI. More often than not, the final values of many of these parameters are the \nresult of a manual selection process based on experience and trial and error. Under certain \nbasic conditions, PSO can be a valuable tool for better tuning these parameters in an \nautomated fashion. \nA Few Words on Optimization \nOptimization problems are typically modeled as finding the maximum or minimum value of \na set of m-dimensional real functions, whose variables are normally subject to constraints. \nMaximizing a function f is equivalent to minimizing the function –f and vice versa. Using \nvector notation, the optimization problem is reduced to finding the roots (solutions) of the \nvector function: \nEquation 1  \n",
      "content_length": 2683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": " \n \n \n \nwhere, for the general case of p non-linear equations in m variables: \nF(X) = [f1 (X),..., fp (X)]T, X = x1,..., xm]T and 0 = [0,...,0]T \nIn some cases, Equation (1) may be solved analytically, which means that the optimum \npoint (or set of points) can be calculated exactly. In most real applications, however, this is \nnot possible, and numerical methods are applied to find an approximate solution. \nNumerical Methods \nNumerical methods are algorithms that return a numerical value that in most cases \nrepresents an approximation to the exact solution of a problem. Classical optimization \ntechniques have been successful at solving many optimization problems common in industry \nand science. However, there are optimization scenarios that classical approaches cannot \nsolve in a reasonable amount of time. Combinatorial optimization problems—in other words, \nproblems with a discrete set of solutions from which we want to find the optimal one—and \nNP-hard problems in general are good examples. For those cases, approximate solutions \ncan be obtained relatively quickly using meta-heuristic approaches (that is, high-level \nstrategies used for guiding different heuristics in search problems). \nNo Free Lunch (NFL) Theorem for Optimization \nOptimizing can be regarded as a search problem where the selected optimization method \nrepresents the particular style or mechanism of executing the search. The NFL theorem \n[Wolpert97] states that for finite spaces and algorithms that do not resample points, the \nperformance of all search (optimization) algorithms averaged over all possible objective \nfunctions is the same. In practice, however, we do observe that some algorithms perform, \non average, significantly better than others over certain objective functions. This is because \nthe objective functions considered in most real optimization scenarios have a structure that \nis far from random. As a result, algorithms that exploit such a structure will perform, on \naverage, better than ―uninformed‖ search strategies (for example, a linear sequential \nsearch). The PSO search strategy is based on an exchange of information between members \nof a swarm of candidate solutions and operates under the assumption that ―good solutions‖ \nare close together in the search space. If this is not the case, PSO will not perform better \nthan a random search. \n \nThe PSO Paradigm and Its Canonical Formulation \nPSO is a stochastic, population-based computer algorithm modeled on swarm intelligence. \nThe PSO paradigm was originally developed by Dr. Eberhart and Dr. Kennedy [Kennedy95], \nan electrical engineer and a social psychologist, respectively, who had the idea of applying \nsocial behavior to continuous non-linear optimization problems. \nThey reasoned that social behavior is so ubiquitous in the animal kingdom because it \noptimizes results. Social behavior is the result of an exchange of information within the \nmembers of the society. The original PSO algorithm evolved from a bird flocking simulator \ninto an optimization tool motivated by the hypothesis that social sharing of information \namong members of the society offers an evolutionary advantage. In order to implement and \nexploit that flow of information, each member was provided with a small amount of memory \nand a mechanism to exchange some knowledge with its social network (in other words, its \nneighbors). Although different improvements and variations have been proposed since the \n",
      "content_length": 3467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": " \n \ninitial PSO algorithm was presented, most of them resemble closely the original formulation. \nA significant exception is the Quantum PSO (QPSO) algorithm [Sun04], which for reasons of \nspace will not be covered here. \nCanonical Equations of Motion \nThe way PSO works is by ―flying‖ a swarm of collision-free particles over the search space \n(also called the problem space). Each particle represents a candidate solution of the \noptimization problem, and each element (dimension) of the particle represents a parameter \nto be optimized. The movement of each particle in the swarm is dictated by the equations of \nmotion that the particular flavor of the PSO algorithm defines. In its canonical form, these \nequations can be expressed as: \nEquation 2  \n \n \nwhere x(t) and V(t) represent respectively the particle‘s position and velocity at time t. By \nchoosing Δt = 1, t becomes the iteration step. Note that x(t) and V(t) are m-dimensional \nvectors. At t = 0, the position of each particle is selected from a uniform random \ndistribution, i.e. xj (0) = U(xj min, xj max), j = 1,2,...m, where xj min, xj max are the \nsearch space boundaries for the j-th dimension. V(0) can either be randomly initialized or \nset equal to the zero vector. \nVelocity Update \nAs Equation (2) shows, the search process is driven by the velocity update, based on: \n \nCognitive information. Experience gained during the particle‘s search, expressed \nas the best location ever visited by the particle xCognitive_Best. \n \nSocially exchanged information. The best location found so far by any member of \nthe particle‘s social network (xSocial_Best). The calculation of xSocial_Best depends on the \nchosen network‘s topology. Figure 2.4.1 shows some common topologies. In a fully \nconnected network (for example, Star), xSocial_Best represents the best location found \nby any particle in the swarm, hence called global best (gBest). If the swarm is not \nfully connected xSocial_Best becomes the best position found by any particle in its local \nneighborhood, hence called local best (IBest). Using IBest tends to provide more \naccurate results, whereas algorithms running gBest execute faster [Engelbrecht02]. \nThe neighborhood is typically defined based on the particles‘ indices, although spatial \ninformation could also be used. Allowing neighborhood overlapping helps the \ninformation exchange across the swarm. \nFigure 2.4.1. Three classical social network structures: (a) Star topology \n(fully connected), (b) Ring topology, (c) Cluster topology. \n",
      "content_length": 2530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": " \n \n \n \nInertia. Provides certain continuity to the motion of the particle. Adding inertia to \nthe velocity update helps to decrease the change in momentum between two \nconsecutive iterations and provides a means of controlling the balance between the \nparticle‘s exploration and exploitation. An exploratory strategy tends to direct the \nparticle toward unexplored areas of the problem space. Exploitation refers to the \nmovement of a particle around previously explored areas, resulting in a finer-grain \nsearch on these areas. Clearly, low inertia will favor exploitation, whereas higher \nvalues will result in a more exploratory behavior. \nIn general, the PSO strategy is to use the cognitive and social best positions as attractors in \nthe search space of the particle. It achieves that by defining the particle‘s velocity vectors \nas: \n \nwhere ω∊(0,1). r1 and r2 are random m-dimensional vectors taken from a uniform random \ndistribution, in other words, ri = {ri\nj ∊ U(0,1)}, j = 1,2,...,m, i=1,2. The values of the two \nacceleration coefficients are normally chosen to be the same (typically cCognitive = cSocial ∊ \n(0,2.05]). It is important to notice that here the operator ⊗ denotes a per-component \nmultiplication of two vectors, and therefore its result is another vector. \nFinally, the equations of motion for the basic canonical PSO system can be expressed as: \nEquation 3  \n \n \nAlgorithm 1 details the basic canonical PSO formulation implementing the gBest strategy, \nand Figure 2.4.2 illustrates graphically the position and velocity update of one particle based \non the equations of motion presented in Equation (3). \nAlgorithm 1: Canonical PSO (<<eq32.pdf>> version) \n 1: // Create a swarm of N randomly distributed particles \n 2: FOR EACH particle(P) i=1,...,N do \n 3:          FOR EACH dimension j=1,..,M do \n 4:              P[i].X[j] = Xmin[j]+rand(0,1)*(Xmax[j]- \nXmin[j]); \n",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": " \n \n 5:              P[i].V[j] = rand(0,1)*Vmax[j]*sign(rand(0,1)-\n0.5);// or =0; \n 6:          END \n 7: END \n 8: g_Best = P[0]; \n 9: numIterations = 1; \n10: // Iterative optimisation process \n11: REPEAT \n12:    FOR EACH particle(P) i=1,...,N do \n13:          IF Eval(P[i].X) BETTER_THAN Eval(g_Best.X) \n14:              g_Best = P[i]; \n15:          END \n16:          IF Eval(P[i].X) BETTER_THAN Eval(P[i].x_best) \n17:              P[i].x_best = P[i].X; \n18:          END \n19:    END \n20:    // Apply_Equations_of_Motion \n21:    FOR EACH particle i=1,...,N do \n22:          FOR EACH dimension j=1,..,M do \n23:              V_Inertia = w*P[i].V[j]; \n24:              V_social = c1*rand(0,1)*(g_Best.X[j] - \nP[i].X[j]); \n25:              V_cognitive = c2*rand(0,1)*(P[i].x_best[j] - \nP[i].X[j]); \n26: \n27:              P[i].V[j] = V_Inertia + V_social + V_cognitive; \n28:                     Clamp(P[i].V[j]); // optional \n29:              P[i].X[j] = P[i].X[j] + P[i].V[j]; \n30:                     Clamp(P[i].X[j]); // or any other \nstrategy \n31:          END \n32:  END \n33:  numIterations++; \n34: UNTIL (m_numIterations > MAX_NUM_ITER || \nGOOD_ENOUGH(Eval(g_Best))) \n \nFigure 2.4.2. Illustration of the velocity and position updates of a particle during \nthe optimization of a simple 2D parabolic function f(x,y). (Left) Geometrical \nillustration showing contour lines of f(x,y). (Right) Sketch of f(x,y) illustrating the \nparticle’s relevant information and labels. \n",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": " \n \n \n \nThe position update may take the particle outside of the predefined boundary. In fact, it has \nbeen proven that in high-dimensional swarms, most of the particles will leave the search \nspace after the first iteration [Helwig08]. Some strategies to constrain the particles to the \nsearch space include: \n \nMoving the particle to its closest boundary and setting its velocity to zero. \n \nAssuming a cyclic problem space. For instance, for ε < | x j max – x j min |, if x j \n(t) = x j max + ε, then it is recalculated as x j (t) = x j min + ε . \n \nAllowing the particle to leave the boundary, omitting its evaluation if the objective \nfunction is not defined at the new coordinates. \n \nReinitializing all components whose values are outside of the search space. \nEvaluating the Particles: The Objective Function \nThe objective function, also referred to as the fitness function or cost function, does not \nneed to be the actual mathematical representation of the system to be optimized, but a \nmeasure of the quality of any solution found. Therefore, each call to Eval(P[i].X) in \nAlgorithm 1 requires the simulation of the system using the parameters encoded in P[i]. \nDuring the simulation, various indicators related to the performance of the solution are \nrecorded and combined in a function to provide a numerical measure of performance. This \nfunction is the objective function. \nImagine that we want to optimize the parameters that define the way an NPC performs a \ncertain task, such as handling a car. Most likely, we do not have a dynamic model of the car \nand its controllers, so we cannot directly optimize the non-linear system of equations that \ndefines the problem at hand. Nevertheless, by defining an objective function that penalizes, \nfor instance, the time the NPC spends outside the training path and the time required to \ndrive a predetermined distance, we are effectively giving the particles valid feedback to \nevaluate their quality, and therefore a means of improving in later iterations. \n \nAdd-Ons to the Classical Formulation \nBeyond the basic formulation, we can add a few improvements to deal with specific cases \nthat may come up in a particular use case. \nVelocity Clamping \n",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": " \n \nThe formulation described so far may generate particles with increasingly large oscillations \naround a potential optimum due to an ―explosion‖ in the velocity values. This issue was \naddressed in [Kennedy01] by the introduction of a positive velocity clamping parameter \nVmax, which modifies the velocity update as follows: \n \nwhere V j culated (t + 1) is the j-th component of the velocity vector calculated using \nEquation (3). The selection of Vmax is problem dependent. Typically, each of its components \nis initialized within the size of the search space boundaries, in other words, V j max = κ · | x j \nmax – x j min |, κ ∊ (0,1). Initially, we want κ to be close to 1 in order to favor exploration. \nDuring the optimization process, Vmax can be periodically adjusted by: \n \nSetting Vmax to the dynamic range of the particle. \n \nUsing linearly or exponentially decaying Vmax in order to progressively move toward \nexploitation (fine-grained searches) of the area around the best particle(s). \n \nIncreasing or decreasing Vmax after α iterations without improvement in the best \nfound solution. \nVariable Inertia Weight ω(t) \nThe inertia coefficient has a clear influence in the particle‘s search strategy. Large values of \nω favor exploring new areas, while small values favor the exploitation of known areas. \nInitially, we would like the algorithm to favor exploration, and as we approach the vicinity of \nthe optimum, we could decrease the value of ω in order to perform finer-grain searches. \nThere are many ways to define a decreasing ω(t) . A common one is a linear interpolation \nbetween the desired values of ωInitial and ωFinal : \n \nwhere t is the current iteration and T represents the number of iterations required to reach \nωFinal, keeping ω(t) = ω,Final for t > T. Another strategy is to select T = MAX_NUM_ITER/N, \nresetting ω(t) and applying again the interpolation every N iterations. \nConstriction Coefficient \nBased on the dynamic analysis of a simplified and deterministic PSO model, Kennedy and \nClerc [Clerc02] reformulated the velocity update in terms of the constriction coefficient χ. \nThey also analyzed the convergence properties of the algorithm, obtaining a set of \ntheoretical optimal values for the PSO parameters to control both convergence and the \nvelocity explosion. A simple constriction model for velocity update can be expressed as: \nV (t + 1) = χ · [V(t) + φ1 · (X Cognitive_Best – X (t)) + φ 2 · (X Social_Best – X(t))] X (t + \n1) = X(t) + V(t + 1) \nwhere X is a diagonal matrix, whose elements are calculated as: \n",
      "content_length": 2563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": " \n \n \nand: \nφ = φ1 + φ2, φ > 4, x jj ∊ (0,1), κ ∊ (0,1) \nLow values of κ encourage exploitation, whereas high values result in a more exploratory \nsystem. Typical values are χjj = 0.7298 and θ1 = θ2 = 2.05. Notice that this model can be \neasily converted into the original inertia-based formulation. \nExtended Full Model for Velocity Update \nThe velocity update could include both the global best and the local best particle: \nV (t + 1) = ω · V(t) + Θ 1 ⊗ (X Cognitive._Best – X(t)) + Θ 2 ⊗(g Best – X(t)) + Θ 3 ⊗ \n(IBest; – X(t)) \nwhere typically Θ i ∊ U(0,1). \nAdding Constraints \nIn an optimization scenario, the constraints define feasible subspaces within the search \nspace of the unconstrained problem. In the original PSO algorithm, each dimension is \nbounded individually. (See Line 4 of Algorithm 1.) This translates into an axes-aligned \nhyper-rectangular search space. Therefore, constraints such as g (x) ≤ b cannot be \ngenerally defined. As a result, neither a simple circular search space (x2 + y2 ≤ radius 2), \nnor a combination of constraints (for example, a rectangular search space with a \nrectangular hole inside), is supported. Different approaches have been proposed to extend \nthe constraint-handling capabilities of evolutionary algorithms. (For a review, see \n[Michalewicz96] and [Mezura09].) A simple approach presented in [Hu02] uses the \ncanonical PSO algorithm and introduces two modifications: \n \nAll particles are initialized inside the feasible region. \n \nAfter evaluating the population, only particles within the feasible region can update \nthe values of xCognitive_Best and xSocial_Best. \nInteger Optimization \nAlthough PSO deals natively with optimization problems of continuous variables, it can also \nbe modified to cope with discrete and mixed-parameter problems. The simplest way to do \nthat is to use the same equations of motion, but round the results of the integer \ncomponents (at least for the position update) to the nearest integer. In some cases this can \nresult in invalid particles (for example, duplicated elements in a particle are generally not \nallowed in permutation problems), and particles need to be ―fixed‖ after every position \nupdate. The fixing mechanism is problem dependent, but it may consist of making all the \nelements of a particle different, while keeping their values within a certain range. \nMaintaining Diversity \nA common problem in numerical optimization is premature convergence—in other words, \ngetting trapped around a local optimum. In stochastic methods, the diversity of the \npopulation has a major effect on the convergence of the problem; hence, diversity loss is \ncommonly regarded as a threat to the optimization process. Some ways of favoring diversity \nare: \n",
      "content_length": 2744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": " \n \n \nAlternating periodically PSO with other search methods. \n \nEvery α iterations, reinitializing (randomizing) the location of a random number of \nparticles in the swarm. \n \nEvery β stalls, reinitializing the system. \n \nIncluding mutation and crossover operators and applying them to the position of a \nsmall subset of particles in the swarm. \n \nRunning multiple independent swarms, allowing an occasional exchange of \ninformation (for example, gBest) and/or particles between swarms. Furthermore, each \nswarm could use different search strategies (for example, some could use IBest, some \nothers could use gBest, some could use parameters that favor exploration, some \nothers could use more exploitative ones, and so on). \nIt is important to notice that the selected networking strategy also has an impact on the \ndiversity of the population. Distributed topologies, such as Ring or Cluster, generate a \nnumber of local attractors and tend to maintain higher levels of diversity, leading to good \nsolutions more often than other centralized approaches. \n \nA Note on Randomness \nPSO is a stochastic optimization technique that initializes its particles by placing them \nrandomly in the search space. In order to do so, pseudo-random values are drawn from a \nuniform distribution. The reason is that in the absence of any prior knowledge about the \nlocation of the optimum, it is best to distribute the particles randomly, covering as much of \nthe problem space as possible. However, as Figure 2.4.3(a) shows, the result may be less \nuniform than desired. The spatial uniformity of the randomized particles can be improved by \nusing Halton point sets, which are uniformly distributed and stochastic-looking sampling \npatterns, generated by a deterministic formula at low computational cost [Wong97]. \nFigure 2.4.3. One instance of two sets of 25 particles initialized in a two-\ndimensional space using (a) a uniform random distribution and (b) a Halton \nsequence. \n \n \n \n \nCase Study 1: Typical Benchmarking Functions \n",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": " \n \nThere are sets of functions that are commonly used for benchmarking optimization \nalgorithms. These functions tend to have areas with a very low gradient (for example, \nRosenbrock‘s saddle) and/or numerous local optima (for example, Rastrigin‘s function). \nFigure 2.4.4 illustrates the 2D versions of three of them. The accompanying CD includes the \nmultidimensional version of a larger set of benchmarking functions together with a PSO \nalgorithm implementation. You are encouraged to experiment with the different PSO \nparameters, observing the variation in convergence speed and accuracy. More information \nabout different benchmarking functions can be found in [Onwubolu04]. \nFigure 2.4.4. 2D versions of some common benchmarking functions. \n \n \n \nCase Study 2: Optimization of Physical Parameters for In-Game \nVehicle Simulation \nTo obtain realistic vehicle responses in a game, simplified physical models of their key \ncomponents are generated. These models contain a number of physical parameters that \ndefine the way the vehicle reacts in many situations. Those parameters are often obtained \nbased on experience and a trial-and-error process of selection. Such a manual process \ntends to be inevitably slow, and as a result, little of the search space is often covered. It is \nclear that there is room for improvement, and PSO is a good candidate for automating the \nsearch process. The only prerequisite is to be able to estimate numerically the quality of \neach candidate solution. The following example illustrates the general principle. We want to \noptimize the performance of a car, whose simplified model is shown in Figure 2.4.5. The \nmodel includes friction between the tires and the ground, the suspension, and the car‘s \nmass distribution. Based on the model‘s parameters, the particle‘s components can be \ndefined as: \nP.X[0] = Forward Friction Coeff.; P.X[1] = Side Friction Coeff.; \nP.X[2] = Spring Constant (Rear); P.X[3] = Damper Constant \n(Rear); \nP.X[4] = Spring Constant (Front); P.X[5] = Damper Constant \n(Front); \nP.X[6] = Mass 1; P.X[7] = Mass 1 location (along Z); P.X[8] = \nMass 2; \n \nFigure 2.4.5. A simplified physics model of a car. \n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": " \n \n \n \nAs an example of dimensionality reduction, the value and location of the mass M3 and the \nlocation of the mass M2 have been fixed. They can be included at any moment in the \noptimization by extending the dimensionality of the particle. \nMost importantly, we need to define an objective function capable of quantifying the \nresponse of the vehicle while following a path along a network of roads. The objective \nfunction to be minimized could include the following variables: distance to goal on a crash, \ntotal distance driven without being on the marked spline, accumulated distance from the \ncar‘s center of mass to the marked spline (shown as d in Figure 2.4.6 (b)), accumulated \nheading deviation with respect to the selected path, oscillations around the Z-axis, and tilt \naround the Z-axis on curves (c.f. Figure 2.4.6 (a)). The function can also include big \npenalties for crashing and in cases where only two wheels are in contact with the road. The \nfinal objective function could be a quadratic combination of each of these variables ri. \n \nFigure 2.4.6. Illustration of some of the parameters included in the objective \nfunction. (a) Tilt around the Z-axis during a curve. (b) Distance to the path’s \nsplines and heading error. \n \n \n",
      "content_length": 1252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": " \n \nThe normalization constant ηi ensures that all the variables have the same weight (typically \nbetween 0 and 1), regardless of their particular ranges. The relative importance of ri, with \nrespect to the rest of the variables, is represented by the coefficient ki, which can also be \nused for scaling the objective function. Squaring ri is useful in cases where only the \nmagnitude of ri is relevant, ensuring that the objective function does not decrease based on \nthe sign of the variables. \nAs Algorithm 1 showed, each iteration requires the evaluation of the whole population (in \nother words, N particles). Therefore, it would be convenient to evaluate N vehicles \nsimultaneously following identical paths. \n \nCase Study 3: Physics-Based Animation of Mechanical Systems \nPhysics-based animation can improve the gaming experience by adding physical realism and \nbelievability to the game. Cloth and hair procedural animation, ragdolls, and general \ncollisions between rigid bodies are some examples featured in many titles. Extending this \nconcept to complex mechanical systems, such as robotic NPCs and vehicles, can result in \nmore realistic behaviors that are in accordance with our experience of how mechanical \nsystems move. \nFigure 2.4.7 shows a hexapod NPC robot exhibiting a tripod gait, which is a gait pattern \noften used by insects and other animals due to its performance and stability. Gait patterns \ncan be defined by a series of parameters (for example, maximum joint angles, feet \ntrajectory, stance phase offset, stance duty factor, step duration) whose values will depend \non the physical characteristics of the robot, the environment, the pattern configuration, the \ntask (for example, turn, go forward, climb, and so on), and the task qualifiers (for example, \nfast, safe, and so on). PSO can be used for finding good values for these parameters. Each \ngait could be optimized individually by using specialized objective functions and running \nsingle-gait simulations. Next, gait transitions could be worked out. Once the optimization is \nfinished, the result would be a hexapod robotic NPC with newly acquired locomotion skills. \nFigure 2.4.7. Snapshot of a hexapod NPC robot using a tripod gait. The legs shown \nin white are lifted, while the darkened ones are in stance. \n \n \nThe design of the robot can be taken one step further. The configuration of the legs could \nbe optimized by adding the length of their segments and the number of joints into the \noptimization‘s parameters list. Ultimately, PSO could be used as a tool for evolving different \ntypes of robots by including the number of legs and their position in body coordinates as \noptimization parameters. \nOnce the robot design is finished and the gait parameters optimized, the gait controller of \nthe robot would send the appropriate sequence of actions to the different joints, and the \nphysics engine would do the rest. Incidentally, the controller of each joint (typically some \n",
      "content_length": 2976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": " \n \nflavor of proportional-integral-derivative controller, or PID) could also be optimized using \nPSO. \nLet‘s see one final example. We want to find the parameters that generate the fastest \nwalking gait for the particular robotic NPC illustrated in Figure 2.4.8.Figure 2.4.8 (b) shows \nthe feet trajectory selected for this gait. This half-ellipse trajectory is defined by two \nparameters, namely the step height (h) and the step size (s). Another important parameter \nis the duration of the step (Ts). To improve stability, the torso of the robot is initially tilted \nsideways to shift its center of mass toward the supporting leg (refer to Figure 2.4.8 (a)). \nThis can be parameterized by a maximum tilt angle (α) and the percentage of step time that \nthe body is tilted toward the leg in stance (Tt) before shifting the weight to the other side. \nAssuming a 50-percent duty cycle for each leg, the structure of the particles could be \nP.X[]= {h,s,Ts,α,Tt}. \nFigure 2.4.8. Illustration of some basic parameters in a humanoid-like robot. (a) \nWeight-shifting angle for improving stability, (b) half-ellipse gate parameters, (c) \nthe different joint angles in a crouching action. \n \n \nThe objective function could be designed as in Case Study 2, taking into account the \nwalking time, the distance to the goal before falling, the accumulative heading error, the \naccumulative position error with respect to the given path, and the stability of the gait \nmeasure in terms of oscillations around various axes. It is worth noticing that the gait \nparameters obtained are subject to external factors, such as the friction between the robot‘s \nfeet and the ground. In principle, it could be possible to find a continuous mapping between \nthe walking parameters and different types of terrains by feeding the friction data and the \nPSO solutions to a neural network or a fuzzy inference system. In that way, the robotic NPC \ncould try to adapt its gait to optimize the walking performance when the type of terrain \nchanges. \nGaits are not the only movements that could be optimized. Learning other actions, such as \ncrouching (refer to Figure 2.4.8 (c)) or climbing stairs would follow the same principles. \n \nConclusion \nThe aim of meta-heuristic optimization algorithms is to provide approximated solutions to \ncomplex non-linear optimization problems that more traditional approaches have difficulties \n",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": " \n \naddressing. These algorithms are not guaranteed to converge at a good solution, but they \nare designed to find good approximations to the global optimum with high probability. \nPopulation-based algorithms are sensitive to the diversity of their individuals and to their \nconfiguration parameters. PSO relies on a small set of intuitive parameters, such as the \nnumber of particles, the maximum number of iterations, and the topology of the social \nnetwork. Often, the mapping between an optimization problem and the function that \nquantifies the quality of a solution is not unique. In these cases, the quality of the selected \nobjective function has a significant impact on the convergence of the algorithm. This gem \nhas shown a number of variations proposed around the canonical PSO algorithm. The \nsimplicity of the PSO paradigm makes its extension a relatively simple task. \nGames are becoming extremely complex and highly parameterized software products. In \nmany cases, the response of different systems is driven, or at least influenced, by sets of \nparameters stored in configuration files. PSO could be a useful tool for optimizing some of \nthese parameters in an automated fashion. AI, physics, and animation are examples of \nsystems that could benefit from this optimization technique. \n \nReferences \n[Clerc02] Clerc, M. and J. Kennedy. ―The Particle Swarm-Explosion, Stability and \nConvergence in a Multidimensional Complex Space.‖ IEEE Transactions on Evolutionary \nComputation 6 (2002): 58–73. \n[Engelbrecht02] Engelbrecht, A.P. Computational Intelligence: An Introduction. Wiley, 2002. \n[Hassan05] Hassan, R., B. Cohanim, and O. de Weck. ―A Comparison of Particle Swarm \nOptimization and the Genetic Algorithm.‖ Proceedings of 46th AIAA/ASME/ASCE/AHS/ASC \nStructures, Structural Dynamics and Materials Conference. 2005. \n[Helwig08] Helwig, S. and R. Wanka. ―Theoretical Analysis of Initial Particle Swarm \nBehavior.‖ Proceedings of the 10th International Conference on Parallel Problem Solving \nfrom Nature (Sept 2008): 889–898. \n[Hu02] Hu, X. and R. Eberhart. ―Solving Constrained Nonlinear Optimization Problems with \nParticle Swarm Optimization.‖ 6th World Multiconference on Systemics, Cybernetics and \nInformatics (SCI 2002): 203–206. \n[Kennedy95] Kennedy, J. and R. Eberhart. ―Particle Swarm Optimization.‖ Proceeding of \nIEEE International Conference on Neural Networks 4 (Dec. 1995): 1942–1948. \n[Kennedy01] Kennedy, J., R.C. Eberhart, and Y. Shi. Swarm Intelligence. Morgan Kaufmann \nPublishers, 2001. \n[Mezura09] Mezura-Montes, E. Constraint-Handling in Evolutionary Optimization. Springer, \n2009. \n[Michalewicz96] Michalewicz, Z. and M. Schoenauer. ―Evolutionary Algorithms for \nConstrained Parameter Optimization Problems.‖ Evolutionary Computation 4 (1996): 1–32. \n[Onwubolu04] Onwubolu, G. and B. Babu. New Optimization Techniques in Engineering. \nSpringer, 2004. \n[Sun04] Sun, J., B. Feng, and Wenbo Xu. ―Particle Swarm Optimization with Particles \nHaving Quantum Behavior.‖ Proc. Cong. Evolutionary Computation 1 (June 2004): 325–331. \n",
      "content_length": 3072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": " \n \n[Wolpert97] Wolpert, D.H. and W.G. Macready. ―No Free Lunch Theorems for Search.‖ IEEE \nTransactions on Evolutionary Computation 1.1 (April 1997): 67–82. \n[Wong97] Wong, Tien-Tsin, Wai-Shing Luk, and Pheng-Ann Heng. ―Sampling with \nHammersley and Halton Points.‖ Journal of Graphics Tools 2.2 (1997): 9–24. \n \n2.5. Improved Numerical Integration with Analytical Techniques \nEric Brown \nanalytic.spinors@gmail.com \nThere is a fairly standard recipe for integrating the equation of motion in the context of a \ngame physics engine. Usually the integration technique is based on the Symplectic Euler \nstepping equations. These equations are fed an acceleration, which is accumulated over the \ncurrent time step. Such integration methods are useful when the exact nature of the forces \nacting on an object is unknown. In a video game, the forces that are acting on an object at \nany given moment are not known beforehand. Therefore, such a numerical technique is \nvery appropriate. \nHowever, though we may not know beforehand the exact nature of the forces that act on an \nobject, we usually know the exact nature of forces that are currently acting on an object. If \nthis were not so, we would not be able to provide the stepping equations with the current \nacceleration of the body. If it were possible to leverage our knowledge of these current \nforces, then we might expect to decrease the error of the integration dramatically. \nThis gem proposes such a method. This method allows for the separation of numerical \nintegration from analytic integration. The numerical integration steps the state of the body \nforward in time, based on the previous state. The analytic integration takes into account the \neffect of acceleration acting over the course of the current time step. This gem describes in \ndetail the differences and implications of the integration techniques to aid the physics \ndeveloper in understanding design choices for position, velocity, and acceleration updates in \nphysics simulation. \nAs we build up to the introduction of this method, we will first discuss a heuristic model for \nclassifying errors of integration techniques. \nClassifying Errors \nOften, the method for classifying errors in integration techniques is to label them as first \norder, second order, and so on. Methods that are first order have an upper bound error on \nthe order of the time step taken to the first power. Methods that are second order have an \nupper bound error on the order of the time step to the second power. Taking a small \nnumber to a large power makes the small number smaller. Thus, higher-order methods \nyield more accuracy. \nError can also be classified in terms of how well an integrator conserves energy. Integrators \nmight add or remove energy from the system. Some integrators can conserve energy on \naverage. For instance, the semi-implicit, or Symplectic Euler, method is a first-order \nmethod, but it conserves energy on average. If an integrator adds energy to the system, \nthe system can become unstable and diverge, especially at higher time steps. The accuracy \nof a method can affect its stability, but it does not determine it, as shown by the Symplectic \nEuler method. More often than not, it is the stability of a method that we desire, more than \nthe accuracy. \n",
      "content_length": 3292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": " \n \nIn this gem we will be taking a different approach to classifying error. This approach is \nbased on the fact that the stepping equations usually assume that the acceleration is \nconstant over the course of a time step. The kinematics of constant acceleration are a \nproblem that can be solved easily and exactly. Comparing the kinematic equations of \nconstant acceleration with the results of a numerical method provides qualitative insight into \nsources of error in the method. \nWhen derivatives are discretized, it is done by means of a finite difference. Such a finite \ndifference of positions implies that the velocity is constant over the course of the time step. \nIn order to introduce a non-constant velocity, you must explicitly introduce an equation \ninvolving acceleration. Similarly, the only way to deal with non-constant acceleration is to \nexplicitly introduce an equation involving the derivative of acceleration. Since many \nnumerical methods do not involve any such equation, we are safe in making the comparison \nwith the kinematic equations for constant acceleration, at least over the course of a single \ntime step. \n \nKinematics of Constant Acceleration \nWe know the exact form of the trajectory of particles that are subject to a constant \nacceleration. \n \nWe can compare this set of equations to the results of common numerical methods in order \nto gain a qualitative idea about the error in the method. Consider the standard Euler \nmethod: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn Δ t \nThis set of equations can be transformed into a set that more closely resembles the \nkinematic equations by inserting the velocity equation into the position equation: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn-1 Δ t + an-1 Δ t2 \nThe appearance of vn-1 in the position equation is due to the fact that we must insert vn and \nmust therefore re-index the velocity equation. The differences in form of this equation to \nthe kinematic equations can be considered as qualitatively representative of the error of the \nmethod. \nWe may perform this same procedure with the Symplectic Euler method: \nvn+1 = vn + an Δ t \nxn+1 = xn + vn+1 Δ t \nInsert the velocity equation into the position equation to transform to the kinematic form: \n",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": " \n \nvn+1 = vn + an Δ t \nxn+1 = xn + vn Δ t + an Δ t2 \nThis equation is a much closer match in form to the kinematic equations. We can use this \nresemblance to justify the fact that the Symplectic Euler method must in some way be \nbetter than the standard Euler method. \n \nThe Kinematic Integrator \nIf we are trying to find an integration method that when converted to a kinematic form is \nidentical to the kinematic equations, why not just use the kinematic equations as the \nintegration method? \n \nIf we do this, then we are guaranteed to get trajectories that are exact, within the \nassumption that acceleration is constant over the course of the time step. However, the \naccelerations we are usually interested in modeling are not constant over the course of the \ntime step. We must use a value for the acceleration that encapsulates the fact that the \nacceleration is changing. \nWe could use the acceleration averaged over the time step, \n, as the constant \nacceleration value. By inserting the average acceleration into the kinematic equations, we \nachieve a method that we will refer to as the average acceleration method. In order to \ncalculate this average exactly, we must analytically integrate the acceleration over the time \nstep, which in many instances can be done easily. The average acceleration method \ntherefore represents a blend between numerical and analytic integration. We are \nnumerically integrating the current position and velocity from the previous position and \nvelocity, but we are analytically integrating accelerations that are acting during the current \ntime step. \nOf course, calculating the average acceleration exactly requires that we know how to \nintegrate the particular force in question. Luckily, most forces that are applied in game \nphysics are analytic models that are easily integrated. Calculating the average acceleration \nfrom an analytic model of a force is usually just as easy as calculating the acceleration at an \ninstant of time. \nIf the average acceleration is calculated analytically, then the velocity portion of the \nkinematic equations produces exact results. However, the position portion would require a \ndouble integral in order to achieve an exact result. If the forces that we are dealing with \nfollow simple analytic models, then calculating a double integral is usually just as easy as \ncalculating a single integral. \nWe will generalize the idea of the average acceleration method in order to introduce the \nkinematic integrator. The kinematic integrator is a set of stepping equations that allow for \nexact analytic calculation of both the velocity integral and the position integral. \nvn+1 = vn + dv \n",
      "content_length": 2670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": " \n \nxn+1 = xn + vn Δ t + dx \nThe exact method uses the following definitions for dv and dx: \n \nIf we are using the average acceleration method, then we define dv and dx as: \n \nIn the case of constant acceleration, it is very easy to perform both the single and the \ndouble integral. The integral contributions of a constant acceleration are given as: \n \nIf there are multiple forces acting on a body, we can express the integral contributions as a \nsum of contributions that are due to each force. \ndv = dv1 + dv2 + … \ndx = dx1 + dx2 + … \nThus for the kinematic integrator we accumulate dv‘s and dx‘s rather than accelerations. All \nforces acting on a body provide contributions to dv and dx. The amount that is contributed \nis dependent on the nature of the force and can usually be calculated exactly. If all forces \nacting on a body are integrable, then every contribution is exact. \nThe kinematic integrator can be used to perform the Symplectic Euler method with the \nfollowing integral contributions: \ndv = an Δ t \ndx = an Δ t2 \nThis method is useful if the acceleration is not integrable (or if we are too lazy to calculate \nthe integrals). These contributions are not going to be exact, but they will at least conserve \nenergy, which will maintain stability. \n",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": " \n \nThe kinematic integrator does not represent a specific integration method, but rather the \nability to isolate the portions of the stepping equations that actually require integration. \nSince the method used to evaluate the integral contributions is not explicitly specified, we \nhave a degree of freedom in choosing which method might be best for a particular force. For \ninstance, if there is a contribution from a constant gravitational force, then we can easily \nuse the exact method. We will see that for some forces we will want to use the average \nacceleration method. Or we could use the Symplectic Euler method if the force in question is \ntoo complicated to integrate or if we are performing a first pass on the implementation of a \nparticular force. \n \nIntegral Contributions Due to a Spring Force \nProbably one of the most common forces, next to a constant uniform force, is a spring \nforce. The spring force is proportional to the displacement of the spring from equilibrium. \nThis results in the following equation of motion: \n \nThe solution of this equation of motion is analytically solvable and is given by: \nx – l = A cos (ωt) + B sin(ωt) \nv = –A sin(ωt) + ωB cos(ωt) \nUsing this exact trajectory, we can determine the integral contributions: \n \nwhere c and s represent the cosine and sine, respectively, of ω(t). We could represent this \ncalculation as a matrix operation. \n \nThe components of this matrix depend on the size of the time step Δt, the mass of the body \nm, the strength of the spring k, and the equilibrium position of the spring l. The components \nof the matrix can be cached and reused until any of these parameters change. In many \ninstances, these parameters do not change for the lifetime of the spring. Thus, the \ncalculation of the integral contributions of a spring is relatively trivial—in other words, six \nmultiplies and four adds. \nWe can consider that the spring that we have been discussing is anchored to an infinitely \nrigid object at the origin, since we have only taken into account the action of the spring on a \nsingle body, and the spring coordinates are the same as the body coordinates. It is only \n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": " \n \nslightly more complicated to calculate the integral contributions due to a spring that \nconnects two movable bodies. \n \nMultiple Forces \nBefore discussing the integral contributions of other possible forces, we need to discuss \nwhat happens when multiple forces act on a body. \nIf all of the forces acting on a body depend only on time, then the result of accumulating \nexact integral contributions will be exact. But consider the case where at least one of the \nforces depends on the position of the body, such as the spring force. \nThe integral contribution of the spring takes into account the position of the body at \nintermediate values as the spring acts over the course of the time interval. However, the \ncalculation is not aware of intermediate changes in position that are due to other forces. The \nresult is a very slight numerical error in the resulting trajectory of the particle. \nAs an example, consider two springs that are acting on the same body. For simplicity, both \nsprings are attached to an infinitely rigid body at the origin, and the rest length of both \nsprings is zero. The springs have spring constants k1 and k2. The acceleration becomes: \na = –(ω1)2 x – (ω2)2x \nIf we are to handle these forces separately, then we would exactly calculate the integral \ncontributions of two springs, with frequencies ω1 and ω2, and accumulate the results. \nThe springs can also be combined into a single force. \n \nIn this case, only a single integral contribution would be calculated. It might be surprising to \ndiscover that these two methods produce different results. The second method is exact, \nwhile the first contains a slight amount of numerical error. This seems to imply that: \n∫(a1 + a2)dt ≠ ∫ a1dt + ∫ a2dt \nwhich is usually not true. However, in the current circumstance, if the acceleration is a \nfunction of the position, which in turn is a function of the acceleration, then the integrals \nhave feedback. The effect of this feedback is that we cannot separate the sum into \nindependent integrals, since the independent integrals will not receive feedback from each \nother. \nIf the acceleration only depends on time, there is no feedback, and the integrals can safely \nbe separated. Because of this, you may want to separate out forces that depend only on \ntime and accumulate their integral contributions as a group. The integral contributions of \nthese forces can be safely added together without introducing numerical error. \nUnfortunately, most of the forces that are applicable to game physics depend on the \nposition of the body. \nThough the error due to multiple forces is incredibly small, it represents a tiny violation of \nenergy conservation. If this violation persists for long enough, then the trajectory can \n",
      "content_length": 2755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": " \n \neventually diverge. The error due to multiple forces is initially much less than the error in \nthe Symplectic Euler method, but it can eventually grow until it is out of control. \nSince we can integrate exactly the system with two springs, we can use this example to \ngauge the error present in the different methods for calculating the integral contributions. \nResults of this error calculation are contained in the figures. The first figure contains the \nresults of integrating one spring; the second figure represents errors due to integrating two \nsprings. The integration takes place over one half of the period of the two-spring system. \nThe Symplectic Euler method as well as the pulse method (which will be introduced later) \nboth conserve energy on average. Neither the average acceleration method nor the exact \nmethod conserve energy over long intervals of time. It takes quite a while, but the exact \nmethod will eventually diverge. The average acceleration method will eventually dampen \nout. For this reason, the average acceleration method is preferred if the integration is going \nto take place over a long interval. \nFigure 2.5.1. The relative errors in the case of one spring and two springs. \n \n \nIntegral Contributions of a Pulse \nOne solution to the problem due to multiple forces is to approximate the action of the force \nas a pulse. A pulse is a force that acts very briefly over the course of the time step. A \nperfect pulse acts instantaneously. Pulses acting at the beginning of the time step do not \ndepend on intermediate values of position or velocity; therefore, pulses are immune to the \nmultiple-force problems. \nConsider a rectangular pulse. The area under the curve of a rectangular pulse is dv = adt, \nwhere dt is the width of the pulse, and a is the magnitude of the pulse. It is possible to \nshrink the width of the pulse while maintaining the same area. To do this, we must, of \ncourse, increase the magnitude of the pulse. If we allow the pulse width to go to zero, the \nheight diverges to infinity. However, the product of the width and the height is still equal to \ndv. In the case where dv = 1, there is a special name for this infinite pulse. It is called a \ndelta function, and is denoted as δ(t). The delta function is zero at all values of t, except at \nt = 0. At t = 0, the delta function is equal to infinity. Integrating the delta function is very \neasy to do. \n \n",
      "content_length": 2418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": " \n \nfor all values of a and b where a < 0 < b. If the interval between a and b does not contain \nzero, then the result of the integral is zero. \nTo represent the force as a pulse, we define the acceleration as: \na = dvδ(t) \nHere, dv is the area under the curve of this instantaneous pulse, as previously described. \nIntegrating this acceleration is easy to do: \n∫ adt = dv \nIntegrating the position contribution is now just integrating a constant. \n \nWe can choose dv as the amount of acceleration we want to pack into the pulse. If we want \nto pack all of the acceleration due to an arbitrary force into the pulse, then we use the \naverage value of the acceleration \n. This produces stepping equations of the form: In a \nmore familiar form, the stepping equations for the pulse would appear as: \n \nIn a more familiar form, the stepping equations for the pulse would appear as: \n \nIf you make the assumption that the acceleration is constant over the time interval, then \nyou can replace \nwith an. Doing this, you will arrive back at the stepping equations for \nthe standard Symplectic Euler method. This gives meaningful insight into the Symplectic \nEuler method. It is a method that assumes that the acceleration is constant over the course \nof the time step and delivers all of the acceleration in an instantaneous pulse at the \nbeginning of the time step. Since pulses can be accumulated without introducing error, the \nSymplectic Euler method can be considered to be an exact solution to forces of an \napproximated form. Since the method is exact, it intrinsically conserves energy. The error in \nthe method is entirely due to the differences between the actual form of the force and the \napproximate form of the pulse. \n \nIntegral Contributions of Collision Forces \nOne very common thing that needs to be done in a game physics engine is to resolve \ncollisions. To calculate our collision, we will assume that the forces due to the collision act \n",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": " \n \nwithin some subinterval that fits entirely within our given time interval. The nature of the \nforce does not matter as much as the fact that it operates on a very small timescale and \nachieves the desired result. \nFor simplicity, we will assume that the object has normal incidence with a plane boundary. \nWe will also assume that the collision with the boundary is going to take place within the \ntime interval. \nTo handle the collision correctly, we should sweep the trajectory of the body between xn and \nxn+1 and find the exact position and the exact time where the collision begins to take place. \nIf we want to find the projected position at the next time step xn+1, we should wait to apply \nthe collision force until all other forces have been applied, so that the projected next \nposition is as accurate as possible. \nFor the purpose of simplification, we will consider that the trajectory is a straight line that \nconnects the position xn with xn+1. The position of the body at some intermediate value of \nthe time step is given by: \nXf = Xn(1–f)+Xn+1(f) \nwhere f is the fraction of the time step that represents the moment of impact. The \nintermediate velocity is simplified as well, to be: \nVf = Vn(1–f)+Vn+1(f) \nWith this physically inaccurate yet simplified trajectory, we can perform a sweep of the body \nagainst the collision boundary. The result of this sweep will determine the time fraction f. \nThis will tell us the position and velocity (almost) of the body at the moment of impact. To \ndetermine the force that is applied in order to resolve the collision, we need to apply a pulse \nforce that reflects the component of velocity vf about the contact plane. This reflection \nincorporates the interaction of the body with the contact surface; thus, the result should \nincorporate surface friction. For now, we will assume that there is no friction. \nTo apply the pulse force, we will begin by applying a constant force over a small subinterval \nof the time step. The subinterval begins at time fΔt and ends at (f + df)Δt. The velocity of \nthe body at the end of the subinterval is given by: \nvf + df = vf + avdf \nAccording to our initial simplification, the velocity is aligned with the normal of the contact \nplane. Thus, all of the velocity is reflected. We choose av to enact this reflection. \n \nThe integral contributions of this force in the limit that df → 0 are given by: \ndvv – 2vf \ndxv = 0 \nSince this force pulse does not contribute anything to the position integral, the application \nof this force does not change the value of xn+1 from what it would have been. Thus, the \nvalue of xn+1 will still violate the collision constraint. \n",
      "content_length": 2667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": " \n \nWe need to apply a second force pulse, which moves the position of the body out of the \ncollision surface. Assuming that the body has normal incidence to the plane, then the \namount the body needs to be pushed is given by Δx = xf – xn+1. Again, we are going to \napply a constant force on a subinterval. The acceleration of this force ax is determined by \nconsidering the kinematic equations for constant acceleration. \n \nThe integral contributions for this acceleration as df → 0 are: \n \nAdding these two sets of contributions together gives the final result for the collision force. \nThis set of contributions can be expressed in terms of the position at the beginning and end \nof the interval, as well as the parameter f, which represents the moment when the collision \nfirst begins to happen. \n \n \nIntegral Contributions of Viscous Forces \nA viscous force is a force that is related to, and usually opposing, the velocity of an object. \nF = –KV \nThe equation can be solved for the velocity in terms of the constant k and the mass m. This \nis done by expressing the acceleration as the derivative of velocity. \n \nThis equation can be integrated to get: \n \n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": " \n \nThe integral contributions of this force can be determined exactly and are given by: \n \nIt may very well be sufficient to approximate the exponent function with the first-order \nTaylor series expansion. Since the force dissipates energy, exactness for the sake of \naccuracy is not required. \n \nViscosity can be applied in an anisotropic fashion, meaning that the viscosity constant k can \nactually be a vector \n. The viscosity force then contains the dot product of \nwith vn. \nA simple method of introducing surface contact friction is to generate an anisotropic viscous \nforce in the event that a collision is determined. The viscosity in the direction of the collision \nsurface normal can be tuned separately from the components in the surface plane in order \nto decouple the restitution of reflected velocity with sliding friction. Of course, this general \nviscosity force does not accurately model dry friction, where there is a transition from static \nto kinetic friction. But it is a good place to start. \n \nIntegral Contributions of Constraint Forces \nMany physics engines offer a variety of constraints on objects. We will not calculate the \nintegral contributions of every possible constraint, but rather suggest a general mechanism \nof determining resolution forces to enforce that constraints are satisfied. In many physics \nengines, collisions are handled as constraints. The pattern for evaluating the resolution \nforces for a collision can apply to all constraints. This general pattern is as follows. \n1. Approximate the trajectory and find the value of f, which represents the moment \nwhen the constraint violation began. \n2. Forces are applied as pulses. Using the Jacobian of the constraint equations to \ndetermine the direction of the force, the magnitude is calculated in order to bring the \nvelocity vn+1 into compliance with the constraint. \n3. If needed, another force pulse is applied in the same direction in order to bring the \nposition vn+1 into compliance. \n4. The contributions for both pulses are added together. \n \n",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": " \n \nSummary \nWhen using the kinematic integrator, the state of a body is defined by the position xn and \nvelocity vn at the beginning of the current time step tn. \nThe state of a body is stepped forward in time using the kinematic integration equations: \nvn+1 = vn + dv \nxn+1 = xn + vn Δt + dx \nThe quantities dv and dx represent the portions of the stepping equations that can be \nanalytically integrated per force and accumulated. \nThe integral contributions can be calculated using the exact method as: \n \nwhere the inner integral of dx is indefinite, and the outer integral is a definite integral. \nThe integral contributions for the average acceleration method are: \n \nThe integral contributions for a pulse are: \n \nIf desired the integral contributions of the Symplectic Euler method are defined as: \ndv = an Δt \ndx = an Δt2 \nThese different methods can be mixed, depending on the desired results. When there is \nmore than one force applied that depends on position, then the average acceleration \nmethod can be much more accurate than the exact method. \nThe acceleration contributions for the spring with a spring constant k and a natural length l \nare: \n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": " \n \n \nThe contribution of a viscous force, with a viscosity constant v, is: \n \nFor a collision response force, the integral contributions are defined as: \n \nwhere f is the fraction of the time step when the collision begins. \nFor a general constraint, the generic framework is as follows: \n1. Approximate the trajectory and find the value of f, which represents the moment \nwhen the constraint violation began. \n2. Forces are applied as pulses. Using the Jacobian of the constraint equations to \ndetermine the direction of the force, the magnitude is calculated in order to bring the \nvelocity vn+1 into compliance with the constraint. \n3. If needed, another force pulse is applied in the same direction in order to bring the \nposition xn+1 into compliance. \n4. The contributions for both pulses are added together. \n \nConclusion \nUsing a well-chosen mix of numerical integration and analytic integration, it is possible to \nachieve exact trajectories for some force models. If there are multiple forces applied, error \nmay accumulate, since the analytic integration of individual forces cannot take other forces \ninto account. Using the average acceleration method for calculating the integral \n",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": " \n \ncontributions can result in relative errors that are millions of times smaller than the errors \nfrom the Symplectic Euler method alone. \nWe have seen that the Symplectic Euler method can be thought of as an exact method, \nwhich approximates the force as pulses. The errors in this method are due to this \nmisrepresentation of the force. \nThis gem has demonstrated the fact that the kinematics of simple physical models, which \nare prevalent in game-related physics, can be leveraged to dramatically reduce and \nsometimes eliminate the error of integration methods. \nGoing forward, work on this topic might include determining the integral contributions due \nto the different flavors of translational and rotational constraints. Also, determining whether \nthere is a way to pre-accumulate elements of the acceleration integrands prior to \nintegration would provide a very natural solution to the problems that arise because of \nmultiple forces that depend on position. \n \n2.6. What a Drag: Modeling Realistic Three-Dimensional Air and Fluid \nResistance \nB. Charles Rasco, Ph.D., President, Smarter Than You Software \ncharlie@smarterthanyou.com \nBasic physics simulations have been around in games for quite a while. But beyond simple \ngravity, simple 2D collisions, and not-so-simple 3D collisions, there is a lack of further \nrefinement of object motion. Drag physics is one area that has not been adequately \naddressed. Most games simulate drag physics with a simple linear model, if they simulate it \nat all. This gem demonstrates and contrasts two different mathematical models that \nsimulate drag: a linear drag model and a quadratic drag model. The quadratic drag model is \nmore realistic, but both models have applicability to different situations in physical \nsimulations for many different types of games. The gem also defines and explains relevant \nparameters of drag physics, such as the terminal velocity and the characteristic decay time. \nThe linear three-dimensional drag problem and both the one-dimensional linear and parts of \nthe quadratic drag problems are discussed in [Davis86]. This gem adds a stable \nimplementation of quadratic drag in three dimensions. The integrals used in this article may \nbe found in any calculus book or on the web [Salas82, MathWorld]. \nGames could use improved drag physics for many things: artillery shells, bullets from \nsmaller guns, golf ball trajectories, car racing games (air resistance is the main force that \nlimits how fast a car can go), boats in water, and space games that involve landing on alien \nplanets. Even casual games could use improved drag physics. The quadratic drag model in \nthis gem was developed to make the iPhone and iPod Touch app Golf Globe by ProGyr. In \nthis game, the user tries to get a golf ball onto a tee inside a snow globe. Originally the \ngame was designed with linear water resistance, but it did not feel quite right. This was the \nmotivation to improve the water resistance to the more realistic, and more complicated, \nquadratic model. I have a real golf ball in a snow globe and have never been able to get it \nonto the tee. To be honest, I have never gotten the ball on the tee in the Golf Globe game \non the 3D eagle level, but in the game the easy levels are much simpler because the easier \nlevels are two-dimensional. \nPhysics \n",
      "content_length": 3333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": " \n \nThis section provides the general mathematics and physics background for both the linear \nand the quadratic drag models. \nThree-Dimensional Physics \nThe forces for the linear model are: \n \nAnd the forces for the quadratic model are given by: \n \nwhere \nis the velocity squared, and \nis \nthe unit vector that points in the direction of the velocity. \nThe first term of both equations is the force due to gravity, and in my representation down \nis the negative z direction. The direction of gravity depends on your coordinate system. The \nsecond term of both equations is the drag term. I chose the alpha and beta as different \nlabels for the drag coefficient in order to keep it clear which version of air resistance I am \nreferencing. For both models, if this value is big, there is a lot of fluid resistance, and if it is \nsmall, there is little fluid resistance. This parameter should always have positive value and \ndoes not actually need to remain constant. It can change if an object changes shape or \nangle from the direction of the fluid motion. A sail in the wind does not have a constant \nvalue, but changes depending on how taut the sail is and how perpendicular it is relative to \nthe wind, among other factors. The velocity in these equations is the velocity of the object \nin the fluid, so if there is wind, then it would be the velocity of the object minus the velocity \nof the wind. \nThe linear equation is solvable in a complete analytical closed-form solution. Since the \nquadratic mixes all of the components of the velocity vector together, it is not solvable in a \nclosed form, but we can solve it numerically. In addition, the quadratic drag model has the \nadvantage of being a better approximation of realistic air and fluid resistance. Interestingly, \nboth are exactly solvable when considered as one-dimensional problems. This will be \ndiscussed in more detail later in this gem. \nConversion from Three-Dimensional Physics to One-Dimensional Physics \nTo convert the drag problem from three dimensions into one dimension, we need to break \nthe object‘s motion into two components: motion along the velocity (one dimension) and \nmotion perpendicular to the current velocity (two dimensions). The drag force only acts \nalong the current direction of velocity; thus, we can treat it as a one-dimensional problem. \nThe unit vector in the direction of the velocity is given by \n, or if \nthe velocity is zero it is valid to set \n. Setting the normal to zero works because if the \nobject is not moving, there will be no fluid resistance. The components of the forces along \nthe direction of motion and perpendicular to the direction of motion are shown in Figure \n2.6.1. We can update the velocity along the current direction of motion as well as \nperpendicular to it once it is decomposed this way. \nFigure 2.6.1. Breaking three-dimensional space into components along the \ndirection of motion and perpendicular to the current direction of motion. \n",
      "content_length": 2969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": " \n \n \n \n \nOne-Dimensional Solutions \nFor a one-dimensional solution, we start from the one-dimensional version of Newton‘s \nSecond Law, F = MA, and from the definition of acceleration, a = dv/dt. If the force is a \nfunction of the velocity alone, then these two equations can be combined to get time as a \nfunction of velocity, as shown here: \n \nThe force equations that are relevant to the present matter are F = mg cos(θ)– αv, the \nlinear fluid resistance equation, and F = mg cos(θ)±βv2, the quadratic fluid resistance \nequation. The cosine in these equations is the cosine of the angle between the current \nvelocity and the vertical. For the linear equation, the negative sign in front of the velocity \nterm means the force always opposes the direction of motion. If the velocity is positive, \nthen the force is in the negative direction. If the velocity is negative, then the force is in the \npositive direction, which is taken care of automatically by the linear equation. In the \nquadratic equation, the different sign must be applied based on which way the object is \ntraveling, since squaring the velocity loses the directional information. \nFor the linear equation, we have: \n \nwhere vt1 = mg cos(θ)/α is the terminal velocity. There is one other variable that describes \nthe motion, and that is the characteristic time, η1 = m/α. The characteristic time is a \nmeasure of how quickly (or slowly) the object speeds up or slows down to the terminal \nvelocity, and it shows up in the equations after we integrate. The formula for the quadratic \ndrag model‘s terminal velocity and characteristic time is different than the linear drag \nmodel‘s characteristic time and terminal velocity. The linear drag models are labeled with \nsubscript 1s. Another way to calculate the terminal velocity is to set the force equation to \n",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": " \n \nzero and solve for the velocity. This integral is evaluated with a change of variables x = 1 – \nv/vt1, so that dx = – dv/vt1. \n \nLastly, we solve this somewhat intimidating equation for vf, which turns out nicely as: \nEquation 1  \n \n \nIf you feel like practicing taking limits, see what happens when the air resistance coefficient \ngoes to zero. The traditional vf = v0 + g cos(θ)t pops up. The first few terms in this \nexpansion are useful if you want to approximate the exact solution in order to avoid the \nexpensive exponential function calls. \nFor the quadratic case, we solve the equations in the same way. We start with the general \nequation: \n \nFrom this equation we define the terminal velocity, \n, and the \ncharacteristic time is (not obvious from the equation) \n, which \nshows up after we integrate. Integrating this equation is straightforward. Although related, \nthe plus version and the minus version are different integrals, and we need to evaluate \nboth. \nFirst, let us do the integral when the terminal velocity term (labeled with subscript 2s for \nthe quadratic model) and the velocity square terms have the same sign. This is the case \nwhere the force of gravity and the fluid resistance are in the same direction, which happens \nwhen the particle is traveling up and both forces are pointing down. \nTo keep track of the overall sign relative to the velocity direction, we introduce the variable, \nsign, which is either 1 or –1, on the outside of the integral. This is important to get the \ncorrect overall sign compared to the direction of the velocity unit vector, \n. We continue to \nsolve for the time as a function of initial and final velocity to obtain: \n \nNow we solve this equation for the final velocity and obtain: \nEquation 2  \n",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": " \n \n \n \nSecondly, we solve for the case where the terminal velocity and velocity squared terms \nhave opposite signs: \nThere is a relevant connection between the inverse hyperbolic tangent and the natural log \nof the form: \n \nThe absolute value bars from the integral are important in order to get the equation to \ninvolve the inverse hyperbolic tangent correctly. We must answer whether the initial and \nfinal velocities are bigger than the terminal velocity. In addition, given that the initial \nvelocity is less than the terminal velocity, is the final velocity necessarily less than the \nterminal velocity? It turns out that it is (see the solution below), and it is acceptable to \nconvert both natural logarithms to inverse hyperbolic tangents. A similar question can be \nasked if the initial velocity is greater then the terminal velocity. \nSo there are four different cases with which we need to deal: V0 <Vt2, V0 > Vt2, V0 = Vt2, and \nVt2 = 0. \nFor the first case V0 < Vt2: \nand solving for the final velocity: \nEquation 3a  \n \n \nFor the second case V0 > Vt2: \n",
      "content_length": 1068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": " \n \nand solving for the final velocity: \nEquation 3b  \n \n \nFor the third case, v0 = vt2, where the object begins traveling at the terminal velocity in the \nsame direction as the force of gravity, we get: \nEquation 3c  \n \n \nThere are several ways to arrive at this result. We can take a limit of both of the first two \ncases, Equations (3a) and (3b), or if we look at the force in the quadratic force equation, it \nis zero, hence the object must travel at a constant velocity. \nAnd lastly is the case when the terminal velocity is identically zero, Vt2 = 0. It is easiest to \nstart from the original equation and solve like the previous cases. \n \nAs previously, we solve this for the final velocity: \nEquation 3d  \n \n \nAll right, this is all the information we need to solve for the linear and quadratic three-\ndimensional cases, and we now transition to solving the three-dimensional linear case. \n \nSolution of the Three-Dimensional Linear Case \nWe could solve the linear three-dimensional case in a similar manner as we solve the \nquadratic version. However, we choose to treat it as three separate one-dimensional \n",
      "content_length": 1118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": " \n \nproblems because the results can be used by the AI in your game. The previous equations \nare modified to suit each of the Cartesian directions x, y, and z. Again, gravity is in the \nnegative z direction. The solutions of the three different velocity equations are: \nvxf = vx0 exp(–t/η1) \nvyf = vy0 exp(–t/η1 \nand \nvzf = (vz0 – vtl)exp(– t/η1) + vtl \nwith vtl = –mg/α and η1 = m/α. The terminal velocities in the horizontal direction are zero. \nIf we integrate these equations with respect to time, then we get the position as a function \nof time. \nx = vx0 η1 (1 – exp(–t/η1) + x0 \ny = vy0 η1 (1 – exp(–t/η1)) + y0 \nand \nz = (vz0 – vtl)η1(1 – exp(–t/η1)) + vtlt + z0 \nThe complete distance equations are useful for AI targeting, AI pathfinding, and calculating \ntargeting info for players, without actually propagating the solutions forward in time step by \nstep. In general, the distance equations are well approximated for most video games with x \n= x0 + vt, since the time for most games is very small and is much faster than updating \nwith the complete position equations. \n \nSolution of the Three-Dimensional Quadratic Case \nThe three-dimensional quadratic equations are \n, \n. It is impossible to solve these as \nthree independent equations since they are all coupled by the quadratic term of the fluid \nresistance. One way to solve this is to break the motion into two directions, along the \ncurrent velocity and perpendicular to the current velocity, and then update the velocity \nbased on the force calculated. \nAlong the direction of motion, the force is Fparallel = –mgnz – βv2, where nz is the z \ncomponent of \nand is equal to negative of the cosine of the angle between the direction of \nmotion \nand the gravity vector. Notice that the drag term is always negative since it is in \nthe opposite direction as the current velocity. \nPerpendicular to the direction of motion, the forces are \n. If the velocity is either straight \nup or straight down, nz = ±1 and nx = ny, there is no component of force perpendicular to \nthe current velocity. And if the velocity is completely horizontal, the perpendicular force is \ncompletely in the negative z direction. If you have a different coordinate system, this \nequation will look slightly different. \n",
      "content_length": 2257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": " \n \nFor the motion along the velocity normal, there are three different cases to consider: nz> 0, \nnz< 0, and nz= 0. If nz > 0, the object is moving upwards, the opposite direction from \ngravity, and the fluid resistance is in the same direction as gravity. If nz< 0, then the object \nis moving in the same direction as gravity, and the fluid resistance force is in the opposite \ndirection as gravity. Lastly, if nz= 0, then the object is moving horizontally, and gravity has \nno effect on horizontal motion. \nFor nz> 0, the relevant solution is Equation (2) with \n, \n, and sign = – 1 The velocity is: \n \nFor nz < 0, the relevant solution is Equation (3a), (3b), (3c), or (3d) depending on the \ncurrent velocity relevant to the current terminal velocity with \n, \n, and sign = – 1. The velocity update is: \n \nor \n \nNotice that the characteristic time for the last instance is undefined, but it is never used in \nthe solution, so there should not be a problem. \nFor the force perpendicular to the initial velocity, the updated velocity is the same as \nEquation (3) with v0= 0 and its own distinct terminal velocity, \n, and characteristic time, \n. \nThis leads to: \n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": " \n \n \nFor most cases this can be approximated (since the characteristic time is usually large and \nthe update time is usually small for games) as: \nEquation 4  \n \n \nThis last equation should look familiar. So usually there is no need to calculate the separate \nterminal velocity or the characteristic time for the perpendicular motion if the mentioned \napproximations are valid. \nThere is one issue that occurs when a slow-moving object changes from traveling upward to \ngoing downward, as shown in Figure 2.6.2. If this case is not handled, then the object \nappears to go back in the direction it came from, when in fact it would stop traveling in the \nhorizontal direction and travel down along the direction of gravity. \nFigure 2.6.2. A bad thing that can happen for slow-moving particles and/or large \nupdate times. \n \n \nThere are several ways to handle this. The most straightforward approach is to propagate \nthe object for the time it takes to get to the top with Equation (2) and then for the rest of \nthe time propagate with Equation (3a). From Equation (2), we can calculate the time it \ntakes to stop moving in the positive velocity direction. The result is: \nEquation 5  \n",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": " \n \n \n \nIf the time to the top is less than the total time to propagate, then subtract the time to the \ntop from the total time to propagate. Then use the downward velocity update for the time \nremaining and use zero as the initial velocity. There might be a slight discontinuity in the \nobject‘s smooth motion on the screen, but usually there is not. If there is too much, then \nupdate the position in a more rigorous manner. \nThe last thing to consider is how to update the position. All of these equations are \nintegrable, which is more accurate, but they provide little extra accuracy noticeable in a \ngaming environment and are computationally expensive to evaluate. For most game \npurposes, it is accurate enough to use Euler integration to calculate the position Xf = X0 + \nVt, where the time, t, in the equation is the time since the last update and is not the global \ntime. For the Golf Globe game on an iPod Touch, the game runs smoothly with no visible \nnumerical issues at 60 frames per second by updating the position using Euler integration. \n \nPseudocode \nThis is the pseudocode for a function that calculates the final velocity, velocityFinal, \ngiven the initial velocity, velocityInitial, of an object by the time, deltaTime. This \ndoes not update the position of the object, which should be updated separately but with the \nsame deltaTime. \nvoid updateVelocityWithFluidResistance( \n      const float deltaTime, \n      const ThreeVector& velocityInitial, \n      ThreeVector& velocityFinal ) \n{ \n      Calculate Velocity Normal or zero it. \n      Calculate terminal velocity and characteristic time along \nvelocity \nnormal. \n \n      For objects going up use Equation (2), but first check \nto make sure the velocity does not reach the top \nEquation (5). \n \n      For objects going down use either Equation (3a), (3b), \n(3c), or (3d) as is appropriate to the initial velocity at the \nbeginning of the frame. \n \n      Update the velocity perpendicular to the initial velocity \nwith \nEquation (4). \n} \n \n \n",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": " \n \n \nComparison of Linear versus Quadratic Solutions \nThere are benefits to using either the linear version or the quadratic version. The \ncomputational advantages lie with the linear solution. The quadratic computational cost is \nhardly prohibitive, especially if there are not too many objects updated with the quadratic \nmodel. And the quadratic model looks and feels much nicer, especially in a game where the \nuser is getting feedback from movement due to handheld motion, such as from the iPod \nTouch or another handheld device. \nThere are several advantages to the linear solution: There are exact velocity and position \nequations for all time, which is useful for AI and targeting; it is faster computationally; and \nthe exponent in the solution is easily expanded, making it only minutely more \ncomputationally intense than updates with gravity only. \nOne disadvantage to the linear solution is that it is not as realistic, especially when \ncompared directly to the quadratic case. \nThere are some advantages to using the quadratic solution: It is more realistic and it is not \nhugely more computationally intense (as long as there are not too many objects). There are \nseveral disadvantages to the quadratic solution: It is computationally more intense, \nespecially if there are many objects; there is no analytic solution for all time, thus it is not \nas easy to use for AI and player targeting; and its approximations are more difficult and are \nmore numerically touchy to the input drag parameters. \nThere is a possible way to balance the two approaches if there are too many objects to \nupdate with the quadratic drag model. Use the linear model for the vast majority of these \nobjects and then use the quadratic for the more important, or highly visible, objects. \nFor both the linear and the quadratic solutions, it is a good approximation to use Euler \nintegration to update the object position. It will save big on computational cost if there are \nmany objects, especially for the quadratic case. \n \nConclusion \nThe concept of drag is simple: Slow things down in the opposite direction they are moving. \nHowever, it is difficult to implement in a numerically accurate and stable manner. The \nsimple and mathematically accurate linear drag model as presented here would be an \nimprovement to many current games. The mathematically and physically accurate quadratic \nmodel is an additional improvement. There are many areas of simulation in games that \nwould visibly benefit from using the mathematically and physically accurate quadratic drag \nmodel instead of the linear drag model: boats, sailboats (both with respect to the water and \nthe wind in the sails), parachutes, car racing, snow-globe games, golf ball trajectories, craft \nflying through atmospheres, and artillery shells. \nAlong the lines of this gem, there are several areas that would be interesting to research \nfurther. Physically based drag coefficients depend on all sorts of factors, including \ntemperature, density, type of fluid, and the shape of the object. For spacecraft entering an \natmosphere, several of these things are relevant and can make for some interesting \ngameplay while flying into land or chasing AIs or other players in a multiplayer game. \n \n",
      "content_length": 3251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": " \n \nReferences \n[Davis86] Davis, A. Douglas. Classical Mechanics. Academic Press, Inc., 1986. \n[MathWorld] ―Wolfram Math World.‖ n.d. <http://mathworld.wolfram.com/>. \n[Salas82] Salas, S. L. and Einar Hille. Calculus: One and Several Variables with Analytic \nGeometry: Part 1. John Wiley and Sons, 1982. \n \n2.7. Application of Quasi-Fluid Dynamics for Arbitrary Closed Meshes \nKrzysztof Mieloszyk, Gdansk University of Technology \nkrzysztof.mieloszyk@gmail.com \nPhysics in real-time simulations as well as in computer games has been gaining importance. \nThe game producers have observed that the advanced realism of the graphics is not enough \nto keep players content and that the virtual world created in a game should follow the rules \nof physics in order to behave more realistically. Until now, such realism has been mainly \nreflected by illustrating the interactions between the material objects. However, the \nphenomenon of reciprocity between such media as liquid or gas and more complex objects \nis also of high importance. With increasing processor power, the growing number of physics \nsimulation methods that were initially developed purely for scientific reasons are being \nutilized in the entertainment market. However, simulation methods commonly used in \ngames tend to be relatively simple in the computational sense or very focused on a specific \ncase. An attempt to adapt them to other game variants is often impossible or requires an \napplication of complicated parameters with values that are difficult to acquire. Hence, some \nmore universal methods that allow for more easily modeled game physics are valuable and \nattractive tools for hobbyists and creators of later game extensions. \nRequirements for the Mesh Representing the Object \nCommonly, the medium is a physical factor that in various ways affects the complete \nsurface of the object. Hence, for the purpose of fluid dynamics simulation, every shape \nneeds to be represented as a closed body with all its faces pointing outward and fully \ncovering its outside surface. The inside should be looked upon as full; it must not have any \nempty spaces or ―bubbles.‖ Every edge should directly contact exactly two surfaces, while \nthe entire object mesh must fulfill the following condition: \nEquation 1  \n \n \nwhere V= vertices, F= faces, and E= edges. \nBecause the preliminary interaction value is calculated on points, it is convenient to \norganize the mesh in a list of points in the form of coordinates and a list of triangles \nrepresented as three sequential indices of its vertices. Additionally, every triangle should \nhave a normal vector of its surface, as well as the surface value. In some cases, depending \non the computational method used, the position of the triangle center can also be useful. In \nthis article, we will sometimes refer to vertices as points and triangle meshes as faces. \n",
      "content_length": 2878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": " \n \n \nPhysical Rudiments \nUsing some physical basics of fluid mechanics (referring among others to Pascal‘s Law), \npressure is defined as a quotient of force F, which exercises perpendicularly to the surface \nsector limiting the object given, and the surface S of this sector [Bilkowski83]. \nEquation 2  \n \n \nThe total pressure affecting the body surface in a given medium point can be represented \nby a sum of two components: static and dynamic pressure. \nEquation 3  \n \n \nIn order to define the static pressure that is needed to create buoyancy in a selected \nmedium, we need the medium density ρ[kg/m3], gravitational field intensity g [m/s2], and \nthe distance to the medium surface h. (In the case of air, the altitude is needed.) \nEquation 4  \n \n \nDynamic pressure originates from the kinetic energy pressure of the medium, which results \nfrom its movement and can be determined by knowing the medium density ρ as well as the \nfluid velocity V [m/s] at a selected position (applicable for subsonic speed). \nEquation 5  \n \n \nEquation (5) can be extended with a nondimensional pressure coefficient cp, whose value \ncan be calculated as a cosine of the angle between the normal vector of the surface sector \nand the vector of the actual velocity in a selected point (see Figure 2.7.1). This can also be \nrepresented as a scalar product of the normal surface and the normalized velocity vector. \nFigure 2.7.1. Normal vector of the surface and normal vector of velocity and the \nangle between them. \n",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": " \n \n \n \nEquation 6  \n \n \nAs an effect, we obtain a general equation for dynamic pressure exerted on a segment of \nthe surface shown in Equation (7). The method presented in this gem solves the \ncomputation of dynamic pressure based on velocity and assumes that we use a rigid body. \nThe use of physical models based on joints or springs is also possible. \nEquation 7  \n \n \n \n \nPre-Computation Based on a Triangle Mesh \nThe whole issue of calculating the pressure distribution on a mesh comes down to \ncomputing the values on the mesh vertices (or center of triangles) taking into consideration \nthe immersion, velocity, surface area, and normal of each of the triangles that form the \nobject. This allows us to calculate the force of the pressure that influences the physical \nmodel. Additional problems arise in the case of the object being placed in two different \nmedia simultaneously. Here the division plane separates the object faces into those faces \nthat are totally immersed in only one medium and faces that are located in both of them. \nThe factors requiring serious consideration are characterized in the following section. \nCalculating the Distance to Medium Boundary \nFor calculations of the static pressure affecting the object, it is necessary to define the \ndistance h between the selected point and the medium boundary (for example, a water \nsurface). For this, the scalar product can be applied, giving us the following Equation (8), \nschematically illustrated in Figure 2.7.2. \nEquation 8  \n \n",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": " \n \n \nFigure 2.7.2. Defining the distance of the point from the surface. \n \n \nAssuming that the normal vector of the surface is pointing upward, the negative value of \nthe obtained distance means that the point is located under the ―water‖ surface, and the \npositive is above its surface. In cases when the boundary is located at the virtual world ―sea \nlevel,‖ we know in which of the two media the point is located. To reduce the possibility of a \ncalculation error, point Psurface (through which the surface dividing the media is crossing) \nshould correspond to the projection of the object‘s center onto this surface, in accordance \nwith its normal vector. \nCalculating the Point Velocity \nBecause the objects simulated in the virtual world are frequently moving, one of the most \nimportant factors needing to be improved in their dynamics is the influence of the \nsurrounding medium. Therefore, it is required to include the velocities of each of the object \nmesh points (Equation (9)) as a sum of the mass center‘s linear velocity Vobj and the \nobject‘s angle velocity (Figure 2.7.3). The velocity Vp obtained this way represents the \nglobal velocity for selected point P. To calculate the dynamic pressure, the local velocity \nvector, which is equal to the negative global point velocity, is required [Padfield96]. \nFigure 2.7.3. Velocity of the arbitrary point belonging to the moving object. \n \n",
      "content_length": 1404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": " \n \n \nWe represent this mathematically as follows: \nEquation 9  \n \n \n \nMorphing Shape Objects \nThe simulated object is not always a rigid body, because some objects move by changing \ntheir body shape. A fish or a bird can be used as an example here because various body \nfragments of those animals move with different velocities in relation to other parts. For such \nobjects, when calculating the point velocity, it is necessary to take the so-called morphing \neffect into consideration. The simplest method here is to find the change between the old \nand new local positions of the mesh points (see Figure 2.7.4), followed by the velocity \ncomputation, keeping the time difference Dt between the morphing animation frames in \nmind. As a result and an extension to Equation (9), we obtain Equation (10), which is \npresented below. \nEquation 10 \n \n \nFigure 2.7.4. Moving vertex position in morphing mesh. \n \n \n \n",
      "content_length": 911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": " \n \nTriangle Clipping \nBecause there is a situation in which the object triangle can be located in both media \nsimultaneously, it is necessary to determine the borderline and find those parts that are \ncompletely submerged in only one of the media. To achieve this, the triangle should be \ndivided into three triangles calculated by cutting points of the edges that cross the medium \nborder. By applying Equation (11), we obtain PAB, PAC, and dividing the triangle ABC, we \ncreate a triangle A PAB PAC, located in one medium, and two arbitrary triangles located in \nthe other medium (for example, B C PAB and C PAC PAB), as presented in Figure 2.7.5. \nFigure 2.7.5. Assigning the cutting points on the surface by edge AB. \n \n \nIt is important to keep in mind that the newly created triangles must have the same normal \nvectors as the primary triangle. Even though each of the new triangles has the same \nnormal, it is still necessary to recalculate the current surface area of each of those triangles. \nIt might be very useful to apply the cross-product property, described in Equation (12), for \nthe exemplary triangle ABC. This surface area can then be used to calculate the pressure \nforce. \nEquation 11 \n \n \nEquation 12 \n \n \n \n \nCalculation of the Pressure on the Triangle Surface \n",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": " \n \nTo calculate the pressure force exerted on the arbitrary triangle of the body, the pressure at \neach of the triangle vertices should be taken into consideration. Hence, we need to calculate \nthe distance of each vertex from the medium surface for static pressure (Equation (6)) and \nthe vertex local velocity, taking the angle between the velocity and the triangle surface \nnormal into account, for dynamic pressure (Equation (7)). In the next step, the average \ncomplete pressure of the triangle vertices is calculated. However, it is useful to know that \nsome simplifications are possible for cases when any mesh vertex has the velocity \ncalculated from ωx r that is considerably smaller than the object linear velocity Vobj or \nwhen ω is close to zero. As a result of this, the pressure gradient distribution is almost \nlinear, and the calculation of the complete pressure exerted on the triangle surface is \nreduced to obtaining the pressure in the triangle center (based on h and Vp). Thus, the \npressure force is a product of a triangle normal, its surface area, and pressure (Equation \n(2)). For models based on a rigid body, calculation of the complete force and the torque \ncomes down to summing the pressure forces from each surface fragment, as well summing \nall torques as pressure force vector cross-products and the forces at the center of the \ntriangles. \n \nSimple Correction of Dynamic Pressure \nThe method for computing the static pressure presented in this gem is based solely on \nsimple physical rules. However, the effect of dynamic pressure is one of the most \ncomplicated problems of the mechanics, for which no simple modeling method exists. The \ncomputational method used here is highly simplified and based on a heuristic parameter cp. \nThis is sufficient for the purpose of physical simulation, which visually resembles the correct \none; however, obtained values strongly diverge from the real values, resulting in too low of \nlift and too high of drag values. Hence, when calculating a force vector obtained from \ndynamic pressure, it is beneficial to introduce a correction in accordance with the velocity \nvector at a selected point. As in Equation (13), the factors used here are determined \nempirically and should be interpreted as recommended, not required. The corrected \ndynamic pressure force vector of the selected triangle has to be summed with the static \npressure force vector occurring on its surface. The results obtained in that way approximate \nthe results of the real object (for example, an airplane) for which the dynamic pressure is \nmost important. \nEquation 13 \n \n \n \n \nConclusion \nThe simplified method of computing the flow around an arbitrary closed body presented \nhere allows us to easily obtain approximate values of forces surrounding a medium. This is \npossible thanks to simulating some fundamental physical phenomena, such as static and \ndynamic pressure. Figure 2.7.6 shows screenshots from an application that interactively \nsimulates real time using the algorithm presented here. This clearly shows a potential \npossibility for use in simulating the behavior of objects with fairly complicated shapes. The \ncontrol here is carried out by changing the interaction between the medium and the object \nobtained, by morphing the shape of the object‘s surface, or by moving or rotating the object \nelements. This allows us to create controlled planes, ships, airships, submarines, hydrofoils, \n",
      "content_length": 3454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": " \n \nseaplanes, swimming surfers, or drifting objects, as well as objects that undergo morphing, \nsuch as birds or fish. All of this is simulated based on the object mesh only. \nFigure 2.7.6. Examples of applications: drifting object (top), static object (middle), \nmorphing object (bottom). (Grayscale marks the pressure distribution in the \nmiddle and bottom images.) \n \n \nThe simplifications used in the algorithm bring some inaccuracies. The main problem here is \nthe lack of consideration for the reciprocal interferential influences for each face on the final \npressure values. This is a consequence of analyzing each face separately, as if it were \nmoving in the flow consistent with its own velocity. In reality, the velocity of the flow at a \ngiven fragment of the body surface depends on the velocity over the adjacent surface \nfragments [Wendt96, Ferziger96]. Dependence of the value on the pressure coefficient \nparameter cp (as angle cosine) is a generalization. In fact, this issue would require some \nmore specialized characteristics. Another drawback of the material presented here is that \nthe medium viscosity has not been taken into consideration, which also adds to inaccuracies \nin the results. These computation versus performance tradeoffs need to be evaluated when \ncomparing real-time methods and those that may be used in fluid dynamics engineering and \ndesign. \nImplementations of the equations in this article motivated by specific examples are \navailable on the CD-ROM. \n \nReferences \n[Bilkowski83] Bilkowski, J. and J. Trylski. Physics. Warsaw: PWN, 1983. \n",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": " \n \n[Ferziger96] Ferziger, Joel H. and Milovan Peri. Computational Methods for Fluid Dynamics. \nBerlin: Springer-Verlag, 1996. \n[Padfield96] Padfield, Gareth D. Helicopter Flight Dynamic. Oxford: Blackwell Science \nLimited, 1996. \n[Wendt96] Wendt, John F. Computational Fluid Dynamics. Berlin: Springer-Verlag 1996. \n \n2.8. Approximate Convex Decomposition for Real-Time Collision \nDetection \nKhaled Mamou \nKhaled_mamou@yahoo.fr \nCollision detection is essential for realistic physical interactions in video games and \nphysically based modeling. To ensure real-time interactivity with the player, video game \ndevelopers usually approximate the 3D models, such as game characters and static objects, \nwith a set of simple convex shapes, such as ellipsoids, capsules, or convex hulls. While \nadequate for some simple interactions, these basic shapes provide poor approximations for \nconcave surfaces and generate false collision detections (see Figure 2.8.1). \nFigure 2.8.1. Convex hull versus approximate convex decomposition. \n \n \nIn this gem, we present a simple and efficient approach to decomposing a 3D mesh into a \nset of nearly convex surfaces. This decomposition is used to compute a faithful \napproximation of the original 3D mesh, particularly adapted to collision detection. First, we \nintroduce the approximate convex decomposition problem. Next, our proposed \nsegmentation technique and performance characteristics will be evaluated. Finally, we \nconclude with some areas of future work. \nApproximate Convex Decomposition \nLet S be a triangular mesh of connectivity k and geometry γ. Intuitively, S is a piecewise \nlinear surface composed of a set of triangles stitched along their edges. The connectivity k is \nrepresented as a simplical complex describing the topology of the mesh. k is composed of a \nset of vertices X = {v1,v2,...,vv} ∊ IN (IN denotes the set of positive integers and V the \nnumber of vertices of k), together with a set of non-empty subsets of X, called simplices \nand verifying the following conditions: \n \nEach vertex v ∊ X is a simplex of k, and \n \nEach subset of a simplex of k is also a simplex of k. \nAd-simplex is defined as a simplex composed of d + 1 elements of X. The 0-simplices \ncorrespond to the set of vertices of k, the 1-simplices to the set of its edges, and the 2-\n",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": " \n \nsimplices to the set of its triangles, denoted as θ = {t1,t2, ...,tT}. (T represents the number \nof triangles.) \nThe geometry γ specifies the shape of the surface by associating 3D positions and usually \nsurface normals to the vertices of k. A surface S is convex if it is a subset of the boundary \nof its convex hull (in other words, the minimal convex volume containing S). Computing an \nexact convex decomposition of an arbitrary surface S consists of partitioning it into a \nminimal set of convex sub-surfaces. Chazelle, et al. prove that computing such \ndecomposition is an NP-hard problem and evaluate different heuristics to resolve it \n[Chazelle95]. Lien, et al. claim that the exact convex decomposition algorithms are \nimpractical since they produce a high number of clusters, as shown in Figure 2.8.2 [Lien04]. \nTo provide a tractable solution, they propose to relax the exact convexity constraint and \nconsider instead the problem of computing an approximate convex decomposition (ACD) of \nS. Here, for a fixed parameter ε>0, the goal is to determine a partition Π = {π1,π2, ...,πK} of \nθ with a minimal number of clusters K and verify that each cluster has concavity lower than \nε. \nFigure 2.8.2. Exact convex decomposition versus approximate convex \ndecomposition. \n \n \nThe ACD problem has been addressed in a number of recent publications [Lien04, Lien08, \nKraevoy07, Attene08]. Attene, et al. apply a hierarchical segmentation approach to a \ntetrahedral mesh generated from S [Attene08]. The tetrahedralization process exploited by \nAttene, et al. is, in practice, hard to compute and introduces extra computational \ncomplexity. Other methods avoid this limitation by considering the original 3D mesh directly \n[Lien04, Lien08, Kraevoy07]. Kraevoy, et al. introduce an incremental Lloyd-type \nsegmentation technique exploiting a concavity-based seed placement strategy [Kraevoy07]. \nHere, the concavity measure is defined as the area weighted average of the distances from \nthe clusters to their corresponding convex hulls. Lien, et al. claim that such concavity \nmeasure does not efficiently capture the important features of the surface [Lien04, Lien08]. \nThey propose instead to compute the maximal distance between the mesh vertices and the \nclusters‘ convex hulls. Their divide-and-conquer approach iteratively divides the mesh until \nthe concavity of each sub-part is lower than the threshold ζ, as shown in Figure 2.8.3. Here, \nat each step i, the vertex \nwith the highest concavity is selected, and the cluster to which \nit belongs is divided into two sub-clusters by considering a bisection plane incident to \n. \nFigure 2.8.3. The divide-and-conquer ACD approach introduced in [Lien04] and \n[Lien08]. \n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": " \n \n \n \nThe main limitation of this approach is related to the choice of the ―best‖ cut plane, which \nrequires a sophisticated analysis of the model‘s features. A public implementation of \n[Lien04] is provided in [Ratcliff06]. Here, the sophisticated feature analysis procedure is \nreplaced by a simple cut plane selection strategy that splits each cluster according to its \nlongest direction. This suboptimal choice generates over-segmentations, which are \noptimized by applying a post-processing procedure aiming at aggregating the maximal \nnumber of clusters while ensuring the maximal concavity constraint. As illustrated in Figure \n2.8.7, the ACDs produced by Ratcliff are, in practice, suboptimal, since the aggregation \nprocedure is applied to clusters generated only by plane-based bisections. \nTo overcome the aforementioned limitations, we introduce, in the next section, a simple and \nefficient hierarchical ACD approach for 3D meshes. \n \nHierarchical Approximate Convex Decomposition \nOur proposed hierarchical approximate convex decomposition (HACD) proceeds as follows. \nFirst, the dual graph of the mesh is computed. (See the upcoming ―Dual Graph‖ section.) \nThen, its vertices are iteratively clustered by successively applying topological decimation \noperations, while minimizing a cost function related to the concavity and the aspect ratio of \nthe produced segmentation clusters. Finally, by approximating each cluster with the \nboundary of its convex hull [Preparata77], a faithful approximation of the original mesh is \ncomputed. This surface approximation is piecewise convex and has a low number of \ntriangles (when compared to T), which makes it particularly well adapted for collision \ndetection. \nLet‘s first recall the definition of the dual graph associated with a 3D mesh. \nDual Graph \nThe dual graph S* associated with the mesh S is defined as follows. Each vertex of S* \ncorresponds to a triangle of S. Two vertices of S* are neighbors (in other words, connected \nby an edge of the dual graph) if and only if their corresponding triangles in S share an edge. \nFigure 2.8.4 illustrates an example of a dual graph for a simple 3D mesh. \nFigure 2.8.4. Example of a dual graph. \n",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": " \n \n \n \nDecimation Operator \nOnce the dual graph is computed, the algorithm starts the decimation stage, which consists \nof successively applying half-edge collapse operations to S*. Each half-edge collapse \noperation applied to an edge (v,w), denoted hecol (v,w), merges the two vertices v and w, \nas illustrated in Figure 2.8.5. The vertex w is deleted, and all its incident edges are \nconnected to v. \nFigure 2.8.5. Half-edge collapse decimation operation. \n \n \nLet A(v) be the list of the ancestors of the vertex v. Initially, A(v) is empty. At each \noperation hecol (v,w) applied to the vertex v, the list A(v) is updated as follows: \nEquation 1 \n \n \n \nSimplification Strategy \n",
      "content_length": 683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": " \n \nThe decimation process described in the previous section is guided by a cost function \ndescribing the concavity and the aspect ratio of the surface S(v,w) resulting from the \nunification of the vertices v and w and their ancestors [Garland01]: \nEquation 2 \n \n \nAs in [Garland01], we define the aspect ratio Eshape(V,W) of the surface S (v,w) as follows: \nEquation 3 \n \n \nwhere ρ(S(v,w)) and ζ(S(v,w)) represent the perimeter and the area of S (v,w), \nrespectively. \nThe cost EShape(v,w) was introduced in order to favor the generation of compact clusters. In \nthe case of a disk, the cost EShape equals one. The more irregular a surface, the higher its \naspect ratio cost. \nInspired by [Lien08], we define the concavity C(v,w) of S(v,w), as follows (see Figure \n2.8.6): \nFigure 2.8.6. Concavity measure for a 3D mesh. \n \n \nEquation 4 \n \n \n",
      "content_length": 843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": " \n \nwhere P(M) represents the projection of the point M on the convex hull CH(u,w) of S(v,w), \nwith respect to the half ray with origin M and direction normal to the surface S(v,w) at M. \nThe global decimation cost E(v,w) associated with the edge (v,w) is given by: \nEquation 5 \n \n \nwhere \n \nD is a normalization factor equal to the diagonal of the bounding box of S, and \n \nα is a parameter controlling the contribution of the shape factor EShape(v,w) with \nrespect to the concavity cost. (See the upcoming ―Choice of the Parameter α‖ \nsection.) \nAt each step of the decimation process, the hecol (v,w) operation with the lowest \ndecimation cost is applied, and a new partition Π(i)= {π1(i),π2(i), ...,πK(i) } is computed as \nfollows: \nEquation 6 \n \n \nwhere \nrepresents the vertices of the dual graph S* obtained after i half-\nedge collapse operations. This process is iterated until all the edges of S* generating \nclusters with concavities lower than ω are decimated. \nChoice of the Parameter α \nThe clusters detected during the early stages of the algorithm are composed of a low \nnumber of adjacent triangles with a concavity almost equal to zero. Therefore, the \ndecimation cost E is dominated by the aspect ratio related cost EShape, which favors the \ngeneration of compact surfaces. This behavior is progressively inverted during the \ndecimation process, since the clusters become more and more concave. To ensure that the \ncost (α.EShape) has no influence on the choice of the later decimation operations, we have set \nthe parameter α as follows: \nEquation 7 \n \n \nThis choice guarantees, for disk-shaped clusters, that the cost (α.EShape) is 10 times lower \nthan the concavity-related cost \n. \n",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": " \n \n \nExperimental Results \nTo validate our approach, we have first compared its segmentation results to those of \n[Ratcliff06], which provides a simplified version of the original algorithm described in \n[Lien04] and [Lien08]. In Figure 2.8.7, we compare the ACDs generated by our approach to \nthose obtained by using Ratcliff‘s method. Here, the accuracy of the generated piecewise \nconvex approximations is objectively evaluated by using the root mean squares (RMS) and \nHausdorff errors [Aspert02]. Let‘s recall that the RMS error measures the mean distance \nfrom the original mesh S to its piecewise convex approximation S′. It is defined as follows: \nEquation 8 \n \n \nFigure 2.8.7. Comparative evaluation: (a,d,g,j,m,p) original meshes, (b,e,h,k,n,q) \npiecewise convex approximations generated by [Ratcliff06], and (c,f,i,l,o,r) \npiecewise convex approximations generated by the proposed HACD technique. \n",
      "content_length": 910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": " \n \n \nwhere D is the diagonal of the bounding box of S, α(S) is its area, and d(p,S′) is the \ndistance from a point p∊S to S′. The distance d(p,S′) is given by: \nEquation 9 \n \n \n",
      "content_length": 178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": " \n \nThe Hausdorff error, denoted H, measures the maximal distance from S to S′ and is defined \nas follows: \nEquation 10 \n \n \nThe reported results show that the proposed HACD technique provides significantly (that is, \nfrom 20 percent to 80 percent) lower RMS and H errors, while detecting a lower number of \nclusters. Figures 2.8.7(j) through 2.8.7(r) clearly show the limitations of the plane-based \nbisection strategy and the aggregation post-processing procedure of Ratcliff, which \ngenerates over-segmentations and poor ACDs. \nColor Plate 8 presents the segmentation results and the approximate convex \ndecompositions generated by our approach for different 3D meshes. For all the models, the \ngenerated segmentations ensure a concavity lower than ε and guarantee that the maximal \ndistance from S to S′ is lower than 3 percent of D. Therefore, the generated piecewise \nconvex approximations provide faithful approximations of the original meshes with a small \nnumber of clusters. Moreover, our technique successfully detects the convex parts and the \nanatomical structure of the analyzed 3D models. \nFor all of the models shown in Color Plate 8, the piecewise convex approximations were \ncomputed by considering an approximation of the clusters‘ convex hulls with a maximum of \n32 vertices for each. The number of triangles composing the obtained convex surfaces is \nlower than 8 percent of T. Furthermore, the piecewise convexity property makes the \ngenerated approximations particularly well suited to collision detection. \n \nConclusion \nWe have presented a hierarchical segmentation approach for approximate convex \ndecomposition of 3D meshes. The generated segmentations are exploited to construct \nfaithful approximations of the original mesh by a set of convex surfaces. This new \nrepresentation is particularly well suited for collision detection. We have shown that our \nproposed technique efficiently decomposes a concave 3D model into a small set of nearly \nconvex surfaces while automatically detecting its anatomical structure. This property makes \nthe proposed HACD technique an ideal candidate for skeleton extraction and pattern \nrecognition applications. \n \nReferences \n[Attene08] Attene, M., et al. ―Hierarchical Convex Approximation of 3D Shapes for Fast \nRegion Selection.‖ Computer Graphics Forum 27.5 (2008): 503–522. \n[Chazelle95] Chazelle, B., et al. ―Strategies for Polyhedral Surface Decomposition: An \nExperimental Study.‖ Symposium on Computational Geometry (1995): 297–305. \n[Garland01] Garland, M., et al. ―Hierarchical Face Clustering on Polygonal Surfaces.‖ \nSymposium on Interactive 3D Graphics (2001): 49–58. \n",
      "content_length": 2648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": " \n \n[Hoppe96] Hoppe, H. ―Progressive Meshes.‖ International Conference on Computer Graphics \nand Interactive Techniques (1996): 99–108. \n[Kraevoy07] Kraevoy, V., et al. ―Model Composition from Interchangeable Components.‖ \nPacific Conference on Computer Graphics and Applications (2008): 129–138. \n[Lien04] Lien, J.M., et al. ―Approximate Convex Decomposition.‖ Symposium on \nComputational Geometry (2004): 457–458. \n[Lien08] Lien, J.M., et al. ―Approximate Convex Decomposition of Polyhedra and Its \nApplications.‖ Computer Aided Geometric Design (2008): 503–522. \n[Aspert02] Aspert, N., et al. ―MESH: Measuring Error Between Surfaces Using the Hausdorff \nDistance.‖ IEEE International Conference on Multimedia and Expo 1 (2002): 705–708. \n[Preparata77] Preparata, F. P., et al. ―Convex Hulls of Finite Sets of Points in Two and Three \nDimensions.‖ ACM Communication 29.2 (1977): 87–93. \n[Ratcliff06] Ratcliff, J. ―Approximate Convex Decomposition.‖ John Ratcliff‘s Code \nSuppository. April 2006. Bolgspot.com. n.d. \n<http://codesuppository.blogspot.com/2006/04/approximate-convex-decomposition.html>. \n \nSection 3: AI \nIntroduction \nAI Level of Detail for Really Large Worlds \nA Pattern-Based Approach to Modular AI for Games \nAutomated Navigation Mesh Generation Using Advanced Growth-Based Techniques \nA Practical Spatial Architecture for Animal and Agent Navigation \nApplying Control Theory to Game AI and Physics \nAdaptive Tactic Selection in First-Person Shooter (FPS) Games \nEmbracing Chaos Theory: Generating Apparent Unpredictability through Deterministic \nSystems \nNeeds-Based AI \nA Framework for Emotional Digital Actors \nScalable Dialog Authoring \nGraph-Based Data Mining for Player Trace Analysis in MMORPGs \nIntroduction \nBorut Pfeifer \n",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": " \n \nborut_p@yahoo.com \nWith recent advances in graphics and animation, AI is one of the areas of game \nprogramming with the most potential for growth. The techniques in this section address \nsome of the most troublesome and encouraging future areas for game AI development. \nAdvances in AI architecture, believable decision-making, more detailed character \nsimulation, and player modeling all offer the possibility of creating new or improved aspects \nof gameplay, as well as enhancing our ability to actually build that gameplay more quickly. \nAI architecture is crucial for effective development. A poor architecture can bring the \ndevelopment of new AI gameplay on a project to a halt. A good one can empower new \nfeatures and new experiences. Cyril Brom, Tomáš Poch, and Ondřej Šerý discuss creating \nworlds with high numbers of NPCs in their gem, ―AI Level of Detail for Really Large Worlds,‖ \nmanaging multiple simulation levels for each area of the game world. Kevin Dill writes about \nusing patterns in AI decision-making code in his gem ―A Pattern-Based Approach to Modular \nAI for Games.‖ The more effective an AI programmer is in creating reusable and scalable \ncode, the more time there is to iterate on gameplay. \nAI movement and pathfinding are always the earliest difficult problems an AI programmer \nhas to solve on a project. ―Automated Navigation Mesh Generation Using Advanced Growth-\nBased Techniques,‖ by D. Hunter Hale and G. Michael Youngblood, details their research on \nnew methods of creating navigation meshes using space-filling algorithms. Michael Ramsey \ncovers the pathfinding used for a wide variety of animals and movement types in World of \nZoo in his gem ―A Practical Spatial Architecture for Animal and Agent Navigation.‖ Brian \nPickrell sheds some light on the often-overlooked area of control theory and how it can be \napplied to an agent‘s steering in ―Applying Control Theory to Game AI and Physics.‖ \nDecision-making is the aspect of AI that affects players most directly. The challenge is \nfinding meaningful decisions characters can make to appear intelligent, while also being \nunderstandable to the player. It is much easier to create decision-making that allows for an \nNPC‘s success than it is to balance understandable behavior with the complexity and depth \nrequired to appear intelligent. Thomas Hartley and Quasim Mehdi describe a method to \nallow NPCs to adapt better to players‘ combat behavior over time with their gem ―Adaptive \nTactic Selection in First-Person Shooter (FPS) Games.‖ Dave Mark discusses how to use \ncomplexity in building AI decision-making models in ―Embracing Chaos Theory: Generating \nApparent Unpredictability through Deterministic Systems.‖ \nMore detailed character simulation can also help create that depth and believability. Rob \nZubek breaks down an AI decision-making approach used in games such as the Sims series \nin his gem, ―Needs-Based AI.‖ Phil Carlisle takes a look at how we can easily add emotional \nmodeling to a behavior tree–based architecture in his gem, ―A Framework for Emotional \nDigital Actors.‖ Baylor Wetzel‘s gem, ―Scalable Dialog Authoring,‖ confronts the difficult \nproblem of creating dialog for many NPCs by abstracting concepts, such as a cultural group \nan NPC belongs to, and describes how to empower designers to author more variety in \ndialog with these abstractions. \nPlayer modeling is a burgeoning area of game AI. It can be used to help improve a player‘s \nexperience, such as in Left 4 Dead. In the gem ―Graph-Based Data Mining for Player Trace \nAnalysis in MMORPGs,‖ Nikhil Ketkar and G. Michael Youngblood write about their work \nmodeling players in massively multiplayer games (MMOs). They‘ve used MMO player data to \ndetermine models to detect gold farmers and bots as well to determine effective locations \nfor in-game advertising. Such models and processes can be used to improve a wide variety \nof aspects of the player experience. \nAs game AI programmers, we are now in the spotlight to create the next level of innovative \ngame experiences. People are no longer impressed by the same fancy normal maps and \nmotion-captured animation; they want their characters to be more believable and more \n",
      "content_length": 4218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": " \n \nentertaining in how they act. It‘s up us to make this a reality, and I hope the gems in this \nsection will help give you ideas to solve some of these problems. \n \n3.1. AI Level of Detail for Really Large Worlds \nCyril Brom, Charles University in Prague \nbrom@ksvi.mff.cuni.cz \nTomáš Poch \npoch@dsrg.mff.cuni.cz \nOndřej Šerý \nondrej.sery@dsrg.mff.cuni.cz \nOne challenge for games featuring large worlds with many non-player characters (NPCs) is \nto find a good balance between the consumption of computational resources and simulation \nbelievability. On the one hand, the cost of simulation of a whole world, including all the \nNPCs, is enormous. On the other hand, if one simulates just the proximity of a player, one \nasks for plausibility troubles. For instance, when a player is supposed to return to a once \nleft area, NPCs and objects in this area may not be in a believable state—the area has not \nchanged since it was left, people have not moved, the ice cream left on the table has not \nmelted, and so on. Additionally, this approach cannot handle NPCs that can move freely \naround the world. \nTo compromise between these two extremes, a number of level-of-detail AI techniques \n(LOD AI) have been invented. While LOD AI for a traffic simulation [Chenney01] and for \nenemies in an RPG [Brockington02] and an action game [Grinke04] has already been \nwritten about, less is known about how to vary simulation detail for general NPCs. \nThis gem presents a LOD AI technique tailored for simulations of large worlds featuring \nhundreds of commonplace NPCs with relatively complex behavior. These NPCs can perform \ntasks that include manipulation with several objects and require a variety of everyday \nbehaviors, such as harvesting, merchandising, or watering a garden (as opposed to pure \nwalking and fighting). The NPCs are interactive, and they can move around the world as the \nstory dictates. The technique is gradual, which means that it allows for several levels of \ndetail (LOD) based on the distance from the player or important places (Figure 3.1.1). The \ntechnique also considers the fact that whole locations containing objects and NPCs may \ncease to exist when the LOD decreases and need to be re-created when it increases again. \nFigure 3.1.1. Three types of level-of-detail AI techniques. Places are projected on \nthe X-axis. \n",
      "content_length": 2349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": " \n \n \n \n \nGradual LOD Example \nThink of a classic fantasy RPG setting: a medieval county with many villages. There is a pub \nin one of them, where miners go to have some fun after their shift. A player can go there as \nwell; she can leave and return any time. You want to avoid the situation in which she would \nrealize that the miners were not being simulated properly when she was at the other end of \nthe village, but you want to save the resources when she is out. You may define the \nfollowing LODs ranging from the full simulation to almost no simulation. \n \nDetail 5: Full simulation—used when the player is in the pub or in its proximity. \n \nDetail 4: Every room from the pub is abstracted to a single point. The tables in the \nsaloon are organized just in a list or an abstract graph; exact positions become \nunimportant. Miners (and other NPCs) are still sitting at the tables, but they are not \ndrinking properly; they just empty the whole glass of beer in one go, say, every 20 \nto 40 minutes. The barman brings new beer, but now he is not walking as he would \nwith LOD 5. Instead, he ―jumps‖ from table to table. The beer still has to be paid for. \nA miner can jump to another table as well, or to a next room, or he can leave the \npub. LOD 4 will be used typically if the player is, say, 100 to 300 meters from the \npub. \n \nDetail 3: The glasses, the tables, and the pub‘s rooms cease to exist. The whole pub \nis abstracted to a single point. Yet the miners can still be there (we don‘t know \nwhere exactly, but we need not care, as will be detailed later). The beer level in the \nbarrel will decrease a bit every half an hour or so based on the number of miners in \nthe pub. The miners can leave the pub or enter. LOD 3 will be used if the player is at \nthe other end of the village. Notice that when she is approaching the pub, the detail \nfirst jumps to 4 and only then to 5. Additionally, when it jumps to 4 from 3, the \nminers must be assigned to tables, and glasses must be generated properly (for \nexample, at the tables). \n \nDetail 2: The whole village is abstracted to a single point. The barman is not \nsimulated because his story, as specified by designers, never leads him out of the \nbar, while the miners still are; they may go from a village (meaning the pub or \nhome) to the mine (in other words, to work). This detail is used when the player is \nnot in the village but is in its proximity. \n \nDetail 1: The village and its surroundings are abstracted to a single point; no \nvillage‘s inhabitants are simulated, but a foreigner may pay a visit. \nYou can imagine that the foreigner is a story-important persona. His importance can \ndemand that the area he is located in has an LOD of at least 4. Thus, as he moves, he \nredefines LOD similarly to the player. Alternatively, some events may be considered \n",
      "content_length": 2833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": " \n \nimportant. When miners—in the pub, at LOD 3—start a brawl, the brawl can level up the \ndetail to 5 no matter where the user is. \nThroughout, we will assume that LOD increases or decreases just by 1; larger changes can \nbe done by repetition. We will assume one player in the simulation, but the technique can \nbe used for multiplayer games as well. \nThe LOD technique introduced in this game has been implemented as a part of a general \nsimulator of 2D virtual words, including the aforementioned example (with some \nmodifications). The simulator and the example are included on the CD. \n \nGraphical LOD versus LOD AI \nConceptually, it is often useful to think about a game‘s control mechanisms in terms of a \ntwo-layered architecture. While AI is the higher layer, physics and graphics are the lower \nlayer. (See [Chenney01] for more on this point.) This is actually a simplified view, \nnevertheless useful for explanatory purposes. According to this metaphor, the player‘s \ndirect experience is provided by the lower animation layer, which is only influenced by the \nhigher layer. The LOD AI technique is only related to the higher layer. \nSince the position of the player‘s avatar forces the maximum LOD AI in correctly designed \nworlds, only the areas (or their parts) simulated at the maximum detail may be visualized. \nThe animation layer takes the outcome of the AI layer as an abstract prescription of what to \nshow. The animation layer operates with several graphical levels of detail, and it may add \nadditional complexity above the finest LOD AI detail. In our example, LOD AI Detail 5 takes \ncare of whether a miner will drink a beer or whether he will go to the waypoint on the right \nor on the left, but the movement of his hand or walking smoothly will be dealt with by the \nanimation engine. \n \nSimulation at the Full Detail \nThis gem concerns itself only with the AI layer. Assume to start that our goal is to simulate \nthe whole world at the full AI LOD and that we need not care about animation. A good way \nto think about what happens in the AI layer is in terms of a discrete-event simulation \nparadigm. According to this view, time is represented by a chronological sequence of \nsimulation events, which are ordered in an event list. These simulation events are abstract \nentities that mark time instants in which the state of the game is modified. Additionally, \nevery simulation event can generate a new simulation event to the event list or remove an \nexisting event. Technically, every simulation event is associated with a piece of code. In a \nnutshell, after initialization, the whole system works in the following cycle: \n1. Take the first simulation event from the event list and remove it from the list. \n2. Process this event; that is, run the code associated with the event. As a part of this: \na. Change the state variables of some entities. \nb. Insert new simulation events to the event list at appropriate places. \nc. Remove some simulation events from the event list. \nWhen this paradigm is used, it is important to distinguish between real time, which is the \ntime the user experiences, and simulation time, which is the time of the simulated world as \nrepresented by the event list. One processing cycle happens, by definition, in zero \nsimulation time (though it cannot happen in zero real time). For real-time games, these two \ntimes must of course be synchronized. For more on discrete-event simulations and event \nhandling, see [Channey01, Harvey02, Wiki09]. \n",
      "content_length": 3507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": " \n \nThe simulation paradigm per se says nothing about how the simulation events relate to \nwhat really happens in the game. There are designers who specify this; they must create \nsimulation events around those changes in the state of the game that the AI layer should be \naware of. In a sense, the simulation events present the AI layer‘s window into the virtual \nworld. \nOne class of simulation events is story-important events (for example, the dungeon‘s gate \nwill open every day at midnight). These events typically will be pre-scheduled in the event \nlist from the beginning or hooked into the event list by a trigger in run time (for example, \nfrom the moment the player enters the village, the neighboring dungeon will open every \nday at midnight). A different class of events is due to slow changes of objects‘ states (for \nexample, increasing the rust level of a sword every week by one). But the most important \nclass is due to changes caused by atomic actions of NPCs or the player; these actions must \nbe represented by simulation events in the event list. \nBecause these actions are indivisible from the standpoint of the AI layer, it suffices to \nrepresent each of them by a start event and an end event. Typically, these two events will \nnot be present in the event list simultaneously. During processing of a start event, first, \nother parts of the game are notified that the action starts, and second, it is estimated how \nlong the action will last, and its end event is hooked to the event list at the appropriate \nplace (Figure 3.1.2). When this time comes and the end event is processed, states of some \nobjects are changed, and the NPC‘s AI controller is invoked to decide what the next action \nof this NPC is, hooking the start event of that action to the event list. In fact, because start \nevents tend to be hooked at the beginning of the event list, it is often possible to skip their \ngeneration and to hook the respective end events directly. Note that this mechanism allows \nyou to have atomic actions with various durations. \nFigure 3.1.2. Atomic action of sipping a beer is represented in the event list by a \nstart event and an end event. \n \n \nClassic discrete-event simulations typically work only with simulation events hooked into the \nevent list. However, for games, asynchronous events are also needed. That is, sometimes \nanother part of the game can generate an event that has to be propagated to the AI layer \nand processed by it immediately. For instance, the collision detection may recognize that \nsomeone has nudged the sipping person with an elbow. This means that the atomic action \nof sipping a beer cannot be finished. Thus, the AI layer must delete its end event and \ngenerate the start event of spilling action instead. \nAnother issue is that the time of actions‘ ends (thus, the time of end events) is only \nestimated by the AI layer. This is fine when we simulate a part of the world that is not \nvisualized: An estimate becomes the actual duration provided the action is not interrupted \nby an asynchronous event. However, for visualized parts, duration of some atomic actions \nwill be determined by the animation engine. End events of these actions need to be \nsynchronized with the actual end of the action. \n",
      "content_length": 3264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": " \n \n \nToward LOD AI: Hierarchical Behavior \nThe question of this gem is how to make the mechanism of start events and end events \ncheaper when the full detail is not required. Our solution will capitalize on the fact that \nbehavior of NPCs can be represented hierarchically. This means, more or less, that the NPCs \nare conceived as having a list of behaviors that are decomposed to sub-behaviors, which \nare further refined until some atomic actions are reached. Figure 3.1.3 shows how this \napplies to a drinking miner. \nFigure 3.1.3. Hierarchical representation of a drinking miner’s behavior. Atomic \nactions are in gray. Other nodes represent tasks. Note that one detail can be \nassigned to more levels. \n \n \nConceptually, this follows the Belief-Desire-Intention architecture [Bratman87]. This will \nmost likely be implemented with behavior trees or hierarchical FSMs—for example \n[Fu&Houlette04, Isla05, Champandard08]. \nIn academic literature, the decomposition of high-level behaviors to sub-behaviors is often \nmore complicated. There, the distinction between goals and tasks is often made. While \ngoals represent what shall be achieved, tasks represent how to achieve this. Every goal can \nbe accomplished by several tasks, and every task can be achieved by adopting some sub-\ngoals. Importantly, an NPC needs to perform only one task to achieve a goal, provided there \nis no failure, but it must fulfill all sub-goals, or most of them, to solve a task. Consequently, \nthe behavioral decomposition is given by an AND-OR tree (AND levels for goals, OR levels \nfor tasks). The CD examples feature such AND-OR trees; however, the LOD technique works \nwell with any behavioral decomposition; the distinction between goals and tasks is \nunimportant for it. Thus, for explanatory purposes, we assume throughout that we have just \nordinary sub-behaviors, and we call them tasks. \n",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": " \n \nWhatever the exact representation is, the key is that a) the hierarchy can be constructed so \nthat its highest level represents abstract behavior to which lower levels add more detail, \nand b) each level can be made executable. Thus: \n1. In the design phase, construct the hierarchy in this way and manually assign LODs to \nits levels (refer to Figure 3.1.3). \n2. During execution, determine what level of the behavioral hierarchy is the lowest that \nshould be executed. This is the level of the LOD that corresponds to the detail of the \nsimulation at the place where the NPCs are located (see Figure 3.1.4). \nFigure 3.1.4. ―Drinking at a table‖ is executed as atomic. Atomic tasks are in \ngray. \n \n3. Execute this task as atomic, approximating what would happen when the simulation \nis run at a finer detail. \nUnfortunately, these three points brings many problems. We will address them in turn. \nWhat Does It Mean to Execute a Task as Atomic? \nAtomic execution means two things. First, every task to be executed atomically must be \nrepresented in the event list by a start event and an end event, similar to atomic actions. \nFor instance, assume that there is LOD 4 in the pub. The corresponding behavioral level \ncontains the task of drinking at a table. The event list will contain the end event of this act \nbut not of atomic actions of sipping a beer or chatting. \nSecond, for every task, designers must specify a) its result, and b) its estimated duration. \nWhen a task is simulated in full detail, its particular runs can have different durations. \nOften, the exact duration of a run will be influenced by the animation layer (for example, an \nNPC walks from one room to another in a different amount of time based on the positions of \nobjects the NPC has to avoid). Fortunately, with lower LODs, you need not worry about \nexactly how long the task would have lasted had the simulation run in the full detail. All you \nneed is a plausible estimate of that time. Based on this estimate, you place the task‘s end \nevent in the event list. Note that you may generate this estimate from a probabilistic \ndistribution given by designers. \n",
      "content_length": 2147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": " \n \nWhen should the result (in other words, Point A) be revealed? There are three possibilities: \n1) at the beginning of the task (that is, when the start event is processed); 2) at the end (in \nother words, when the end event is processed); or 3) during the task. For the drinking \nminer, (1) would mean drinking the glass at once and then doing nothing for 20 to 40 \nminutes, while (2) would be doing nothing for 20 to 40 minutes and then drinking the glass \nat once. Variant (3) is not consistent, and (1) changes the world sooner than it is known \nthat the task is successfully finished. Thus, we recommend using (2). \nHowever, designing the scenario at different levels presents extra work, and you have to \nconsider in which situations these shortcuts are really needed. In extreme, you program the \nwhole simulation from scratch at each LOD, though programming more abstract levels is \nmuch simpler than the full detail. \nWhat to Do When the Detail Increases \nSadly, the detail can elevate in the middle of a task‘s atomic execution—in other words, \nbetween its start event and end event. For instance, the player may enter the pub while the \nminers are drinking at a table atomically. We call such situation a task expansion and the \npart of the task that has already been performed at the lower detail a task stub (see Figure \n3.1.5). When this happens, you need to: \n1. Remove the end event of the expanded task from the event list. \n2. Compute the partial effect of the task stub, if needed. \n3. Start the simulation at the higher detail. \nFigure 3.1.5. LOD has increased from 4 (up) to 5 (bottom), creating a stub from \ndrinking atomically. The original end event must be removed. \n \n \nThe partial effect of a task stub should approximate the outcome of the stub‘s detailed \nsimulation (in other words, as if the task was running from the beginning until the moment \nof expansion at the higher detail). You again need the designer‘s specification and the code \nfor determining the partial effect, which is extra work. \nActually, it is necessary to perform Point 2 only if the user may notice the discrepancy; \notherwise, it is sufficient to start execution of the expanded task at the lower detail from its \nbeginning, pretending that the task stub has never happened. In other words, you specify \nthat the partial effect is nothing. In our drinking example, this would mean that all the \ndrinkers would start with a full glass at the time of the LOD increase, but this may be fine if \n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": " \n \nyou do not visualize how much beer is in the glasses. However, consider watering a garden: \nHere, the player may find it strange that the gardener is always just starting with the first \ngarden bed at the time the player enters the garden. It may be necessary to decide that \nsome fraction of the garden has already been watered at the time of expansion. \nWhen you need to compute the partial effect of a task stub, what are the options? Often, it \nis desirable to avoid simulation of the task stub at the higher detail. This would give you an \nexact result, but at the cost of too much overhead. Instead, you need to consider which \nstate variables the task changes predictably (see Figure 3.1.6, left and middle) and which it \ndoes not (see Figure 3.1.6, right). In the former case, the partial effect can be determined \nby a simple formula. In the latter case, a more sophisticated ad hoc mechanism has to be \ncreated. \nFigure 3.1.6. Although the final outcome of three tasks being executed atomically \nis the same (Point A), their details differ. At the finer LOD, the state variable may \nchange in a predictable way (left, middle) or in an unpredictable way (right). The \narrows denote the LOD increase. \n \n \nFor instance, think of the LOD increase from Detail 3 to 4 in the pub. On one hand, you can \neasily figure out how much beer has been drunk based on the number of miners in the pub. \nOn the other hand, you also need to assign the miners to the tables. To do the latter, the \ndesigner has to come up with an ad hoc mechanism. \nWhat to Do When the Detail Decreases \nLet us have a location in which detail n should be decreased to n-1. We say that tasks (or \natomic actions) corresponding to detail n are being pruned away, whereas tasks \ncorresponding to detail n-1 are being shrunk—that is, starting to be executed as atomic. \nSince the LOD decrease must apply to a whole area, as detailed later, more tasks may need \nto be pruned away—for example, drinking tasks of all individual miners in the pub. These \ntasks may not end at the same moment. Thus, they should be stopped using the \nmechanism of partial effect described previously. Only then can you execute shrinking tasks \nas atomic. It is important to stop the tasks being pruned away at one instant; otherwise, \nthe area will end up in an inconsistent state, with some tasks being simulated at detail n \nand others at n-1. \nAn important point of any LOD AI is that, by definition, some information is lost during a \nLOD decrease. This brings two problems. First, the partial effect of tasks and actions being \npruned away should be partly forgotten but also partly exploited by the subsequent \nsimulation at LOD n-1. Second, when the LOD increases again, the missing information \nshould be reconstructed. This is similar to lossy compression. \nThe second problem was partly treated earlier (assigning miners to tables), and we will also \nreturn to it later. The first problem is exemplified now in Figure 3.1.7, showing a barman \nwalking to a table while LOD goes from 5 to 4. In this example, we are concerned with the \nbarman‘s position, but similar reasoning applies for any state of an object. \n",
      "content_length": 3174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": " \n \nFigure 3.1.7. LOD decrease. ―Steps‖ are being pruned away at t2, while ―go-to-\nnext-object‖ task is being shrunk into the atomic ―jump.‖ \n \n \nRecall that the saloon is a single point at LOD 4. Figure 3.1.7 shows that when the \nsimulation runs on the LOD 4, the walking barman is engaged in the task ―go-to-next-\nobject,‖ which makes the barman ―jump‖ from one object to another. These objects are \ntables or the bar. If the saloon is not too oblong, we may further assume unitary distances \n(at LOD 4) between all pairs of these objects. At LOD 5, ―go-to-next-object‖ breaks down \ninto a sequence of ―step-to‖ atomic actions. \nNow, consider the LOD decrease from 5 to 4 at time t2 —that is, around the middle of the \n―go-to-next-object‖ from Object X to the table. Should the barman start his ―jump‖ from \nObject X or Place Y, and how long should it take? \nBecause Place Y does not exist at LOD 4, it seems that the best option is to let the barman \nstart his ―jump‖ from X. However, since you cannot roll back what has already happened, \nthe barman would walk from X to Y twice, and a clever user may notice this should the \ndetail increase soon (say, at time t3, when the barman is expected to be around Z). \nThe second option is to say that the barman is somewhere on the way and compute how \nlong the rest of the ―jump‖ (in other words, from Y to the table) will last. This is more \naccurate, but it creates an extra work. Additionally, this works well only when the object‘s \nstate is updated in a predictable way at the finer detail (refer to Figure 3.1.6, left and \nmiddle). \nThe first method actually works acceptably if you manage to avoid an early LOD increase. \nThe longer the period between a LOD decrease and the subsequent LOD increase, the more \ninconsistencies due to a simplified determination of the initial state of the simulation at LOD \nn-1 are disguised. A simple mechanism for avoiding early LOD increase will be shown later. \n \n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": " \n \nSpace Representation \nSo far, we have spoken about how to simplify behavior given a LOD, but we also need to \nknow the value of the LOD and its radius, and we need these values for all parts of the \nvirtual world. Perhaps the simplest way to get this information is to represent the world \nhierarchically, as you can examine on the CD example. The spatial hierarchy keeps \ninformation about children, parents, and neighbors of every location except for the leaves \nand the root, which lack children or has only them, respectively. \nFor simplification, assume now that the number of LODs equals the number of spatial levels. \n(This is not a strict requirement.) Now, a membrane metaphor can be used to describe \nwhich LOD is where. Imagine an elastic membrane cutting through the spatial hierarchy \n(see Figure 3.1.8), touching some locations. We say that every location or an atomic place \nthat is at the membrane at a particular instant is simulated as an abstract point. No location \n―below‖ the membrane exists. Every NPC is simulated at the LOD equal to the level on \nwhich the membrane is touching the area in which that NPC is located; spatial LOD \ndetermines behavioral LOD. \nFigure 3.1.8. LOD membrane. \n \n \nFormally, the membrane is nothing more than a list of locations that are simulated as \nabstract points at a particular moment. The membrane can be reshaped in every time step. \nFor the purposes of coherence, we enforce the following shaping rules: \n1. If a location or an atomic place X is at the membrane, every location or atomic place \nwith the same parent as X is at the membrane, too. \n2. If a location Y is above the membrane, every location with the same parent as Y is \nabove or at least at the membrane, too. For example, if an atomic place from the \nsaloon is at the membrane, all the saloon‘s atomic places will be at the membrane, \ntoo (Rule 1). This ensures that when the simulation runs in the full detail somewhere \nin the saloon, it will run in the full detail everywhere here. Because the saloon itself \nis above the membrane in this case, all rooms from the pub must be simulated at \nleast as abstract points (Rule 2). This ensures that almost always at least something \nhappens in the locations near the center of attention. \n",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": " \n \nBecause LODs of two adjacent locations can differ (that is, when they do not have \nthe same parent), the visibility of a player should be limited to the location she is in. \nIf this is not possible, such as in open spaces, another shaping rule must be settled. \n3. The detail must be maximum in all locations the player can see at a given instant. \nLimitations of Rules 1 through 3 \nRules 1 through 3 work fine, but you should know that there is an exception in which they \ndo not guarantee that something happens near the center of attention. The problem is that \ntwo neighboring locations may have different parents, thus their LODs can differ by more \nthan 1. Think of a world with two kingdoms. There are two houses there next to each other, \nbut the border goes right between them. Even though the first house is simulated in full \ndetail, the second kingdom may still have just LOD 1; the houses do not have a common \nparent. \nTo deal with this, a mechanism of influences is needed. This would allow you to specify \nanother shaping rule based on the influence a location has on its all neighboring locations. \nThe trouble with this mechanism is that it may cause cascade effects during membrane \nreshaping, increasing the overhead. You can read more about this topic in the \ndocumentation on the CD. \nPathfinding \nSo far, we have been silent on pathfinding. With the spatial hierarchy, it is apparent that a \nhierarchical refinement of A* can be used easily for simplifying pathfinding at a lower LOD. \n \nPositions of Objects and NPCs \nWe know that the pub is abstracted to a single point at LOD 3. Assume there is one miner \nand one barman there. What happens with their positions during LOD changes? \nBecause the pub‘s rooms do not exist at LOD 3, both the barman and the miner are, so to \nspeak, spread out in the whole pub as two uncollapsed quantum particles; they are at the \nmembrane. Assume further that the LOD decreases to 2. While this lifts the miner a level \nup, spreading him out in the village, the barman becomes non-simulated because his story \nnever leads him out of the pub (see Figure 3.1.9). This means that the barman ―fell \nthrough‖ the membrane; his traces have been lost for the rest of the simulation. \nFigure 3.1.9. LOD increase. Some objects are ―lifted up,‖ while others are not. \n",
      "content_length": 2320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": " \n \n \n \nThe problem is that we need to narrow down object and NPC positions after the detail \nelevates. When the LOD goes from 2 to 3, we know that the barman has to be collapsed \ninto the pub, but this is not the case of the miner. We do not know where to generate him; \nhe can be collapsed into the pub or into his house or on the street, and so on. A similar \nproblem arises for the LOD increase from Detail 3 to 4. \nWe now introduce a basic mechanism dealing with generation of positions of objects. The \nreal case is actually more complex than this mechanism. Following, we will extend it and \ncomment on how to use it for NPCs. \n1. Objects in every location should be divided into two groups—those that are location-\nnative, for objects owned by the location, and those that are location-foreign, for \nobjects owned by other locations. (A glass is location-native in the pub, as opposed \nto a mine truck.) An object can be location-native in more locations. (A glass is \nlocation-native in the bar as well as in the kitchen.) \n2. Every location from the nth level of the spatial hierarchy should have a method \nimplementing where to generate location-native objects if the detail elevates from n \nto n+1 (for example, during the Detail 3 to 4 transition, the pub ―knows‖ where to \ngenerate the tables and glasses). \n3. When LOD decreases in a particular location, the detailed positional information for \nall the location-foreign objects having been ―lifted up‖ is memorized, but not for the \nlocation-native objects having been ―lifted up‖ (due to Point 2). \n4. When location-native objects ―fall through‖ the membrane, only their total number in \nthe area where they ―fall though‖ is remembered (for example, after the 4-to-3 \ntransition, it will be remembered that there are, say, 27 beer glasses in the pub). For \nlocation-foreign objects, the exact positions are remembered. (After the same \ntransition, the watering can will become non-simulated, but two memory records will \nbe kept: for the 5-to-4 and 4-to-3 transitions.) \n5. When LOD increases, location-foreign objects are generated based on their stored \npositional information. In some rare situations, this information may not be \navailable; this will be discussed in a moment. \nThe idea behind this is that during the design phase, one can specify relatively easily which \nobjects are native in which locations and implement placing algorithms appropriately. \nStorage of Positional Information \n",
      "content_length": 2466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": " \n \nWhen a location-foreign object is simulated, it can hold its own positional information. The \nrecords about location-foreign non-simulated objects‘ positions and about numbers of \nlocation-native non-simulated objects can be kept by parent locations. (For example, the \npub ―knows‖ that it contains 27 glasses.) \nOften, when the parent location ceases to exist, the player is so far away that the records \nheld by this location can be happily forgotten. Nevertheless, should a piece of information \nsurvive destruction of the location to which it is attached, you can do the following. In the \ndesign phase, specify the information level denoting the LOD at which this particular kind of \ninformation can be forgotten. At run time, after the location ―falls through‖ the membrane, \nattach your record to the upper location (in our case, the village) provided the new LOD is \nstill higher than or equal to the information level of this record. (Mind the direction: A higher \nLOD is lower in the hierarchy!) \nGeneral Information and Expirations \nThe aforementioned mechanism can be generalized in two ways. First, one can store not \nonly positional information, but any state information. Assume a player has broken a \nwindow in the pub. A clever player would not return soon, but if she does, she may expect \nthe window to still be broken. The record about the broken window can be stored by the \npub. Similarly, sometimes it may be beneficial to record the exact positional information for \na location-native object. If the player moves a table a bit, and the pub‘s LOD goes to and \nfro, the basic aforementioned mechanism would generate the table incorrectly, on the \noriginal place. The extra record will remedy this problem. \nAfter a while, this record may become useless; sooner or later, someone may repair the \nwindow and move the table back. If you need this feature, you can basically delete a record \nafter a specified amount of time using a mechanism of expirations. \nInitialization and Traveling Objects \nThere are two situations in which it is necessary to initialize objects‘ positions. The first one \noccurs after the simulation starts. Additionally, sometimes it is necessary to initialize \nobjects‘ positions when the objects move between locations. \nThe first situation is trivial: You must have an implicit initialization procedure. Let us now \nelaborate on the second case. Assume the pub is simulated at LOD 4, meaning all the pub‘s \nrooms are single points. A miner puts down a watering can in the saloon, where the can is \nlocation-foreign. Now, the LOD elevates to 5. Where do you place the can? Sometimes, the \nprocedure initializing positions at the beginning of the simulation can help. More often, you \nwould find useful a general placing mechanism that groups objects based on their typical \nplaces of occurrence into floor-objects, table-objects, and so on, and that generates these \nobjects randomly within constraints of the objects‘ categories. Consider now that LOD \ndecreases later from 5 to 4, and the miner picks up the can in the pub and takes it out. If \nthis happens, do not forget to delete the record about the can‘s detailed position. \nNPCs \nWhen the detail increases, we have to refine a) positions of NPCs, and b) tasks they are \nengaged in. For (a), we can treat NPCs as objects. For (b), the task expansion mechanism \ndescribed earlier should be used. \n \nReshaping the Membrane \n",
      "content_length": 3429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": " \n \nThe final question is how to shape the membrane. A simple solution, adopted by the \nexample on the CD, is to assign an existence level and a view level to every object. If the \ndetail decreases below the existence level, the object ceases to exist; otherwise, it is to be \nsimulated. The view level then determines the detail required by the object after it starts to \nbe simulated. Hence, the view level is always equal to or higher than the existence level. \nFor many common objects, these levels will be equal, but not for story-important objects or \nNPCs. Every user‘s avatar will have the existence level equal to 1 and the view level equal \nto the maximum. \nAssume now a traveler with his view level set to 4 and his existence level to 2. He will not \nexist until the detail is at least 2, but when it elevates to this value, the traveler will demand \nthat in the location to which he has been generated, the detail goes further to 4. Note that \naccording to the shaping rules, this further determines increasing detail in some of the \nneighboring locations. Because this may create new objects demanding additional LOD \nincreases, a cascade of changes may be triggered. The algorithm for doing this consistently \nis detailed in [Šerý06]. \nSometimes, you may want to increase the detail even if no important object is around; for \ninstance, think of a brawl being simulated in a village far away that is expected to grow into \na local uprising. You may need to show the behavior of those NPCs to the player in a cut \nscene. To do this, you can simply put an invisible object into the simulation with the \nappropriate view level to ensure the player will see the AI behavior. \nWhat Is the Radius of LODs? \nUsing this technique, all users and important objects/NPCs tend to automatically create a \n―simulation crater‖ around them due to the spatial hierarchy and the shaping rules. As they \nmove, the detail elevates (typically) by one in locations at the edge of the crater, avoiding \nabrupt jumps of LOD. LODs are not specified in terms of metrical distances, but in terms of \nnumber of locations between the object/NPC and the edge of the crater. \nThis has two advantages. First, the LOD does not change all the time, as would be the case \nwith pure metrical distances, helping to reduce the overhead. Further, the overhead is \nspread in time: If you go from 1 to 5, you do this in more steps. Figure 3.1.10 \ndemonstrates that there is indeed a qualitative difference in overhead between the 4-to-5 \nincrease (left) and 3-to-5 (right) in the pub in our CD example. \nFigure 3.1.10. Processor consumption during the LOD increase. \nThe X-axis represents the time in the game. The LOD increases at 10:15 p.m.; this time is \narbitrary. This figure shows data from three particular runs. Note that the data for the 3-\nto-4 increase resembles the data for the 4-to-5 increase. \n \nSecond, by the time a person arrives at a location, that location typically has been \nsimulated for a while, disguising inconsistencies caused due to lower LODs. \n",
      "content_length": 3047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": " \n \nWhen to Decrease the Detail \nThere is a problem with reshaping the LOD membrane when an object moves between two \nlocations repeatedly. The LOD may start to oscillate, increasing the resources‘ consumption. \nYou can do either of the following two things. First, you can have a larger crater \ndetermining when to decrease the LOD. This larger crater would embrace the smaller crater \nthat enforces the LOD increase. Second, you can use, as we did, a garbage mechanism. \nWith the garbage mechanism, you do not decrease LOD until the resources are needed for \nsomething else. This mimics the larger crater automatically. However, because the cleanup \ncauses overhead, it is better to use the garbage mechanism a bit sooner than the resources \nare actually needed, letting the mechanism work over several time steps. \n \nCreating the Structure of the World \nIn practice, it rarely makes sense to have more LODs than levels of the spatial hierarchy. In \nthe spatial hierarchy, you should keep a reasonable number of sublocations for every \nparent—neither 1 nor 50. This is not a strict rule, but if you violate this principle too often, \nthe hierarchy should be reconstructed. Even though higher numbers may make sense \nlogically (a building with many small rooms), with respect to the technique presented here, \nthis would increase the overhead for a LOD change. \nRecall that LODs also have to be assigned to the behavioral hierarchy. You should do this \nconsistently, meaning the degree of abstraction of two tasks from a particular level should \nbe similar. For instance, if LOD 3 is assigned to watering a garden, it should also be \nassigned to cooking, but not peeling potatoes. For technical purposes, you may need more \nlevels in behavioral hierarchy than LODs, as demonstrated on the CD example. \nAnother aspect to keep in mind is that after assigning LODs to tasks, objects required for \nthese tasks must have their existence level set accordingly. Assume watering a garden has \nLOD 3, and the garden is an abstract point at LOD 3. If it is required by designers that \nwatering a garden has to run with a watering can at this LOD, meaning the gardener has to \npick up the can before he comes to the garden, the existence level of the can must be 3 or \nless. Watering a garden would then mean something like, ―Stay there holding the can for a \nspecified amount of time, after which the garden will become watered at one instant.‖ Of \ncourse, designers might also specify that the gardener is able to water the garden without \nthe can at LOD 3; the task‘s outcome would be the same, only the can would not be \nrequired. In the latter case, you would need to generate the can after the LOD increase. \nNote that the technique does not allow for two existence levels for one object kind—for \nexample, it is not possible that a knife for fighting has existence level 3, while a knife for \ncooking has 5. You must define either one object kind with the lower number or two \ndifferent object kinds with two different existence levels. \n \nSource Code Summary \nThe application included on the CD is a simulator of virtual worlds with LOD AI (in other \nwords, it is not a game). The examples discussed in this chapter are based on the \nimplemented demo world, which features five LODs and three kinds of NPCs: miners, \nbarmen, and singers. The following behaviors have been implemented: walking home from \na pub or a mine and vice versa, leisure time behavior (for pubs), and working behavior (for \nmines). The code is in Java, and the documentation that is included details the LOD \ntechnique further. \n",
      "content_length": 3600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": " \n \n \nConclusion \nLevel-of-detail techniques allow for compromising between the consumption of \ncomputational resources and simulation plausibility. However, every LOD technique \npresents extra work for designers and programmers. Thus, it should be contemplated \ncarefully which LOD approach is needed and whether it is needed at all. \nThe technique introduced in this gem fits well for large worlds with many NPCs with \neveryday behavior. It capitalizes on the fact that both space and behavior of NPCs can be, \nand often already are, represented hierarchically. The different number of LODs helps not \nonly with simulation plausibility, but also with keeping the overhead under control during \nLOD changes. \nThe technique is generic, which means that for specific domains, special-purpose \nmechanisms can outperform it—for example, for fighting behavior [Brockington02, \nGrinke04]. Even in complex worlds, any special-purpose mechanism can be augmented with \nsome or all of this technique if needed. \n \nAcknowledgements \nThis technique and the simulator on the CD were developed as a part of research projects \n1ET100300517 of the Program Information Society and MSM0021620838 of the Ministry of \nEducation of the Czech Republic. We want to thank several students who participated in the \nsimulator development: Martin Juhász, Jan Kubr, Jiří Kulhánek, Pavel Šafrata, Zdeněk Šulc, \nJiří Vorba, and Petr Zíta. \n \nReferences \n[Bratman87] Bratman, Michael E. Intention, Plans, and Practical Reason. Harvard University \nPress, 1987. \n[Brockington02] Brockington, Mark. ―Level-Of-Detail AI for a Large Role-Playing Game.‖ AI \nGame Programming Wisdom I. Boston: Charles River Media, 2002. 419–425. \n[Champandard08] Champandard, Alex J. ―Getting Started with Decision Making and Control \nSystems.‖ AI Game Programming Wisdom IV. Boston: Charles River Media, 2008. 257–264. \n[Chenney01] Chenney, Stephen. ―Simulation Level-Of-Detail.‖ 2001. University of \nWisconsin. n.d. <http://www.cs.wisc.edu/~schenney/research/culling/chenney-\ngdc2001.pdf>. \n[Fu04] Fu, Dan, and Ryan Houlette. ―The Ultimate Guide to FSMs in Games.‖ AI Game \nProgramming Wisdom II. Boston: Charles River Media, 2004. 283–302. \n[Grinke04] Grinke, Sebastian. ―Minimizing Agent Processing in ‗Conflict: Desert Storm.‘‖ AI \nGame Programming Wisdom II. Boston: Charles River Media, 2004. 373–378. \n[Harvey02] Harvey, Michael, and Carl Marshall. ―Scheduling Game Events.‖ Game \nProgramming Gems 3. Boston: Charles River Media, 2002. 5–14. \n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": " \n \n[Isla05] Isla, Damian. ―Handling Complexity in Halo 2.‖ 3 Nov. 2005. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/2250/gdc_2005_proceeding_handling_php>. \n[Šerý06] Šerý, Ondřej, et al. ―Level-Of-Detail in Behaviour of Virtual Humans.‖ Proceedings \nof SOFSEM 2006: Theory and Practice of Computer Science 3831 (2006): 565–574. \n[Wiki09] Wikipedia, The Free Encyclopedia. ―Discrete Event Simulation.‖ 2009. Wikipedia. \nn.d. <http://en.wikipedia.org/wiki/Discrete_event_simulation>. \n \n3.2. A Pattern-Based Approach to Modular AI for Games \nKevin Dill, Boston University \nkdill4@gmail.com \nA great deal of time and effort is spent developing the AI for the average modern game. \nYears ago, AI was often an afterthought for a single gameplay programmer, but these days \nmost game projects employ at least one dedicated AI specialist, and entire AI teams are \nbecoming increasingly common. At the same time, more and more developers are coming \nto realize that, even in multiplayer games, AI is not only a critical component for providing \nfun gameplay, but it is also essential if we are going to continue to increase the sense of \nrealism and believability that were previously the domain of physics and rendering. It does \nno good to have a brilliantly rendered game with true-to-life physics if your characters feel \nlike cardboard cutouts or zombie robots. \nGiven this increase in team size, the increasing prominence of AI in the success or failure of \na game, and the inevitable balancing and feature creep that occur toward the end of every \nproject, it behooves us to search for AI techniques that enable fast implementation, shared \nconventions between team members, and easy modification. Toward that end, this gem \ndescribes methods for applying patterns to our AI in such a way as to allow it to be built, \ntuned, and extended in a modular fashion. \nThe key insight that drives the entirety of this work is that the decisions made by the AI can \ntypically be broken down into much smaller considerations, individual tests used in \ncombination to make a single decision. The same considerations can apply to many different \ndecisions. Furthermore, the evaluation of those considerations can be performed \nindependent of the larger decision, and then the results can be combined as necessary into \na final decision. Thus, we can implement the logic for each consideration once, test it \nextensively, and then reuse that logic throughout our AI. \nNone of the core principles described here are new. They can be found throughout software \nengineering and academic AI in a variety of forms. However, all too often we game \nprogrammers rush into building code, trying to solve our specific problem of the day and get \nthe product out the door, without taking a step back and thinking about how to improve \nthose systems. With some thought and organization, it could be easier to change, extend, \nand even reuse bits and pieces in future projects. Taking the time to do that would pay off \nboth in the short run, making life easier (and thus improving the final result) for the current \ntitle, and in the long run, as more AI code is carried forward from game to game. \nA Real-World Example: Apartment Shopping \nLet‘s begin with a real-world example that illustrates the general ideas behind this work. \nImagine that you have just taken a new job as an AI engineer at Middle of Nowhere Games, \nand you are in the process of searching for an apartment in some faraway city. You might \n",
      "content_length": 3500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": " \n \nvisit a variety of candidates, write down a list of the advantages and disadvantages of each, \nand then use that list to guide your final decision. \nFor an apartment in a large complex near a busy shopping area, for example, your list \nmight look something like this: \n606 Automobile Way, Apt 316 \nPros \nCons \nClose to work \nGreat view…of a used car lot \nEasy highway access \nNo off-street parking \nConvenient shopping district Highway noise \n \nAnother apartment, located in the attic of a kindly old lady‘s country house, might have a \nwholly different list: \n10-B Placid Avenue \nPros \nCons \nLow rent \n45-minute commute \nNearby woods, biking trails \nNo shopping nearby \nElectricity and water included Thin walls, landlady downstairs \n \nThese lists clearly reflect the decision-maker‘s personal taste—in fact, it seems likely that if \ntwo people were to make lists for the same apartment, their lists would have little in \ncommon. It is not the actual decision being made that‘s important, but rather the process \nbeing used to arrive at that decision. That is to say, given a large decision (Where should I \nlive for the next several years of my life?), this process breaks that decision down into a \nnumber of independent considerations, each of which can be evaluated in isolation. Only \nafter each consideration has been properly evaluated do we tackle the larger decision. \nThere is a reasonably finite set of common considerations (such as the rent, ease of \ncommute, size of the apartment, aesthetics of the apartment and surrounding environment, \nand so forth) that would commonly be taken into account by apartment shoppers. From an \nAI point of view, if we can encode those considerations, we can then share the logic for \nthem from actor to actor, and in some cases even from decision to decision. \nAs an example of the latter advantage, imagine that we wanted to select a location for a \npicnic. Several of the considerations used for apartments—such as overall cost, the \naesthetics of the surrounding environment, and the length of the drive to get there—would \nalso be used when picking a picnic spot. Again, if it were an NPC making this decision, then \ncleverly designed code could be shared in a modular way and could perhaps even be \n",
      "content_length": 2257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": " \n \nconfigured by a designer once the initial work of implementing the overall architecture and \nindividual considerations was complete. \nAnother advantage of this type of approach is that it supports extensibility. For example, \nimagine that after visiting several apartments, we came across one that had a hot tub and \npool or a tennis court. Previously, we had not even considered the availability of these \nfeatures. However, we can now add this consideration to our list of pros and cons without \ndisturbing the remainder of our logic. \nAt this point we have made a number of grandiose claims—hopefully sufficient to pique the \nreader‘s interest—but we clearly have some practical problems as well. What is described \nabove is a wholly human approach to decision-making, not easily replicated in code. Our \nnext step, then, should be to see whether we can apply a similar approach to making \nrelatively simple decisions, such as those that rely on a single yes or no answer. \n \nBoolean Decisions \nMany common architectures rely on simple Boolean logic at their core. For example, from a \nfunctional point of view, finite state machines have a Boolean decision-maker attached to \neach transition, determining whether to take that transition given the current situation. \nBehavior trees navigate the tree through a series of Boolean decisions (take this branch or \ndon‘t take this branch) until they arrive at an action they want to take. Rule-based \narchitectures consist of a series of ―rules,‖ each of which is a Boolean decision stating \nwhether or not to execute the associated action. And so forth. \nConstructing Decisions \nTo build a pattern-based architecture for our AI, we first need to define a shared interface \nfor our considerations and then decide how to combine the results of their evaluation into a \nfinal decision. For Boolean decisions, consider the following interface: \nclass IConsiderationBoolean \n{ \npublic: \n    IConsiderationBoolean()                    {} \n    virtual ~IConsiderationBoolean()           {} \n \n    // Evaluate this consideration \n    bool Evaluate(const DecisionContext& context) = 0; \n    // Load the data that controls our decisions \n    void LoadData(const DataNode& node) = 0; \n} \n \nEvery consideration will inherit from this interface and therefore will be required to specify \nthe Evaluate() and LoadData() methods. \nEvaluate() takes a DecisionContext as its only argument. The context contains \nwhatever information might be needed to make a decision. For example, it might include \nthe current game time, a pointer to the actor being controlled, a pointer to the game world, \nand so on. Alternatively, it might simply be a pointer to the actor‘s knowledge base, where \nbeliefs about the world are stored. Regardless, Evaluate() processes the state of the world \n(as contained in the context) and then returns true if execution is approved or false \notherwise. \n",
      "content_length": 2915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": " \n \nThe other mandatory function is LoadData(). The data being loaded specifies how the \ndecision should be made. We will go into more detail on this issue later in this gem. \nGames are rife with examples of considerations that can be encoded in this way. For \nexample, we might have a health consideration that will only allow an action to be taken if a \ncharacter‘s hit points are in a specified range. A time-of-day consideration might only allow \nan action to take place during the day (or at night or between noon and 1:00 p.m.). A cool-\ndown consideration could prevent an action from being taken if it has been executed in the \nrecent past. \nThe simplest approach for combining Boolean considerations into a final decision is to give \neach consideration veto power on the overall decision. That is, take the action being gated \nby the decision if and only if every consideration returns true. Obviously, more robust \ntechniques can be implemented, up to and including full predicate logic, but this simple \napproach works well for a surprisingly large number of cases and has the advantage of \nbeing extremely simple to implement. \nA Simple Example: First-Person Shooter AI \nAs an example of this approach in a game environment, consider the simple state machine \nin Figure 3.2.1, which is a simplified version of what might be found in a typical first-person \nshooter‘s combat AI (for example). \nFigure 3.2.1. A simplified state machine for FPS combat AI. \n \n \nIn this AI, Chase is our default state. We exit it briefly to dodge or take a shot, but then \nreturn to it as soon as we‘re done with that action. Here are the considerations we might \nattach to each transition: \nChase ⇒ Shoot: \n \nWe have a line of sight to the player. \n \nIt has been at least two seconds since our last shot. \n \nIt has been at least one second since we last dodged. \n \nThe player‘s health is over 0 percent (that is, he‘s not dead yet). \nChase ⇒ Dodge: \n \nWe have a line of sight to the player. \n \nThe player is aiming at us. \n \nIt has been at least one seconds since our last shot. \n \nIt has been at least five seconds since we last dodged. \n \nOur health is below 60 percent. (As we take more damage, we become more \ncautious.) \n",
      "content_length": 2217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": " \n \n \nOur health is less than 1.2 times the player‘s health. (If we‘re winning, we become \nmore aggressive.) \nShoot ⇒ Chase: \n \nWe‘ve completed the Shoot action. \nDodge ⇒ Chase: \n \nWe‘ve completed the Dodge action. \nOne advantage of breaking down our logic in this way is that we can encapsulate the shared \nlogic from each decision in a single place. Here are the considerations used above: \n \nLine of Sight Consideration. Checks the line of sight from our actor to the player \n(or, more generally, from our actor to an arbitrary target that can be specified in the \ndata or the DecisionContext). \n \nAiming at Consideration. Checks whether the player‘s weapon is pointed toward \nour actor. \n \nCool-down Consideration. Checks elapsed time since a specified type of action \nwas last taken. \n \nAbsolute Health Consideration. Checks whether the current health of our actor \n(or the player) is over (or under) a specified cutoff. \n \nHealth Comparison Consideration. Checks the ratio between our actor‘s health \nand the player‘s health. \n \nCompletion Consideration. Checks whether the current action is complete. \nEach of those considerations represents a common pattern that can be found not only in \nthis specific example, but also in a great many decisions in a great many games. These \npatterns are not unique to these particular decisions or even this particular genre. In fact, \nmany of them can be seen in one form or another in virtually every game that has ever \nbeen written. Line-of-sight checks, for example, or cool-downs to prevent abilities from \nbeing used too frequently…these are basic to game AI. \nAI Specification \nAt this point we have all the pieces we need. We know what behaviors we plan to support \n(in this case, one behavior per state), we know all of the decisions that need to be made \n(represented by the transitions), and we know the considerations that go into each decision. \nHowever, there is still some work to do to put it all together. \nMost of what needs to be done is straightforward. We can implement a character class that \ncontains an AI. The AI contains a set of states. Each state contains a list of transitions, and \neach transition contains a list of considerations. \nKeep in mind that the considerations don‘t always do the same thing. For example, both \nChase ⇒ Shoot and Chase ⇒ Dodge contain an Absolute Health consideration, but those \nconsiderations are expected to return true under very different conditions. For Chase ⇒ \nShoot, we return true if the health of the player is above 0 percent. For Chase ⇒ Dodge, on \nthe other hand, we return true if the health of our actor is below 60 percent. More \ngenerally, each decision that includes this condition also needs to specify whether it should \nexamine the health of the player or our actor, whether it should return true when that value \nis above or below the cutoff, and what cutoff it should use. The information used to specify \nhow each instance of a consideration should evaluate the world is obtained through the \nLoadData() function: \nvoid CsdrHealth::LoadData(const DataNode& node) \n",
      "content_length": 3091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": " \n \n{ \n    // If true we check the player’s health, otherwise \n    // we check our actor’s health. \n    m_CheckPlayer = node.GetBoolean(―CheckPlayer‖); \n \n    // The cutoff for our health check – may be the \n    //  upper or lower limit, depending on the value \n    //  of m_HighIsGood. \n    m_Cutoff = node.GetFloat(―Cutoff‖); \n \n    // If true then we return true when our health is \n    //  above the cutoff, otherwise we return true when \n    //  our health is below the cutoff. \n    m_HighIsGood = node.GetBoolean(―HighIsGood‖); \n} \n \nAs you can see, this is fairly straightforward. We simply acquire the three values the \nEvaluate() function will need from our data node. With that in mind, here is the \nEvaluate() function itself: \nbool CsdrHealth::Evaluate(const DecisionContext& ctxt) \n{ \n   // Get the health that we’re checking – either ours \n   // or the player’s. \n   float health; \n   if (m_CheckPlayer) \n       health = ctxt.GetHealth(ctxt.GetPlayer()); \n   else \n       health = ctxt.GetHealth(ctxt.GetMyActor()); \n \n   // Do the check. \n   if (m_HighIsGood) \n       return currentHealth >= m_Cutoff; \n   else \n       return currentHealth <= m_Cutoff; \n   } \n} \n \nAgain, there‘s nothing complicated here. We get the appropriate health value (either ours or \nthe player‘s, depending on what LoadData() told us) from the context and compare it to \nthe cutoff. \nThe simplicity of this code is in many ways the entire point. Each consideration is simple \nand easy to test in its own right, but combined they become powerfully expressive. \nExtending the AI \nNow that we have built a functional core AI for our shiny new FPS game, it‘s time to start \niterating on that AI, finding ways in which it‘s less than perfect, and fixing them. As a first \nstep, let‘s imagine that we wanted to add two new states: Flee and Search (as seen in \nFigure 3.2.2). Like all existing states, these new states transition to and from the Chase \nstate. Here are the considerations for our new transitions: \n",
      "content_length": 1998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": " \n \nFigure 3.2.2. Our state machine with two new states. \n \n \nChase ⇒ Flee: \n \nOur health is below 15 percent. \n \nThe player‘s health is above 10 percent. (If he‘s almost dead, finish him.) \nFlee ⇒ Chase: \n \nThe player‘s health is below 10 percent. \n \nWe have a line of sight to the player. \nChase ⇒ Search: \n \nWe don‘t have a line of sight to the player. \nSearch ⇒ Chase: \n \nWe have a line of sight to the player. \nAs you can see, adding these states should be fairly simple. We will need to implement the \nnew behaviors, but all of the considerations needed to decide whether to execute those \nbehaviors already exist. \nOf course, not all changes can be made using existing considerations. For example, imagine \nthat we want to make two further changes to our AI: \n \nQA complains that dodging is too predictable. Instead of always using a five-second \ncool-down, they want us to use a cool-down that varies randomly between three and \nseven seconds. \n \nThe artists would like to add a really cool-looking animation for drawing your \nweapon. In order to support this, the designers have asked us to have the characters \ndraw their weapons when they enter Chase and then holster them again if they go \ninto Search. \nThe key is to find a way to modify our existing code so that we can support these new \nspecifications without affecting any other portion of the AI. We certainly don‘t want to have \nto go through the entire AI for every character, find every place that these considerations \nare used, and change the data for all of them. \n",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": " \n \nImplementing the variable cool-down is fairly straightforward. Previously the Cool-Down \nconsideration took a single argument to specify the length of the cool-down. We‘ll modify it \nto optionally take minimum and maximum values instead. Thus, all the existing cases will \ncontinue to work (with an exact value specified), but our Cool-Down consideration will now \nhave improved functionality that can be used to fix this bug and can also be used in the \nfuture as we continue to build the AI. We‘ll have to take some care in making sure that we \ncheck that exactly one type of cool-down is specified. In other words, the user needs to \nspecify either an exact cool-down or a variable cool-down; he or she can‘t specify both. An \nAssert in LoadData() should be sufficient. \nFor the second change, we can make the Chase behavior automatically draw the weapon (if \nit‘s not drawn already), so that doesn‘t require a change to our decision logic. We do need \nto ensure that we don‘t start shooting until the weapon is fully drawn, however. In order to \ndo that, we simply implement an Is Weapon Drawn consideration and add it to the \ntransition from Chase to Shoot. \nData-Driven AI \nOne thing to notice is that all of the values required to specify our AI are selected at game \ndesign time. That is, we determine up front what decisions our AI needs to make, what \nconsiderations are necessary to support those decisions, and what tuning values we should \nspecify for each consideration. Once the game is running, they are always the same. For \nexample, our actor‘s hit points might go up or down, but if a decision uses an Absolute \nHealth consideration, then the threshold at which we switch between true and false never \nchanges during gameplay. \nSince none of this changes during gameplay, nearly all of the decision-making logic can be \nspecified in data. We can specify the AI for each character, where the AI contains a set of \nstates, each state contains a list of transitions, each transition contains a list of \nconsiderations, and each consideration contains the tuning values used for that portion of \nthe AI. This sort of hierarchical structure is something that XML does well, making it an \nexcellent choice for our data specification. \nAs with all data-driven architectures, the big advantage is that if we want to change the way \nthe decisions are made, we only have to change data, not code. No recompile is required. If \nyou‘ve implemented the ability to reload the data while the game is live, you don‘t even \nneed to restart the game, which can save a lot of time when testing a situation that is tricky \nto re-create in-game. Of course, some changes, such as implementing an entirely new \nconsideration, will require code changes, but much of the tuning and tweaking—and \nsometimes even more sizeable adjustments—will not. \nOne thing to consider when taking this approach is whether it‘s worth investing some time \ninto the tools you use for data specification. Our experience has been that the time spent \nspecifying and tuning behavior is significantly greater than the time spent writing the core \nreasoner. Good tools can not only make those adjustments quicker and easier (especially if \nthey‘re integrated into the game so that adjustments can be made in real time), but they \ncan also include error checking for common mistakes and help to avoid subtle behavioral \nbugs that might otherwise be hard to catch. Further, since many of these considerations are \nbroadly applicable to a variety of games, not only the considerations but also the tools for \nspecifying them could be carried from game to game as part of your engine (or even \nintegrated into a new engine). Finally, it is often possible to allow designers and artists to \nspecify AI logic if you create good tools for doing so, giving them more direct control over \nthe look and feel of the game and freeing you up to focus on issues that require your \ntechnical expertise. \nMeta-Considerations \n",
      "content_length": 3985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": " \n \nOne of the benefits of this approach is that it reduces the amount of duplicate code in your \nAI. For example, if there are seven different decisions that evaluate the player‘s hit points, \ninstead of writing that evaluation code in seven places we write it once, in the form of the \nAbsolute Health consideration. \nAs our work on the AI progresses, we might quickly find that we also have an Absolute \nMana consideration, an Absolute Stamina consideration, a Time of Day consideration, a \nCool-Down consideration, and a Distance to Player consideration. Although each of these \nconsiders a different aspect of the situation in-game, under the covers they each do exactly \nthe same thing. That is, they compare a floating-point value from the game (such as the \nplayer‘s hit points, the current time of day, the distance to the player, and so forth) to one \nor two static values that are specified in data. \nWith that in mind, it‘s worth looking for opportunities to further reduce duplicate code by \nbuilding meta-considerations, which is to say high-level considerations that handle the \nimplementation of more specific, low-level considerations, such as the ones given above. \nThis has the advantage of not only further reducing the duplication of code, but also \nenforcing a uniform set of conventions for specifying data. In other words, if all of those \nconsiderations inherit from a Float Comparison consideration base class, then the data for \nthem is likely to look remarkably similar, and a designer specifying data for one that he \nhasn‘t used before is likely to get the result he expects on his first try, because it works the \nsame way as every other Float Comparison consideration that he‘s used before. \n \nFloat-Based Decisions \nWhile nearly all decisions are ultimately Boolean (that is, an AI either takes an action or it \ndoesn‘t), it is often useful to evaluate the suitability of a variety of options and then allow \nthat evaluation to guide our decisions. As with Boolean approaches, there are a variety of \napproaches for doing this. Discussion of these approaches can be found throughout the AI \nliterature. A few game-specific examples include [Dill06, Dill08, and Mark09]. For the \npurposes of this gem, however, the interesting question is not how and when to use a float-\nbased approach, but rather how to build modular, pattern-based evaluation functions when \nwe do. \nAn Example: Attack Goals \nImagine that we are responsible for building the opposing-player AI for a real-time strategy \ngame. Such an AI would need to decide when and where to attack. In order to do this, we \nmight periodically score several prospective targets. For each target we would consider a \nnumber of factors, including its economic value (that is, whether it generates revenue or \ncosts money to maintain), its impact on the strategic situation (for example, would it allow \nyou access to the enemy‘s territory, consolidate your defenses, or protect your lines of \nsupply), and the overall military situation (in other words, whether you can win this fight). \nOur next step should be to find a way for each of these considerations to evaluate the \nsituation independently that enables us to easily combine the results of all of those \nevaluations into a single score, which can be used to make our final decision. \nCreating an evaluation function is as much art as science, and in fact there is an entire book \ndedicated to this subject [Mark09]. However, just as with Boolean decisions, there are \nsimple tricks that can be used to handle the vast majority of situations. Specifically, we can \nmodify our Evaluate() function so that it returns two values: a base priority and a final \nmultiplier. When every consideration has been evaluated, we first add all of the base \npriorities together and then multiply that total by the product of the final multipliers. This \n",
      "content_length": 3881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": " \n \nallows us to create considerations that are either additive or multiplicative in nature, which \nare two of the most common techniques for creating priority values. \nComing back to our example, the considerations for economic value and strategic value \nmight both return base priorities between –500 and 500, generating an overall base priority \nbetween –1,000 and 1,000 for each target. They would return a negative value if, from their \npoint of view, taking this action would be a bad idea. For example, capturing a building that \nhas an ongoing upkeep cost might receive a negative value from the economic \nconsideration (unless it had some economic benefit to offset that cost), because once you \nown it, you‘ll have to start paying its upkeep. Similarly, attacking a position that, if \nobtained, would leave you overextended and exposed would receive a negative value from \nthe strategic consideration. These considerations could return a final multiplier of 1. \nThe consideration for the military situation, however, could be multiplicative in nature. That \nis, it would return a base priority of 0 but would return a final multiplier between 0 and 3. \n(For a better idea of how to generate that multiplier, see our previous work [Dill06].) Thus if \nthe military situation is completely untenable (in other words, the defensive forces are much \nstronger than the units we would use to attack), then we could return a very small \nmultiplier (such as 0.000001), making it unlikely that this target would be chosen no matter \nhow attractive it is from an economic or strategic standpoint. On the other hand, if the \nmilitary situation is very favorable, then we would strongly consider this target (by \nspecifying a multiplier of 3), even if it is not tremendously important in an economic or \nstrategic sense. If we have no military units to attack with, then this consideration might \neven return a multiplier of 0. \nWe do not execute an action whose overall priority is less than or equal to zero. Thus if all \nof the base priorities add up to a negative value, or if any consideration returns a final \nmultiplier of zero, then the action will not be executed. Just as in the Boolean logic case, a \nsingle consideration can effectively veto an action by returning a final multiplier of zero. \nAs with the previous examples, the greatest value to be found is that these same \nconsiderations can be reused elsewhere in the AI. For example, the economic value might \nbe used when selecting buildings to build or technologies to research. The strategic value \nmight be used when selecting locations for military forts and other defensive structures. The \nmilitary situation would be considered not only for attacks, but also when deciding where to \ndefend, and perhaps even when deciding whether to build structures in contested areas of \nthe map. \nAlternate Approaches \nOne weakness of the aforementioned approach is that it only allows considerations to have \nadditive or multiplicative effects on one another. Certainly there are many other ways to \ncombine techniques—in fact, much of the field of mathematics addresses this topic! One \ncommon trick, for example, is to use an exponent to change the shape of the curve when \ncomparing two values. Certainly we can extend our architecture to include this, perhaps \nincluding an exponent to the set of values returned by our Evaluate() function and \napplying all of the exponents after the final multipliers. Doing so significantly increases the \ncomplexity of our AI, however, because this new value needs to be taken into account with \nevery consideration we implement and every decision we make. This may not seem like a \nbig deal, but it can make the task of specifying, tuning, and debugging the AI significantly \nharder than it would otherwise be—especially if it is to be done by non-technical folks who \nmay not have the same intuitive sense of how the numbers combine as an experienced AI \nengineer would. \nAlong the same lines, in many cases even the final multiplier is overkill. For simpler \ndecisions (such as those in an FPS game or an action game), we can often have the \nEvaluate() function return a base priority as before, but instead of returning a multiplier, \nit can simply return a Boolean value that specifies whether it wants to veto this action. If \n",
      "content_length": 4333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": " \n \nany consideration returns false, then the action is not taken (the score is set to 0); \notherwise, the score is the sum of the base priorities. \n \nConclusion \nThis gem has presented a set of approaches for building modular, pattern-based \narchitectures for game AI. All of these approaches function by breaking a decision into \nseparate considerations and then encoding each consideration independently. There are \nseveral advantages to techniques of this type: \n \nCode duplication between decisions is dramatically reduced, because the code for \neach consideration goes in a single place. \n \nConsiderations are reusable not only within a single project, but in some cases also \nbetween multiple projects. As a result, AI implementation will become easier as the \nlibrary of considerations grows larger and more robust. \n \nMuch of the AI can be specified in data, with all the advantages that implies. \n \nWith proper tools and a good library of considerations, designers and artists can be \nenabled to specify AI logic directly, both getting them more directly involved in the \ngame AI and freeing up the programmer for other tasks. \n \nReferences \n[Dill06] Dill, Kevin. ―Prioritizing Actions in a Goal-Based AI.‖ AI Game Programming Wisdom \n3. Boston: Charles River Media Inc., 2006. 321–330. \n[Dill08] Dill, Kevin. ―Embracing Declarative AI with a Goal-Based Approach.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media Inc., 2008. 229–238. \n[Mark09] Mark, Dave. Behavioral Mathematics for Game AI. Course Technology PTR, 2009. \n \n3.3. Automated Navigation Mesh Generation Using Advanced \nGrowth-Based Techniques \nD. Hunter Hale \nG. Michael Youngblood \nWhen implementing a navigation system for intelligent agents in a virtual environment, the \nagent‘s world representation is one of the most important decisions of the development \nprocess [Tozour04]. A good world representation provides the agent with a wealth of \ninformation about its environment and how to navigate through it. Conversely, a bad \nrepresentation of the world can confuse or mislead an agent and become more of a \nhindrance than an aid. Currently, the most common type of world representation is the \nnavigation mesh [McAnils08]. This mesh contains a complete listing of all of the navigable \nareas (negative space) and occupied areas (positive space) present in a level or area of a \ngame. \nTraditional methods of generating the navigation mesh focus on using the vertices of \nobjects to generate series of triangles. These triangles then become the navigation mesh \n[Tozour02]. This does generate high-coverage navigation meshes, but the meshes tend to \n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": " \n \nhave areas that can cause problems for agents navigating through the world. These \nproblem areas take the form of many separate triangular negative space areas coming \ntogether at a single point. Agents or other objects that are standing on or near this point \nare simultaneously in more than one region. The presence of objects in more than one \nregion at once means that every region will have to be evaluated for events involving the \noverlapping objects instead of just a single region if objects were well localized. \nInstead of using a triangulation-based navigation mesh generation technique, we \napproached the problem using the Space Filling Volume (SFV) algorithm [Tozour04] as a \nbase. SFV is a growth-based technique that first seeds the empty areas of a game world \nwith quads or cubes and then expands the objects in every direction until they hit an \nobstruction. The quads and the connections between them define the navigation mesh. By \nusing quads as the core shape in the algorithm, the problem of many regions coming \ntogether in a single point is dramatically reduced, since quads can only meet at most four \ncorners. This basic approach works well for worlds composed of axis-aligned obstructions \nbut produces low-coverage navigation meshes when applied to non-axis-aligned worlds or \nhighly complex worlds. Our improved algorithms address these limitations, as well as \nprovide several other benefits over traditional navigation mesh generation techniques. \nThe two algorithms described here are enhancements to the traditional implementations of \n2D and 3D Space Filling Volumes. The first is a 2D algorithm called Planar Adaptive Space \nFilling Volumes (PASFV). PASFV consumes a representation of an arbitrary non-axis-aligned \n3D environment, similar to a blueprint of a building, and then generates a high-quality \ndecomposition. Our new decomposition algorithms seed the world with growing quads, \nwhich, when a collision with geometry occurs, dynamically increase their number of sides to \nbetter approximate the shape the growing region intersected. Using the ability to \ndynamically create higher-order polygons from quads along with a few other features not \npresent in classic SFV, PASFV generates almost 100-percent coverage navigation meshes \nfor levels where it is possible to generate planar splices of the obstructing geometry in the \nlevel. \nThe second algorithm, Volumetric Adaptive Space Filling Volumes (VASFV), is, as the name \nimplies, a native 3D implementation of Adaptive Space Filling Volumes with several \nenhancements. The enhancements allow VASFV to grow cuboids that morph into complex \nshapes to better adapt to the geometry of the level they are decomposing, similar to the 2D \nversion. This algorithm, like its 2D cousin, generates a high-coverage decomposition of the \nenvironment; however, the world does not need to be projected into a planar \nrepresentation, and the native geometry can be decomposed without simplification. In \naddition, this algorithm has a speed/time advantage over the PASFV in post-processing \nbecause it can consume complex levels in a single run. PASFV generally has to decompose a \nlevel one floor at a time, and the generated navigation meshes must then be reconnected to \nbe useful. VASFV will generate a single navigation mesh per level, removing a potentially \nexpensive step from the process of generating a spatial decomposition. \nThe Algorithms \nBoth the PASFV and VASFV algorithms work off the common principle of expanding a grid of \npre-seeded regions in a game environment to fill all of the available negative space. In \npractice, this filling effect looks somewhat similar to a marshmallow that has been heated in \nthe microwave. Both of these algorithms use a similar approach, but the implementations \nare sufficiently different that each algorithm deserves a full explanation. \nPASFV \nThe PASFV algorithm [Hale08] is an iterative algorithm that can be broken down into a \nseries of simple steps. First, a set of invariants and starting conditions must be established \nand maintained. For our implementation of PASFV, all of the input geometry must be \nconvex. This allows the use of the point-in-convex-object collision test [Schneider03] when \n",
      "content_length": 4250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": " \n \ndetermining whether a growing region has intruded into a positive space area. If the input \ngeometry is not natively convex, it can be converted to be convex by using a triangular \nsubdivision algorithm. While the algorithm is running, the following two conditions must be \nmaintained to have a successful decomposition. First, at the end of every growth cycle, all \nthe negative space regions in the world must be convex; otherwise, the collision detection \ntests, which are based on an assumption of convexity, will return invalid data. Second, if a \nregion has ended a growth cycle covering an area of the level, it must continue to cover \nthat area. If this restriction is not maintained, there will be gaps in the final decomposition. \nOur algorithm begins in a state we refer to as the initial seeding state, where the world is \n―seeded‖ with a user-defined grid at specified intervals of negative space regions. These \nregions will grow and decompose the world. If the proposed seed placement falls within a \npositive space obstruction, it is discarded. Initial regions are unit squares (a unit square is a \nsquare with an edge length equal to one of the base units of the world) with four edges \narranged in a counterclockwise direction from the point closest to the origin. \nThe initial placement of these regions in the world is such that they are axis-aligned. After \nbeing seeded in the world, each of the placed regions is iteratively provided a chance to \ngrow. Growth is defined for a region as a chance to move each edge outward individually in \nthe direction of each edge‘s normal. The decomposition of a level may take two general \ncases. The first case occurs when all of the positive space regions are axis-aligned. The \nmore advanced decomposition case occurs if there is non-axis-aligned geometry. \nFirst, we will examine the base case or axis-aligned case for a spatial decomposition in \nPASFV. Growth occurs in the direction of the normal for each of the edges of a region and is \na single unit in length. After an edge has advanced, we verify the new regional coverage \nwith three collision detection tests. We want to guarantee that no points from our newly \nexpanded region have intruded into any of the other regions or any positive space \nobstructions. We also want to prove that no points from other regions or obstructions would \nbe contained within the growing region. Finally, the region will perform a self-test to ensure \nthat the region is still convex. This final check is not necessary for the base case of the axis-\naligned world and can be omitted if there are no non-axis-aligned collision objects. \nAssuming all tests return results showing there are no collisions or violations of convexity, \nthe region finalizes its current shape, and the next region grows. \nIf a collision is detected, several things must be done to correct it. First, the growing region \nmust return to its previous shape. At this point, since both the region and the obstruction it \ncollided with are axis-aligned, we know the region is parallel and adjacent to the object, as \nshown in Figure 3.3.1(a). Stopping growth here will provide an excellent representation of \nfree space near the collided object. Finally, we set a flag on the edge of the region where \nthe collision occurred. This flag indicates the edge should not attempt to grow again. The \niterative growth proceeds until no region is able to grow. This method of growth is sufficient \nto deal with axis-aligned worlds and produces results similar to traditional SFV. \nFigure 3.3.1. In this illustration we see all of the potential collision cases in PASFV. \nThe growing negative space regions are shown as white boxes, and the direction \nof growth is marked with an arrow. Positive space regions are drawn in gray. In \n(a) we see the most basic axis-aligned collision case. Then (b) shows the collision \ncase that occurs when a vertex of a positive space object intersects a growing \nnegative space region. Finally, in (c) we illustrate the most complex case, where a \nnegative space region is subdivided into a higher-order polygon to better adapt to \nworld geometry. \n",
      "content_length": 4155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": " \n \n \n \nThe advanced case algorithm for PASFV is able to deal with a much wider variety of world \nenvironments. This case begins by building from the base case algorithm; however, because \nit needs to deal with non-axis-aligned positive space regions, it incorporates several new \nways of dealing with potential collisions. When a collision with a positive space object \noccurs, one of three cases handles the collision. The first case occurs if the colliding edge of \nthe positive space object is parallel to the edge of the growing negative space region. The \nparticular obstruction is axis-aligned, so we revert to the base case. \nThe second case occurs when a single vertex of an obstruction collides with a growing edge, \nas shown in Figure 3.3.1(b). In this case, there is, unfortunately, nothing we can do to grow \nfurther in this direction. This occurs because unless we are willing to lose the convex \nproperty of the region or relinquish some of the area already covered by the region, doing \neither of these things would violate our invariants. This case reverts to the base case, and \nthe negative space around the object will have to be covered by additional seeding passes, \nwhich are described at the end of this section. \nThe final, most complex, collision case occurs when a vertex of the growing region collides \nwith a non-axis-aligned edge of a positive space obstruction, as shown in Figure 3.3.1(c). In \nthis case, the colliding vertex must be split into a pair of new vertices and a new edge \ninserted between them. This increases the order of the polygon that collided with the \nobstruction. The directions of growth for these two newly created vertices are modified so \nthey will follow the line equation for the edge that they collided with instead of the normal \nof the edge they were on. \nIn addition, potential expansions of these new vertices are limited to the extent of the \npositive space edge they collided with. In this manner, the original edges, which were \nadjacent to the collision point, grow outward. This outward movement expands the newly \ncreated edge, so that it is spread out along the obstruction. Limiting the growth of these \nvertices is important because they are creating a non-axis-aligned edge as they expand. As \nlong as this newly generated non-axis-aligned edge is adjacent to positive space, no other \nregion can interact with it, and we limit region-to-region collisions to the base case. \nBy using these three advanced case collision solutions, we are able to generate high-quality \ndecompositions for non-axis-aligned worlds. As in the simple case with an axis-aligned \nworld, the algorithm stops once all of the regions present in the world are unable to grow \nany further. \n",
      "content_length": 2736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": " \n \nThe aforementioned growth methods are not, by themselves, enough to ensure that the \nentirety of the world is covered by the resulting navigation mesh. In particular, the \ndecompositions resulting from the second collision case are suboptimal. In order to deal \nwith these issues, the second half of the PASFV algorithm comes into play. After all growth \nhas terminated, the algorithm enters a new seeding phase. In this phase, each region \nplaces new regions (seeds) in any adjacent unclaimed free space, and then we flag the \nregion as seeded so they will not be considered if there are any later seeding passes. If any \nseeds are placed, they are provided the opportunity for growth like the originally placed \nregions. This cycle of growth and seeding repeats until there are no new seeds placed in the \nworld, as shown in Figure 3.3.2. At this point, the algorithm has fully decomposed the world \nand terminates. \nFigure 3.3.2. In this illustration we see several growth and seeding cycles in the \nPASFV algorithm to decompose an area. The growing negative space regions are \nshown as white boxes, and the direction of growth is marked with an arrow. \nPositive space regions are drawn in gray. \n \n \n \nVASFV \nThe Volumetric Adaptive Space Filling Volume (VASFV) algorithm [Hale09] is a natural \nextension of the PASFV algorithm. Unlike PASFV, the VASFV algorithm works in native 3D \nand grows cubes and higher-order polyhedrons instead of quads. The initial constraint of \nconvex input shapes is the same for both algorithms. In addition, the requirements that \ndecomposed areas remain decomposed and that all regions always end a growth step in a \nconvex state are still necessary. \nThe initial setup of VASFV is very similar to its predecessor. Both algorithms begin by \nseeding a grid of initial regions throughout the world. In VASFV, the grid extends upward \nalong the Z-axis of the world as well as X-axis and Y-axis. Then, in the first of many \ndepartures from the previous algorithm, the seeds fall down in the direction of gravity until \nthey come to rest on an object. Seeds that end in the same location are removed. This \nhelps to prevent the formation of large, relatively useless regions that float above the level \nand allows the ground-based regions to grow up to the maximum allowable height. These \nseeds are initially spawned as unit cubes represented as regions with four faces listed in a \ncounterclockwise order, starting with the closest to the origin followed by the bottom and \ntop faces, respectively. At this point, the regions may grow and expand. \n",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": " \n \nAs with the planar version of this algorithm, there are two main cases to deal with: axis-\naligned and non-axis-aligned worlds. The base case for the axis-aligned world proceeds in a \nmanner almost identical to the planar algorithm. Each region is iteratively provided the \nopportunity to grow out each face one unit in the direction of the normal of each face. We \nthen run the same three tests for error conditions as performed in the planar version of the \nalgorithm. As a reminder, these three tests are checks to ensure that the growing region \nhas not intersected an existing region or obstruction with one of its vertices, that no \nregion‘s or object‘s vertices have intersected the newly expanded region with their vertices, \nand that the region is still convex. If any of these checks fail, the region will revert to its \nprevious size and not attempt to grow again in that direction. These steps are identical to \nthe planar case; however, the secondary algorithms required to implement them are more \ncomplex in 3D. As with the planar version of this algorithm, this base case will produce a \nvery good decomposition of an axis-aligned world. \nLike the planar version of the algorithm, the advanced growth case for dealing with \ncollisions with non-axis-aligned geometry can be broken down into four cases (shown in \nFigure 3.3.3). The primary determining factor of which of the cases the algorithm will go \ninto is determined by how many vertices are in collision and whether negative space \nvertices collide with positive space, or vice versa. The simplest case occurs when the \ngrowing cubic region has intersected one or more vertices of a positive space obstruction. \nJust like in 2D, since there is nothing that the growth algorithm can do to better \napproximate the free space around the object it has collided with, it returns to its previous \nvalid shape, halting further growth in that direction. \nFigure 3.3.3. This illustration shows the possible collision cases for the VASFV \nalgorithm. The growing negative space regions are shown in white. The positive \nspace objects are shown in gray. Section (a) shows the base axis-aligned collision \ncase from above. Section (b) shows the more complex positive space vertex \ncollision case, again from above. Sections (c) and (d) are illustrated slightly \ndifferently. In order to clearly show how the negative space region reacts to a \npositive space collision, the positive space object is not drawn, and the colliding \nvertex is marked with a circle. Section (c) shows the single vertex collision case \nwhere a new triangular face is inserted into the negative space region. Section (d) \nshows the more complex two-vertex collision where a quadrilateral face is \ninserted into a negative space region to better approximate the object it collided \nwith. \n",
      "content_length": 2826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": " \n \n \nThe next three collision cases occur when vertices from a single face of the growing \nnegative space region intersect positive space. The first and simplest of these cases occurs \nwhen three or more vertices of a negative space region intersect the same face of a positive \nspace object. When this happens, it means that the growing face of the negative space \nobject is parallel to and collinear with the face of the positive space obstruction it \nintersected. Therefore, these two faces are both axis-aligned. We know this because three \npoints define a plane, so by sharing these three points, both of these faces are on the same \nplane. This tells us that since the negative space face is axis-aligned, the positive space face \nwe collided with must be as well. We can thus revert to the base case for this collision. \nThe final two collision cases require the insertion of a new face into the negative space \nregion so that it adapts to the face of the collided object. The first case occurs when a single \nvertex of the region intersects an obstruction. In this case, the vertex will be subdivided into \nthree new vertices, and a new triangular face is inserted (which has the same plane \nequation as the face of the object it collided with). The normal of this new face will be the \ninverse of the normal of the face of the intersected obstruction. These new points are \nrestricted to prevent them from growing beyond the face of the obstruction they collided \nwith in order to not create more non-axis-aligned geometry. \nThe final collision case occurs when exactly two vertices of a negative space region intersect \nanother object. This means that a single edge of the region is in contact with the shape it \ncollided with and that edge needs to be split. We split the edge by adding a new rectangular \nface to the region and by subdividing each colliding vertex into two vertices. This new face \nis once again created using the negation of the normal of the face it would intersect. The \npoints involved in the collision are restricted to growing just along the collided obstruction. \nWith the last two special collision cases, it is possible to generate navigation meshes with a \ndegree of accuracy and fidelity to the underlying level that is not possible using previous \ngrowth-based techniques. \nLike the planar version of this algorithm, VASFV also uses a seeding algorithm to ensure full \ncoverage of a level. However, the volumetric seeding approach is slightly different from the \nplanar version. Once each region has reached its maximum possible extents, the seeding \n",
      "content_length": 2593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": " \n \nalgorithm iteratively provides each region a chance to create new seeds in adjacent \nnegative space regions. However, instead of immediately growing the new regions, the \nnewly placed seeds are subjected to a simulated gravity and projected downward until they \nhit something, and duplicate seeds that end up occupying the same space as already placed \nseeds are removed. At this point, the algorithm allows the newly placed seeds a chance to \ngrow. This cycle of growth and seeding repeats until no new seeds are successfully placed, \nat which point the algorithm terminates. \nThe application of gravity to seeds might not be the most obvious approach to seeding in \n3D, but it serves an important purpose in orienting region growth to better accommodate \nagent movement through the world. A good example of this occurs on staircases. First, \nconsider the case where seeds do not drop due to gravity. As shown in Figure 3.3.4, a \nregion that grows up adjacent to the bottom of the stairs will generate a single seed midway \nup the stairs, which will contain air space above the first several stairs and only land on one \nof the middle stairs. Then later seedings will result in a confusing mess of regions, none of \nwhich accurately models stair usage. Some of these regions require the agent to crawl \nthrough, while other regions require the agent to fly. \nFigure 3.3.4. This figure serves to illustrate how gravity affects the seeding \nprocess for VASFV. In this figure, we see a staircase viewed from the side. Positive \nspace regions are marked in white. Negative space regions in this illustration are \nmarked with the light gray gradient. The upper set of four time steps shows what \nhappens if the generated seeds are allowed to float freely and grow in midair. The \nlower six time steps show how the decomposition changes to better model the \nstair steps after the application of gravity to each generated seed. \n \nNow consider the same staircase decomposed using a gravity-based seeding method. With \ngravity-assisted seeding, when a seed is generated from the initial region at the bottom of \nthe stairs, it falls into the floor space of the first stair. The seed then grows outward to fully \ndecompose the floor of that single stair and grows up into the airspace over the stair. In this \nmanner, the seeds gradually climb the stairs, and each stair will become its own logical \nregion, which makes sense given how stairs are typically traversed. This gravity-based \nseeding is also applicable to other methods of world space traversal, such as flight, because \nbiasing the decomposition world in respect to the features present on the ground still makes \nsense. \n",
      "content_length": 2681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": " \n \n \nPost-Processing \nDecompositions generated using either of the algorithms presented here can be improved \nby the application of several simple post-processing steps. Most importantly, if any two \nregions are positioned such that they could be combined into a single region while \nmaintaining convexity, these regions should combine into a single region. The same \ntechnique can be applied to compress three input regions into two convex regions. This \ntechnique can be extended to higher numbers of regions, but it becomes harder to \nimplement and the returns generally decline, because larger combinations are less likely to \nyield new convex shapes. All of the negative space regions should be examined for zero-\nlength edges or collinear vertices, which, if detected, should be removed. \nA full navigation mesh can be constructed from the decomposition by linking adjacent \nnegative space regions. Aside from which negative space regions connect to each other, \neach region can also store least-cost paths to every other region. Other navigation mesh \nquality metrics can be applied to the generated mesh to determine whether it is good \nenough for its intended purpose or if some of the input parameters for initial seeding should \nbe adjusted and a better navigation mesh generated. \n \nConclusion \nIn this gem, we have presented two new growth-based methods of generating navigation \nmeshes. Both of these new methods are derived from the classic Space Filling Volumes \nalgorithm. These two methods each have areas where they specialize, and both generate \nexcellent decompositions, as shown in Figure 3.3.5. \nFigure 3.3.5. The first image shows the results of PASFV. Black areas indicate \nobstructions, while the variously colored regions show negative space. The second \nimage shows a navigation mesh generated by VASFV on a non-axis-aligned \nstaircase and viewed from the side. \n \n \nThe PASFV technique works very well for levels that can be projected to a single 2D plane. \nIt also can deal with levels that have more than one 2D representation, though these will \nrequire a touch more post-processing to combine all of the navigation meshes into a single \n",
      "content_length": 2171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": " \n \nmesh. The navigation meshes produced by this algorithm are very clean with few sharp \npoints or narrow regions that taper off to a point. Such regions are problematic because \nthey cannot contain all of an agent moving through the world, and an agent can end up in \nmany different regions at the same time. This is not a problem for PASFV because at most \nfour different negative space regions can come together at a point, since all negative-space-\nto-negative-space collisions will be axis-aligned, and the angles involved in these collisions \ntend to be around 90 degrees. PASFV is also highly comparable to other navigation mesh \ngeneration algorithms in terms of speed, as it can be shown to run in O(n1/x) with an upper \nbound of O(n), where n is the number of square units of space to decompose and x is a \nfunction of how many seeds are placed in the world [Hale09b]. \nFor more complex world environments that do not approximate to 2D very well or that \nwould require multiple 2D approximations, Volumetric Adaptive Space Filling Volumes is a \nsolution for navigation mesh generation, even though it is slightly harder to implement and \ntakes longer to run due to the more complex collision calculations. VASFV will also provide \ngood decompositions that have few narrow corners or poorly accessible regions. In addition, \nbecause of its gravity-based seeding, VASFV will better model how agents move, resulting \nin superior decompositions to traditional triangulation-based methods [Hale09a]. The run \ntime for VASFV is algorithmically the same as PASFV; however, instead of n being the \nnumber of square units in the world, it is the number of cubic units. \nBy presenting both 2D and 3D algorithms for generating spatial decompositions, we are \noffering multiple options to move beyond traditional triangulation-based methods of \nproducing a navigation mesh and into advanced growth-based techniques. The most current \nimplementations of both of these algorithms can be found at \nhttp://gameintelligencegroup.org/projects/cgul/deaccon/, along with some other interesting \ntools and techniques for navigation mesh generation and evaluation. \n \nReferences \n[Hale08] Hale, D. H., G. M. Youngblood, and P. Dixit. ―Automatically-Generated Convex \nRegion Decomposition for Real-time Spatial Agent Navigation in Virtual Worlds.‖ Artificial \nIntelligence and Interactive Digital Entertainment (AIIDE). Stanford University, Stanford, \nCA. 2008. \n[Hale09a] Hale, D. H., and G. M. Youngblood. ―Full 3D Spatial Decomposition for the \nGeneration of Navigation Meshes.‖ Artificial Intelligence and Interactive Digital \nEntertainment (AIIDE). Stanford University, Stanford, CA. 2009. \n[Hale09b] Hale, D. H., and G. M. Youngblood. ―Dynamic Updating of Navigation Meshes in \nResponse to Changes in a GameWorld.‖ Florida Artificial Intelligence Research Society \n(FLAIRS). The Shores Resort and Spa, Daytona Beach, FL. 2009. \n[McAnils08] McAnils, C., and J. Stewart. ―Intrinsic Detail in Navigation Mesh Generation.‖ AI \nGame Programming Wisdom 4. Boston: Charles River Media, 2008. 95–112. \n[Touzor02] Tozour, P. ―Building a Near-Optimal Navigation Mesh.‖ AI Game Programming \nWisdom. Boston: Charles River Media, 2002. 171–185. \n[Tozour04] Tozour, P. ―Search Space Representations.‖ AI Game Programming Wisdom 2. \nBoston: Charles River Media, 2004. 85–102. \n[Schneider03] Schneider, P. and D. Eberly. ―Point in Polygon/Polyhedron.‖ Geometric Tools \nfor Computer Graphics. Morgan Kaufmann Publishers, 2003. 695–713. \n",
      "content_length": 3514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": " \n \n \n3.4. A Practical Spatial Architecture for Animal and Agent Navigation \nMichael Ramsey—Blue Fang Games, LLC. \nmiker@masterempire.com \n―Not so many years ago, the word ‗space‘ had a strictly geometrical meaning: \nthe idea evoked was simply that of an empty area.‖ \n—Henri Lefebvre \nGame literature is inundated with various techniques to facilitate navigation in an \nenvironment. However, many of them fail to take into account the primary unifying medium \nthat animals and agents use as locomotion in the real world. And that unifying medium is \nspace [Lefebvre97]. The architectonics[1] of space relative to an animal‘s or agent‘s motion \nin a game environment is the motivation for this gem. Traditional game development \nfocuses on modeling what is physically in the environment, so it may seem counterintuitive \nto model what is not there, but one of the primary reasons for modeling the empty space of \nan environment is that it is this spatial vacuum that frames our interactions (be they \nlocomotion or a simple idle animation) within that environment. Space is the associative \nsystem between objects in our environments. \n[1] A unifying structure is commonly referred to as an architectonic, as it is used to describe and \nassociate elements that are separated into a perceived whole. \nThis article will discuss this spatial paradigm and the techniques that we used during the \ndevelopment of a multi-platform game, entitled World of Zoo (WOZ). WOZ was a \nchallenging project not only by any standard definition of game development, but also \nbecause we desired our animals‘ motion to be credible. \nAn important aspect of any animal‘s believability is that they are not only aware of their \nsurroundings, but that they also move through a dynamic environment (Color Plates 1 and \n2 contain examples of WOZ‘s environment) in a spatially appropriate and consistent \nmanner. This maxim had to hold true whether the animal was locomoting over land, over \nwater, or even through air! To help facilitate the representation of our spatial environments, \nwe used several old tools in new ways, and in conjunction with a few inventions of our own, \nwe believe we accomplished our goals. \nFundamental Components of the Spatial Representation System \nThe primary element for constructing a spatial representation in WOZ was the sphere, \ntermed a navsphere. Figure 3.4.1 shows a subset of a navsphere layout from an exhibit. \nThe navsphere is fundamentally important because not only is it used to generate the \nnavigable representation of the world (see below), but more importantly, it defines the \ninteractable spatial dimensions of the world. What this means is that we define in 3D space \nwhere an animal can go, not just where it cannot. To keep animals from going places, we \nrely not only upon the tried-and-true techniques of collision detection [Bergen04], but also \non collision determination. \nFigure 3.4.1. Two navspheres in a level. Connectivity information between \nneighboring navspheres is accomplished by having a slight overlap. \n",
      "content_length": 3051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": " \n \n \n \nCollision determination is a technique of knowing ahead of time that a collision may occur \n(similar to how the majority of physics packages handle contact points). This determination \nof a potential collision is implicit when using navspheres, because we are able to determine \nthat an animal is nearing the edge of navigable representation. These edges are termed \nspatial boundaries, and in this specific example, it defines the implicit relationship between \nthe navspheres and the outer lying geometry [Gibson86]. Because of this knowledge, we \ncan augment the animal‘s behavior to slow down, start to turn away, or skid to a stop. \nIt should be noted that the navsphere is an approximation of space, and it is not an exact \nspatial inverse of the placed geometry. A spatial system that is modeled from constructive \nsolid geometry (CSG) principles would definitely be ideal, but the dynamic nature of WOZ‘s \nenvironments made this unfeasible. However, we did invest some initial time into \ninvestigating and utilizing various CSG techniques. The primary aspect of CSG that we \ndiscovered to be applicable for a spatially accurate representation of an environment was \nthe Boolean intersection operator, which is the overlap of two objects merged into a convex \ncomponent. The cumulative Boolean intersections would form the space through which our \nanimals would move. The complexity and cost would come from the determination of an \nagent‘s occupancy within that environment‘s CSG representation, as they are not primitives \nbut complex convex objects. This approach would definitely be more accurate than spheres, \nbut at the cost of not being usable on current-generation consoles or the typical desktop PC \n(mainly due to the arbitrary manner in which WOZ‘s environments were modeled). \nAs an animal moves through an environment, there need to be mechanisms in place to help \ncontrol an animal‘s interaction with navspheres—we do this by assigning properties to the \nnavsphere that define how certain interactions will occur. Some of these properties include \ndefining the type of animal locomotion allowed in that navshape (whether it be land, water, \nor air) and spatial parameters for certain animal sizes. \nHaving a system that is spatially centric requires a single enabling component that allows it \nto be easily accessed by other game systems. By allowing navspheres to overlap, we are \ncapable of generating a navigable representation—a navrep—of the environment. A similar \nsystem is the circle-based waypoint graph [Tozour03]. For a visual example of this process, \ncompare Figure 3.4.2 and Figure 3.4.3. Figure 3.4.2 shows the overlapping navspheres for \none of WOZ‘s nursery levels. Figure 3.4.3 shows the type of connectivity information \ngenerated from that navsphere layout. You can think of the navrep generated as the \nwalkable surface for the world; however, note that the navrep is in 3D space and can wrap \naround other geometry as well as other navreps. \nFigure 3.4.2. A navsphere layout for the bear nursery. \n",
      "content_length": 3050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": " \n \n \n \nFigure 3.4.3. This figure shows how the connectivity information is generated from \nthe overlapping navspheres in Figure 3.4.2. \n \n \nThe primary reason to construct a connectivity graph from the spatial representation is that \nwe need to execute potentially expensive operations on the game world, such as pathing \nqueries, reachability tests, and visibility queries. The basic algorithm for generating the \nnavrep is to iterate over the navspheres, searching for overlap with any other navspheres. \nIf we find any overlap, we establish a bidirectional link between the navspheres. Later on in \nthe development of WOZ, we also found that we could use the same mechanism for one-\nway links by embedding directed connectivity information in the navsphere itself; this \nmanifested itself in game objects such as one-way teleport doors. \n \nNavigation System Architecture \nAs we move on to discussing the spatial aspects of the WOZ navigation system, it will help \nto understand the basic structure and components of the system as a whole (see Figure \n3.4.4). The primary interface between the navigation system and the other components of \nthe WOZ game is the navigation manager. The navigation manager facilitates access to the \nplanner. The planner contains the navigable representation of the environment—both the \nspatial vacuum and the generated connectivity graph. The pathfinder uses the A* algorithm, \nwhich provides support for both spatial and routing biases [Hart68, Stout00]. \nFigure 3.4.4. The navigation system. \n",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": " \n \n \n \nAlso provided is a general utilities object that contains general navigation code. When an \nanimal (noted as an entity in Figure 3.4.4) needs to route through an environment, it will \nissue a call through the animal planner into the navigation manager. The navigation \nmanager will then access its own world planner, and using the navrep it generates a coarse \nroute through the environment. This coarse route is then returned to the animal‘s planner \nfor use. As you‘ll notice in Figure 3.4.4, WOZ has two planners: the navigation planner and \nthe animal planner. The animal planner handles any immediate spatial or interanimal tasks, \nwhile the navigation planner handles the more rudimentary routing operations (for example, \npath biasing) as well as handling the interactions with the navsphere reservation system. \n \nNavrep Continuity and Stitching \nNavreps are not necessarily continuous. What this means is that the level designers can \nauthor disparate navreps based upon differing navrep types (for example, land or water), as \nwell as navreps that represent differing levels of elevation in a zoo exhibit, such as the \nledges on a cliff face. Linking multiple, non-overlapping navreps in the navigation system \nrequires the creation of navfeelers. Figure 3.4.5 (left) contains an example of two navreps \nthat were authored as disconnected. There is a navrep on the ledge and also a navrep on \nthe base of the exhibit. The navfeeler is the fishing pole–like extrusion from the top \nnavsphere to one of the bottom navspheres. \nFigure 3.4.5. On the left is an example of a navfeeler authored in level to connect \ntwo disparate navreps with the final result, on the right, of a navigable navrep. \n",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": " \n \n \n \nNavfeelers allow the level authors to link these navreps together. It attempts to find any \nnavsphere below; if it finds a navsphere below itself, we then establish a bidirectional link \nbetween the two. The analogy that we used during development to help explain this was to \nthink of a fisherman at the end of pier, with his fishing pole sticking out over the water at \nroughly 45 degrees. His line would dangle into the water, which effectively links land and \nwater for the navigation system. If the navfeeler finds a navsphere below it, we then \nestablish a bidirectional link between the two navspheres. Once these navreps are linked \ntogether (refer to Figure 3.4.5, right), the navigation system can then pathfind over the \nmultiple navreps. Although this example is shown for a land-to-land navrep, the same \nmechanism is used for land-to-water bridges, which allows the penguins to dive into and \njump out of water. \n \nHandling Locomotion and Turning \nLocomotion in WOZ is executed by root accumulation of multiple animations that form a \nfinal pose. By root accumulation, we mean that the animators author with full displacement; \nany movement of the root of the animal‘s skeleton is contained in the animation. This allows \nthe animations to retain all the inherent tweaks, such as deceleration, that might otherwise \nbe lost in the typical engineer-centric approach, where the animators are required to author \nanimations on the spot. While root motion is very necessary for exhibiting animator-\nenvisaged motion, it also does not mesh well with traditional navigation paradigms. \nTurning was handled independently of locomotion, which was advantageous because it \nallowed the animators to avoid generating a host of different turn animations. To generate a \nturn angle, an animal selects a target point, such as the next point along a route or a game \nobject. This target point is then turned into a turn angle (by doing a dot product between \nthe target point and the heading of the animal and then solving for theta), which is then \nused to twist the spine of an animal in the desired direction. If we had an animal with a \nfour-bone spine and a turn angle of 40 degrees, we would simply apply 10 degrees of twist \nto each of the bones. In this section I‘ll talk about how we handled two central components \nof progression through our navigation system: locomotion and turning. \nWhile the navigation system plans through the world using the connectivity graph (which \nwas generated from the navspheres), we turn to the spatial representation of the world in \norder to facilitate locomotion and turning validation. Each animal has associated with itself \nan occupancy shape (see Figure 3.4.6). This occupancy shape is used to control progression \nthrough the suggested route. The occupancy shape is not static; it can grow in size, as well \n",
      "content_length": 2864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": " \n \nas placement, according to the current animation. A fast-galloping zebra will have its \noccupancy shape projected out in front, whereas a slow-moving zebra would have the \noccupancy shape centered more on itself. \nFigure 3.4.6. Each animal has an occupancy shape. This variable occupancy shape \nis used to denote the rough spatial representation relative to the navigation \nsystem. \n \n \nOne key differentiation in the navigation system implemented for WOZ versus other games \nis that during normal locomotion an animation could and generally would deviate from the \nproposed path through the environment. A combination of root motion, behavioral prodding, \nand turn speeds that vary based upon the state in the behavior graph makes following an \nexact route impossible without adversely affecting the quality of an animal‘s movement. \nThis is not dissimilar to how real animals or humans move through the world. Humans don‘t \nplan exact motions; we don‘t plan our exact muscle contractions—we move in accordance \nwith our understanding of the space made available to us. It‘s this space that allows us to \nidentify boundaries and make use of the relationships between the space and objects we are \nafforded [Gibson86]. So it made sense to ensure that WOZ‘s animals respect the spatial \nrelationships of the environment accordingly. \nTo help influence the turning of an animal, we implemented a system that uses a series of \nspheres projected around an animal in order to determine the suggested turn angles. We \naccomplished this by determining whether the projected spheres were inside the navrep. If \none of the projected spheres was completely outside the navrep, we would execute a turn in \nthe opposite direction. For example, if an animal‘s motion wanted to move it into a wall, we \ncould execute a tight turn in the opposite direction simply by altering the blend weights of \nthe current animation. The blend weights would effectively bias the animal away from \nobstructions. It is important to remember that we modeled the world not only \ngeometrically, but also its spatial representation, so this type of inside or outside test would \nbe possible. By projecting these spheres around the animal, we could make turns that \nwould take the animal away from objects according to their spatial proximity to the navrep \nboundaries. Another positive side effect of this approach is that we avoided doing costly and \npotentially numerous raycasts for this operation. \n \nConclusion \n",
      "content_length": 2487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": " \n \nWhile there are many navigable representations that can be used in a game, very few of \nthem concern themselves with the spatial vacuum that exists in between the static \ngeometry. This gem has shown you how to represent this space using several common \ntools, in conjunction with some new approaches to modeling locomotion. While modeling \nlocomotion based on a desired sense of progression through the environment is different \nthan most locomotion models, it also opens the door to modeling the actual motion of an \nanimal or agent as it occurs in the real world. Animals or agents don‘t follow exact routes; \nthey alter their movement according to an understanding of not only the static objects in \nthe world, but also the available space in which to exhibit their behaviors. \nHopefully, the presentation of a few old ideas intermixed with a few new ones will help you \nrethink an animal‘s or agent‘s interactions inside an environment. By considering the space \nin between physical aspects of an environment, we now have the capability to make \ndecisions about our environment that are not purely reactive by nature. We are afforded the \nmechanisms to forecast environmental interactions, which is perhaps one of the first steps \ntoward a credible motion management system. \n \nAcknowledgements \nI wish to thank the following team members for their contributive efforts to the WOZ AI \nsystem: Bruce Blumberg, Steve Gargolinksi, Ralph Hebb, and Natalia Murray. \n \nReferences \n[Bergen04] Bergen, Gino Van Den. Collision Detection in Interactive 3D Environments. \nMorgan Kaufmann, 2004. \n[Ericson05] Ericson, Christer. Real-Time Collision Detection. Morgan Kaufmann, 2005. \n[Gibson86] Gibson, James J. The Ecological Approach to Visual Perception. LEA, 1986. \n[Hart68] Hart, P. E., N. J. Nilsson, and B. Raphael. ―A Formal Basis for the Heuristic \nDetermination of Minimum Cost Paths.‖ IEEE Transactions on Systems Science and \nCybernetics 4.2 (1968): 100–107. \n[Lefebvre97] Lefebvre, Henri. The Production of Space. Blackwell Publishing, 1997. \n[Richenbach58] Reichenbach, Hans. The Philosophy of Space and Time. Dover, 1958. \n[Stout00] Stout, Bryan. ―The Basics of A* for Path Planning.‖ Game Programming Gems. \nBoston: Charles River Media, 2000. 254–263. \n[Tozour03] Tozour, P. ―Search Space Representations.‖ AI Game Programming Wisdom 2. \nBoston: Charles River Media, 2003. 85–102. \n[Week01] Week, Jeffrey. The Shaping of Space. Marcel Dekker, Inc., 2001. \n \n3.5. Applying Control Theory to Game AI and Physics \n",
      "content_length": 2523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": " \n \nBrian Pickrell \nbobthrollop@brandx.net \nControl theory is the engineering study of dynamic systems, such as airplanes and other \nmachines. The name is a bit misleading, since designing controls (in other words, airplane \nautopilots or missile guidance systems) is only one application of the theory. Control theory \nis actually the analysis of equations to extract some fundamental information about how \nentire classes of systems act in all possible circumstances. \nControl theory is something of a sister science to simulation. A simulation looks at a specific \nsituation and predicts what the system will do in great detail, but it doesn‘t explain how the \nsystem behaves in general. Control theory, on the other hand, doesn‘t predict anything at \nall, but it gives general information in the form of quantifiable measurements that are true \nfor any situation or inputs. In other words, it tells you what the system can and cannot do. \nThese measures are mathematical and quite abstract, but they are important, and for the \nmost part, they are things that simulation engines just do not provide. Some of the ideas \ndescribed in this gem may seem like shortcuts to avoid doing proper simulation, but in fact \nthey have just as much scientific validity. It is better to think of controls analysis as \ncomplementary to simulation, and as a way to do some things that your physics engine was \nnot designed for. \nOne task for which control theory is better suited is designing the steering and motion of \nphysics objects in games. To give a specific example, how would you depict the motion of \nan in-game car making a sudden turn? We all know that a simple, abrupt change of \ndirection is not realistic and doesn‘t look believable—the vehicle should swerve and sway a \nlittle bit while turning. If you just program a motion curve by guessing, the results will not \nbe much more believable. If you use a high-fidelity physics engine to simulate the turn, you \nwill likely have to work out the forces and other parameters required to make it sway \nconvincingly by trial and error, and the results of this tweaking cannot be reused for other \ncars in other turns. \nOur first formula describes the family of functions that you can use for this situation. An \nobject such as a moving vehicle must obey these functions in its motions. If your game \nobject doesn‘t move like this, it‘s not physically realistic. Conversely, you can create a \nplausible motion curve at very little cost in physics analysis by following this formula and \nusing some common-sense rules of thumb. \nHere is that result: Any unforced dynamic system moving under a set of linear differential \nequations has an output motion of the form: \nEquation 1  \n \n \nThis is a set of harmonic oscillators, each of the form: \nEquation 2  \n \n",
      "content_length": 2801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": " \n \n \nThis is a sinusoidal wave where A is the amplitude and ω (omega) is the frequency, and the \noscillations die out (or grow) exponentially according to a damping coefficient ζ (zeta). A \nsingle harmonic oscillator looks something like Figure 3.5.1. \nFigure 3.5.1. Simple damped harmonic oscillator. \n \n \nSeveral harmonic modes added together, each with its own amplitude, frequency, and \ndamping coefficient, as in Equation (1), look like Figure 3.5.2. \nFigure 3.5.2. Oscillator with two harmonic modes. \n \n \nWe will spend most of the rest of this gem defining the concepts behind Equation(1) and \nshowing how they were derived. \nDynamic Systems \n",
      "content_length": 651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": " \n \nThe concept of a dynamic system in control theory is the same as that used in simulation; \nthat is, a system is any set of parts that act on each other in some quantifiable way and \nchange over time. The underlying mathematics applies equally to all different sorts of \nsystems, using any units of measurement that are appropriate and any scientific laws that \nproduce linear equations. The ―parts‖ themselves may be conceptual rather than physical, \nas when measuring the concentrations of different reagents in a chemical reaction. The \n―output‖ can be any measurable quantity at some point in the system, measured in \nwhatever units are appropriate. In this gem, we will mostly refer to objects moving under \nthe physical laws of mass, force, and so on, but with the understanding that other dynamic \nsystems work the same way. \nWe did not say what the units of Equation (1) are or how you should implement the result. \nThis is up to the game programmer to decide. Your output y(t) can be distance \nmeasurements (xyz coordinates) or velocities or angular units, such as steering heading, as \nlong as your system is linear in the units you choose. This explains how an airplane flying in \na circle (which doesn‘t look like Equation (1) at all) is consistent with linear control theory: \nbecause in angular units, its heading changes at a constant rate, which does fit the \ntemplate of Equation (1). This should be good news to programmers of cockpit view–style \ngames. Everything we tell you here can be implemented directly in angular units, without \nrequiring messy polar coordinate conversions. \nLinear Systems \nWe have already stated the requirement that a system be linear. What does this mean? A \nlinear dynamic system is one whose defining equations of motion follow the form: \nEquation 3  \n \n \nwhere all of the coefficients An are constants. The reason that we could say with such \ncertainty that all linear systems follow the form of Equation (1) is that the result is a \nnecessary mathematical consequence of Equation (3). If you don‘t make that assumption, \nthen the results don‘t look like those sinusoidal functions. Engineers talk all the time about \nnon-linearities in their systems; this usually means that the coefficients A in the equations \nof motion are not constant. They are functions of some other value, or they change over \ntime. (It can also mean that something else is being done with the derivatives—for \nexample, \nis not linear.) When engineers try to apply linear control theory to systems \nwith non-linearities (and they always do; a good part of controls engineering consists of \nfinding ways to approximate non-linear reality with linear equations), the results often look \nsimilar to Equation (1) but the ω, ζ, and A values of the different modes keep changing, or \nthe poles and frequency responses in the charts we‘ll soon see keep moving around. \nAll that said, it is safe in an in-game world to define all physical responses as being always \nlinear. Then we can go ahead and use our linear results without worry. \nFeedback, Damping, and Stability \nSystems oscillate the way they do because of internal feedback among components. \nSometimes unexpected feedback effects can cause a system to oscillate more rather than \nless; in fact, the 19th-century origins of control theory lie in explanations of why a steam \nengine‘s mechanical governor couldn‘t keep it running at a steady speed. One of the \n",
      "content_length": 3441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": " \n \nprimary questions that controls engineers analyze is whether the system is stable. If any of \nthe damping coefficients ζ in Equation (1) is negative, then the entire system is unstable (in \nother words, the equation does not converge as t → ∞). Notice that in such a case, the \nexponential part of the equation is positive, so the sinusoidal oscillations expand \ncorrespondingly. Fortunately, instability is never a surprise in the game world, since we \nhave the luxury of declaring positive ζ ‘s for all of our systems. However, you will see this \neffect in physics simulations that drift out of limits and in game object movements that go \nwild because of player overcontrolling. \nMost programmers know a little bit about feedback and use it occasionally but do not have a \nway to quantify the system stability or instability that results. Here are some different types \nof damping: \nζ > 2 ω \nOverdamped (stable, no oscillations) \nζ = 2 ω \nCritical damping (stable) \n0 < ζ < 2 ω Underdamped (stable) \nζ = 0 \nUndamped (oscillates) \nζ < 0 \nUnstable \n \nSecond-Order Linear Differential Equations \nLet‘s look at how Equation (2) is derived for a couple of simple harmonic oscillators. \nEquation (2) is the general solution of the second-order linear differential equation (LDE); \nthat is, one where the differential terms go as high as the second derivative \nbut no \nhigher. Each of these examples, then, can form a single component of a more complicated \nsystem that has multiple oscillatory modes. \nBoth of these examples are basic textbook cases. You can see from them how the same \ncontrol theory applies in so many different scientific fields; it is because many natural laws \n(in these cases, Coulomb‘s law and Newton‘s second law) express themselves as similar \nfirst- and second-order differential equations. \nExample 1: Mass and Spring \nFigure 3.5.3 shows a heavy object M bouncing up and down on a spring K, obeying \nNewton‘s laws of motion. The ―output‖ value being measured is x2, the position of the object \nmeasured in units of distance. We‘ll pretend that all of the friction in the system occurs at \ndashpot B. (A dashpot is the working part of a shock absorber.) The equations show that \nthe weight moves under the forces of the spring and friction. The spring force changes as \nthe object moves, a built-in feedback. The friction force depends on velocity (which is the \nderivative of the position). Newton‘s second law converts force to acceleration, which is the \nsecond derivative of position. Outside forces pushing on the system are represented by \nx1(t). (We‘ll get to that later; for now, assume it is zero.) \nFigure 3.5.3. Mass and spring. \n",
      "content_length": 2669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": " \n \n \n \nExample 2: RCL Circuit \nFigure 3.5.4 shows a very simple electronic circuit with a resistor R, a capacitor C, an \ninductor L, and a voltage input E. The inductor tends to keep current i flowing once started, \nwhile the capacitor builds up a charge that fights against the current. The interaction of \nthese two causes the current (and voltage) in the circuit to oscillate back and forth. Either \nvoltage or current can be read as the output value; in this case, we‘re using current. \nFigure 3.5.4. RCL circuit. \n \nSome Math Formulae \nWe are almost ready to give the solution to the general second-order LDE, but let‘s review \ntwo fundamental mathematical formulae we will use. \nEuler’s Formula \nEquation 4  \n \n \nThis is a fundamental identity from complex mathematics that relates exponentials to \ntrigonometric functions. It says that e to any power that is a pure imaginary number lies on \nthe unit circle in the complex plane, and that for an exponential with a complex exponent, \nthe imaginary part of the result oscillates in a sine-like way while the real part either grows \n",
      "content_length": 1089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": " \n \nor diminishes exponentially (depending on whether the real part is positive or negative), as \nshown in Figure 3.5.5. \nFigure 3.5.5. Roots and exponentials in the complex plane. \n \n \nFundamental Theorem of Algebra \nA real polynomial of degree n: \nEquation 5  \n \n \nhas n roots (including complex and multiple roots). Complex roots always come in \nconjugate pairs a+bi, a-bi. \nBetween them, these two formulae mean that, in the complex number domain, sine and \nexponential functions are the same thing! \nHarmonic Oscillator—Derivation \nNow for the solution to the equation resulting from Examples 1 and 2: \nFind all equations y(t) that obey the differential relation \nEquation 6  \n \n \nThe derivation of the result is interesting because it shows how imaginary numbers keep \ncoming up in control theory. Solving a differential equation, solving an exponential \nequation, and finding roots of a polynomial are tantamount to the same thing. Also, we‘ll \nsee how a complex exponential can be separated into two parts: an exponential (ζ) and a \n",
      "content_length": 1041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": " \n \nsinusoidal component (ω). Control engineers tend to talk lightly of complex ―roots‖ as \ndamping ratios and frequencies, an abstraction that can be weird and confusing for \noutsiders. This is where that equivalency comes from. \nThe solution starts by assuming that all solutions are variations on the basic form y = ert \nwhere r is a complex number. \nEquation 7  \n \n \nThe general solution, then, is: \nEquation 8  \n \n \nGeneral solution of second-order LDE (unforced)[1] \n[1] It would be a fair question to ask what the meaning is of the imaginary part of the equation, since \nyou can‘t have imaginary distances or voltages. An (evasive) answer is that the imaginary part is \nrequired to be zero as a constraint condition in an analysis, which we‘re skipping over. The reader may \njust ignore the imaginary part and read only the real part of the equation. \nwhere ζ and ω can be derived directly from the coefficients A, B, and C once you know the \nsolution. c1 and c2 are arbitrary constants—any values are valid, and the values for a \nparticular case depend on the initial conditions. You can find the system‘s frequency and \ndamping ratio directly from the system constants (such as inductance and capacitance) \nwithout going back through the differential equation. Here is another formulation that is \nhandy to have: \nEquation 9  \n",
      "content_length": 1336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": " \n \n \n \nBlock Diagrams \nWhat about a system that has more parts than our examples? When an analysis extends to \nmultiple equations of motion, the interrelationships quickly become much harder to sort out. \nA block diagram analysis is one way to manage this. Making a suitable block diagram to \nrepresent a complex system is a matter of engineering judgment, much like designing a \nsystem model for a simulation. We do not want to explain all the ins and outs of block \ndiagramming, but a block diagram can help explain how control inputs apply to the \nequations we‘ve done so far. \nWe said that Equations (1) and (2) applied to unforced systems—that is, ones with no \ninputs. In the case of modeling a car‘s steering, that would mean that there was no one at \nthe wheel. What use is a model like that? One answer is that we can include the steering \ninside the block being modeled. \nEquations (1) and (2) are what are called transfer functions. In a block diagram, you can \nrepresent an entire subsystem as a single box with an input and an output, and the transfer \nfunction is the conversion between input and output. Figure 3.5.6 shows a diagram that \ndemonstrates ―closing the loop‖—that is, converting an open-loop (uncontrolled) equation \ninto one that accounts for the effects of steering or other control inputs. \nFigure 3.5.6. Closed-loop block diagram. \n \n \nG(s) is our original transfer function. R(s) is an external input (steering), and G(s) converts \nthe input to the final output value C(s). H(s) is a feedback control system that reads the \noutput, transforms it in a certain way, and uses it to modify the command input. \nThe following is important if you want to model autonomously steered (NPC) vehicles in a \ngame: H(s), which represents the internal dynamics of the steering system, is modeled as a \nlinear system of components exactly like the main system G(s). H(s) could be a collection of \nlevers, springs, hydraulics, electronics, and so forth—it is just another linear system and is \nconceptually no different than G(s). Furthermore, the entire diagram that results, including \ncontrols, is also linear and can be redrawn as a single ―black box‖ transfer function. \nTherefore, you can legitimately represent a vehicle and its driver as a single, complicated \ntransfer function where the input is the desired course and the output is the actual motions \nof the vehicle. This does not work for a real person but can represent any linear controls, \n",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": " \n \nincluding a simple autopilot, a smart or dumb robot driver, or a human non-player character \n(NPC) whose driving style is a linear function. \nThe caveat to this is that you would be simulating the steering behavior of a control system \nwithout actually doing the steering. Only in extremely simple cases is it possible to extract \nthe implied control responses from a system transfer function. The demo on the CD shows \nan example of this. \nLaplace Transform \nThe block diagram of Figure 3.5.6 replaces differential equations with Laplace transforms. \nFor our examples, G(s) is the Laplacian L of Equation (6), which is As2 + Bs + C. The \nLaplace transform is a clever mathematical trick that converts differential equations to more \nmanageable polynomial functions. \nEquation 10  \n \n \nDefinition of Laplace Transform[2] \n[2] By convention, a lowercase letter denotes an original function and a capital letter denotes a Laplace \ntransform with the parameter s. It is important to notice which is which throughout this article. \nLaplace transforms are written as capital letters in the block diagrams, and the original \nfunction parameter—time t—becomes s, which is not a measurable value but an abstract \ncomplex number. Laplace transforms have several useful properties. First of all, the \ntransforms of the most common functions are all polynomials. Also, they are linear and can \nbe added and multiplied by constants. They support integration and differentiation; they \nconvert step inputs and other discontinuous functions into continuous functions. And, \nfunction composition is the same as multiplying in the Laplacian domain—in other words: \nEquation 11  \n \n \nYet another handy property of the Laplacian is that it is units-independent. Since all units of \nmeasurement transform to the same s, it is possible to mix transfer functions whose \noutputs are in different units in the same block diagram. Using the properties of the Laplace \ntransform, Figure 3.5.6 leads to the following relationships: \nEquation 12  \n",
      "content_length": 2025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": " \n \n \n \nThe denominator of the last function is called the characteristic equation[3] of the system. \n[3] This is the same as the characteristic equations found in linear algebra, if you represent the \nunderlying system of differential equations as a matrix. \nEquation 13  \n \n \nFrom Characteristic Equation to Equations of Motion \nThe next step is simple but powerful. The Laplace transform implicitly does the same \nconversion to exponentials that was done in the derivation of Equation (8). Therefore, the \nroots of the characteristic equation of the system correspond to the n harmonic modes in \nEquation (1), with –ζ as the real part and ω as the imaginary part[4]. If you can factor a \nsystem‘s characteristic equation into the form: \n[4] Note the – sign in front of ζ. A positive real part of a root means a negative damping coefficient. \nEquation 14  \n \n \nthen any function matching Equation (1) with any constants An is a possible motion of this \nsystem.[5] \n[5] Phase offsets of the various modes, which we have neglected in this gem, are also allowable and in \nfact are very important in matching Equation (1) to real motion curves. \nIf you‘re not allowed to make up an answer, characteristic equations are hard to solve! The \ntransfer function from our examples was easy, but in general both G(s) and H (s) may be \npolynomial fractions (one polynomial divided by another) that require numerical methods to \nfactor. But for game simulations, you may invent a characteristic equation in already-\nfactored form! If you do so, you are implicitly stating that the physics of the system are \nunknown, the internal logic of the controls is unknown, the roots were found in an unknown \nway, but the overall response of the entire system including controls is just what the \ndesigner specified. \nRoot-Locus Plots \n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": " \n \nRoot-locus plots are a bit of a digression, but they give a measure of insight into how one \nwould choose the invented roots mentioned earlier. Aeronautical engineers found that in \nmany cases, control systems included an electronic amplifier or the equivalent, and the \namplification, or gain, came out as a constant multiplier in H (s) in Figure 3.5.6 and that \nfurthermore, the stability or instability of the entire system depended in hard-to-anticipate \nways on what gain (K) was chosen. The original root-locus method[6] used some extremely \nclever pencil-and-ruler methods to replace the computation-intensive factoring of Equation \n(13) into Equation (14); the plots are still useful even though we‘re doing them by \ncomputer now[7]. A root-locus plot is a plot of all possible roots of the characteristic equation \nfor values of K from zero to ∞. Here are some examples: \n[6] The root-locus method is also known as the Evans method. It was developed in 1948 by Walter R. \nEvans, a graduate student at UCLA. \n[7] For instance, Matlab‘s rlocus() function, or [El-Dawy03], a freeware root-locus plotting program. \nIn a root-locus plot, circles represent roots of the numerator of the characteristic equation, \ncalled zeroes. X‘s represent roots of the denominator, called poles because the function is \ninfinite at the poles. The actual roots begin at the poles when gain K=0 and move toward \nthe zeroes as K increases. The graph is drawn in complex s-space, where the real (X-axis) \ncomponent is damping coefficient – ζ and the imaginary (vertical axis) component is \nfrequency ω. Again, each root corresponds to one of the modes of Equation (1), so you can \ndraw an output function directly if you know what the roots are. \nAs mentioned, if any mode of a system has a negative damping ratio, the entire system is \nunstable. In a root-locus plot, this means that if any root lies on the right of the Y-axis, the \nsystem is unstable. This happens in Figure 3.5.7(c) —the system is stable at low gain, but if \nyou turn up the gain past a certain point, it becomes unstable. \nFigure 3.5.7. Root loci of characteristic equations. \n",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": " \n \n \nIn this example, notice that (b) and (c) are modifications of (a) made by adding a zero and \na pole, respectively, to the characteristic equation. This represents adding components \n(such as resistors and capacitors) to the control system. The shape of the plot and the \nstability of the system changed dramatically each time. Controls engineers make it their \nbusiness to know what to add to a circuit to shape the root locus the way they want. \nFortunately for the game designer, we can usually just place roots where we like and \npretend that a root-locus analysis has been done. A root close to the vertical axis represents \nunderdamping and an oscillation that takes a long time to die out; roots far to the left \nrepresent great stability and rapid response. \nFrequency Analysis[8] \n[8] See [D‘Azzo60], chapters 8–1010, and [Thaler60], §4.8, 7.5. \nThis gem has completely skipped over a branch of control theory that is actually of equal \nimportance. This is frequency analysis. It is based on the idea that a system‘s behavior can \nbe broken down into its responses to sine-wave inputs at different frequencies. \nIf the input to a linear system is a pure sinusoidal function—in other words, sin(ϕt), where ϕ \nis any frequency (not necessarily ω)—the output will always be a sinusoidal wave at the \nsame frequency ϕ, but with a different amplitude A. It will also be delayed from the input \n",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": " \n \nwave by a time offset that is expressed as a phase angle θ. A and θ are different for every \ninput frequency ϕ, and a graph of frequency response A(ϕ) and phase delay θ(ϕ) is a \nfrequency analysis. The best-known plot types are the Bode plot and the Nyquist plot[9]. \n[9] Developed by Hendrik Bode and Harry Nyquist, respectively, who were both engineers at Bell \nLaboratories in the 1930s. \nAny input function can be broken down into a sum of a (possibly infinite) set of sinusoidal \nfunctions and can usually be approximated by just a few. (This is the basis of Fourier \ntheory.) Therefore, a frequency analysis can be used to break down a system‘s motions into \na few harmonic modes based on A and θ rather than ζ and ω. The demo on the book‘s CD \nhas chosen not to follow this route, for simplicity. \nCode Example: Control Law Racers \nThe demo program on the book‘s CD shows one way to apply the ideas behind a root-locus \nanalysis in a game. Rather than modeling a real system and then analyzing it, the demo \ngoes the other way by asking what behavior a system ought to have and then displaying it \nwithout actually doing a model. \nThe demo shows several slot cars moving down a track. The simulation uses a side-scrolling \nformat, so there is just one degree of freedom: y position. When the player tells them to \nchange lanes, the cars swerve abruptly and then maneuver into the new lane. Their steering \nis a bit weak compared to the weight of the cars, so they weave back and forth before \nfinding their target. What is interesting is that every car is a little bit different and steers \ndifferently. The steering behavior of the cars is encapsulated in the mode structure, which \ncontains the frequencies and damping rates of their motions. \nstruct mode { \n  float zeta;  // damping coefficient \n  float omega; // frequency \n  float A;     // Initial amplitude of wave \n}; \n \nEach mode supplies one of the terms in Equation (1). Each car can have any number of \nmodes. There is no steering or simulation loop in this demo; the control law models both \nthe physics of the car itself and the actions of its driver. Since a car with only one mode is \nno more complex than the weight and spring of Example 1, this may be a simple autopilot \nindeed. \nTo design your car‘s control law, add modes. Imagine a root-locus plot like Figure 3.5.7. But \ninstead of plotting a real function, just draw one or more X‘s for the roots. Think about how \nwell you want your car to handle and how quickly you want it to sway. A real car weaving \nback and forth on the road may move with a period of one to five seconds or so; that is, an \nomega of 0.2–1.0. As for damping, if you want your car to keep shimmying for a long time, \nthen give it a zeta close to 0; if you want it to handle well and accurately, then give it a \nlarge zeta. A critical damping ratio of zeta = 2 * omega will produce fastest settling. \nThe in-code comments explain how to add modes to your car. The step after that, giving an \nactual steering command by setting the A for each mode, is handled arbitrarily by the \ndemo. The next step of interest is the navigation step. This is where the values in the car‘s \nmodes are applied to Equation (1). \nvector<driver>::iterator drivIt; \n \n// <for-each> car \n",
      "content_length": 3272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": " \n \nfor( drivIt = driverList.begin(); \n     drivIt != driverList.end(); \n     drivIt++ ) \n{ \n     // Sum of all the oscillatory modes \n \n     // measuring distance from target lane implicitly commands \nthe \n     // car to go there when all oscillations have settled \n     drivIt->driverPos.y = targetLane + Y_BASE; \n \n     modeList::iterator theModeIter; \n     // <for-each> mode \n     for( theModeIter = drivIt->modes.begin(); \n          theModeIter != drivIt->modes.end(); \n          theModeIter++ ) \n     { \n         // Add this mode’s contribution to position \n         float zeta = theModeIter->zeta; \n         float omega = theModeIter->omega; \n         float A = theModeIter->A; \n         // Time conversion from dimensionless units (radians \nand \n         // rad/sec) to full cycles per second \n         float t = 2.0 * PI * resetTime; \n         drivIt->driverPos.y += A * exp(-zeta * t) * cos(omega * \nt); \n     } \n} \n \nThe last statement computes the output position driverPos.y directly, without going \nthrough a simulation step. \nYou will have to go into source code to change the cars. Try adding more cars and changing \nmodes. See what happens when a car‘s modes include both large and small damping \ncoefficients. \nConclusion \nThe control theory presented in this gem is the so-called classical control theory, based on \nlinear systems, that was current practice in the aerospace and electronics industries from \nroughly the 1930s through the 1960s and has been eclipsed by the rise of computers but \nhas certainly not gone away. It‘s a bit surprising that such a well-established body of \nknowledge is so forgotten in the game industry. This gem is meant to introduce \nprogrammers and designers to the concepts of control theory as well as to present one way \nto simulate and direct physical objects. Other applications of these ideas provide a virtually \nunexplored field for the industry. \n \nReferences \n",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": " \n \n[D‘Azzo60] D‘Azzo, John J., and Constantine H. Houpis. Feedback Control System Analysis \nand Synthesis. McGraw-Hill Book Company, Inc., 1960. \n[El-Dawy03] El-Dawy, Ahmed Saad. ―RootLocus.‖ n.d. Geocities. n.d. \n<http://www.geocities.com/aseldawy/root_locus.html>. \n[Graham61] Graham, Dunstan, and Duane McRuer. Analysis of Nonlinear Control Systems. \nJohn Wiley & Sons, Inc., 1960. \n[Thaler60] Thaler, George J., and Robert G. Brown. Analysis and Design of Feedback Control \nSystems. McGraw-Hill Book Company, Inc., 1960. \n \n3.6. Adaptive Tactic Selection in First-Person Shooter (FPS) Games \nThomas Hartley, Institute of Gaming and Animation (IGA), University of \nWolverhampton \ntom.hartley5@googlemail.com \nand \nQuasim Mehdi, Institute of Gaming and Animation (IGA), University of \nWolverhampton \nOne of the key capabilities of human game players that is not typically employed by non-\nplayer characters (NPCs) in commercial first-person shooter (FPS) games is in-game \nlearning and adaptation. The ability of a human player to adapt to opponents‘ tactics is an \nimportant skill and one that separates an expert game player from a novice. NPCs that \nincorporate in-game learning and adaptation are more responsive to human players‘ actions \nand are therefore more capable opponents. This gem presents a practical in-game approach \nto adapting an NPC‘s selection of combat tactics. \nA Dynamic Approach to Adaptive Tactic Selection \nTo achieve successful in-game (that is, run-time) tactic selection in FPS games, we \nproposed to adapt the online adaptation algorithm dynamic scripting [Spronck06, \nSpronck05]. The reinforcement learning–inspired dynamic scripting algorithm has been \ndesigned specifically for use in online learning scenarios and has previously shown \nsignificant promise in other genres of scripted games, such as role playing and strategy \ngames. Consequently, the approach offers an interesting augmentation to the traditionally \nscripted tactic selection techniques used in commercial FPS games. However, the approach \ndoes need to be adapted to make it suitable for use in an FPS environment. \nThe FPS version of the dynamic scripting algorithm presented in this gem has been adapted \nto the selection of tactics rather than rules for scripts, so it has a number of differences \nfrom previously implemented versions. First, the library of tactics in the developed system \nis organized in groups according to a dual-layered state representation. Previous \nimplementations organize the dynamic scripting rulebases according to NPC type and high-\nlevel game states [Ponsen and Spronck04]. However this limited state representation is not \nsuited to FPS games, as the selection of player-versus-player tactics greatly depends on the \ncurrent state of the NPC [Thurau06]. The second contribution is the development of fitness \nfunctions to evaluate the success of a tactic and the encounter with an opponent. The third \ncontribution is the development of a tactic selection process that makes use of a prioritized \nlist approach in combination with the K-Nearest Neighbor algorithm (K-NN) [Mitchell97]. \n",
      "content_length": 3126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": " \n \nThe goal of this approach is to organize the tactics so that the most successful ones are \nmore likely to be selected first. \n \nOverview of the Adaptive Tactic Selection Architecture \nTactics in the system are organized into tactic libraries, which in turn are organized \naccording to a dual-layered representation of the game environment. As illustrated in Figure \n3.6.1, the upper layer determines which tactic library should be selected according to an \nabstract state representation and/or rules, which are typically described in terms of a \nbehavior type or goal. For example, an ―engage enemy‖ library of tactics could be defined \nby health above 50 percent and the possession of a high-powered weapon. This process can \nbe performed manually using a game developer‘s domain knowledge and traditional game \ndevelopment techniques, such as finite state machines (FSM), decision trees, or rules. For \nexample, the abstract state space in Figure 3.6.2 is based on the hierarchical \nFSM/prioritized list approach used to control NPC behavior in Halo 2 [Isla05]. The highest \npriority state (arranged right to left) that matches the current game state is selected. This \napproach should allow the tactic selection architecture to be easily integrated with existing \ngame AI techniques. \nFigure 3.6.1. High-level overview of the tactic selection architecture. \n \n \nAs illustrated in Figure 3.6.2, the lower state space contains instances of a library‘s tactics \nat points in an n-dimensional feature space. The feature space and the number of tactic \n",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": " \n \ninstances within a tactic library are kept relatively small in order to maintain performance. \nOnce the upper layer has determined the current tactic library, the K closest library \ninstance(s) to the current game state are selected and used to determine the NPC‘s current \nlist of tactics. The following procedure outlines how an instance of the lower state space is \nselected. If the closest library instance to the query state (in other words, the NPC‘s current \nstate) is equal or below a predefined threshold, it is used to determine a prioritized tactic \nlist. If the closest library instance is above a predefined query threshold, the K closest tactic \ninstances to the current environment state are used for tactic selection. This multi-layered \napproach to selection allows tactics to be associated with detailed game states, while also \nbeing efficient in retrieving from the library. \nFigure 3.6.2. Overview of the tactic selection process and the state organization. \nThe K closest library instances are selected. The weights of each tactic are combined using \ndistance weighting and are used to determine a prioritized list. \n \n \n",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": " \n \nAll tactics have an associated weight that influences the order in which they are selected in \na given state. The weights reflect the success or failure of a tactic during previous \nencounters with an opponent. Learning only occurs between the tactics in each library \ninstance, and one library does not affect the learning in another, even if the same tactic is \nfound in both libraries. Tactic selection is achieved through a prioritized list [Isla05], which \nis adapted to support online list generation. \nFirst, the procedure outlined above is used to determine the K library instance(s) closest to \nthe current game state. If K > 1 and the closest tactic instance is above a query threshold, \nthe tactic‘s weights are combined using the distance-weighted K-NN algorithm. The \nindividual or combined tactic library instance is used to create the prioritized list. The K-NN \nalgorithm classifies an unlabeled instance (for example, s in Figure 3.6.2) by comparing its \nn-dimensional feature vector to stored instances. The K-Nearest Neighbors to the query \ninstance are found and used to determine a combined weight for a tactic. This is achieved \nthrough a weighted average that reflects a stored instance‘s distance to the query instance. \nThe position of a tactic within the prioritized list is determined by ordering tactics according \nto their weight. (For example, the tactics with the largest weights have the highest priority \nof selection.) A new prioritized list of tactics is generated whenever an upper layer state \nchange occurs or when the library instance changes. If a distance-weighted prioritized list \nhas been created, a predefined distance threshold is used to determine whether a new list \nshould be generated. The distance threshold is based on the distance from the current list‘s \nquery state to the NPC‘s present world state. \nOnce a list of tactics is generated, the tactic that has the highest priority and is capable of \nrunning is selected as the current behavior. If higher-priority tactics become available, they \ncan interrupt the current tactic on the next game loop. To avoid repetitive behavior, the \ntactic selection process can also include a check to prevent the same variant of tactic from \nfollowing each other. (For example, ―strafe a short distance‖ would not follow ―strafe a long \ndistance.‖) Therefore, if the next tactic is a similar variant to the current tactic, it is skipped, \nand the subsequent tactic is evaluated. This rule doesn‘t apply to tactic interrupts, which \noccur regardless of tactic variation. \nWhen a tactic is complete, is no longer capable of running, or a fixed amount of time has \nelapsed, the next tactic in the list that is capable of running is selected. When a tactic is \ninterrupted, is completed, or times out; a library change occurs; or a library instance \nchange occurs, the tactic has its weight updated according to a weight-update and fitness \nfunction (outlined in the next section). If the query instance was above a distance \nthreshold, the weight update is applied to the tactics in each K close library instance \naccording to the instance‘s distance from the initial query point. \n \nFitness and Weight-Update Functions \nThe weight-update function alters the weights associated with tactics in response to a \nreinforcement signal that is supplied by tactic and encounter fitness functions. The aim of \nthe fitness function is to reward tactics that defeated an opponent or improved the NPC‘s \nchance of defeating the opponent. The fitness functions for the tactic and the encounter are \ndefined below. Each function returns a value in the range of [–1,1]. \nEquation 1  \n \n \n",
      "content_length": 3667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": " \n \nThe equation above evaluates the fitness of a tactic and contains two components. In the \nequation, f refers to the fitness of the tactic t that is being evaluated. The first component \nA(t) ∊ [–1,1] determines the difference in life caused by the tactic [Andrade04]. That is the \nlife lost from the opponent minus the life lost from the NPC. The difference in life lost \nrepresents a key metric in measuring the performance of a tactic, as removing all an \nopponent‘s life is the ultimate goal of FPS combat. \nThe second component of the fitness function S(t) ∊ [–1,1] evaluates the surprise of the \ntactic. A surprise is the anticipation of an experience that the actual experience does not \nfulfil [Saunders02]. In the tactic selection architecture, the experience is the average value \nof the difference in life lost from previous encounters, and the actual experience is the \ncurrent difference in life lost. \nEach component of the tactic fitness function is weighted according to its contribution as in \nthe standard dynamic scripting approach [Spronck05]. The selection of the fitness function‘s \ncontribution is based on game goals. (For example, an NPC places high value on its health.) \nThe two components of the tactic fitness function are determined using Equations (2) and \n(3). The function hl (NPC) in Equations (2) and (3) refers to the health (in other words, life) \nlost from the adapting NPC, and hl(opp) refers to the health lost from an opponent (in other \nwords, the damage caused by the adapting NPC). AvghLifeLostn in Equation (3) contains the \naverage difference in life caused by the tactic from the previous n weight updates. \nEquation 2  \n \n \nEquation 3  \n \n \nEquation (4) evaluates the fitness of an encounter, that is, the tactics used by the adapting \nNPC during a combat to reach a winning or losing state. When an NPC reaches a terminal \nstate, the tactics used during the encounter are updated according to whether the NPC won \nor lost. In Equation 4, f refers to the fitness of the encounter e that is being evaluated. \nhlt(NPC) refers to the health (in other words, life) lost from the adapting NPC during the \nperformance of tactic t. hlt(opp) refers to the health lost from an opponent during the \nperformance of tactic t. \nEquation 4  \n \n \nEquations (2), (3), and (4) assume that the life lost or damage is in the range of 0 to 100. \nEquation (3) divides surprise by 200 so that the returned value will be in the range of –1 to \n1. Equation (4) divides health lost by 125 in order to give the tactic a guaranteed reward or \npunishment depending on whether the encounter was winning or losing. The scale of the \nreward or punishment can be set according to game requirements. \n",
      "content_length": 2719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": " \n \nThe weight update function uses the tactic and encounter fitness functions to generate a \nweight adaptation for the library tactics (i.e. an amount to adjust the current weight of a \ntactic). The function is based on the standard dynamic scripting technique, where weights \nare bound by a range [Wmin, Wmax] and surplus weights (that is, weight adjustments that \nresult in a remainder outside the minimum and maximum range) are shared between all \nother weights [Spronck05]. However the fitness functions outlined in Equations (1) and (4) \nreturn a value between [–1, 1], where a negative fitness value indicates a losing tactic, \nwhile a positive fitness value indicates a winning tactic. Therefore a simplified weight update \nfunction is used in this approach. In Equation (5), Pmax is the maximum penalty, Rmax is the \nmaximum reward, F is the fitness of the tactic, and b is set to 0 and is the break-even point \nfor the tactic‘s fitness. Above this point, the tactic‘s fitness is considered to be winning, \nwhile below this point the tactic is considered to be losing. In this simplified weight update \nfunction, the break-even point is built into the function itself. A fitness of 0 means the \nweights are unchanged. \nEquation 5  \n \n \nWhen multiple tactic library instances are used to create a combined prioritized list, the \nweight update is shared between the K instances according to their distance to the initial \nquery state (in other words, the point that resulted in the library instances being retrieved). \nThe sharing of the weight update is performed using Equation (6), where Δ wi is the weight \nupdate for a library instance, di is the distance to the library instance from the query state, \nand d is the total weight. \nEquation 6  \n \n \n \n \nAdapting Tactic Selection \nTo illustrate the tactic selection architecture, we describe the example of an NPC learning to \nadapt its selection of combat tactics. In this example, the adapting NPC is endowed with an \n―Attack‖ library of tactics that could include melee attack, charge attack, strafe charge \nattack, circle strafe attack, and attack from cover. Tactics are manually coded and comprise \nmultiple lines of logic that define the behavior to be performed (for example, movement and \nweapon handling code). In addition to behavior logic, there should also be code to \ndetermine whether a tactic is capable of running. For example, the circle strafe tactic may \nnot be capable of running in a narrow corridor. \nFor clarity, only one tactic library is used in this example. The upper state space controls \nthe selection of tactic libraries using conventional techniques, and adaptation occurs \n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": " \n \nindependently within each tactic library instance; therefore, additional tactic libraries can be \neasily added. After a library has been defined, the default weight of the tactics needs to be \nset. Tactics can be given the same initial weight, or a weight can be selected based on \ndomain knowledge. In this example, all the tactic weights are initially set to the same \ndefault weight; therefore, the position of tactics within the initial prioritized list is randomly \ndetermined. \nOnce a tactic library has been defined, the next step is to determine the instances of the \nlibrary. This involves defining the state space, the number of instances, the position of the \ninstances within the state space, and how the state space is organized. In this example, the \nlower state space is defined as the relative distance between the adapting NPC and the \ntargeted opponent. This simple feature vector was selected because distance is a prominent \nfactor in the selection of FPS tactics. In most cases the requirements of the game can be \nused to determine the most appropriate state space representation. \nThe number of library instances is determined by the size of the state space. In this \nexample, the state space is compact; therefore, only a few instances are needed. The \nrequisite number of instances is also affected by their position in the state space and how \nthe state space is organized. The position of tactic instances in the state space can be \nmanually managed using domain knowledge or automatically added using a distance metric, \nsuch as: if s > r, then create a new instance of the tactic library, where s is the distance \nbetween the current state and the closest tactic instance and r is a predefined distance \nthreshold. If the distance is below the threshold, then the closest tactic cases are used. \nFigure 3.6.3 illustrates an example of tactic library instances and state space organization. \nFigure 3.6.3. Example arrangement of the lower state space. \n \n \nAt the start of a combat encounter, the tactic selection architecture generates a list of \ntactics that are ordered by their associated weight. The NPC attempts to defeat the \nopponent by executing the first tactic in the list. If the tactic is not capable of running, the \nnext tactic in the list is selected. This is repeated until a tactic capable of running is found. \nThe list of tactics is generated by determining the closest K library instances to the NPC‘s \nposition in the lower state space. Once a tactic and encounter are complete, the weight of \nthe tactics used are updated using Equations (1) and (4). In this example, K is set to 1, and \nthe initial weights of tactics are the same; therefore, only the closest library instance is \nused, and the order of the tactics is initially random. The process of an NPC selecting and \nadjusting its weights is summarized below: \n",
      "content_length": 2874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": " \n \n \nA prioritized list of tactics is generated, and the melee attack tactic is selected for \nexecution. The tactic performs poorly during the encounter and results in 100-\npercent health loss for the adapting NPC and 5-percent health loss for the opponent. \n \nThe encounter finishes with the death of the adapting NPC; therefore, the tactic and \nencounter fitness are determined. The tactic‘s weight is updated based on its \nsuccess. The first step in the process is to determine a tactic‘s fitness—in this \nexample, hl_npc equals 100 and hl_opp equals 5. \n \nint hl_npc = botDamage() \n \nint hl_opp = playerDamage() \n \nfloat a = (hl_opp - hl_npc) / 100 \n \nfloat s = ( (hl_opp - hl_npc) - avgDiffInLifeLost ) / 200 \nfloat fitness = 1 / 10 * (7 * a + 3 * s) \n \nOnce the fitness of the tactic has been determined, the weights of all the tactics in \nthe library instance are updated. If the number of library instances equals 1, as in \nthis example, the weight adjustment for the performed tactic is determined as \nfollows: \n \nif (fitness < 0) \n \n    weightAdjustment = tacticPenaltyMax * fitness \n \nelse \n    weightAdjustment = tacticRewardMax * fitness \n \nNext, the encounter weight update is performed. This process involves the algorithm \nlooping through the tactics used during the encounter, determining their encounter \nfitness, weight adjustment, and weight update. The calculation of the encounter \nfitness is shown below. The tacticResults array contains information on each \ntactic used during the encounter and its performance. \n \nif (hasWon == true) \n \n    fitness = 1 - (tacticResults[i].getHl_npc() / 125) \n \nelse \n    fitness = -1 + (tacticResults[i].getHl_opp() / 125) \n \nAfter the tactic weights have been updated, the adapting NPC can generate a new \nprioritized list. In this example, the melee attack tactic will be at the end of the list \ndue to its poor performance. \n \nConclusion \nThis gem has outlined an approach to the in-game adaptation of scripted tactical behavior in \nFPS computer games. The technique enables NPCs in these games to adapt their selection \nof tactics in a given state based on their experience of the tactics‘ success. Less successful \ntactics are not selected or are only selected when more preferred tactics are not available. \n \nReferences \n[Andrade04] Andrade, G. D., H. P. Santana, A. W. B. Furtado, A. R. G. A. Leitão, and G. L. \nRamalho. ―Online Adaptation of Computer Games Agents: A Reinforcement Learning \nApproach.‖ 1st Brazilian Symposium on Computer Games and Digital Entertainment \n(SBGames2004). \n",
      "content_length": 2556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": " \n \n[Isla05] Isla, D. ―Handling Complexity in the Halo 2 AI.‖ GDC 2005 Proceedings. 2005. \nGamasutra. n.d. <http://www.gamasutra.com/gdc2005/features/20050311/isla_01.shtml>. \n[Mitchell97] Mitchell, T. Machine Learning. McGraw-Hill, 1997. \n[Ponsen and Spronck04] Ponsen, M. and P. Spronck. ―Improving Adaptive Game AI with \nEvolutionary Learning.‖ Computer Games: Artificial Intelligence, Design and Education \n(CGAIDE 2004): 389–396. \n[Saunders02] Saunders, R. ―Curious Design Agents and Artificial Creativity: A Synthetic \nApproach to the Study of Creative Behaviour.‖ Ph.D. thesis. University of Sydney, Sydney. \n2002. \n[Spronck05] Spronck, P. ―Adaptive Game AI.‖ Ph.D. Thesis. Universiteit Maastricht, \nMaastricht. 2005. \n[Spronck06] Spronck, P. ―Dynamic Scripting.‖ AI Game Programming Wisdom 3. Ed. S. \nRabin. Boston: Charles River Media, 2006. 661–675. \n[Thurau06] Thurau, C. ―Behavior Acquisition in Artificial Agents.‖ Ph.D. thesis. Bielefeld \nUniversity, Bielefeld. 2006. \n \n3.7. Embracing Chaos Theory: Generating Apparent Unpredictability \nthrough Deterministic Systems \nDave Mark, Intrinsic Algorithm LLC \ndave@intrinsicalgorithm.com \nOne of the challenges of creating deep, interesting behaviors in games and simulations is to \nenable our agents to select from a wide variety of actions while not abandoning completely \ndeterministic systems. On the one hand, we want to step away from having very obvious \nif/then triggers or monotonous sequences of actions for our agents. On the other hand, the \nneed for simple testing and debugging necessitates the avoidance of introducing random \nselection to our algorithms. \nThis gem shows how, through embracing the concept and deterministic techniques of chaos \ntheory, we can achieve complex-looking behaviors that are reasonable yet not immediately \npredictable by the viewer. By citing examples from nature and science (such as weather) as \nwell as the simple artificial simulations of cellular automation, the gem explains what causes \nchaotic-looking systems through purely deterministic rules. The gem then presents some \nsample, purely deterministic behavior systems that exhibit complex, observably \nunpredictable sequences of behavior. The gem concludes by explaining how these sorts of \nalgorithms can be easily integrated into game AI and simulation models to generate deeper, \nmore immersive behavior. \nThe Need for Predictability \nThe game development industry often finds itself in a curious predicament with regard to \nrandomness in games. Game developers rely heavily on deterministic systems. \nProgramming is inherently a deterministic environment. Even looking only at the lowly \nif/then statement, it is obvious that computers themselves are most ―comfortable‖ in a \nrealm where there is a hard-coded relationship between cause and effect. Even non-binary \nsystems, such as fuzzy state machines and response curves, could theoretically be reduced \n",
      "content_length": 2925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": " \n \nto a potentially infinite sequence of statements that state, ―Given the value x, the one and \nonly result is y.‖ \nGame designers, programmers, and testers also feel comfortable with this technological \nbedrock. After all, in the course of designing, developing, and observing the complex \nalgorithms and behaviors, they often have the need to be able to say, with certainty, ―Given \nthis set of parameters, this is what should happen.‖ Often the only metric of the success or \nfailure of the development process we have is the question, ―Is this action what we \nexpected? If not, what went wrong?‖ \n \nShaking Things Up \nGame players, on the other hand, have a different perspective on the situation. The very \nfactor that comforts the programmer—the knowledge that his program is doing exactly what \nhe predicts—is the factor that can annoy the player…the program is doing exactly what he \npredicts. From the standpoint of the player, predictability in game characters can lead to \nrepetitive, and therefore monotonous, gameplay. \nThe inclusion of randomness can be a powerful and effective tool to simulate the wide \nvariety of choices that intelligent agents are inclined to make [Mark09]. Used correctly and \nlimiting the application to the selection of behaviors that are reasonable for the NPC‘s \narchetype, randomness can create deeper, more believable characters [Ellinger08]. While \nthis approach provides a realistic depth of behavior that can be attractive to the game \nplayer, it is this same abandonment of a predictable environment that makes the analysis, \ntesting, and debugging of behaviors more complicated. Complete unpredictability can also \nlead to player frustration, as they are unable to progress in the game by learning complex \nagent behavior. The only recourse that a programming staff has in controlling random \nbehaviors is through tight selection of random seeds. In a dynamic environment, however, \nthe juxtaposition of random selection of AI with the unpredictable nature of the player‘s \nactions can lead to a combinatorial explosion of possible scenario-to-reaction mappings. \nThis is often a situation that is an unwanted—or even unacceptable—risk or burden for a \ndevelopment staff. \nThe solution lies in questioning one of the premises in the above analysis—that is, that the \nexperience of the players is improved through the inclusion of randomness in the decision \nmodels of the agent AI. While that statement may well be true, the premise in question is \nthat the experience that the player has is based on the actual random number call in the \ncode. The random number generation in the agent AI is merely a tool in a greater process. \nWhat is important is that the player cannot perceive excessive predictable regularity in the \nactions of the agent. As we shall discuss in this article, accomplishing the goal of \nunpredictability can exist without sacrificing the moment-by-moment logical determinism \nthat developers need in order to confidently craft their agent code. \n \nA Brief History of Chaos \nThe central point of how and why this approach is viable can be illustrated simply by \nanalyzing the term chaos theory. The word chaos is defined as ―a state of utter confusion or \ndisorder; a total lack of organization or order.‖ This is also how we tend to use it in general \nspeech. By stating that there is no organization or order to a system, we imply randomness. \nHowever, chaos theory deals entirely within the realm of purely deterministic systems; \nthere is no randomness involved. In this sense, the idea of chaos is more aligned with the \nidea of ―extremely complex information‖ than with the absence of order. To the point of this \narticle, because the information is so complex, we observers are unable to adequately \n",
      "content_length": 3779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": " \n \nperceive the complexity of the interactions. Given a momentary initial state (the input), we \nfail to determine the rule set that was in effect that led to the next momentary state (the \noutput). \nOur inability to perceive order falls into two general categories. First, we are often limited \nby flawed perception of information. This occurs by not perceiving the existence of relevant \ninformation and not perceiving relevant information with great enough accuracy to \ndetermine the ultimate effect of the information on the system. \nThe second failure is to adequately perceive and understand the relationships that define \nthe systems. Even with perfect perception of information, if we are not aware of how that \ninformation interacts, we will not be able to understand the dynamics of the system. We \nmay not perceive a relationship in its entirety or we may not be clear on the exact \nmagnitude that a relationship has. For example, while we may realize that A and B are \nrelated in some way, we may not know exactly what the details of that relationship are. \nPerceiving Error \nChaos theory is based largely on the first of these two categories—the inability to perceive \nthe accuracy of the information. In 1873, the Scottish theoretical physicist and \nmathematician James Clerk Maxwell hypothesized that there are classes of phenomena \naffected by ―influences whose physical magnitude is too small to be taken account of by a \nfinite being, [but which] may produce results of the highest importance.‖ \nAs prophetic as this speculation is, it was the French mathematician Henri Poincaré, \nconsidered by some to be the father of chaos theory, who put it to more formal study in his \nexamination of the ―three-body problem‖ in 1887. Despite inventing an entirely new branch \nof mathematics, algebraic topology, to tackle the problem, he never completely succeeded. \nWhat he found in the process, however, was profound in its own right. He summed up his \nfindings as follows: \nIf we knew exactly the laws of nature and the situation of the univ erse at the \ninitial moment, we could predict exactly the situation of the same univ erse at \na succeeding moment. But even if it were the case that the natural laws had \nno longer any secret for us, we could still know the situation approximately. If \nthat enabled us to predict the succeeding situation with the same \napproximation, that is all we require, and we should say that the \nphenomenon had been predicted, that it is governed by the laws. But [it] is \nnot always so; it may happen that small differences in the initial conditions \nproduce very great ones in the final phenomena. A small error in the former \nwill produce an enormous error in the later. Prediction becomes impossible…. \n[Wikipedia09] \nThis concept eventually led to what is popularly referred to as the butterfly effect. The origin \nof the term is somewhat nebulous, but it is most often linked to the work of Edward Lorenz. \nIn 1961, Lorenz was working on the issue of weather prediction using a large computer. \nDue to logistics, he had to terminate a particularly long run of processing midway through. \nIn order to resume the calculations at a later time, he made a note of all the relevant \nvariables in the registers. When it was time to continue the process, he re-entered the \nvalues that he had recorded previously. Rather than reenter one value as 0.506127, he \nsimply entered 0.506. Eventually, the complex simulation diverged significantly from what \nhe had predicted. He later determined that the removal of 0.000127 from the data was \nwhat had dramatically changed the course of the dynamic system—in this case, resulting in \na dramatically different weather system. In 1963, he wrote of his findings in a paper for the \nNew York Academy of Sciences, noting that, ―One meteorologist remarked that if the theory \nwere correct, one flap of a seagull‘s wings could change the course of weather forever.‖ (He \nlater substituted ―butterfly‖ for ―seagull‖ for poetic effect.) \n",
      "content_length": 4017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": " \n \nDespite being an inherently deterministic environment, much of the problem with predicting \nweather lies in the size of the scope. Certainly, it is too much to ask scientists to predict on \nwhich city blocks rain will fall and on which it will not during an isolated shower. However, \neven predicting the single broad path of a large, seemingly well-organized storm system, \nsuch as a hurricane, baffles current technology. Even without accounting for the intensity of \nthe storm as a whole, much less the individual bands of rain and wind, the various forecasts \nof simply the path of the eye of the hurricane that the different prediction algorithms churn \nout lay out like the strings of a discarded tassel. That these mathematical models all process \nthe same information in such widely divergent ways speaks to the complexity of the \nproblem. \nThankfully, the mathematical error issue is not much of a factor in the closed system of a \ncomputer game. We do not have to worry about errors in initial observations of the world, \nbecause our modeling system is actually a part of the world. If we restart the model from \nthe same initial point, we can guarantee that, unlike Lorenz‘ misfortune, we won‘t have an \nerror of 0.000127 to send our calculations spinning off wildly into the solar system. \n(Interestingly, in our quest for randomness, we can build a system that relies on a truly \nrandom seed to provide interesting variation—the player.) Additionally, we don‘t have to \nworry about differences in mathematical calculation on any given run. All other things being \nequal (for example, processor type), a given combination of formula and input will always \nyield the same output. These two factors are important in constructing a reliable \ndeterministic system that is entirely under our control. \nBrownian Motion \nAs mentioned earlier, the second reason why people mistake deterministic chaos for \nrandomness is that we often lack the ability to perceive or realize the relationships between \nentities in a system. In fact, we often are not aware of some of the entities at all. This was \nthe case with the discovery of a phenomenon that eventually became known as Brownian \nmotion. Although there had been observations of the seemingly random movement of \nparticles before, the accepted genesis of this idea is the work of botanist Robert Brown in \n1827. As he watched the microscopic inner workings of pollen grains, he observed minute \n―jittery‖ movement by vacuoles. Over time, the vacuoles would even seem to travel around \ntheir neighborhood in an ―alive-looking‖ manner. Not having a convenient explanation for \nthis motion, he assumed that pollen was ―alive‖ and was, after the way of living things, \nmoving of its own accord. He later repeated the experiments with dust, which ruled out the \n―alive‖ theory but did nothing to explain the motion of the particles. \nThe real reason for the motion of the vacuoles was due to the molecular and atomic level \nvibrations due to heat. Each atom in the neighborhood of the target vibrates on its own \npattern and schedule, with each vibration nudging both the target and other adjacent atoms \nslightly. The combination of many atoms doing so in myriad directions and amounts \nprovides a staggering level of complexity. While completely deterministic from one moment \nto the next—that is, ―A will nudge B n distance in d direction‖— the combinatorial explosion \nof interconnected factors goes well beyond the paltry scope of Poincaré‘s three-body \nproblem. \nThe problem that Brown had was that he could not perceive the existence of the atoms \nbuffeting the visible grains. What‘s more, even when the existence of those atoms is known \n(and more to the point, once the heat-induced vibration of molecules is understood), there \nis no way that anyone can know what that relationship between cause and effect is from \nmoment to moment. We only know that there will be an effect. \nThis speaks to the second of the reasons we listed earlier—that we often lack the ability to \nperceive or realize the relationships between entities in a system. This effect is easier for us \nto take advantage of in order to accomplish our goal. By incorporating connections between \nagents and world data that are beyond the ability of the player to adequately perceive, we \ncan generate purely deterministic cause/effect chains that look either random or at least \nreasonably unpredictable. \n",
      "content_length": 4438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": " \n \n \nExploring Cellular Automata \nOne well-known example bed for purely deterministic environments is the world of cellular \nautomata. Accordingly, one of the most well-known examples of cellular automata is \nConway‘s Game of Life. Conway‘s creation (because the term ―game‖ is probably pushing \nthings a little) started as an attempt to boil down John von Neumann‘s theories of self-\nreplicating Turing machines. What spilled out of his project was an interesting vantage point \non emergent behavior and, more to the point of this gem, the appearance of seemingly \ncoordinated, logical behavior. Using Conway‘s Life as an example, we will show how \napplying simple, deterministic rules produce this seemingly random behavior. \nThe environment for Life is a square grid of cells. A cell can be either on or off. Its state in \nany given time slice is based on the states of the eight cells in its immediate neighborhood. \nThe number of possible combinations of the cells in the local neighborhood is 28 or 256. (If \nyou account for mirroring or rotation of the state space, the actual number of unique \narrangements is somewhat smaller.) The reason that the Game of Life is easy for us to \ndigest is its brevity and simplicity, however. We do not care about the orientation of the live \nneighbors, but only a sum of how many are alive at that moment. The only rules that are in \neffect are: \n1. Any live cell with two or three live neighbors lives on to the next generation. \n2. Any live cell with fewer than two live neighbors dies (loneliness/starvation). \n3. Any live cell with more than three live neighbors dies (overcrowding). \n4. Any dead cell with exactly three live neighbors becomes a live cell (birth). \nFigure 3.7.1 shows a very simple example of these rules in action. In the initial grid, there \nare three ―live‖ cells shown in black. Additionally, each cell contains a number showing how \nmany neighbors that cell currently has. Note that two of the ―dead‖ cells (shown in gray) \nhave three neighbors, which, according to Rule 4 above, means they will become alive in \nthe next iteration. The other dead cells have zero, one, or two neighbors, meaning they will \nremain at a status quo (in other words, dead) for the next round. The center ―live‖ cell has \ntwo neighbors, which, according to Rule 1 above, allows it to continue living. On the other \nhand, the two end cells have only a single live neighbor (the center cell) and will therefore \ndie of starvation the next round. The results are shown on the right of Figure 3.7.1. Two of \nthe prior cells are now dead (shown in gray), and two new cells have been born to join the \nsingle surviving cell. \nFigure 3.7.1. A simple example of the rule set in Conway’s Game of Life. In this \ncase, a two-step repeating figure called a blinker is generated by the three boxes. \n \n \nInterestingly, this pattern repeats such that the next iteration will be identical to the first (a \nhorizontal line), and so on. This is one of the many stable or tightly repeating patterns that \ncan be found in Life. Specifically, this one is commonly called a blinker. \n",
      "content_length": 3113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": " \n \nFigure 3.7.2 shows another, slightly more involved example. The numbers in the initial \nframe make it easier to understand why the results are there. Even without the numbers, \nhowever, the relationships between the initial state and the subsequent one are relatively \neasy to discern on this small scale. \nFigure 3.7.2. The dynamic nature of the cells acting together. \n \n \nWhile the rule set seems simple and intuitive enough, when placed on a larger scale and run \nover time, the ―behavior‖ of the entire ―colony‖ of cells starts to look random. This is due to \nthe overwhelming number of interactions that we perceive at any one moment. Looking at \nthe four panels of Figure 3.7.3, it is difficult for us to intuitively predict what would have \nhappened next except for either in the most general sense (for example, that solid blob in \nthe middle is too crowded and will likely collapse) or in regard to very specific subsets of the \nwhole (for example, that is a blinker in the lower-left corner). \nFigure 3.7.3. When taken as a whole, the simple cells in Life take on a variety of \ncomplex-looking behaviors. While each cell’s state is purely deterministic, it is \ndifficult for the human mind to quickly predict what the next overall image will \nlook like. \n \n \nInterestingly, it is this very combination of ―reasonable‖ expectations with not knowing \nexactly what is going to appear next that gives depth to Conway‘s simulation. Over time, \none develops a sense of familiarity with the generalized feel of the simulation. For example, \nwe can expect that overcrowded sections will collapse under their own weight, and isolated \npockets will die off or stagnate. We also recognize how certain static or repeating features \nwill persist until they are interfered with—even slightly— by an outside influence. Still, the \ncasual observer will still perceive the unfolding (and seemingly complex) action as being \nsomewhat random…or at least undirected. Short of pausing to analyze every cell on every \nframe, the underlying strictly rule-based engine goes unnoticed. \n",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": " \n \nOn the other hand, from the standpoint of the designer and tester, this simulation model is \nelegantly simplistic. The very fact that you can pause the simulation and confirm that each \ncell is behaving properly is a boon. For any given cell at any stage, it is a trivial problem to \nconfirm that the resulting state change is performing exactly as designed. \n \nLeveraging Chaos Theory in Games \nWhat sets Conway‘s Life apart from many game scenarios is not the complexity of the rule \nset, but rather the depth of it. On its own, the process of passing the sum of eight binary \ninputs through four rules to receive a new binary state does not seem terribly complex. \nWhen we compare it to what a typical AI entity in a game may use as its decision model, we \nrealize that it is actually a relatively robust model. \nFor instance, imagine a very simple AI agent in a first-person shooter game (see Figure \n3.7.4). It may take into account the distance to the player and the direction in which the \nplayer lies. When the player enters a specified range, the NPC ―wakes up,‖ turns, and \nmoves toward the player. There is one input state—distance—and two output states: ―idle‖ \nand ―move toward player.‖ While this seems extraordinarily simple, as recently as 10 to 15 \nyears ago, this was still common for enemy AI. Needless to say, the threshold and resultant \nbehavior were easy to discern over time. Players could perceive both the cause and the \neffect with very little difficulty. Likewise, designers and programmers could test this \nbehavior with something as simple as an onscreen distance counter. At this point, there is \nvery little divergence between the simplicity for the player and the simplicity for the \nprogrammer. \nFigure 3.7.4. If there is only one criterion in a decision model, it is relatively \nsimple for the player to determine not only what the criterion is, but what the \ncritical threshold value is for that criterion to trigger the behavior. \n \n \n \nAdding a Second Factor \nIf we were to add a second criterion to the decision, the situation does not necessarily \nbecome much more complicated. For example, we could add a criterion stating that the \n",
      "content_length": 2178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": " \n \nagent will only attack the player when he is in range and carrying a weapon. This is an \nintuitively sound addition and is likely something that the player will quickly understand. On \nthe other hand, this also means that the enemy is again rigidly predictable. \nOther factors can be added to a decision model, however, which could obscure the point at \nwhich a behavior change should occur. Even the addition of other binary factors (such as \nthe states of the cells in Life) can complicate things quickly for the observer if they aren‘t \nintuitively obvious. For instance, imagine that the rule for attacking the player was no \nlonger ―if the distance from player to enemy < n‖ but rather ―if the player‘s distance to two \nenemies < n‖ (see Figure 3.7.5). As the player approaches the first enemy, there would be \nno reaction from that first enemy until a second enemy is within range as well. While this \nmay seem like a contrived rule, it stresses an important point. The player will most certainly \nbe interested in the actions of the first enemy and will not easily recognize that its reaction \nwas ultimately based on the distance to the second enemy. \nFigure 3.7.5. The inclusion of a second criterion can obscure the threshold value \nfor—or even the existence of—the first criterion. In this case, because the second \nenemy is included, the player is not attacked as he enters the normal decision \nradius of the first enemy. \n \n \nThe player may not be able to adequately determine what the trigger conditions are \nbecause we have masked the existence of the second criterion from the player. People \nassume that causes and effects are linked in some fashion. In this example, because there \nis no intuitive link between the player‘s distance to the second enemy and the actions of the \nfirst, the player will be at a loss to determine when the first enemy will attack. The benefit \nof this approach is that the enemy no longer seems like it is acting strictly on the whims of \nthe player. That is, the player is no longer deciding when he wants the enemy to attack—it \nis seemingly attacking on its own. This imparts an aura of unpredictability on the enemy, \nwhich, in essence, makes it seem more autonomous. \nOf course, we programmers know that the agent is not truly autonomous but merely acting \nas a result of second criterion. In fact, this new rule set is almost as simple to monitor and \ntest as it is to program in the first place. We have the benefit of knowing what the two rules \nare and how they interact—something that is somewhat opaque to the player. \nSelecting the Right Factors \n",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": " \n \nAs mentioned, the inclusion of the player‘s distance to the second agent as a criterion for \nthe decisions of the first agent is a little contrived. In fact, it has the potential for \nembarrassing error. If the second agent was far away, it is possible that the player could \nwalk right up to the first one and not be inside the threshold radius of the second. In this \ncase, the first agent would check his decision criteria and determine that the player was not \nin range of two people and, as a result, would not attack the player. This does not mean \nthat there is a flaw in the use of more than one criterion for the decision—simply that there \nis a flaw in which criteria are being used in conjunction. In this case, the criterion that was \nbased on the position of the second agent—and, more specifically, the player‘s proximity to \nthe second agent—was arbitrary. It was not directly related to the decision that the agent is \nmaking. \nThe solution to this is to include factors on which it is reasonable for an agent to base his \ndecisions. In this example, the factors may include items such as: \n \nDistance to player \n \nPerceived threat (for example, visible weapon) \n \nAgent‘s health \n \nAgent‘s ammo \n \nAgent‘s alertness level \n \nPlayer‘s distance to sensitive location \nWe already covered the first two. The others are simply examples of information that could \nbe considered relevant. For the last one, we could use the distance measurement from the \nplayer to a location such as a bomb detonator. If the player is too close to it, the agent will \nattack. This is different than the example with two agents above in that the distance to a \ndetonator is presumably more relevant to the agent‘s job than the player‘s proximity to two \nseparate agents. \nWhile a number of the criteria listed previously could be expressed as continuous values, \nsuch as the agent‘s health ranging from 0 to 100, for the sake of simplicity, they can also \nbe reduced to Boolean values. We could rephrase ―agent‘s health‖ as ―agent has low \nhealth,‖ for instance. If we define ―low health‖ as a value below 25, we are now able to \nreduce that criterion to a Boolean value. The same could be done with ―agent‘s ammo.‖ \nThis, of course, is very similar to what we did with the distance. We could assert that ―if \nagent has less than 10 shots remaining,‖ then ―agent has low ammo.‖ \nWhat we have achieved with the above list could be summed up with the following pseudo-\ncode query: \nIf  PlayerTooCloseToMe() \n    or PlayerCloseToTarget() \n    and WeaponVisible() \n    and IAmHealthy() \n    and IHaveEnoughAmmo() \n    and IAmAlert()) \n    then Attack() \n \nEven with these criteria, the number of possible configurations is 27 or 128. (Incidentally, \nthe number of configurations of cells in Life is 28 or 256.) In our initial example, it would \ntake only a short amount of time to determine the distance threshold at which the agent \nattacks. By the inclusion of so many relevant factors into the agent‘s behavior model, the \nonly way that any one threshold can be ascertained is with the inclusion of the caveat ―all \nother things being equal.‖ Certainly in a dynamic environment, it is a difficult prospect to \ncontrol for all variables simultaneously. \n",
      "content_length": 3249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": " \n \nWhile having 128 possible configurations seems like a lot, it is not necessarily the number of \npossible configurations of data that will obscure the agent‘s possible selection from the \nplayer. Much of the difficulty that a player would have in knowing exactly what reaction the \nagent will have is due to the fact that the player cannot perceive all the data. This is similar \nto the impasse at which Robert Brown found himself. He could not detect the actual \nunderlying cause of the jitteriness of the pollen grains and dust particles. His observation, \ntherefore, was that the motion was random yet reasonable; he perceived lifelike motion \nwhere there was no life. \nA good way of illustrating this point is by working backward—that is, looking at the situation \nfrom the point of the player. If the agent‘s behavior changes from one moment to the next, \nthe player may not be able to determine which of the aforementioned factors crossed one of \nthe defined thresholds to trigger the change. In some cases, this would be easy. For \nexample, if the player draws his weapon and the agent attacks, the player can make the \nassertion that the weapon was the deciding factor. However, if the agent does not attack \nand, for instance, runs away instead, the player may not be able to determine whether it \nwas due to the agent having low health or low ammo. \nSimilarly, if the player is moving near the agent with his weapon drawn, and the agent \nbegins to attack, the player may not be able to ascertain whether it was his proximity to the \nagent, a secondary point (for example, a detonator), or a change in the agent‘s alertness \nstatus that caused the transition to occur. Once combat is engaged and values such as \nhealth and ammo are changing regularly, the number of possible reasons for a change in \nbehavior increases significantly. \nOf course, this is the reason why it is important to use relevant information as part of your \ndecision. If you use rational bases for your decisions, it makes it more likely that the \ndecision can at least be understood after it happens. There is a big difference between \npredictability and understandability. The player may not know exactly when the agent is \ngoing to change behaviors, but he should be able to offer a reasonable guess as to why it \nhappened. \nFrom a development and testing standpoint, the important issue to note here is that this is \nstill purely deterministic. There is no randomness included at all. A simple code trace or \nonscreen debug information would confirm the status of all seven of these criteria. When \ncompared against the decision code, the developer can confirm whether the agents are \noperating as planned or, in the case that they are not, determine which of the criteria needs \nto be adjusted. \nBeyond Booleans \nWe can extend the aforementioned ideas to go beyond purely Boolean flags, however. By \nincorporating fuzzy values and appropriate systems to handle them, we could have more \nthan one threshold value on any of the previous criteria. For instance, we could use the \nseven aforementioned factors to select from a variety of behaviors. Rather than simply \ndetermining whether the agent will attack the player, for example, we could include actions \nsuch as finding cover, running away, reloading, or calling for help. In order to do this, we \ncould partition one or many of the factors into multiple zones. \nFor example, if we were to arrange two factors on two axes and determine a threshold \nacross each, we would arrive at four distinct ―zones‖ (see Figure 3.7.6, Panel 1). Each of \nthese zones can be assigned to a behavior. In this case, using only two factors and two \nthreshold values, we can arrive at four distinct behaviors. The more thresholds we insert, \nthe more zones are created. Using our two-axis example, by increasing the threshold values \nfrom 1 to 3 in each direction, we increase the number of result spaces from 4 to 16 (see \nFigure 3.7.6, Panel 2). \n",
      "content_length": 3972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": " \n \nFigure 3.7.6. As the number of logical partitions through axes is increased, the \nnumber of potential results increases exponentially as a factor of the number of \naxes involved. By combining the factors prior to partitioning (Panel 3), thresholds \ncan be made more expressive. \n \n \nWe can visualize how this would affect behaviors if we imagine the values of our two factors \nmoving independently along their respective axes. For example, imagine a data point \nlocated in Behavior G in Panel 2. If Factor 1‘s value were to change, we could expect to see \nBehaviors F and H—and even E—depending on the amount of change in Factor 1. If Factor 2 \nwere the only one changing, we could expect to see changes to Behaviors C, K, and O. If \nchanges were occurring in only Factor 1 or 2, by observing the changes in behavior that \noccurred, we could eventually determine where those thresholds between behaviors are. \nHowever, if both factors were continually changing independent of each other, we now could \npossibly see any one of the 16 behaviors. This would make it significantly more difficult to \nexactly predict the cause-and-effect chain between factors and behaviors. \nFor instance, assume once again that we start with the data point in G. If we were \nwitnessing a reduction in Factor 1, we may see a state change to B, C, F, J, or K. All of \nthose states can be reached from G if Factor 1 is decreasing. What we would have to realize \nis that the ultimate state is decided by Factor 2 as well—for example, G→C could happen if \nFactor 1 was decreasing slowly and Factor 2 was increasing. Correspondingly, G→K could \nhappen if Factor 1 was decreasing slowly and Factor 2 was decreasing. Naturally, similar \nextensions of logic apply to result states of B and J. The conclusion is, while we can make a \ngeneral statement about where our data point may end up while Factor 1 is decreasing, we \ncan‘t know the eventual result state without also knowing the behavior of Factor 2. \nThe examples shown in Figure 3.7.6 only show two dimensions due the limitations of what \ncan be shown on paper. Our decision model does not share those limitations, however. By \nusing each potential factor in our decision model and defining one or more thresholds of \nmeaning, we can create a complex state space of potential results. \nAs a measurement of how this expands the potential number of actions, we can compare \nthe number of possible outcomes. When the system was composed of 7 Boolean values, we \nhad 128 possible combinations of data. Even by simply partitioning each of the 7 inputs into \n3 ranges rather than 2, the number of combinations becomes 37 or 2,187. These results \ncould then be mapped to a startling array of behaviors. Of course, not all the result zones \nneed to represent individual behaviors. Imagine mapping the 2,187 possible result spaces \nonto 30 different behaviors, for instance. \nTo further leverage the power of this system, we can step beyond the mentality of mapping \na single value onto a series of states. Instead, we can use systems that can take multiple \ninputs in conjunction and select a state that is dependant on values of those multiple inputs. \n",
      "content_length": 3179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": " \n \nFor example, we may want our agent to consider a combination of its distance to the player \nand their own health. That is, the state of their own health becomes more important if the \nplayer is closer. The two endpoints of the ―relevancy vector‖ of this decision would be \nbetween (―player far‖ + ―perfect health‖) and (―player near‖ + ―zero health‖). As either of \nthose factors moves from one end to the other, it has less effect on the outcome than if \nboth of these factors were moving simultaneously. \nFigure 3.7.6, Panel 3 shows a two-dimensional visualization of this effect. In this case, the \nfactors themselves are not partitioned. Instead, they remain continuous variables. When \ncombined, however, they create a new directed axis in the state space— in this case shown \nby the combined shading. We can set a threshold across that new axis (dotted line) that can \nbe used to determine where the behavior changes. Now, from the point of view of the \nplayer, he cannot determine at what point on Factor 1 the behavior changes without taking \ninto account changes in Factor 2 as well. \nJust as we did with single-axis-based thresholds, by determining one or more multi-axis \nthreshold lines in an n-dimensional space, we partition our space into a variety of zones. By \nanalyzing in which sections of the resulting partitioned space our current input data falls, we \ncan select from multiple potential outputs. By combining values in this way, we can \npotentially arrive at more expressive outputs at any stage of our decision model. \nSpecific techniques for accomplishing and managing this sort of decision complexity can be \nfound elsewhere [Mark08]. The methods that we use to arrive at the resulting values are \nnot the focus here, however. The important part is that we are doing all of this in a purely \ndeterministic fashion—that is, we could verify that any given combination of factors is \nmapped to the appropriate action. While there is still no random factor being included in \nthese calculations, the dizzying number of potential combinations provides for reasonable-\nlooking, yet not inherently predictable results. \n \nConclusion \nTo sum up, while our desire as game developers may be to express a variety of reasonable-\nlooking but slightly unpredictable behaviors, we do not have to resort to randomness in \norder to generate that effect. By including more than one or two simple, easily perceivable \ncriteria in our decision models, we can begin to obscure the workings of that model from the \nplayer, yet leave it perfectly exposed and understandable to the programmer and even the \ndesign team. However, in order to avoid the potential for arbitrary-looking decisions by our \nagents, we must be careful to select criteria that are relevant to the decision being made. In \nthis way we are also providing deeper, more realistic-looking, and potentially more \nimmersive behaviors for our agents. \n \nReferences \n[Ellinger08] Ellinger, Benjamin. ―Artificial Personality: A Personal Approach to AI.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media, 2008. \n[Mark08] Mark, Dave. ―Multi-Axial, Dynamic Threshold Fuzzy State Machine.‖ AI Game \nProgramming Wisdom 4. Boston: Charles River Media, 2008. \n[Mark09] Mark, Dave. Behavioral Mathematics for Game AI. Boston: Charles River Media, \n2009. \n",
      "content_length": 3328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": " \n \n[Wikipedia09] ―Henri Poincaré.‖ n.d. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Henri_Poincaré>. \n \n3.8. Needs-Based AI \nRobert Zubek \nNeeds-based AI is a general term for action selection based on attempting to fulfill a set of \nmutually competing needs. An artificial agent is modeled as having a set of conflicting \nmotivations, such as the need to eat or sleep, and the world is modeled as full of objects \nthat can satisfy those needs at some cost. \nThe core of the AI is an action selection algorithm that weighs those various possible actions \nagainst each other, trying to find the best one given the agent‘s needs at the given \nmoment. The result is a very flexible and intuitive method for building moderately complex \nautonomous agents, which are nevertheless efficient and easy to understand. \nThis gem presents some technical details of needs-based AI. We begin with a general \noverview and then dive directly into technical implementation, presenting both general \ninformation and some specific hints born out of experience implementing this AI. Finally, we \nfinish with an overview of some design consequences of using this style of AI. \nBackground \nIn terms of its historical context, the needs-based AI approach is related to the family of \nbehavior-with-activation-level action selection methods common in autonomous robotics. \n(For an overview, see [Arkin98], page 141.) In game development, it was also \nindependently rediscovered by The Sims, where it has been enjoyed by millions of game \nplayers. The Sims also contributed a very useful innovation on knowledge representation, \nwhere behaviors and their advertisements are literally distributed ―in the world‖ in the \ngame, and therefore are very easily configurable. \nMy own interest in this style of AI was mainly driven by working with The Sims (at \nNorthwestern University and later at EA/Maxis). I have since reimplemented variations on \nthis approach in two other games: Roller Coaster Kingdom, a web business simulation \ngame, and an unpublished RPG from the Lord of the Rings franchise. I found the technique \nto be extremely useful, even across such a range of genres and platforms. \nUnfortunately, very few resources about the original Sims AI remain available; of those, \nonly a set of course notes by Ken Forbus and Will Wright, plus a Sims 2 presentation by \nJake Simpson are freely downloadable on the web at this point. (Links can be found in the \n―References‖ section at the end of this gem.) My goal here is to present some of this \nknowledge to a wider audience, based on what I‘ve gained from robotics and The Sims, as \nwell as personal experience building such agents in other games. \n \nNeeds-Based AI Overview \nThere are many ways to drive an artificial agent; some games use finite state machines, \nothers use behavior trees, and so on. Needs-based AI is an alternative with an exciting \nbenefit: The smarts for picking the next action configure themselves automatically, based \non the agent‘s situation as well as internal state; yet the entire algorithm remains easy to \nunderstand and implement. \n",
      "content_length": 3103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": " \n \nEach agent has some set of ever-changing needs that demand to be satisfied. When \ndeciding what to do, the agent looks around the world and figures out what can be done \nbased on what‘s in the area. Then it scores all those possibilities, based on how beneficial \nthey are in satisfying its internal needs. Finally, it picks an appropriate one based on the \nscore, finds what concrete sequence of actions it requires, and pushes those onto its action \nqueue. \nThe highest-level AI loop looks like this: \n \nWhile there are actions in the queue, pop the next one off, perform it, and maybe \nget a reward. \n \nIf you run out of actions, perform action selection, based on current needs, to find \nmore actions. \n \nIf you still have nothing to do, do some fallback actions. \nThat second step, the action selection point, is where the actual choice happens. It \ndecomposes as follows: \n1. Examine objects around you and find out what they advertise. \n2. Score each advertisement based on your current needs. \n3. Pick the best advertisement and get its action sequence. \n4. Push the action sequence on your queue. \nThe next sections will delve more deeply into each of these steps. \nNeeds \nNeeds correspond to individual motivations—for example, the need to eat, drink, or rest. \nThe choice of needs depends very much on the game. The Sims, being a simulator of \neveryday people, borrowed heavily from Maslow‘s hierarchy (a theory of human behavior \nbased on increasingly important psychological needs) and ended up with a mix of basic \nbiological and emotional drivers. A different game should include a more specific set of \nmotivations, based on what the agents should care about in their context. \nInside the engine, needs are routinely represented as an array of numeric values, which \ndecay over time. In this discussion we use the range of [0, 100]. Depending on the context, \nwe use the term ―need‖ to describe both the motivation itself (written in boldface—for \nexample, hunger) and its numeric value (for example, 50). \nNeeds routinely have the semantics of ―lower is worse and more urgent,‖ so that \nhunger=30 means ―I‘m pretty hungry,‖ while hunger=90 means ―I‘m satiated.‖ Need values \nshould decay over time to simulate unattended needs getting increasingly worse and more \nurgent. Performing an appropriate action then refills the need, raising it back to a higher \nvalue. \nFor example, we simulate agents getting hungry if they don‘t eat by decaying the hunger \nvalue over time. Performing the ―eat‖ action would then refill it, causing it to become less \nurgent (for a while). \n \nAdvertisements and Action Selection \nWhen the time comes to pick a new set of actions, the agent looks at what can be done in \nthe environment around them and evaluates the effect of the available actions. \n",
      "content_length": 2801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": " \n \nEach object in the world advertises a set of action/reward tuples—some actions to be taken \nwith a promise that they will refill some needs by some amount. For example, a fridge \nmight advertise a ―prepare food‖ action with a reward of +30 hunger and ―clean‖ with the \nreward of +10 environment. \nTo pick an action, the agent examines the various objects around them and finds out what \nthey advertise. Once we know what advertisements are available, each of them gets scored, \nas described in the next section. The agent then picks the best advertisement using the \nscore and adds its actions to their pending action queue. \nAdvertisement Decoupling \nPlease notice that the discovery of what actions are available is decoupled from choosing \namong them: The agent ―asks‖ each object what it advertises, and only then scores what‘s \navailable. The object completely controls what it advertises, so it‘s easy to enable or disable \nactions based on object state. This provides great flexibility. For example, a working fridge \nmight advertise ―prepare food‖ by default; once it‘s been used several times, it also starts \nadvertising ―clean me‖; finally, once it breaks, it stops advertising anything other than ―fix \nme‖ until it‘s repaired. \nWithout this decoupling, imagine coding all those choices and possibilities into the agent \nitself, not just for the fridge but also for all the possible objects in the world—it would be a \ndisaster and impossible to maintain. \nOn the other side of the responsibility divide, the agent can also be selective about what \nkinds of advertisements it accepts. We can use this to build different agent subtypes or \npersonalities. For example, in a later section we will describe how to use advertisement \nfiltering to implement child agents with different abilities and opportunities than adults. \nAdvertisement Scoring \nOnce we have an object‘s advertisements, we need to score them and stack them against \nall the other advertisements from other objects. We score each advertisement separately, \nbased on the reward it promises (for example, +10 environment) and the agent‘s current \nneeds. Of course it‘s not strictly necessary that those rewards actually be granted as \npromised; this is known as false advertising, and it can be used with some interesting \neffects, as described later. \nHere are some common scoring functions, from the simplest to the more sophisticated: \nA. Trivial scoring \nfuture valueneed = current valueneed + advertised deltaneed \nscore = ∑ all needs (future valueneed) \nUnder this model, we go through each need, look up the promised future need value, \nand add them up. For example, if the agent‘s hunger is at 70, an advertisement of \n+20 hunger means the future value of hunger will be 90; the final score is the sum \nof all future values. \nThis model is trivially easy and has significant drawbacks: It‘s only sensitive to the \nmagnitude of changes, and it doesn‘t differentiate between urgent and non-urgent \nneeds. So increasing hunger from 70 to 90 has the same score as increasing thirst \nfrom 10 to 30—but the latter should be much more important, considering the agent \nis very thirsty! \n",
      "content_length": 3167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": " \n \nB. Attenuated need scoring \nNeeds at low levels should be much more urgent than those at high levels. To model \nthis, we introduce a non-linear attenuation function for each need. So the score \nbecomes: \nscore = ∑ all needs Aneed (future valueneed) \nwhere Aneed is the attenuation function, mapping from a need value to some numeric \nvalue. The attenuation function is commonly non-linear and non-increasing: It starts \nout high when the need level is low and then drops quickly as the need level \nincreases. \nFor example, consider the attenuation function A(x) = 10/x. An action that increases \nhunger to 90 will have a score of 1/9, while an action that increases thirst to 30 will \nhave a score of 1/3, so three times higher, because low thirst is much more \nimportant to fulfill. These attenuation functions are a major tuning knob in needs-\nbased AI. \nYou might also notice one drawback: Under this scheme, improving hunger from 30 \nto 90 would have the same score as improving it from 50 to 90. Worse yet, \nworsening hunger from 100 to 90 would have the same score as well! This detail \nmay not be noticeable in a running system, but it‘s easy to fix by examining the \nneed delta as well. \nC. Attenuated need-delta scoring \nIt‘s better to eat a filling meal than a snack, especially when you‘re hungry, and it‘s \nworse to eat something that leaves you hungrier than before. To model this, we can \nscore based on need level difference: \nscore = ∑ all needs (Aneed (current valueneed) – Aneed (future valueneed)) \nFor example, let‘s consider our attenuation function A(x) = 10/x again. Increasing \nhunger from 30 to 90 will now score 1/3 – 1/9 = 2/9, while increasing it from 60 to \n90 will score 1/6 – 1/9 = 1/18, so only a quarter as high. Also, decreasing hunger \nfrom 100 to 90 will have a negative score, so it will not be selected unless there is \nnothing else to do. \nAction Selection \nOnce we know the scores, it‘s easy to pick the best one. Several approaches for arbitration \nare standard: \n \nWinner-takes-all: The highest-scoring action always gets picked. \n \nWeighted-random: Do a random selection from the top n (for example, top three) \nhigh-scoring advertisements, with probability proportional to score. \n \nOther approaches are easy to imagine, such as a priority-based behavior stack. \nIn everyday implementation, weighted-random is a good compromise between having some \npredictability about what will happen and not having the agent look unpleasantly \ndeterministic. \nAction Selection Additions \nThe model described earlier can be extended in many directions to add more flexibility or \nnuance. Here are a few additions, along with their advantages and disadvantages: \n",
      "content_length": 2696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": " \n \nA. Attenuating score based on distance \nGiven two objects with identical advertisements, an agent should tend to pick the \none closer to them. We can do this by attenuating each object‘s score based on \ndistance or containment: \nscore = D ( ∑ all needs ( … ) ) \nwhere D is some distance-based attenuation function, commonly a non-increasing \none, such as the physically inspired D(x) = x / distance2. However, distance \nattenuation can be difficult to tune, because a distant object‘s advertisement will be \nlowered not just compared to other objects of this type, but also compared to all \nother advertisements. This may lead to a ―bird in hand‖ kind of behavior, where the \nagent always prefers a much worse action nearby rather than a better one farther \naway. \nB. Filtering advertisements before scoring \nIt‘s useful to add prerequisites to advertisements. For example, kids should not be \nable to operate stoves, so the stove should not advertise the ―cook‖ action to them. \nThis can be implemented in several ways, from simple attribute tests to a full \nlanguage for expressing predicates. \nIt‘s often best to start with a simple filter mechanism, because complex prerequisites \nare more difficult to debug when there are many agents running around. An easy \nprerequisites system could be as simple as setting Boolean attributes on characters \n(for example, is-adult, and so on) and adding an attribute mask on each \nadvertisement; action selection would only consider advertisements whose mask \nmatches up against the agent‘s attributes. \nC. Tuning need decay \nAgents‘ need levels should decay over time. This causes agents to change their \npriorities as they go through the game. We can tune this system by modifying need \ndecay rates individually. For example, if an agent‘s hunger doesn‘t decay as quickly, \nthey will not need to eat as often and will have more time for other pursuits. \nWe can use this to model a bare-bones personality profile—for example, whether \nsomeone needs to eat/drink/entertain themselves more or less often. It can also be \nused for difficulty tuning—agents whose needs decay more quickly are harder to \nplease. \nD. Tuning advertisement scores \nThe scoring function can also simulate simple personality types directly, by tuning \ndown particular advertisement scores. To do this, we would have each agent contain \na set of tuning parameters, one for each need, that modify that need‘s score: \nnew scoreagent,need = old scoreagent,need * tuningagent,need \nFor example, by tuning down the +hunger advertisement‘s score, we‘ll get an agent \nthat has a stronger preference for highly fulfilling food; tuning up a +thirst \nadvertisement will produce an agent that will happily opt for less satisfying drinks, \nand so on. \nE. Attenuation function tuning \n",
      "content_length": 2791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": " \n \nAttenuation functions map from low need levels to high scores. Each need can be \nattenuated differently, since some needs are more urgent than others. As such, they \nare a major tuning knob in games, but a delicate one because their effects are \nglobal, affecting all agents. This requires good design iterations, but analytic \nfunctions (for example, A(x) = 10/x) are not easy for designers to tweak or reason \nabout. \nA happy medium can be found by defining attenuation functions using piecewise-\nlinear functions (in other words, point pairs that define individual straight-line \nsegments, rather than continuous, analytic formulas). These can be stored and \ngraphed in a spreadsheet file and loaded during the game. \n \nAction Performance \nHaving chosen something to do, we push the advertisement‘s actions on the agent‘s action \nqueue, to be performed in order. Each action would routinely be a complete mini-script. For \nexample, the stove‘s ―clean‖ action might be small script that: \n \nAnimates the agent getting out a sponge and scrubbing the stove \n \nRuns the animation loop and an animated stove condition meter \n \nGrants the promised reward \nIt‘s important that the actual reward be granted manually as part of the action, and not be \nawarded automatically. This gives us two benefits: \n \nInterrupted actions will not be rewarded. \n \nObjects can falsely advertise and not actually grant the rewards they promised. \nFalse advertisement is an especially powerful but dangerous option. For example, suppose \nthat we have a food item that advertises a hunger reward but doesn‘t actually award it. A \nhungry agent would be likely to pick that action—but since they got no reward, at the next \nselection point they would again likely pick, and then again, and again. This quickly leads to \nvery intriguing ―addictive‖ behaviors. \nThis may seem like a useful way to force agents to perform an action. But it‘s just as hard \nto make them stop once they‘ve started. False advertisements create action loops that are \nvery difficult to tune. In practice, forcing an action is easier done by just pushing the \ndesired action on the agent‘s action queue. \nAction Chaining \nPerforming a complex action, such as cooking a meal, usually involves several steps (such \nas preparing and cooking) and several objects (a fridge, a cutting board, a stove). This \nsequence must not be atomic—steps can be interrupted, or they can fail due to some \nexternal factors. \nComplex sequences are implemented by chaining multiple actions together. For example, \neating dinner might decompose into several separate actions: \n \nTake a food item from the fridge. \n \nPrepare the food item on a counter. \n \nCook the food item on the stove. \n \nSit down and eat, thereby getting a hunger reward. \n",
      "content_length": 2775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": " \n \nIt would be suboptimal to implement this as a single action; there is too much variability in \nthe world for it to always work out perfectly. \nWe can create action sequences in two ways. The simpler way is to just manufacture the \nentire sequence of actions right away and push the whole thing on the agent‘s queue. Of \ncourse, these steps can fail, in which case the remaining actions should also be aborted. For \nsome interesting side effects, aborting an action chain could create new actions in its place. \nFor example, a failed ―cook food‖ action sequence could create a new ―burned food‖ object \nthat needs to be cleaned up. \nThe second method, more powerful but more difficult, is to implement action chaining by \n―lazy evaluation.‖ In this approach, only one action step is created and run at a time, and \nwhen it ends, it knows how to create the next action and front-loads it on the queue. \nFor an example of how that might look, consider eating dinner again. The refrigerator‘s \nadvertisement would specify only one action: ―take food.‖ That action, toward the end, \nwould then find the nearest kitchen counter object, ask it for the ―prepare food‖ action, and \nload that on the queue. Once ―prepare food‖ was done, it would find the nearest stove, ask \nit for a new ―cook food‖ action, and so on. \nLazy action chaining makes it possible to modify the chain based on what objects are \navailable to the agent. For example, a microwave oven might create a different ―cook food‖ \naction than a stove would, providing more variety and surprise for the player. Second, it \nmakes interesting failures easier. For example, the stove can look up some internal variable \n(for example, repair level) to determine failure and randomly push a ―create a kitchen fire‖ \naction instead. \nIn either case, using an action queue provides nice modularity. Sequences of smaller action \ncomponents are more loosely coupled and arguably more maintainable than standard state \nmachines. \nAction Chain State Saving \nWhen an action chain is interrupted, we might want to be able to save its state somehow so \nthat it gets picked up later. \nSince all actions are done on objects, one way to do this is to mutate the state of the object \nin question. For example, the progress of ―cleaning‖ can be stored as a separate numeric \ncleanness value on an object, which gets continuously increased while the action is running. \nBut sometimes actions involve multiple objects, or the state is more complicated. Another \nway to implement this is by creating new state objects. An intuitive example is food from \nthe original Sims: The action of prepping food creates a ―prepped food‖ object, cooking then \nturns it into a pot of ―cooked food,‖ which can be plated and turned into a ―dinner plate.‖ \nThe state of preparation is then embedded right in the world; if the agent is interrupted \nwhile prepping, the cut-up food will just sit there until the agent picks it up later and puts it \non the stove. \n \nDesign Consequences of Needs-Based AI \nWith the technical details of needs-based AI behind us, let‘s also consider some of the \ndesign implications of this style of development, since it‘s different from more traditional \ntechniques. \n",
      "content_length": 3221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": " \n \nFirst of all, the player‘s experience with this AI really benefits from adding some feedback to \nthe agents. Developers can just look at the internal variables and immediately see ―the \nagent is doing this because it‘s hungry, or sleepy, or other such.‖ But the player will have \nno such access and is likely to build an entirely different mental model of what the agent is \ndoing. Little bits of feedback, like thought bubbles about what needs are being fulfilled, are \neasy to implement and go a long way toward making the system comprehensible to the \nplayer. \nA second point is about tuning. Some of the tunable parameters have global effect and are \ntherefore very difficult to tune after the game has grown past a certain size. The set of \nneeds, their decay rates, score attenuation functions, and other such elements will apply to \nall characters in the game equally, so tuning them globally requires a lot of testing and a \ndelicate touch. \nIf a lot of variety is desired between different parts of the game, it might be a good idea to \nsplit the game into a number of smaller logical partitions (levels, and so on) and have a \ndifferent set of those tunable parameters, one for each partition. Ideally, there would be a \nset of global tuning defaults, which work for the entire game, and each partition could \nspecifically override some of them as needed. Partitioning and overriding tuning values buys \nus greater flexibility, although at the cost of having to tune each partition separately. \nThird, this AI approach tends heavily toward simulation and makes it hard to do scripted \nscenes or other triggered actions. Imagine implementing some actions on a trigger, such as \nhaving the agent approach the player when he comes into view. One might be tempted to \ntry to implement that using just needs and advertisements, but the result will be brittle. \nIf particular one-off scripted behaviors are desired, it would be better to just manually \nmanufacture appropriate action sequences and forcibly push them on the agent‘s action \nqueue. But in general, this overall approach is not very good for games that need a lot of \ntriggered, scripted sequences (for example, shooter level designs). Needs-based AI works \nbetter for simulated worlds than for scripted ones. \n \nConclusion \nNeeds-based AI is computationally very efficient; only a trivial amount of the CPU is \nrequired to pick what to do and to handle the resulting action sequence. The system‘s \ninternals are very easy to understand; by just inspecting the agent‘s internal needs values, \nyou can get a good idea of why it does what it does. And by externalizing the set of possible \nactions into the world, the system also achieves great modularity—the AI can be \n―reconfigured‖ literally by adding or removing objects around the agent. \nIn spite of unusual design consequences, the needs-based approach is very capable, easy to \nimplement, and effective at creating good characters. It‘s a powerful tool for many \nsituations. \n \nAcknowledgements \nThanks to Ken Forbus and Richard Evans, from whom I‘ve learned most of what I know \nabout this style of AI. \n \n",
      "content_length": 3136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": " \n \nReferences \n[Arkin98] Arkin, R. Behavior Based Robotics. MIT Press, 1998. \n[Forbus02] Forbus, Ken, and Will Wright. ―Simulation and Modeling: Under the Hood of The \nSims.‖ 2002. Northwestern University. n.d. <http://www.cs.northwestern.edu/~forbus/c95-\ngd/lectures/The_Sims_Under_the_Hood_files/frame.htm>. \n[Simpson] Simpson, Jake. ―Making The Sims the Sims.‖ n.d. \n<https://www.cmpevents.com/Sessions/GD/ScriptingAndSims2.ppt>. \n \n3.9. A Framework for Emotional Digital Actors \nPhil Carlisle \nzoombapup@gmail.com \nIn this gem, we will describe a framework that may be used to endow game characters with \nsome level of emotional behavior. Using a simple behavior tree implementation augmented \nwith supporting appraisal and blackboard implementations, we will demonstrate that \nemotions can be easily implemented and can enhance the behavior and expression of game \ncharacters. We use the term ―digital actor‖ because although the emotional system we \npresent is based on sound academic research in psychology and cognitive science, the \nintention is to produce the ―illusion‖ of emotion, rather than specifically trying to model \nemotion itself. However, the reader is advised to read [Oatley06] as an accessible \nintroduction to emotional psychology, which is a useful starting point for anyone trying to \nadd more emotion to their games. \nThe first section of this gem will describe models of personality, mood, and emotion derived \nfrom academic literature in the area. The second section will describe how these models are \nincorporated into an emotional framework. The third section will describe how the \nframework is used in a number of game scenarios. In conclusion, we will offer some ideas \nfor further work. \nModels of Emotion, Mood, and Personality \nIn addition to emotion, we need to represent both personality and mood in order to have a \ncomplete emotional framework. It is useful to consider these elements in terms of the \ntimescale required for change. \nTypically, our personality changes very slowly, if at all. It can take many years for our \npersonalities to alter. Mood, on the other hand, tends to change in a relatively shorter \nperiod of days or weeks. Finally, we have emotions, which are frequently expressed minute \nto minute and can often only be portrayed for fleeting seconds. For instance, it is not \nunusual to hear someone described as having a ―quick temper.‖ \nEmotion \nThere are many theories of emotion and models of personality associated with them. For \nexample, [Eyesenck65] describes models of personality and emotion often cited in academic \nliterature. One model of emotion used in a large volume of academic literature is the OCC \nmodel [Ortony88] created by Ortony, Chlore, and Collins. This model is discussed very well \nin [Bartneck02], and, as is the case with many academic implementations, we are going to \nuse a subset of the OCC model for our emotional representation. \n",
      "content_length": 2928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": " \n \nThe OCC model typically represents 22 discrete emotional categories. In practice, this is \nprobably too complex a model for most video games; thus, the model should be simplified \nbased on the requirements for a particular game. The OCC model, broadly speaking, breaks \ndown emotions into three categories: emotional reaction to objects, events, and agents. \nEach category is then broken down further depending on whether the emotion is related to \nself or other and whether the emotion is considered a good thing or a bad thing. The \ncomplexity arises because we have to account for the consequences for others and our \nrelationship with them. Consider the following case: \n \nAgent A likes apples. \n \nAgent B likes apples and Agent A. \n \nAgent C likes apples but not Agent A. \nIn a simulation with all three agents and one apple, assuming that Agent A acquires the \napple, we then have the following emotional reactions: \n \nAgent A is happy (due to acquiring the apple). \n \nAgent B is unhappy at not acquiring the apple but happy at Agent A acquiring the \napple. \n \nAgent C is unhappy at not acquiring the apple and is even less happy at seeing \nAgent A acquire the apple. \nThese are relatively simple direct relationships between goal (acquire apple, self/ friend) \nand emotion (happy, sad). But human emotion is a little more complicated. In this case, \nAgent C may immediately feel unhappy, but that feeling may then cause him to feel \nashamed that he was made to feel unhappy by the actions of A. Clearly, we are going to \nhave to simplify the model somewhat to be useful in a video-game context, but be aware \nthat sometimes the most interesting emotional expression comes from the feelings created \nby social situations exactly like this. \nMood \nMood is generally more changeable than personality. However it is often represented very \nsimply. A useful paper that represents mood simply is [Egges04], which maps mood to a \nsingle floating-point number in the range of –1..1 to represent negative or positive moods, \nrespectively. Typically, mood changes over weeks or months and acts as an overall bias that \nchanges as different emotional events occur. \nInterestingly, mood can color our perception of emotional events. For instance, people in a \nnegative mood may view all emotional events as negative, even if the event is generally to \ntheir advantage. In the case of the earlier example, a negative mood might be used within \nthe behavior tree to disable certain actions. For instance, if Agent A is in a highly negative \nmood, even the perception of an apple may not trigger the behavior tree action required to \nseek attainment of the apple. This can be accomplished only by adding knowledge of the \navailable apple to the agent‘s blackboard when it has a mood that is higher than a given \nthreshold, although care must be taken to allow the desire to maintain the agent‘s health \n(by acquiring food, for example) to override this behavior. See the description of the \nappraisal class later in this gem for further information. \nPersonality \nPersonality is often represented in the academic literature via the OCEAN model of \n[McRae96]. This personality model represents a person‘s personality across five dimensions \nrepresenting: \n \nOpenness \n \nConscientiousness \n \nAgreeability \n",
      "content_length": 3300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": " \n \n \nExtroversion \n \nNeuroticism \nThe values portrayed for each of the five dimensions are typically in the range of 0..1, \nwhere, for example, 0 for openness means that the person is fully closed off and +1 means \nthe person is fully open. Essentially, the model of personality provides a default biasing \nmechanism for further formulas and allows us to represent different personality types by \nsimply configuring each dimension with different default floating-point values. \nPersonality is the least changeable part of our emotional makeup, so it is reasonable to \nsimply store personality as a series of fixed values. Personality provides bias toward a \nparticular set of possible actions within the behavior tree. Referring to the earlier example \nscenario for Agent B, given the choice of two potential actions of either trying to obtain the \napple for oneself or allowing Agent A to obtain the apple, the personality model can be used \nto select from the two choices. An agent with high agreeability would choose to allow Agent \nA to obtain the apple, whereas the converse would simply try to obtain the apple for itself. \nThus, the personality model allows us to create unique behavior for each agent without the \nneed for per-agent behavior trees. \n \nThe Emotional Framework \nGiven the emotional model described in the previous section, we need to be able to \nincorporate code that represents the model within an architecture that enables it to affect \nour characters‘ behavior. In this example, we will incorporate the emotional model by \nimplementing an appraisal class, which modifies values within a character‘s blackboard. \n(See [Isla02] for information concerning blackboards.) The blackboard will be inspected by \na simple behavior tree in order to incorporate the emotional values within the characters‘ \nupdate logic, and the same mechanism can also be used to incorporate the emotional \nvalues with the characters‘ movement logic—for example, animating with a sullen walk \ncycle if the emotional mood is negative. A very good reference concerning the use of \nbehavior trees and emotional models is the work of the Oz project group at Carnegie Mellon \nUniversity [Reilly96], which went on to be used in the game Façade. \nFigure 3.9.1. The emotional framework architecture. \n \n \n",
      "content_length": 2297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": " \n \nAppraisal/Arousal \nTypically, emotion is broken up into appraisal, where goals are created and events and \nobjects are classified, and arousal, where the magnitude of the reaction to the sensory input \nis processed. The purpose of the appraisal class is to map any sensory input to changes in \nthe agent‘s emotional variables and blackboard data. In our framework, for convenience this \nclass handles both appraisal and arousal. During appraisal, sensory input and event data \nare fed to the appraisal class, which then determines the appropriate changes in blackboard \ndata. During the appraisal, data may be added to the knowledge representation, changes \nmay be made to the emotional variables, and in certain cases input may be ignored. \nSimilarly, the appraisal class is responsible for determining the changes occurring from the \nsuccess or failure of the agent‘s goals, which may again alter the agent‘s blackboard or \nemotional variables. \nKnowledge Representation \nThere is a choice to be made with regard to the appraisal class relating to its usage of \nmemory and dynamic data structures. In a human context, we are capable of learning \nabout new objects we have not encountered before, new events that happen, or new agents \n(human or otherwise) that we meet. This implies that we are able to store information as \nwe build up an emotional picture relating to these objects, events, and agents. In a game \ncontext, we may or may not be able to spare the memory to process previously unknown \ninformation. \nThe easiest case for implementation and perhaps the more robust case for design is that we \nexplicitly determine all possible known objects/events/agents a priori and simply load that \ninformation at run time. This is the method chosen for the example implementation. An \neasy method of expanding on this simple implementation is to incorporate a classifier \nsystem such that instead of storing a reaction to an individual object/event/agent, we \nclassify them and store a reaction to the ―class‖ rather than a specific instance of the class. \nAn alternative method is to simply place an upper limit on the number of memories stored \ndynamically, allowing new memories to be created. These memories would then expire over \ntime to allow further memories to be formed. In addition, this scheme may be extended \nwith different limits on the amount of memories stored for each type of event, thus allowing \nfor more ―important‖ memories to be stored. The use of a blackboard allows us to store all \nper-agent data in a generic structure that allows access from all of our actor‘s systems. \nTypically, this is used to store agent goals or attributes, such as the currently selected \ntarget enemy. However, the blackboard is a useful storage mechanism to employ for the \ncreation of emotional agents. \nIt is worth bearing in mind that the most common case of retrieval from the blackboard—to \nretrieve a particular value associated with a new sensed object/agent/ event—must be as \nefficient as possible. An alternative structure, such as a semantic network [Sowa92], may \nprove to be a far better solution, if slightly more complex in terms of code. \nFor convenience, each agent‘s blackboard and behavior tree configurations are parsed from \nXML data. This allows for run-time configuration of each agent, using a unique blackboard, a \nunique behavior tree, or both. \nXML data for an example blackboard: \n<BB> \n<Personality O=\"0.1\" C=\"0.6\" E=\"0.1\" A=\"0.9\" \nN=\"0.2\"></Personality> \n<Mood default=\"1.0\"></Mood> \n<Event name=\"PickupApple\"   valence=\"1.0\">   </Event> \n<Event name=\"PickupOrange\"  valence=\"-1.0\">  </Event> \n",
      "content_length": 3638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": " \n \n<Event name=\"PickupGrenade\" valence=\"-10.0\"> </Event> \n<Object name=\"Apple\"        valence=\"1.0\">   </Object> \n<Object name=\"Orange\"       valence=\"-1.0\">  </Object> \n<Object name=\"Grenade\"      valence=\"-10.0\"> </Object> \n<Agent name=\"Fred\"          valence=\"2.0\">   </Agent> \n<Agent name=\"Wilma\"         valence=\"3.0\">   </Agent> \n<Agent name=\"Betty\"         valence=\"30.0\">  </Agent> \n</BB> \n \nHere we can see a simple blackboard specification for an agent. Each sub-element with a \ntag of <agent>, < object>, or <event> defines a unique structure that is stored within \nthe blackboard. Each element is stored internally as a simple stl::vector of the \nappropriate type. \nThe Appraisal Process \nTo modify our behavior based on the emotional framework, we need to consider the steps \nthat occur when new emotional responses are required. \nSensing a New Object \nAn agent‘s perception system typically responds to queries instigated by its behavior tree \n(which we will refer to as a BT for brevity‘s sake). For example, the BT may have executed \na sequence of nodes that resulted in a query for the availability of nearby food. Our goal in \nthis case is to determine the agent‘s emotional reaction to each sensed object—specifically, \nthe more useful case of allowing the agent to determine the selection of which food object \nto try to obtain based on the emotional reward associated with those available. \nConsider the case where a query returns three different food items within the query radius. \nIn the simplest case, we can simply determine our like/dislike of the available food items \nbased on a simple classification, such as whether the item is fruit or vegetable or whether \nthe item is sweet or sour. In the ideal case, we need to consider our past experiences either \nwith the unique object or with objects with a similar classification. It is beyond the scope of \nthis gem to discuss the intricacies of human memory and its method of classification. \nAnother aspect of this pattern of memory is that the relative novelty of an object can \ngreatly alter the intensity of the reaction to the object. An agent who is unused to seeing \nguns may react significantly to the sight of an armed friend, whereas a gangster would be \nless likely to have a similar reaction. The final aspect we should consider when dealing with \nobjects is the penalty or reward associated with interactions. For example, an agent may \nhave a strong liking for apples, but if the agent consumes an apple that is sour, it should \nhave some effect on the subsequent desire for more apples. \nIn practical terms, in the case of our object queries, we first use the appraisal class to \nreturn preference values for each object in turn. We then simply choose the object with the \nhighest appraisal value for interaction. Once an object has been selected, we store that \nobject as a goal within the blackboard as an object to obtain. It is important to constrain \nour memory usage at this point, as new objects may be perceived quite often and marked \nfor attainment. We can achieve this by attaching an expiration value to each new object \nattainment goal. The blackboard then removes all expired goals within its update loop. \nOnce an object is attained (typically via another node in the behavior tree), we then \nconsider any attainment goals relating to the object. If a specific attainment goal is found \nwithin the blackboard, we then consider the arousal value of achieving the specific goal. \nThis arousal processing takes into account the goals for attainment of the object. There are \ntwo major reactions to consider here. The first is that the agent must consider his own \nreaction to the object. Typically, we would try to obtain objects we like, but if the game \n",
      "content_length": 3760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": " \n \nallows attaining of objects by other means—for example, by allowing agents to simply give \nobjects to each other—there may be negative consequences. An agent who obtains a ticking \ntime bomb should definitely not be happy about its attainment. The second reaction to the \nattainment of an object is with respect to other agents. An agent who obtains an object that \nis highly desired by another agent, depending on whether the other agent is liked or \ndisliked, may feel guilty or happy for acquiring the item, respectively. \nAlternatively, if no specific attainment goals are stored in the blackboard, we can simply \nconsider the attainment goals of other agents for the object, or we can consider the general \nvalence of the object and react based on our positive or negative feelings about it. For \nexample, an object that is attained may allow us to accomplish a goal for another agent if \nwe give it to him. In this case, we may simply decide to create a goal to pass on the object \nif it achieves a goal of an agent we have positive affect towards, or we may decide to keep \nor dispose of the object if it denies a goal of an agent we have negative affect towards. \nSensing a New Agent \nThe term ―agent‖ in the OCC model does not describe an AI agent, but instead describes an \nagent of change. Typically, these are often other characters in a game context. However, \nthis is not always the case, and an agent in OCC terms may be some other external force \nthat has an effect on the world. In the most common case, the agents in our emotional \nframework will actually describe characters within the game world. With this in mind, we will \nuse the term ―agent‖ to mean both the OCC model of an agent and the AI game character \nagent. \nModeling of inter-agent affect affords us some unique social interactions, such as the \nseemingly altruistic act of passing on an object. The most obvious use of agent knowledge \nwhen considering emotion is for a like/dislike evaluation. This can be simply stored as a \npositive or negative value associated with a unique identifier. In the example framework, \nagents are stored by name with a valence value associated with them within the \nblackboard. This value is useful for when any opportunities are presented to the agent, such \nas denying another agent a resource or being able to give another agent something they \nrequire. \nThis brings up an important point about agent-to-agent interactions. Typically, when \nhumans interact, they create mental models of the motivations of the interacting agent in \norder to determine how to proceed with the interaction. For instance, when a human \nconsiders giving a gift to another, they try and imagine the reaction to the gift the other \nperson will have, using this as a method of deciding whether to proceed with the \ninteraction. This agent mental modeling is important for social interactions; however, it is \nproblematic for games because of the amount of memory and processing time required for \nimplementation. The problem is compounded by the notion that humans often model other \nhumans‘ models of themselves (in other words, how does this person feel about us?). In the \nexample framework, we have decided to leave this mental modeling unimplemented for the \nsake of brevity and for practical purposes. However, for a truly deep social simulation, this \nmodeling is a highly desirable feature. \nSensing an Event \nA great deal of an agent‘s behavior will generally stem from sensing some event that occurs \nwithin the world. This event could be an object attainment event, for self or for other. It \nmight also be an important event that requires immediate action, such as hearing a grenade \ndrop at the agent‘s feet. In this situation, the appraisal class uses its understanding of \navailable events to create the associated knowledge within the agent‘s blackboard. \nIn the case of the grenade event, the appraisal class simply adds a threat object with a high \nreaction value to the blackboard. This will then allow the BT that has a branch ―respond to \nthreat‖ to take the appropriate actions. The reason why it is useful to pass events via the \n",
      "content_length": 4151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": " \n \nappraisal class is that the importance of an event can change over time as an agent \nresponds to more of the same event. Consider the case of the grenade event. There is a \nradius within which we can expect to take damage, but outside of that radius, the reaction \nto the grenade can change depending on how many times we have seen grenades explode. \nIf we know from experience that outside of a certain radius we may sustain injury but that \nthe injury is entirely random, then we may, over time, be conditioned to simply block the \ngrenade from thought. This effect can happen in any stressful situation where our emotions \nallow us to regulate our reactions, essentially becoming ―numb‖ to what would normally be \nhighly stressful situations over time. This change in attitude toward events is due to the \nappraisal/arousal process. Essentially, this is a feedback loop that changes an agent‘s \nresponse over time as the agent adds positive or negative arousal to the event depending \non the event‘s outcome; in addition, this effect is subject to some decay in the dulling of the \narousal. In essence, this means that events that occur frequently become less arousing \nemotionally, but that if the event has not occurred for some time, the arousal may once \nagain be relatively high. In the example framework, this dulling of arousal values for events \nhappens by a simple scaling factor being applied to the arousal value for the event. Over \ntime, the scaling factor is reset to 1.0, with any events of that type causing the scale factor \nto reduce slightly. Thus, over time, the arousal associated with the event can move between \n0..1 depending on the frequency of the event. \nAnother aspect to the appraisal class is the incorporation of personality and mood into the \nemotional outcomes expressed in the blackboard. In effect, personality and mood modulate \nthe intensity of the emotional reaction to any given stimulus. For a good introduction to why \nthis is important, see [Eckman04]. To correctly simulate the effect of personality, mood, \nvarying arousal, and decay, we therefore apply different functions for calculating the effect \nof any given emotional stimulus and then apply the results of these functions to the agent‘s \nblackboard. For event stimulus, the first role of the appraisal is to determine whether to \nrespond to the event at all. Some events can simply be ignored, especially when in a state \nof high overall arousal. For instance, if a grenade event is perceived, any subsequent event \nis blocked from being perceived until the agent has dealt with the response. This simulates \nthe effect of our emotions, which act as regulatory systems allowing for rapid response to \ndangerous threats. Given a relatively low state of overall arousal, we can often respond to \nrelatively minor events. \nThe personality model acts to bias available choices within the behavior tree. Given two \npossible outcomes for any sensory input, we can use a scale factor based on the personality \nvariable associated with a given choice to determine which outcome has a higher priority. \nFor example, when given the choice to interact in a conversation with another agent or to \nobtain a required item, an agent with an introverted personality would choose the latter. \nThe mechanism for this choice involves classifying each behavior tree selection with respect \nto personality and then using this as a scaling value when doing priority selection, each \nchoice essentially scaling its priority up or down based on the personality trait variable \nvalue. \nThe model of mood, although simplistic in nature, allows us to further apply some filtering \non the selection of available behavior tree choices. The mood value is initially used in the \ninput phase of the appraisal class. This simply scales the emotional valence of input senses, \nwhich may cause some sensory input to be ignored when it otherwise may have been acted \nupon. For example, if we perceive an object that is beneficial to another agent, we calculate \na valence for the goal of attaining the apple for the other agent. Normally, we would then \ncreate the goal for the attainment of the apple by adding the apple to the blackboard. \nHowever, when the mood is negative, the positive affect generated by attaining the apple \nfor the other agent is cancelled out, and we simply never add the apple to the blackboard. \nGiven the framework described thus far, what does a typical update loop look like for the \nagent? See Figure 3.9.2. \nFigure 3.9.2. Agent update loop. \n",
      "content_length": 4548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": " \n \n \nThe update loop begins with sensory input. In practical usage, the event input can happen \nat any point in time, but it follows a very similar cyclic pattern represented by the sensory \nupdate loop represented in Figure 3.9.2. Input is fed into the appraisal class for processing. \nThis appraisal class is used to then map the input into changes in the agent blackboard. The \nappraisal may add or remove goals within the blackboard depending on the input. It also \nmay affect the emotional values associated with agents, objects, or events stored within the \nblackboard, which in turn may affect the processing within the appraisal class during the \nnext update cycle. Then the behavior tree responds to new knowledge within its blackboard \nand in turn effects changes in the game world. \nThe cycle is then repeated continually. This reactive framework simply represents a sense > \nthink > act cycle that is enhanced with emotion to become a sense > feel > think > act \ncycle. The blackboard may also be interrogated by animation and locomotion systems. For \nexample, to alter walk-cycle blending to allow for a display of mood, an agent with a \nnegative mood value may blend in more of a labored walk cycle. In comparison, an agent \nwith a positive mood may blend more of a bouncy walk cycle. \n \nConclusion \nEmotion is a very complex subject, and there are many academic theories that attempt to \ndescribe how emotions work and how they might be classified. Video games are unlikely to \never completely simulate the entire spectrum of emotional responses, even if it were \ndesirable to do so. As game designers and developers, we can incorporate simple models of \nemotion in order to add some personality to individual agents. \nNon-verbal communication is an important part of human social interaction [Mehrabian72], \nand emotional values modify this communication. For instance, we tend to hold gaze on \nagents we are attracted to for a lot longer. A large part of the motivation for the \nincorporation of emotion into video games is that we can start to add non-verbal \ncommunication signals to our agents. This helps to create agents that feel more realistic \nand alive. Emotions can be used to modify things such as posture, gesture, gaze, gait, \nbehavior, and memory. Imagine a world where agents remember your actions and are \npositively brimming with joy when you come to visit them! Imagine a world where you can \nunderstand simply from the posture of an agent whether it is happy to see you or intends to \ndo you harm. To learn more about non-verbal communication and how it works in humans, \nsee [Argyle75]. \nAs more memory and processing time become available on newer platforms, we may begin \nto consider deeper models of agent emotion and memory, which in turn should lead to a \nmore effective display of an agent‘s emotional state. This emotional display should lead to \n",
      "content_length": 2892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": " \n \nmore engaging and believable characters—agents that can express themselves and their \nemotions non-verbally and engage the emotions of players at a deeper level. \n \nReferences \n[Argyle75] Argyle, M. Bodily Communication. Methuen & Co Ltd., 1975. \n[Bartneck02] Bartneck, C. ―Integrating the OCC Model of Emotions in Embodied \nCharacters.‖ Workshop on Virtual Conversational Characters, 2002. \n[Eckman04] Ekman, P. Emotions Revealed. UK: Phoenix Books, 2004. \n[Egges2004] Egges, Arjan, Sumedha Kshirsagar, and Nadia Magnenat-Thalmann. ―Generic \nPersonality and Emotion Simulation for Conversational Agents.‖ Computer Animation and \nVirtual Worlds 15.1 (2004): 1–13. \n[Eysenck65] Eysenck, H. J. Fact and Fiction in Psychology. Harmondsworth: Penguin, 1965. \n[Isla02] Isla, D., and B. Blumburg. ―Blackboard Architecture.‖ AI Game Programming \nWisdom 1. Boston: Charles River Media, 2002. 333–344. \n[McRae96] McCrae, R., and P. T. Costa Jr. ―Toward a New Generation of Personality \nTheories: Theoretical Contexts for the Five-Factor Model.‖ The Five-Factor Model of \nPersonality: Theoretical Perspectives. Guilford Press, 1996. 51–87. \n[Mehrabian72] Mehrabian, A. Non-Verbal Communication. Transaction Publishing, 1972. \n[Oatley06] Oatley K., D. Keltner, and J. Jenkins. Understanding Emotions (2nd edition). \nBlackwell Publishers Inc., 2006. \n[Ortony88] Ortony, A., G. Clore, and A. Collins. The Cognitive Structure of Emotions. \nCambridge University Press, 1988. \n[Reilly96] Reilly, W. S. ―Believable Social and Emotional Agents.‖ PhD Thesis. School of \nComputer Science, Carnegie Melon University, Pittsburgh. 1996. \n[Sowa92] Sowa, J. ―Semantic Networks.‖ n.d. John F. Sowa. 15 Sept. 2009. \n<http://www.jfsowa.com/pubs/semnet.htm>. \n \n3.10. Scalable Dialog Authoring \nBaylor Wetzel, Shikigami Games \nbaylorw@ShikigamiGames.com \nIt has been a goal of many a game to create a large city filled with people you can talk to. \nNot an inn or castle or a small town, but a city. A big city filled with hundreds (or \nthousands, or more!) of agents, each of which acts like an individual. But there‘s a reason \nwe fill shopping malls with zombies and countrysides with monsters but not cities with \npeople—creating hundreds of people, each with their own personality, takes a lot of time…a \ncost-prohibitively long time. There won‘t be games with large spaces truly filled with \nintelligent, conversational non-player characters (NPCs) until we find a way to create these \n",
      "content_length": 2466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": " \n \nagents more efficiently. Although the techniques in this gem don‘t try to tackle every \nproblem and bottleneck that you‘ll encounter in building a dialog system, hopefully they will \nhelp you create large groups of agents much faster. \nConversational Agents Today \nGames are filled with characters that talk. Although not every game and every character \nneeds to be able to answer questions or carry on a conversation, conversational ability is \nimportant to a wide variety of games. Dialog varies from the ―select a topic‖ approach used \nin the Elder Scroll series (where one scenario involves convincing a love-addled stalker to \ngive back the item he stole from a woman who would not go out with him), to the deep \nconversational trees of the original Fallout, to the multi-way conversations of Planescape: \nTorment, to the jury trials in Jade Empire. NPCs in these games are normally more complex \nthan NPCs in other games. They might refuse to discuss a given topic with someone they \ndon‘t know, ignore someone they previously argued with, insult someone from a rival \ngroup, or yell at someone trying to strike up a conversation in the ladies‘ bathroom. \nConversations can lead characters to give up their evil plans, join the player‘s team, or \nreveal the secret of their miniature giant space hamster. \nThe work described here is part of research done at Alelo, which makes ―serious games‖ \ndesigned to teach foreign languages and cultures. Many (though not all) of these games are \nused to train soldiers how to perform tasks overseas. These tasks range from manning \ncheckpoints and conducting house-to-house searches to negotiating with local leaders and \nhelping set up clinics. Success often depends on showing the proper level of politeness and \nprofessionalism, earning trust through culturally appropriate small talk and asking the \ncorrect questions in the correct way. In this gem, we‘ll use the example of a U.S. soldier in \nIraq to explain the techniques. \nTypical Methods for Building Conversational Agents \nThere are a few ways to make conversational agents, one of the more common (and \npainful) ways being to build them manually in script (if (1==option) \nbobDialog42() else…). An easier approach is to use a dialog editor to build a tree, \nwhere one node is what the NPC says, the nodes under that are things the player can say in \nresponse, the nodes under those are the NPC‘s response, and so on. Each node typically \ncontains the exact text the agent will say. You say, ―Do you like football?‖ and the NPC will \nreply ―Sure, who doesn‘t?‖ An NPC might have several responses based on whether they \nlike you, if you have fulfilled a quest for them, if you are both at a bar, and so on. In \nNeverwinter Nights, this is done by calling the TextAppearsWhen script, and in the Elder \nScrolls editors (including Fallout 3 ‘s G.E.C.K. editor), it‘s done by checking the \nConditions field, but the idea is the same—for every possible dialog option, the designer \nwrites the input text (player choice), the output text (from the NPC), and for each possible \noutput writes a script (either by hand in NWN or using a spreadsheet-like tool in Elder \nScrolls) to determine whether that particular output should be used. If not, the game \nchecks the next output in the list. The results can be very good, but it takes a lot of time, \nthought, and planning to build. \nThe Scalability Problem \nLet‘s start with a positive—the range of dialog that can be created by current techniques is \nessentially perfect. If you want an NPC‘s response to change based on the player‘s shoes, \nintelligence, the last enemy they fought, the health of the NPC‘s dog, and the phase of the \nmoon, you can do that. The problem is that it‘s going to take you a long time. \nThis isn‘t the only problem. Because of the sheer volume of data, designers face the \nproblem of covering the whole possibility space—you handled a lot of the possible variable \ncombinations, but did you get all of them? In Fallout, NPCs asked about quests that have \nlong since been completed. In Mass Effect, the person sitting next to you will calmly inform \n",
      "content_length": 4131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": " \n \nyou that they‘ve picked up a communications signal right rather than ask why you‘ve just \ndriven off a bridge into a bottomless chasm. In Neverwinter Nights, you can rescue a girl \nfrom a giant, go to the girl‘s farm, kill her family, then talk to her, and she‘ll thank you and \nask you to visit again soon. \nFinding and correcting problems like this isn‘t hard, it just takes time. How much time? To \nbuild a professional dialog tree, you not only have to decide which topics an agent can \ndiscuss, the words to use, and the flow of the conversation, you need to think about all the \nfactors (NPC personality, NPC culture, NPC state, world state, player state, conversational \nhistory, history with player, and so on) that should affect a conversation and make sure the \nNPC reacts appropriately. For one of the games I worked on in 2009, creating the full dialog \ntree for an important NPC took two to three weeks. Even then, the agent had the standard \nlapses in awareness and limited conversational ability that you find in any game. That \nparticular game was a bit more complex than most, but creating a decent conversational \nagent in any game is still measured in days and weeks, not minutes or hours. \nThe required effort influences how games are made. It would take a small team of designers \nworking on nothing but conversations a full year to make 100 individual (non-cloned) NPCs, \nand those NPCs would still suffer from limited conversational ability and situational \nawareness. Assuming one wishes to at least break even on their game, it is essentially \nimpossible to make large worlds filled with unique, believable agents using current \ntechniques. As a result, time and money force those worlds to be filled with a handful of \nhigh-quality NPCs (those that drive the plot) and dozens to hundreds of generic NPCs with \nno or almost no conversational abilities at all. \nUnique Personalities and Other Things We Might Want \nOur primary goal is to reduce the time it takes to create a conversational agent. The \nassociated goal is to reduce the cost to create a single agent, allowing us to reduce the cost \nof making the game or to create significantly more agents for the same cost. (This gem \nfocuses on the latter.) \nWe have already said we want agents with better conversational breadth and situational \nawareness. (In other words, responses are based on the NPC‘s personality, state, job, \nculture, feelings toward the player, and so on.) Another desirable trait is realistic \nuniqueness—characters in the game are roughly as diverse as people in the real world. \nWhich highlights the problems of a common technique—making a few high-quality NPCs (or \ndialog trees) and cloning them. Using templates (―Hi, my name is %this.name; I live here in \n%this.city‖), you could fill a world with hundreds of agents who knew some basic \ninformation about themselves but who all acted the same (or behaved like one of a handful \nof personality types). What we want are people who are realistically unique—based on who \nthey are, two agents will give different answers when it makes sense and the same answer \nwhen it makes sense. \nAnother desirable feature (already present in some games) is for the player to be able to \nchange an agent‘s attitude and behavior toward them. Scenarios often require the player to \nearn the trust of an NPC. Likewise, bad behavior on the player‘s part should have \nconsequences. Being able to win an agent‘s trust is often the key to a mission, and being \nable to make someone hopping mad is simply fun. \nCulture describes how a group of people behave in certain circumstances. For example, it \nmight be considered rude to ask an Afghani man about his wife, refuse a cup of tea in Iraq, \nor ask a first-level character about their flying mount. If you‘re dealing with a large number \nof cultures (groups, roles, character types, and so on), the sheer volume of dialog data \nmakes it hard to verify that agents behave consistently or behave the way the lead designer \nrequested. For serious games, where the behavior often has to be evaluated by an \neducational expert and/or people from the culture being modeled, unless those people are \nalso game programmers, this is a serious problem. \n",
      "content_length": 4235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": " \n \nFormat is also a problem. All of the behavioral information can be captured in the standard \nscript and tree structure of most dialog systems, but if the knowledge is explicit (say, a \nspreadsheet that focuses on behavior rather than wording), it‘s easier for an expert to \nreview (and author) the information. It‘s much harder to bring in a group of people from \nthat culture and ask them to review the information if the information is scattered across \nhundreds of script files. So another desirable feature is the ability to explicitly describe a \nculture or group of people. \nA benefit of an explicit cultural representation is that it allows another desirable feature—\nplug-and-play cultures. If the culture of the NPCs could be swapped out with other cultures, \nmaking a new city filled with conversational agents would be as easy as cloning an existing \ncity and swapping the culture, which meets both the goal of being fast (and cheap) and the \ngoal of the NPCs being realistically different. \nEditing dialog must entail an easy-to-understand workflow (not requiring a Ph.D. to use it). \nIt needs to be data driven in a way that makes it quick to create easy-to-use tools, as well \nas quick to write unit tests for the system. \nA final thing is something we don‘t want—the tool should not preclude designers from being \nable to do things they can do now. An example is a tool that uses psychological data to \ngenerate realistic behavior but doesn‘t allow the designer to override that behavior. While \nrealism is often nice, it is more important that designers be able to achieve the behavior \nthey want. In entertainment games, realism must sometimes be sacrificed to fun or moving \nthe plot along. In educational games, characters must sometimes do things to further the \neducational goal, such as a character correcting rather than overlooking an error, or leading \nthe player to the correct behavior rather than harshly punishing them. \nWhat We Won’t Cover \nIt likely comes as no surprise that a gem of this length will not cover every aspect of \nconversational agents. The focus of this gem is on intention planning, which means deciding \nhow you want to respond to a topic. We‘ll use topics, concept trees, response types, trust \nlevels, rapport modifiers, temperament stats, explicitly modeled cultural groups, sets of \nsparse culture wrappers, and a bit of memory to help decide when we should answer a \nquestion, feign ignorance, or insult the speaker‘s mother. \nWhat this gem doesn‘t cover is realization, the actual words that come out of the NPC‘s \nmouth. In the old days, this was a simple problem—if the designer decides the NPC should \ninsult the player, the response type Insult would map to one or more insults. If the NPC‘s \nintent is Answer, the response type and topic can be used to look up a specific answer, \nwhich might be specific to that character or used by the entire world. Using the techniques \npresented here, if the designer decides halfway through the project that all 300 guards in \nthe game need to be able to discuss bunnies or Pre-Raphaelite poetry with complete \nstrangers (but still act stuffy around people they actively dislike), the change can be made \nin a few minutes or less. \nBeing able to add entirely new responses or whole topics to hundreds of NPCs in a matter of \nminutes is a nice feature and offers all sorts of dreams of large, expansive dialog-filled \nworlds. Unfortunately, these days, things are a little more difficult. Most high-end games \nnow use voice actors, meaning each statement a designer adds to an NPC must be \nrecorded, a slow and expensive process. In these situations, voice recording, rather than \ndesigner creativity or predicting variable combinations, becomes the bottleneck. The topic of \nfaster, cheaper speech won‘t be covered in this gem, but it is worth noting that even when \nthe set of lines an NPC can say are fixed, considerable time must still be spent mapping the \nplayer‘s input to the NPC‘s output. The techniques presented here can help you more \nintelligently (and quickly) build those mappings. \n \n",
      "content_length": 4102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": " \n \nOverview \nThe goal of this gem is to describe a way to scale how one authors conversational agents. \nCurrent systems typically use hard-coded input-output mappings annotated with gateway \nscripts to decide which of the hard-coded responses to use. The system described here uses \na variety of techniques, but at the core, it tries to break the hard-coded links and replace \nthem with abstractions. Rather than linking the player‘s input directly to the NPC‘s output, \nwe use the player‘s input to determine the NPC‘s intention and then use the intention to \nselect the NPC‘s behavior. \nAll inputs are mapped to a Topic. The Topic is checked against the NPC‘s \nCulturalGroup and current level of Trust toward the player to determine a \nResponseType. Topics belong to a topic hierarchy, so if there is no match on the Topic \nin the NPC‘s dialog specification, the system moves up a level and checks for a \nResponseType to the parent Topic. \nCulturalGroup is a sparse set of {Trust-Topic-ResponseType} mappings. \nCulture represents not just nationality, but any group membership that affects how the \nagent will respond to a topic. An agent can (and almost certainly will) belong to multiple \ngroups. Groups are prioritized, and conflicts are resolved by selecting the highest-priority \nmatching group. \nThroughout this gem, we‘ll use the example of a U.S. soldier (the player) in an Iraqi city. \nTable 3.10.1 lists seven NPCs the player might interact with—a typical civilian, a policeman, \na policeman secretly working for the insurgents, another U.S. soldier, an insurgent \n(although the player doesn‘t know this), and two doctors. \nTable 3.10.1. NPCs in an Iraqi City \nName: Nori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nBio: \nIraqi Man Policeman Policeman U.S. \nSoldier \nInsurgent Doctor \nDoctor \nRoles: Iraqi \nPoliceman Zuhair \nScott \nSoccerFan AidWorker \nGovRep \n  \nSoccerFan GovRep \nGovRep \nSoldier \nInsurgent GovRep \nAidWorker \n  \nPerson \nIraqi \nInsurgent USCitizen \nIraqi \nIraqiFemale IraqiFemale \n  \n  \nPerson \nPoliceman Person \nPerson \nIraqi \nIraqi \n  \n  \n  \nPerson \n  \n  \nPerson \nPerson \nTrust: 0 \n0 \n–2 \n2 \n–6 \n0 \n0 \n \n \n \nIntention Modeling \n",
      "content_length": 2162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": " \n \nImagine a game in which there are 100 NPCs, and the player can insult any of them. How \nwill they react? Most will return the insult in a dozen different ways, based on their \npersonality, intelligence, culture, and preferred insult. Many will ignore the player. A few will \nattack. There are potentially 100 actual actions or phrases that might be used, but (in our \nexample) there are only three intentions (insult, ignore, attack). In most games, the \nplayer‘s action (the input) is hard-coded directly to the NPC‘s behavior (the output). The \nresult is that the designer must write thousands of pairings, such as {input:Hear(―Do \nyou like films about gladiators?‖), output:Say(―Get away from \nme, weirdo.‖)}. Much of this work is redundant—the same output is used for multiple \ninputs, and the same pairings are used in dozens of NPCs. If it is decided that this behavior \nis no longer desired (say, if designers decide late in the process that elves, unlike dwarves \nand humans, never insult others), the dialog pairings must be tracked down across dozens \nor hundreds of scripts or dialog files and changed. \nDoing duplicate work is not only inefficient and hard to maintain, it‘s not fun. To make the \ndesigner‘s life easier, we‘ll have them map inputs to intentions (a much easier task) and \nseparately map intentions to behaviors. \nTopics, Response Types, and Trust \nIn our approach, for a given culture (which we‘ll discuss a little later), a Topic and Trust \nlevel are used to select ResponseType. Topic is the subject the player is asking about \n(swords, rumors, dragons, and so on). In this gem, we‘ll assume that Topic is the only \ninput. While this is sufficient for most current video games, more demanding games will \nlikely use a more complex input consisting of an action (asking a question, demanding, \ngreeting, complimenting, insulting, and so on), a topic, and possibly some metadata (vigor, \npoliteness, and so on). The type of input is irrelevant to the rest of the system, so we‘ll \nkeep things simple and assume Topic is the sole input. \nThe ResponseType represents the responding NPC‘s intention. Possible values include \nAnswer (give the player the information they‘re looking for, if possible), Refuse, Evade, \nChangeTopic, VagueAnswer, Lie, Ignore, Insult, Threaten, Correct (if the \nplayer has made a mistake in what they asked for; this is more useful in educational \ngames), PositiveLie (say something is great, regardless of whether it is), \nNegativeLie, and Custom (e.g., attack). More complicated (and academically \nrespectable) schemes exist, but this works well for our purposes. \nThe actual words spoken by the NPC are based on the ResponseType. Suppose the player \nhas been told that a bomb has been placed in the market. The player stops a person on the \nstreet and asks for directions. Table 3.10.2 shows how the NPCs defined in Table 3.10.1 \nmight react. Five of the seven will attempt to answer the question (although one, Scott, \ndoes not know the answer). Shakir, an insurgent, will insult the player, while Zuhair, a \ncorrupt cop, will lie to the player, sending him away from the market. (Note that the \npoliceman does not necessarily know there is a bomb and wants it to explode, he merely \ndislikes the player; if the policeman knew about the bomb, he could be made to respond \ndifferently using a context modifier, but that is outside of the scope of this gem.) \nTable 3.10.2. What an NPC Says Depends on Their Intention \nPlayer asks: Where is the market? \nlying \nName: Nori \nAnwar \nZuhair Scott \nShakir \nSuha \nHalema \nRole: \nIraqi \nIraqi \nIraqi \nUSCitizen Insurgent Iraqi \nIraqi \n",
      "content_length": 3637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": " \n \nTable 3.10.2. What an NPC Says Depends on Their Intention \nPlayer asks: Where is the market? \nlying \nIntent: Answer Answer Lie \nAnswer \nInsult \nAnswer Answer \nSay: \nLeft \nLeft \nRight Not sure Pig \nLeft \nLeft \n \nTrust is the amount of trust the NPC has in the player. It‘s what ties the Topic to the \nResponseType for that cultural group. This attribute does not have to be Trust— it \ncould be rapport or some combination of other attributes, although trust as a single value \nworks well in most instances. The important thing is that there is an attitudinal value that \nunambiguously ties an input to a ResponseType. \nUsing the example in Table 3.10.1, let‘s assume that the player has insulted Anwar the \npoliceman. Let‘s measure trust from –10 (distrust) to 10 (full trust). Topic=insult, \nCulture=Police, and Trust=0. We have the rules (evaluated in order): \n{Trust >=  5, Ignore} \n{Trust >=  0, Insult} \n{Trust >= -7, Threaten} \n{Trust <  -7, Custom:Attack} \n \nWhen the player insults Anwar, Anwar will decide to insult the player. Assuming an insult \nlowers Trust by 1, if the player insults Anwar again, Anwar will threaten him. If the player \nkeeps it up, Anwar will eventually attack. \nKnowing that an NPC will insult the player does not automatically determine what the NPC \nwill do. The behavior generation system might be as simple as mapping \nResponseType=Insult to Say(―Oh yeah, your momma.‖). The intent is mapped \nat the group level (all policemen), but the behavior could be different for each individual \npoliceman. Each NPC could have his own favorite insult. Insults could be chosen based on \nthat NPC‘s intelligence. They could be based on the player‘s class, how they‘re dressed, or \ntheir location (sports arena, store, and so on). This decision is made independent of the \nintention system. \nSeparating intention from behavior has several important implications. First, separate \ndesigners can be assigned to intent (say, someone familiar with personality or social \npsychology) and realization (for example, a writer). Second, because it‘s a smaller set of \ndata and explicit in its goals, it is easier for one person to view and correct the data \n(important when striving for consistency across agents and designers). Third, the smaller \nset of options (which presumably will be chosen from a list rather than entered as free text) \nmeans the intent portion can be built faster and more easily. By removing duplicate data (in \nthe behavior system, you only have to map behaviors to a small set of intents, not the \nmuch larger set of inputs), the overall amount of work should be reduced. Fourth, it makes \nit easier for designers to tweak dialog later in the development process without editing (and \npossibly adding bugs to) individual NPC dialog trees. Fifth, having a separate intention layer \nmakes it easier to write unit tests, in part because there‘s less data to test and in part \nbecause the tests aren‘t dependent on free text (important both because text is often \nchanged and because of internationalization). Sixth, the explicit ResponseType and \nTrust levels help designers remember which conditions they need to handle. (Note that \n",
      "content_length": 3177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": " \n \nthis is not required: One can create a group Person that returns Answer for all topics at \nall Trust levels.) \nA final, and important, reason why separating intention from behavior is important is \nbecause it allows for design by composition, as seen in the next section. \nConcept Hierarchies \nTo enhance conversational breadth, topics belong to a topic hierarchy. If the player asked \nabout murders and the NPC‘s group didn‘t have an entry for Murder, the system would \ncheck the parent topic (say, Problems). If that was missing, it would check the next level \nuntil it had reached the root topic. There are several advantages to this, but two are worth \nmentioning. First, by placing a ResponseType on the root node, the agent has a default \nanswer to anything the player asks. This helps cover errors when a mapping has been \nforgotten. Second, it allows a designer to add new topics through extension, which is \nnormally lower risk than edits. If the designer decides that some characters need special \nbehaviors when discussing Pre-Raphaelite poetry, they can add it to the topic hierarchy. The \ngroups configured to discuss obscure Victorian poetry will do so, and everyone else will \nrespond to the general topic of poetry, writing, or something more general. \nIt should also be noted that the {Topic-Trust-ResponseType} mappings do not have \nto be completely specified. An NPC can be set up to talk about local crime when Trust is \neight or better and have no other mapping for that topic. If Trust was below that value, \nthe system would then use the parent topic. This comes in handy when a person belongs to \nmultiple groups, as described in the next section. \n \nCultural Wrappers \nWhen designers specify intent, they do so at the group level, not for individual NPCs. We \nwill refer to these groups as CulturalGroups. A CulturalGroup can represent race, \nnationality, occupation, political affiliation, or any group membership that affects how one \nreacts to something. Examples include Thai, Rural, Soldier, FootballFan, \nPunkRocker, AngryLoner, and Parent. Designers can also use the cultural group \nconcept to model personality traits, such as Paranoid and Bully. \nTable 3.10.3. Questions about Football Can Be Answered as Questions about Either \nFootball or Sports \nDo you like football? rapport building, concept abstraction \n  \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nTopic: \nFootball \nSports \nSports \nSports \nFootball \nSports \nSports \nRole: \nSoccerFan \nIraqi \nIraqi \nUSCitizen \nSoccerFan \nIraqi \nIraqi \nIntent: \nAnswer \nAnswer \nAnswer \nAnswer \nEvasive \nAnswer \nAnswer \nSay: \nYes \nYes \nYes \nNo \nMaybe \nNot really Yes \n",
      "content_length": 2658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": " \n \nTable 3.10.3. Questions about Football Can Be Answered as Questions about Either \nFootball or Sports \nDo you like football? rapport building, concept abstraction \n  \nTrust: \n+1 \n+1 \n+1 \n+1 \n+1 \n+1 \n+1 \n \nTable 3.10.4. U.S. and Iraqi Cultures Differ in Their Willingness to \nDiscuss Their Spouse with a Stranger \nTell me about your spouse. \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nTopic: \nSpouse \nSpouse \nSpouse \nSpouse \nSpouse \nSpouse Spouse \nRole: \nIraqi \nIraqi \nIraqi \nUSCitizen \nIraqi \nIraqi \nIraqi \nIntent: \nRefuse \nRefuse \nRefuse \nAnswer \nInsult \nRefuse Refuse \nSay: \nHow rude! \nI refuse \nNo \nChevy‘s nice \nYou‘re a \npig! \nNo \nNo \nTrust: \n–1 \n–1 \n–1 \n+1 \n–1 \n–1 \n–1 \n \nA cultural group contains one or more {Topic-Trust-ResponseType} mapping. These \nmappings are typically sparse—most doctors have a predictable reaction to medical \nquestions but not to questions about books, movies, or enchanted swords. \nAgents belong to one or more groups. Typically, one of those groups will be Person, which \nwill contain default mappings. Other groups specialize the agent. Some, such as Iraqi, will \nbe fairly broad and dense, containing a lot of mappings, while others, such as \nFootballFan, will be small and focused. The agent can have an unlimited number of \ngroups. \nWhen designing agents, two design principles are used: design by composition and design \nby exception. \nDesign by composition says that the designer should build the agent by selecting pieces \n(cultural groups) rather than writing the agent from scratch. The process is simple and \nfast—Agent A is a Doctor, GovernmentRepresentative, Iraqi, IraqiFemale, \nand Person, and Agent B is a GovernmentRepresentative, Insurgent, \nPoliceman, Iraqi, and Person. It takes only a few seconds to select the groups from a \nlist, and assigning the groups fully specifies how the agents will react to any dialog option in \nthe game. (Note that if per-agent behavior is used, that work will still need to be done, \nalthough it should still be less work than in a traditional system.) Design by composition \nspeeds up the authoring process for a single agent. \n",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": " \n \nDesign by exception speeds up the group authoring process. Following the principle of \ndesign by exception, default values should be set up in the base group (in our example, \nPerson), and only those values that differ from the default should be placed in new \ngroups. For example, you could have the group LittleGirl love to talk to complete \nstrangers about any type of animal, the group DemonEnthusiast love to discuss \ndemons, and the group RabbitPhobe be too terrified to discuss rabbits with anyone but \ntheir closest friends. Assigning those three groups to an agent produces an agent that will \ngladly talk about animals and demons yet refuse to discuss bunnies. The group \nRabbitPhobe does not contain mappings for any Topic other than Rabbit, making it \nfast to create, and the designer is not forced to create hundreds of combination groups, \nsuch as PeopleWhoLoveAnimalsAndDemonsButNotRabbits. \nAlthough it won‘t be frequent, conflicts between CulturalGroups can occur. Consider a \ndoctor who runs a government clinic in a war zone. Although the clinic is trying its best, \nthere are still rampant health problems in the area. If you ask the agent about the \nproblems, the Doctor in her wants to complain that they aren‘t doing enough, while the \nGovernmentRepresentative in her wants to say that the clinic is doing just fine. \nThe dialog system we describe here works more generally for any problem of determining \nan agent‘s reaction to an event. How one handles the cultural group conflicts depends on \nthe domain. For the dialog system, we decided to use a first-chance event handler with a \nprioritized group list (referred to as Cultural Wrappers). When setting up the agent, the \ndesigner must select the order of the groups. In Table 3.10.1, Suha has Doctor prioritized \nover GovernmentRepresentative, while Halema has the same groups but in a \ndifferent order. When asked about problems (Table 3.10.5), Suha complains about \nhealthcare, while Halema says there are no problems. \nTable 3.10.5. An NPC’s Answer Is Based on the Order (Prioritization) of Their \nGroups \nAre there any problems here? \n  \n  \n  \nName: \nNori \nAnwar \nZuhair \nScott \nShakir \nSuha \nHalema \nRole: \nIraqi \nIraqi \nGovRep \nUSCitizen \nInsurgent \nIraqi \nGovRep \nIntent: Answer Answer Deny \nAnswer \nInsult \nAnswer \nDeny \nSay: \nYes \nCrime \nIt‘s very safe I don‘t know You! \nHealthcare \nNo \n \nIn a very small number of cases, there is no acceptable ordering of groups—sometimes \nGroup A supersedes Group B, and other times B supersedes A. As an example, in Table \n3.10.1, Zuhair is both a policeman and (secretly) an insurgent. He wants to help the \nterrorists, but not at the risk of blowing his cover. He might voice support for the terrorists \naround people he trusts (as a terrorist) but be polite to the player (as a policeman). In \nthese instances, a simple solution is to create a new group that contains only those Topics \nand Trust levels needed to resolve conflicts. This new group might be an actual general-\npurpose group, such as Undercover-Insurgent, but it can also represent that specific \nindividual (in this case, Zuhair). Individuals have their personal quirks that can‘t be \ncaptured by any group, so modeling the things that are truly specific to an individual is \nokay, but the ―individual group‖ should only contain the exceptions. Unless the agent is \n",
      "content_length": 3365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": " \n \ntruly, eccentrically unique, most of his responses should be specified in the more general \ngroups. \nEarlier it was mentioned that Topics belong to a topic hierarchy. When determining the \nagent‘s intent, if a match on the Topic isn‘t found, the parent Topic is used, moving up \nthe tree until a match is found. When multiple groups are used, preference is given to \nTopic specificity. Consider the Topic TableTennis, child of Sports, and the ordered \ngroup list [USCitizen, PingPongFan]. USCitizen does not have a mapping for \nTableTennis but does for Sports, while PingPongFan matches on TableTennis. \nAssuming a Trust level of 0, the program first checks for a match on {USCitizen, \nTableTennis, 0} and fails to find a match. It then checks {PingPongFan, \nTableTennis, 0}, where it finds a match. Had it failed, it would have then checked \n{USCitizen, Sports, 0} and then {PingPongFan, Sports, 0}. This gives \ngreater freedom in arranging groups and allows for a greater number of groups to be used. \nIf the system moved up the topic heirarchy before checking the next group, any group with \na value high in the tree (for example, at the root node, which matches everything) would \nprevent any other group from having an influence. \n \nCreating Unique Individuals \nEach combination (and ordering) of groups results in an individual who is unique. It does \nnot mean that he will behave differently than every other agent under all circumstances at \nall times. We could design them to do so, but the agents wouldn‘t appear realistic, they‘d \nappear insane. It doesn‘t matter whether someone loves kittens, is a doctor, or grew up in a \nsmall town; if you ask him whether a particular neighborhood is dangerous or whether a \ngiven restaurant is good, there are only a limited number of responses you should get. \nResponding to the question ―where is the train station‖ by juggling cats might be unique, \nbut it isn‘t helpful. That said, there‘s nothing preventing the designer from adding that \nreaction. \nThe number of unique individuals you can create by combining groups grows quickly with \nthe number of groups. With three groups, 15 unique individuals can be made. With five \ngroups, the number is 325. With 10 groups, the number is 9,864,100, a number larger than \nmost cities in the world. Add one more group, and you can cover all cities and most \ncountries. \nThe process of assigning and ordering groups is not the only way to create unique NPCs. \nIntent can be tweaked at the NPC level using the TrustModifier property. This \nrepresents how trusting someone is and how quick they are to change their trust level. It is \nmultiplied against Trust to produce a modified trust score used in Trust checks. The \ndefault TrustModifier is 1. An agent with a TrustModifier of 1.5 is 50 percent \nmore trusting than normal—the agent needs only a Trust of four to trigger responses that \nother agents with the same group list require a six for. \nCircumstantial modifiers can be used to modify Trust scores. For example, if the agent \nhas been arrested or has a gun pointed at him, the threshold for giving an answer could be \nlowered. It seems likely that this would need to vary by NPC somehow (perhaps by a \nwillpower property). \nAlthough beyond the scope of this article, uniqueness can also be created in the behavior \ngeneration system. A single {Topic, ResponseType} pairing (where Topic can be a \nwildcard when topic is irrelevant, such as when insulting or ignoring the player) can map to \na set of realizations, one of which is selected at random (preferably using an intelligent \nrandom system that filters out long repeated sequences). Behaviors can also be chosen \n",
      "content_length": 3678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": " \n \nbased on attributes of the agent. For example, if the ResponseType was Compliment, \nan agent with a high intelligence or charisma might say something clever, while someone \nwith low intelligence might stumble badly and say something offensive. \n \nConclusion \nOne of the biggest obstacles to creating games filled with hundreds of intelligent, \nconversational agents is the sheer amount of work (and therefore cost) required to create \nthem. It‘s not that it‘s hard work (although designing interesting characters and dialog can \ncertainly be difficult); it‘s that any kind of work done several hundred times is a lot of work. \nAnd sometimes, quantity is as important as quality—you can‘t make a living, breathing, \nrealistic city with just three characters. One of the keys to improving AI is to improve its \nauthoring scalability; there needs to be processes and tools that make it easier to populate \nvirtual worlds. Hopefully, the ideas presented in this gem will be a big step toward helping \nyou fill your own worlds with intelligent, interesting, unique characters. \n \n3.11. Graph-Based Data Mining for Player Trace Analysis in \nMMORPGs \nNikhil S. Ketkar and G. Michael Youngblood \nIn this gem we will present techniques for analyzing player trace data in massively \nmultiplayer role-playing games (MMORPGs). As MMORPGs become increasingly popular, \nwith the number of subscribers going into the millions, an MMORPG provider is faced with a \nnumber of technical and business questions. For instance, how do you place an \nadvertisement in MMORPGs, or how do you detect cheating in the form of bots and gold \nfarmers? We observe that these and a lot of other such questions can be answered by \nanalyzing player traces (graph representations of the player‘s movement in the world), but \nmost traditional approaches in machine learning and data mining fall short when applied to \nthe task due to the inherent structural nature of player trace data. We claim that the \napplication of techniques in the area of graph-based data mining that are designed to work \nwith structured data are most suitable for the analysis of player traces. \nData Logging and Preprocessing \nTypically, player movements in the world can be logged as the location of the player in 3D \nspace at discrete intervals in time. Thus, the logged data for a single player is a sequence of \nthe form {(x0, y0, z0, t0),(x2, y2, z2, t2)...}. The player spawns in the world at time t0 (which \nis 0) and (x0, y0, z0) refers to the position where the player spawns. Subsequently, the \nplayer moves in the world, and assuming that we are logging data at every second, (x1, y1, \nz1, t1) refers to the position the player is at t1 = 1. Similarly, we have a number of positions \nwith the corresponding time for the player movements until the player exits the world. \nWe refer to this sequence as a walk. Our overall dataset consists of a set of such walks, \nwhere each walk corresponds to one session of a single player in the game. Figure 3.11.1 \npresents a visual representation of three walks. This data was collected in a world shown in \nFigures 3.11.2 and 3.11.3. This world is a part of the Urban MMO Testbed (UMMOT). \nUMMOT is an experimental environment designed to study human interactions in virtual \nworlds and is an extension of the Urban Combat Testbed [Cook07, Youngblood08]. \nFigure 3.11.1. A visual representation of three walks in a world. \n",
      "content_length": 3420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": " \n \n \n \nFigure 3.11.2. Urban MMO Testbed: Birds-eye view. \n \n \nFigure 3.11.3. Urban MMO Testbed: Screenshot. \n",
      "content_length": 110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": " \n \n \n \nWhile data in such raw form is easy to log, for the purposes of analysis it needs to be \npreprocessed to a more suitable form. Dealing with data at this level of granularity \n(locations in 3D coordinate space) is computationally intensive and leads to poor results. \nHence, we convert the 3D locations to a discrete form by superimposing a grid on the world. \nFigure 3.11.4 illustrates this process. Once such a grid is superimposed, all points inside one \ngrid cube are assigned to the same discrete location. Once data is preprocessed in this \nmanner, it consists of a set of walks of the form W = {(l1, t1), (l2, t2)...}, which is a \nsequence of discrete grid locations for the corresponding time instance. \nFigure 3.11.4. Superimposing a grid to get discrete locations. \n \nSelecting a proper granularity for the grid (the size of a single cube) is important. Too small \na cube size will lead to too many locations, and too large will lead to too few locations. It is \nrecommended that the cube size be equal to, or a small multiple of, the bounding box of the \ncharacter model. \n \nAdvertisement Placement in MMORPGs \n",
      "content_length": 1129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": " \n \nPlacing advertisements in MMORPGs can provide host companies with an additional source \nof revenue. However, in order to capitalize on this business opportunity, companies hosting \nthe MMORPGs need to provide coverage guarantees to their clients who would pay for \nplacing advertisements. The notion of advertisement coverage is central to all marketing \nand is loosely defined as the estimated number of prospects reached by an advertisement. \nA lot of value is placed on advertisement coverage, as the clients placing the advertisement \nare solely interested in reaching as many prospects as possible and would be willing to pay \na higher amount of money for higher coverage. An advertisement in the New York Times \ncosts significantly more than an advertisement in the Charlotte Observer precisely because \nan advertisement in the New York Times will achieve higher coverage. \nIn the case of MMORPGs, estimating and providing guarantees on coverage is challenging \nfor a number of reasons. Players typically spawn in different locations, moving around the \nworld performing tasks, and this is quite different from a reader reading a newspaper or \nvisiting a webpage. Advertisements can be placed in different locations in the world, but \nwhere should they be placed? Furthermore, can some guarantees on coverage be provided? \nGiven a set of preprocessed walks, as described in the previous section, advertisements \ncould be placed at any of the locations in the walks. We define coverage for a set of \nlocations as the number of walks that contain the specified locations divided by the total \nnumber of walks. Intuitively, coverage captures the number of players that will get quite \nclose to and most likely see an advertisement. Note that it is quite possible, although \nunlikely, that a player will get close to a location but not see the advertisement because he \nor she is looking in a different direction. In our setting, we do not explicitly model where the \nplayer is looking. Such an approach has been tried before and has been found to be quite \ncomputationally expensive [Dixit08]. \nFigure 3.11.5 illustrates examples of how the coverage is computed. Note that there are \nfour distinct walks, as illustrated in the large graph on the left. Subfigures (a),(b), (c), and \n(d) illustrate four different location selections for this graph. For location selection as \nillustrated in Subfigure (a), two locations are selected, which cover Walks 2, 3, and 4. \nBecause there are a total of four walks, this amounts to coverage of 75 percent. For (b), \ntwo locations are selected, which covers all four walks, which amounts to coverage of 100 \npercent. Similarly for (c) and (d) we have 50-percent and 100-percent coverage, \nrespectively. \nFigure 3.11.5. Coverage computation. \n \nIn such a setting, our task is to maximize the coverage while minimizing the number of \nadvertisements placed. This task is equivalent to the set-cover problem, which is NP-\n",
      "content_length": 2964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": " \n \nComplete. Thus, optimal solutions are not feasible, and it is necessary to develop \napproaches that can produce near-optimal solutions at moderate computation cost. \nAn important, additional dimension in the task of advertisement placement is that of \ngeneralization to future player behavior (in terms of walks). Given a certain amount of \ntraining data, suppose that we select a set of locations that maximize coverage on this data. \nThen, our selected set of locations should achieve the required level of coverage on future \nwalks. Assuming that training data is a good reflection of the entire population, maximizing \ncoverage on training data will most likely achieve high coverage on unseen data. The \nimportant question here is how large of a sample is required to get a good generalization on \nunseen data. \nAnother factor to consider is the cost of collecting and logging data. Clearly, in order to have \ntraining sets on which to base advertisement placement, some data needs to be collected. \nThe most expensive case is where actual positions (3D locations) are logged. The space \nrequired for such logging grows linearly with the number of walks and may not be feasible. \nAnother approach is to simply log the number of players that visit a particular location, \nwhich is constant in the number of walks. There is an inherent tradeoff in the amount of \nlogged data and the quality of the solution. \nWe now present a set of approaches for these tasks and discuss their strengths and \nweaknesses. \nFrequency Maximizing Approach \nFrequency-based placement is a relatively simple approach that selects locations based on \nthe number of walks passing through a particular location. Walks in the training data are \nprocessed sequentially to count the number of walks passing through each location, and \nthese counts are used to select the topmost locations. Note that this approach only requires \nthe logging of frequencies of visits to each position, which requires space constant with \nrespect to the number of walks. Another important thing to note is that this approach might \nproduce suboptimal solutions in many cases, because it does not consider the overlap \nbetween walks. Figure 3.11.6 illustrates an example of such a case. Assuming that Position \nB has already been selected, the frequency maximizing approach will select Location A over \nLocation C. This is because, individually, A covers four walks and C covers two. This is \nclearly suboptimal, as there is an overlap of three walks between A and B (Walks 1, 2, and \n3). \nFigure 3.11.6. Suboptimal selection by frequency maximizing approach. \n \n \nMarkov Steady-State Probability-Based Approach \nThe Markov steady-state probability-based placement approach is based on computing the \nprobability of a player visiting a particular location based on the transition probabilities. The \nfirst step in this approach is to process the walks to generate a transition probability matrix. \nThe transition probability matrix stores probabilities of transitions from a location to any \n",
      "content_length": 3051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": " \n \nother location and is computed by counting the transitions and dividing by the total number \nof outgoing transitions. Once this matrix is generated, which we will refer to as M, the \nsteady-state probabilities can be computed by solving xM = x. There are several exact and \niterative approaches to solve such a system of linear equations. Based on the steady-state \nprobabilities, advertisements can be placed by selecting locations with the highest \nprobabilities. \nFigure 3.11.7 illustrates an example of this. Note that this approach requires the logging of \ntransitions and requires space constant with respect to the number of walks. \nFigure 3.11.7. Computation of steady-state probabilities. \n \n \nGreedy, Marginal Gain Maximizing Approach \nThis approach is based on selecting locations in a greedy manner, maximizing the marginal \ngain with each added location. Initially, the location with the highest frequency (or \ncoverage) is selected. This is followed by considering each location (not already selected) \nand evaluating the coverage of the newly formed set of locations (previously selected \nlocations with the current location). After evaluating each location in this manner, the \nlocation that maximizes the coverage is added to the set. \nThis is illustrated in Figure 3.11.8. Note that this approach considers the marginal coverage \nwhile considering a new location to add and therefore would produce solutions that are \nmore optimal as compared to the frequency-based approach. Another important point is \nthat this approach requires logging the actual walks—that is, the space required to log the \nnecessary data grows linearly with the length of the walk as well as the number of walks. \nFigure 3.11.8. Computing the marginal coverage. \n \n \nExperimental Comparison \n",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": " \n \nWe experimentally compared the three approaches to advertisement placement on a \ndataset of 2,436 walks. More details on the dataset can be found in [Cook07] and \n[Youngblood08]. Experiments were conducted for various sizes of training sets, ranging \nfrom 0.25 percent to 50 percent of the entire dataset, while the remaining data was used \nfor testing. (Advertisement placements were selected based on the walks in the training set, \nand these placements were evaluated on the test set.) \nFigure 3.11.9 shows the coverage achieved by each of the three approaches for different \nnumbers of advertisements, with 5 percent of the data used for training, on the training set. \nFigure 3.11.10 shows coverage on the test set. As a baseline, we also include a random \nplacement approach in the experimentation. Each result is an average over five runs of \ndifferent samples of training and test sets. Results indicate that the greedy marginal gain \nmaximizing approach significantly outperforms both the frequency-based and the Markov \nsteady-state probability-based approaches. The frequency-based approach is comparable to \nthe Markov steady-state probability-based approach. Similar results are observed on \nvarious other training sizes greater than 5 percent. An interesting observation is that we get \ndiminishing returns with an increased number of advertisements placed. That is, a lot of \ncoverage is achieved due to the initial advertisements, but after about seven or eight \nadvertisements are placed, there is very little improvement in coverage. \nFigure 3.11.9. Comparison of approaches to advertisement placement. Five \npercent of the data used for training, coverage on the training set. \n \n \nFigure 3.11.10. Comparison of approaches to advertisement placement. Five \npercent of the data used for training, coverage on the test set. \n",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": " \n \n \n \nThe closeness between the coverage on the training sets and the coverage on the test sets \nimplies that our advertisement placement generalizes well to unseen data. However, this is \nnot the case for very small training sizes. Figure 3.11.11 shows the coverage achieved by \neach of the three approaches for various budgets on the number of advertisements placed \nwith 0.25 percent of the data used for training, on the training set. Figure 3.11.12 shows \ncoverage on the test set. For such a small size of training set, 100-percent coverage is \nachieved for very few advertisements, but these results do not carry over to the unseen \ndata. For very small training sizes, we see that the performance of each of the three \napproaches is no better than random. The important point to take home is to have a \nsufficiently large sample size. Unfortunately, our present work does not include a theoretical \nbound on the sample size, and we advise users to partition their datasets (into training and \ntesting sets) to determine the appropriate size experimentally. In general, the size of the \ntraining set depends on the size of the world and the variability in the walks, and we are \nworking toward proving an upper bound on the training set size. \nFigure 3.11.11. Comparison of approaches to advertisement placement. 0.25 \npercent of the data used for training, coverage on the training set. \n",
      "content_length": 1398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": " \n \n \n \nFigure 3.11.12. Comparison of approaches to advertisement placement. 0.25 \npercent of the data used for training, coverage on the test set. \n \n \nEach of the three approaches generalizes well to future data given a sufficient size of the \ntraining data. In our experimentation, the greedy marginal gain maximizing approach \n",
      "content_length": 331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": " \n \nsignificantly outperforms the frequency-based and Markov steady-state probability-based \napproaches. However, it should be noted that for a large-scale implementation, the greedy \nmarginal gain maximizing approach requires the logging of the actual walks, which can \nconsume space that grows linearly with the number of walks as well as the length of walks. \nThe other approaches require the logging of visits to particular locations or transition \nprobabilities, which is constant with respect to the number of walks. \n \nBuilding Player Profiles with Clustering \nThe idea behind clustering is partitioning a given set of examples into subsets (referred to \nas clusters) such that examples in each subset are similar to other examples in the subset \nby some measure. Cluster analysis is an unsupervised learning technique that allows us to \ncategorize data such that trends in the data are identified. A good introduction to cluster \nanalysis can be found in [Jain99]. \nIn the case of player trace analysis, we are interested in building player profiles that group \nplayers into categories such that players in a group have similar behaviors. Figure 3.11.13 \nshows player traces for six different players. Although visualizing such information can lead \nto important insight, this is only possible for small datasets. When player traces become \nlong or there are too many player traces, analyzing them visually becomes a tedious \nprocess. Clustering player traces serves as an important step in analyzing traces because it \nreduces the data from individual player traces to groups of player traces. Since there are far \nfewer groups than individual traces, it becomes possible to perform a visual analysis of \nthese groups instead of individual traces. \nFigure 3.11.13. Visualizing player traces in 3D space. \n \nThe key challenge in applying clustering algorithms to player trace data is that typically \nclustering algorithms are designed for attribute-valued data (data represented as a single \ntable), and player trace data is structured and cannot be represented as a single table \nwithout losing important information. A simple example of such data can be a table \n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": " \n \nrepresenting information about customers where each row represents a customer and each \ncolumn represents a specific attribute, such as age or yearly income. \nTo produce such a grouping, a similarity measure between two examples (in this case, \ncustomers) is required. There are several distances measured for attribute-valued data—for \ninstance, Euclidean distance, which can be used to achieve good results. Contrast this \ncustomer data (represented as a table) with player trace data introduced earlier, which \ncannot be represented as a table. The notion of Euclidean distance cannot be used to \nmeasure the similarity between two player traces. This is because Euclidean distance can \nonly be used on data points represented as n-dimensional vectors of equal length, and \nwalks in the world are a sequence of points with variable length. \nTo address this difficulty, we introduce a similarity measure between two player walks in the \nworld. Using this similarity measure, any of the standard clustering algorithms can be \napplied to clustering player trace data. \nDistance Measure \nThe largest common subsequence (LCS) is used to measure the similarity between two \nwalks. An illustration of longest common subsequence can be found in Figure 3.11.14. Using \nLCS accounts for fragments of similarity between two walks. For example, suppose that \nsome walks consist of an important set of behaviors that are sequentially repeated in each \nof the walks. However, these repeating behaviors are interlaced with other actions that are \nnot common to all the walks. \nFigure 3.11.14. Longest common subsequence. \n \n \nSuch a case is illustrated in Figure 3.11.15. Here we have four walks, where Walks 1 and 2 \nhave two behaviors in common. Walks 3 and 4 also have two behaviors in common. The \nuncommon actions represent the variability or the noise in the data and should be ignored. \nWhat should be considered are the sequentially repeating, common aspects of the walks, \nwhich are in fact captured by the LCS. If we group the walks in Figure 3.11.15 based on \nLCS, we will have two groups—the first with Walks 1 and 2 and the second with Walks 3 and \n4. However, the LCS by itself does not take into account what fraction of two traces is \nsimilar. For example, in Figure 3.11.15, Walks 3 and 4 have much longer chunks of portions \nin common (with respect to the length of the entire walk) as compared to Walks 1 and 2. \nHence, Walks 3 and 4 are much more similar to each other than Walks 1 and 2. \nFigure 3.11.15. Longest common subsequence captures common behavior. \n",
      "content_length": 2572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": " \n \n \n \nTo take this into consideration, we define the similarity measure as: \n \nwhere A and B are the two walks under consideration. Note that this similarity measure is \ntypically a number between 0 and 1. Identical walks will have a similarity measure of 1, \nwhile completely dissimilar walks will have a similarity measure of 0. \nThe LCS problem is quite well studied in literature, with numerous applications in \nbioinformatics. An O(mn) time algorithm (where m and n are the lengths of the input \nsequences) for LCS can be found in [Wagner74]. \nApplying Standard Clustering Algorithms \nUsing the distance measure for player trace walks specified earlier, it is now possible to \nextend any of the standard clustering algorithms for the task of clustering player traces. The \ngeneral idea is to replace the distance computation (which is typically Euclidian distance) by \nthe distance measure based on LCS. Alternatively, for a given set of walks, we can \nprecompute a similarity matrix, which is basically a triangular matrix where each entry \nindicates the similarity between the example in the row and the example in the column \n(illustrated in Figure 3.11.16). Many clustering algorithms can operate on such a similarity \nmatrix to produce clusters that can be used for subsequent analysis. \nFigure 3.11.16. Similarity matrix. \n",
      "content_length": 1336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": " \n \n \n \nA common way to visualize clustering results is a dendrogram, which is a treelike structure \nthat depicts the similarity between the examples. We illustrate a dendrogram on the first \nfive walks (due to space limitations) in our datasets in Figure 3.11.17. \nFigure 3.11.17. Results of clustering (on a very small subset). \n \nThe overall procedure for cluster analysis is to first generate a dendrogram (source code for \ngenerating a dendrogram has been provided on CD, and more details can be found in \n[Jain99]) for the entire dataset, as depicted in Figure 3.11.17, and then focus on specific \nclusters to understand their common elements. The dendrogram is an extremely effective \ntool for visual data analysis because it allows the user to focus on specific samples in the \ndata rather than the entire dataset. The LCS procedure can also be used to produce the \ncommon subsequence thereby identifying such common elements. For example, in Figure \n3.11.17, the common element between Walks 1 and 3 is the walk up the stairs. Following \nsuch a procedure (looking at clusters and identifying common elements) can allow the \nidentification of several important behaviors. \n \nDetecting Bots and Gold Farmers with Classification Models \n",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": " \n \nThe basic task underlying the detection of bots and gold farmers is that of learning a binary \nclassification model. This task is quite well studied in the field of machine learning and is \ncommonly referred to as supervised learning. In supervised learning, we are given a set of \nexamples labeled as positive or negative (positive and negative are the two classes or \ncategories), and a supervised learning algorithm induces a function that can classify unseen \nexamples into these two categories. A good introduction to supervised learning can be \nfound in [Mitchell97]. While the supervised learning problem is quite well studied, most \nalgorithms for supervised learning only deal with attribute valued data. As mentioned \nearlier, player traces cannot be represented as attribute valued data, and hence applying \nexisting algorithms to the task can be quite challenging. \nAn important class of supervised learning algorithms is support vector machines (SVMs), \nwhich have been successfully applied to many application domains. A good introduction to \nSVMs can be found in [Cristianini00]. While SVMs also typically deal with attribute valued \ndata, they can be extended to operate with structured data by specifying a kernel function. \nA kernel function basically computes a similarity measure between two examples. The LCS-\nbased similarity measure used for clustering can also be used as a kernel function, allowing \nus to apply SVMs to classify player traces. \nUsing an LCS-Based Similarity Measure with K-NN \nWe begin by discussing the use of the LCS-based similarity measure with the K-Nearest \nNeighbor (K-NN) algorithm, which is a relatively simple classification algorithm and would \nallow the reader to develop an intuition for the task of player trace classification. The K-NN \nalgorithm is a simple lazy algorithm that stores all the input examples, and when a \nprediction is to be made on an unseen example, it first computes the K nearest neighbors \nusing some measure of similarity and predicts the class of the unseen example as the \nmajority of its neighbors. Typically, in the case of attribute valued data, Euclidian distance is \nused to measure the similarity between two examples. Although simple, the K-NN algorithm \ncan produce good classification models. \nTo extend the K-NN algorithm to operate on player trace data, we use the LCS-based \nmeasure to calculate neighbor distance. To predict whether a given player trace is a bot \ntrace or a human trace, we detect K nearest neighbors of the trace under consideration and \npredict its class (bot or human) based on the majority of the neighbors. \nTo see why the LCS similarity measure serves the purpose of distinguishing between bots \nand humans, consider the following observations. First, bots (or gold farmers) constantly \nrepeat a set of actions. While these actions may be interlaced by random movements, in \norder to achieve their objective (for example, killing boars in World of Warcraft (WoW) to \ngain experience points), they have to repeat some sequence of actions. Second, the areas \non the world where these actions can be performed are specific. (For example, there are \nparticular locations in WoW that are intended for neophyte players to kill boars and gain \nexperience points.) A set of player traces that represent bots (or gold farmers) will have \nspecific repeating locations easily captured by the LCS. \nWhile K-NN is conceptually simple, it is computationally unfeasible for the task of player \ntrace analysis on large datasets. This is because in order to identify the K nearest \nneighbors, we have to compute the LCS similarity measure of the unseen examples with all \nthe other examples in the training set. The LCS-similarity measure can be computed in \nO(mn). (m and n are lengths of the input sequences.) This is sufficiently fast for batch \nprocessing (offline classification and analysis of player traces); however, when bot detection \nneeds to be performed in real time, this is too slow. To address this issue, we need a more \nsophisticated technique, namely SVMs. \nUsing an LCS-Based Similarity Measure with SVMs \n",
      "content_length": 4131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": " \n \nThe LCS-based similarity measure can also be used in conjunction with SVMs. SVMs \ntypically operate on attribute valued data and, given a set of training examples (categorized \ninto two categories), produce a hyperplane (a higher dimensional plane) that separates \nexamples into the two categories. In order to classify an unseen example, its distance and \norientation with the hyperplane are computed, and based on this, we can make a prediction \nabout its category. Figure 3.11.18 illustrates this process. \nFigure 3.11.18. Support vector machines. \n \n \nThe hyperplane, more correctly referred to as the maximum margin hyperplane, is a plane \nthat puts the maximum distance between the positive and negative examples. The \nmaximum margin hyperplane is defined by the examples, which, in a sense, lie on the \nboundary of the positive and negative regions and are referred to as the support vectors. \nThe key point to note here is that in order to classify an unseen example, the LCS measure \nonly needs to be computed against the support vectors, and not the entire set of examples. \nThis significantly speeds up the process of prediction. \n \nConclusion \nWe presented a number of techniques to analyze data in MMORPGs, dealing with specific \nproblems such as advertisement placement, profile building, and bot detection. In \nconclusion, the most important point we would like to convey to the community is the \nadded value of logging player data. Understanding interaction and behavior in virtual worlds \ncan help us design better virtual worlds, and this is only possible through the collection and \nanalysis of such data. In most cases, the cost of collecting such data is a small price to pay \ncompared to the insight received by analyzing the data. \n \nReferences \n",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": " \n \n[Cook07] Cook, D. J., L. B. Holder, and G. M. Youngblood. ―Graph-Based Analysis of Human \nTransfer Learning Using a Game Testbed.‖ IEEE Transactions on Knowledge and Data \nEngineering 19.11 (2007): 1465–478. \n[Cristianini00] Cristianini, N. and J. Shawe-Taylor. An Introduction to Support Vector \nMachines and Other Kernel-based Learning Methods. Cambridge University Press, 2000. \n[Dixit08] Dixit, Priyesh N., and G. Michael Youngblood. ―Understanding Information \nObservation in Interactive 3D Environments.‖ Sandbox ‗08: Proceedings of the 2008 ACM \nSIGGRAPH Symposium on Video Games. 2008. 163–170. \n[Jain99] Jain, A. K., M. N. Murty, and P. J. Flynn. ―Data Clustering: A Review.‖ ACM \nComputing Surveys 31.3 (1999): 264–323. \n[Mitchell97] Mitchell, T. Machine Learning. WCB McGraw Hill, 1997. \n[Wagner74] Wagner, R. A., and M. J. Fischer. ―The String-to-String Correction Problem.‖ \nJournal of the ACM (JACM) 21.1 (1974): 168–173. \n[Youngblood08] Youngblood, G. M. and P. N. Dixit. ―Understanding Intelligence in Games \nUsing Player Traces and Interactive Player Graphs.‖ Game Programming Gems 7. Boston: \nCharles River Media, 2008. 265–280. \n \nSection 4: General Programming \nIntroduction \nFast-IsA \nRegistered Variables \nEfficient and Scalable Multi-Core Programming \nGame Optimization through the Lens of Memory and Data Access \nStack Allocation \nDesign and Implementation of an In-Game Memory Profiler \nA More Informative Error Log Generator \nCode Coverage for QA \nDomain-Specific Languages in Game Engines \nA Flexible User Interface Layout System for Divergent Environments \nRoad Creation for Projectable Terrain Meshes \nDeveloping for Digital Drawing Tablets \nCreating a Multi-Threaded Actor-Based Architecture Using Intel® Threading Building Blocks \n",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": " \n \nIntroduction \nDoug Binks, Intel Semiconductors AG \ndoug.binks@googlemail.com \nGame programming, like many disciplines, is becoming increasingly specialized. The steady \ntrend of technical innovation, along with the broadening requirements of game \ndevelopment, force us to focus our finite mental resources on an ever-narrowing section of \nthe field. Yet the very basis of the programming endeavor is the ability to coerce the \ncomputational architecture into performing to our will. Most of the gems in this section deal \nwith this—the fundamental art of game programming. \nPerformance is a critical aspect of most game software, and so several gems deal with this \nissue, either directly, by showcasing solutions for common tasks with enhanced \nperformance, or indirectly, by providing a better understanding of some aspect of \nperformance programming. Multi-threading is an increasingly important area for \nprogrammers looking for more cycles to execute their instructions, and suitably, a pair of \narticles targets this. Several articles deal with memory issues, from allocation to \noptimization and profiling. \nA good part of getting a system to do what we want is ensuring that it actually does. In this \nvein, a couple of articles deal with error logging and enabling the QA process. The solutions \npresented require minimal effort to implement, so they stand a good chance of being widely \nused if included in a code base. \nOther articles deal with functionality. It‘s here that the gems cover the widest area, partly \nthrough addressing general approaches to adding functionality and partly through \ndescribing specific but different types of functionality. Continuing the trend of many \nprevious editions, there‘s a bias toward tools—rightly so, as tools play an ever-important \nrole in game development. \nWhether you‘re a jack of all trades, a master of one, or new to game programming, you‘ll \nfind a good deal of useful innovation, information, and experience within this section. \n \n4.1. Fast-IsA \nJoshua Grass, PhD \njoshua.grass@gmail.com \nMany advanced scripting languages have notions of class hierarchies similar to those in \nprogramming languages such as C++, C#, or Java. Scripts written in these languages often \nneed to perform safe casts or IsA checks on objects. In our game, This Is Vegas, we found \nthat the amount of time spent performing the IsA check was not insignificant. This gem \ndescribes a method for processing class hierarchy data to change the IsA operation from \nO(N) to O(1). In our case, this resulted in a performance improvement of more than one \npercent for the cost of adding one DWORD for each class (depending on your platform; if the \nplatform does not have the BitScanReverse operator, you will need to store the location \nof the most significant bit along with the index). The algorithm is also an interesting study \nin combining several well-known data structures that we often see in school but rarely get \nto use to achieve some tangible results. \nProblem Definition \n",
      "content_length": 3025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": " \n \nGiven a class hierarchy, we need to be able to determine whether Class A is a subclass of \nClass B. A typical class hierarchy might look like Figure 4.1.1. \nFigure 4.1.1. An example class hierarchy. \n \nOur first implementation of an IsA() function would be as follows: \nbool IsA(Class *pA, Class *pB) \n{ \n     while (*pA != NULL) \n     { \n          if (pA == pB) \n          { \n             return true; \n          } \n          pA = pA->GetParentClass(); \n     } \n     return false; \n} \n \nThis algorithm has two major problems: The worst-case scenario requires a traversal from \nleaf to root of the class tree, which can be very expensive if you are frequently doing IsA \ntests on leaf nodes (for example, we have an array of humans, and we want to process only \nthieves). The second problem is one of cache issues. The class metadata may be loaded \nanywhere in memory, and if one of the classes we traverse is not currently in the cache, \nthis operation can result in cache thrashing and low performance. \n \nBalanced Class Hierarchies \nWhile a graph with a variable number of branches at each node gives the system a huge \namount of flexibility, it also means that there is no regular way in which we can store or \naccess the hierarchy. Let‘s imagine that we were incredibly lucky in our class hierarchy, and \nat the very end of the project, we had a uniform graph in which each node branched exactly \ntwice, such as in the class hierarchy displayed in Figure 4.1.2. \nFigure 4.1.2. A balanced binary class hierarchy. \n",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": " \n \n \nThis graph has many useful properties, the main advantage being that there is a simple way \nof laying out the classes such that they can fit in one contiguous array of memory. \nProgrammers writing A* algorithms use this structure (a heap) all the time because it \neliminates the need for storing pointers and it makes memory management of an open list \nextremely easy. \nNULL \nObject Human Weapon Warrior Mage \nSword \nBow \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \nLevel 0 Level 1 Level 2 Level 2 \nLevel 3 Level 3 Level 3 Level 3 \n \nEach level we add to the tree adds 2(N-1) new nodes to the storage array, where N is the \nnew level. So if we were to add Level 4 in our aforementioned example, we would need to \nadd eight additional items to our array. Adding Level 5 would add 16 new nodes, and so on. \nWe start our table at Entry 1 instead of Entry 0 for reasons that will be discussed later in \nthe gem. \nWhat we‘re really interested in here is the index of the nodes and their relationship to their \nparents. I will refer to this index as the class index for the rest of the gem. In the above \ntable, the class index is the second row. It is important to note that we do not actually \ncreate a heap or store the classes in it. We use the heap structure purely to create a useful \nordering for the classes. \nThe function for getting the parent‘s class index of a node is trivial: \nint parentIndex(int nIndex) \n{ \n     return nIndex >> 1; \n} \n \nIf we take the class index of Sword (6) and right shift it by 1, we get the result 3, which is \nthe class index of the parent node, Weapon. We can do this again and determine that the \nparent node of Weapon (3) is, in fact, Object (1). \nGiven this perfectly balanced tree, we can easily rewrite our IsA function to use the class \nindices in the storage array to determine whether a class is a subclass of another. \n",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": " \n \nbool IsA_Balanced2Tree(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     while (nAIndex != 0) \n     { \n          if (nAIndex == nBIndex) \n          { \n               return true; \n          } \n          nAIndex = nAIndex >> 1; \n     } \n     return false; \n} \n \nSo while this function doesn‘t look that much better initially, it does have one huge \nadvantage over our previous algorithm. It doesn‘t depend on any information from the \nparent classes of A. It only retrieves information from Class A and from Class B, which are \nvery likely to already be in the cache. So we have eliminated the possibility of any \nunnecessary cache misses for intermediary classes between A or B or, in the worst-case \nscenario where A is not a child of B, all of the parent classes of A. \nWe can further improve this algorithm by realizing that once the index for a parent of A is \nless than the index for B, there is no way that they are ever going to be equal. \nbool IsA_Balanced2Tree_V2(Class   *pA, Class   *pB) \n{ \n      int   nAIndex =  pA->GetClassIndex(); \n      int   nBIndex =  pB->GetClassIndex(); \n \n      while (nAIndex >= nBIndex) \n      { \n           if   (nAIndex == nBIndex) \n           { \n                 return   true; \n           } \n           nAIndex = nAIndex >> 1; \n      } \n      return false; \n} \n \nThis has just reduced our worst-case scenario drastically. In the case where we are testing \na list of Humans to see whether they are Mages, we can halt immediately if they are \nWarriors, because the index of Warrior (4) is less than the index for Mage (5). Even in the \ncase where we were searching for Warriors, we would only need to do one right shift before \nwe could halt the function and return false. \n \nEliminating the Tree Traversal \n",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": " \n \nThe class indices have an additional property that allows us to remove the while loop from \nour function. Here is the child function for our nodes in our class tree: \nint childIndex(int nIndex, bool bRight) \n{ \n     if (bRight) \n          return (nIndex << 1) + 1; \n     else \n          return (nIndex << 1); \n} \n \nAny child of Node A has an index equal to the index Node A left-shifted a number of times \nplus a number defining the child‘s position in the sub-tree. The usefulness of this \nobservation becomes much more apparent if we write out the indices in binary (see Figure \n4.1.3). \nThis observation allows us to make the following rule: \nIf Class A is a child of Class B, then the lefimost N bits of B will match A \nwhere N is the highest bit set in A. \nThis works because we started our class hierarchy at Index 1, so we know that all indices \nare 1 followed by an arbitrary number of bits. \nFigure 4.1.3. Binary representation of indices of a node and its children. \n \n \nUsing this rule we can write our IsA function one more time without the use of the while \nloop. (Non-constant bit-shift operators are emulated on the PS3, but this is implemented in \nmicrocode, so it is still much faster than a while loop.) \nbool IsA_Balanced2Tree_V3(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     if (nAIndex <= nBIndex) \n          return nAIndex == nBIndex; \n \n     nAIndex = nAIndex >> \n               (BSR(nAIndex) – BSR(nBIndex)); \n \n",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": " \n \n     return nAIndex == nBIndex; \n} \n \nThe BSR function in our case is a wrapper for an inline assembly function that uses the BSR \nassembly instruction (BitScanReverse). This instruction returns the index of the \nleftmost set bit/most significant bit, which is exactly what we need for this algorithm. If a \nplatform does not have the BSR assembly instruction, we can easily pre-calculate this value \nand store it in the Class object along with the array index (GetArrayIndexMSB()). \nThis was our first implementation of the function before we found out about the BSR \ninstruction. \nFinally, we can take advantage of one further property of the right-shift operator. If the \namount to shift is negative, then the result is 0. And since we start our class hierarchy with \nan index of 1, no class will match 0. This leads to our final implementation of Fast-Isa. \nbool FastIsA(Class *pA, Class *pB) \n{ \n     int nAIndex = pA->GetClassIndex(); \n     int nBIndex = pB->GetClassIndex(); \n \n     return nBIndex == \n          (nAIndex >> (BSR(nAIndex) – BSR(nBIndex)); \n} \n \n \n \nBuilding a Balanced Tree \nAll of the previous work has been built upon the notion that our class hierarchy is a perfectly \nbalanced binary tree. In practice, this is rarely the case. Luckily, what we want out of the \nIsA function isn‘t any notion of depth between nodes, but only if they are in fact ancestors. \nBecause of this, there is no reason why we cannot insert phantom classes to balance our \ntree. Figure 4.1.4 displays an example of the transformation from an unbalanced to a \nbalanced hierarchy. \nFigure 4.1.4. Converting a three-child node into a binary hierarchy. \n \nIn the case of these two class trees, every possible IsA relationship is maintained. \n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": " \n \nIn any situation where we have more than two direct children of a class, we \ncan insert a number of phantom class nodes between the parent and the \nchildren to ensure that the IsA tree is a balanced binary tree. \nIt is important to realize that while we are using the notion of a heap to generate the \nindices, we are not actually storing anything in this structure. It is purely virtual, so adding \nlarge numbers of phantom nodes to balance the tree does nothing except use up our index \nspace. For most games, a 32-bit DWORD will contain more than enough space for the class \nhierarchy. \nThe simplest implementation for building the class tree is the following algorithm. I \nrecommend implementing this and determining whether you are close to running out of \nindex space before moving to a more complicated algorithm. \nvoid BuildTree(Class *pA) \n{ \n     int nCurrentClassIndex = pA->GetClassIndex(); \n     int nNumChildClasses = pA->GetNumberOfChildClasses(); \n     int nNumLevels = BSR(nNumChildClasses) + 1; \n     int nChildIndexStart = \n               nCurrentClassIndex << nNumLevels; \n     for (int i = 0; i < nNumChildClasses; ++i) \n     { \n           Class *pChild = pA->GetChildClass(i); \n           pChild->SetClassIndex(nChildIndexStart + i); \n           BuildTree(pChild); \n      } \n} \n \nThe heart of implementing a more complicated class tree construction algorithm is realizing \nthat in most cases we have a fair amount of play in how we actually lay out the phantom \nnodes. Take the four-child case (see Figure 4.1.5). \nFigure 4.1.5. Alternate ways of decomposing a four-child node into a binary \nhierarchy. \n",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": " \n \n \n \nBoth of these class trees have the exact same IsA relationship, so to the Fast-IsA algorithm \nthere is no distinction. If our class tree was very complex (or deep), we could balance the \ntree based on the number of subclasses or the maximum depth of any subclass of a class. \nThis leads to an algorithm similar to Huffman encoding to optimize the class tree such that \nthe maximum depth of any leaf is minimized. With more than 3,700 script classes in This Is \nVegas, we never encountered a problem with our hierarchy depth. \nIf your class tree is greater than 32 levels deep, there is no reason why you cannot simply \nchange your class index from a DWORD to a QWORD. The memory costs are minimal (since \nwe only pay per class and not per instance), and 64-bit processors will be able to perform \nthese operations at the same speed. \n \nConclusion \nMoving to the Fast-IsA algorithm had a large impact on the performance of our game for \nthe cost of only one additional DWORD per class. In our case, it was a performance \nimprovement of more than one percent for the entire game and far greater in specific \nportions of the code. Given how simple the implementation is and how little it costs, this \nwas an easy optimization win for us. We also found the Fast-IsA function to impact \nperformance in our pipeline‘s baking process, which uses a large number of IsA checks. \n \n4.2. Registered Variables \nPeter Dalton, Smart Bomb Interactive \npdalton@madprog.com \n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": " \n \nInter-system communication is a critical consideration in a game engine, often dictating the \nbroad architecture of the code base. In practice, sacrifices are often made to allow one \nsystem to know about the internal workings of another in order to accommodate better \ncommunication. Although sometimes this can be appropriate, it often results in a loss of \nmodularity. These compromised systems lose their ―black box‖ characteristics and become \nharder to maintain and replace. This gem will present a solution to this problem by \ndemonstrating a technique for linking up shared variables across disparate systems. This \nallows systems to define a set of inputs or control variables that can be seamlessly linked to \nvariables in other systems. \nIt is important to recognize that this is not a messaging system, nor is it meant to replace \none. Rather, it is a system that allows a programmer to control communication across \nvarious systems without requiring blind casts, global variables, or a flat class hierarchy. This \ntechnique allows for basic variable types, such as integers and floats, as well as complex \ndata types, such as arrays, classes, and other user-defined types. This technique has been \nsuccessfully utilized to facilitate the necessary communication required to control animation \nsystems, user interface parameters, display shader parameters, and various other systems \nwhere variable manipulation is required. \nGetting Started \nThe basic idea is to create a wrapper for a variable and then allow these wrapped variables \nto be linked together. The code that is dependent upon the variables can be implemented \nwithout any special considerations. Rather than just accessing the value that has been \nwrapped, the registered variable will walk the chain of linked variables and provide access \nto the appropriate variable. When building registered variables, there are several key goals \nto keep in mind. \n \nKeep the registered variable seamless. To make a registered variable truly \nuseful, it needs to be easy to work with. The goal is to make it transparent to the \nprogrammer whether they are using a regular integer or our newly created integer \nregistered variable. Operator overloading will be the key here. \n \nAllow one registered variable to be linked to another. We are going to allow \nregistered variables to set redirectors, or in other words, allow registered variables \nto be chained together. \n \nTracking of a ―dirty state.‖ To enhance the usefulness of a registered variable, we \nwill include a dirty state in the variable. This provides users with knowledge of when \nthe variable has actually changed, which is useful for run-time optimizations. \n \nCustom run-time type information. This will become necessary when we start \nregistering variables together. It allows us to confidently cast to a specific type \nwithout the need for blind casts. \n \nProvide a way to link registered variables directly. We will provide an initial, \nexplicit method for linking variables together. This method is important when dealing \nwith specific situations where the control variables are well defined. \n \nProvide a way to link registered variables indirectly. As our systems grow in \ncomplexity, we want to allow for variables to be generically linked together without \neither system knowing the internal details of the other. This indirect method will \nbecome the key to dealing with complex situations where all control variables are not \nwell defined or are ambiguous. \n \nAssumptions \nThe code we will present is taken from a commercial Xbox 360 engine. It utilizes several \nroutines and data structures provided by the base engine that are beyond the scope of this \ngem. These dependencies are minimal; however, we need to explicitly mention them in \n",
      "content_length": 3785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": " \n \norder to avoid confusion. A basic implementation of these data structures has been included \non the accompanying CD-ROM. \nTArrays \nThe TArray class is a templated array holder. It is used to hold links to other registered \nvariables. \nFNames \nThis class implements a string token system. It is a holder for all of the strings that exist \nwithin the game engine. Each string is assigned a unique identifier by which it can then be \nreferenced and compared against other FNames with a constant cost of O(1). This \nfunctionality is the key to implementing the required run-time type information and the \nmeans by which registered variables are given unique names for linking. \n \nThe Base Class: RegisteredVar \nThe base class from which all registered variables will be derived is the RegisteredVar \nclass. This class provides all of the support for linking registered variables together and \ntracking the dirty state. Here only key portions of the RegisteredVar class are shown; a \ncomplete implementation can be found on the accompanying CD-ROM. \nclass RegisteredVar \n{ \npublic: \n    // Provides IsA<>() and GetClassType() routines, described \nlater. \n    DECLARE_BASEREGISTERED_VARIABLE( RegisteredVar ); \n \n    RegisteredVar() : m_bDirty(false), m_pRedirector(null) {} \n    virtual ~RegisteredVar() \n    { \n        if (m_pRedirector) \n            m_pRedirector->m_References.RemoveItem( this ); \n        while (m_References.Num()) \n            m_References[0]->SetRedirector( null, false ); \n    } \n    void SetRedirector( RegisteredVar* InRedir ) \n    { \n        if (InRedir!=this && (!InRedir || (InRedir->IsA( \nGetClassType() ) \n            && !InRedir->IsRegistered( this )))) \n        { \n            if (m_pRedirector) \n                m_pRedirector->m_References.RemoveItem( this ); \n            m_pRedirector = InRedir; \n            if (m_pRedirector) \n                m_pRedirector->m_References.AddItem( this ); \n        } \n    } \n    void SetDirty( bool InDirty, bool InRecurse=false ); \n",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": " \n \n    bool IsDirty() const; \n    void SetFName( FName InName ) { m_Name = InName; } \n    FName GetFName() const { return m_Name; } \nprotected: \n    template<class T> T* GetBaseVariable() const \n    { \n        return m_pRedirector ? \n            m_pRedirector->GetBaseVariable<T>() : (T*)this; \n    } \n    FName m_Name; \n    bool m_bDirty; \n    RegisteredVar* m_pRedirector; \n    TArray<RegisteredVar*> m_References; \n}; \n \nThere are two key elements to getting this class correct. The first is preventing dangling \npointers in the destructor. The important consideration here is that since we are going to be \nlinking up registered variables blindly between systems, we do not want to end up pointing \nto a registered variable that has been deleted. This scenario would result in dangling \npointers to invalid memory addresses and severe headaches. To prevent this, we create a \nlink back to the referencing registered variable so that we can clean it up when the \nreferenced registered variable is deleted. This would normally create a doubly linked list; \nhowever, in our case it is common for multiple registered variables to redirect to a single \nregistered variable, thus creating the need for an array of pointers as illustrated in Figure \n4.2.1 \nFigure 4.2.1. This diagram illustrates how registered variables will be linked \ntogether and how we will also be tracking referencing registered variables to \navoid dangling pointers. \n \n \nThe second key to keep in mind is that anytime you access a registered variable, you need \nto ask yourself an important question: Should I be working with ―this‖ copy of the \nregistered variable or should I forward the request to the redirected registered variable? If \nyou decide that the correct answer is to work on the redirected registered variable, the \nGetBaseVariable() routine will retrieve the base registered variable that should be \nused. \n \n",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": " \n \nSingle Variable Values versus Array Variable Values \nThe next step is to divide all the variables into two distinct classifications: single value types \nand arrays. The first classification, single value types, encompasses integers, floats, user-\ndefined types, and so on and will be the focus of the examples provided. The second, array \ntypes, will encompass arrays of integers, floats, user-defined types, and so on. The \nimplementation of array types is very similar to single values types with just a few minor \nalterations. The implementation of array types has been provided on the accompanying CD-\nROM. Having made this distinction, we will now create a templated base class that provides \n99 percent of the functionality required by any variable type. \ntemplate<class T, class RegVar> \nclass RegisteredVarType : public RegisteredVar \n{ \npublic: \n    RegisteredVarType(); \n \n    T Get() { return GetBaseVariable<RegVar>()->m_Value; } \n    const T& Get() const { return GetBaseVariable<RegVar>()-\n>m_Value; } \n    void Set( const T& InV ) \n    { GetBaseVariable<RegVar>()->SetDirectly( InV ); } \n    operator T() const { return GetBaseVariable<RegVar>()-\n>m_Value; } \n    operator T&() { return GetBaseVariable<RegVar>()->m_Value; } \n \n    void operator=( const RegVar& InV ) { Set( InV.Get() ); } \n \n    // Implement comparison operators >,<,>=,<=,==,!=, see CD-\nROM. \n    bool operator>( const T& InV ) { return Get() > InV; } \n \n    // Implement mathematic operators /,*,+,-, see CD-ROM. \n    T operator/( const T& InV ) const { return Get() / InV; } \n \n    // Implement assignment operators /=,*=,+=,-=, see CD-ROM. \n    RegVar& operator/=( const T& InV ) \n    { Set( Get() / InV ); return *(RegVar*)this; } \n \nprotected: \n    void SetDirectly( const T& InValue ) \n    { \n        if (m_Value != InValue) \n        { \n            m_Value = InValue; \n            SetDirty( true ); \n        } \n        for (int ii = 0; ii < m_Parents.Num(); ++ii) \n            ((RegVar*)m_References[ii])->SetDirectly( InValue ); \n    } \n    T m_Value; \n}; \n \n",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": " \n \nExamining the code should illustrate the emphasis placed on providing the appropriate \noverloaded operators to allow the programmer to seamlessly use registered variables. The \nprogrammer should not have to change the code whether they are using a standard variable \nor a registered variable. This ensures that the registered variable is used correctly and \nseamlessly. It also makes it easy to add and remove registered variables from a system \nsince only the variable definition and linking code needs to be updated. \nAnother important consideration is the SetDirectly() routine used by the Set() \nmethod. The SetDirectly() routine first determines whether the value is actually \ndifferent than the current value and sets the dirty flag if appropriate. This dirty flag allows \nthe owner of the variable to effectively track when the state of the variable has truly \nchanged, thus allowing for run-time optimizations. \nA common optimization, when dealing with shader parameter blocks within DirectX, is to \nprevent the blocks from being invalidated and rebuilt unless absolutely necessary. Thus, if \nyou have a variable controlling the state of a shader, you will want to make sure that the \nvariable has actually changed before processing it. You should also notice that there is no \nautomatic means by which the dirty flag is cleared. To clear the flag, the owner of the \nvariable will need to explicitly call the SetDirty( false ) routine when the owner is \ndone dealing with the change. Since the dirty flag is stored in each variable, the owner of \nthe variable can deal with the flag in its own way. In the case of a variable controlling a \nshader parameter, we would not want to handle the variable change and rebuild the state \nblock until the material is required by the renderer. However, another variable might also be \nlinked to this state and want to handle the change immediately. It is also safe for the owner \nto choose to ignore the dirty flag if it isn‘t required. \nThe SetDirectly() routine also has the task of copying the value to the entire chain of \nlinked registered variables. This feature is important to retain the most recent value in the \nevent that a registered variable clears its redirector either explicitly or if the redirector is \ndeleted. If the value was not copied, we would see a pop from the old value to whatever \nvalue is currently stored. While this might not be a critical issue, it can cause undesired \nbehavior, as the variable might appear un-initialized. Copying the value is also useful when \ndebugging, allowing for the value to be easily shown in the watch window without digging \nthrough a list of linked variables. \n \nType-Specific Registered Variable \nAt this point we have built all the base classes required, and creating registered variables is \nnow straightforward. \nclass RegisteredVarBOOL \n    : public RegisteredVarType<bool, RegisteredVarBOOL> \n{ \nDECLARE_REGISTERED_VARIABLE( RegisteredVarBOOL, \nRegisteredVarType ); \n    RegisteredVarBOOL& operator=( const bool& InValue ) \n    { Set( InValue ); return *this; } \n}; \n \nclass RegisteredVarFLOAT \n    : public RegisteredVarType<float, RegisteredVarFLOAT> \n{ \nDECLARE_REGISTERED_VARIABLE(RegisteredVarFLOAT, \nRegisteredVarType ); \n",
      "content_length": 3247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": " \n \n    RegisteredVarFLOAT & operator=( const float& InValue ) \n    { Set( InValue ); return *this; } \n}; \n \nThe listings above add support for both the standard Boolean and float types. Implementing \nadditional types is as simple as duplicating the provided code and updating the names and \ntypes appropriately. Note that the operator=() was not specified within the templated \nbase class RegisteredVarType in order to resolve conflicts when using the Visual Studio \n2008 C++ compiler. \n \nSetting a Registered Variable Directly \nWe‘ll now look at a simple example to illustrate what registered variables can do. We have a \nweapon class attached to a vehicle class, and the vehicle needs to tell the weapon when to \nfire. If we create a Boolean registered variable within the vehicle and link it to the weapon, \nwe can then just manipulate the variable within the vehicle and control the state of the \nweapon. Also, if we have multiple components that need to know about the weapon firing, \nsuch as AI logic, user interfaces, or game code, we now only have one variable that needs \nto be updated to keep everyone in sync. In contrast, without using registered variables we \nwould need to create a Fire() function within the weapon and call it to start and stop \nfiring. We would also need to manually notify all other systems that the weapon is firing. \nThe registered variable approach has the advantage that once the variables are correctly \nregistered, it is much easier to control communication. \nclass Weapon \n{ \n    void SetFireRegVar( RegisteredVar* InVar ) \n    { \n        m_Fire.SetRedirector( InVar ); \n    } \n    void HeartBeat( float InDeltaTime ) \n    { \n        if (m_Fire) FireWeapon(); \n    } \n    RegisteredVarBOOL m_Fire; \n}; \nclass Vehicle \n{ \n    void Initialize() \n    { \n        m_MyWeapon.SetFireRegVar( &m_FireWeapon ); \n    } \n    void HeartBeat( float InDeltaTime ) \n    { \n        m_FireWeapon = DoWeWantToFire(); \n    } \n    bool DoWeWantToFire(); \n    RegisteredVarBOOL m_FireWeapon; \n    Weapon m_MyWeapon; \n}; \n",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": " \n \n \n \n \nIsA Functionality \nThe DECLARE_REGISTERED_VARIABLE macro requires further explanation to assist in \nunderstanding the implementation. The purpose of this macro is to provide type information \nfor the registered variable. It ensures that we do not link two registered variables together \nthat are not of the same basic type. It also allows us to determine the type of register \nvariable that we have given only a pointer to the base class RegisteredVar. \n#define DECLARE_REGISTERED_VARIABLE( InClass, InBaseClass )            \n\\ \n  protected:                                                           \n\\ \n       typedef InClass ThisClass;                                      \n\\ \n       typedef InBaseClass Super;                                      \n\\ \n  public:                                                              \n\\ \n       virtual FName GetSuperClassType( FName InComponentType ) \nconst     \\ \n       {                                                               \n\\ \n           FName SuperType = NAME_None;                                \n\\ \n           if (InClass::StaticGetClassType() == InComponentType)       \n\\ \n               SuperType = Super::StaticGetClassType();                \n\\ \n           else if (InComponentType ! = NAME_None)                     \n\\ \n               SuperType = Super::GetSuperClassType ( \nInComponentType );  \\ \n           return SuperType == InComponentType ? NAME_None : \nSuperType;   \\ \n       }                                                               \n\\ \n       virtual FName GetClassType() const                              \n\\ \n       {                                                               \n\\ \n           return InClass::StaticGetClassType();                       \n\\ \n       }                                                               \n\\ \n       static FName StaticGetClassType()                               \n\\ \n       {                                                               \n\\ \n",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": " \n \n           static FName TypeName = FName( STRING( InClass));           \n\\ \n           return TypeName;                                            \n\\ \n       } \n#define DECLARE_BASEREGISTERED_VARIABLE( InClass )                     \n\\ \n   DECLARE_REGISTERED_VARIABLE( InClass, InClass )                     \n\\ \n   template<class T> bool IsA() const                                  \n\\ \n       {                                                               \n\\ \n           return IsA( T::StaticGetClassType() );                      \n\\ \n       }                                                               \n\\ \n       bool IsA( const FName& InTypeName ) const                       \n\\ \n       {                                                               \n\\ \n           for (FName Type = GetClassType(); Type != NAME_None;        \n\\ \n               Type = GetSuperClassType( Type ))                       \n\\ \n           {                                                           \n\\ \n               if (Type == InTypeName)                                 \n\\ \n                   return true;                                        \n\\ \n           }                                                           \n\\ \n           return false;                                               \n\\ \n       } \n \nWhile this code is being utilized here to provide IsA functionality for registered variables, it \nis generic in nature and can be used to provide RTTI functionality to any class or structure. \nThe code is written in the form of a macro to prevent the code from being duplicated due to \nit being required at every level of the inheritance chain. An important consideration is to \nrecognize that this implementation does not support multiple inheritance but could be \nextended to do so. \n \nSetting a Registered Variable Indirectly \nNow that we have basic RTTI information, we can safely link registered variables together \nwithout knowing the internals of other systems. Let‘s examine another example. \nSuppose we have a material used for rendering that has a parameter we can adjust to \nchange its damage state. The damage state is defined within the material and used to \n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": " \n \ndetermine how the material is rendered. In this example we would like to create a generic \nsystem in which a high-level object can register a variable with another object and have it \ncorrectly link to a control variable. We want a vehicle class to provide a variable to control \nthe damage state of the material, and then the vehicle can drive the material‘s control \nvariable by simply modifying its own variable. In this example, adding a function or \nparameter to the Material class would not be desirable because it would lead to bloat \nand would not be applicable to all materials. \nclass RegisterVariableHolder \n{ \n    virtual void RegisterVariable( RegisterVar& InVar ) {} \n}; \nclass BaseClass : public RegisteredVariableHolder \n{ \n    virtual void RegisterVariables(RegisterVariableHolder& \nInHolder) {} \n}; \nclass Material : public BaseClass \n{}; \nclass DamageStateMaterial : public Material \n{ \n    void Initialize() \n    { \n        m_DamageState.SetFName( \"DamageState\" ); \n    } \n    virtual void RegisterVariable( RegisteredVar& InVar ) \n    { \n        if (InVar.IsA<RegisteredVarFLOAT>() && \n            InVar.GetFName() == m_Trans.GetFName()) \n        { \n            m_DamageState.SetRedirector( &InVar ); \n        } \n    } \n    RegisteredVarFLOAT m_DamageState; \n}; \nclass Vehicle : public BaseClass \n{ \n    void Initialize() \n    { \n        m_VehicleDamageState.SetFName( \"DamageState\" ); \n        m_Fire.SetFName( \"Fire\" ); \n        RegisterVariables( *m_pMaterial ); \n    } \n    virtual void RegisterVariables( RegisterVariableHolder& \nInHolder ) \n    { \n        InHolder.RegisterVariable( m_VehicleDamageState ); \n        InHolder.RegisterVariable( m_Fire ); \n    } \n    RegisteredVarFLOAT m_VehicleDamageState; \n    RegisteredVarBOOL m_Fire; \n    Material* m_pMaterial; \n}; \n \n",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": " \n \nNow, whenever the vehicle changes m_VehicleDamageState, the material class‘s \nm_DamageState variable will be automatically updated without the material being \nrequired to provide accessor routines or the vehicle knowing the type of material it has been \nassigned. The vehicle can also ignore the material since the only thing it needs to do is \nupdate its own registered variable. While the example is fairly simple, the principle can be \napplied to solve many more problems. \n \nConclusion \nWithin our game engine we have found registered variables to be an essential part of inter-\nsystem communication because they abstract the communication layer and minimize \nsystem dependencies. Registered variables are utilized to control the state of animation flow \nsystems, expose data to user interfaces, such as hit points and ammo counts, and control \nmaterial parameters, such as damage states and special rendering stages. We provide tools \nwithin the game‘s editor to allow artists and level designers to specify exactly which \nregistered variables should be linked together within the game. Systems have been \ndesigned to allow users to dynamically create new registered variables within the game \neditor and link them to any other appropriate registered variable. For us, this has opened \nthe door for content builders to access any set of data within the game engine and gives \nthem the necessary controls to manipulate gameplay. \nWe hope that you will have fun experimenting with the concept of registered variables and \nthat you will find them useful in improving your code. You will find an implementation of the \ntechniques presented on the CD-ROM. \n \n4.3. Efficient and Scalable Multi-Core Programming \nJean-François Dubé, Ubisoft Montreal \njfdube75@gmail.com \nNowadays, multi-core computers (and game consoles such as the Microsoft Xbox 360 and \nSony PlayStation 3) are very common. Programmers are faced with the challenges of \nwriting multi-threaded code: data sharing, synchronization, deadlocks, efficiency, and \nscalability. Adding multi-threading to an existing game engine is an enormous effort and \nmight give an initial speed gain, but will it run twice as fast if you double the number of \ncores? What if you run it on a 16+ cores system, such as Intel‘s Larrabee architecture? Will \nit be able to run on platforms with co-processors, such as Sony‘s PlayStation 3? \nIn this gem, we‘ll see how to write efficient and scalable multi-threaded code. The first \nsection will deal with the ―efficiency‖ part of the problem, while the second part will deal \nwith the ―scalability‖ part. \nEfficient Multi-Threaded Programming \nMulti-threaded programming introduces a variety of issues the programmer must be aware \nof in order to produce code that performs the required operations correctly. Additionally, \ncertain operations can lead to additional overheads in a multi-threaded program. In this \nsection we‘ll look at high-performance methods for resolving these issues. \nShared Data \n",
      "content_length": 3001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": " \n \nThe main problem with multi-threaded programming is concurrent access to the same \nmemory locations. Consider this simple example: \nuint32 IncrementCount() \n{ \n    static uint32 Count=0; \n    return Count++; \n} \n \nThis is commonly translated into three operations: a load, an addition, and a store. Now, if \ntwo threads execute this function slightly at the same time, what will happen? Here‘s an \nexample: \nThread 1 read Count and store it into register R1 \nThread 1 increment R1 \nThread 2 read Count and store it into register R1 \nThread 2 increment R1 \nThread 1 store the value of R1 into Count \nThread 2 store the value of R1 into Count \n \nIf Count was originally 5 before this sequence of events, what will be the result afterward? \nWhile the expected value is 7, the resulting value would be 6, because each thread has a \ncopy of Count in its register R1 before it was actually updated to memory. This example is \nvery simple, but with more complex interactions this could lead to data corruption or invalid \nobject states. This can be fixed by using atomic operations or by using synchronization \nprimitives. \nAtomic Operations \nAtomic operations are special instructions that perform operations on a memory location in \nan atomic manner; that is, when executed by more than one core on the same memory \nlocation, it is guaranteed to be done atomically. For example, the \nInterlockedIncrement function could be used in the previous example to make it \nthread-safe and lock-free. \nA very useful atomic operation is the Compare And Swap (CAS) function, implemented as \nInterlockedCompareExchange on Windows. Essentially, it compares a value with \nanother and exchanges it with a third value based on the outcome of the comparison, \natomically. It then returns the original value before the swap. Here‘s how it can be \nrepresented in pseudocode: \nuint32 CAS(uint32* Ptr,   uint32 Value, uint32   Comperand) \n{ \n    if(*Ptr   == Comperand) \n    { \n        *Ptr = Value; \n        return Comperand; \n    } \n    return *Ptr; \n} \n \n",
      "content_length": 2035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": " \n \nThis atomic operation is very powerful when used correctly and can be used to perform \nalmost any type of operation atomically. Here‘s an example of its usage: \nuint32 AtomicAND(volatile   uint32* Value, uint32   Op) \n{ \n    while(1) \n    { \n        uint32 CurValue = *Value; \n        uint32 NewValue = (CurValue & Op); \n        if(CAS(Value, NewValue, CurValue) == CurValue) \n        { \n            return NewValue; \n        } \n    } \n} \n \nIn this example, we read the current value and try to exchange it with the new value. If the \nresult of the CAS returns the old value, we know that it wasn‘t changed during the operation \nand that the operation succeeded. (It was swapped with the new value.) On the other hand, \nif the result is not equal to the old value, we must retry, since it was changed by another \nthread during the operation. This is the basic operation on which almost all lock-free \nalgorithms are based. \nSynchronization Primitives \nSometimes, atomic operations are not enough, and we need to be able to ensure that only a \nsingle thread is executing a certain piece of code. The most common synchronization \nprimitives are mutexes, semaphores, and critical sections. Although in essence they all do \nthe same thing—prevent execution of code from multiple threads—their performance varies \nsignificantly. They are kernel objects, which means that the operating system is aware \nwhen they are locked. Therefore, they will generate a costly context switch if already locked \nby another thread. On the other hand, most operating systems will make sure the thread \nthat has a critical section locked will not be preempted by another thread while it holds the \nlock. So using those primitives depends on a lot of factors: the time span of the lock, the \nfrequency of locking, and so on. \nWhen locking is required very frequently and for a very low amount of time, we want to \navoid the overhead from operating system process rescheduling or context switching. This \ncan be achieved by using a spin lock. A spin lock is simply a lock that will actively wait for a \nresource to be freed, as seen in this simplified implementation: \nwhile(CAS(&Lock, 1, 0)) {} \n \nWhat it does is simple: It uses the CAS function to try to gain access to the lock variable. \nWhen the function returns 0, it means we acquired the lock. Releasing the lock is simply \nassigning 0 to the Lock variable. \nA real implementation of a spin lock usually should contain another waiting loop that doesn‘t \nuse atomic functions to reduce inter-CPU bus traffic. Also, on some architectures, memory \nbarriers are required to make sure that the state of the Lock variable is not reordered in \nsome ways, as seen in the next section. The complete implementation is available on the \nCD. \n",
      "content_length": 2769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": " \n \nOn architectures that don‘t change a thread‘s affinity (in other words, that don‘t reschedule \nthreads on different processors, such as Xbox 360), running several threads on the same \ncore competing for a shared resource using a spin lock is a very bad idea. If the lock is held \nby a thread when it gets interrupted by the operating system scheduler, other threads will \nbe left spinning trying to acquire the lock, while the thread holding it is not making progress \ntoward releasing it. This results in worse performance than using a critical section, as shown \nin Figures 4.3.1 and 4.3.2. \nFigure 4.3.1. Wasting cycles when using spin locks for a long time. \n \n \nFigure 4.3.2. Efficient locking when using critical sections. \n \n \n \nMemory Ordering \nMemory can be read and written in a different order than written in your code for two main \nreasons: compiler optimizations and hardware CPU reordering. The latter differs a lot \ndepending on the hardware, so knowing your architecture is important. See [McKenney07] \nfor a detailed look at this problem. \n",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": " \n \nConsider the following pieces of code running simultaneously: \nThread 1 \nThread 2 \nGlobalValue = 50; \nwhile(!ValueIsReady) {} \nValueIsReady = true; LocalValue = GlobalValue; \n \nWhile this looks completely fine and will almost always work when running on a single core \ndepending on the compiler optimizations, it will probably fail when running on different \ncores. Why? First, the compiler will most likely optimize the while loop and keep \nValueIsReady in a register; therefore, declaring it as volatile should fix the problem on \nmost compilers. Second, due to out-of-order memory accesses, ValueIsReady might get \nwritten first; therefore, Thread 2 can read GlobalValue before it is actually written by \nThread 1. Debugging this kind of bug without knowing that memory reordering exists can \nbe long and painful. \nThree types of memory barriers exist: read, write, and full barriers. A read memory barrier \nforces reads from memory to complete, while a write memory barrier forces writes from \nmemory to complete, so other threads can access the memory safely. A full memory barrier \nis simply forcing both reads and writes to complete. Some compilers will also correctly \nhandle reads and writes of volatile variables by treating them as memory barriers (Visual \nStudio 2005 for instance, as described in [MSDN]). Also, some of the Interlocked functions \ncome with the Acquire and Release semantics, which behave like read and write barriers, \nrespectfully. \nThe previous example can be solved by using a write memory barrier before setting \nValueIsReady to ensure that the write to GlobalValue actually gets written before \nValueIsReady, ensuring that other threads will see the new GlobalValue before \nValueIsReady is written. \nFalse Sharing \nMost multi-core architectures have a per-core cache, which is generally organized as an \narray of memory blocks, each with a power of two size (128 bytes, for example), called \ncache lines. When a core performs a memory access, the whole line is copied into the cache \nto hide the memory latency and maximize speed. False sharing occurs when two cores \noperate on different data that resides in the same cache line. In order to keep memory \ncoherency, the system has to transfer the whole cache line across the bus for every write, \nwasting bandwidth and memory cycles. The solution to this problem is to make sure that \nthe data is structured in a way that avoids this problem. \nMemory Allocator Contention \nMemory allocators can rapidly become a bottleneck in multi-threaded applications. Standard \nmemory allocator implementations provided in the C run time (malloc/free) or even \noptimized and widely used allocators, such as Doug Lea‘s dlmalloc [Lea00], aren‘t designed \nto be used concurrently; they need a global lock to protect all calls, which inherently leads \nto false sharing. The design of a multi-processor‘s optimized memory allocator is beyond the \nscope of this gem, but a good comparison of available multi-processor allocators can be \nfound in [Intel07]. \nIdle Thread State \n",
      "content_length": 3048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": " \n \nWhen a thread is idle, it is important that it doesn‘t waste CPU cycles. Here‘s some code we \noften see from a person writing multi-threaded code for the first time: \nwhile(!HasWork()) \n{ \n    Sleep(0); \n} \n \nThe Sleep(0) will make the thread give up the remainder of its time slice for other \nthreads, which is fine. But, when it gets rescheduled, it will loop back, unnecessarily wasting \nCPU time (and multiple context switches) until it has work to do. A better solution is to put \nthe thread in sleep mode, waiting for an event. This is achieved by using the \nCreateEvent and WaitForSingleObject functions. Waiting for an event essentially \ntells the operating system scheduler that the thread is waiting and shouldn‘t get any CPU \ntime until the event is triggered. \nThread Local Storage \nIt is possible to declare per-thread global variables using Thread Local Storage (TLS). The \ndeclaration differs from compiler to compiler, but here‘s how it works under Visual Studio: \n__declspec(thread) int GlobalVar; \n \nEach thread now has its own GlobalVar variable copy. This can be especially useful for \nper-thread debugging information (such as the thread name, its profiling stats, and so on) \nor for custom memory allocators that can operate on a per-thread basis, effectively \nremoving the need for locking. \nLock-Free Algorithms \nA good introduction to such things is discussed in detail in [Jones05] and [Herlihy08]. \nEssentially, these algorithms are pieces of code that can be executed by multiple threads \nwithout locking. This can lead to enormous speed gain for some algorithms, such as \nmemory allocators and data containers. For example, a lock-free queue could need to be \nsafe when multiple threads push data into it, while multiple threads also pop data at the \nsame time. This is normally implemented using CAS functions. A complete implementation \nis available on the CD. \n \nScalable Multi-Threaded Programming \nThe most common way to rapidly thread an existing application is to take large and \nindependent parts of the code and run them in their own thread (for example, rendering or \nartificial intelligence). While this leads to an immediate speed gain and lots of \nsynchronization problems, it is not scalable. For example, if we run an application using \nthree threads on an eight-core system, then five cores will sit idle. On the other hand, if the \napplication is designed from the start to use small and independent tasks, then perfect \nscalability can be achieved. To accomplish this, several options already exist, such as the \nCilk language [CILK], which is a multi-threaded parallel programming language based on \nANSI C, or Intel‘s Threading Building Blocks [TBB]. An implementation of a simple task \nscheduler is presented next. \nTask Scheduler Requirements \n",
      "content_length": 2799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": " \n \nThe required properties of our scheduler are: \n1. Handle task dependencies. \n2. Keep worker threads‘ idle time at a minimum. \n3. Keep CPU usage low for internal task scheduling. \n4. Have extensibility to allow executing tasks remotely and on co-processors. \nThe scheduler is lock-free, which means that it will never block the worker threads or the \nthreads that push tasks to it. This is achieved by using fixed-size lock-free queues and a \ncustom spin lock and by never allocating memory for its internal execution. \nTasks \nA task is the base unit of the scheduler; this is what gets scheduled and executed. In order \nto achieve good performance and scalability, the tasks need to be small and independent. \nThe (simplified) interface looks like this: \nclass Task \n{ \n    volatile sint* ExecCounter; \n    volatile sint SyncCounter; \npublic: \n    virtual void Execute()=0; \n    virtual sint GetDependencies(Task**& Deps); \n    virtual void OnExecuted() {} \n} \n \nA task needs to implement the Execute function, which is what gets called when it is \nready to be executed (in other words, no more dependencies). \nTo expose dependencies to the scheduler, the GetDependencies() function can be \noverloaded to return the addresses and the number of dependent tasks. A base \nimplementation that returns no dependencies is implemented by default. \nA task is considered as fully executed when its Execute function has been called and when \nits internal SyncCounter becomes zero. The ExecCounter is an optional counter that \ngets atomically decremented when the Execute function is called. For tasks that spawn \nsub-tasks, setting the sub-task‘s ExecCounter pointer to the parent‘s SyncCounter \nensures that their OnExecuted functions will only be called when all sub-tasks have been \nexecuted. \nWorker Threads \nThe scheduler automatically creates one worker thread per logical core. Figure 4.3.3 \nillustrates how the worker threads behave: Each worker thread is initially pushed in a lock-\nfree queue of idle threads and waits for a wakeup event. When a new task is assigned to a \nworker thread by the scheduler, it wakes up and executes the task. Once the task is done, \nthe worker thread does several things. First, it tries to execute a scheduling slice (which will \nbe explained in the next section) by checking whether the scheduler lock is already acquired \nby another thread. Then, it tries to pop a waiting task from the scheduler. If a task is \navailable, it executes it, and the cycle restarts. On the other hand, if no tasks are available, \nit pushes itself in the lock-free queue of idle threads and waits for the wakeup event again. \nFigure 4.3.3. Worker thread logic. \n",
      "content_length": 2679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": " \n \n \n \n \nScheduler \nThe scheduler is responsible for assigning tasks to the worker threads and performing \ntidying of its internal task queues. To schedule a task, any thread simply needs to call a \nfunction that will push the task pointer in the lock-free queue of unscheduled tasks. This \ntriggers a scheduling slice, which is explained next. \nScheduling Slice \nAt this point, pending tasks are simply queued in a lock-free queue, waiting to be scheduled \nby the scheduler. This is done in the scheduling slice, which does the following, as described \nin Figure 4.3.4: \n1. Register pending tasks. \n2. Schedule ready tasks. \n3. Delete executed tasks. \nFigure 4.3.4. Scheduler slice logic. \n",
      "content_length": 692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": " \n \n \n \n \nRegistering Pending Tasks \nDuring this phase, the scheduler pops the pending tasks and registers them internally. \nThen, it needs to handle dependencies. If a task is dependent on previously scheduled \ntasks, it goes through the dependencies and checks to see whether they have been \nexecuted. If that‘s the case, the task is ready to execute and is marked as ready. \nOtherwise, the task is kept as pending until the next scheduling slice. \nScheduling Ready Tasks \nThe second phase of the scheduling slice is to assign tasks that are ready to be executed to \nworker threads. The scheduler first tries to pop an idle thread from a lock-free queue of idle \nthreads. If it succeeds, the task is directly assigned to that thread, and the thread waiting \n",
      "content_length": 760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": " \n \nevent is signaled. If all threads are working, the task is queued in the lock-free queue of \nwaiting tasks. The scheduler then repeats the process until there are no more tasks to \nassign. \nDeleting Executed Tasks \nThe last phase of the scheduling slice handles the tasks that are considered fully executed \nby calling the OnExecuted function on them and by deleting them if they are marked as \nauto-destroy; some tasks may need to be manually deleted by their owner, as they might \ncontain results, and so on. \n \nFuture Work \nSince tasks are usually independent from each other, the scheduler can be extended to \nsupport execution of tasks on remote computers or on architectures with co-processors. To \nachieve this, each task would have a way to package all the data it needs to execute. Then, \nthe scheduler would dispatch the task to other computers or co-processors though a simple \nprotocol. Compilation of the Execute function might be required to target the co-\nprocessor‘s architecture. When a completed task message arrives, the task would then \nreceive and unpackage the resulting data. This could be used as a distributed system (such \nas static lighting/normal map computations, distribution of the load of a game server to \nclients, and so on) or as a way to distribute work on the PlayStation 3 SPUs automatically. \n \nOptimizations \nThe single task queue could become a bottleneck on systems with a large number of cores \nor ones that have no shared cache between all cores. Here, using per-worker thread task \nqueues along with task stealing could prove advantageous. Furthermore, the scheduling \ncould be made more cache friendly through a policy of inserting newly created tasks into \nthe front of the worker thread‘s queue that generated the task, as these are likely to be \nconsuming the data generated by the task that created them. All of these optimizations \ndepend on the target architecture details. \n \nConclusion \nAs we have just seen, the scheduler is completely lock-free; its internal CPU usage is kept \nto a minimum, the worker threads are either executing a task or waiting for one, and most \nof all, it scales with the number of cores. The complete source code on the CD comes with \nseveral sample tasks that have been tested on a Intel Core 2 Quad CPU running at 2.83 \nGHz, with the following results (also see Figure 4.3.5): \nFigure 4.3.5. Test results. \n",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": " \n \n \n  \nFibonacci Sequence Perlin Noise QuickSort \n1 thread \n1.05ms \n9.06s \n9.30s \n2 threads 0.29ms \n4.55s \n4.84s \n3 threads 0.22ms \n3.03s \n3.44s \n4 threads 0.15ms \n2.29s \n2.74s \n5 threads 0.14ms \n2.21s \n2.81s \n6 threads 0.12ms \n2.31s \n2.86s \n \nThe Fibonacci sequence test has been created to see how dependencies are handled. The \nPerlin noise test, on the other hand, has been implemented to see how performance and \nscalability can be achieved; the test consists of computing a 2048 × 2048 Perlin noise \ngrayscale image with 16 octaves. The QuickSort test consists of sorting 65 million random \nintegers. As we can see, the expected scalability is achieved for all of these tests. \n \nReferences \n[CILK] ―The Cilk Project.‖ Massachusetts Institute of Technology. n.d. \n<http://supertech.csail.mit.edu/cilk>. \n[Lea00] Lea, Doug. ―A Memory Allocator.‖ 2000. State University of New York, Oswego. n.d. \n<http://gee.cs.oswego.edu/dl/html/malloc.html>. \n",
      "content_length": 952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": " \n \n[Herlihy08] Herlihy, Maurice and Nir Shavit. The Art of Multiprocessor Programming. Morgan \nKaufmann Publisher, 2008. \n[Intel07] ―The Foundations for Scalable Multi-Core Software in Intel® Threading Building \nBlocks.‖ 2007. Intel. n.d. <http://download.intel.com/technology/itj/2007/v11i4/5-\nfoundations/5-Foundations_for_Scalable_Multi-core_Software.pdf>. \n[Jones05] Jones, Toby. ―Lock-Free Algorithms.‖ Game Programming Gems 6. Ed. Mark \nDeloura. Boston: Charles River Media, 2005. \n[McKenney07] MeKenney, Paul E. ―Memory Ordering in Modern Microprocessors.‖ 2007. \nRaindrop Laboratories. n.d. \n<http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf>. \n[MSDN] ―Synchronization and Multiprocessor Issues.‖ n.d. Microsoft. n.d. \n<http://msdn.microsoft.com/en-us/library/ms686355(VS.85).aspx>. \n[TBB] ―Intel Threading Building Blocks.‖ n.d. Intel. n.d. \n<http://www.threadingbuildingblocks.org>. \n \n4.4. Game Optimization through the Lens of Memory and Data \nAccess \nSteve Rabin, Nintendo of America Inc. \nsteve.rabin@gmail.com \nAs modern processors have become faster and faster, memory access has failed to keep \npace. This is such a pressing issue on current console hardware that experts advise \ndevelopers to treat memory as if it were as slow as hard drive access [Isensee06]. As a \nresult of this vast disparity between CPU and memory speed, a great deal of horsepower \ngoes to waste as CPUs wait for data to work on. Thus, a key aspect to optimizing games is \nkeeping the CPU well fed with data. \nBut isn‘t the cache supposed to alleviate this problem? While the cache is indispensible to \nany complex computer architecture, it isn‘t a panacea. To its benefit, cache is brilliantly \ntransparent to the code, but if we want to truly optimize our game, we‘ll have to pull back \nthe curtains and understand how the cache works in order to help it out. \nOnce we better understand the cache and the memory architecture, it will become apparent \nthat we need to respect the cache by shrinking the size of data and keeping it better \norganized. These will be our two guiding principles for most of our optimizations. \nUnderstand the Cache \nMain memory is extremely slow relative to the CPU. It‘s slow because it‘s typically far away \nfrom the CPU (on another chip) and made with fewer transistors per bit (which requires the \nbits to be refreshed so they don‘t fade away—which takes extra time). However, it‘s \npossible to make more expensive memory that is closer to the CPU (directly on the same \nchip) and faster by using more chip real estate per bit (so it doesn‘t need to be refreshed). \nHowever, this more expensive memory can‘t be as large as main memory because it simply \nwon‘t fit on the silicon die next to the CPU. So what is a computer architect to do? \n",
      "content_length": 2796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": " \n \nThe solution is to keep copies of the most recently used main memory bits in the ultra-fast \ncache memory that sits on the CPU die. In fact, this strategy works so well that modern \ncomputer architectures have multiple levels of cache, usually referred to as level 1, level 2, \nand level 3 (L1, L2 and L3, respectively). \nL1 cache is the smallest and fastest. Because of different access patterns between data and \ninstructions, it‘s commonly split into an L1 data cache and an L1 instruction cache (typically \n32 KB each). L2 cache is slightly slower and is usually shared between data and instructions \n(typically 256 KB to 6 MB, larger if shared among cores). L3 cache is a relatively new \ndevelopment in consumer hardware and appears on Intel‘s i7. On the i7, each core has a \ndedicated L1 and L2, but the 8-MB L3 is shared among the four cores. Table 4.4.1 shows \ncache sizes for various platforms [AMD09, Bell08, Lanterman07, Shimpi09, Wikipedia09]. \nTable 4.4.1. Cache Sizes for Various Platforms \nPlatform \nL1 Instruction/Data \nCache \nL2 Cache \nL3 \nCache \nCache Line \nSize \niPhone 3GS \n32 KB/32 KB \n256 KB \nN/A \n64 bytes \nWii \n32 KB/32 KB \n256 KB \nN/A \n32 bytes \nPS3 \n32 KB/32 KB \n512 KB \nN/A \n128 bytes \nXbox 360 \n32 KB/32KB \n1 MB \nN/A \n128 bytes \nAMD Athlon \nX2 \n64 KB/64 KB \n512 KB to 1 \nMB \nN/A \n64 bytes \nIntel Core 2 \n32 KB/32 KB \n1 MB to 6 MB \nN/A \n64 bytes \nIntel i7 \n32 KB/32 KB \n256 KB \n8 MB \n64 bytes \n \nBut how do these multiple levels of cache work? If the CPU needs to load a word from \nmemory, it will first ask the L1 cache. If the data is there, it‘s a hit in the L1 cache, and the \ndata will be delivered very quickly to the CPU. If the data isn‘t in the L1 cache, then it is a \nmiss in the L1 cache, and then the L2 cache is checked. If the data is in the L2 cache, it is a \nhit, and the data is delivered to the L1 cache and the CPU. If it is a miss in the L2 cache, \nthen the next level must be checked. If the next level is main memory, then the requested \ndata will be delivered to the L2, the L1, and the CPU. \nHowever, there is one more twist to how cache works. Memory isn‘t just copied to different \nlevels of cache in bytes or words. It‘s copied in chunks known as cache lines, which are \naligned sequential pieces of memory (for example, 128 bytes on the PS3 and Xbox 360, \nwhich is 32 words of 4 bytes each). So when the CPU asks for a 4-byte word and it misses \nin each cache, the entire cache line is copied from main memory into each cache level. This \ncache line copy is the reason why spatial coherency is so important for the working data \nset. Table 4.4.1 shows cache line sizes for various platforms. \nKnowing that main memory is slow and the cache is fast, our goal will be to keep data and \ninstructions in the cache as long as possible. The worst thing that can happen is cache \nthrashing, where the working set of data and instructions can‘t fit inside the cache at the \n",
      "content_length": 2925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": " \n \nsame time. In this case, memory is brought into the cache only to be thrown out because \nthere isn‘t enough room for the next needed piece of data. Imagine a tight loop that \ncontinually operates on 512 KB of data when the L2 cache size is only 256 KB. \nWith the limited size of cache and the importance of our data being in the cache, it becomes \nclear why we must keep data small and well organized. \n \nPinpoint Problem Areas \nIf we were to optimize every system in our game, we would waste a lot of time and effort. \nFor example, if we double the speed of a function that only takes 0.01 percent of the frame \ntime, then we have effectively done nothing toward speeding up our game. It‘s a hard truth \nto face since doubling the speed of a function sounds so satisfying, but the overall \nimprovement is too small to make a difference. \nProfiling our game is the only true way to pinpoint which areas are worth spending time to \nimprove. Unless you can prove that a system or section of code is a bottleneck, you \nshouldn‘t waste your effort even thinking about optimizing it. \nOnce you identify code that takes a significant amount of time, how can you tell whether \nthe CPU is waiting for data? The answer is performance counters. Performance counters are \nmeasurement tools built directly into the CPU that can count events and help identify \nproblems during code execution. Many profilers can record performance counters, or you \ncan usually turn them on and record the results directly in your code. \nUsing the performance counters, there are two key metrics to measure in order to identify \nwhen the CPU is spinning, waiting for data. The first is instructions per cycle (IPC), which \ngives a rough measure of how much work gets done per CPU cycle. If you measure an IPC \nof 0.8, then on average 0.8 instructions are completing per CPU cycle. The second key \nperformance counter is the percentage of loads or stores that resulted in an L2 cache miss \n(which means a load from main memory). If the IPC is low and the percentage of L2 cache \nmisses is high, then the cache isn‘t working well for this piece of code, perhaps because of \ncache thrashing. \nOnce you know which code or data needs to be optimized, you can go to work with the \nfollowing suggestions. \n \nAvoid Waste \nThe key to being efficient is avoiding waste. One source of wasted CPU cycles comes from \nreading memory that you don‘t use. Since the cache copies memory in cache line chunks, \nit‘s critical to use everything you read, in order to be efficient. For example, if you have an \narray of structs and only need to operate on one element in each struct, then you might be \nwasting a majority of the data you‘re bringing in from main memory. For example, if you \nonly operate on 30 percent of the struct, then as much as 70 percent of the data you‘re \nbringing in from main memory is wasted. Solutions to this problem are presented in the \n―Organize the Data‖ section. \nAnother source of waste comes from reading or writing memory that is not contiguous. \nAgain, since memory is copied in cache line chunks, it is important to read and write \nsequentially to avoid waste. \n",
      "content_length": 3154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": " \n \nLastly, waste can arise from redundancy. It would be wasteful to read the same data \nmultiple times a frame. Instead, batch operations together and try to optimize for a single \npass through the data per frame. \n \nShrink the Data \nOur first main strategy is simple: If your data is smaller, more of it will fit in the cache. This \nis as easy as carefully managing your data type sizes. If you‘re working with integers that \nwon‘t get very large, consider using a short (2-byte integer) or just a single byte to \nrepresent the value. Instead of using 64-bit doubles, consider using 32-bit floats. However, \none of the biggest wins is with Booleans. Typically, a Boolean takes up 4 bytes, but this is a \nhuge waste of space since ideally it should be represented as a single bit. \nPacked Structures \nAn easy way to manage your data type sizes is by defining them inside a structure. The \nfollowing code is an example of an inefficient structure for a billboard particle, followed by \nan efficiently packed one. Notice the use of reduced ranges, bitfields, and indices, as well as \nhow types are reordered by size to reduce padding. The result is a 1/3 savings in space, or \nabout 7 K saved for 500 particles. \nstruct InefficientParticle //total size 44 bytes \n{ \n   bool visible;           //31 bits of padding \n   Texture *texture;       //pointer to texture \n   int alpha;              //only needs 0 to 255 \n   float  rotation;        //too much precision \n   int type;               //enumeration - 4 possible types \n   Vec3 position; \n   Vec3 velocity; \n}; \n \nstruct EfficientParticle   //total size 30 bytes \n{ \n   Vec3 position; \n   Vec3 velocity; \n   unsigned char alpha;    //saved 3 bytes (0-255) \n   unsigned char rotation; //saved 3 bytes (0-255 degrees) \n   unsigned texture:4;     //saved 28 bits (texture index) \n   unsigned type:2;        //saved 29 bits (enumeration) \n   unsigned visible:1;     //saved 31 bits (single bit) \n}; \n \nTo emphasize the importance of ordering struct variables from largest to smallest (which \nreduces padding and results in smaller structs), consider the following two examples of the \nsame data: \nstruct WastedPadding       //20 bytes total \n{ \n   char      var1;         //1 byte \n   float     var2;         //4 bytes \n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": " \n \n   char      var3;         //1 byte \n   int       var4;         //4 bytes \n   char      var5;         //1 byte \n}; \n \nstruct OptimalPadding      //12 bytes total \n{ \n   float    var2;          //4 bytes \n   int      var4;          //4 bytes \n   char     var1;          //1 byte \n   char     var3;          //1 byte \n   char     var5;          //1 byte \n}; \n \nThe following is a struct packing checklist for quick reference: \n \nAre numbers represented as the smallest reasonable data type? \no \nUse a float instead of a double? \no \nUse an 8-bit or 16-bit integer instead of an int? \no \nUse an 8-bit or 16-bit integer instead of a float (lose some precision)? \no \nUse the minimal bits necessary to represent the highest number? \n \nCan a char array be converted to a pointer and the string stored elsewhere? \n \nCan a pointer be converted to an index? \n \nAre all Booleans converted to a single bit? \n \nAre all enumerations converted to the range of numbers needed? \n \nAre the data types ordered from largest to smallest to reduce padding? \nCompile for Size \nCode is data! The smaller your code is, the more code will persist in the L1 instruction cache \nand in the shared instruction/data L2 cache. Since both code and data compete for L2 \ncache, smaller code also helps keep data in the cache. \nAll compilers offer the ability to optimize the compiled code for either speed or size. By \ncompiling for speed, small functions will be inlined, and loops will be unrolled (which will \nbloat the size of the code). If you compile for size, then compact code will be favored. Since \nit‘s not clear which option will make your game run the fastest, you‘ll need to profile each \noption. It also may be the case that you want to compile some parts for speed and others \nfor size. \n \nOrganize the Data \nThe second main strategy is to better organize your data to be cache-conscious. This means \ncreating contiguous data structures (rather than scattered around memory) and grouping \nfrequently used data together, away from infrequently used data. \nPrefer Compact Contiguous Containers \nNode-based containers, such as linked lists, hurt cache performance in two ways. The first \nis that they waste space by storing pointers to the next node, thus bloating the data \nstructure. The second is that they allow the container nodes to be scattered around \nmemory, hurting spatial locality [Isensee06]. A much more efficient data structure is either \nan array, an STL vector, or an STL deque, where the data is stored sequentially without the \n",
      "content_length": 2528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": " \n \nuse of pointers. If you are using STL node–based containers, be sure to allocate new nodes \nfrom a dedicated heap to maintain spatial locality. \nSeparate Hot and Cold Data \nWhile object-oriented programming and encapsulation lead to effective organization and \ncomprehension, they can also lead to inefficient cache use. Within each struct or class, \nsome members are referenced often by the code, and others are referenced infrequently. \nIdeally, all of the hot data (data that is used often) is placed together and is separate from \nthe cold data (data that is seldom used) [Ericson03, Kaeli01, Franz98]. This would result in \nbetter cache utilization since the hot data is adjacent and more likely to be in the cache. \nHowever, it‘s not enough to just separate hot and cold data within a struct or class. \nConsider what happens with an array of structs that have both hot and cold data. The \nproblem that arises is that the cold data causes gaps between the groups of hot data, as in \nthe left side of Figure 4.4.1. \nFigure 4.4.1. On the left, an array of structs containing a mix of hot and cold data. \nThe middle image shows splitting the hot and cold data for each struct, with a \npointer linking data from the same struct. The right image is slightly more efficient \nsince the pointers are eliminated and struct correspondence is implicit in the array \nindex. \n \nClearly, we need to further distill the hot and cold data even between structs or classes. In \norder to preserve encapsulation, one solution is to keep the hot data within the data \nstructure, but then reference the cold data with a pointer (with the cold data living in some \nother part of memory). This is shown in the middle image of Figure 4.4.1. \nThe most cache-efficient solution would be to further weaken encapsulation and eliminate \nthe extra pointer. In this scheme, there are two corresponding arrays: one holding the hot \ndata and another holding the cold data. The link between the hot and cold data is \nmaintained implicitly by the array index. For example, if a particular struct was originally \nstored in buffer[2], then the hot data is now stored in hot[2], and the cold data is stored in \ncold[2]. This is shown in the right image of Figure 4.4.1. \n \nManipulate the Cache \nSo far the optimizations have been centered around being cache-conscious and playing nice \nwith the cache. The following two optimizations are more direct and attempt to directly \nmanipulate the cache to our benefit. \n",
      "content_length": 2483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": " \n \nPrefetch Data \nSince the CPU will spin while waiting for data, it can be advantageous to prefetch data so \nthat it‘s in the cache when the CPU is ready to use it. Some CPUs have specific prefetch \ninstructions, but if these aren‘t available, you might have to cleverly prefetch the data \nyourself. The following code performs software prefetching for the next four array elements \nin a loop [Ericson03]. \nfor (int i = 0; i < 4 * n; i += 4) \n{ \n   Touch(array[i + 4]);   //Forces prefetch of memory \n   Process(array[i + 0]); \n   Process(array[i + 1]); \n   Process(array[i + 2]); \n   Process(array[i + 3]); \n} \n \nWhen prefetching data, timing is of the essence. You must not get the data too early, since \nit could be evicted from the cache before it‘s ever used. Conversely, you must not do it too \nlate, since it might not be ready in time. Verifying with a profiler is the only surefire way to \nconfirm that prefetching is having an effect. \nLock the Cache \nTo maximally use the cache, some consoles, such as the Wii, allow a portion of the cache to \nbe locked and directly managed by the game. (Data must be manually moved in and out.) \nThis can be extremely effective, since you are guaranteed to have particular data in the \ncache. However, this is console dependent, so check with your target platform to see \nwhether this is available. General-purpose computers, such as PCs, do not allow the cache \nto be locked, because this would interfere with other processes. \n \nConclusion \nOptimization in games is about eliminating wasted cycles. With CPUs chronically waiting on \nmemory access, waste comes in several forms: from waiting for bloated data structures \nfrom main memory, not using everything that is read into the cache, and redundantly \npulling the same data into the cache. These things can be avoided by shrinking your data \nstructures and organizing them better to be cache-conscious. Lastly, you can manipulate \nthe cache through prefetching and potentially locking the cache. \nHowever, as with any optimization work, ensure that you‘re spending time improving actual \nbottlenecks. Only a profiler and performance counters can give you a good idea of where to \nconcentrate your efforts. Finally, measure and compare improvements with a profiler to \nensure that you‘re making a tangible difference, resulting in the entire game running faster \nand not just the code you modified. \n \nReferences \n[AMD09] AMD. ―Key Architectural Features AMD Athlon™ X2 Dual-Core Processors.‖ 2009. \nAdvanced Micro Devices. n.d. \n",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": " \n \n<http://www.amd.com/us/products/desktop/processors/athlon-x2/Pages/amd-athlon-x2-\ndual-core-processors-key-architectural-features.aspx>. \n[Bell08] Bell, Brandon. ―Intel Core i7 (Nehalem) Performance Preview.‖ 2008. Firing Squad. \nn.d. \n<http://www.firingsquad.com/hardware/intel_core_i7_nehalem_performance_preview/>. \n[Ericson03] Ericson, Christer. ―Memory Optimization.‖ Game Developers Conference. 2003. \nSony Computer Entertainment. n.d. \n<http://www.research.scea.com/research/pdfs/GDC2003_Memory_Optimization_18Mar03.p\ndf>. \n[Franz98] Franz, Michael and Thomas Kister. ―Splitting Data Objects to Increase Cache \nUtilization.‖ 1998. Technical Report – University of California-Department of Information \nand Computer Science. n.d. <http://www.ics.uci.edu/~franz/Site/pubs-pdf/ICS-TR-98-\n34.pdf>. \n[Isensee06] Isensee, Pete. ―C++ on Next-Gen Consoles: Effective Code for New \nArchitectures.‖ 2006. Game Developers Conference. n.d. \n<https://www.cmpevents.com/sessions/GD/S1549i1.ppt>. \n[Kaeli01] Kaeli, David. ―Profile-Guided Instruction and Data Memory Layout.‖ 2001. \nNortheastern University Computer Architecture Research Laboratory. n.d. \n<http://www.ece.neu.edu/groups/nucar/publications/Tufts.pdf>. \n[Lanterman07] Lanterman, Aaron. ―Architectural Comparison: Xbox 360 vs. PlayStation 3.‖ \n2007. Georgia Institute of Technology. n.d. \n<http://users.ece.gatech.edu/~lanterma/mpg/ece4893_xbox360_vs_ps3_4up.pdf>. \n[Shimpi09] Shimpi, Anand. ―The iPhone 3GS Hardware Exposed & Analyzed.‖ 2009. \nAnandTech. n.d. <http://www.anandtech.com/gadgets/showdoc.aspx?i=3579>. \n[Wikipedia09] Wikipedia. ―Broadway (microprocessor).‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Broadway_microprocessor>. \n \n4.5. Stack Allocation \nMichael Dailly \nmike@dailly.org \nPerformance in games has always been important—and fun! Indeed, it‘s what brings many \npeople into our profession in the first place, and it‘s always fun looking for simple new ways \nof speeding up our code. One of the best places to optimize has always been in the center \nof a tight loop, where every cycle counts. Although this doesn‘t happen as much as it used \nto, if you‘re dealing with anything that allocates, speed is almost always important. This \ngem describes a method of allocation that allows you to shave valuable cycles off of your \nallocator, yet is frighteningly simple to follow and implement. \nOverview \nThe standard way of doing rapid allocation is to use a linked list of sorts, either single or \ndoubly linked. This allows you to get the next free element quickly, and all you need to do is \nmaintain the links. However, when dealing with linked lists (particularly doubly linked lists), \n",
      "content_length": 2681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": " \n \nyou can end up touching more memory than you want, and thus incur cache misses and \ndelays. The code is also longer than we‘d all like, and in these days of 64-bit address \npointers, you can end up burning more memory than you really need to. So what‘s the \nanswer? Well, you could use indices instead of pointers to do your linked list with, but they \ncan be even slower if you‘re not careful. \nSo, in steps the stack allocator. This system uses a pre-allocated list of items, with \narbitrary-sized indices—be that pointers, INTs, WORDs, BYTEs, or even BITs! But unlike a \nnormal list, we use a simple stack concept and position the stack pointer (SP) at the end. \nWe can then pop the next free item off the stack and return it with minimal fuss or code, \nwhile we push items onto the list to free them. \nFigure 4.5.1 illustrates a typical sequence where several objects are allocated and freed \n(where SP is the current stack pointer). \nFigure 4.5.1. Object allocation in action. \n \nThe ability to easily use varying sizes of indices, bits, pointers, or even POD types directly \nwithout having to really change the implementation is very powerful. But before looking at \nsome examples, let‘s look at how we would implement the code. \n \nExample Implementation \nWe‘ll start with a simple example implementation using a stack. But unlike a normal stack, \nyou pre-fill it with your object‘s pointers, indexes (INTs, SHORTs, BYTEs, or even bits), or \nany other kind of object handle. First, we need to create and initialize the array. In this \nexample we‘ll use basic pointer allocation. \n#define      MAX_OBJECT          10000 \n \nParticle*    Stack[MAX_OBJECT];            // Our object stack \nint          SP;                           // The Stack Pointer \nParticle     ParticleArray[MAX_OBJECT];    // The object pool \n \n// \n################################################################\n#### \n// Inititialise the stack with object indexes from 0 to \nMAX_OBJECT-1 \n// \n################################################################\n#### \n",
      "content_length": 2047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": " \n \nvoid InitStack( void ) \n{ \n      // Create our object list \n      // Pre-Fill stack with indexes (or pointers) \n      for(int i=0;i<MAX_OBJECT;i++){ \n            Stack[i] = &ParticleArray[i]; \n       } \n \n       // Initialise the stack pointer \n       SP = MAX_OBJECT; \n} \n \nAs you can see, creation is simply a matter of filling the array with object pointers and then \nsetting up a stack pointer. Next, allocation and releasing of a particle object: \n// \n################################################################\n###### \n//  Popa particle from the free  pool. \n//  -1 is returned if there are no free particles left. \n// \n################################################################\n###### \nParticle *  Pop(void ) \n{ \n      ASSERT( SP>=0, \"Error: Stack pointer has gone negative!\"); \n      if( SP==0 ) return -1; \n      return pStack[—SP]; \n} \n \n// \n################################################################\n### \n// Push a used particle back onto the free pool \n// \n################################################################\n### \nvoid Push(Particle* _pParticle ) \n{ \n      ASSERT( SP>=0, \"Error: Stack pointer has gone negative\"); \n      ASSERT( SP!= MAX_OBJECT, \"Error: Not enough space in \nstack. Object \nfreed twice?\"); \n      pStack[SP++] = _pParticle; \n} \n \nAllocating a particle is simply a matter of calling Pop(), while releasing it only requires you \nto call Push(). Again, not rocket science. Of course, these functions could just as well be \ncalled Alloc() and Free(), but for the sake of clarity when dealing with a stack method, \nwe‘ll stick with Push() and Pop(). \n",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": " \n \nYou can find code for a template based on this method on the accompanying CD \n(TemplateExample_V1). \n \nIndex-Based Implementation \nLet‘s now look at another couple of possible uses. The most obvious one is to allocate an \nindex as a handle to an object rather than a pointer to the object. In this example, we have \n256 sprites that we wish to allocate from, and rather than allocating an object and returning \na whole pointer, we‘ll simply allocate and return the BYTE index, thereby saving memory. \n#define           MAX_SPRITES        256 \n \nunsigned char     Stack[MAX_SPRITES]; \nSprite            SpritePool[MAX_SPRITES]; \n \n// \n################################################################\n#### \n// Inititialise the stack with object indexes from 0 to \nMAX_OBJECT-1 \n// \n################################################################\n#### \nvoid InitStack( void ) \n{ \n       // Pre-Fill stack with ready to use particles. \n       for(int i=0;i<MAX_SPRITES;i++){ \n             Stack[i] = (unsigned char) i; \n       } \n \n       // Initialise the stack pointer \n       SP = MAX_ SPRITES; \n} \n \nHere you can see that by simply replacing the Particle* with a sprite index, we‘ve \nchanged the allocation type. This is important because it opens up all manner of interesting \ntricks. In the above example, we knew that we were only ever going to have 256 sprites, so \nwe were able to allocate an index and keep our memory footprint down. The calling class \ncan then store the index and use it as a handle directly, or at the very least take the \naddress of &SpritePool[index] just before using it. This obviously saves more storage \nwith only minimal effort on the coder‘s part. \nThe CD holds code for another example (TemplateExample_V2), based on the index \nmethod. \nHow about taking an extreme case of 16 million objects (a full 24-bit number). This would \nnormally be hard to allocate, as you would normally either opt for a full INT index or use a \n1D linked list. However, using the stack method, you can easily store 3 bytes per entry, \neither by working out the byte offset yourself or by storing two tables of a byte and a short, \nthereby saving you 16 MB of data. So instead of 64 MB of memory required for rapid \nallocation, you can quickly and easily reduce it down to 48 MB. \n",
      "content_length": 2296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": " \n \nTaking this further, there‘s nothing to stop you from doing bit allocation. That is allocation \nof (say) 18 bits of information. Normally, you would again have to round this up to an INT \nso that you can easily deal with the number, but in this case you can easily deal with \nmultiple tables, one holding a short and another holding the leftover bits. Due to the nature \nof the Push/Pop, bit allocation can be processed fairly simply, either by masking and shifting \ndown or by assuming the next 2 bits are the lowest two in the INT and shifting them off as \nneeded. \nLet‘s see how this could be achieved. First, we need to allocate the arrays and create the \nindex list. \nconst   int                             MAX_OBJ = 0x40000; \nunsigned   short                        Stack_short[MAX_OBJ]; \nunsigned   int                          Stack_bits[MAX_OBJ/16]; \nint                                     SP; \n \n//  \n################################################################\n###### \n///  Function:<summary> \n///              Initialise  the free list \n///           </summary> \n//  \n################################################################\n###### \nvoid   Init( void ) \n{ \n       // Create and fill our 18bit table. \n       // Use Push to build the table as it's easier in this \ncase. \n       SP = 0; \n       for(int i=0;i<MAX_OBJ; i++){ \n             Push(i); \n       } \n} \n \nIn this case, we‘ll use Push to actually create the buffer, as it‘s far easier. This will create a \nlist of indices from 0 right up to 0x3ffff (262,143). \nNext we need to create the Push command itself. \n// \n################################################################\n#### \n/// Function:<summary> \n///             Push an 18bit index onto the object stack \n///          </summary> \n/// \n/// In:      <param name=\"_value\">18Bit number to store</param> \n// \n################################################################\n#### \nvoid Push( int _value) \n",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": " \n \n{ \n      assert(_value>=0); \n      assert(_value<0x40000); \n      assert(SP<MAX_OBJ); \n \n      Stack_short[SP] = (unsigned short)_value&0xffff; \n \n      // Clear the bits we're about to OR into. \n      int shift = (SP&0xf)<<1; \n      int index = SP>>4; \n      Stack_bits[index] &= ~(3<<shift); \n      Stack_bits[index] |= ((_value>>16)&0x3) << shift; \n      SP++; \n} \n \nWhile much slower than a linked list or using the stack with a straight array of pointers, it‘s \nobviously written for storage efficiency, in this case saving around 458 K compared to \nstoring the whole 32-bit pointer. Next, allocation... \n//  \n################################################################\n###### \n///  Function:<summary> \n///                Return the next free index (an 18bit number) \n///           </summary> \n/// \n///  Out :    <returns> \n///                Return the next free 18bit index, or -1 for \nan error. \n///           </returns> \n// \n################################################################\n###### \nint   Pop( void ) \n{ \n      // If none left, then return an error \n      if( SP==0) return -1; \n \n      // Get the main 16bits \n      int val = Stack_short[—SP]; \n \n      // Now OR in the extra bits we need to make up the 18bit \nindex. \n      val |= ( (Stack_bits[SP>>4]>>((SP&0xf)<<1)) &0x3)<<16; \n \n      return val; \n} \n \nThe ease and speed at which you can adapt the stack allocation system to your needs is a \nreal strength, and the ability to rapidly allocate not only pointers, BYTEs, SHORTs, and INTs, \nbut also groups of bits efficiently is always going to be a winner. \n",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": " \n \nUsing this system, you can compress your data right down into as few bits as required \nwithout sacrificing all your speed. There are always better ways to compress data down, but \nthis method often allows you to strike a middle ground in a matter of minutes, not days. \nThese are obviously extreme cases, but when trying to save memory, these types of options \naren‘t usually open to you, so you usually end up either not bothering or wasting valuable \ntime on far more complex methods; after all, how else could you easily allocate using 18-bit \nindices? It‘s normally not such a straightforward problem. \nThe example 18 BitAlloc can also be found on the CD. \n \nPOD Types \nLastly, we‘ll show how to use POD types for more complex indexing requirements. For those \nwho don‘t know, POD stands for Plain Old Data. POD types can store whole structures as \nlong as they fit inside a standard native type, such as an INT, SHORT, or BYTE. Here‘s an \nexample of a simple POD type. \nstruct STileCoordinate \n{ \n      unsigned char    X;    // 64x64 X tile coordinate \n      unsigned char    Y;    // 64x64 Y tile coordinate \n      short            page; // Texture page to use. (also \npadding) \n}; \n \nThis POD stores several pieces of pre-generated information—in this case, a 64× 64 tile \ncoordinate inside multiple 4096× 4096 texture pages. First, let‘s see how you would \nnormally deal with them. \nclass TileCoordinate \n{ \n      STileCoordinate* m_pNext;   // Next in the list \n      unsigned char    m_X;       // 64x64 X tile coordinate \n      unsigned char    m_Y;       // 64x64 Y tile coordinate \n      short            m_page;    // Texture page to use. \n}; \n \nIf we had 10 texture pages, this would mean 40,960 tiles we need to allocate from. At 8 \nbytes per tile (the m_page member is padded for better alignment), this makes 327,680 \nbytes. Calling a standard allocator would then return a pointer to this structure, allowing us \nto use it as an origin for copying tile data into the free space. \nSo how can we do better? First, let‘s remove the pointer, as we know the stack system \ndoesn‘t need that (or rather stores it outside the object itself). This leaves us with a 4-byte \nstructure, and while we could use the very first example to allocate using a pointer to the \nstructure (and saving no extra memory), we can do better. \nSince this data fits inside 4 bytes, we can actually allocate and return the whole structure‘s \ndata directly without the need for pointers. Taking the STileCoordinate type, we can \nactually make an array of these that is the same amount of memory as an array of INTs \n",
      "content_length": 2609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": " \n \n(since the whole structure is 4 bytes long). Now modern compilers will recognize this and \nallow us to pass around this data inside a register; that is, the whole structure is copied \naround without the need for pointers or expensive memcpy() operations. In fact, this is \ncopied around at the same speed as a standard integer number. \nNow our Pop() command doesn‘t return a pointer, but a structure, like so: \n// \n################################################################\n###### \n/// Function:<summary> \n///               Return the a free tile structure \n///          </summary> \n/// \n/// Out:     <returns> \n///               Return the next free tile structure, or an \nerror tile. \n///          </returns> \n// \n################################################################\n###### \nSTileCoordinate   Pop( void ) \n{ \n      // If none left, then return an error \n      if( SP==0) return EmptyTile; \n \n      // Get the main 16bits \n      return TileStack[—SP]; \n} \n \nWe need to pre-define what an empty tile is (probably just –1 for X,Y and page), but aside \nfrom that, we get the data passed to us directly. You can then store this as an \nSTileCoordinate (without the need for a pointer) and save yourself 163,840 bytes of \nmemory in the process. \nThe beauty of the stack allocator is that it doesn‘t care what it returns; it will return pretty \nmuch anything to you, and it‘s up to you to decide what that anything is. You could just as \neasily define the type as plain UNSIGNED INT and mask out bits of information as you need \nto, rather than relying on bytes or shorts to make things fit. \nFor example, we could have assigned 12 bits for the X and Y coordinates and 8 for the page \nnumber, which would allow us to specify an actual pixel coordinate inside the 4096× 4096 \ntexture if we really wanted to. The important thing to remember is that the stack system \nreturns data quickly, and that data doesn‘t have to be a pointer to some data or even a \nhandle to data! In fact, it can be an entire structure if you wanted it to be. POD types are \nbest because there‘s no special copying as they fit inside a register, but any type could be \nused. \nYou can find the POD example on the CD in the PODAlloc folder. \n \nKnown Issues \n",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": " \n \nOne of the few drawbacks to the stack allocation method is that you currently can‘t do it \natomically, and so it‘s not automatically thread-safe. This means you must manage \nmultiple-thread access yourself using all the standard lock methods normally at your \ndisposal or use per-thread stacks. \nYou also can‘t put any limits on the allocation; that is, you can‘t search through the whole \nlist and allocate one that better suits your needs—not without manually compacting the \nremain elements in the list, that is. If this is exceptional, then it might be worth occasionally \ncompressing the list, but chances are if you need to do this, then you shouldn‘t use this \nmethod. \nThe only other real disadvantage is that there‘s no easy way to keep a used list. In a doubly \nlinked list, you can move an item from the free to the used list, and this can help you debug \nthings because it‘s easy to see what‘s in use. In this system, it‘s very much a free-only list. \nThis doesn‘t mean you can‘t maintain debug tables or lists, but it does mean it‘s not a \nnatural part of the system. \n \nAdvantages and Disadvantages \nFirst, the advantages: \n1. The code is microscopic, so small it can easily be placed inline. The compiler will \noften do this anyway. \n2. Since all the objects‘ pointers or indices are gathered next to each other, they are \nvery cache-friendly. In fact, you‘ll get multiple items per cache line, reducing cache \nmisses to a bare minimum, particularly if you‘re allocating one after the other. \n3. You can allocate a number of any type and of any bit size, from only a few bits to \nlonger, more complex streams, and you can do so almost as easily as any other \nvalue or item. \n4. You can allocate POD types (or bit-packed data) directly and rapidly. This saves on \nhaving to wrap simple types or include pointers through them, thereby increasing \ntheir size. \n5. You don‘t have to modify any object to put links through it or have wrapper objects \nor templates to link things together. This simplifies your code. \n6. It requires either the same or less memory than a 1D linked list system. \n7. Allocation time is constant no matter how many elements you‘re allocating from. \n8. Block allocation is not only possible, but very simple and quick. Depending on use, \nyou could even return a pointer into the stack that holds the allocated block so you \ndon‘t even have to allocate an array to return. \n9. It‘s fast and easy to implement, even on severely restricted CPUs, and in any \nlanguage. It works just as well on an old 6502 as it does on a modern PC. \n10. It‘s very simple to debug, since you don‘t have to follow links all over the place. \nNext, the disadvantages: \n1. You can‘t easily remove items from the middle of the stack. This means it‘s hard to \nkeep an allocated list as well as a free list. \n2. It‘s not automatically thread-safe. \n3. You can‘t easily place any restrictions on allocation without severely affecting \nperformance. \n \nConclusion \n",
      "content_length": 2976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": " \n \nThis system has been used to allocate a single bullet from 20, sprites for rendering, \nscripting slots, blocks of memory, particles—you name it. The ease in setting up and using, \nnot to mention the fact that debugging is simply a matter of viewing the stack, makes it \nsimple for any level of ability to implement. Also, on modern hardware, with its relatively \nslow memory and cache lines, the cache coherency is an added bonus that‘s particularly \nuseful when dealing with thousands (or even tens of thousands) of allocations in a single \ngame cycle. Memory bandwidth is the ultimate enemy when dealing with large numbers of \nobjects in a game, and being able to reduce the size of an object handle to an index and \ngroup them into the same cache line can be a big win. Finally, it‘s fast, simple, easy to \nimplement, usually uses less memory than most other methods, and is very easy to adapt \nas your needs change! \n \n4.6. Design and Implementation of an In-Game Memory Profiler \nRicky Lung \nmtlung@gmail.com \nThis gem introduces an architecture and implementation for a low-overhead in-game \nmemory profiler with multi-threading support. With this profiler, one can examine the \nmemory allocation distribution of a running game in a call-stack-style table in real time. \nThese statistics can be extremely valuable for performing memory usage optimization and \nmemory leak tracking. \nIntroduction \nMemory consumption management is one of the cornerstones of technical quality for \ngames. Developers try their best to fit as much content as possible into a finite memory \nresource. We all know that choosing the right tool to do any kind of performance tuning is \ncrucial, as it provides solid data instead of guesswork. Strangely, there is only a very limited \nnumber of memory profiling tools available for C++ in both the open-source and \ncommercial worlds. This gem tries to narrow this gap by providing a lightweight memory \nprofiling library. Now, let us have a look at what it will provide. \nThe profiler can generate a call graph at a given time interval, as shown in Figure 4.6.1. \nEach column of the table reveals useful information: \n \nName. Mimics the program structure as a call stack, where the name of the function \ncomes from the user-supplied string literal. \n \nTCount. Total number of allocations currently made in the calling function and its \nchild call. \n \nSCount. Number of allocations currently made in the calling function without \ncounting its child call. \n \nTkBytes. Total amount of memory currently allocated in the calling function and its \nchild call, in units of kilobytes. \n \nSkBytes. Amount of memory currently allocated in the calling function. If you are \nlooking for the memory eater, this column might be the first place to look. \n \nSCount / F. Number of allocations performed by this function per frame. You might \nwant to reduce this number such that less run-time overhead of your game is spent \non memory allocation/deallocation. \n \nCall / F. Tells you how many times a function is invoked in a single rendering frame. \nFigure 4.6.1. A view of the memory profiling remote client. \n",
      "content_length": 3129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": " \n \n \n \nMemory Profiling Basics \nThe memory profiling mechanism can be divided into three main parts: collect memory \nallocation information, relate the collected information with the program structure, and, \nfinally, present the results. At first glance, the technique we use in CPU profiling can be \napplied: taking a measure at the beginning of a code block of interest and again at the end, \nminus their difference. But the challenge is yet to come, because memory allocated within a \ncode block can be deallocated somewhere else. Therefore, instead of collecting information \nfor a single code block at a time, we turn the problem into asking which corresponding code \nblock the current memory operation is related to by intercepting every memory allocation \noperation. \nThe remaining sections of this gem will discuss how to utilize function hooking to intercept \nallocation operations and how to get their corresponding call-stack information and \nstatistics. Of course, in this multi-core era, we will add support for multi-threaded \napplications. \n \nFunction Hooking \nOne simple way of intercepting memory allocation is defining your own new/delete operator \nin C++, but it has the limitation of not being able to intercept code that you don‘t have \nsource code access to. Instead, we use a more low-level approach called function hooking, \nwhich is extremely powerful. \nThe term ―hooking‖ in computer programming covers a range of techniques used to alter or \naugment the behavior of an application. Some applications, such as DirectX/OpenGL \ndebuggers and profilers, use such a technique. Although it can be very complicated to hook \nother processes‘ functions as those standalone profiling applications do, in this memory \n",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": " \n \nprofiler we only need to do some patching on the memory that a function is resident in. \nAfter the patching is done on a function—for example, the malloc function—whenever \nanywhere else in the program tries to invoke it, a proxy function myHookedMalloc is \ninvoked instead of the original one. The patching process needs a few assembly tricks, so \nlet‘s get our hands dirty and examine what the assembly of the malloc function looks like \nunder x86 first. \nvoid * __cdecl malloc(size_t size) { \n    78583D3F mov    edi,edi \n    78583D41 push   ebp \n    78583D42 mov    ebp,esp \n    78583D44 push   esi \n    ... \n \nThe above assembly shows the first four assembly instructions of the malloc function, \nwhich prepare the stack and register for use with that function, commonly called the \nfunction prologue. We will replace the first instruction with a jump instruction. \nvoid * __cdecl malloc(size_t size) { \n78583D3F jmp    myHookedMalloc \n78583D41 push   ebp \n78583D42 mov    ebp,esp \n78583D44 push   esi \n... \n \nWith the unconditional jump instruction added to the very beginning of the malloc \nfunction, all control flow will be directly transferred to our proxy function. \nvoid* myHookedMalloc(size_t size) { \n    void* p = (*originalMalloc)(size); \n    logMallocUsage(p, size); \n    return p; \n} \n \nWhat the proxy function does is simple: It invokes the original malloc function to perform \nthe actual allocation and log the memory size along with the allocated memory pointer for \nfurther processing. We then need to resolve the problem of how we can invoke the original \nmalloc function. To get back the original function, we need to back up the first assembly \ninstruction before we perform the patching and store it in an executable memory location. \n(Note that you can make a block of memory executable on the Windows platform using \nVirtualProtect.) Following that memory location, put a jump instruction pointing to the \nsecond instruction of the original function. \nmov        edi,edi \njmp        &malloc + sizeof(mov   edi,edi)   = 78583D41 \n \nWe give this block of executable memory a meaningful name, originalMalloc, with the \nsame function signature as malloc does. These backup instructions can also be utilized for \nrestoring the patched function when the memory profile gets shut down. One particular \nissue that we haven‘t addressed is how to get the size of a binary assembly instruction, \n",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": " \n \nwhich is necessary to perform the replace and copy operations correctly. A easy way to \ntackle this problem is to simply hardcode the instruction size, but this will create \nmaintenance problems because different C run times may have different compiled binary \ncode, even for the same function. A better solution is to calculate it at run time. Luckily, \nthere are free libraries that can do the task, such as libdasm. \nAll the aforementioned details of function hooking are encapsulated into a single class with \nonly three functions, which can be found on the CD-ROM. \nclass FunctionPatcher { \n    void* copyPrologue(void* func, int givenPrologueSize); \n    void patch(void* func, void* replacement); \n    void unPatchAll(); \n} \n \nAgain, using malloc as the example, the usage of the class is as follows. \nvoid (*origianlMalloc)(size_t); \nFunctionPatcher functionPatcher; \norigianlMalloc = functionPatcher.copyPrologue(&::malloc, 5); \nfuncitonPatcher.patch(&malloc, &myMalloc); \n \nAlthough all the source code and example here only work under x86, the same concept \nshould be able to be applied to other platforms, including game consoles. \n \nCall-Stack Generation \nIn the current generation of software architecture, most programs are built on a command \nand control scheme: One method calls another method and instructs it to perform some \naction. The place that stores the nested method call information is the call stack. From a \nlow-level point of view, it is just a simple memory block with some integer offsets giving the \nstack size; while at a higher-level point of view, we can use a graph structure to represent \nthe call stack. (The terms call graph and call stack can be used interchangeably.) \nTo collect the full call stack of a running program, a stack-walker [Gaurav08] with debug \nsymbol information can be used. However, generating a full call stack is not only time \nconsuming, but it also clutters the profiling results. Thus, a custom function call annotation \nscheme inspired by the previous Gems articles [Rabin00] and [Hjeistrom02] is used instead, \nwhich gives users a more flexible way to collect statistics for their most important functions. \nThose applications that already use similar profiling tools can easily integrate with this \nmemory profiler by combining the scope variable/macro. \nWe use a tree-like structure in C++ to mimic the actual call-graph structure. A tree is used \ninstead of a graph because we will accumulate the profiling data recursively instead of \nmaking more general node connections. This tree structure contains a number of nodes, \nwhile each of these nodes contains the necessary memory profiling statistic and the link to \nits sub-nodes. Obviously, the root node of the tree represents the entry point of the \nprogram, usually the main function. More nodes can be added to the tree on user‘s request, \nthrough declaring a scope variable. \nvoid createMesh() { \n",
      "content_length": 2926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": " \n \n    ProfilingScope scope(\"createMesh\"); \n... \n} \n \nSuppose Function A calls Function B, and both functions have a scope variable declared. \nThen a node named B will be inserted as a child of the node named A. The next time \nFunction A invokes B, a linear search for all the child nodes under A with the name B is \nperformed. This implementation assumes that the name argument of the scope variable \nmust be a string literal; therefore, we can use a simple pointer comparison instead of a full-\nblown string comparison during searching. Provided that the average number of child node \nis around the order of 10, the call-stack generation process is reasonably fast. \nFor the hooked memory allocation function to make use of the call-stack information, the \ncall tree will store a variable that points to the current call-stack node. Thus, a connection \nbetween the current allocation and the current call stack is made. \n \nCollecting Statistics \nOnce we have the call-stack data structure on hand, we can start to collect statistics. Only a \nhandful of statistics need to be stored in each node: \n \nInvocation count \n \nExclusive allocation count \n \nExclusive allocation size \nThe exclusive allocation count and size will be updated whenever the hooked functions are \ninvoked. Apart from simply updating the statistics, the hooked functions need to perform \nsome bookkeeping in order to retrieve the corresponding call-stack node for a particular \nallocated memory block during the deallocation operation. \nThe mapping between the two can be established using an STL map, but this results in a \nhigh run time and memory overhead; therefore, we use the approach of embedding the \ncall-stack node pointer with the allocated memory block. \nvoid* myHookedMalloc(size_t size) { \n    void* p = (*originalMalloc)(sizeof(Node*)+sizeof(int)+size); \n     (Node*)p = currentCallStackNode; \n    *(int*)((char*)p + sizeof(Node*)) = size; \n    currentCallStackNode->allocCount++; \n    currentCallStackNode->byte += size; \n    return (char*)p+sizeof(Node*)+sizeof(int); \n} \nvoid myHookedFree(void* p) { \n    int allocSize = *(int*)((char*)p-sizeof(int)); \n    p = ((char*)p-sizeof(Node*)-sizeof(int)); \n    Node* n = (Node*)p; \n    n->allocCount—; \n    n->byte -= allocSize; \n     (*originalFree)(p); \n} \n \n",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": " \n \nOther statistics that take into account the child function calls can be calculated on the fly \nduring profile report generation, because this step needs to traverse the call tree anyway. \n \nMulti-Thread Issues \nBy definition, each executing thread should have its own call stack. This means every call of \nmyHookMalloc can access the per-thread call-stack data structure freely without any \nproblem. Thread Local Storage (TLS) comes to assistance here. For each thread, we store \nthe pointer to its corresponding call-stack root node and another pointer to the current node \nusing the win32 API TlsSetValue. This data will be retrieved by myHookedMalloc \nusing TlsGetValue. Care has to be taken in implementing myHookedFree because it \nmay operate on a call-stack node that doesn‘t belong to the current thread. \nTo protect the deallocation routine from race conditions, each thread‘s root node is assigned \na critical section, and all its ancestor nodes will keep a reference to it. When \nmyHookedMalloc or myHookedFree is invoked, the critical section of that node will be \nlocked before any statistic information update. Such a design keeps the number of critical \nsections to a minimum while preserving a very low lock contention level; moreover, it does \nnot create a locking hierarchy, so it should be free from dead-lock. \n \nThe Source Code \nThe source code on the CD comes with Microsoft Visual Studio solutions that allow you to \ncompile and execute several demo programs related to the memory profiler. Although the \ncode snippets in this gem are simplified for illustration purposes, the code on the CD is \ncarefully crafted for maximum correctness and robustness. A client-to-server architecture is \nalso employed in one of the demos, which shows you how to make an external tool to \nmonitor the memory usage of your application. \nA CPU profiler based on the same code base as the memory profiler is also supplied as a \nreference. \n \nConclusion \nBy moving a step forward, our in-game CPU profiling techniques can be extended to \nmeasure memory usage as well. This gem also demonstrated how to modify the profiler to \ncope with the multi-core era. We hope you will utilize this profiler to develop a greater \nunderstanding of your program, locate memory hotspots, and improve your memory \nconsumption and performance. \n \nReferences \n[Gaurav08] Kumar, Gaurav. ―Authoring Stack Walker for X86.‖ 6 Jan. 2008. WinToolZone. \nn.d. <http://www.wintoolzone.com/PermaLink.aspx?ID=141>. \n[Rabin00] Rabin, Steve. Game Programming Gems. Boston: Charles River Media, 2000. \n",
      "content_length": 2576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": " \n \n[Hjeistrom02] Hjeistrom, Greg, and Byon Garrabrant. Game Programming Gems. Boston: \nCharles River Media, 2002. \n \n4.7. A More Informative Error Log Generator \nJ.L. Raza and Peter Iliev Jr. \njraza@versus-software.com, pete.iliev@gmail.com \nProgrammers commonly create error logging functions to aid in debugging during \ndevelopment. Usually, these functions print out error messages onscreen or to a log file. \nAlthough it is functional, the error message is only as good as the programmer‘s insight into \nhow the problem could have occurred. In other words, a one-line message is not always \nenough to help fix the bug. You may need to know which function passed in bad data or \nhow the program got into this bad state, and that is what this gem seeks to help with. \nIn this gem, we‘ll present a new error logging function that automatically generates useful \ninformation for the programmer and/or tester via what is known as retrieving the Run-Time \nStack Information (RTSI). \nDefinition of RTSI \nRetrieving the RTSI is a debugging tool conventionally used to aid in software development. \nDuring debugging, stack walking is the act of pausing the program and taking a glimpse into \nthe program‘s function stack at run time. For example, given the following code: \nvoid C() \n{ \n      int i = 0;//set a breakpoint here \n} \n \nvoid B() \n{ \n      C() ; \n} \n \nvoid A() \n{ \n        B() ; \n } \n \nint   main() \n{ \n        A() ; \n        ... \n        return 1 ; \n} \n \nWhen the compiler reaches the breakpoint in function C(), and the programmer requests \nthe program‘s run-time stack information, it would return something like this: \n",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": " \n \nC \nB \nA \nMain \n... \n \nThe bottom of the stack shows the program‘s entry point. For simplicity, the operating \nsystem functions called have been omitted from this example. In this case, the entry point \nis main(). It then details what function main() called, A(), and recursively what \nfunction A() called, and so on until it reaches the current function where the breakpoint \nwas set. As the flow of execution on the program continues, retrieving the RTSI again will \nreturn a different result based on where the program‘s execution is paused. \n \nPotential Uses \nRTSI is a normal component in many commercial and open-source compilers. It can be \nuseful for tracking stack-overflow bugs, as well as taking a snapshot of the program‘s \ncurrent execution status once it hits a breakpoint. \nMost often, programmers will only use the RTSI inside their compiler. But when working in \ngame development teams, there are more people around than just programmers. There are \nartists, level designers, and testers. All of these people will be playing the game. Yet, not \neveryone may have access to a compiler or a debug development kit, and anyone can run \ninto a crash bug. Having the program dump out as much info as possible during these \nsituations is vital, and dumping out the RTSI can be a big help. \nThe RTSI‘s usefulness extends beyond just the work environment, too. Online games, such \nas MMORPGs or RTS games, do beta testing where they release the game to a select few \npeople in their homes and let them play the game before it‘s out. The main intent is \nbalancing and getting feedback. Beta testing is also valuable time that can be used for \nbulletproofing the game. So as not to waste the testing time, whenever bad data is \nencountered or a crash occurs, the RTSI can be sent from the tester‘s machine to a \ndeveloper-owned server for use later. \nC/C++ does not have a standard function to call to print the program‘s RTSI, so it‘s up to \nthe operating system and hardware vendors to distribute a set of functions in an API to \naccomplish this job. In this gem, we‘ll explore the API functions on the Windows XP platform \nfor gathering the RTSI on an x86 processor. The example will also work on x64 processors, \nbut some of the assembly code will need to change to reference the correct registers. \n \nSetting Up the Code \nSo you have decided that the RTSI is a resource that‘s going to be used in your game. \nHenceforth, it needs to go through the three basic steps any resource goes through: \nloading, usage, and unloading. \nLoading \nSince the RTSI resource will probably be used in several different spots in the game code, \nwe have the issue of when to load it, when to unload it, and how to handle access to it. All \n",
      "content_length": 2735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": " \n \nthese issues can be solved by setting it up as a singleton resource. For reference on \nsingletons, see [Bilas00]. To load our RTSI API function calls, we use the following code: \n//load the dbghelp.dll which is the windows library used \n//for crawling up the stack \nm_dllHandle = LoadLibrary( \"dbghelp.dll\" ); \n \n//sets up which processes symbols \n//we are going to look at \nm_pSymInitialize = (SymInitialize) GetProcAddress( \n      m_dllHandle, \"SymInitialize\" ); \n \n//used to crawl up the stack \n//and look at each function call \nm_pStackWalk64 = (StackWalk64) GetProcAddress( \n      m_dllHandle, \"StackWalk64\" ); \n \n//used to retrieve the line number in each file the \n//function was called from \nm_pSymGetLineFromAddr64 = (SymGetLineFromAddr64) GetProcAddress( \nm_dllHandle, \n\"SymGetLineFromAddr64\" ); \n \n//used to get the name of each function in string form \nm_pSymGetSymFromAddr64 = \n      (SymGetSymFromAddr64)GetProcAddress( \n             m_dllHandle, \"SymGetSymFromAddr64\" ); \n \n//get and set for the options of what information we want \n//to retrieve. \n//this example only cares for line numbers and \n//function names. \nm_pSymGetOptions = (SymGetOptions) GetProcAddress( \n       m_dllHandle, \"SymGetOptions\" ); \n \nm_pSymSetOptions = (SymSetOptions) GetProcAddress( \n       m_dllHandle, \"SymSetOptions\" ); \n \n \nUsage \nWe need to set up an access point to the RTSI with the required function pointers. We only \nneed to activate it when something is about to go wrong; this is what the ASSERT() call is \nfor. For reference on how to use that and a few handy tricks, see [Rabin00]. \nThis access point can be created as such: \n#if DEBUG \n      #define ASSERT( expression ) \\ \n            if ( !expression ) \\ \n            { \\ \n                  printf( \"ASSERT[\" #expression \"]\\n\" ); \\ \n",
      "content_length": 1797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": " \n \n                  AssertWrapper::a.PrintStack(); \\ \n                  printf( \"\\n\\nPress enter to continue:\\n\" );\\ \n                  getchar(); \\ \n            } \n \n#else \n       //If we aren't in a debug configuration then just compile \nout \n       //Asserts \n       #define ASSERT( expression ) \n#endif \n \nNow, when we call the ASSERT() macro and the result is false, we‘ll print out the RTSI. \nNotice that it‘s the PrintStack function inside the Assert class that does the interesting \nwork. There are a couple of important pieces of code in that function. The first is how we fill \nthe CONTEXT data structure: \nCONTEXT context; \nmemset( &context, 0, sizeof(CONTEXT)); \ncontext.ContextFlags = CONTEXT_FULL; \n \n__asm    call x \n__asm    x: pop eax \n__asm    mov context.Eip,  eax \n__asm    mov context.Ebp,  ebp \n__asm    mov context.Esp,  esp \n \nHere we are copying the instruction, frame, and stack pointers into Context. This will let \nthe StackWalk function know where to look for all the debugging info we need. Normally, \nyou would copy the instruction pointer directly (EIP), but on x86 processors there is no \ndirect way to access that pointer, so we use a trick. This section of code is processor family–\nspecific, so the register will probably be named differently for the different companies. For \nexample, x64 (AMD) uses Rip, Rbp, and Rsp. \nThe other important aspect of the PrintStack function is the loop that displays the \nfunctions on the run-time stack. \nunsigned int i = 0; \ndo \n{ \n      // get the next stack frame info \n      this-<m_pStackWalk64( IMAGE_FILE_MACHINE_I386, \n                             process, \n                             thread, \n                             &frame, \n                             (void*)&context, \n                             NULL, \n                             SymFunctionTableAccess64 , \n                             SymGetModuleBase64, \n                             NULL ); \n \n",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": " \n \n      //now we try to print the current frame's info \n      if ( frame.AddrReturn.Offset != 0 && i != 0 ) \n      { \n            char line1[32]; \n            char line2[32]; \n            memset( line1, '\\0', 32 ); \n            memset( line2, '\\0', 32 ); \n            PrintLineNumber( process, frame.AddrPC.Offset, line2 \n); \n            PrintFuncName( process, frame.AddrPC.Offset, line1 \n); \n            printf( \"%-32s || %s\\n\", line1, line2); \n      } \n      i++; \n} while( frame.AddrReturn.Offset != 0 ); \n \nThus, if we want to integrate the RTSI to a game GUI, then that is where we would make \nour changes. \nUnloading \nTo unload it, we perform the reverse of the loading steps. \nFreeLibrary( m_dllHandle ); \n \nm_pSymInitialize = NULL; \nm_pStackWalk64 = NULL; \nm_pSymGetLineFromAddr64 = NULL; \nm_pSymGetSymFromAddr64 = NULL; \nm_pSymGetOptions = NULL; \nm_pSymSetOptions = NULL; \n \n \n \nError Logs Redundancy Problem versus Using RTSI \nHaving explained the RTSI and how to implement it in a non-compiler environment, we can \nnow begin to exemplify scenarios in which this feature could be quite useful. It is common \nfor programmers to create a set of functions that display an error message when something \ngoes wrong during testing. The problem with a generic error log function is that the \nprogrammer has to keep track of where he is in the code in order for the error message to \ncreate any useful information. Following is an example of this common scenario. \nint A() \n{ \n      if   ( did_an_error_occur()   ) \n      { \n              assert( ! \"Problem with function A()\" ) ; \n              return 0 ; \n      } \n      else \n",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": " \n \n      { \n              ... \n              return 1 ; // All went well \n      } \n} \n \n \nint B() \n{ \n      if   ( did_an_error_occur   () ) \n      { \n              assert( ! \"Problem with function B()\" ) ; \n              return 0 ; \n      } \n      else \n      { \n              ... \n              return 1 ; // All went well \n      } \n} \n \nint C() \n{ \n      if( A() && B() ) \n             return 1 ; \n \n      return 0 ; \n} \n \n... \n \nThe problem with this design is that as the software evolves, so will its function calls and \nthe context in which these functions are called. Keeping track of those error messages adds \nextra overhead to the development of the game. This could be solved if the error log \nfunction could ―know‖ its current context and report accordingly, which is exactly what RTSI \ncan do. \nWith the given code sample, the programmer could then bind the error log to the in-game \nGUI. This means that once an error log function gets triggered, a tester can report to the \nprogrammer a wider span of technical information (since the log is intrinsically related to \nthe game‘s code) as well as not have to resort to an error check table. \n \nConclusion \nIt‘s important for there to be a continuous flow of information among the staff on a game \ndevelopment team. Unfortunately, quantitative data regarding when a game hits an error in \na non-compiler environment is not an easy thing to capture. With tools like the RTSI, these \npotential problems can be smoothed out. One could even bind the RTSI entry point to a \nscript that fills a form in a bug database with this kind of information so that it could be \nlater analyzed by the programming team. \n",
      "content_length": 1668,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": " \n \n \nReferences \n[Bilas00] Bilas, Scott. ―An Automatic Singleton Utility.‖ Game Programming Gems. Boston: \nCharles River Media, 2000. 36–40. \n[Rabin00] Rabin, Steve. ―Squeezing More Out of Assert.‖ Game Programming Gems. \nBoston: Charles River Media, 2000. 109–114. \n \n4.8. Code Coverage for QA \nMatthew Jack \nmtj22@cantab.net \nWhen working on a rapidly developing code base, a good QA team is a highly versatile \nresource that no automated testing can replace. However, actually playing the game is such \na high-level form of testing that it can be hard to relate it to low-level changes in the code. \nFirst, changes that a programmer has made may only be executed in certain situations \ndepending on the player‘s actions, which can be hard to give a procedure to re-create. For a \ngiven piece of low-level code, a programmer may have little idea where and when in the \ngames it is actually used! Further, even when we can give clear instructions for testing, we \nare rarely absolutely sure whether they worked and the code was actually executed; in \nsome cases there will be a clear visible indication, but in many others the effect of the code \nis subtle. Often, it is simplest in the end to throw up our hands, give no details, and just ask \nQA to ―be thorough‖—which is time-consuming for them and still provides no guarantees \nabout the result. \nThis gem describes a framework that combines code coverage analysis and conventional QA \nby using real-time feedback. It addresses these issues and improves the testing process for \nboth QA and programmers. It can bring rigor and repeatability to testing and yield valuable \ninsight to the low-level programmer about where the code is actually used. \nCommon Approaches \nBefore examining the approach of this gem, let‘s consider some of the established testing \nmethodologies. \n \nUnit testing. Especially for localized, pure refactoring, unit testing can be rigorous \nand fast. However, to be pragmatic, most games are developed without \ncomprehensive unit testing. One reason for this, at least for rapidly changing game \ncode, may be that unit testing is considered to add too much development overhead. \nIf unit tests are not present in the original code, then restructuring to add them is \nlikely to be an enormous task that could itself introduce bugs. \n \nAutomated functional testing. For instance, a repeatable play-through using an \ninput recorder. This is very convenient for detecting crashes; however, an \nexperienced eye is still required to watch for changes in behavior, and it in no way \naddresses the thoroughness of the original play-through. Furthermore, if the game \nyou are testing is currently under development, it may change so quickly that a \nrecording has a very short lifespan. \n \nQA testing. Plain old QA is essentially a manual form of functional testing. A good \nQA team can adapt rapidly to a changing specification, report changes in behavior, \n",
      "content_length": 2928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": " \n \nand benefit from human intuition when something looks wrong. These are valuable \nqualities to build upon. \nIn all of these approaches, we might ask: How complete is their testing? One way of \nassessing this is through code coverage analysis. This measures how rigorously a given test \nexercises source code, allowing this to be quantified and improved. Fundamental to this \ngem, we will examine it in detail. \n \nAn Analogy: Breakpoint Testing \nThere is a rough-and-ready version of this gem that I have often employed, and it turns out \nit is already available in your IDE. We might call it breakpoint testing. \nIn refactoring a function/class/file/blob, one systematic approach to testing on the \ndeveloper‘s machine is to place a breakpoint on every statement that represents a relevant \ncode path and then run the game, removing each breakpoint as it is hit. When all \nbreakpoints have been removed, without any crashes or unexpected behavior, testing is \ncomplete. \nThis can be a good minimal test of changes, as it ensures the developer has executed all the \ncode he changed and has observed the overall results. However, playing through takes \ntime, and it must be done on that computer, so this cannot easily be delegated. \nFurthermore, it is not repeatable: Future regression tests would require re-creating all of \nthe breakpoints. \nHowever, note that breakpoint testing works for new code as well as refactored code and \nthat it directly couples high-level functional tests to low-level details in the source. \nThis gem is a framework that formalizes and expands that ad hoc hybrid of code coverage \nand manual testing. \n \nCode Coverage \nConventional code coverage analysis is used to quantify how completely a testing procedure \n(usually automatic) exercises source code [Cornett96]. It works by monitoring at run time \nthe number of unique lines (or functions, branches, code paths, and so on) that have been \nexecuted by the test, compared to the number theoretically possible. This basic principle of \ndiscovering which code we have actually run is attractive, but there are several questions \nwe must address when applying this to games. \nThe first is what kind of testing procedure we should use, since code coverage is only useful \nin analyzing tests. Unit testing is the standard in the wider software industry, but for games \nwe propose QA testing of real levels. This is simple and flexible, and chances are your \ncompany is already doing it every day. It also better represents the end product, where \ncode and assets intertwine to determine final quality. \nNext, we have to consider granularity. Instrumenting every line will cause a significant \nslowdown that may make the game unplayable—especially on consoles—and the increased \nexecutable size may prevent it from loading at all. Furthermore, this is sheer data overload: \nSuch a glut of information would take further processing to make any sense to us. \nMonitoring every function is too arbitrary—some functions are trivial and/or speed-critical, \nwhile others may, for instance, contain large switch statements. Tracking all branches may \nagain generate too much data. \n",
      "content_length": 3153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": " \n \nAlso, we must consider the final output, which is related to the granularity. A list of all the \nnumbers of all the lines executed would be meaningless in itself—and rendered useless as \nsoon as the source code changes. Function names would be instantly recognizable, but what \nif we change them? \nThe result most commonly quoted from code coverage is a single figure: a percentage \nrepresenting the completeness of testing, where targets are commonly 90- to 100-percent \ncoverage [Cornett96, Obermeit06]. Note that if we use real levels as tests, in most games \nwe will see much lower coverage, as each level usually employs only a subset of the \nfeatures. If we accept that we will only test a fraction of our code, we have an ambiguity: \nWas a given feature left unexercised because testing was insufficient or because it is not \nused in this level? It may also beg the question: Much of the code may have been tested, \nbut was this piece of code tested? \nFinally, if we use human testers to perform the tests, our results may vary wildly with \ndifferent play-throughs. Can we guide them toward comprehensive, repeatable testing? \nThere are various code coverage packages already available, but without addressing these \nissues, their value in practical games development is limited. \n \nImplementation \nKey to our approach is that, rather than use any automatic instrumentation, the \nprogrammer places his own markers into the code using a simple macro. This allows us to \nsolve the problems of granularity and meaningful output and leads us to solutions for the \nremaining issues. \nBy using manual markers, we can keep them to a manageable number, applied only where \nrequired and omitted from performance-critical or trivial code. This—and careful \nimplementation of the marker code—avoids slowdown and code-bloat. \nWe use a unique string to label each marker. You could base these on the class and method \nname—for example, CCoverBehaviour_ThrowGrenade_A—or use a pure feature \ndescriptor—for example, ThrowGrenadeFromCover_A. The labels make the output \nmeaningful, while still allowing the programmer to add markers to individual if/else \nbranches, cases of a switch statement, loop bodies, or anywhere else he thinks they may be \nuseful. \nThe Markers \nThe following macro expands to code that constructs a static object with a trivial constructor \nand calls its Hit() function: \n#define CCMARKER( label ) \\ \n  { \\ \n    static CCodeCoverageMarker ccMarker_##label( #label ); \\ \n    ccMarker_##label.Hit(); \\ \n  } \n \nwhere Hit() registers the marker with a singleton [Alexandrescu01] for tracking the first \ntime it is executed. Keeping this method separate allows for a reset feature. \ninline void Hit() \n",
      "content_length": 2717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": " \n \n{ \n  if (!m_bHit) \n  { \n    m_bHit = true; \n    CCodeCoverageTracker::GetInstance().Register(this); \n  } \n} \n \nSome of the advantages of this approach include: \n \nMacros are easily compiled out, ensuring zero performance impact in the release. \n \nUsing a static object allows a fast check for whether this is the first time hit. \n \nFunctions can be renamed or moved into a different class or file, but while marker \nlabels are preserved, code coverage results remain undisrupted. This is helpful as we \nbuild up a history. (See the upcoming ―Collecting Results‖ section.) \n \nDescriptive labels immediately provide hints for real-time feedback to QA. When \nwondering how to hit that last one percent of markers, a tester will really appreciate \nthe onscreen text ―ThrowGrenadeFromCover_A‖. \nC++ compilers include predefined macros that could generate marker labels automatically, \nbased on function names, line numbers, and so on. Note, however, that for this saving of a \nfew moments, we lose much of the advantage of a manually defined label. \nIf labels are not unique, our results will of course be misleading. This would usually occur if \nsomeone copied and pasted code somewhere else, but it could also occur if a single marker \ndeclaration was instantiated multiple times in the binary. This could happen through use \nwithin macros or templates. Functions inlined as an optimization are not a problem, as the \nC++ standard defines that all inlined copies of a function must share one copy of any local \nstatic variables [ISO/IEC14882-98]. Note that some older compilers did not handle this \nproperly. \nThe best way to avoid the more common copy/paste problem would be to write a simple \nscript to scan the source code for duplicate labels. All types of duplicates can also be \ndetected by the framework itself at run time—the example implementation includes such a \ncheck. \nIn my own experience, there have been no problems with duplicates. \nExamples: Applying Markers \nMarkers are commonly embedded at the start of methods: \nvoid BrainClass::Update() \n{ \n  CCMARKER(\"Brain_Update\"); \n  UpdateVision(); \n  UpdateBlackboard(); \n  ... \n} \n \nwhile, if the method includes various early-out checks, it is most effective to place the \nmarker at the end, where the ―meat‖ of the code has already been used: \n",
      "content_length": 2311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": " \n \nvoid Brain::SendSignal( const char * signal, int targetID) \n{ \n  if (targetID == m_myTargetID) // Am I the target? \n    return; \n  if (GetDistanceToTarget(targetId ) > 20.0f ) // Is  it too \nfar? \n    return; \n  ... \n  CCMARKER(\"Brain_SendSignal\"); \n} \n \nHere I‘ve used a structured name, confident that sending signals will always be a \nresponsibility of my Brain class, or perhaps distinguishing it from other places where \nsignals are sent—but in many cases you may prefer a totally pure name, as below. Where \nyou only describe the feature, you can freely move the code and preserve the label without \nit becoming misleading. \nYou can also include multiple markers at different points in the same scope if desired, in \nloops (if not speed-critical): \nfor (int i=0; i<nSignalsToSend; ++i) \n{ \n  CCMARKER(SignalSent); \n  SendSignal(i); \n} \n \nor in branches of switch statements: \nCObject * pObj = NULL; \nswitch (n) \n{ \n   case 0: CCMARKER(FetchedLeader); \n          pObj = m_pLeader; \n          break; \n   case 1: CCMARKER(FetchedCompanion); \n          pObj = m_pCompanion; \n          break; \n   ... \n} \n \n \nThe Tracker \nAs earlier, each marker registers with the tracker the first time it is hit, which maintains the \nset of all markers hit thus far. In the example implementation, the tracker adds the marker \nto an STL vector and does nothing more; all other work is put off to help avoid disrupting \ncaches and skewing profiling results. \nThe markers are completely unknown to the program until they are hit, because their scope \nis local to function bodies. A simple script could extract the complete set from the source, \nbut usually we are only interested in the subset used by a given level. \n",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": " \n \nPer-Level Coverage Input File \nSince we have established that real levels only use a subset of features, we keep a separate \nlist of expected marker labels for each level. Once loaded, this defines the 100-percent \ncoverage that testers will aim for. After the testing process, these files are updated using \nthe results from code coverage—in particular, adding any ―unexpected‖ markers that were \nhit. \nWe can form the initial set of input files in the same way, employing ―blind‖ testing to find \nmost of the markers without guidance. We can then improve upon them in subsequent runs \nas testers find any remaining markers, gradually raising the bar for test quality. \nThis also smoothly assimilates new code: The markers will appear in results as \n―unexpected,‖ confirming to the programmer where they were used, and will be expected in \nthat level from then on. \nMaintenance of these per-level coverage files should obviously be automated by a \nprocessing script, but summaries of their changes are informative. (See the upcoming \n―Collecting Results‖ section.) \nThe Manager \nThe manager class loads the per-level input file containing the set of ―expected‖ marker \nlabels for the level being tested. It also monitors which of these have actually been hit, \nwhich are still remaining, and whether any ―unexpected‖ markers have been hit. The class \nexposes a simple interface to this data, allowing progress to be checked and results to be \noutput. \nThe same interface makes it easy to write a GUI for real-time feedback. We describe this in \nthe next section. \n \nQA Interface and Workflow \nGetting the most out of this approach requires real-time feedback, guiding testers toward \n100-percent coverage while making the smallest changes possible to workflow. One-\nhundred percent is often a realistic target. Markers have been deliberately placed and so \nshould be reachable. They must have been hit in this level before to be ―expected,‖ and we \nprovide guidance in reaching the last few percent. However, in branching levels it may be \nnecessary to combine multiple runs. (See the upcoming ―Collecting Results‖ section.) \nThe GUI \nThe implementation within CryENGINE, as shown in Figure 4.8.1, uses a simple onscreen \ndisplay comprising: \n \nProgress bar (0 to 100 percent ―expected‖ markers hit) \n \nIndication whenever a marker is hit for the first time \n \nList of the remaining marker labels as hints \nFigure 4.8.1. Code coverage GUI implementation within CryENGINE, including \nprogress bar, hit indicators, and a list of markers hit within the last few seconds. \n",
      "content_length": 2574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": " \n \n \nThe list of expected markers for each level is stored as a separate file in the level folder \nitself, loaded automatically when the code coverage display is activated. \nTo avoid clutter, it is best to only display marker labels when few remain. Normal testing \nshould easily hit 90 percent of markers reachable in a given level, but guidance on the last \nfew is important if high coverage scores are to be reliably reached. \nLabels as Hints \nIt would be a fair criticism that in some cases the ―meaningful‖ labels given by \nprogrammers may be useless when appearing out of context on a tester‘s screen. However, \nthey form the vocabulary of a common language between testers and programmers, \nallowing testers to find out how they might hit that specific marker and to communicate that \nknowledge to others. \nFor example, while ―Throw_Grenade‖ might be helpful, ―Reorganise_Brain_E‖ is less \nintuitive, but for the programmer still only a moment‘s string search away— hence, it can \nbe clarified by a quick email from tester to programmer. Also, should a tester discover how \nto hit this infamous marker through trial and error, he can easily relate that to others on the \nQA team. A Wiki or similar maintained by QA would be a good repository for details of any \ntricky markers. \nWe make it simple for programmers to help QA achieve high coverage, but we also give QA \nthe means to be self-sufficient in this regard, so in practice little or no programmer time \nshould be required. \nFigure 4.8.2. Code coverage within CryENGINE, when 93 percent of markers for \nthe level have already been hit and the remainder are displayed as hints to the \ntester. \n",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": " \n \n \n \nOutput of Results \nResults are output as a list of labels of the markers hit, either dumped to a log file or sent \nautomatically over the network. \nThe use of log files is simple and quite robust. Since it is common for testers to send logs \nwith their bug reports, it may be best for workflow to add them to the existing log file in an \neasily extracted format. This can be as the complete status of known labels in one block, or \ndumped periodically or manually, or you could also write individual labels as their markers \nare hit. The latter has the advantages of forming a record of when they were hit and losing \nno results should the game crash, although it does add clutter to the log. Log files must be \ncollected by testers and delivered to the programming team. \nSending results over the network is potentially much more elegant, with no impact on QA \nworkflow and coping easily with crashes. However, it requires more work to set up and \nsome thought in its deployment. (See the upcoming ―Possible Expansions‖ section.) \nQA Workflow \nFinal instructions to QA are simply: \n \nLoad the level, activate coverage, and test as normal. \n \nTry to reach 100-percent coverage, using the textual hints to help if need be. \n \nSend the logs as usual when you‘re finished (if you use a log-based approach). \nYou may also ask testers to quit and restart the game between levels—this is to reset the \ncoverage state and thus avoid mixing results. However, in the example code provided with \nthis gem, the state can be reset at any time and so chain-loading levels may be practical, \ndepending on how much state your game carries over. \nAs a final note on presentation: It may already have occurred to the reader that testing with \ncode coverage feedback is rather like playing a mini-game. An example of screen output \nwhile testing is shown in Color Plate 11. \n",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": " \n \n \nCollecting Results \nOutput from a code-coverage testing run should contain at least the following data: \n \nLevel tested \n \nMarkers hit (as text labels) \nIt may be useful to include: \n \nBuild identifier \n \nDate/time \n \nTester‘s name \n \nSpecifications of test platform \n \nSeparate list of unexpected markers hit \n \nExpected markers that were not hit \nSome of this may be found already in your log file and so, as above, you may choose to \nsimply add your code coverage results there. \nYou might find that on a small scale, this information is already enough. Your QA testing will \nbenefit from being guided to cover all markers, and by examining the logs you can now \ncheck that good coverage was achieved, check that specific code was tested, and look out \nfor code that should be run that isn‘t or code that needn‘t be run but is! \nHowever, you can get much more from the system by going further: \n \nMerge. Merge the results from multiple runs of the same level to reach a higher \ncombined coverage and to confirm unusual results. In many cases, branching player \nchoices may even make it impossible to reach 100-percent coverage in a single play-\nthrough. Further, merge the results of all your levels, looking for code that is not \nused anywhere. \n \nUpdate. Update your lists of expected checkpoints. After making changes to the \ncode and addressing any anomalies in the code coverage results, extract the list of \nmarkers hit to form a new list of expected markers for subsequent runs of the level. \n \nHistory. Build a history of results for each level over time. You can achieve this by \nsimply checking in the list of expected checkpoints for each level—which may in any \ncase be the best way to include them in the build. You can then leverage your \nexisting diff tools to analyze changes in results over time. For programmers, this can \nhelp with diagnosing bugs or tracking changes in levels under active development. \nWith a very large marker set, this could also be helpful in test planning to track when \na feature was last tested. \nAll of this can be done automatically. We use a simple text-processing tool that takes any \ncollection of log files and digests them into final results: sorting by level, merging separate \nruns, highlighting anything unusual, and outputting updated lists of expected checkpoints \nfor the next run. \n \nPossible Expansions \nThere is potential for getting more out of the framework: \n",
      "content_length": 2431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": " \n \n \nCollecting results over the network. Rather than using log files and extracting \ntheir results offline, it may be better to send results directly over the network to a \ndatabase server. This assumes that all builds tested can be reliably distinguished. If \nprogrammers often send their own local builds to QA, a simple build number may not \nbe enough. This approach simplifies the workflow for collecting and processing \nresults. \n \nAutomated testing. As described, the core testing method recommended for use \nwith this gem is a good QA team. However, the automated methods discussed, such \nas recorded play-throughs, are also compatible with this framework and would \ncomplement the QA process well. (Refer to the ―Common Approaches‖ section earlier \nin this gem.) In particular, code coverage reports from any automated tests \nperformed by your build machine would provide very fast feedback. \n \nCategorized markers. While the original implementation was applied to just AI \nsystem code, were it deployed more widely, it could make sense to divide the \nmarkers by module and/or feature. The ability to only consider a certain group of \nmarkers could then become useful, focusing testing on, say, the sound system, \ngame code, HUD features, and so on. \n \nPrioritized markers. Assigning priority levels to markers could help with test \nplanning, especially in times of fast turnaround. A simple high/medium/low scheme \nwould ensure the most important code is tested first. This would be most effective if \nintegrated into the GUI and if programmers had a simple way to promote/demote \nmarker priority in the builds. \n \nActivity display for markers. Flagging a group of markers to give a visual \nindication every time they are executed could be useful for better understanding the \nactivity of code or for guided stress-testing of a particular feature. \n \nHit counts. Rather than a hit flag, keeping a count for each marker could be useful \nas part of built-in performance tracking or monitoring the relative frequency of \nbranches, loop counts, and so on. \n \nRecentness tracking. Add a global counter that is advanced every time a marker is \nhit, writing the current counter value to that marker. This could be useful for tricky \ncrash bugs, to find out what code ran in the preceding moments. \n \nCustom levels for testing. This framework could be used to guide the creation of \nlevels for testing specific features, or indeed a single level that includes all features! \nThis offers the potential for very fast turnaround. \n \nA Post-Mortem \nThis framework was developed in order to safely make a particularly major refactor to our \nAI system, replacing a fundamental aspect of the system and involving around 25 percent \nof the code. The AI system is in constant use by developers, so full confidence in the result \nwas required before finally merging the branch. Milestones could not be disrupted, and no \none enjoys an endless bug-hunt. \nAs we have actively maintained our last major title as a test platform—which QA knows \ninside out—the levels of Crysis formed the subject of code coverage tests. In the process we \nformed a list of the features used—in other words, markers hit—in each level. Before this, \nvarious programmers and designers had different pieces of the puzzle, but no one had a \ncomplete, verified record of what was used and where—which is an invaluable reference for \nfuture maintenance. \nThe GUI proved useful to developers as well as QA testers, making it easy to observe when \nand where markers were first hit during their own testing, and with this information allowing \nQA time to be better targeted. By merging results from all levels, we were also able to find \nredundant code that would escape other forms of analysis— for example, minor features \nthat were not used in the released title. \n",
      "content_length": 3830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": " \n \nReception in the QA team was enthusiastic: They appreciated the feedback of the new tool \nand would like to see its use expanded. They also found no measurable impact on frame \nrate. \nHaving a text-processing tool to form the reports was essential in dealing with results from \nmultiple testers and many different levels. The tool processes entire folders of log files (one \nor more tests of each level of the game) in one pass, merging results, updating the label \ndata files in each level folder, and outputting a summary of changes to the console. After \nthat, it is easy to check the updated label files into source control. \nWhen investigating a bug, it was often useful to look through the history of the per-level \nlabel files and verify that this was the first test of that code, or to see that the code really \nhad been running successfully for some time, implying the problem must lie elsewhere. \nUse of the framework was focused on this refactor and testing in the following weeks. \nGreater automation would be instrumental in realizing its potential as an everyday \nmaintenance tool—in particular, a network-based collection of results to a central server, \nemailed summaries of results, and combination with automated tests. \nCode coverage helped inform the refactor, helped discover bugs quickly, and gave a new \nmeasure of confidence in the result. Some months after the final merge, we‘ve found only \ntwo bugs. \n \nConclusion \nThis gem presented an adaptation of the code coverage methodology specifically for \npractical games development, employing QA testing guided by real-time feedback. It can be \nadded to existing code and workflows to augment your existing testing procedures, \nimproving their rigor. It also provides programmers with more information about the usage \nof their code and greater confidence in their changes. \nIncluded on the CD is an efficient implementation of the framework, complete except for a \nGUI. This also forms a suitable basis for the various possible extensions of the framework \nthat have been discussed. \n \nReferences \n[Alexandrescu01] Alexandrescu, Andrei. ―Modern C++ Design.‖ 2001. \n[Cornett96] Cornett, Steve. ―Code Coverage Analysis.‖ 1996. Bullseye Testing Technology. \nn.d. <http://www.bullseye.com/coverage.html.> \n[ISO/IEC14882-98] ―Standard for Programming Language C++.‖ dcl.fct.spec/4. 1998. \n[Obermeit06] Obermeit, Tony. ―Code Coverage Lessons.‖ 2006. Mobile Me. n.d. \n<http://homepage.mac.com/hey.you/lessons.html>. \n \n4.9. Domain-Specific Languages in Game Engines \n",
      "content_length": 2537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": " \n \nGabriel Ware \ngabrielware@free.fr \nDomain-specific languages, DSLs for short, are computer languages used to solve problems \nwithin the explicit boundaries of the problem domain. The benefits of DSLs are multiple: \nThey help by separating domain-related code from application code; they let domain \nexperts solve problems using a language they understand; they can have multiple outputs, \nand users can easily shift from one to another; and last but not least, designing domain-\nspecific languages usually tighten relations between programmers and experts. This gem \nwill dig into domain-specific languages, answering the following questions: What is a DSL? \nWhen should I use a DSL? How do I build a DSL? \nDomain-Specific Languages in Depth \nIn this section we‘ll explore DSLs and their uses, concluding with some guidance on when to \nuse them. \nDomain-Specific Languages: Definitions and Examples \nSeveral definitions have been proposed for domain-specific languages. DSLs can be defined \nas artificial languages expressing instructions to a machine while working on a narrow field \nof expertise, a specific domain. These computer languages are sometimes referred to as \nlittle languages or micro-languages because of the limited expressivity of their syntaxes. \nTheir syntaxes are restricted to the problem domain they are modeling, including only what \nis relevant to the problems. Languages such as C, C++, or Java, which are labeled general \nprogramming languages, or GPLs, provide generic solutions to a broad range of problems \nand, as such, can be opposed to DSLs that provide more tailored solutions to a restricted \nset of problems. \nDomain-specific languages have existed for a long time, and their use in computer science \nis widespread. Successful examples include Lex and Yacc, programming languages intended \nto create lexers and parsers to help in building compilers; SQL, a computer language \ntargeted at relational databases; and LaTeX, a document markup language providing a \nhigh-level abstraction of TeX. DSLs have several characteristics emerging from their form \nand the process used to build them. The main characteristic of DSLs is their syntaxes, which \nprovide appropriate notations to the domain model and a very limited set of instructions. \nThis limits what problems users can solve but at the same time allows the language to be \nlearned quickly. DSLs are also usually declarative. They can sometimes be viewed as \nspecification languages, providing domain experts with the capability of writing \nspecifications that will become new tools, solve problems, and encode domain knowledge. \nBecause they encode domain knowledge as perceived by domain experts, DSLs are usually \nbuilt from a user perspective. Such a user-centric process tries not to take into account \nexternal factors, such as compiler capabilities, and prefers focusing on user experience. \nData mining is a domain that could be used as an example illustrating how domain-specific \nlanguages help. Code to search for persons matching certain criteria in tables can be written \nin any GPL, but it is much easier to write with a language that is appropriate to the domain. \nListings 4.9.1 and 4.9.2 show SQL and C++ code snippets that provide the same feature to \nan application. Even if C++ is more adequate on a general basis, it does not focus on the \ndomain and thus is harder to use in this specific case than SQL. \nListing 4.9.1. Mining a table for persons named Paul in SQL \nSelect users from table where name='Paul' \n \n",
      "content_length": 3525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": " \n \nListing 4.9.2. Mining a table for persons named Paul in C++—relying on the STL to \nhandle allocations and strings manipulation \nstd::list<CUser>::const_iterator const table_end = table.end(); \n    std::string SearchedName(\"Paul\"); \n    for(std::list<CUser>::iterator it = table.begin(); \n        it != table_end; \n         ++it) \n    { \n        if (it->Name == SearchedName) \n        { \n            users.push_back(*it); \n        } \n    } \n \nAnother interesting feature these two listings exhibit is the differences in term of interface. \nWhile GPLs provide a satisfying interface for programmers, some DSLs provide very light \nsyntaxes nearly exempt from notations that do not translate into everyday language, \ndropping parentheses, braces, and any other artifact as much as possible. These notations \nare referred to as language noise, and programming interfaces that minimize this noise are \ncalled fluent interfaces. Domain-specific languages do not always provide a fluent interface \nto their users, but this can be a useful feature to provide when end users do not have a \nprogramming background. \nThe Different Types of Domain-Specific Languages \nWhile GPLs are usually classified by their programming paradigm and the type of output \nproduced by their compilers, domain-specific languages are distinguished by the methods \nused to build them. As such, two main categories are emerging—internal DSLs, sometimes \nreferred to as embedded DSL, and external DSLs. \nWhen a DSL provides a custom syntax and relies on a custom-made lexer, parser, and \ncompiler, it is categorized as an external DSL. Building an external DSL is the same process \nas building a new general-purpose language: Programmers have to design the language and \nimplement it as well as any needed tools, such as editors, parsers, compilers, and \ndebuggers. On the other hand, internal DSLs are built from general programming languages \nthat offer syntaxes malleable enough to build new language from them. This greatly reduces \nthe amount of work needed to implement the language as programmers rely on the existing \ntool chain and language‘s features. On the down side, internal DSLs‘ syntaxes usually \ninclude language noise from their host language. \nIn addition to these two categories, DSLs are also often classified by the type of interface \nthey provide to end users. While some DSLs are created by programmers to be used by \nother programmers, others are designed to be used by domain experts who do not have \nprogramming experience. Thus, some DSLs provide textual interfaces, but other DSLs adopt \ngraphical front ends in order to ease programming. An example of a successful visual \ndomain-specific language is Unreal Engine‘s Kismet, which allows designers to control \nactions and handle events by connecting boxes using the graphical user interface provided \nby Unreal Editor. \nAdvantages of DSLs \nDomain experts may not be able to write code but are usually able to review code written \nusing domain-focused syntaxes. DSL code can sometimes even be written directly by \n",
      "content_length": 3062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": " \n \ndomain experts, achieving end user programming. DSLs concentrate on domain knowledge, \nand thus it is important that coders creating new DSLs deeply understand the problem \ndomain. As a side effect, this usually tightens relations between programmers and domain \nexperts, resulting in more accurate solutions. \nThe limited expressiveness of DSLs limits user input and, as such, can help to reduce user \nerrors. DSLs are easier to master, and as interfaces become more fluent, the code starts to \nbe self documenting. \nThrough these characteristics, DSLs are able to express important information while hiding \nimplementation details. Just like good APIs, DSLs provide users the ability to program at a \nhigher level of abstraction. This leads to a clear separation between domain knowledge and \nimplementation that allows for better conservation and reuse of this knowledge. \nFinally, DSLs provide new opportunities to do error checking, statistical analysis, or any \nother transformation of the domain knowledge. \nDisadvantages of DSLs \nThere are also several drawbacks to using domain-specific languages. The most problematic \nis the cost of building and maintaining a new language. While building external DSLs still \nrequires quite a bit of effort, new tools and new techniques have been used to reduce these \ncosts. Another alternative is to embed the DSL in a host language. As the language evolves \nand requirements change, language maintenance can become a burden. It can be very \ntempting to grow the problem domain by adding new keywords and notations, but this \nusually leads to building general-purpose languages with some domain-specific keywords. \nThis is a very costly approach and should be avoided unless it is desired. Another drawback \nof using multiple languages to build an application is that programmers need to learn more \nthan a few languages to control the whole pipeline, and thus they need to quickly learn and \nadapt. One last problematic aspect of using domain-specific languages is that it introduces \nan extra layer of complexity, which can slow the debugging process. \nRelations between DSL and Game Development \nGame development provides a wide variety of challenges in many different domains. To \ntake up these challenges, programmers usually use a few general-purpose languages and \nbuild frameworks that will help resolve domain-related issues. DSLs appear to be a good fit \nto this environment. Typical examples of problem domains related to video games are game \nlogic, navigation, animation, locomotion, data modeling, serialization, and transport. \nThinking in terms of modeling problem domains and user experience helps to define what \nsolutions are needed. \nWhen to Create a New Language \nCreating new languages is a difficult and time-consuming task, so deciding when to use a \nDSL is a very important process. The need for a domain-specific language usually arises \nwhen a common pattern is detected from several problems. Those patterns can occur at \ncode level, in programs, subroutines, or data, as well as at the application level, building \nsimilar tools or architecture several times. The problem domain can usually be identified \nfrom these patterns, and the boundaries of the domain can then be determined. Domain-\nspecific languages stress staying domain-focused, so it is important to deeply understand \nthe domain‘s definition. If the boundaries are blurry and users can‘t anticipate \nrequirements, then it may be impossible to design the language. It is important to carefully \nchoose the bounds because if they are too narrow, the DSL won‘t be used to encode enough \ndomain language, whereas if they are too broad, the language may lose its focus. \nBoundaries also influence the language interface by defining which variants are to be \nexposed to the user. Exposing too many variants will slow language learning, while not \n",
      "content_length": 3886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": " \n \nexposing enough will render it less usable. Domain experts, documentations, and \nspecifications can help determine such boundaries. \nLastly, because creating a new language is a difficult task, it is important to know whether \nsuch a language will be reused. If the problem domain is too narrow and domain knowledge \nshould be encoded only once, it may be better to build a framework over a general-purpose \nlanguage, but if domain knowledge needs to be encoded multiple times, solve multiple \nissues, or requires a lot of effort to be encoded using a GPL, then creating a new domain-\nspecific language may be a good option. \nFigure 4.9.1 depicts part of the decision process. \nFigure 4.9.1. DSL decision process. \n \n \n \n \nCreating a DSL \nThe process applied when creating a domain-specific language can be summed in six steps, \nas illustrated by Figure 4.9.2. \nFigure 4.9.2. DSL creation process. \n",
      "content_length": 903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": " \n \n \n \nWe start with the problems that must be solved to meet our goals. As stressed earlier, it is \nimportant to be able to detect recurrent patterns coming from different problems because it \nwill lead to the identification of a problem domain. If patterns are detected soon enough, \nthe domain can be examined, and new problems may be anticipated. The second step is to \nacquire as much knowledge about the domain as possible. Documentation and domain \nexperts are the best sources of domain knowledge and will be able to explain what users \nexpect from the domain. This leads to a user-centric approach and designing the language \nfrom a user perspective. The last step before designing the language is to choose between \ninternal and external DSLs and the type of interface the language will provide to the end \nuser. Interfaces are usually driven by the domain model to represent end user ability to use \ngraphical and textual interfaces. In the language design phase, the specifications of the \nlanguage are laid down. Notations and keywords needed to model the domain are chosen, \nand variants—what the interface will show—and invariants—what assumptions about the \nmodel will be hidden in the implementation—are identified. Lastly, all tools required to \nimplement the language are created. \nChoosing between Types of DSLs \nDetermining the user interface of a new language is a decisive factor for its adoption as a \nnew tool. By understanding what users are expecting from the tool, programmers will be \nable to refine the required features. \nIf the domain can be modeled using text, then both internal and external DSLs can satisfy \nthe needs, and choosing between them is a matter of understanding their constraints along \nwith the programming proficiencies of the end user. Internal DSLs rely on the availability of \na host language that is malleable enough to let a DSL emerge from its own syntax. When \nsuch a language is available, its syntax and tool chain will influence the look and feel of the \nDSL. If those constraints are acceptable, then building an internal DSL is the quickest way \nto create a new textual DSL, as it will provide needed tools for the language. On the other \nhand, external DSLs do not rely on another language, allowing for a better customization of \ntheir syntaxes. But they also require that programmers build tools such as parsers, \n",
      "content_length": 2380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": " \n \ninterpreters, or compilers to support the new language. If no host language that satisfies \nsyntax needs is available, then external DSLs are the way to go. \nWhile some domains are very easy to model using words, other domains cannot be \nmodeled, or at least are difficult to model, using text. In this case, the domain-specific \nlanguage can rely on a graphical interface to help end users encode their knowledge. \nAnother factor for building a graphical front end to the DSLs is the programming \nbackground of their users. Some DSLs are intended for non-programmers, and if the \ndomain is too complex and thus requires exposing too many variants and keywords, it will \nthen probably be easier for the user to represent the domain knowledge using graphical \ntools. Although graphical DSLs are usually built from scratch, some tools do exist that help \nin creating them. \nFigure 4.9.3 synthesizes the whole process. \nFigure 4.9.3. DSL type decision process. \n \n \n \nCommon Programming Techniques for Building Internal DSLs \nAs domain-specific languages become more and more popular, several programming \npatterns used to build them emerge. Luckily, most programming tips used to build domain-\nspecific languages are easy to understand and use, but some may not be available from all \nhost languages. \nThe first, and probably oldest, technique is the use of macros. It has been widely used by C \nand C++ developers to pre-process source code. It uses the preprocessor capabilities to \nbuild fluent interfaces that generate complex code at pre-processing time. \nAnother old and widely available technique is called function sequencing. Domain knowledge \nis encoded using sequences of function calls. The implementation of this method relies \nheavily on the side effects of each function to affect the execution context of each \nsubsequent call. While this method provides a potentially acceptable solution in terms of \ninterface, relying on side effects can be dangerous and hard to debug as sequences become \nmore and more complex. Listing 4.9.3 shows an example of function sequencing. \nListing 4.9.3. Function sequencing \nanimation_engine(); \n    character_controller(); \n",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": " \n \n        playanimation(\"run_fast\"); \n            easing_in_using(LINEAR_EASE_FUNCTION); \n               during(10_MSEC); \n \nAn evolution of function sequencing is called method chaining. It uses objects to pass the \ncontext between calls without adding noise to the language. With this technique, each \nmethod call returns an object that provides a part of the language interface. This helps to \nfragment the interface across multiple types of objects. Rewriting the previous example \nusing method chaining leads to Listing 4.9.4. \nListing 4.9.4. Method chaining \nanimation_engine(). \n    character_controller(). \n        playanimation(\"run_fast\"). \n            easing_in_using(LINEAR_EASE_FUNCTION). \n                during(10_MSEC); \n \nNested functions are another way to call functions while removing language noise as much \nas possible. When using this method, all calls are nested, as presented in Listing 4.9.5. The \nmain characteristic of nested functions is the order in which functions get called. It can be \nvery useful when domain knowledge can be expressed as a sum of properties and \ncontainers. \nListing 4.9.5. Nested functions \ncharacter_controller( \n    playanimation( \n        \"run_fast\", \n        easing_in_using(LINEAR_EASE_FUNCTION), \n        during(10_MSEC) ) ) \n \nAnother frequently used technique for building domain-specific languages on top of an \nexisting framework is to separate the fluent interface from the existing API. Fluent \ninterfaces are usually created using assumptions about the calling context of their routines. \nAlthough this helps naming methods that chain efficiently and produce nearly fluent code, \nthis way of writing an API violates what is considered good programming practices. Thus, it \nmay be interesting to get the best of both worlds by adding a fluent interface on top of a \nmore standard framework. \nLambda functions, sometimes named blocks, anonymous methods, or closures, are a \nfeature that only recently became widespread in many mainstream languages. They have \nbeen successfully applied to creating DSLs because they offer the key characteristic of \nevaluating code, with minimal language noise, in a predetermined context. Lambda \nfunctions are very similar to a standard method definition but do not require the same \ntextual overhead as functions: They do not need to have names, complete parameter lists, \n",
      "content_length": 2375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": " \n \nand return types and are simply associated to standard variables that can be passed across \nfunctions. \nThe dynamic handling of missing methods is another widespread technique to create \ndomain-specific languages. It is a popular feature of languages such as Smalltalk and Ruby, \nwhere you can override doesNotUnderstand and method_missing, respectively. \nOther languages, such as Python, can provide similar features using other internal \nmechanisms. Handling missing keywords can be very convenient when the language has to \ndeal with unknown keywords and unknown function names. This technique allows the user \nto create keywords as needed. It allows creating new modeling languages with very little \nnoise very easily. \nListing 4.9.6. Animation DSL relying on method_missing, nested function, and \nclosures \n1:  animset   = define_animation_set( :terrestrial_locomotion) { \n2:    idle(from_file(\"tm_idle\")) \n3:    run_forward from_file \"tm_run_fwrd\" \n4:    walk_forward from_file \"tm_wlk_fwrd\" \n5:    turn_90degsLeft from_file \"tm_trn_90deg\" \n6: \n7:    jump_forward from_file \"tm_jmp_fwrd\" \n8:    jump_forward { can_blend_with \nall_from(\"terrestrial_locomotion\") } \n9:    jump_forward { can_blend_with \ntransitions_from(\"aerial_locomotion\") \n10: } \n \nListing 4.9.6 shows a domain-specific language where animation identifiers are keywords \nchosen by the user and thus impossible to predict. It demonstrates usage of blocks and \nnested functions using the Ruby programming language. The code block is written between \nbrackets and, in our example, it is given to the define_animation_set function. In this \ndomain-specific language, define_animation_set creates an animation set object and \nasks Ruby to evaluate the given block in the context of this object. The animation set object \ninterface will provide functions such as from_file, which is used to load an animation \nfrom a given filename. In order to reduce language noise, the language relies on the ability \nof Ruby to deduce parentheses placement. Lines 2 and 3 are similar and interpreted the \nsame way by the Ruby language, the only difference coming from parentheses‘ presence. \nLastly, Ruby handles function calls such as idle or run_forward as missing calls that \nare handled by our implementation to identify new animations. A sample demonstrating \nhow Listing 4.9.6 is implemented using the Ruby programming language is provided on the \naccompanying CD-ROM. \nAnother easy and very powerful technique to add meaning to code is called literals \nextension and is usually available from object-oriented languages. Extending literals helps \nreaders by allowing modifiers, which may or may not do anything useful, to be used on \nliterals in order to add fluency to the code. It requires the language to handle everything, \nincluding literals, as objects that can call methods. Literal extension also relies on the ability \nof the language to reopen and extend class definitions. \nListing 4.9.7. Literal extension \n",
      "content_length": 2983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": " \n \nrun if distance_to(nearest_enemy) < 10.meters \n \nOne last technique worth mentioning is abstract syntax tree and parse tree manipulations. \nIt is a rare feature allowing programmers to access the parse tree or the abstract syntax \ntree after the code has been parsed by the host-language parser. Ruby‘s ParseTree and C# \n3.0 both work in a similar function; a library call is used to parse a code fragment and \nreturn a data structure representing the code expressions. This feature is useful when \ntranslating code from one language to another or when the DSL needs to rely on a wider \nrange of expression than happens to be available in the host language and thus needs to be \ntransformed before use. \nTools Easing Language Construction \nExternal domain-specific languages have fewer constraints than internal ones but need \nslightly more effort to create because of the time required to build parsers and compilers. \nLuckily, tools have been created to ease this process and reduce this overhead. \nLexical analyzers and parser generators, such as Lex and Yacc, have been around for a long \ntime and are still a great help to build languages, but they tend to be replaced by new tools, \nsuch as ANTLRWorks or Microsoft‘s DSL tools, which are both providing powerful \ndevelopment environments focused on creating domain-specific languages. ANTLRWorks is \nan integrated development environment for creating languages using ANTLR V3 grammars. \nIt offers rapid iteration cycles by providing a full-featured editor, embedding an interpreter, \nand providing a debugger and a lot of other tools to ease the development process. While \nANTLRWorks uses textual grammars to create external textual DSLs, Microsoft‘s DSL tools \nprovide a way to create visual domain-specific languages to be integrated into Microsoft \nVisual Studio. The Microsoft DSL tools help design the language and its graphical interface \nby providing wizards and tools easing domain modeling, specifying classes and \nrelationships, and binding designers‘ shapes to the model concepts. Although Microsoft‘s \ntools for domain-specific languages can‘t be used for building run-time DSLs, they offer \nopportunities for integrating a custom visual domain-specific language inside Visual Studio. \n \nMulti-Language Game Engine Development \nThis section presents domain-specific languages for two domains related to low-level engine \nprogramming. Other examples of DSLs for game engines are shading and rendering passes, \nsound logic encoding emitters, occluders and propagation logic, behaviors of artificial \nagents, and locomotion rules for animation systems. \nThe first example of a domain-specific language in a game engine relates to data structure \nmodeling. Data management issues occur in many places, such as a pipeline‘s applications \nintercommunication, engine working set, multi-threading and performance issues, and \nnetwork replication. A common problem is the need to write data manipulation and \nserialization code multiple times. Thus, it may be interesting to encode as much knowledge \nabout data structures as possible using a domain-specific language that handles tasks such \nas generating code for serializing and accessing data in all languages used in the pipeline. \nSuch a DSL could also allow for statistical analysis of working sets, which would help profile \nthe engine‘s need in terms of data. \nAcquiring knowledge about the domain of data management is easy because programmers \nare the domain experts. This type of DSL should solve data-related problems by providing a \nsimple syntax to encode the data structures‘ layout. The language will encode structures \n",
      "content_length": 3651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": " \n \nand should provide end users with a way to control fields‘ identification, alignment \nproperties, and serialization requirements. Lastly, this domain-specific language will be used \nby programmers, and thus it is acceptable to use a textual interface with low language \nnoise. Listing 4.9.8 shows a sample of such data management DSL using Ruby as its host \nlanguage. You can find an implementation of this DSL on the CD-ROM. \nListing 4.9.8. Simple structure layout using a domain-specific language \nstruct (:PlayerInfos) { \n  required string :name, replicate_over_network! \n  required key :race \n  required boolean :is_male \n  required int32 :level \n  required int32 :exp_points \n  required vector3f :position, replicate_over_network!, \n16.bytes.alignment \n  required quaternion :orientation, replicate_over_network! \n  optional int32 :money \n  optional float :reputation \n} \n \nThe second use case for domain-specific languages targets the engine‘s threading model. \nScalability of the engine‘s performance over machine generations has become a very active \nfield of research. Console engines usually take advantage of running on fixed hardware with \nknown specifications, but good engines must allow evolution of hardware. A recurring \npattern when changing hardware is to rewrite the threading model to reflect new hardware \nand get better performance. Another pattern related to threading models happens during \ndevelopment when programmers try to offload heavy tasks from one processor to another, \nthus changing how tasks update. Again, a language focused on task dependencies and \nhardware specifications can help handling modifications of the threading model. Such a \nlanguage has to expose to the user variants such as number of cores, number of threads \nper core, or preferred number of software threads. It can also expose tasks that are run by \nthe engine and their dependencies in order to help scheduling given the hardware \nconstraints. The output of such a language can be either code or data that would drive \ncurrent engine‘s threading framework. Like the previous domain-specific language \npresented, this language is targeted at programmers, and an internal DSL‘s properties \nsatisfy our requirements. Listing 4.9.9 shows what such a DSL could look like, and its \nimplementation is provided on the accompanying CD-ROM. \nListing 4.9.9. Threading a domain-specific language \nhardware { \n      has 3.cores.each { |core| core.have 2.hardware_threads } \n} \n \nsoftware do \n      instanciate 6.software_threads \n      instanciate :camera.module \n      instanciate :player.module, :bots.module, :sound.module \n      instanciate :physics.module, :graphics.module \n \n      camera.depends_on(:player) \n",
      "content_length": 2715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": " \n \n      bots.depends_on(:player) \n      graphics.is_bound_to(thread(0)) \nend \n \n \nIntegrating DSLs into the Pipeline \nWe will now focus on how DSLs integrate in the production pipeline. \nEngine Integration through Embedding \nThe quickest and easiest way to integrate a domain-specific language into an engine is to \ndirectly embed it. As such, creating an internal DSL that relies on the engine‘s main \nlanguage seems to be an evident way to provide domain-specific languages from the game \nengine. But, with C++ being the preferred language for building game engines, it is difficult \nto provide a domain-specific language that allows for rapid iterations. C++ provides a very \nstrict syntax, and most of the advanced features used to build DSLs are difficult, if not \nimpossible, to use. Another problem of relying on C++ to build an internal DSL is the \ncompilation process that may disturb domain experts without any programming \nbackground. However, C++ provides macros, nested functions, method chaining, and \ntemplates that are powerful tools for building fluent interfaces. \nDevelopers who create DSLs using C++ as a host language must be careful about build \ntimes, ease of debugging, and code bloat, as many of the aforementioned techniques can \nlead to these problems if not used properly. \nEngine Integration through Code Generation \nIntegrating a DSL that relies on a language other than the one used by the engine is made \npossible by using DSLs as application generators. In this case, the domain-specific language \nis used to input domain knowledge and transform these high-level specifications to low-level \ncode that will be included in the engine. This approach provides the same advantages as \nany other code generation technique: End users can easily input data without worrying \nabout the implementation, programmers can modify an implementation without the user \nnoticing, and code need only be optimized once per code generator. \nAlthough this technique has the advantage of separating the domain-specific language from \nthe language used to implement the engine, it has the major drawback of increasing the \ncomplexity of the build process. \nAn example of DSL relying on code generation is Unreal Script, as it binds scripts to native \nclasses by generating C++ headers. Although this is very convenient for debugging and for \nvery tight integration into the engine, it requires script programmers to be really careful \nwhen modifying scripts, because it may trigger a full rebuild of the engine. When using this \ncode generation technique, developers must try to reduce compilation and link time as \nmuch as possible. \nEngine Integration through Interpretation \nDSLs can also be integrated into the engine by using a virtual machine that will read and \nexecute domain-specific code at run time. Embedding virtual machines for languages such \nas Lua or Python has been a popular method for years, and it is possible to build internal \nDSLs on top of such languages. Another path is to create an external DSL and embed its \nvirtual machine inside the engine, like [Gregory09] and [Sweeney06]. This integration \n",
      "content_length": 3137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": " \n \nmethod has the advantage of removing any constraint previously imposed by the engine‘s \nlanguage and also helping reduce iteration cycles, but it sacrifices run-time performances. \nIndependent of building your own virtual machine or using a preexisting one, it is crucial to \nprovide tools that will assist the debugging phase, since this new language will add an extra \nlayer of complexity. \nEngine Integration through a Hybrid Approach \nAn interesting way to integrate DSLs in an engine is a hybrid approach where DSL code can \nbe either compiled to machine code or interpreted by a virtual machine. Although such an \napproach requires substantial effort to write compilers and interpreters, it would provide the \nbest of both worlds, allowing for fast iteration during development and maximizing release \nbuild performance. \nTools are crucial to overcome debugging issues, but developers need to also care about the \nexecution environment of scripts, as it will change when going from interpreted to compiled. \nThe quickest way to set up this hybrid approach for game engines is to create an internal \nDSL using an already available interpreter, such as Lua, Python, Lisp, or Ruby, and bind it \nto the native engine‘s framework. Code generation routines should be written in the host \nlanguage and used to translate DSL code to native code relying on the engine‘s framework. \nPipeline Integration through Data Generation \nTools that help domain experts input their knowledge into the pipeline usually provide an \ninterface relying on domain-specific languages. Tools providing DSLs integrated into game \npipelines are very common. Unreal Engine provides Kismet for scripting game events \n[Unreal05], Crytek‘s CryENGINE offers Flow Graph—a visual editing system allowing \ndesigners to script game logic [Crytek09]. Other examples exist in the field of artificial \nintelligence [Borovikov08]. \nPipeline Integration through Centralization \nDSLs let users encode domain knowledge using custom syntax and usually help centralize \nthis knowledge. As a side effect, it can be very interesting to use DSLs not only to encode \ndomain knowledge, but also to distribute it to any application of the production pipeline, \neasing knowledge transfer across multiple languages and applications. For example, tools \nsuch as Google‘s protocol buffers or Facebook‘s thrift provide domain-specific languages \nthat ease data transfer across complex application architectures, which are very similar to \ngame pipelines. \n \nConclusion \nDomain-specific languages have been around for a long time and are successfully employed \nto solve a wide variety of problems throughout the software industry. They offer tailored \nsolutions, are easy to learn and manipulate, enable various opportunities to mine the \nknowledge they encode, and focus on end user experience. It is still difficult to reduce the \ncosts associated with creating and learning several languages, but because video game \ndevelopment addresses such a wide range of problem domains, it seems to be a perfect fit \nfor domain-specific languages. \n \nReferences \n",
      "content_length": 3106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": " \n \n[Borovikov08] Borovikov, Igor, and Aleksey Kadukin. ―Building a Behavior Editor for \nAbstract State Machines.‖ AI Game Programming Wisdom 4. Boston: Charles River Media, \n2008. \n[Crytek09] CryEngine Team, Crytek. ―CryEngine3 specifications.‖ 11 March 2009. \n[Gregory09] Gregory, Jason. ―State-Based Scripting in Uncharted2.‖ Game Developers \nConference. 2009. \n[Sweeney06] Sweeney, Tim. ―The Next Mainstream Programming Language: A Game \nDeveloper‘s Perspective.‖ Symposium on Principles of Programming Languages. 2006. \n[Unreal05] Unreal Engine Team, Epic. ―Unreal Kismet, the Visual Scripting System.‖ 2005-\n2008. <http://www.unrealtechnology.com/features.php?ref=kismet>. \n \n4.10. A Flexible User Interface Layout System for Divergent \nEnvironments \nGero Gerber, Electronic Arts (EA Phenomic) \nkontakt@gerogerber.de \nThe more people you want to address with your game, the more divergent system \nenvironments you have to support and take into account. The differences are not only \ndifferent CPUs and/or GPUs, but also displays. So what you want and need to develop is \nsoftware that scales with its environment and makes efficient use of it in all aspects. \nIn this gem, we highlight an approach to efficient resource usage, especially the available \nscreen size, from the perspective of the user interface (UI) layout. We show how you can \nkeep the UI layout system sufficiently flexible so that, once in place, you can achieve \noptimal layout results without the need to handle special cases in the source code. \nThe Problem \nThe more UI elements (widgets) you have in your game, and the smaller the screen size of \nyour minimum supported system requirements, the more important it is to have efficient \nand flexible UI layouts. In addition to the fact that there‘s a wide range of screen resolutions \nout there, you also have to take into account the fact that there are different aspect ratios. \nFor example, many current laptops make use of previously unusual aspect ratios. So when \ndesigning the UI layout, you may need to consider more than the widget‘s size, position, \nand font size. In most cases, where the difference between minimum supported system \nrequirements and high end is large, you also have to make use of different assets—for \nexample, textures, which you put on the widgets in order to have a crisply rendered widget. \nFinding an algorithmic solution for making optimal use of screen space and giving a useful \nlayout is difficult and may, if done in code, not always result in solutions UI designers or \nartists want. \n \nSome Cheap Solutions \nThere are some solutions that solve this problem in a cheap and sometimes acceptable way. \n",
      "content_length": 2668,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": " \n \nThe first solution makes use of virtual screen coordinates (see Figure 4.10.1). Here the \nspace the UI works with is always the same (for example, 1024×768). All widgets are \npositioned in this virtual space. When the physical screen resolution differs from the virtual \nspace, all widgets are then scaled automatically. This effectively projects the virtual screen \ncoordinates onto the physical screen resolution. As long as the two spaces do not differ that \nmuch in size or aspect ratio, everything can look acceptable. But if the difference in size or \naspect ratio becomes too large, you get non-uniform-scaled widgets with blurred textures. \nFigure 4.10.1. Virtual screen coordinates. \n \n \nThe second solution works for designs with only a few widgets. Here, all widgets are bound \nto a combination of screen borders or are centered. So when, for example, you define a \nwidget that is bound to the lower and right screen borders, the widget will stick to the \nlower-right part of the screen when the screen resolution increases in width or height. You \ncan see border linking as a form of alignment (see Figure 4.10.2). (.Net users may know \nborder linking under the name of anchoring.) The difference is that with border linking, in \ncontrast to simple alignment, you can define multiple borders to link to, and you can define \na specific number of pixels the widget has to stay away from the borders. This way, you can \nmake use of a large number of screen resolutions without non-uniform-scaled widgets or \nblurred textures. The drawback here is that with increasing screen resolutions, the widgets \nmake use of a smaller part of the screen and so become harder to read. At the other end, \nbelow a given screen resolution the widgets may start overlapping each other, which may \nnot be the desired behavior. But for some designs, this may be the way to go. \nFigure 4.10.2. Border linking. \n",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": " \n \n \n \nHowever, both solutions may not work optimally when additional constraints exist. For \nexample, when you develop for consoles such as Xbox 360, you have to take into account \nthe fact that on some TVs, only 80 to 90 percent of the picture may be visible. This area is \ncalled the title-safe region. Parts of the display outside this region may not be visible and \nshould not be occupied by any widget or other important game elements. This case has to \nbe considered in the UI layout when running in this special environment. \nAnother solution would be to do multiple UI layouts and check inside the source code for \nwhich layout to use (see Figure 4.10.3). This results in significant extra work for the UI \nartist, and you have many duplicate layout definitions that need to be kept in sync. Adding, \nremoving, or editing existing widgets in multiple layouts can become complicated and error \nprone. \nFigure 4.10.3. Multi UI layout scenario. \n",
      "content_length": 953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": " \n \n \n \nThings become worse for the software engineer in a multi-layout scenario when, at run \ntime, widgets have to be created (for example, adding new widgets to a list box), widget \ntextures have to be exchanged, or UI effects have to be started (for example, you want to \nuse different scaled UI effects for different screen resolutions). In these cases there would \nhave to be a check in the source code that decides which asset to load in order to fit into \nthe current UI layout. When, later during development, new layouts are added or removed, \nall these code sections have to be adjusted in order to make use of this new layout. \n \nA More Flexible Solution \nWidgets are defined by a set of properties (position, size, texture, font style, and so on) \nthat can be seen as key-value pairs. Many applications make use of XML to define widgets. \nUsing a binary format would result in faster loading times, but for the sake of readability, \nwe will stick to plain text XML in the examples. The following is a sample XML definition for \na widget that defines the widget‘s name, position, and size. \n<Widget \n   name = \"player_name\" \n   x = \"10\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n/> \n \nThis example is static and may not suit all environments. It could be better to have a larger \nwidget in larger screen resolutions. A more flexible solution we have already used in a \nsimpler form in our last project is the use of conditional modifiers (CM). A conditional \nmodifier is a node in a widget‘s XML definition that contains an additional set of widget \nproperties. You can attach a CM to any widget. A CM always contains a set of conditions \nconnected with condition operators, which, when evaluated to true, enable the properties \n",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": " \n \ndefined inside the CM. An example of a CM inside a widget‘s XML definition is presented \nhere: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n> \n   <CM \n      cm_name = \"xbox360\" \n      x = \"30\" \n      y = \"30\" \n      width = \"40\" \n      height = \"50\" \n   > \n      <Conditions> \n         <Condition_Platform \n            value = \"xbox360\" \n         /> \n      </Conditions> \n   </CM> \n</Widget> \n \nIn this example the widget is created with the properties defined in the Widget node. As a \nchild node of the widget, we added a CM node. As a child node of the CM, there is a \ncondition that evaluates to true when the current platform is equal to xbox360. The CM is \nloaded into memory when the corresponding condition is fulfilled. After the widget is fully \ncreated, we iterate over all loaded CMs attached to the widget (in the order defined in the \nXML). For each CM whose conditions evaluate to true, we apply the specified properties to \nthe outer widget. The property cm_name inside the CM node is just cosmetic and shall help \nto give the CM construct a meaning. So in the example above on a PC, the widget would be \nplaced at position (15, 15) with a size of (20, 30), and on the Xbox 360 its position would \nbe (30, 30) with a width of (40, 50). Of course, you do not have to re-specify all widget \nproperties inside a CM, but only those will be applied when the CM evaluates to true. So in \nthis example, you could, for example, only change the size of the widget if needed. \nIt is also possible to combine the previously discussed border-linking solution with the CM \nsystem. \n \nConcatenate Conditional Modifiers and Conditions \nUsing only a single CM or a single condition inside a CM is not that useful. So to further \nimprove the flexibility of CMs, you can chain together multiple conditions and CMs to form a \nmore complex set of conditions. In the following example you can see how this works: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n",
      "content_length": 2024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": " \n \n   width = \"20\" \n   height = \"30\" \n   texture = \"high_res_texture.tga\" \n> \n   <CM \n      cm_name = \"above_minspec_screen_size_pc\" \n      x = \"30\" \n      y = \"30\" \n   > \n      <Conditions> \n         <Condition_ScreenWidth \n            operator = \"greater_than\" \n            value = \"1024\" \n         /> \n         <Condition_Operator_And/> \n         <Condition_ScreenHeight \n            operator = \"greater_than\" \n            value = \"768\" \n         /> \n         <Condition_Operator_And/> \n         <Condition_ Platform \n            negate = \"true\" \n            value = \"xbox360\" \n         /> \n      </Conditions> \n   </CM> \n   <CM \n      cm_name = \"low_res_gfx\" \n      texture = \"low_res_texture.tga\" \n   > \n      <Condition_MinSpec/> \n   </CM> \n</Widget> \n \nHere we apply the properties inside the CM only in the case that screen width is greater \nthan 1024, screen height is greater than 768, and if the current platform is not Xbox 360. \nIn this case we change the widget‘s position. As you can see in this example, there‘s a \nsecond CM that changes the widget‘s texture to a low-resolution variant when \nCondition_MinSpec evaluates to true. Both CMs are independent from each other and \nchange different properties of the widget in a different environment. It is also possible for \ntwo different CMs to change the same widget property in different cases. The number of \ndifferent conditions only depends on the needs you have to obtain the desired UI layout for \na specific environment. \n \nImplementation Details of the CM System \nImplementing the CM system is straightforward. In order to keep the creation of CMs and \ntheir corresponding conditions in one place in the code, you can use the well-known Factory \nMethod Pattern [Gamma94] for creating conditions. Figure 4.10.4 shows the class diagram \nfor the CM system. \n",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": " \n \nFigure 4.10.4. UML class diagram for the conditional modifier system. \n \nWhat you can see from the UML diagram is that a widget can contain an arbitrary number of \nCMs and that each CM contains at least one condition. Two conditions with a condition \noperator in between form a condition pair that can be evaluated. Of course, you have to \nconsider operator precedence in these cases. For the sake of simplicity, we show only two \ncustom conditions in the diagram (ConditionScreenWidth and \nConditionScreenHeight). Additional condition types can be added, depending on your \nneeds. You can also add custom condition operators derived from Condition-\nOperatorBase. \n \nConditional Modifier Condition References \nAs you may have noticed, a non-trivial CM definition adds a good deal of potentially \nredundant data to each widget definition. This redundancy can be resolved by adding a CM-\nspecific property (cm_conditions_reference) to the CM definition that references an \nXML file containing all the relevant conditions and condition operators. The advantage of \nthis is that when you have to modify this set of conditions, you only need to touch one \nsingle file, and all referring CMs will work with the new definition. Here‘s an example: \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   width = \"20\" \n   height = \"30\" \n",
      "content_length": 1340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": " \n \n> \n  <CM \n      cm_name = \"xbox360\" \n      x = \"30\" \n      y = \"30\" \n       width = \"40\" \n       height = \"50\" \n       cm_conditions_reference = \"conditions_xbox360.xml\" \n   /> \n</Widget> \n \nFollowing are the corresponding definitions from conditions_xbox360.xml. \n<Conditions> \n   <Condition_ScreenWidth \n      operator = \"greater_than\" \n      value = \"1024\" \n   /> \n   <Condition_Operator_And/> \n   <Condition_ScreenHeight \n      operator = \"greater_than\" \n      value = \"768\" \n   /> \n   <Condition_Operator_And/> \n   <Condition_ Platform \n      value = \"xbox360\" \n   /> \n</Conditions> \n \nFigure 4.10.5. Conditional modifier condition references. \n \n \n \n \n",
      "content_length": 662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": " \n \nWidget Templates \nWe can further improve the way we define widgets, especially if we make use of many \nsimilar widget definitions. A good example for this is the definition for a default button style \nin a game. Default buttons may share many properties that are equal in all instances. For \nexample, the default button‘s size, texture states, sounds, and so on would be the same for \neach instance of a default button. It would not make much sense if at each place where you \ndefine some sort of default button, you have to specify all the properties by which a default \nbutton is defined. For these cases you can make use of widget templates (see Figure \n4.10.6). \nFigure 4.10.6. Defining widgets with widget template files. \n \n \nA widget template defines a widget with a default set of properties. A widget template is an \nadditional XML widget definition file. A concrete widget definition can refer to widget \ntemplates via the additional property template_filename. The widget template can \ncontain CMs and conditions. From the implementation point of view, you create a widget \ninstance from the specified template XML definition, apply all properties found in the widget \ntemplate, then apply all CMs defined in the template, and at the end you set the instance-\nspecific properties for the concrete widget instance. If we stay with the default button \nexample instance, specific properties would be the text shown on the button or the event \nthat gets fired when clicking the button. This way you can save a lot of data inside the \nwidget definitions, and you can make changes quickly because you only have to change the \nwidget template itself. This feature is particularly useful for global styles. The following \nexample shows how you can make use of widget templates. \n<Widget \n   name = \"player_name\" \n   x = \"15\" \n   y = \"15\" \n   template_filename=\"default_widget.xml\" \n/> \n \nThe property template_filename refers to the widget template and contains all default \nproperties and CMs including conditions. So in this example, the only custom properties for \nthis widget instance are its position and its name. \n",
      "content_length": 2128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": " \n \n \nProxy Assets \nMany situations require assets such as textures or UI effects to be loaded at run time. One \nexample would be that you want to exchange a texture or show some UI effects to highlight \nsome widget. This requires the specific UI effects that fit the current widget shape and size \nto be loaded. At this point, when using CMs, you don‘t know which asset to load, because \non different widget sizes you may want to apply textures with different resolutions, and on \ndifferent widget shapes and widget sizes you have to use different UI effects. In order to \ndecouple these problems from the source code, you can make use of proxy assets. A proxy \nasset is an XML file that contains the asset information itself (for example, the path to some \ntexture or some UI effects file) and some CMs that control which asset to use in which \nenvironment. This is similar to the use of CMs in conjunction with widgets. Here is an \nexample of a proxy asset for a texture: \n<Texture \n   filename = \"high_res_texture.tga\" \n> \n   <CM \n      cm_name = \"minspec\" \n      filename = \"low_res_texture.tga\" \n   > \n      <Conditions> \n         <Condition_MinSpec/> \n      </Conditions> \n   </CM> \n</Texture> \n \nThis example by default uses the texture high_res_texture.tga. Only when \nCondition_MinSpec evaluates to true do we actually load low_res_texture.tga. \nSo in the source code you would reference the proxy asset‘s XML definition instead of a \nconcrete texture filename. This way, we have decoupled the asset from the source code. \nBecause we use the same CM system here as described in conjunction with widgets, we can \nmake use of the same factory as well. So when adding new conditions, you can use them \nautomatically for proxy assets, too. See Figure 4.10.7. \nFigure 4.10.7. Proxy assets decouple assets from the source code. \n \n \n \n \n",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": " \n \nPerformance \nOf course, all this parsing of CMs takes time. The conditions need only be evaluated once, \nso it would make sense to keep the conditions in memory and query the result when \nneeded. This is, of course, only valid in cases where you do not create conditions that can \nchange during run time. The same is true for widget templates. If your engine supports the \ncloning of widgets, you can keep a copy of the widget template in memory and clone it \nwhen you create a new widget instance of it. Proxy assets can be optimized by loading and \nevaluating them only once and reusing the results from memory. \n \nProblems \nWhen using CMs, the definition of widgets becomes a more complex task, and additional \nwork is required to configure all CMs and their corresponding conditions. We recommend \ntool support for adding, removing, and modifying CMs. In our last project, we had \neffectively three different UI layouts we used for all different screen resolutions. We defined \na minimum screen resolution of 1024×768 (4:3), a medium one of 1280×1024 (5:4), and a \nlarge one defined by 1680×1050 (16:10). Screen resolutions greater in width and height \nthan the ones defined use the large screen resolution layout. Screen resolutions in between \nuse the next smaller layout. UI elements positioned relative to a screen border keep the \ncorrect position via their border linking property. Because of this, it is possible to scale the \nUI layout to higher screen resolutions. This way, we covered the most common screen \nresolutions used by our customers. We also disabled some laptop screen resolutions in order \nto save time. Of course, when running the game with extremely large screen resolutions, \nyou run into the problem described earlier (widgets become small in relation to screen size). \nFortunately, screen sizes do not grow at such a pace that this should become a problem. \nThe UI Editor we used in house was capable of generating CMs recursively regarding the \nposition and size of the widgets. Therefore, we created our base UI layout for the smallest \nsupported screen resolution of 1024×768 and let the UI Editor create CMs with a \ncorresponding scale factor for the higher resolutions. \nAnother aspect you have to consider is the additional amount of testing you need when you \nhave different UI layouts. Therefore, you should plan early which UI layouts you need and \nby which conditions these layouts are being controlled. This way, your QA can check out the \ndifferent UI layouts with a defined checklist. \n \nConclusion \nThe CM system described in this gem gives you a good deal of flexibility in defining your UI \nlayout. It offers UI designers and UI artists many options to define the UI layout without \nrequiring corresponding source code changes. On the other hand, you have to invest some \nwork to integrate this system into your engine, and testing the UI layouts is more work \ncompared to just testing a single simple UI layout. But from experience with this system, \nthe advantages outweigh the additional amount of work. \n \nReferences \n[Gamma94] Gamma, E., et. al. Design Patterns: Elements of Reusable Object-Oriented \nSoftware. Addison-Wesley Professional, 1994. \n",
      "content_length": 3203,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": " \n \n \n4.11. Road Creation for Projectable Terrain Meshes \nIgor Borovikov, Aleksey Kadukin \nigor.borovikov@gmail.com, akadukin@gmail.com \nRoads are becoming an increasingly important part of large environments in modern games. \nThey play a significant role for both the aesthetics and the functionality of the environment. \nIn this gem, we explore several techniques that are useful for modeling roads in a large \ngame environment like that of The Sims 3 by Electronic Arts [EA:Sims3]. The techniques \ndiscussed are general, and for simplicity we limit our discussion to the case of projectable \nterrain meshes. Some of our inspiration was drawn from earlier works, such as Digital \nElement World Builder [DEWB], a package for modeling and rendering natural scenes. \nWe start by building a trajectory for a road on a projectable mesh using elements of \nvariational calculus and describe an algorithm for creating optimal paths on the terrain \nsurface. Next, we move to building a mesh for the terrain while satisfying a number of \nnatural design requirements for the local road geometry. \nRoads as Geodesic Curves \nMany roads in the real world exhibit certain optimal properties in a sense that they were \nbuilt to minimize the cost of connecting one location to another. As a trivial example, \nconsider an ideal road between two points A and B on a horizontal plane, which is a straight \nline and is the shortest length curve connecting the points. \nWhen placing roads between two locations on a terrain model, we can use the same \nprinciple and require that a certain cost function is minimized on the curve representing the \nroad. Variational calculus provides a framework for describing such curves. An introduction \nto variational calculus can be found in any good book on differential geometry, for example \n[DNF91]. We will limit the formal part of our presentation here to the bare minimum, \nskipping details that are not important for the algorithm we propose. \nVariation Problem for Roads \nWe will represent a road between points A and B on a terrain with a smooth curve γ(t) \nparameterized with the natural parameter t ∊[0, L] (in other words, the curve is \nparameterized by the arc length), where γ(0)=A, γ(L)=B, and L is the length of the curve. \nThen we can define the cost of moving from A to B along such curve as the length of the \ncurve: \nEquation 1  \n \n \nwhere l(t) is the cost function: \nEquation 2  \n \n",
      "content_length": 2422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": " \n \n \nThis cost function is calculated based on a Euclidian metric in 3D space. The cost function \n(2) restricted to the terrain surface induces the so-called Riemannian metric on the terrain \n(which still directly corresponds to our intuitive geometric notions of distance). Shortest \nlength curves on such a Riemannian space are called geodesics. \nFor terrain modeled as a height map z = f(x,y), we have: \nEquation 3  \n \n \nGeodesics for such a cost function are similar to a rubber band stretched across the terrain \nalong the shortest trajectory between destination points while keeping contact with the \nsurface across their entire length. Such curves provide the shortest path but do not \ncorrespond to our desire to provide natural-looking roads, and in particular they ignore the \nrole of gravity. \nAn important observation is that actual roads also minimize variation of altitude along the \ntrajectory. This is true for both foot trails and automobile roads. Instead of going along the \nshortest path across a hill, roads tend to bend around while trying to maintain the same \naltitude. The altitude variation element along a path is: \nEquation 4  \n \n \nWe can define a new length element as follows: \nEquation 5  \n \n \nwith λ>=0. For larger values of λ, we expect the geodesics to be ―flatter‖ in the z direction. \nUsing in (1) the new cost function (5) will give us: \nEquation 6  \n \n \nwhich will provide a better approximation to the behavior of actual roads. The parameter λ \ncorresponds to the tradeoff between the desire to reach the destination point as soon as \npossible versus the desire to save on climbing up and down any slopes along the pathway. \n",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": " \n \nSolutions for the variational problem (1) can be found among the solutions of Euler-\nLagrange equations: \nEquation 7  \n \n \nWhen solving the equations (3), we need to also satisfy the boundary conditions γ(0) = A \nand γ(L) = B. However, solving the boundary condition problem for the equations (7) is not \na simple task. Instead, in the next section, we will obtain a numeric solution by directly \noptimizing a discrete version of (6). \nNumerical Solution for Geodesics \nHere we will look for the minimum of the cost functional (6) directly by approximating the \nsmooth curve γ with a piecewise linear curve on the surface. The following procedure \nconverges to the solution of the original continuous problem in a wide range of conditions; \nhowever, in the interest of brevity, we will leave out the proof. \nA piecewise linear representation of the curve γ(t) requires n+1 nodes: {γi, i=0,...,n} or, in \ncoordinates, γi=(xi, yi, f(xi, yi)). For such a curve, there will be n line cuts between nodes: \nci=[γi, γi+1]. The length of the line cut ci is denoted as |ci|. Obviously, \nEquation 8  \n \n \nThe discrete approximation of the cost along a curve for the variational problem (7) is the \nfollowing sum instead of an integral: \nEquation 9  \n \n \nFor a fixed number of nodes, the following iterative optimization process allows us to find \nlocal minimum for (9). \nG := calculate the cost using (9) \nCostChange := G \nWhile cost change is greater than threshold do: \nFor each node 0<i<n do: \n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": " \n \n \nNewG := calculate using (9) \nCostChange: = G – NewG \nG := NewG \nwhere Mi is the line equidistant from the current nodes γi-1 and γi+ 1. In other words, for \neach three neighbor nodes, we optimize the location of the middle node by finding the \nminimum cost local to the two-segment part of the path by varying the location of the \nmiddle node on the middle line Mi between the two other nodes. See Figure 4.11.1. \nFigure 4.11.1. Optimization of the middle node. \n \n \nSuch a choice of the per-node optimization domain ensures that the spacing between nodes \nwill remain relatively uniform, thus adding to the method‘s stability. Also, single-parameter \noptimization is much cheaper computationally than the two-parameter optimization case, \nwhich searches for an optimal location in an area between two nodes. Another advantage is \nthat such an optimization is in direct correspondence with the variations used in the proof, \nsuch that we get the correct continuous limit for our procedure when the number of nodes \napproaches infinity. Note that we optimize the location of every node except the first and \nlast nodes of the curve. The iterations stop when the cumulative change of cost is less than \na given threshold, which is input as a parameter for the method. \nThe method can be relatively expensive for a detailed representation of a road curve with \nmany nodes. This can be addressed by starting with a less detailed representation with \nfewer nodes and subdividing it when the iterations on this curve stop. Subdivision stops \nwhen the length of the road segment reaches a minimal allowed length. This minimal length \ndepends on the design requirements but can be naturally set comparable to the distance \nbetween vertices for the regular mesh built from the height field. \n",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": " \n \nA Python-based version of the algorithm is provided on the CD for your experimentation. \nThis takes the file ridge.png as input and generates ridge_path.png as output. These are \nshown in the left and right sides, respectively, of Figure 4.11.2. Note that the Python code \nrequires Python 2.4 or later along with a matching version of the Python Imaging Library \n(PIL). \nA final touch for the created curve could be node resolution optimization. We can eliminate \nnodes where the angle between adjoined segments is sufficiently small. Again, a cost \nfunction and angle can provide criteria for optimization so that nodes are not removed too \naggressively. \nFigure 4.11.2. Sample paths built with the proposed algorithm. \n \nThe branching of roads requires a little extra work. The problem of connecting Point C to \none of the Points A or B in the presence of an already-built road from A to B can be reduced \nto the same variational problem with any movement along the existing road being negligibly \nsmall in comparison to the movement across terrain without roads. This simplification may \nbe used to reduce the problem to a similar variational problem but with a slightly different \nboundary condition: The end point D of the new curve will belong to the existing road rather \nthan being fixed to A or B. The required modification to the algorithm is very simple: We \ninclude D in the optimization by allowing it to move freely between the nodes of the existing \nroad. \n \nRoad Grading Techniques for The Sims 3 Worlds \nAfter creating a trajectory for a road, we need to deliver a road mesh that is consistent with \nthe terrain—this means the terrain mesh also needs to be locally modified around the road. \nThis produced a number of challenges during The Sims 3 [EA:Sims3] development. \nBlending Roads with Terrain in The Sims 3 \nThe Sims 3 worlds were designed without grid-based placement restrictions for roads. The \nroads could be placed anywhere with respect to the mesh, and the road segments were not \nlimited to straight lines and simple intersections. \n",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": " \n \nRoad segments were the spline-based curves connected to each other by connection ports. \nThe intersections have fixed (prebuilt) geometry. The world designer could place and edit \nroads in an in-house build tool. The Sims 3 world terrain is a 3D mesh generated from a 2D \nimage height map, which played a critical role in road network construction algorithms. \nThe road mesh is generated using a base spline and world terrain geometry that is covered \nby the road segment. This provides a relatively easy and intuitive way to lay down the road \nnetwork. However, as shown in Figure 4.11.3, the road surface generated in such way \nfollows the terrain surface exactly and may not be smooth enough. \nFigure 4.11.3. The road segment placed on the uneven terrain. \n \n \n \nRoad Grade and Road Slope \nThe road surface must satisfy a number of conditions enforced by the road-grading tool. \nThe most important characteristics controlled by this tool are road grade and road slope. A \nroad slope is defined as a road surface tilt (the gradient perpendicular to the road), and a \nroad grade is defined as steepness of the road (the gradient parallel to the road) [Wiest98]. \nThe tool allows designers to set up the road grade and slope limits and automatically modify \nthe road surface to create a realistic look and improve the routable functionality of roads. \nThe algorithm adjusts individual or connected road segments‘ tilt and steepness using \nheight map grid cells. We describe a technique that deals with the slope and grade limits \nseparately. \nFlattening Road Slope \nThe purpose of the road slope flattening algorithm is to set to zero the slope across a road \nsegment. The same technique can be applied to the connected road segments. \nThe actual road mesh modification is accomplished by a world terrain height map data \nmodification. The advantage of this method is a smooth coupling of road surface with the \nsurrounding terrain. The disadvantage is that the road surface geometry will become \nimprinted to the terrain geometry. Moving the graded road will leave a modified terrain \nbehind, and this may lead to visual artifacts. \n",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": " \n \nAlternatively, the road network surface could be managed by a separate height map, and \nterrain generation code should support multiple height map layers to combine the original \nterrain data and road network maps. \nThe road segment spline defines a road profile. At the first step, we divide a road segment \nby sub-dividers, as shown in Figure 4.11.4. \nFigure 4.11.4. The road segment divided by sub-dividers. \n \n \nThe distance between sub-dividers depends on the terrain height map resolution and the \nroad edges‘ lengths. For example, if each height map point represents a 1×1 unit in world \ncoordinates, then the sub-divider distance could be an average value between maximum \nand minimum road edge lengths. \nThe second step is determining a height for sub-divider end points. The height of sub-\ndivider end points should match the height of the sub-divider center point (a height at the \nintersection between the sub-divider and a road spline) to set zero road slope. \nThe third step is applying a new height to each height map point inside of a sub-quad \nformed by a pair of sub-dividers. Since there is no guarantee that the four points defining \nthe sub-quad corners belong to the same plane, we need to triangulate each sub-quad and \nflatten the inner height map points for each triangle, as shown in Figure 4.11.5. We use the \nplane equation to calculate the new height of height map points inside the triangle. \nFigure 4.11.5. Triangulated road segment. \n \n \nThe fourth and final step is to create a smooth road surface. The results of the previous step \ncan leave visual artifacts between road sub-quads. To fix the surface, we use a bilinear \n",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": " \n \ninterpolation between neighboring height map points across the entire road segment. In the \ncase of connected road segments, the bilinear interpolation should be performed across the \nentire road network. Figure 4.11.6 shows the result. \nFigure 4.11.6. Smoothed road segment. \n \n \n \nLimiting Road Grade Angle \nTo limit a road segment grade, we use a modified road slope flattening technique. After the \nroad segment subdivision, we calculate an inclination angle for each sub-quad. Then we \nadjust the angle by raising or lowering sub-quad dividers. The challenge here is to \ndetermine which road direction should be used for raising or lowering each end. Figure \n4.11.7 shows the difference. \nFigure 4.11.7. The road grade limit difference for opposed road directions. \n \n \n \nResults and Possible Generalizations \nThese algorithms were very useful during The Sims 3 development and saved a significant \namount of world designers‘ time by automating mesh modifications. The in-game routing \nsystem also benefited from flattened road slopes and reasonably limited road grades. \n",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": " \n \nThe same algorithms could also be applied to the flattening of water surfaces (such as \nrivers) or can be modified to achieve more complex road surfaces, such as road banking on \ncurves. However, using such an algorithm on non-height map–based terrain meshes may \nrequire different approaches for sub-quad height calculation, surface triangle flattening, and \nsurrounding geometry transitions. \n \nConclusion \nThese techniques cover a relatively complete set of road creation tasks for large \nenvironments. An abstract definition of a road as a geodesic curve may not be sufficient to \ndeliver fully automated road creation, but it can provide a good starting point for the world \ndesigners, from which they can tweak road layout to their liking. The road-grading tool \nproved to be so versatile that very few (if any) manual adjustments of the meshes were \nrequired after the road was created and imprinted into the terrain. \n \nReferences \n[EA:Sims3] The Sims 3, PC/Mac. Electronic Arts, 2009. \n[DEWB] ―Digital Element WorldBuilder, PC.‖ n.d. Digital Element Inc. n.d. <http://www.digi-\nelement.com/wb/index.htm>. \n[DNF91] Dubrovin, B. A., A. T. Fomenko, and S. P. Novikov. Modern Geometry—Methods \nand Applications: Part I: The Geometry of Surfaces, Transformation Groups, and Fields. \nSpringer, 1991. \n[Wiest98] Wiest, Richard L. ―A Landowner‘s Guide to Building Forest Access Roads.‖ July \n1998. National Forest Service. n.d. \n<http://www.na.fs.fed.us/SPFO/pubs/stewardship/accessroads/accessroads.htm>. \n \n4.12. Developing for Digital Drawing Tablets \nNeil Gower \nneilg@vertexblast.com \nA walk though the art department in almost any game studio will reveal a wide selection of \ndigital drawing tablets. In their most basic form, tablets replace the mouse with a stylus for \npointer input. However, treating a tablet as merely a mouse substitute greatly underutilizes \nits potential. Most tablets offer a rich set of inputs, including pressure and tilt sensitivity as \nwell as a variety of buttons. \nIn this gem, we look at ways to harness the full potential of the drawing tablet as an input \ndevice, first by surveying the research on pen-based interfaces and then by developing an \ninterface layer in C++ to conveniently access tablet functionality. \nEquipped with this knowledge, you will be able to make full use of tablet features directly in \nyour tools. Along the way, we also note best practices to follow when developing for tablets. \n",
      "content_length": 2453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": " \n \nBackground \nThe pen-based interface devices we find in game development are typically digitizer tablets, \ntablet PCs, or displays with built-in tablet functionality. Standard tablets provide a form of \nindirect interaction—the user moves the pen on a tablet on their desk to move the pointer \non the screen. Tablet PCs and tablet displays provide direct interaction by allowing the user \nto place the pen directly on the display. Direct interaction can be very empowering, \nalthough it is not yet commonplace. The majority of the pen-based devices we work with \ntoday are indirect tablet interfaces. \nBuilding a Better Mouse \nIf you and your users are already proficient with the mouse (usually a safe assumption in a \ngame studio), you might wonder why you should bother with tablets. \nLow-level studies, such as [MacKenzie91], have shown that pen tablets perform as well as \nthe mouse for pointing tasks and even slightly better for dragging tasks. Considering more \ncomplex tasks, such as drawing, we find that mouse users have trouble reproducing strokes \nmore complex than straight lines [Kurtenbach93]. This is an intuitive result when you \nconsider the difficulty of signing your name with a mouse versus with a pen. \nFigure 4.12.1. Tablet terminology. \n \n \nIn addition to pen position and touch detection, a modern tablet offers features a mouse \ncannot match, such as reporting the amount of pressure on the pen tip, the tilt and \norientation of the pen, and even which end of the pen is being used. These input channels \ncan be combined with keyboard modifiers to create a huge, expressive palette of inputs to \nyour application. Recently, tablets and touch-based devices have begun to offer the ability \nto detect multiple points of contact as well, which opens up opportunities for more advanced \ngestures and for interfaces that support multiple users working together on a single device. \nApplications \n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": " \n \nTwo-dimensional drawing is one of the archetypal applications of pen tablets. Another class \nof application is digitizing, which involves precisely transcribing real-world data (such as \nschematics) into digital form. In most drawing and creative applications, precision is less \nimportant than expressiveness and convenience. Pressure is commonly used to vary brush \nparameters, such as width or opacity. Using keyboard shortcuts and extra buttons on the \ntablet, we can enable quick tool changes with minimal interruptions to the creative process. \nThe same principles from drawing can be transferred directly to game development \napplications. For example, terrain height-map editing—tip pressure can control height \nchanges or foliage density, and the eraser can be used to reverse the height changes or thin \nout foliage. Of course, with a modern tablet this is only the beginning. For example, \nadditional parameters, such as the normal direction of the foliage, can be taken from the \npen‘s tilt values. \nAnother important area of pen-based applications is sketch-based interfaces. This includes \nsketch-based modeling, which involves taking 2D pen strokes and transforming them into \n3D modeling operations, and sketch-based design tools. Sketch-based design tools can be \nused for everything from game logic state diagrams, to UI design and layout, to even UML \nor box-and-arrow software design. \nResearchers have found that pen-based interfaces are good for encouraging users to focus \non quick design iterations. (For example, see the discussion in [Kimura93].) This is in part \nbecause for most users, drawing with a pen is a familiar way to brainstorm. Other factors \ninclude the low level of detail and consequent low level of commitment to sketched content. \nKimura, et al. also discuss evidence that users of fully featured 3D programs get easily \ndistracted by style issues rather than focusing on substance during the design process, and \neven that tools that present content in a ―sketchy‖ style encourage faster iteration. By \nstripping our early design tools down to little more than the pen and the user, removing \ncomplex menus and tool palettes, we can potentially create situations where users just draw \nwhat they think, which would be the ideal design tool. \n \nTablet Theory \nBefore we jump into coding tablet-enabled tools, we will take a look at the research and \ntheory related to pen-based interfaces. The field of human-computer interaction (HCI) is \nrich with innovative ideas and empirical studies to help you make informed design decisions \nabout your tablet-enabled tools. \nSince most tools will not be designed exclusively for tablets, a comparison to the mouse is a \ngood place to start. Although low-level speed and accuracy are comparable, there are some \nsubtle differences between these devices. For example, users tend to produce larger \ngestures with the stylus than with the mouse [Moyle02], which may require you to scale the \nuser‘s input to compensate if you use a gesture-recognition system. There is also a \ntendency for right-handed users to err upward when moving the pointer to the right and \ndownward when moving left. The reverse is true of left-handed users. This horizontal error \neffect is less noticeable in mouse users; however, they tend to have more difficulty with \naccurate vertical motions compared to pen users. These differences imply that we should try \nto make decisions early on about whether to design our interfaces to favor one device type \nor to provide customized modes for each. \nAnother issue to consider is accuracy. When targets are only a few pixels wide, accurate \nselection can become an issue in pen-based systems. This is a common scenario in 3D \nediting applications—for example, when selecting vertices. Remember, too, that if the user \nmust hold the pen over a location before touching the tablet, he has to steady his hand \nwhile doing so. This is more difficult than with a mouse, which sits perfectly still on the desk \n",
      "content_length": 4014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": " \n \nwhen at rest. Direct-input setups, such as tablet PCs, introduce additional challenges for \nprecise selections, caused by the parallax between the display and its protective surface. \nOne solution to precision selection problems is presented in [Ramos07], which describes a \npressure-activated zoom lens. They found this interface effective at increasing both speed \nand accuracy, and it was well received by users. In pressure-activated systems, it is good \npractice to provide feedback to the user about how much pressure is required to activate \nthe system. In the case of the zoom lens, as the user approaches the pressure threshold, \nthe lens becomes more opaque to indicate its readiness. \nWorking in 3D \nManipulating 3D objects viewed with 2D displays using 2D input devices is always a \nchallenge. [Chen88] contains a good discussion of techniques for three-axis rotation \ntechniques that work with tablets. Their paper provides a good implementation-level \ndescription of their ―Virtual Sphere Controller.‖ \nThe virtual sphere essentially places a bounding sphere around the object to be rotated. \nDragging the stylus up and down or left and right rotates around X (forward) and Y (up), \nrespectively. To rotate around Z (out of page), the user moves the stylus around the \ncircumference of the sphere. \nFigure 4.12.2. 3D rotation controllers. \n \n \nAnother interesting stylus technique is the ―stirrer,‖ introduced by [Evan81]. For this tool, \nusers make small circular motions, analogous to stirring, to modify a single axis value. The \nstirrer has several nice properties. It can be used anywhere on the tablet, so it doesn‘t \nrequire the user to pay attention to pen location. Instead, users can focus on the object \nbeing manipulated. Users can also vary the rate of change by varying the size of the circles, \nwith larger circles changing the value more slowly. Evans uses the stirrer to create a virtual \n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": " \n \nthree-axis trackball, which maps x and y stylus movement to rotation around X and Y and a \nstirrer to rotation around Z. \nShortcuts and Gestural Interfaces \nAs users become more proficient with software tools, they seek ways to optimize their \nworkflow and focus their efforts on the creative aspects of their work. A typical optimization \nis the use of shortcut keys to directly invoke commands. In practice, however, studies have \nshown that many users fail to make the transition to using shortcut keys. Gestural shortcuts \nhave been shown to be easier to learn while still providing the same level of performance to \nexpert users. In [Appert09], there is a good discussion of the performance and \nimplementation of a gesture-recognition system for this purpose, including references to a \nvariety of alterative approaches as well. \nA general principle of UI design is to make the functions of the system self-revealing, which \njust means that users should be exposed to the functions through normal interaction with \nthe system. For example, listing shortcut keys with their menu items makes the shortcuts \nself-revealing. You can similarly show gesture marks next to menu items or draw the \ngestures on the screen briefly when the user invokes the command. This concept of self-\nrevealing gestural shortcuts has also evolved into an elegant menu solution—marking \nmenus. \nGestures and Pie Menus: Marking Menus \nMarking menus evolved out of pie menus. Pie menus are menus arranged in a circular \nlayout, rather than the traditional linear box arrangement. The user touches the pen down \nto open the menu and then drags the pointer into the pie slice corresponding to the \ncommand he wants. Once the correct command is highlighted, the user lifts the pen to \ninvoke the command. Different menus can be triggered by opening the menu in different \nareas of the workspace, similar to right-click context menus in mouse interfaces. \nPie menus present several advantages to tablet users. The circular layout means different \nmenu options are accessed by moving the pointer in different directions, rather than just \nvarying the magnitude in a single direction. This requires less precision and effort to make \ncorrect menu selections. Another useful property is that due to the widening of the slices as \nthe pointer moves away from the center of the pie, the user is able to dynamically increase \nthe effective size of the menu items, simply by making larger gestures. \nThe circular layout can also be used to represent logical relationships between menu \noptions—for example, placing logically opposite commands such as cut and paste spatially \nopposite each other in the menu. This helps users remember where commands are in the \nmenus. Studies show that even numbers of pie slices facilitate the best performance, \nespecially 4, 8, and 12. There is also evidence that right-handed users access the upper-\nright and lower-left quadrants of the pie most quickly (see [Hancock04]), so you may want \nto place your most frequently used commands in these spots. Since the effect is related to \nthe handedness of the user, it‘s also a good idea to provide an option to flip the menu \nlayout for left-handed users. \nMarking menus take pie menus and combine them with gestural shortcuts to create a highly \nefficient interface [Kurtenbach93]. The basic marking menu implementation is very similar \nto a pie menu. The menu is pressure activated, and the user‘s pen motions are shown as a \ntrail on the screen. There is also a delay before the pie menu appears. The user can move \nthe pen in the anticipated direction of their selection and lift the pen to complete the \nselection during this delay before the menu appears. In this case, they have ―marked‖ their \nselection. See Figure 4.12.3. \nFigure 4.12.3. Marking menus. \n",
      "content_length": 3821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": " \n \n \n \nWhat makes marking menus so effective is that using the pie-menu interface is also a \nrehearsal of the shortcut marking gesture, so learning the shortcuts does not require any \nadditional effort from the user. Since the gestures created by the marking menus are all \nsequences of linear motions, marking menus can be combined with other gesture-\nrecognition systems that use non-linear gestures. This concept can be extended to \nhierarchical marking menus as well. \n \nTablet Programming \nA high-level point about adding tablet support to your tools is that it‘s highly advisable to \npackage your tablet code into a reusable toolkit. Your toolkit can include things such as a \ngesture-recognition system, custom UI widgets, and low-level tablet interface code. This \nhelps provide consistency across applications and makes it easier and therefore more likely \nthat other tool developers will add gesture support to their projects. \nFor Windows applications, Wintab is the industry-standard API for accessing tablet devices. \nFor Mac OS X, we have a choice between the older Carbon (C) API and the newer Cocoa \n(Objective C) API. On Linux, we use the XFree86 input extension and XFree86 event queue. \nOn the CD-ROM, you will find an example program that uses Wintab to read and display the \nvarious types of tablet data discussed in this gem. \nTablets generally have a wide variety of device capabilities, much like PC joysticks. It‘s \nimportant to query the capabilities of the device at run time and make the necessary \nadjustments in your code. A few of these capabilities can even change at run time, such as \nthe type of stylus being used. \nGetting Started with Wintab \nTo get things started, let‘s take a look at what‘s needed to initialize Wintab. \n#define PACKETDATA (PK_X | PK_Y | PK_BUTTONS) \n#define PACKETMODE PK_BUTTONS \n#include \"pktdefs.h\" \n \nif ( !WTInfo(0, 0, NULL) ) { \n    // Wintab is not available on this system \n    return -1; \n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": " \n \n} \n \n// start with a default context... \nLOGCONTEXT contextParams; \nWTInfo( WTI_DEFCONTEXT,0, &contextParams ); \n \n// customize it for your application... \ncontextParams.lcOptions  |= CXO_MESSAGES | CXO_SYSTEM; \ncontextParams.lcPktData   = PACKETDATA; \ncontextParams.lcPktMode   = PACKETMODE; \ncontextParams.lcBtnUpMask = contextParams.lcBtnDnMask; \n \n// then get a context handle and we're ready to go! \nHCTX hContext = WTOpen( hWnd, &contextParams, TRUE ); \n \nAt the top, we have some macro magic that enables pktdefs.h to generate the PACKET \nstruct that matches the values you set in lcPktData further on. The second WTInfo() \ncall initializes our context parameters struct with reasonable defaults for most applications. \nThen we just tweak the parts we need before giving it to WTOpen() to initialize the tablet \ncontext and generate a handle for it. \nNow that we‘re up and running, let‘s see how to get position data from the tablet. With \nWintab, we get data from the tablet via packets in a message queue. When we configured \nthe context parameters, we had to set CXO_MESSAGES in lcOptions. This tells Wintab \nto send our application WT_PACKET messages through the standard Windows messaging \nsystem. WT_PACKET signals our code when the tablet has data queued up for the \napplication. In the handler for this message, we use WTPacket() to extract the data. \ncase WT_PACKET: \n    PACKET pkt; \n    WTPacket( hContext, wParam,&pkt ); \n    cursorX = pkt.pkX; \n    cursorY = pkt.pkY; \n    break; \n \nIt‘s important to stay on top of the packet queue, because if it overflows we‘ll stop getting \nWT_PACKET messages, and the tablet will stop sending state information. Practically \nspeaking, this means we‘ll usually use WTPacketsGet() to retrieve all of the packets \ncurrently in the queue and then process each one (demonstrated in the example code on \nthe CD-ROM). \nAs shown in the previous code listing, each packet contains the state information we \nrequested in PACKETDATA during initialization. In this case, pkX and pkY are the cursor \ncoordinates. \nNote that when we set up the context, we also set CXO_SYSTEM in lcOptions. This tells \nWintab to move the system cursor, rather than depending on our code to do it. This is the \nsimplest way to handle tablet versus mouse positioning. Clearing CXO_SYSTEM allows you \nto track mouse and tablet positions separately. This can get tricky and is potentially \nconfusing to the user. If you are rendering your own pointer, hiding the system cursor is a \ngood solution that still leverages Wintab to manage the pointer location. \n",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": " \n \nBy default, the tablet will deliver position information in tablet units (in other words, at the \nresolution of the tablet). With Wintab, you can specify a different range. For example, you \ncan have it scale positions to screen units by setting lcOutExtX/Y to the screen \ndimensions. Keep in mind that the origin of the default tablet coordinate system is in the \nlower-left corner of the tablet. You can change this by supplying negative extents to the \ncontext. If you also want to translate the tablet origin—for example, to match the origin of \nyour application‘s drawing area—you can do so with lcOutOrgX/Y. \nThe pen tip and eraser post standard Windows button up/down messages. This provides \ncompatibility with non-tablet-aware applications, since pen-down is usually equivalent to \nleft-click. Our code is tablet aware, though, so it can ignore the button messages and \ninstead process the tablet‘s button and pressure values directly from the packet queue. \ncase WT_PACKET: \n    PACKET pkt; \n    WTPacket( hContext , wParam, &pkt ); \n    DWORD buttonNum     = LOWORD( pkt.pkButtons ); \n    DWORD buttonChange  = HIWORD( pkt.pkButtons ); \n    // buttonChange is one of TBN_UP, TBN_DOWN, TBN_NONE \n \nThis code listing extracts the button number and button state from the packet. This is based \non the button reporting being in relative mode, which we configured by including \nPK_BUTTONS in our PACKETDATA and setting PK_BUTTONS in our context‘s \nlcPktMode. \nBeing a Good Neighbor \nRemember that more than one application can be running that uses the tablet for input. \nWith Wintab, tablet contexts are layered so that usually only the application with the top-\nmost context receives tablet events. It‘s up to you to make sure your context‘s stacking \nposition is in sync with the application‘s window position. When your window loses focus or \ngets minimized, you should push your tablet context to the bottom of the context stack with \nWTOverlap(). \nCursor Proximity \nWe have a basic level of functionality now, but really this only gets us to the same level as a \nmouse. Let‘s start adding some more advanced tablet features to the application. \nA key part of the tablet system is the cursor. The cursor is the physical device that the user \nuses to interact with the tablet surface. There are a variety of cursor types in addition to the \npen, including the eraser (usually the opposite end of the pen), airbrush, and puck. \nThe cursor does not have to be touching the tablet surface for it to be detected. Most \ntablets sense the cursor from about half a centimeter above the actual surface. Depending \non how general you want your application to be, you can request WT_CSR-CHANGE events \nwhen a cursor comes within proximity of the tablet by setting CXO_CSRMESSAGES in \nlcOptions. If you‘re not interested in the details of the cursor being used with the tablet, \nyou can just do everything with WT_PROXIMITY, which you‘ll get whenever any cursor \nenters or leaves the range of the tablet. WT_CSRCHANGE is an extra message that you get \nonly when the cursor enters proximity, which is the ideal time to update your cursor \ncapabilities and options. \ncase WT_PROXIMITY: \n",
      "content_length": 3187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": " \n \n    bIsInProximity = LOWORD(lParam); \n    break; \n \ncase WT_CSRCHANGE: \n    WTPacket( hContext, wParam, &pkt ); \n    // use pkt.pkCursor with WTInfo() to get properties of the \n    // active cursor... \n    break; \n \n \nAdditional Input Axes \nPressure is a common and very useful pen feature. This data is relatively straightforward to \naccess. When querying the device capabilities, you can get the parameters for the ―normal \npressure axis,‖ which tells you how many levels of pressure the device reports. This can \nthen be used to scale the pressure values you get from pkt.pkNormalPressure when \nyou have PK_NORMAL_PRESSURE set in your PACKET struct. Users can customize \nthresholds and response curves for pressure sensitivity at the system level, but as long as \nyou access the pressure data this way, this will all be transparent to your application. \nIf you‘re looking at the pressure information in the Wintab spec, you‘ll also see there is a \ntangent pressure property. This is pressure parallel to the tablet surface, which can come \nfrom special cursors like the airbrush, which has a finger wheel that produces tangent \npressure values. \nAnother useful feature is tilt sensing. This allows you to detect the angle that the stylus is \nbeing held at relative to the tablet surface. Wintab exposes this as pkt.pkOrientation, \nwhich represents the orientation as an azimuth and altitude angle (see Figure 4.12.4). Like \nthe pressure values, the orientation values should be scaled based on the device \ncapabilities. We can convert the orientation angles into a unit vector with a little \ntrigonometry: \nFigure 4.12.4. Azimuth and altitude representation of orientation. \n \n \nfloat azimuthRads = 2.0f * PI * pkt.pkOrientation.orAzimuth \n                        * azimuthScale; \nfloat altitudeRads = PI * pkt.pkOrientation.orAltitude \n                        * altitudeScale; \nfloat lengthXY = cos( altitudeRads ); \nfloat tiltX = sin( azimuthRads ) * lengthXY; \nfloat tiltY = cos( azimuthRads ) * lengthXY; \n",
      "content_length": 2017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": " \n \nfloat tiltZ = sin( altitudeRads ); \n \n \nButtons \nBesides the tip, most pens have buttons on the barrel as well. The states of these buttons \nare given to your application through pkt.pkButtons, just like the tip. Some tablets \nhave additional buttons on the tablet itself. These are not usually part of the main Wintab \nspecification, so they must be accessed through vendor-specific extensions. Vendors such \nas Wacom supply documentation and examples of how to access these extended features. \n \nConclusion \nWe‘ve looked at a variety of topics and techniques from the human-computer interaction \nresearch community and learned how to use the Wintab API to interact with tablets. The \nexample code on the CD-ROM can serve as a starting point for creating your own tablet-\nenabled software. The papers listed in the ―References‖ section (and the many papers they \nreference) provide much more detail than could be included here about the design and \nimplementation of the various pen-based solutions we touched on. They are an invaluable \nresource when you decide to tackle one of these projects yourself. \nWith the recent popularity of touch-based devices, touch- and pen-based computing has \nbecome an active and exciting area of research. Hopefully, this gem will inspire you to add \nrich and powerful tablet functionality to your next game tool project. \n \nReferences \n[Appert09] Appert, C., and S. Zhai. ―Using Strokes as Command Shortcuts: Cognitive \nBenefits and Toolkit Support.‖ Proceedings of the 27th International Conference on Human \nFactors in Computing Systems (2009): 2289–2298. \n[Chen88] Chen, M., S. Mountford, and A. Sellen. ―A Study in Interactive 3-D Rotation Using \n2-D Control Devices.‖ Proceedings of the 15th Annual Conference on Computer Graphics and \nInteractive Techniques (1988): 121–129. \n[Evans81] Evans, K. B., P. P. Tanner, and M. Wein. ―Tablet-Based Valuators That Provide \nOne, Two, or Three Degrees of Freedom.‖ Proceedings of the 8th Annual Conference on \nComputer Graphics and Interactive Techniques (1981): 91–97. \n[Hancock04] Hancock, M., and K. S. Booth. ―Improving Menu Placement Strategies for Pen \nInput.‖ Proceedings of Graphics Interface (2004): 221–230. \n[Kimura93] Kimura, T. D., W. Citrin, D. Halbert, C. Hewitt, N. Meyrowitz, and B. \nShneiderman. ―Potentials and Limitations of Pen-Based Computers.‖ Proceedings of the \n1993 ACM Conference on Computer Science (1993): 536–539. \n[Kurtenbach93] Kurtenbach, G., A. Sellen, and W. Buxton. ―An Empirical Evaluation of \nSome Articulatory and Cognitive Aspects of Marking Menus.‖ Journal of Human Computer \nInteraction 8 (1993): 1–23. \n",
      "content_length": 2633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": " \n \n[MacKenzie91] MacKenzie, S., A. Sellen, and W. Buxton. ―A Comparison of Input Devices in \nElemental Pointing and Dragging Tasks.‖ Proceedings of the CHI ‘91 Conference on Human \nFactors in Computing Systems (1991): 161–166. \n[Moyle02] Moyle, M., and A. Cockburn. ―Analysing Mouse and Pen Flick Gestures.‖ CHI‘02. \n(2002): 19–24. \n[Ramos07] Ramos, G., A. Cockburn, R. Balakrishnan, and M. Beaudouin-Lafon. ―Pointing \nLenses: Facilitating Stylus Input through Visual- and Motor-Space Magnification.‖ \nProceedings of the SIGCHI Conference on Human Factors in Computing Systems (2007): \n757–766. \n[Wacom09] ―Wacom Software Developer Support.‖ n.d. Wacom Technology. 1 Sept. 2009. \n<http://www.wacomeng.com/devsupport/index.html>. \n \n4.13. Creating a Multi-Threaded Actor-Based Architecture Using \nIntel® Threading Building Blocks \nRobert Jay Gould, Square-Enix \nrobert.jay.gould@gmail.com \nWith the next generation of consoles and personal computers having dozens of smaller \nprocessing cores, developers will have to redesign the architecture of their game engines to \ntake advantage of this processing power. In some areas, such as graphics and physics, \nvarious methods to achieve higher performance already exist. In general, there has been \nless success in adapting higher-level AI- and gameplay-related systems to massively multi-\ncored environments. \nThis gem presents one architecture and implementation that can be used for AI and \ngameplay systems capable of scaling through high concurrency. The proposed architecture \nis a multi-threaded message passing actor architecture, which uses engineering tradeoffs \nsimilar to those of the Erlang language in order to achieve its concurrency and scalability. \nThe implementation is in C++, using Intel‘s Threading Building Blocks (TBB) for high-\nperformance concurrency and Lua to create a friendly programming interface that hides the \ntypical complexity associated with message-passing systems so that designers and \ngameplay programmers can get their work done productively. \nIntroduction \nWhen striving for performance in multi-cored environments, the only solution is to increase \nthe concurrency (amount of work that can be done in parallel) to leverage the system‘s \nprocessing power. Thus, finding concurrency patterns in gameplay systems should be the \nfirst goal when designing scalable architectures. \nFinding Concurrency in Games \nGames are composed of several systems, and although each system might have its own \npeculiarities, they can be broadly grouped together by considering what kind of \nconcurrency, or parallel, programming patterns [Mattson04] best suit them. For example, \nsystems like graphics and physics that are almost entirely data-driven can be easily \nparallelized by applying data decomposition patterns and using implicit parallelization and \nreferential transparency techniques. On the other hand, gameplay systems are primarily \nevent-driven. If one looks at event handling as creating tasks to process events, task \ndecomposition patterns can be applied to gameplay systems to increase their concurrency. \n",
      "content_length": 3096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": " \n \nTask Decomposition \nIn simple terms, task decomposition is about breaking down the useful work of a system \ninto atomic tasks that can be processed concurrently, without compromising the state of the \nsystem. In games, we usually have no problem in dividing work into tasks, as this is \nsomething that is already part of most game architectures today. Instead, fulfilling the \n―without compromising the state‖ part is the tricky part in game development. So finding \nways in which state can be effectively divided and can be protected is the key to achieving \ntask decomposition in the event-driven parts of our game engines. \nState Protection Strategies \nPerhaps the most familiar way to protect the state of a system, for game programmers, is \nthrough lock-based synchronization. As most programmers also know, lock-based systems \ncan grow in complexity quite easily, and avoiding dead locks, live locks, data races, and \nconvoying can consume a large chunk of precious development time. Besides the \ncomplexity and instruction overhead, lock-based synchronization also forces a non-\nnegligible degree of serialization on processing, meaning that as the number of processing \ncores increases, scalability levels out, and it provides decreasing returns. Of course, the \nideal alternative to lock-based synchronization is to use lock-free algorithms, but besides \nimplementations being devilishly difficult even for algorithm experts, in many cases there \nare simply no known lock-free algorithms for some problems. \nOther common techniques involve attacking the problem by using memory policies as \nopposed to processing policies, as the previous two alternatives do. One of these \ntechniques, generally applicable to any sort of system, is to use immutable data like pure \nfunctional languages have decided to do. The benefit of immutable data is that state cannot \nbe compromised; instead, state changes are implemented through data copying and \ngarbage collection. Unfortunately, in systems that require lots of data and have rather tight \nmemory constraints, such as games, all this redundancy can become an issue in itself. One \nsolution to reduce this redundancy is called transactional memory. It consists of maintaining \nmetadata of the protected state, so it is possible to determine when conflicts arise and roll \nback and retry conflicting operations. \nThe strength of transactional memory is that unlike locks, it provides increasing returns as \nconcurrency increases, so one day transactional memory may be the easiest solution for \nhigh throughput systems, such as games [Sweeney08]. However, we are limited to software \ntransactional memory (STM), as there is no hardware support for these features. STM‘s \ncurrent downsides are mostly due to its experimental nature, which requires experimental \ncompiler extensions and proprietary run-time libraries. It also has a large memory and \ninstruction overhead due to the current bookkeeping algorithms, meaning that its \nperformance penalties typically outweigh its benefits when shared state contention levels \nare low [Cascaval08], as they typically are in games. \nFinally, one more solution—and the one used by the architecture presented in this gem—is \nthe shared nothing [Stonebraker86] approach, where state cannot be compromised because \nit is not shared. This is typically achieved by having each concurrent process be completely \nresponsible for modifying its own data through strong encapsulation. For this approach to \nscale on multi-cored environments, implementing lightweight processing units that can work \nconcurrently is the way to go, and these techniques can be found in actor-based \nprogramming. \n \nActor-Based Programming \nActor-based programming is a programming paradigm in which the basic programming and \nprocessing unit is the actor. Actors are similar to objects in that they encapsulate state, but \n",
      "content_length": 3893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": " \n \nunlike typical objects they are also capable of processing information on their own, sort of \nlike a cross between an object and a thread. Another difference between C++ style objects \nand actors is that instead of relying on method invocation for interaction, actors use \nmessage passing for communication. \nUnfortunately, many game programmers relate message passing to the typical centralized \nmessage-pump design commonly used in games (as if the entire game were a single actor). \nMessage passing is also associated with lots of the boilerplate code necessary to define and \nhandle all these messages, but these are just the artifacts of a poor interface design and \nC++‘s syntax. In fact, message-passing interfaces can be quite elegant without any tedious \nboilerplate code, with Smalltalk and Objective-C being good examples of elegant message-\npassing interfaces. \nThe actor-based architecture in this gem is in great part inspired by the Erlang language, \nwhich is the most successful actor-oriented language in use and also one of the most \nscalable languages in general. Erlang is a mature language that was designed by Joe \nArmstrong while working at Ericsson in 1986 to develop robust and scalable applications to \nhandle telecommunications systems [Armstrong07]. Nowadays, Erlang is used by many \ntelecoms around the world to power their telephony systems, and recently Erlang has been \ngaining ground in scalable commercial web applications, such as Amazon‘s EC2 and \nSimpleDB platforms and Facebook‘s chat system. Erlang‘s younger sibling and close \ncompetitor, Scala, which runs on the Java Virtual Machine, is used to provide the scalability \nbehind services such as Twitter, demonstrating the power of actor-based designs. \nOne critical diverging engineering point between Erlang and the architecture in this gem is \nthat Erlang is optimized for scaling in highly distributed environments, meaning that it has \nefficient IO handling as one of its central pillars. The actor-based architecture in this gem is \noptimized for running in highly multi-cored environments, focusing on efficient use of a \nsingle machine‘s multiple cores, foregoing IO-based considerations almost entirely. \n \nImplementing an Actor Engine \nThe first concern of implementing an actor engine is to fulfill the need to support the ability \nof each actor to process its own work individually and concurrently. The simplest, yet least \nscalable solution to this is to provide an individual physical thread (or even OS-level \nprocess) for each actor. Unfortunately, that means that as the number of actors increases, \nthe memory and processing overhead due to context switches increases, and scalability will \nlevel off and even decrease. The better solution is to use something lighter than threads, as \nErlang does, so we can support thousands of concurrent actors with near linear scalability. \nIn Erlang, this primitive processing unit is simply called a process, which is a lightweight \ncontext similar in nature to a green thread or a coroutine in implementation. The actor \nengine herein utilizes Intel‘s TBB library to provide the parallel processing needs of its \nactors, based on task-processing algorithms. \nIntel‘s TBB is an open-source threading library that provides a platform- and compiler-\nindependent production-ready foundation for building multi-threaded applications that only \nrequire standard C++ support from the compiler. It includes several high-level tools such as \nconcurrent algorithms like tbb::parallel_for and several thread-safe containers like \ntbb::concurrent_hash and tbb::concurrent_queue. There are also lower-level \ntools, such as scalable memory allocators, and a scalable task scheduler, \ntbb::task_scheduler, that is used to power all of TBB‘s higher-level concurrent \nalgorithms. It is these lower-level task scheduler components that are used to construct the \nprocessing workhorse for the actor engine of this gem. \nThe Task Scheduler \n",
      "content_length": 3975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": " \n \nThe task scheduler‘s main job is to effectively map its task-based workload onto the \nresources available to it in a way that avoids unnecessary context switching, thus providing \na scalable solution for task-based concurrency. Yet its performance benefits don‘t stop \nthere. Unlike most simple schedulers, the tbb::task_scheduler it isn‘t a fair scheduler \nthat uses a round-robin-like algorithm. Instead, it is implemented as a finely tuned work-\nstealing scheduler [Intel09] that schedules tasks in a fashion that decreases cache misses \nand memory thrashing. It does this by first working on tasks that are hot in the cache and \nthen working its way onto colder tasks as necessary. This cache-friendly scheduling also \nmeans that the task scheduler actually resembles a LIFO-like processor more than a typical \nFIFO-like processor. \nWhat this means to our architecture, besides high performance processing, is that it doesn‘t \nimpose any ordering restrictions on message processing. Although this might sound strange \nat first, allowing messages to be handled out of order is just what Erlang‘s message passing \nalgorithm does, because this is an important area that is best left open for scalability \noptimizations. \nThe Message Class \nWith the processing implementation decided, we‘ll quickly look at the messages that will be \nused to allow actors to interact. This, however, is actually just a simple implementation \ndetail, and the precise type of a message is probably best left for each system to define \nbased on the system‘s other requirements. For this reason, the message type that actors \nhandle is simply defined as a template argument in our implementation. \nThe Actor Class \nIn implementing the actor class, we can divide its functionality into four fundamental \nparts—its internal state, a message queue, a message processor, and the message \nhandling. The message queue and message processing are the critical core features that are \nin the domain of the system programmer to implement efficiently to provide good scalability \nwith little overhead, and as such, they are implemented in the base actor class. The internal \nstate and the message handling are the extensible features of the actor, so they are derived \nby subclasses of the base actor class. \nThe Message Queue \nAs one of the core features of the actor class, the message queue is important because it is \nthe only contention point in the actor‘s architecture where several threads may interact on \nthe same data. This implies that its performance directly affects the overall performance of \nthe actor and the scalability of system at large. The obvious candidate as a container for our \nmessage queue is tbb::concurrent_queue. It provides a thread-safe queue that \nallows several actors to add messages to it concurrently, with a relatively low overhead \nwhen compared to a std::queue with a big lock around it. \nOn this subject, an optimization not present in the sample code but likely of interest to \nsystem programmers is that the tbb::concurrent_queue is what is called a thread-\nsafe multi-producer multi-consumer (MPMC) queue, which is safe when several producers \nand consumers push and pop on the queue concurrently. However, our actors only need a \nmulti-producer single-consumer (MPSC) queue, because the only consumer of the queue is \nthe actor itself. So we are paying for more than we need by using \ntbb::concurrent_queue. While doing tests and benchmarks using an excellent lock-\nfree MPSC queue algorithm [V‘jukov08], the system‘s performance increased as much as 10 \npercent under a heavy load with high contention rates, showing just how important the \nperformance of the message queue is to the actor engine. \nActor Hierarchies \n",
      "content_length": 3738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": " \n \nIn this architecture, any number of root actors can be spawned, and all actors are allowed \nto spawn children, which become their responsibility. This means there can typically be \nmany trees of actors (a forest) running concurrently. This is useful because it allows any \nparticular actor hierarchies to be spawned or shut down independently, allowing particular \nsubsystems to be restarted without affecting others. As long as two actors can understand \neach other‘s messages, they can communicate, even if they belong to different hierarchies; \nhowever, actors within a single hierarchy will likely be processed in closer temporal \nproximity. \nListing 4.13.1. Actor construction \nActor::Actor( Actor* parent) :pParent(parent) \n{ \n    if (pParent) \n    { \n        //Get root processing unit/task \n        pRootTask = pParent->pRootTask; \n    } \n    else \n    { \n        //Become a new root   processing   unit/task \n        pRootTask = new   ( \n            tbb::task::allocate_root())tbb::empty_task(); \n        pRootTask->set_ref_count(++childrenCounter); \n    } \n} \n \nAs seen in Listing 4.13.1, root actors keep circular references to their own TBB processing \nroot task. This prevents the scheduler from cleaning up the task associated with each \nprocessing hierarchy when no work is left, allowing us to reuse the same root task for each \nactor tree indefinitely. Also of note is TBB‘s use of in-place object allocation using \nnew(tbb::task::allocate_root()). This idiom has the purpose of avoiding the \noverhead of creating tasks by recycling tasks from object pools behind the scene. Both \nfeatures serve to avoid memory management bottlenecks due to the large number of tasks \nthat will be spawned during the engine‘s lifetime. \nMessage Processing \nThe message processing cycle is the heart of the actor engine, and most of the important \nimplementation details are located in this section. Things will get a bit grittier from here on. \nAs in Erlang, message passing between actors in our design is totally asynchronous. The \ninteractions among actors are entirely one-way affairs, not requiring any sort of handshake \nbetween actors. This allows the sender to continue on with its own processing without \nhaving to wait on the recipient to handle the message sent. If we forced interactions to be \nsynchronous by requiring handshakes, message passing would not be scalable as Erlang‘s \nmessage passing is, and instead it would be similar to the message passing found in \nObjective-C or Smalltalk. \nBy decoupling actor interactions, the message handling can be decomposed into a great \nnumber of very finely grained discrete tasks. Theoretically, this allows the system to scale \nlinearly as long as actors outnumber the number of processing cores. However, in reality, \nthe creation and processing of tasks by the task scheduler has an overhead in itself. \n",
      "content_length": 2871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": " \n \nBecause of this, TBB‘s documentation recommends that the grain size of a task be between \n100 and 100,000 instructions for the best performance [Intel09]. So unless most message \nhandling is quite heavy, assigning one task per message can become quite wasteful. This \nissue requires optimizations to increase the total throughput of the system. \nWhen an actor places a message into another actor‘s inbox, the processing thread kick-\nstarts the recipient actor, as shown in Listing 4.13.2. In situations where the sender will be \nwaiting for a reply from the recipient, this optimization allows for reduced latency as the \nrecipient‘s task will be hot in the cache and favored by the task scheduler for being \nprocessed next. More importantly, this design also removes the need for having actors \nwastefully poll for work, making processing entirely event-driven. \nListing 4.13.2. Message passing \nvoid Actor::inbox(Actor*   sender,   const   MESSAGE_TYPE&  msg) \n{ \n     this->messageQueue..push(msg); \n     this->tryProcessing(sender->GetProcessingTask()); \n} \nvoid Actor::tryProcessing(tbb::task* processing_unit) \n{ \n     if( !messageQueue.empty() && \n         isProcessingNow.compare_and_swap(true,false)) \n     { \n         //use a continuation task \n         tbb::empty_task* continuation = new( \n             root->allocate_continuation())tbb::empty task; \n         this->pMessageProcessorTask = new( \n             continuation.allocate_child())MsgProcessor(this); \n         continuation.set_ref_count(1); \n         this->root->spawn( this->pMessageProcessorTask); \n     } \n} \n \nAlso in Listing 4.13.2, you can see that TBB continuations are utilized. This allows more \nfreedom to the scheduler for optimizing its own workflow by decoupling execution order. A \ndetailed explanation of continuation-style programming can be found in [Dybvig03]. \nAs mentioned earlier, to increase the workload of a single task over 100 instructions, an \nactor will consume the entirety of its message queue, as well as any messages that arrive \nwhile it is processing within the execution of a single task, as seen in Listing 4.13.3. This \ndesign dynamically eases the task-processing overhead as work increases. \nListing 4.13.3. Message consumption loop \nvoid Actor::processAllMessages() \n{ \n     isProcessingNow = true; \n     Msg_t msg; \n     while(messageQueue.trypop(msg)) \n     { \n",
      "content_length": 2378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": " \n \n         try \n         { \n              if(!this->receive(msg)) \n              { \n                  throw(messageHandlingError); \n               } \n         } \n         catch(tbb::exception e) \n         { \n             sendException(pParent, e, msg); \n         } \n \n     } \n \n     isProcessingNow = false; \n} \n \nAlso of note is the error handling logic employed for message processing. In an actor-based \narchitecture, when an actor encounters an exception or error it cannot handle, it doesn‘t \nmake sense for the error to bubble up through the call stack. Instead, it needs to bubble up \nthrough its actor hierarchy, forwarding the exception to its parent. Generally, when a parent \nreceives an error from a child, it has few options because it can‘t query or fix the child‘s \nstate directly. Typical error handling includes ignoring the message altogether, reporting the \nerror to another actor, or as is common in Erlang, creating a fresh new child actor to replace \nthe actor having problems. The functionality of the failing actor is thereby automatically \nreset to a clean state. \nMessage Handling \nLike actors in Erlang, our actors have a receive method, which can be seen in Listing \n4.13.3. The receive method is called whenever a message needs handling; the actual \nimplementation of this method is the responsibility of the derived actor classes. Typical \nimplementations of receive consist of matching message signatures to specific message \nhandling routines. This can be done in many ways, from something like a switch/case \nconstruct or a series of if/else statements to something more complex, such as \nrecognizing message signatures, regular expression matching, hierarchical state-machines, \nor even neural networks. A couple of implementation prototypes that might serve as \ninspiration can be found on the CD, but the one design that will be the focus for the rest of \nthis gem is that of a scripted actor, which matches message signatures directly to Lua \nfunctions registered to the actor. This straightforward design provides a simple and generic, \nboilerplate-free solution that can be easily extended by gameplay programmers and \ndesigners using Lua alone to build whatever class hierarchies or aggregations they need to \nget their work done, without having to touch any of the library code directly. \nAs promised earlier in this gem, message-passing APIs can be friendly and don‘t have to \ninvolve lots of boilerplate, as the Lua code in Listing 4.13.4 demonstrates. In fact, it looks \npretty much like any ordinary Lua API. \nListing 4.13.4. Lua-based actor API \nclass ―Player‖:inherit ―Character‖ \n{ \n",
      "content_length": 2633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": " \n \n    attack=function(target) \n        target:mod_hp(math.random(5,10)) \n    end, \n} \n–Example use case \nlocal player = getActor(―player1‖,‖Player‖) \nlocal enemy = getActor(―goblin01‖) \nplayer:attack(enemy) \n \nTo provide a clean, scriptable API, getActor takes an actor name and returns an actor \nproxy, not an actual actor reference. Using the proxy‘s metatable‘s index logic, it is capable \nof responding to any sort of function call. The proxy then converts these object-oriented \nfunction calls and its arguments into a message that gets sent to the C++ actor. Of course, \nsimply allowing game code to call any method on the proxy with no error checking could \nmake debugging quite complicated, but typically an unhandled message log combined with \nsome class-based checking makes most debugging reasonable. To add the class-based \nmessage checking feature to a proxy, an optional second argument is passed in—the class \nname of the class that should be used for verifying message calls, seen in Listing 4.13.5. \nListing 4.13.5. Lua actor proxies \nfunction getActor(address,classname) \n    local class = findClass(classname) \n    return    \nsetmetatable({__address=address,__class=class},mt_actor) \nend \n \nmt_actor.__index = function(self,message,...) \n    if rawget(self,―class‖) and not self.class[message] then \n        error((―class [%s] can’t handle message [%s]‖):format( \n               self._class.name,message)) \n    else \n        self._message = message \n        return send(self) → a C-function connected to the actor \nengine \n    end \nend \n \n \nMessage-Passing Patterns \nWhen working with asynchronous actors and message passing, some issues may appear. As \nwith other styles of programming, there are already several useful and tested programming \npatterns that can be applied to resolve most of these problems. \nQuerying for State and Promises \nAt times an actor may need to query the state of another actor, not just send it a message. \nBecause actor message passing is asynchronous, this can be an issue. One solution is to \n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": " \n \nadd a way to force a synchronous message passing when required, but that adds coupling \nto the system that could compromise its concurrency. Another solution, and the one \ncommonly used by actor-based architectures, is to handle this scenario using a future or \npromise. Promises are implemented by sending along with the message the address of the \ncalling actor and a continuation context, to which the recipient will respond back to, \nresuming the context of the sender, or in this implementation by use of TBB continuations \n[Werth09]. In the proposed API, this is handled as seen in Listing 4.13.6, by calling the \npromise to block until the value is returned. \nListing 4.13.6. Using promises to query an actor for state \npromiseHp = enemy:get_hp() \nif promiseHp() > 50 then –-invoking the promise returns its \nvalue \n    player:run() \nend \n \nAnother type of promise is to obtain an actor reference from another actor, as in Listing \n4.13.7. This style of promise is accomplished by chaining proxy promises. \nListing 4.13.7. Using promises to obtain actors in second degree \nplayer.healClosest = function(self) \n    local map = getActor(―map‖) \n    local closestActor = \n        map:findClosestActor(self:get_position()) \n    closestActor:heal(100) \nend \n \nSequential Message Processing \nAnother special case that requires consideration because the actor-based system is \nasynchronous is how to handle messages when they only make sense in a certain order. For \nexample, when opening a door, it is required to first insert a key and then push it open. \nHowever, there is no assurance that the ―insert key‖ message will actually arrive before the \n―push door‖ message, even if they were sent in the correct order. The typical actor-based \nsolution is to use a sequencer actor that works in conjunction with the door. The job of the \nsequencer is to queue and sort messages according to some internal logic and then forward \nthem in an appropriate order as they become available. In our example, the sequencer \nwould not send the ―push door‖ message to the door before it has received the ―insert key‖ \nmessage. Although sequencers tend to be more effective than direct coupling, they do \nintroduce a degree of serialization to the code, so they should be used only where truly \nnecessary. \nMessage Epochs \nOne more common scenario is that it is possible for an actor to receive a message that, say, \nlowers its HP below 0, although a heal message had actually been sent before the damage \nmessage was issued. In most cases within a single atomic gameplay frame, the actual \nproduction order of messages should not be important, making this sort of issue actually \nmore like a quantum physics conundrum than a real gameplay issue. This means that \n",
      "content_length": 2755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": " \n \ngenerally, this sort of event can be ignored with no detriment to the game. Nevertheless, \nwhen disambiguation is required, an easy solution is to use epochs [Solworth92] or \ntimestamps to determine which message was sent first. \n \nConclusion \nThis gem reviewed the requirements and alternatives to build a highly scalable architecture \nand went into the details of implementing one viable alternative, based on the actor model \nand a shared nothing policy using Intel‘s Threading Building Blocks. On the CD there is a \nreference implementation in the form of an actor-based Lua console, along with sample \nscripts for experimentation with the concepts presented in this gem. \n \nReferences \n[Armstrong07] Armstrong, Joe. Programming Erlang: Software for a Concurrent World. \nPragmatic Bookshelf, 2007. \n[Cascaval08] Cascaval, Calin. ―Software Transactional Memory: Why Is It Only a Research \nToy?‖ ACM QUEUE (October 2008): n.p. \n[Dybvig03] Dybvig, Kent R. ―The Scheme Programming Language.‖ Sept. 2009. Cadence \nResearch Systems. n.d. <http://scheme.com/tspl3/further.html#./further:h4>. \n[Intel09] Intel. ―Intel(R) Threading Building Blocks, Reference Manual.‖ Sept. 2009. Intel. \nn.d. <http://www.threadingbuildingblocks.org/documentation.php>. \n[Mattson04] Mattson, Timothy G. Patterns for Parallel Programming. Addison-Wesley \nProfessional, 2004. \n[Solworth92] Solworth , Jon. ACM Transactions on Programming Languages and Systems \n14.1 (Jan. 1992): n.p. \n[Stonebraker86] Stonebraker, Michael. ―The Case for Shared Nothing Architecture.‖ \nDatabase Engineering 9.1 (1986): n.p. \n[Sweeney08] Sweeney, Tim. ―The End of the GPU Roadmap.‖ Sept. 2009. Williams College. \nn.d. <http://graphics.cs.williams.edu/archive/SweeneyHPG2009/TimHPG2009.pdf>. \n[V‘jukov08] V‘jukov, Dmitriy. ―Scalable Synchronization Algorithms, low-overhead mpsc \nqueue.‖ 13 May 2008. Google. n.d. <http://groups.google.com/group/lock-\nfree/browse_thread/thread/55df71b87acb8201>. \n[Werth09] Werth, Bradley. ―Sponsored Feature: Optimizing Game Architectures with Intel \nThreading Building Blocks.‖ 30 March 2009. Gamasutra. n.d. \n<http://www.gamasutra.com/view/feature/3970/sponsored_feature_optimizing_game_.ph\n>. \n \nSection 5: Networking and Multiplayer \n",
      "content_length": 2233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": " \n \nIntroduction \nSecure Channel Communication \nSocial Networks in Games: Playing with Your Facebook Friends \nAsynchronous I/O for Scalable Game Servers \nIntroduction to 3D Streaming Technology in Massively Multiplayer Online Games \nIntroduction \nCraig Tiller and Adam Lake \nThe current generation of consoles all possess the capability to create a networked, \nmultiplayer experience. In effect, multiplayer networked gameplay has gone mainstream. \nOn the Xbox 360, there are 771,476 people playing Call of Duty: Modern Warfare 2 online \nthis very minute (4:37 p.m. on 11/28/2009). Several portable devices, such as the \nNintendo DS and Apple‘s iPhone, enable multiplayer networked experiences. Significant \nnumbers of men, women, and children now spend their entertainment dollars and time \nsocializing through online games. \nEach of the authors in this section addresses critical components in the networked \nmultiplayer architecture: security, scalability, social network harvesting, and streaming. \nFirst, in the gem ―Secure Channel Communication,‖ we discuss the issues related to creating \nand maintaining secure communication and the various attacks and responses posed in a \nnetworked gaming environment. Next, leveraging social network APIs to obtain player data \nis discussed in the gem ―Social Networks in Games: Playing with Your Facebook Friends.‖ \nThis allows a game developer, with the user‘s permission, to gain access to a player‘s \nFacebook friends list to be leveraged to create a multiplayer experience. The third gem, \n―Asynchronous I/O for Scalable Game Servers,‖ deals with the issues of scaling the I/O \nsystem architecture to handle the large number of requests generated in networked \nmultiplayer scenarios. Finally, the gem ―Introduction to 3D Streaming Technology in \nMassively Multiplayer Online Games,‖ was written by Kevin He at Blizzard and includes \nsource code to a terrain streaming application. This article is longer than a typical gem but \ncontains many details useful for those creating such a large system. \nIt is our hope that you will find these gems useful in your own applications and that you will \ncontribute your own innovations to this exciting and important area of game development. \n \n5.1. Secure Channel Communication \nChris Lomont \nchris@lomont.org \nThis gem is an overview of creating secure networking protocols. There is not enough space \nto detail all pieces, so instead a checklist of items is presented that covers necessary points. \nOnline games must prevent cheaters from using tools and hacks to their advantage, often \nto the detriment of other players‘ enjoyment. Cheating can be done through software add-\nons or changes, often making the cheater too powerful for other players or performing \ndenial-of-service attacks, making the game unresponsive for others‘ requests, such as gold, \nitems, services, and accounts. Since any code running on the client can be disassembled, \n",
      "content_length": 2941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": " \n \nstudied, and modified, security decisions must be made assuming the cheater has access to \ngame source code. Any system should be designed with the twin goals of making it difficult \nto cheat and making it easy to detect cheaters. \nThe main reason game and network security is often broken is that security designers must \nprotect against every possible avenue of attack, while a cheater only needs one hole for an \nexploit. This asymmetric warfare makes doing it right very hard and a continual arms race. \nThe goal of this gem is to supply a checklist of items to consider when designing and \nimplementing secure networking protocols. The ordering of topics is designed to be top \ndown, which is a good way to think through security design. Many related security features \nare mentioned, such as copy protection and code obfuscation, which, although not exactly \nnetworking, do play a role in an overall security of game networking by making it harder to \nchange assets. \nArchitecture \nThe most important decision when designing a gaming networking protocol is to decide how \nthe architecture is going to work before any programming is started. This architecture \nchoice has profound effects on later choices; making a significant change to the networking \ncomponent will have costly ripple effects throughout the rest of your game components. \nThree aspects of game engine design need up-front thought: multi-threading design, \nnetworking architecture, and code security. None of these can be bolted onto an existing \nengine without severe problems and bugs, resulting in poor quality for all components. So \nfix these three design decisions up front, document them as gospel, and build the rest of \nthe game around these choices. \nCode Security \nCode security is based on using secure coding practices. Without having this lowest layer \ndone well, it is impossible to get networking secure. Many games are written in C/C++. \nThree good references are [Seacord05, Howard03, and Graff03]. \nPeer to Peer \nThe main choice in architecture is whether to be peer to peer or client/server. In a peer-to-\npeer architecture, it is up to peers to detect cheaters, and of course such mechanisms can \nbe subverted by a cheating client. For example, if there is a ―feature‖ that allows a client to \nkick a suspected cheater off a network, then cheaters will subvert this to kick off legitimate \nplayers. In this case, a secure and anonymous voting protocol should be used, so numerous \nclients need to agree on a cheater before a ban occurs. \nClient/Server \nMost online games are a client/server architecture, where a central server is the authority \non game state, and clients send player input to and receive game state back from the \nserver. The main benefit of this architecture from a security point of view is the server can \nbe assumed to be a trusted, central authority on the game state. Unless your server is \ncompromised or there are fake servers for players to log onto, the server can be trusted to \ndetect cheaters and ban them and their accounts. \nThe Unreal Engine [Sweeny99] uses what Tim Sweeny calls generalized client-server, where \nthe server contains the definitive game state, and clients work on approximate and limited \nknowledge of the world. This information is synced at appropriate intervals. The limited \ngame state supports the security principle of least privilege, covered later. \nProtocol \n",
      "content_length": 3418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": " \n \nThe next big networking decision is selecting protocols to use and how to use them. A \ncommon tradeoff is between using slower TCP/IP for guaranteed delivery or faster UDP for \nspeed. Ensure selected security methods work with the protocol chosen. For example, if \nyour packets are encrypted in a manner requiring that all packets get delivered, then UDP \nwill cause you headaches. Good encryption needs an appropriate block-chaining method, as \ncovered later, but it will cause problems if some packets in a chain are not delivered. \nA recommendation is to use TCP/IP for login and authentication to guarantee \ncommunication, and then use UDP if needed for speed or bandwidth with symmetric key \nencryption during gameplay. \n \nAttacks \nThe level of attack sophistication against your game is directly proportional to popularity \nand longevity. Hence, more security is needed for a triple-A title than for a casual game. For \nexample, World of Warcraft (WoW) uses the Warden, a sophisticated kernel mode anti-\ncheat tool described in [Hoglund07, Messner09, and WikiWarden09]. \nReverse Engineering \nUsing tools such as IDA Pro and OllyDbg and a little skill, one can disassemble game \nexecutables into assembly code for any platform, and there are plug-ins that reverse the \ncode into C/C++ with good results. It only takes one cracker skilled in reverse engineering \nto remove or break weak security measures, and he then distributes the tool/crack to \neveryone. Assume crackers have access to your code and algorithms. \nKernel Mode \nKernel mode is the security layer that operating system code runs in (for the Windows PC \nand many other protected OSes), and most games run in user mode. WoW‘s Warden and \ncopy protection schemes such as SecuROM run as kernel mode processes, giving them \naccess to all processes and memory. However, even kernel mode software can be subverted \nby other kernel mode software. Using kernel mode tricks like those used in rootkits and \nother malware, a sophisticated cracker can run tools that hide under any snooping you \nmight do and can watch/record/modify run-time structures. This is done to circumvent \nWarden and many of the CD-ROM and DVD protection schemes. To detect and prevent \nkernel mode attacks on your code, you need kernel mode services, likely your own driver or \na commercial product, to do the work for you. \nLagging \nAlso known as tapping, this is when a player attaches a physical device called a lag switch \nto an Ethernet cable, slowing down communication to the server and slowing down the \ngame for all involved. However, the player with the lag switch can still run around and act, \nsending updates to the server. From the opponent‘s view, the player with the lag switch \nmay jump around, teleport, have super speed, and generally be able to kill opponents with \nease. \nIn peer-to-peer network architecture, this can also be achieved with packet flooding the \nopponents since each client sees other IP addresses. \nOther Attacks \n \nInternal misuse. A game must protect against employee cheating. For example, an \nonline poker site was implicated in using inside information for some individual to \n",
      "content_length": 3160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": " \n \nwin an online tournament using full knowledge of other player hands [Levitt07]. \nWhen these prizes reach tens or hundreds of thousands of dollars, there is a lot of \nincentive for employee cheating. \n \nClient hacking. Client hacks are changes made to a local executable, such as \nmaking wall textures transparent to see things a player should not (called \nwallhacking). \n \nPacket sniffing. To reverse network protocol, looking for weaknesses such as \nplayback attacks, DDoS, usernames/passwords, chat packet stealing. Some \nnetworks allow some users to see others‘ packets, such as colleges and others on \nthe same LAN, making packet sniffing attractive. \n \nBots. Numerous bots help by using auto aiming and auto firing and collecting goods \nsuch as gold and other items. Several years ago, this author created an (unnamed) \nword game-playing bot that was very successful, climbing to the top of the ranks \nduring the course of the experiment. \n \nAids. Aids assist a player, such as auto aiming, auto firing when aimed well, back-\ndoor communication, poker stats, poker playing games, and so on. Imagine how \neasy it is to cheat online for chess, checkers, Scrabble, and similar games where \ncomputers are better than humans. Other tools give multiple camera angles, better \ncamera angles, player highlighting, and so on. \n \nDesign flaws. Game design can be exploited. For example, on a game with scoring, \nit would be a design flaw if a player about to record a bad score can quit before the \nscore gets recorded and not be penalized. \n \nResponses \nIn response to all the attack methods, here are some methods to help defeat cheaters. \nCode Integrity \nFirst of all, write secure code. Then key code assets can be further hardened using methods \nfrom the malware community, such as code packing, encryption, and polymorphism. These \nall slow the code down but could still be used for infrequent actions, such as logging on or \nperiodic cheat detection. Further tools and methods along these lines should be researched \non the Internet, starting at sites such as www.openrce.com and www.rootkit.com. \nOne way to check code integrity is to integrate a small integrity scripting system and have \nthe server (or perhaps other clients in a peer-to-peer setting) send snippets of script code \nto execute. These snippets perform integrity checks such as hashing game assets, checking \ngame process memory for problems, and so on, returning the answer to the server for \nverification. The queries are generated randomly from a large space of possible code \nsnippets to send. To defeat this technique, a cheater has to answer each query correctly. \nThis requires keeping correct answers on hand, keeping a copy of modified game assets and \nremapping the scripting system, or something similar. Although doable, this adds another \nlevel of complexity for a cracker to work with since the script language is not built into \nexisting reversing tools. A variant of this takes it further and randomizes the language per \nrun and updates the client side as needed. \nA final method of code protection is integrated into commercial copy protection schemes, \nsuch as SecuROM, Steam, and PunkBuster. \nAn interesting attack on PunkBuster (which likely would work on other anti-cheat tools) was \nthe introduction of false positives getting players banned from games. The false positives \nwere caused by malicious users transmitting text fragments from known cheat programs \ninto popular IRC channels, and PunkBuster‘s aggressive memory scanning would see the \nfragments and [Punk08] ban players. This is likely to be fixed by the time this gem reaches \nprint. \n",
      "content_length": 3644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": " \n \nAgain, any code will eventually be reverse engineered given a determined cracker. A good \nplace to start reading is [Eliam05]. [Guilfanov09] shows some advanced code obfuscation \ntechniques from the creator of IDA Pro. \nKernel Mode Help \nAs mentioned earlier, this gives the highest level of computer control but also can crash a \ncomputer. An operating system may ban access to kernel mode code for gaming in the \nfuture, making kernel mode code a short-term solution. Kernel code mistakes often crash \nthe computer, not just the game process, so code must be extremely well tested before \nshipping. \nCheat Detection \nHaving the server detect cheaters through statistics is a powerful technique. Statistics from \nplayers should be kept and logged by username, including time online and game stats such \nas kills, deaths, scores, gold, character growth, kill rate, speed, and so on. An automated \nsystem or moderator should investigate any players with stats too many standard \ndeviations outside the norm. A rating system could be implemented behind the scenes like \nELO scores in chess, and players who suddenly show profound skills can be detected and \nwatched. \nContinued Vigilance \nEvery current solution requires vigilance from the game creators to patch games, update \ncheat lists, and evolve the game as cheats evolve. So far there is no one-shot method for \npreventing cheating in online games. However, following all the advice and reading deeper \ninto each topic will make protecting the game much easier by making cheats much harder \nto implement. Game creators should monitor common cheat sites, such as \nwww.gamexploits.com, and per-game forums looking for cheats and techniques. \nBackups/Restore \nTo prevent worst-case damage to a game environment, have a regularly scheduled backup \nin case of server hacking. \n \nDisciplinary Measures \nWhen a cheater is caught, the game has to have a well-defined punishment system. Most \ngames and anti-cheat systems currently ban accounts either temporarily, permanently, or \nwith some resolution process. \n \nExamples \nHere are two examples of current online game security features. \nWoW \nWorld of Warcraft uses a module called the Warden to ensure client integrity. From \n[Hoglund07, Messner09, and WikiWarden09], the following is found: \n \nIt checks the system once about every 15 seconds. \n",
      "content_length": 2349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": " \n \n \nIt dumps all DLLs to see what is running. \n \nIt reads the text of all Windows title bars. \n \nThe DLL names and title bars are hashed and compared to banned item hashes. \n \nIt hashes 10 to 20 bytes for each running process and compares these to known \ncheat program hashes, such as WoW Glider. \n \nIt looks for API hooks. \n \nIt looks for exploitative model edits. \n \nIt looks for known cheating drivers and rootkits. \nUnreal Tournament \nSweeny [Sweeny99] lists the following network cheats that have been seen in Unreal \nTournament: \n \nSpeedhack. Exploits the client‘s clock for movement updates. Fixed by verifying \nclient and server clock stay nearly synced. \n \nAimbots. UnrealScript and external versions. \n \nWall hacks and radars. UnrealScript and external versions. \n \nConclusion \nTo develop a secure networking protocol for gaming, securing all game assets from code to \nart and networking data is important. Performance versus security tradeoffs must be \ndesigned into encryption and message protocols from the beginning. \nSecuring an online game is a constantly evolving war, and whatever methods are used \ntoday may fail tomorrow. Developers must constantly monitor the servers and communities \nto detect, mitigate, and prevent cheating. This involves tools to update clients, protocols, \nservers, and assets as needed to provide an enjoyable, level playing field for all customers. \nFinally, throughout the game development process, keep a list of security checkpoints and \nfollow them religiously. \n \nReferences \n[Eliam05] Eliam, Eldad. Reversing: Secrets of Reverse Engineering. Wiley, 2005. \n[Ferguson03] Ferguson, Neils, and Bruce Schneier. Practical Cryptography. Wiley, 2003. \n[Graff03] Graff, Mark, and Kenneth Van Wyk. Secure Coding: Principles and Practices. \nO‘Reilly Media, 2003. \n[Guilfanov09] Guilfanov, Ilfak. ―IDA and Obfuscated Code.‖ 2009. Hex-Rays. n.d. \n<http://www.hex-rays.com/idapro/ppt/caro_obfuscation.ppt>. \n[Hoglund07] Hoglund, Greg. ―4.5 Million Copies of EULA-Compliant Spyware.‖ 2009. \nRootkit. n.d. <http://www.rootkit.com/blog.php?newsid=358>. \n[Howard03] Howard, Michael, and David LeBlanc. Writing Secure Code, 2nd Edition. \nMicrosoft Press, 2003. \n",
      "content_length": 2196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": " \n \n[Levitt07] Levitt, Steven D. ―The Absolute Poker Cheating Scandal Blown Wide Open.‖ 2007. \nThe New York Times. n.d. <http://freakonomics.blogs.nytimes.com/2007/10/17/the-\nabsolute-poker-cheating-scandal-blown-wide-open/>. \n[Messner09] Messner, James. ―Under the Surface of Azeroth: A Network Baseline and \nSecurity Analysis of Blizzard‘s World of Warcraft.‖ 2009. Network Uptime. n.d. \n<http://www.networkuptime.com/wow/>. \n[Punk08] ―netCoders vs. PunkBuster.‖ 26 March 2008. Bashandslash.com. n.d. \n<http://bashandslash.com/index.php?Itemid=78&id=297&option=com_content&task=view\n>. \n[Schneier96] Schneier, Bruce. Applied Cryptography: Protocols, Algorithms, and Source \nCode in C, 2nd Edition. Wiley, 1996. \n[Seacord05] Seacord, Robert. Secure Coding in C and C++. Addison-Wesley, 2005. \n[Sweeny99] Sweeny, Tim. ―Unreal Networking Architecture.‖ 2009. Epic Games, Inc. n.d. \n<http://udn.epicgames.com/Three/NetworkingOverview.html>. \n[Watte08] Watte, Jon. ―Authentication for Online Games.‖ Games Programming Gems 7 \nBoston: Charles River Media, 2008. \n[WikiCipher09] ―Block Cipher Modes of Operation.‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation>. \n[WikiWarden09] ―Warden (software).‖ 2009. Wikipedia. n.d. \n<http://en.wikipedia.org/wiki/Warden_(software)>. \n \n5.2. Social Networks in Games: Playing with Your Facebook Friends \nClaus Höfele, Team Bondi \nclaus@claushoefele.com \nWhile multiplayer features are now commonplace, games often pit anonymous Internet \nusers against each other. This is a step backward from the enjoyment of playing split-screen \ngames with a friend sitting right next to you. In order to re-create this friendly atmosphere \nin online games, developers have to understand more about a player‘s ties with people, \nwhich is the domain of social networks such as Face-book, MySpace, and Twitter. \nThis gem describes how to access the web services of social networks from your game. As \nan example of how this might be put to use, the application developed in this gem will \ndemonstrate how your game can get access to a player‘s friends on Facebook. \nThe explanations in this gem describe Facebook integration from the point of view of a \nstandalone, desktop-style game as opposed to a game executed in a web browser. \nStandalone applications pose unique challenges because web services are primarily \ndesigned to be good web citizens but do not necessarily integrate well with desktop \napplications. \nRESTful Web Services \n",
      "content_length": 2495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": " \n \nRepresentational State Transfer (REST) is the predominant architecture for offering \nprogrammatic access to data stored on the web. \nA RESTful service is composed of a collection of resources, which are identified by a web \naddress, such as http://example.com/resource. Clients gain access to data through a set of \nwell-defined operations that can be used on these resources. Because RESTful services are \nbased on stateless operations (any state information is held in the client), a service can \nscale to a large number of clients—ideal for web services, which might have millions of \naccesses each day. \nREST does not demand any specific technologies in its implementation, which means every \nweb service has a similar but slightly different way of offering access to its resources. Also, \nweb services comply with pure RESTful design principles to varying degrees. \nIn practice, a RESTful service means that you‘ll send HTTP requests to send and receive \ndata. The most common HTTP operations are HTTP GET to retrieve data and HTTP POST \nto create new data on the server. \nRequesting Data \nAs an example of accessing data from a social network, consider the following request that \nuses Twitter‘s RESTful API [Twitter09]: \ncurl http://search.twitter.com/trends/current.json \n \ncURL [cURL09] is a tool that allows you to issue network requests on the command line. The \nprevious example sends an HTTP GET request to Twitter‘s servers to retrieve the most \npopular topics currently being discussed on Twitter. \nIn this simple example, you could have pasted the web address mentioned in the cURL \ncommand into the address field of your web browser. Because a web browser issues HTTP \nGET requests by default, you would have achieved the same result. When developing \naccess to web services, however, it‘s a good idea to learn how to use cURL because it has \nmany options that allow you to assemble more complex requests. For example, cURL also \nallows you to send HTTP POST requests and use HTTP‘s basic access authentication \nscheme. \nThe cURL command presented previously will result in a response similar to the following \noutput (formatted for easier reading): \n{\"trends\":{\"2009-08-23 04:00:47\":[ \n  {\"query\":\"\\\"Best love song?\\\"\",\"name\":\"Best love song?\"}, \n  {\"query\":\"#fact\",\"name\":\"#fact\"}, \n  {\"query\":\"#shoutout\",\"name\":\"#shoutout\"}, \n  {\"query\":\"#HappyBDayHowieD\",\"name\":\"#HappyBDayHowieD\"}, \n  {\"query\":\"\\\"District 9\\\"\",\"name\":\"District 9\"}, \n  {\"query\":\"\\\"Inglourious Basterds\\\"\",\"name\":\"Inglourious \nBasterds\"}, \n  {\"query\":\"\\\"Hurricane Bill\\\"\",\"name\":\"Hurricane Bill\"}, \n  {\"query\":\"#peacebetweenjbfans\",\"name\":\"#peacebetweenjbfans\"}, \n  {\"query\":\"#Nascar\",\"name\":\"#Nascar\"}, \n  {\"query\":\"Raiders\",\"name\":\"Raiders\"} \n]},\"as_of\":1251000047} \n \n",
      "content_length": 2764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": " \n \nHere, the output from the Twitter API is in JavaScript Object Notation (JSON) format \n[JSON09]. JSON is a lightweight data format that is becoming popular because it is less \nverbose and easier to parse than XML. \nDepending on the request, Twitter—like most web services—can be configured to produce \neither XML- or JSON-formatted output. I find that my applications often need an XML parser \nanyway because of other application requirements. For this reason, I tend to use XML more \noften because it is convenient to have a single data format in your application. \nA quick glance at the JSON data should give you a good idea of the information returned in \nthe request: Each line that starts with the word ―query‖ contains the name of a trending \ntopic as well as the search query that can be used to find all Twitter messages relating to \nthis topic. \nAuthenticating a User \nThe data received from the previous example represents public information that everyone \nhas access to. To gain access to private data, such as contact details and friends, you have \nto confirm a user‘s identity. \nPeople are understandably cautious to give applications access to their private data. For this \nreason, social networks have developed a variety of authentication mechanisms to cope \nwith different situations and technical limitations. Because these mechanisms vary wildly \nfrom service to service, authentication is often the most time-consuming request to \nimplement. \nThe most basic authentication mechanism requires users to enter a user name and \npassword, which your application sends to the web service. Entering authentication data \ninto your application requires users to trust your application not to collect passwords and \nabuse them for other purposes. This fear might stop users from trying out new applications \nbecause they don‘t want to risk their accounts being hijacked by malicious applications. \nApplications on the web have answered this need by offering authentication mechanisms \nbased on forwarding. The basic principle is that when logging in to a website, you are \nforwarded to the login page of the account provider and enter your user name and \npassword there. The application will never see your credentials, but will only receive a \nconfirmation of whether the login was successful. \nIdentifying Your Application \nApart from authenticating the user on whose behalf your application signs in to the service, \nmost websites also require a unique identifier that represents your application. Facebook, \nfor example, requires this. Twitter, on the other hand, doesn‘t use an application identifier. \nApplication identifiers allow for application-specific configurations on the service provider‘s \nwebsite but are also used to enforce service agreements between the developer and the \nservice provider. A social network might, for example, restrict what you are allowed to do \nwith the data received from the network. The service provider can choose to disable your \napplication if you violate the service agreement. \nDebugging RESTful Requests \nWhen developing my applications, I find it useful to see the data that is sent and received in \nthe requests to a web service. There are a number of good HTTP debug proxies [Fiddler09, \nCharles09] that act as middlemen between your application and a website. They often \ncontain special support to display and format XML and JSON data. \n",
      "content_length": 3403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": " \n \nHTTP proxies require a system-specific configuration so that the debugged application uses \nthe proxy instead of accessing the Internet directly. \nFor example: \ncurl —proxy localhost:8080 \nhttp://search.twitter.com/trends/current.json \n \nwill send the Twitter request from the previous example to a proxy on the local machine \n(localhost) using port 8080. An HTTP proxy installed on this port will then forward the \nrequest to the real server at search.twitter.com and record all data that goes back and forth \nbetween your computer and Twitter‘s server. \nAnother possibility is a network protocol analyzer, such as Wireshark [Wireshark09]. \nNetwork protocol analyzers work by listening to network packets going through your \nnetwork adapter. Because Wireshark works on a lower level than HTTP proxies, the \napplication is not aware of the fact that it is being debugged and thus doesn‘t need to \nchange its configuration. This is a more generic solution to monitor network traffic, but \nHTTP proxies are often easier to use because they specialize in HTTP traffic and \nautomatically filter out unnecessary information. \n \nThe Facebook API \nAs an example of how to integrate social networks into your game, this gem demonstrates \nhow to use Facebook‘s REST interfaces. \nSetting Up a Facebook Application \nBefore starting with your Facebook application, you have to register as a developer with \nFacebook. You do this by creating a Facebook user account and adding the Facebook \nDeveloper Application to your profile [Facebook09]. \nWithin the Developer Application, you‘ll find a link to set up your own Facebook application. \nFinishing this process will give you an API key that identifies your application when \nexchanging data with Facebook and a configuration page that contains your application‘s \nsetup. \nOne parameter you have to configure is the Canvas Callback URL, which determines from \nwhere Facebook pulls the content of your application if you were to display a page within \nFacebook. Since the demo application described in this gem is a desktop application, this \nURL is not used at all, but it is required nevertheless. \nMore importantly, you have to switch the Application Type from Web to Desktop. This \nchanges the authentication process when accessing Facebook‘s REST server to better suit \ndesktop applications. \nFacebook’s REST Server \nFacebook runs a RESTful service at the URL http://api.facebook.com/restserver.php. In \norder to exchange data with this server, you have to send an HTTP POST request with at \nleast the following parameters: \n",
      "content_length": 2570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": " \n \n \napi_key. This is the API key you get when registering your application with \nFacebook. \n \ncall_id. A number that increases with every request. \n \nsession_key. The session key obtained from the login process or empty if the \nrequest doesn‘t require a session. \n \nmethod. The API endpoint name that identifies the request. \n \nv. A version identifier, currently 1.0. \nSome requests have additional parameters, which are then appended to this list. \nFor Facebook‘s server to accept a request, you also have to send a signature that identifies \nyour application. To create the signature, you concatenate all input parameters to a string, \nappend a secret key, and build an MD5 hash out of this data. Since both Facebook and your \napplication know the secret key, Facebook‘s server can create the same signature and check \nthat the request is indeed coming from your application. \nThe secret key that‘s used for the signature is obtained by establishing a session. The secret \nis then called a session secret. Requests that are sent without a session context use the \napplication secret that you can look up in your application‘s configuration page on Facebook. \nThe handling of the secret key depends on the way you authenticate the user, so I‘ll have to \ntalk a bit more about authentication methods first. \nAuthenticating Facebook Users \nAs part of their terms of service—which must be agreed to in order to get an API key—\nFacebook forbids you to receive user names and passwords directly in your applications. \nInstead, users have to go through Facebook‘s website to log in. The reasoning is that users \nare more likely to trust the application because the same login screen is used that people \nare already familiar with from logging into Facebook on the web. In addition, it makes it less \nlikely, but not impossible, that applications will capture and hijack the user‘s password \nbecause the credentials are entered into a separate application (the browser). \nObviously, displaying a website for login purposes is easy for web applications. For desktop \napplications, on the other hand, you essentially have two choices: You can use the browser \nthat‘s installed on the user‘s system, or you can integrate a web browser, such as WebKit, \ninto your application. \nAuthentication with an External Browser \nLoading Facebook‘s login page in a browser separate from your application means that the \nuser has to leave your application until the Facebook login is complete. After the login, the \nuser returns to your application and confirms the authentication. Figure 5.2.1 illustrates this \nprocess. \nFigure 5.2.1. Authenticating a Facebook user through an external web browser. \n",
      "content_length": 2682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": " \n \n \n \nTo start with, your application has to request an authentication token from Face-book. (The \nmethod name of this request is auth.createToken.) Since you haven‘t established a \nsession yet, you use the application secret to sign this request. \nNext, you launch the external browser with a login page hosted by Facebook and pass the \ntoken to this website as part of the URL. The user can now log in to Facebook to establish a \nsession for your application. \nFinally, the user returns to your application and confirms the login process, whereupon your \napplication sends an auth.getSession request to Facebook. If the login was successful, \nyou will get a session key and a secret. The session key has to be used as part of the input \nparameters, and the session secret replaces the application secret in subsequent requests. \nThe demo application that comes on the CD for this gem contains an implementation of this \nlogin process, so you can see exactly what data needs to be sent to Facebook. \nAuthentication with an Application-Integrated Browser \nYou can achieve a better user experience by integrating a browser into your application. \nAgain, you have to load Facebook‘s login page to start the process. But this time, the web \npage is displayed as part of your application. Figure 5.2.2 shows the application flow. \nFigure 5.2.2. Authenticating a Facebook user through an application-integrated \nweb browser. \n",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": " \n \n \n \nWhen loading Facebook‘s login page, you pass in a parameter to configure a URL that gets \ndisplayed when the login is successful. This way, your application can figure out whether the \nuser has successfully logged in to Facebook by checking the URL that‘s currently being \ndisplayed in the browser. \nThis tight integration is only possible if you have complete control over the browser, for \nexample, by embedding WebKit [WebKit09] into your application. WebKit is an open-source \nweb browser layout engine that‘s also used as the basis of Apple‘s Safari and Google‘s \nChrome browsers. \nInstead of using WebKit directly, the demo for this gem uses the WebKit version that comes \nwith the Qt application framework [Qt09]. This makes it easy to display the browser \ncomponent as part of a dialog. For games, however, it might be better to render a web \npage‘s content to an image, which could then be displayed as a texture. (Have a look at the \nQWebFrame::render() API.) \nBecause the process starts off with Facebook‘s website when using an integrated browser, \nyour application never needs the application secret. This is a big advantage compared to the \nauthentication process with an external browser because it means the application secret can \nnever be compromised. \nPersisting a User Session \nBy default, a session is valid only for a limited time. To avoid going though the \nauthentication procedure every time the session expires, you can ask the user for offline \naccess. This permission can be requested during the login process by appending a \nparameter to the login URL (authentication with integrated browser only) or by displaying a \nwebsite with a permission form hosted by Facebook (works with both integrated and \nexternal browser authentication). \nTo skip the authentication, you store the session key and secret on the user‘s computer and \nuse these two values again the next time your application needs it. You have to check that \nthe permission is still valid because the user can revoke the authorization on Facebook‘s \nwebsite at any time. The demo application on the CD does this by sending a confirmation \nrequest to Facebook every time you start the application. \nApart from the session information, you should avoid storing data locally because it might \nbecome out of sync with the information on Facebook‘s website. \n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": " \n \nRetrieving the Friends List \nOnce you have obtained a session, getting information about the user‘s friends is \nstraightforward: You send a request with the name friends.get, which will return a list \nof user IDs. Alternatively, you can query only those friends that already have used your \napplication by using friends.getAppUsers. \nYou could match these IDs to a database of high scores, for example, to realize a high score \ntable that only contains friends of the user. Also, if a friend hasn‘t played the game yet, \nyour application could send out invitations to try out the game. \nPosting Messages \nAnother often-used request is stream.publish, which posts a message to the user‘s \nFacebook page. Similar to the offline access, this requires that the user grant special \npermission to your application. \nPublishing messages could be used to post status updates about the user‘s progress in your \ngame. \n \nConclusion \nIn this gem, I have shown you how Facebook can provide social context to your game. By \nintegrating a player‘s friends network, you can make your games a more personal \nexperience, which avoids the anonymity often associated with multiplayer games played \nover the Internet. \nWhile the majority of this gem has focused on Facebook, the information in this article \nshould provide enough information to extend your games with features from other web \nservices. Here are some ideas you might want to try: \n \nRecord a player‘s gameplay as a video and upload it to YouTube so it can be viewed \nby others. \n \nSend status updates to Twitter when someone achieves a new high score. \n \nSend messages to a player‘s Facebook friends to invite them for a game. \n \nRecord the location of the player and create high score lists of people in your \nneighborhood. \n \nReferences \n[Charles09] ―Charles: Web Debugging Proxy Application.‖ n.d. Karl von Randow. n.d. \n<http://www.charlesproxy.com/>. \n[cURL09] ―cURL.‖ n.d. <http://curl.haxx.se/>. \n[Facebook09] Website of Facebook‘s developer application. \n<http://www.facebook.com/developers>. \n[Fiddler09] ―Fiddler Web Debugging Proxy.‖ n.d. Microsoft. n.d. \n<http://www.fiddler2.com/>. \n[JSON09] ―Introducing JSON.‖ n.d. JSON. n.d. <http://json.org/>. \n",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": " \n \n[Qt09] ―Qt Cross-Platform Application and UI Framework.‖ n.d. Nokia Corporation. n.d. \n<http://qt.nokia.com/>. \n[Twitter09] ―Twitter API Documentation.‖ n.d. Twitter. n.d. <http://apiwiki.twitter.com/>. \n[WebKit09] ―The WebKit Open Source Project.‖ n.d. WebKit. n.d. <http://webkit.org/>. \n[Wireshark09] ―Wireshark.‖ n.d. Wirehsark. n.d. <http://www.wireshark.org/>. \n \n5.3. Asynchronous I/O for Scalable Game Servers \nNeil Gower \nneilg@vertexblast.com \nScalability is a critical concern in today‘s online gaming market. The small-scale networking \nof 8- to 32-player LAN games has given way to massively multiplayer Internet games and \ncentralized game-hosting services. Even if a single game instance supports only a small \nnumber of players, the ability to run additional game instances on a single server has \nserious repercussions for the cost of operating a game service. Every physical server incurs \nongoing space, power, and cooling costs on top of the initial hardware purchases and \nsoftware license fees. \nThis gem explores asynchronous I/O as a technique for improving the scalability of \nmultiplayer game servers. We first review traditional synchronous I/O techniques and their \nimplications for server architecture. Then we take a look at the asynchronous alternatives \nfor Windows and POSIX and demonstrate their use in a sample game server. We conclude \nwith an analysis of asynchronous I/O and its applicability to building scalable game servers. \nBackground \nInput/output (I/O) operations have always posed a challenge to game developers and to \nnetwork programmers in particular. I/O operations are often among the most time \nconsuming and unpredictable functions that game code will use. In part, this is because the \nactual I/O systems reside deep in the operating system. Making system calls and crossing \nthe user-space to kernel-space boundary takes time, both in terms of CPU state changes \nand in terms of data transfers between user space and kernel space. Furthermore, I/O \noperations deal with physical devices that don‘t always behave the way they should in \nprinciple. Physical devices such as DVD drives have to cope with things like discs covered in \nreal-world fingerprints and scratches, and network cards have to contend with the real-\nworld tangle of wires and devices that makes up our LANs and the Internet. \nSome of this chaos is hidden from our code by OS buffers. For example, when we call a \nwrite function, the data is rarely written directly to the device. Instead, the OS places the \ndata in an internal buffer, which it will then use to write to the physical device when it is \nready. While this is usually effective at protecting user code from the underlying complexity \nof the device, buffers still get full under heavy loads and traffic spikes. When that occurs, \nthe real world ripples back up to the application code in the form of failed operations and \nunexpected delays. \nAside from the impact of these delays on the performance of our game code, the other \nproblem we encounter with I/O operations is that our process sits idle while synchronous \nI/O operations execute. When there is plenty of processing left to do, we can‘t afford to \nwaste CPU cycles doing nothing. \n",
      "content_length": 3234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": " \n \nBlocking and Non-Blocking Synchronous I/O \nMost standard I/O APIs operate synchronously. They keep us synchronized with the I/O \nsystem by blocking further execution of our code until the current I/O operation is \ncomplete. This is the model you‘ll encounter when using functions such as fread() and \nfwrite(), or send() and recv() for sockets. For applications whose main function is \nI/O, such as an FTP client, these APIs can be perfectly adequate. If there‘s nothing else for \nthe app to do, blocking on I/O may be fine. \nHowever, for real-time games, we generally have a game-world simulation running at 30 to \n60 updates per second. Some game architectures may be more event-driven on the server \nside, but to maintain responsiveness within any architecture, we can‘t waste time waiting on \nI/O. The biggest delay we want to avoid is making blocking I/O requests when the OS is not \nready to handle them. For example, this occurs if we call recv() when there is nothing to \nread or send() when the OS‘s send buffer is full. \nOne way to address this is the socket API‘s non-blocking mode. However non-blocking \nsockets are not quite as great as they sound. In non-blocking mode, when the operation is \nnot possible (for example, the send buffer is full), the function will return immediately with \nan error code telling us to try again later. This ―try again later‖ cycle is polling, and we have \nto be careful that it doesn‘t get out of hand. It is easy with non-blocking sockets to waste \nmany CPU cycles polling for I/O readiness. Non-blocking sockets help us avoid our initial \nobstacle of getting hung up on I/O calls when the OS is not ready, but they don‘t \nnecessarily improve the execution efficiency of our program overall. \nHandling Multiple Clients \nA game server must handle multiple connections, and a scalable server must handle many \nconnections. One simple server design is to iterate over the set of all client connections and \nperform non-blocking I/O operations for each client in turn. However, this can involve a lot \nof polling, especially when the majority of the connections are idle, as is often the case \nwhen reading data from clients. As the number of connections increases, so does the length \nof the iterations, which in turn degrades the server‘s responsiveness to each client. \nRather than polling one socket at a time, an alternative approach is to use an I/O \nmultiplexer, such as select() or WaitForMultipleObjects() (or one of the many \nplatform-specific variants of these functions). This allows our code to block on a whole set \nof sockets and unblock with a set of sockets ready for I/O, which we can then process \nsequentially. \nThis addresses two issues. First, we don‘t have to worry about getting blocked on sockets \nthat aren‘t ready, because in principle the multiplexer only returns ready sockets—although \nin practice a socket‘s state can still change between when the multiplexer returns and our \ncode performs I/O with it. Second, the OS monitors the whole set of sockets for readiness, \nso we don‘t have to explicitly iterate over them in our code, making numerous system calls \non sockets whose state hasn‘t changed since the last iteration. \nHowever, we can‘t block the main game loop on a multiplexer call, because even with a long \nlist of clients, it‘s still impossible to know how long we‘ll have to wait. Putting a timeout on \nthe call is one way around this, but now we‘re turning the I/O multiplexer into a (potentially \nvery) expensive polling system. We need to decouple the I/O processing from the rest of \nthe code. \nThread Carefully \n",
      "content_length": 3608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": " \n \nTo allow the main game loop to execute while I/O operations are being processed and to \ntake advantage of the OS‘s ability to handle many parallel I/O streams, we could introduce \nadditional threads to the server. \nThe obvious one-thread-per-client approach, while relatively easy to implement, scales very \npoorly. This is due to several factors. For one, the cost of synchronization primitives such as \nmutexes generally scales relative to the number of threads involved. Threads also consume \nlimited OS resources and waste cycles as they get swapped in and out of execution focus. \nThe ideal number of threads to minimize context switching is proportional to the number of \ninstruction streams the CPU can process in parallel. On present-day hardware, this means \nwe should really only have a small number of threads, so for any significant number of \nclients it is simply not an option to spawn a thread for each. \nA more practical approach is to dedicate one thread (or a pool of threads) to I/O operations, \nallowing the main thread to proceed while the new thread deals with the I/O operations. \nBy introducing more than one thread into our application, we have also introduced \nsynchronization overhead (mutexes, critical sections, and so on) and some overhead for the \nthread-safe run-time environment and libraries. On the upside, this approach also has a \nwell-defined interface point between threads—the queues used to store I/O requests and \nresults. The synchronization code still requires some careful programming to avoid deadlock \nand race conditions, but this is a textbook application of multi-threading. \nFigure 5.3.1. (a) Thread per-client versus (b) I/O thread with multiplexing. \n \n \nThe solution we‘ve devised so far is very common in network server implementations. It \nessentially emulates asynchronous I/O—the main thread (or threads) submit I/O requests \nto a queue for the I/O thread, which processes the requests and then notifies the callers of \nthe results via an event queue or similar mechanism. \n",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": " \n \nAsynchronous I/O uses native OS services to deliver similar functionality without crossing \nthe system call boundary as often and without introducing additional I/O threads. It also \nenables the OS to take maximum advantage of its own internal scheduling systems to \noptimize I/O. \n \nAsynchronous I/O APIs \nThe two main APIs for asynchronous I/O are Windows Overlapped I/O and the AIO API in \nthe POSIX real-time extensions. Overlapped I/O is supported on Windows 2000 and later. \nPOSIX AIO is available on most UNIX-like operating systems, though the capabilities of the \nimplementations can vary considerably, so look before you leap. \nWorking with either the Windows or the POSIX API is quite similar. AIO requests are \nassociated with a control struct when they are made, which the OS uses to track the \nrequest and return the results to our code. These structs contain hidden internal fields for \nthe OS, so they have to be explicitly zeroed out before each request. Once a request has \nbeen made, the struct is off limits to our code until we have been notified that the operation \nis complete. It is important to realize that the data you pass into the asynchronous API \nmust be valid for at least as long as the asynchronous operation and also must be \nexclusively accessible to the OS during that time. In particular, this means we can‘t pass \nlocal variables into AIO calls, and we can‘t read from or write to the buffers until the \noperation completes. \nAfter the AIO operation has been initiated, there are several ways to get completion \nnotifications. The simplest option is usually to use a callback function. Both POSIX and the \nWindows API provide mechanisms to include an application-defined pointer in the callback‘s \ncontext. This pointer can be used to refer to our own data structures (such as the object \nthat initiated the request), containing any additional information required from the game \ncode‘s perspective to handle the completion event. \nAs you would expect, there are also functions for cancelling active I/O operations and for \nquerying their current status. One thing to watch out for when cancelling asynchronous \noperations is that they are not guaranteed to be done when the cancel function returns—the \ncancel operation itself is asynchronous! This means we have to be a little bit careful in the \nshutdown process of our game, so as not to free any control structs or buffers in use by \nactive operations. \nImplementation \nThe code accompanying this gem on the CD-ROM includes a sample asynchronous game \nserver implemented with both POSIX AIO and Windows Overlapped I/O. The code models a \nsimple game server that runs multiple GameInstance objects, each with a collection of \nSocket objects for the clients connected to that instance. The Socket class is a wrapper \naround a SocketImpl object, which provides access to the platform‘s socket API. \nFor portability, the server uses synchronous I/O to accept incoming connections. Once the \npreconfigured number of connections is made, the server begins running each session‘s \nmain game loop. \nIn GameInstance::tick(), the server looks for input from the clients, as seen in Figure \n5.3.2. Using an asynchronous read call, tick() keeps a read operation open for each \nclient. The SocketImpl code has some logic in it to ignore new read requests if one is \nalready in progress. This is an improvement over a non-blocking read, because the check \nlooks at a simple Boolean flag rather than making any calls into the actual I/O system. \n",
      "content_length": 3529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": " \n \nFigure 5.3.2. Asynchronous I/O server overview. \n \n \nThe socket code also contains logic for the case when we send or receive less than the \nexpected amount of data in a single I/O call. The SocketImpl initiates additional \nasynchronous reads and writes until the expected data arrives. The synchronous \nimplementation of this functionality requires polling and some additional bookkeeping code. \nAfter the game sessions are updated, the server calls GameInstance::update-\nClients() on up to one game instance per iteration. This models game instances that \nsend regular state updates to the clients at a rate that is less than the game‘s main loop \nfrequency. By updating sessions in a round-robin fashion, the load is spread out to avoid \nI/O spikes caused by all of the sessions updating all of their clients at once. \nAs mentioned earlier, we have to be careful to make sure that the AIO control structs are \nvalid for the entire duration of the I/O operations. In the sample code, the ReadRequest \nand WriteRequest structs are used for this purpose. They contain the control struct, \nbuffers, and other I/O request-related information and are stored in the SocketImpl \ninstance, so they will be valid at least as long as the socket handle. \nSocketImpl::close() contains logic for ensuring that outstanding operations are \ncomplete before the socket is destroyed. \n \nResults and Analysis \nRunning the sample server on a supported platform, such as Windows XP or Open-Solaris, \nwe find that as the amount of ―would block‖ time on the sockets increases, the efficiency (in \nterms of time spent executing application code) of asynchronous I/O over synchronous \n",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": " \n \nsockets grows. Small transfers, combined with clients that read and write continuously as \nfast as the server, give synchronous I/O an advantage. However, these are not conditions \nthat are found in real-world applications. There will always be times when clients have no \ndata to send or are not ready to receive more data from the server, and this is where \nasynchronous I/O excels. \nTo summarize, the main advantages of asynchronous I/O for game server design are: \n \nIt eliminates idleness due to blocking. \n \nIt eliminates system call overhead due to polling. \n \nIt eliminates the need for multi-threading to handle I/O. \n \nIt leverages the OS for tricky subsystems that handle concurrent I/O processing and \nnotification dispatching. \n \nIt creates opportunities for internal I/O optimizations in the OS kernel. \nThe main disadvantages of asynchronous I/O are: \n \nIt has greater overhead per system call. \n \nI/O-related code may be harder to understand and debug. \n \nAsynchronous I/O capabilities can vary across platforms. \nThe greater overhead of asynchronous system calls is a consequence of their more complex \nfunctionality. In addition to the functionality equivalent to their synchronous peers, they \nmust also register the operations for asynchronous processing and configure the pending \nnotifications. Due to this overhead, it is best to avoid many small requests and make fewer, \nlarger I/O requests when working with asynchronous I/O. \nAsynchronous I/O code can be more difficult to understand, particularly when the \nunderlying logic is naturally I/O driven. For example, consider a protocol exchange like the \nfollowing pseudocode: \nrecv( playerName ) \nsend( loginChallenge ) \nrecv( loginResponse ) \nif ( loginResponse is valid ) startGameLoop() \nelse close() \n \nImplemented using synchronous I/O, the code for this exchange could read almost exactly \nlike the pseudocode (with suitable error checking added, of course). However, as shown in \nFigure 5.3.3, when implemented using asynchronous I/O, it is necessary to store various \npieces of state information so that we can resume the process at the appropriate stage after \neach operation completes. \nFigure 5.3.3. A simple network protocol state machine. \n",
      "content_length": 2232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": " \n \n \n \nWhether we represent this information with ad hoc flags and status variables or with a \nformal state machine, we now need a state machine to manage the protocol, instead of \nhaving the protocol state maintained implicitly by the game‘s execution stack. How \napplicable this is to real-world game servers is highly design-dependent. In the case of the \nmodel server code, asynchronous I/O actually simplifies the implementation, because the \nmain protocol between the clients and the server is stateless. \nAsynchronous code is often more difficult to debug because of the flow issues just described \nand because of the non-deterministic behavior of the I/O notifications. On the other hand, a \nmulti-threaded server will contain comparable levels of complexity in both the code and \ndebugging techniques required. Which approach is more difficult to work with may be \nlargely a matter of preference. \nThe variability in platforms‘ asynchronous I/O support is a concern for the long-term \nmaintainability of a code base. If we want to port our game server to a new platform, we \nmay get markedly different results, or we may be stopped altogether by lack of support for \nasynchronous I/O. Subtle differences in behavior can also complicate porting efforts. \n \nConclusion \nAsynchronous I/O is a tool for network programmers to use to improve scalability. It is best \nplanned for from the outset when building a game server, but some server architectures \nemulate similar functionality already using threads, so asynchronous I/O can be a \nreasonable retrofit in some code bases. The sample code included on the CD-ROM can serve \nas a starting point for evaluating the applicability of asynchronous I/O to your project. \nWhether we choose to use it for network I/O or even for other server I/O tasks, such as \nwriting log files, asynchronous I/O can offer significant benefits to the scalability of our \nservers. By adopting this approach, we can also hope to see asynchronous I/O \nimplementations mature on game server platforms and someday perhaps become the de \nfacto standard for network server programming. \n",
      "content_length": 2116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": " \n \n \nReferences \n[Schmidt00] Schmidt, D., M. Stal, H. Rohnert, and F. Buschmann. Pattern-Oriented \nSoftware Architecture, Volume 2: Patterns for Concurrent and Networked Objects. John \nWiley & Sons, Ltd, 2000. \n[Schmidt02] Schmidt, D., and S. Huston. C++ Network Programming, Volume 1: Mastering \nComplexity with ACE and Patterns. Addison-Wesley, 2002. \n[Schmidt03] Schimdt, D., and S. Huston. C++ Network Programming, Volume 2: \nSystematic Reuse with ACE and Frameworks. Addison-Wesley, 2003. \n[Silberschatz02] Silberschatz, A., P. B. Galvin, and G. Gagne. Operating System Concepts. \nJohn Wiley and Sons, Inc., 2002. \n[Wright94] Wright, G., and R. Stevens. TCP/IP Illustrated, Volume 2: The Implementation. \nAddison-Wesley, 1994. \n \n5.4. Introduction to 3D Streaming Technology in Massively \nMultiplayer Online Games \nKevin Kaichuan He \nkhe@blizzard.com \nMassively multiplayer online games (MMOGs) have become very popular all over the world. \nWith millions of players and tens of gigabytes of game content, popular MMOGs such as \nWorld of Warcraft face challenges when trying to satisfy players‘ increasing demand for \ncontent. Delivering content efficiently and economically will have more and more impact on \nan MMO game‘s success. Today, game content is distributed primarily through retail DVDs \nor downloads, which are expensive and slow. \nIn the future, it should be possible to deliver game content disc-free and wait-free through \nstreaming technology. Game streaming will deliver the game world incrementally and on \ndemand to players. Any update of the game world on the developer side will be immediately \navailable to players. Sending only the portion of the game world that players are interacting \nwith will save us significant bandwidth. As a result, 3D game streaming will give MMOG \ndevelopers the edge of lower delivery cost, real-time content updates, and design for more \ndynamic gameplay. \nThis gem will give an introduction to the 3D game streaming technology and its challenges. \nIt will also dive into the key components of a 3D streaming engine, including the renderer, \nthe transport layer, the predictive loading algorithm, and client/server architecture. Various \ntechniques to partition, stream, and re-integrate the 3D world data, including the terrain \nheight map, alpha blending texture, shadow texture, and static objects, will be revealed. A \nreal implementation of a 3D terrain streaming engine will be provided to serve the purpose \nof an illustrative demo. Source code is available and written in Visual C++/DirectX. \nThe Problem \nDelivering a large amount of content from one point to the other over the network is not a \nnew problem. Since the inception of the Internet, various file transport tools such as FTP, \n",
      "content_length": 2757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": " \n \nHTTP, and BitTorrent have been designed to deliver content. We could argue that these \nprotocols are sufficient if all of the content we delivered could be perceived as an opaque, \nundividable, monolithic pile of binary numbers and if we had an infinite amount of \nbandwidth to ship this content from one place to another. In reality, we have only limited \nbandwidth, and latency matters. Also, it‘s not only the final result that we care to deliver for \ngames, it is the experience the end user receives at the other end of the network that \nmatters. There are two goals to reach in order to deliver a great gaming experience to the \nusers: \n \nLow wait time \n \nHigh-quality content \nUnfortunately, the two goals conflict with each other using traditional download technology \nbecause the higher the quality of the content, the larger the size of the content, and the \nlonger the delivery time. How do we solve this dilemma? \n \nThe Solution \nThe conflict between the goals of low wait time and high quality leads us to consider new \nrepresentations that enable intelligent partitioning of the data into smaller data units, \nsending the units in a continuous stream over the network, then re-integrating the units at \nthe receiving end. This is the fundamental process of 3D game streaming technology. To \nunderstand how to stream game content, let‘s quickly review how video is streamed over \nthe Internet. \nVideo Streaming \nVideo is the progressive representation of images. Streaming video is naturally represented \nby a sequence of frames. Loss cannot be tolerated within a frame, but each of these frames \nis an independent entity that allows video streaming to tolerate the loss of some frames. To \nleverage the temporal dependency among video frames, MPEG is designed to delta-encode \nframes within the same temporal group. MPEG divides the entire sequence of frames into \nmultiple GOFs (groups of frames) and for each GOF, it encodes the key frame (I frame) with \na JPEG algorithm and delta-encodes the B/P frames based on the I frames. At the client \nside, the GOFs can be rendered progressively as soon as the I frame is delivered. There are \nstrict playback deadlines associated with each frame. Delayed frames are supposed to be \ndropped; otherwise, the user would experience out-of-order display of the frames. \nThe RTP transport protocol is based on unreliable UDP and is designed to ship media \ncontent in the unit of frames, as well as being aware of time sensitivity of the video/audio \nstream and doing smart packet loss handling. \nTo meet the goal of low wait time, linear playback order of the video frames is leveraged. \nMost of today‘s video streaming clients and servers employ prefetching optimization at \nvarious stages of the streaming pipeline to load video frames seconds or even minutes \nahead of the time when they are rendered. This way, enough buffer is created for decoding \nand rendering the frames. As a result, users will enjoy a very smooth playback experience \nat the client side. \nGame Streaming \nMMORPG 3D content has a different nature when compared to video. First, it is not \nconsumed linearly unless we de-generate the 3D content to one dimension and force \nplayers to watch it from beginning to end—which defeats the purpose of creating a 3D \nenvironment in the first place. Second, unlike video, 3D content has no intrinsic temporal \n",
      "content_length": 3383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": " \n \nlocality. With video, the frame we play is directly tied to the clock on the wall. In 3D, an \nimmersed gamer can choose to navigate through the content in an unpredictable way. He \ncan park his avatar at a vista point to watch a magnificent view of a valley for minutes, and \nthere is no deadline that forces him to move. He can also move in an arbitrary direction at \nfull speed to explore the unseen. Thus, we cannot prefetch 3D content according to the time \nthe content is supposed to be played back because there is no such time associated with the \ncontent. On the other hand, just like wandering in the real world, avatars in the virtual \nworld tend to move continuously in the 3D space, and there is a continuity in the subset of \nthe content falling in the avatar‘s view frustum, thus we should leverage the spatial locality \ninstead of temporal locality when streaming 3D content. As a result, 3D world streaming \ngenerally involves the following steps: \n1.  Partition the world geographically into independently renderable pieces. \n2.  Prefetch the right pieces of world at the right time ahead of when the avatar will \ninteract with the pieces. \n3.  Send the pieces from server to client. \n4.  Re-integrate the pieces and render them at the client side. \nThroughout this gem, we will discuss these technologies in more detail, as well as how to \nintegrate them to build a fully functional 3D streaming demo, the 3DStreamer. The full \nsource code and data of 3DStreamer is included on the CD-ROM. Please make sure to check \nout the code and experiment with it to fully understand how 3D streaming works. \n \nThe World \nBefore we can stream a 3D world from an MMO content server to the clients, we have to \nbuild it. In this section we discuss the basic 3D terrain rendering components and how they \nare generated and prepared for streaming. \nWhat Constitutes a 3D World \nIn this gem we will focus on streaming the artistic content of a 3D world, because artistic \ncontent easily constitutes 90 percent of the entire content set of today‘s MMO, and it is the \npart being patched most aggressively to renew players‘ interest in the game. \nTypical artistic content of a MMORPG includes: \n \nTerrain \no \nMesh (height map, normal map) \no \nTextures (multiple layers) \no \nAlpha blending map (for blending textures) \no \nShadow map (for drawing shadows) \n \nTerrain objects (stationary objects) \no \nMesh/textures \n \nAnimated objects (characters, NPCs, creatures) \no \nMesh/textures/animations \n \nSound \n",
      "content_length": 2505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": " \n \nIt is beyond the scope of a single gem to cover them all, and we will focus on terrain and \nterrain objects in this gem because they form the foundation of a 3D world streaming \nengine. \nSlice the Land \nThis section discusses the details of breaking up the world into tiles for rendering and \nstreaming. \nPatches of Land \nI always wondered why my house was bought as Parcel #1234 in the grant deed until I ran \ninto the need of slicing 3D terrain data in a virtual world. Today‘s MMORPG has a huge \nworld that can easily overwhelm a top-of-the-line computer if we want to render it all at \nonce. Similarly, it takes forever to download the entire MMO terrain data. Thus, to prepare \nthe world for streaming, the first step is slicing it into pieces that we can progressively send \nover and render. \nAs shown in Figure 5.4.1, in 3DStreamer, we divide the world into 32 × 32 patches so that \neach patch can be stored, downloaded, and rendered independently. The terrain information \nfor each patch is stored in its own data file, Terrain_y_x.dat, including all the data needed \nto render the patch, such as the height map, normal map, alpha blending textures, shadow \nmap, terrain object information, and so on. \nFigure 5.4.1. World consisting of 32×32 patches. \n \n",
      "content_length": 1267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": " \n \n \n \nTiles and Height Map \nTo generate the mesh for each terrain patch using the height map, we need to further \ndivide each patch into tiles. As shown in Figure 5.4.2, each patch is divided into 32×32 tiles. \nA tile has four vertices, thus we have 33×33 vertices per patch. To render a terrain with \nvarying heights, we assign each vertex of a patch a height. Altogether, we will have 33×33 \nheight values, which constitute the height map of the patch. \nFigure 5.4.2. Patch consisting of 32×32 tiles. \n \n \nTo build a mesh for each patch, we simply render each tile with two triangles, as shown in \nFigure 5.4.3. \nFigure 5.4.3. Patch consisting of 32×32 tiles. \n",
      "content_length": 665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": " \n \n \n \nTo build a 3D height map, we need to map the above 2D mesh to 3D space. Here is the \nmapping between the 2D coordinate (x‘, y‘) we used above and its 3D world coordinates (x, \ny, z): \n{x, y, z} ç {x‘, height, -y‘} \nAs shown in Figure 5.4.4, the x‘ axis of the 2D coordinates becomes the X-axis of the 3D \nworld coordinate. The opposite of y‘ axis of the 2D coordinates becomes the Z-axis of the \n3D world coordinate. The 3D y-coordinate is given by the height of the vertices from the \nheight map. \nFigure 5.4.4. A rendered patch consisting of 32×32 tiles. \n",
      "content_length": 566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": " \n \n \n \nFigure5.4.4 shows a few terrain patches rendered by 3DStreamer. Note that the entire \nterrain (32×32 patches) is located between the X-axis (x‘ axis of 2D space) and –Z-axis (y‘ \naxis of 2D space). This makes the traversal of tiles and patches very easy: Both start from \nzero. \nTo stitch the patches together seamlessly, the 33rd column of vertices of the patch (x‘ , y‘) \nis replicated to the first column of vertices of the patch (x‘+1, y‘). Similarly, the 33rd row of \npatch (x‘, y‘) is replicated to the first row of patch (x‘, y‘+1). \nThe following constants define the scale of the terrain and reveal the relationship among \npatches, tiles, and vertices. \n#define TILES_PER_PATCH_X 32 \n#define TILES_PER_PATCH_Y 32 \n#define PATCHES_PER_TERRAIN_X 32 \n#define PATCHES_PER_TERRAIN_Y 32 \n#define TILES_PER_TERRAIN_X (TILES_PER_PATCH_X * \nPATCHES_PER_TERRAIN_X) \n#define TILES_PER_TERRAIN_Y (TILES_PER_PATCH_Y * \nPATCHES_PER_TERRAIN_Y) \n#define VERTICES_PER_TERRAIN_X (TILES_PER_TERRAIN_X + 1) \n#define VERTICES_PER_TERRAIN_Y (TILES_PER_TERRAIN_Y + 1) \n \n \nTerrain Generation \n3DStreamer has a random terrain generator that will generate random terrain data and \nstore it into two types of output files. \n",
      "content_length": 1215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": " \n \nTerrain_y_x.dat: The terrain patch (x,y) \nTerrain_BB.dat: The bounding boxes for all patches \n \nThe reason that we need Terrain_BB.dat is for collision detection before the patches are \nloaded. To keep the avatar on the ground, even for patches not loaded, we need to be able \nto perform a rough collision detection using the patch‘s bounding box (BB). Also, the BB \nenables us to perform a rough view frustum culling before the detailed mesh information of \nthe patches is streamed over. \nIn a commercial MMO, the terrain data is usually handcrafted by a designer and artist to \ncreate a visually appealing environment. For demo and research purposes, though, it is \nconvenient to generate an arbitrary large terrain procedurally and use it to stress the \nstreaming engine. \nHere is the process of how to generate random terrain data and deploy it using \n3DStreamer. \n1. Run 3DStreamer with ―-g‖ on the command line. Alternatively, copy the pre-\ngenerated data from the 3Dstreamer source folder on the CD-ROM to skip this step. \n2. Upload the terrain data to an HTTP server (such as Apache). \nNow we can run 3DStreamer in client mode to stream the above data from the HTTP server \nand render the terrain incrementally based on a user‘s input. In production projects, \nthough, you probably won‘t use procedurally generated terrain with streaming because it‘s \nmuch cheaper to send the seed parameters of the procedure instead of the output data of \nthe procedure. \n \nThe Rendering \nThis section introduces the basic rendering features of terrain and how they are integrated \nwith streaming. \nTerrain Mesh \nAs described earlier, we will have a per-patch height map. From the height map, we‘ll build \nthe mesh for each patch as in the following code. \nCreateMesh(int patch_x, int patch_y) \n{ \n TERRAINVertex* vertex = 0; \n  D3DXCreateMeshFVF ( nrTri, nrVert, D3DXMESH_MANAGED, \n                  TERRAINVertex::FVF, m_pDevice, &m_pMesh); \n  m_pMesh->LockVertexBuffer(0,(void**)&vertex); \n    for(int y = patch_y * TILES_PER_PATCH_Y, y0 = 0; \n           y <= (patch_y+1) * TILES_PER_PATCH_Y; \n           y++, y0++) \n  for(int  x = patch_x * TILES_PER_PATCH_X, x0 = 0; \n           x<= (patch_x+1) * TILES_PER_PATCH_X; \n           x++, x0++) \n \n   { \n     int height = GetHeight(x, y); \n",
      "content_length": 2286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": " \n \n     D3DXVECTOR3 pos = D3DXVECTOR3(x, height, -y); \n     // Alpha UV stretches across the entire terrain in range \n[0, 1) \n     D3DXVECTOR2 alphaUV = D3DXVECTOR2(x / \nVERTICES_PER_TERRAIN_X, \n                                       y / \nVERTICES_PER_TERRAIN_Y); \n \n     // Color UV repeats every 10 tiles \n     D3DXVECTOR2 colorUV = alphaUV * TILES_PER_TERRAIN_X / \n10.0f; \n     vertex[z0 * (width + 1) + x0] = \n            TERRAINVertex(pos, GetNormal(x, z), alphaUV, \ncolorUV); \n     // Update BBox of the patch \n } \nm_pMesh->UnlockVertexBuffer(); \n… \n \n} \n \nFirst, we create a D3D mesh using the D3DXCreateMeshFVF API. Then we lock the mesh‘s \nvertex buffer and fill it up with vertex information. For each tile (x,y) of the patch \n(patch_x, patch_y), we retrieve its height from the height map and build the position \nvector as (x, height, -y). \nNext, we set up two sets of texture coordinates—colorUV for sampling texels from the \nterrain textures and alphaUV for sampling the alpha blending values of the terrain \ntextures. We‘ll discuss more about alpha blending in the next section. Then we calculate the \nnormals of each vertex based on the height map. Note that we don‘t want to only store the \nposition vectors at the server and re-generate normals at the client, because normals on \nthe patch border depend on the height from multiple patches. Since we need each patch to \nbe independently renderable, it‘s better that we store pre-computed normals with the patch \ndata itself. During the process of adding vertices, we update the BBox of the patch and \nstore it in the patch as well. The total size of vertex data of our demo terrain is about 40 \nbytes * (32 * 32) ^ 2 = 40 MB. \nMulti-Texture and Alpha Blending \nWith the mesh set up for each patch, we‘ll be able to render a wireframe terrain as shown in \nFigure 5.4.4. To give the terrain a more realistic look, we need to draw multiple types of \ntextures on top of the mesh to represent different terrain features (for example, dirt, grass, \nand stone). Three common methods exist for multi-texturing terrain. \nSingle Texture \nThe simplest way to texture the terrain is to manually bake multiple textures into one huge \nimage that covers the entire terrain. This method gives us total control over the texture \ndetails, but the cost of doing so can be prohibitive for large terrains. \nPer-Tile Texturing \nA more common method is to create multiple small textures at sizes from 128×128 to \n512×512 and then tile them across the terrain. For example, we can create a dirt texture, a \n",
      "content_length": 2551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": " \n \ngrass texture, and a stone texture, and then based on the height of each tile, we can \nprocedurally assign one of the textures to the tile. In a D3D environment, we can divide the \nmesh of a patch into three subsets, assign each subset a unique texture, and render the \nthree subsets in three iterations. However, this method has drawbacks. First, it‘s slow \nbecause it needs to render each mesh in three iterations for three subsets. Second, the \nborder between two areas with different textures will have a zigzag-shaped contour and \nabrupt change in tone of color. This is because our tile is square-shaped, and we can only \ndraw the entire tile using the same texture. Unless we apply special treatment to the \ntextures at the borders, the zigzag-shaped texture divider looks ugly. \nAlpha Blending Multi-Layer Textures \nA better way to do this is to assign each texture to its own layer, create an alpha channel \nfor each layer, and use a pixel shader to blend the textures. For example, we have three \ntexture layers (dirt, grass, stone) in 3DStreamer and one alpha texture with three channels \ncorresponding to the three layers. For each vertex, we can independently control whether \nit‘s dirt, grass, or stone. This way, we will not have the obvious zigzag border between \ndifferent textures because textures are assigned at the vertex level, not at the tile level. \nWith per-tile texturing, two triangles of the same tiles are always assigned to the same \ntexture, thus we‘ll have the shape of the tile show up at the border between different \ntexture areas. With alpha-blended multi-textures, the triangle that crosses the border of \ndifferent textures will have different textures assigned to different vertices. Thus, the color \nof pixels inside the triangle will be interpolated based on samples from different textures. \nAs a result, we will have a tile-wide blending zone between different textures, which \neliminates the clear zigzag border line. To further smooth the texture transition, we can \nbroaden the blending zone by assigning partial textures to the vertex at the border. For \nexample, instead of assigning a vertex with entirely dirt, grass, or stone, we can assign a \nvertex with 50 percent dirt and 50 percent grass if it is at the border. \nThe following code shows how we create a single alpha texture with three channels used to \nblend three layers of textures. We assign each vertex a texture type (dirt, grass, or stone) \nprocedurally. We also apply a smoothing filter to the alpha texture so that alpha at the \nborder transitions slower. \nD3DXCreateTexture(pDevice, VERTICES_PER_TERRAIN_X, \nVERTICES_PER_TERRAIN_Y, \n1, D3DUSAGE_DYNAMIC, D3DFMT_A8R8G8B8, D3DPOOL_DEFAULT, \npAlphaMap); \n \n D3DLOCKED_RECT sRect; \n(*pAlphaMap)->LockRect(0, &sRect, NULL, NULL); \nBYTE *bytes = (BYTE*)sRect.pBits; \n \nfor(int i = 0;i < numOfTextures; i++) \n  for(int y = 0; y < VERTICES_PER_TERRAIN_Y; y++) \n  { \n    for(int x = 0; x < VERTICES_PER_TERRAIN_X; x++) \n    { \n        TerrainTile *tile = GetTile(x,y); \n       // Apply a filter to smooth the border among different \ntile types \n        int intensity = 0; \n        // tile->m_type has procedually generated texture types \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x - 1, y); \n",
      "content_length": 3278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": " \n \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x , y - 1); \n        if(tile->m_type == i) ++intensity; \n        tile = GetTile(x + 1, y); \n        if(tile->m_type == i) ++intensity; \n         tile = GetTile(x , y + 1); \n         if(tile->m_type == i) ++intensity; \n         bytes[y * sRect.Pitch + x * 4 + i] = 255 * intensity / \n5; \n       } \n    } \n(*pAlphaMap)->UnlockRect(0); \n \nFigure 5.4.5 shows the effect of alpha blending of three different textures (dirt, grass, \nstone) rendered by 3DStreamer. As you can see, the transition from one to another is \nsmooth. The total size of our terrain‘s alpha-blending data is about 3 bytes * (32 * 32) ^ 2 \n= 3 MB. \nFigure 5.4.5. Multi-texturing. \n \n \n \nStatic Shadow \nTo create a dynamic 3D terrain, we need to draw shadows of the mountains. We can either \ncalculate the shadow dynamically based on the direction of the light source or pre-calculate \na shadow map based on a preset light source. The latter approach is much faster because it \ndoes not require CPU cycles at run time. Basically, we build a per-vertex shadow texture \nthat covers the entire terrain. Each shadow texel represents whether the vertex is in \nshadow. We can determine whether a vertex is in shadow by creating a ray from the terrain \nvertex to the light source and test whether the ray intersects with the terrain mesh. If there \nis intersection, the vertex is in shadow, and it will have a texel value of 128 in the shadow \nmap. Otherwise, it is outside shadow and has a texel value 255. We can then use a pixel \nshader to blend in the shadow by multiplying the shadow texel with the original pixel. \n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": " \n \nFigure 5.4.6 shows the effect of static shadow when the light source is preset at the left-\nhand side. The cost of storing the shadow texture of our terrain is not much—only 1 byte * \n(32 * 32) ^ 2 = 1 MB for the entire terrain. \nFigure 5.4.6. Shadow. \n \n \n \nTerrain Objects \nWithout any objects, the terrain looks boring. Thus, 3DStreamer adds two types of terrain \nobjects (stones and trees) to the terrain. The mesh of each terrain object is stored in a .X \nfile and loaded during startup. We are not streaming the .X files in 3DStreamer because \nthere are only two of them. In a game where a lot of unique terrain objects are used, we \nshould stream the model files of terrain objects as well. \nTo place terrain objects on top of terrain, we can use the terrain generator to randomly pick \none of the object types and place it at a random tile with random orientation with a random \nsize. We need to save the terrain objects‘ placement information with the per-patch terrain \ndata in order to redraw the objects at the client side. The following code fragment is an \nexample of writing terrain object placement information to the disk for each tile during \nterrain generation. \nOBJECT *object = tile->m_pObject; \nIf (object) \n{ \n  out.write((char*)&object->m_type, sizeof(object->m_type)); \n  out.write((char*)&object->m_meshInstance.m_pos, \nsizeof(object->m_meshInstance.m_pos)); \n  out.write((char*)&object->m_meshInstance.m_rot, \nsizeof(object->m_meshInstance.m_rot)); \n  out.write((char*)&object->m_meshInstance.m_sca, \nsizeof(object->m_meshInstance.m_sca)); \n} else \n{ \n      OBJECTTYPE otype = OBJ_NONE; \n",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": " \n \n      out.write((char*)&otype, sizeof(otype) \n} \n \nAssuming 20 percent of tiles have objects on them, the disk space taken by terrain objects‘ \nplacement information is about (4 B+12 B * 3) * (32 * 32)^2 * 20% = 8 MB. \nDivide and Conquer \nBased on discussion in previous sections, our experiment terrain used in 3DStreamer \nconsists of 32×32 patches and about one million tiles. Altogether, this takes about 60 MB of \ndisk space to store. Here is a rough breakdown of the sizes of various components of the \nterrain data. \nComponent \nData Size \nTerrain mesh \n40 MB \nTerrain object \n8 MB \nAlpha blending 3 MB \nShadow map \n1 MB \nOther \n8 MB \n \nAs shown in the Figure 5.4.7, it is a big terrain that takes a broadband user of 1-Mbps \nbandwidth 480 seconds (8 minutes) to download the complete data set. Thus, without \nstreaming we cannot start rendering the terrain for eight minutes! With streaming we can \nstart rendering the terrain in just a few seconds, and we will continuously stream the terrain \npatches the avatar interacts with over the network to the client. \nFigure 5.4.7. Big terrain. \n",
      "content_length": 1100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": " \n \n \n \n \n \nThe Transport \nTo this point we have generated our terrain, partitioned it into patches, and stored the \npatches in Terrain_y_x.dat and the bounding boxes of all the patches in Terrain_BB.dat. We \nalso know how to render the terrain based on these patches of data. The question left is \nhow to store the streaming data and send it over to the client from its data source. \nData Source and File Object \nStreaming is a progressive data transfer and rendering technology that enables a ―short \nwait‖ and ―high-quality‖ content experience. The 3D streaming discussed here targets \nstreaming over the network. However, the basic concepts and technique works for \nstreaming from the disk as well. Disk streaming can be very convenient for debugging or \nresearch purposes (for example, if you don‘t have an HTTP server set up, you can run \n3DStreamer with data stored on a local disk, too). We want to build a data storage \nabstraction layer that allows us to source 3D terrain data from both the local disk and a \nremote file server. 3DStreamer defined a FileObject class for this purpose. \n// Asynchronous File Read Interface for local disk read and \nremote HTTP read \nclass FileObject \n{ \npublic: \n      FileObject(const char* path, int bufSize); \n      ~FileObject(); \n      // Schedule the file object to be loaded \n      void Enqueue(FileQueue::QueueType priority); \n",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": " \n \n      // Wait until the file object is loaded \n      void Wait(); \n      // Read data sequentially out of an object after it is \nloaded \n      void Read(char* buf, int bytesToRead); \n      virtual void Load(LeakyBucket* bucket) = 0; \n}; \n \nFileObject provides an asynchronous file-loading interface consisting of the following \npublic methods: \n \nEnqueue. Schedule a file object to be loaded according to a specified priority. \n \nWait. Wait for a file object to be completely loaded. \n \nRead. Stream data out of the file after it is loaded to memory. \nThis is the main interface between the game‘s prefetching algorithm and the underlying \nmulti-queue asynchronous file read engine. The preloading algorithm will Enqueue() to \nqueue a file object for downloading at the specified priority. The render will call Wait() to \nwait for critical data if necessary. We should avoid blocking the render thread as much as \npossible to avoid visual lag. Currently 3DStreamer only calls Wait() for loading the BB \ndata at the beginning. The design goal of the prefetching algorithm is to minimize the wait \ntime for the render loop. Ideally, the prefetching algorithm should have requested the right \npiece of content in advance, and the renderer will always have the data it needs and never \nneed to block. When a file object is downloaded to the client, the render loop calls Read() \nto de-serialize the content from the file buffer to the 3D rendering buffers for rendering. \nBehind the scene, the FileObject will interact with the FileQueueManager to add \nitself to one of the four queues with different priorities, as shown in Figure 5.4.8. The \nFileQueueReadThread will continuously dequeue FileObjects from the \nFileQueues according to the priority of the queues and invoke the \nFileObject::Load() virtual method to perform the actual download from the data \nsource. We define the pure virtual method Load() as an interface to source specific \ndownloading algorithms. \nFigure 5.4.8. FileObject and FileQueue. \n",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": " \n \n \nBoth HTTPObject and DiskObject are derived from FileObject, and they \nencapsulate details of downloading a file object from the specific data source. They both \nimplement the FileObject::Load() interface. So when FileQueueThread invokes \nthe FileObject::Load(), the corresponding Load method of HTTPObject or \nDiskObject will take care of the data source–specific file downloading. Thus, \nFileObject hides the data source (protocol)–specific downloading details from the \nremainder of the system, which makes the asynchronous loading design agnostic to data \nsource. \nAsynchronous Loading with Multi-Priority Queues \nTo fulfill the ―low wait time‖ goal of game streaming, we need to achieve the following as \nmuch as possible: \nThe render loop does not block for streaming. \nThis translates into two requirements: \n \nWhen we request the loading of a file object, such as a terrain patch, the request \nneeds to be fulfilled asynchronously outside the render thread. \n \nWe only render the patches when they are available and skip the patches not loaded \nor being loaded. \nTo fulfill the ―high quality‖ goal for game streaming, we need to achieve the following \nrequirements: \n \nDynamically adjust the prefetching order of content in response to the player‘s input. \n \nOptimize the predictive loading algorithm so that the patches needed for rendering \nare always loaded in advance. \n",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": " \n \nWe will discuss the first three requirements here and leave the last requirement to later in \nthis gem. \nTo support asynchronous loading (the first requirement), the render thread only enqueues a \nloading request to one of the prefetching queues via FileQueueManager and never \nblocks on loading. The FileQueueReadThread is a dedicated file download thread, and \nit dequeues a request from one of the four queues and executes it. \nFileQueueReadThread follows a strict priority model when walking through the priority \nqueues. It starts with priority 0 and only moves to the next priority when the queue for the \ncurrent priority is empty. After it dequeues a request, it will invoke the transport protocol–\nspecific Load method to download the FileObject from the corresponding data source. \n(Refer to the ―Transport Protocol‖ section for transport details.) At the end of the Load \nfunction, when data is read into memory of the client, the FileObject::m_Loaded is \nmarked as true. \nThe third requirement is to react to players‘ input promptly. Our multi-priority asynchronous \nqueueing system supports on-the-fly cancel and requeue. At each frame, we will reevaluate \nthe player‘s area of interest, adjust the priorities of download requests, and move them \nacross queues if necessary. To support the second requirement, the render loop will do the \nfollowing: For patches in the viewing frustum and already submitted to DirectX (p-\n>m_loaded is TRUE), we render them directly. For patches not submitted yet but already \nloaded to the file buffer (p->m_fileObject->m_loaded is TRUE), we call \nPuntPatchToGPU() to fill vertex/index/texture buffers with the data and then render the \npatches. \nvoid TERRAIN::Render(CAMERA &camera) \n{ \n  … \n  for (int y = 0; y < m_numPatches.y; y++) \n   for (int x = 0; x < m_numPatches.x; x++) \n   { \n    PATCH* p = m_patches[y * m_numPatches.x + x]; \n    if(!camera.Cull(p->m_BBox)) \n     { \n       if (p->m_loaded) \n          p->Render(); \n       if (p->m_fileObject && p->m_fileObject->m_loaded) \n       { \n          PuntPatchToGPU(p); \n          p->Render(); \n       } \n     } \n   } \n … \n} \n \nWith the above multi-priority asynchronous queueing system, we can load patches \nasynchronously with differentiated priority. The dedicated FileQueueReadThread thread \ndecouples the file downloading from the video rendering in the main thread. As a result, \nvideo will never be frozen due to the lag in the streaming system. The worst case that could \nhappen here is we walk onto a patch that is still being downloaded. This should rarely \nhappen if our predictive loading algorithm works properly and we are within our tested and \nexpected bandwidth amount. We did have a safety net designed in this case, which is the \nper-patch bounding box data we loaded during startup. We will simply use the BB of the \n",
      "content_length": 2844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": " \n \npatch for collision detection between the avatar and the terrain. So even in the worst case, \nthe avatar will not fall under the terrain and die—phew! \nTransport Protocol \nWe will use HTTP 1.1 as the transport protocol that supports persistent connections. Thus, \nall the requests from the same 3DStreamer client will be sent to the server via the same \nTCP connection. This saves us connection establishment and teardown overhead for \ndownloading each patch. Also, HTTP gives us the following benefits: \n \nHTTP is a reliable protocol, necessary for 3D streaming. \n \nHTTP is a well-known protocol with stable support. So we can directly use a mature \nHTTP server, such as Apache, to serve our game content. \n \nHTTP is feature-rich. A lot of features useful to 3D streaming, such as caching, \ncompression, and encryption, come for free with HTTP. \n \nThe implementation is easy, since most platforms provides a ready-to-use HTTP \nlibrary. \nAs described in the ―Transport Protocol‖ section, the HTTP transport is supported via \nHTTPObject, whose primary work is to implement the FileObject::Load() interface. \nThe framework is very easy to extend to support other protocols as well, when such need \narises. \nHTTP Compression \nCompression is very useful to reduce bandwidth of streaming. With HTTP, we can enable the \ndeflate/zlib transport encoding for general compression. \nHTTP Caching \nWe will not discuss caching in detail. For what it‘s worth, we can easily enable client-side \nHTTP caching by not giving INTERNET_FLAG_NO_CACHE_WRITE to the \nHttpOpenRequest() in the HttpObject::Load() method. \nLeaky Bucket \nA leaky bucket–based bandwidth rate limiter becomes handy when we need to evaluate the \nperformance of a game streaming engine. Say we want to see how the terrain rendering \nworks at different bandwidth caps—2 Mbps, 1 Mbps, and 100 Kbps—and tune our predictive \nloading algorithms accordingly, or we want to use the local hard disk as a data source to \nsimulate a 1-Mbps connection, but the real hard disk runs at 300 Mbps—how do we do this? \nLeaky bucket is a widely used algorithm for implementing a rate limiter for any I/O channel, \nincluding disk and network. \nThe following function implements a simple leaky bucket model. m_fillRate is how fast \ndownload credits (1 byte per unit) are filled into the bucket and is essentially the bandwidth \ncap we want to enforce. The m_burstSize is the depth of the bucket and is essentially \nthe maximum burst size the bucket can tolerate. Every time the bucket is negative on \ncredits, it returns a positive value, which is how many milliseconds the caller needs to wait \nto regain the minimum credit level. \nint LeakyBucket::Update( int bytesRcvd ) \n{ \n   ULONGLONG tick = GetTickCount64(); \n int deltaMs = (int)(tick - m_tick); \n",
      "content_length": 2791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": " \n \n if (deltaMs > 0) \n { \n // Update the running average of the rate \n  m_rate = 0.5*m_rate + 0.5*bytesRcvd*8/1024/deltaMs; \n  m_tick = tick; \n } \n // Refill the bucket \n  m_credits += m_fillRate * deltaMs * 1024 * 1024 / 1000 / 8; \n  if (m_credits > m_burstSize) \n    m_credits = m_burstSize; \n   // Leak the bucket \n  m_credits -= bytesRcvd; \n if (m_credits >= 0) \n   return 0; \n else \n   return (-m_credits) * 8 * 1000 / (1024 * 1024) / m_fillRate; \n} \n \nThis is the HTTP downloading code that invokes the leaky bucket for rate limiting. \nvoid HttpObject::Load(LeakyBucket* bucket) \n{ \n  … \n  InternetReadFile(hRequest, buffer, m_bufferSize, &bytesRead); \n  int ms = bucket->Update(bytesRead); \n  if (ms) \n     Sleep(ms); \n  … \n} \n \nFigure 5.4.9 shows a scenario where we set the rate limiter to 1 Mbps to run a terrain \nwalking test in 3DStreamer. The bandwidth we displayed is the actual bandwidth the \n3DStreamer used, and it should be governed under 1 Mbps within the burst tolerance. Also, \nit shows how many file objects are in each priority queue. Since we are running fast with a \nrelatively low bandwidth cap, there are some patches being downloaded in the four queues, \nincluding one critical patch close to the camera. \nFigure 5.4.9. Bandwidth and prefetch queues. \n",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": " \n \n \n \n \n \nPredictive Loading \nThe predictive loading algorithm is at the heart of a 3D streaming engine because it impacts \nperformance and user experience directly. A poorly designed predictive loading algorithm \nwill prefetch the wrong data at the wrong time and result in severe rendering lag caused by \nlack of critical data. A well-designed predictive algorithm will load the right piece of content \nat the right time and generate a pleasant user experience. The following are general \nguidelines to design a good prefetching algorithm: \n \nDo not prefetch a piece of content too early (wasting memory and bandwidth). \n \nDo not prefetch a piece of content too late (causing game lag). \n \nUnderstand the dependency among data files and prefetch dependent data first. \n \nReact to user input promptly. (I turned away from the castle, stop loading it.) \n \nUtilize bandwidth effectively. (I have an 8-Mbps fat idle connection; use it all, even \nfor faraway terrain patches.) \n \nUse differentiated priority for different types of content. \nWe designed a very simple prefetching algorithm for 3DStreamer following the guidelines. \nTo understand how it works, let‘s take a look at the camera control first. \nCamera Control \nThe camera controls what the players see in the virtual world and how they see it. To \nminimize visual lag caused by streaming, it is crucial that the prefetching algorithm \nunderstands camera controls and synchronizes with the camera state. \nA camera in the 3D rendering pipeline is defined by three vectors: \n",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": " \n \n \nThe ―eye‖ vector that defines the position of the camera (a.k.a. ―eye‖). \n \nThe ―LookAt‖ vector that defines the direction the eye is looking. \n \nThe ―Up‖ or ―Right‖ vector that defines the ―up‖ or ―right‖ direction of the camera. \nSome common camera controls supported by 3D games are: \n \nShift. Forward/backward/left-shift/right-shift four-direction movement of the camera \nin the Z-O-X horizontal plane (no Y-axis movement). \n \nRotate. Rotate the ―LookAt‖ vector horizontally or vertically. \n \nTerrain following. Automatic update of the y-coordinate (height) of the eye. \n3DStreamer supports all three controls. And the way we control the camera impacts the \nimplementation of the predictive loading algorithm, as you can see next. \nDistance Function \nWhen do we need a patch of terrain to be loaded? When it is needed. We will present a few \nways to calculate when the patches are needed in the following sections and compare them. \nViewing Frustum-Based Preloading \nWe should preload a patch before it is needed for rendering. As we know, the camera‘s view \nfrustum is used by the terrain renderer to cull invisible patches, so it‘s natural to preload \nevery patch that falls in the viewing frustum. With some experiments, we can easily find the \ndynamic range of that viewing frustum is so huge that it‘s hardly a good measure of what to \nload and when. \nSometimes the scope is so small (for example, when we are looking directly at the ground) \nthat there is no patch except the current patch we are standing on in the frustum. Does this \nmean we need to preload nothing except the current patch in this case? What if we \nsuddenly raise our heads and see 10 patches ahead? \nSometimes the scope is too big (for example, when you are looking straight at the horizon \nand the line of sight is parallel to the ground) and there are hundreds of patches falling in \nthe frustum. Does this mean we should preload all of the patches up to the ones very far \naway, sitting on the edge of the horizon? What about the patches immediately to our left \nshoulder? We could turn to them anytime and only see blanks if we don‘t preload them. \nAlso, we don‘t really care if a small patch far away is rendered or not even if it may be in \nthe viewing frustum. \nDistance Function–Based Preloading \nTo answer the question of when a patch is needed more precisely, we need to define a \ndistance function: \nD(p) = distance of patch p to the camera \nIntuitively, the farther away the patch is from the camera, the less likely the avatar will \ninteract with the patch shortly. Thus, we can calculate the distance of each patch to the \navatar and prefetch the ones in the ascending order of their distances. The only thing left is \nto define exactly how the distance function is calculated. \nStraight Line Distance Function \nThe simplest distance function we can define is: \nD(p) = sqrt((x0 – x1) * (x0 – x1) + (z0 –z1) * (z0 – z1)) \n",
      "content_length": 2922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": " \n \n(x0, z0) are the coordinates of the center of the patch projected to the XZ plane, and (x1, \nz1) are the coordinates of the camera projected to the XZ plane. Then we can divide \ndistance into several ranges and preload the patches in the following order: \n \nCritical-priority queue: Prefetch D(p) < 1 * size of a patch \n \nHigh-priority queue: Prefetch D(p) < 2 * size of a patch \n \nMedium-priority queue: Prefetch D(p) < 4 * size of a patch \n \nLow-priority queue: Prefetch D(p) < 8 * size of a patch \nIn other words, we will divide the entire terrain into multiple circular bands and assign the \ncircular bands to different priority queues according to their distance from the camera. \nThis is a prefetching algorithm commonly used in many games. However, this algorithm \ndoes not take into consideration the orientation of the avatar. The patch immediately in \nfront of the avatar and the patch immediately behind the avatar are treated the same as \nlong as they have equal distances to the avatar. In reality, the character has a higher \nprobability moving forward than moving backward, and most games give slower backward-\nmoving speed than forward moving, thus it‘s unfair to prefetch the patch in front of a \ncamera at the same priority as the patch behind it. \nProbability-Based Distance Function \nAn avatar can move from its current location to the destination patch in different ways. For \nexample: \n1. Walk forward one patch and left shift one patch. \n2. Turn left 45 degrees and walk forward 1.4 patch. \n3. Turn right 45 degrees and left-shift 1.4 patch. \n4. Take a portal connected to the patch directly. \n5. Take a mount, turn left 45 degrees, and ride forward 1.4 patch. \nI intentionally chose the word ―way‖ instead of ―path.‖ In order to use a length of the path \nas a distance function, the avatar must walk to the destination on the terrain at a constant \nspeed. In reality, there are different ways and different speeds for the avatar to get to the \npatch, and they may or may not involve walking on the terrain. Also, the distance cannot be \nmeasured by the physical length of the route the avatar takes to get to the patch in some \ncases (such as teleporting). A more universal unit to measure the distance of different ways \nis the time it takes the avatar to get there. With the above example, the time it takes the \navatar to get to the destination is: \n1. Assuming forward speed is 0.2 patch/second and left-shift speed is 0.1 \npatch/second, it takes the avatar 5 + 10 = 15 seconds to get there. \n2. Assuming the turning speed is 45 degrees/second, it takes the avatar 1 + 7 = 8 \nseconds to get there. \n3. It takes 1 + 14 = 15 seconds to get there. \n4. Assuming the portal takes 5 seconds to start and 5 seconds in transit, it takes 10 \nseconds to get there. \n5. Assuming the mount turns and moves two times as fast, it takes 8 / 2 = 4 seconds \nto get there. \nLet‘s say we know that the probabilities of the avatar to use the aforementioned ways to get \nto patch p are: 0.2, 0.6, 0.0 (it‘s kind of brain-dead to do 3), 0.1, and 0.1, respectively. \nThe probability-based distance D(p) will then be given by: 15 * 0.2 + 8 * 0.6 + 10 * 0.1 + \n4 * 0.1 = 3 + 4.8 + 1 + 0.4 = 9.2 seconds. \nThus, the probability-based distance function can be written as: \n",
      "content_length": 3278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": " \n \n \nwhere p(i) is the probability of the avatar taking way i to get to patch p, and t(i) is the time \nit takes to get to p using way i. \nAs you can see, the power of this probability-based distance function is that it can be \nexpanded to an extent as sophisticated as we want depending on how many factors we \nwant to include to guide the predictive loading algorithm. For a game where we want to \nconsider all kinds of combinations of moving options for an avatar, we can model the \nmoving behavior of the avatar statistically in real time and feed statistics back to the above \nformula to have very accurate predictive loading. At the same time, we can simplify the \nabove formula as much as we can to have a simple but still reasonably accurate predictive \nloading algorithm. In the 3DStreamer demo, we simplify the distance function as the \nfollowing. \nFirst, 3DStreamer does not support teleporting or mount, so options 4 and 5 are out of \nconsideration. Second, as in most games, 3DStreamer defines the speeds for left/right \nshifting and moving backward at a much lower value than the speed of moving forward. \nFrom the user experience point of view, it‘s more intuitive for the player to move forward \nanyway. So in 3DStreamer, we assume that the probability of an avatar using the ―turn and \ngo forward‖ way is 100 percent. With this assumption, the distance function is reduced to: \nD(p) = rotation time + moving time= alpha / w + d / v. \nAlpha is the horizontal angle the camera needs to rotate to look at the center of p directly. \nw is the angular velocity for rotating the camera. d is the straight-line distance between the \ncenter of p and the camera. v is the forward-moving speed of the avatar. \nThe following code fragment shows the implementation of the predictive loading in \n3DStreamer based on the simplified distance function. \nD3DXVECTOR2 patchPos(((float)mr.left + mr.right) / 2, \n((float)mr.top + \nmr.bottom) / 2); \nD3DXVECTOR2 eyePos(camera.Eye().x, - camera.Eye().z); \nD3DXVECTOR2 eyeToPatch = patchPos - eyePos; \nfloat patchAngle = atan2f(eyeToPatch.y, eyeToPatch.x); // [-Pi, \n+Pi] \n \n// Calculate rotation distance and rotation time \nfloat angleDelta = abs(patchAngle - camera.Alpha()); \nif (angleDelta > D3DX_PI) \n    angleDelta = 2 * D3DX_PI - angleDelta; \n \nfloat rotationTime = angleDelta / camera.AngularVelocity(); \n \n// Calculate linear distance and movement time \nfloat distance = D3DXVec2Length(&eyeToPatch); \nfloat linearTime = distance / camera.Velocity(); \n \nfloat totalTime = rotationTime + linearTime; \nfloat patchTraverseTime = TILES_PER_PATCH_X / camera.Velocity(); \nif (totalTime < 2 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_CRITICAL); \n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": " \n \nelse if (totalTime < 4 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_HIGH); \nelse if (totalTime < 6 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_MEDIUM); \nelse if (totalTime < 8 * patchTraverseTime) \n      RequestTerrainPatch(patch_x, patch_y, \nFileQueue::QUEUE_LOW); \n  else \n  { \n        If (patch->m_loaded) \n             patch->Unload(); \n        else if (patch->m_fileObject->GetQueue() != \nFileQueue::QUEUE_NONE) \n           CancelTerrainPatch(patch); \n} \n \nFor each frame, we reevaluate the distance function of each patch and move the patch to \nthe proper priority queue accordingly. The ability to dynamically move file prefetching \nrequests across priorities is important. This will handle the case where an avatar makes a \nsharp turn and the high-priority patch in front of the avatar suddenly becomes less \nimportant. In an extreme case, a patch earlier in one of the priority queues could be \ndowngraded so much that it‘s canceled from the prefetching queues entirely. \nNote that in the last case, where D(p) >= 8 * patchTraverseTime, we will process it \ndifferently depending on its current state. \n \nAlready loaded. Unload the patch and free memory for mesh and terrain objects. \n \nAlready in queue. Cancel it from the queue. \nWith this predictive loading algorithm, 3DStreamer can render a terrain of one million tiles \nat 800×600 resolution under 1-Mbps bandwidth pretty decently. At a speed of 15 \ntiles/second, we still get enough time to prefetch most nearby patches in the avatar‘s \nviewing frustum, even though we make sharp turns. \nFigure 5.4.10 shows what it looks like when we run the avatar around at 15 tiles/s with only \n1-Mbps bandwidth to the HTTP streaming server. The mini-map shows which patches of the \nentire terrain have been preloaded, and the white box in the mini-map is the intersection of \nthe viewing frustum with the ground. As you can see, the avatar is moving toward the \nupper-left corner of the map. As shown by ―Prefetch Queues,‖ all the critical- and high-\npriority patches are loaded already, which corresponds to all the nearby patches in the \nviewing frustum plus patches to the side of and behind the player. The medium- and low-\npriority queues have 80 patches to download, which are mostly the faraway patches in front \nof the player. \nFigure 5.4.10. Predictive loading under 1-Mbps bandwidth at 15 tiles/s. \n",
      "content_length": 2445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": " \n \n \nAlso note that the non-black area in the mini-map shows the current memory footprint of \nthe terrain data. It is important that when we move the avatar around, we unload faraway \npatches from memory to create space for new patches. This way, we will maintain a \nconstant memory footprint regardless of the size of the entire terrain, which is a \ntremendously important feature of a 3D streaming engine in order to scale up to an \ninfinitely large virtual world. \n \n3DStreamer: Putting Everything Together \n3DStreamer is a demo 3D terrain walker that implements most of the concepts and \ntechniques discussed in this gem. It implements a user-controllable first-person camera that \nfollows the multi-textured, shadow-mapped large terrain surface. A mini-map is rendered in \nreal time to show the current active area of interest and the viewing frustum. Various \nkeyboard controls are provided for navigation through the terrain as if in the game. (See \nthe onscreen menu for the key bindings.) Here is the one-page manual to set it up. \nCompile the Source \n3DStreamer.sln can be loaded and compiled with Visual Studio 2008 with DirectX November \n2008 or newer. The data is staged to the Debug folder for debug build by default. In order \nto run a release build, you need to manually copy the data (data, models, shaders, \ntextures) from the Debug folder to the Release folder. \nTerrain Generation \n",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": " \n \nRunning the 3DStreamer with the following command will generate a random 32 × 32-patch \nterrain and save the data in the executable‘s folder. It will take a while to complete. \nAlternatively, just use the pre-generated data in the Debug\\Data folder on the CD-ROM. \n3DStreamer -g \n \n \nStaging the Data \nFor HTTP streaming, upload the data (terrain_XX_YY.dat and terrain_BB.dat) to your HTTP \nserver and use the -s= command line argument to specify the server‘s host name and the \n-p= argument to specify the location of the data on the server. \nFor DISK streaming, simply give the data‘s path as the -p= argument. \nRun \nTo run it in VS, please make sure to add $(OutDir) to the Working Directory of the \nproject properties. Alternatively, you can run the executable from the executable folder \ndirectly. By default, 3DStreamer runs in DISK streaming mode with a default data path \npointing to the executable folder. \nTo run it in HTTP streaming mode, you need to give the -s argument for the host name. \nFor example, to stream the data from a server URL \nhttp://192.168.11.11/3dstreamer/32x32, just run it with the following command line: \n3DStreamer -h -s=192.168.11.11 -p=3dstreamer\\32x32\\ \n \nNote that the trailing slash \\ in the path is important. Otherwise, the client cannot construct \nthe proper server URL. \nYou can also adjust the bandwidth cap (the default is 1 Mbps) with the -b= argument. For \nexample, to run it in DISK streaming mode simulating a 2-Mbps link, just enter: \n3DStreamer –b=2 \n \n \n \nConclusion \nIn this gem we described a fundamental problem—delivering game content in a short time \ninterval at high quality. We then discussed a 3D streaming solution. We presented a three-\nstep method to build a 3D streaming engine: Decompose the world into independent \ncomponents at the server, transfer the components with guidance from a predictive loading \nalgorithm over the network, and reintegrate the components and render the world at the \nclient. In the process, we defined the distance function–based predictive loading algorithm \nthat is the heart of a 3D streaming engine. Finally, we integrated all the components to \nbuild a 3DStreamer demo that streams a large terrain of a million tiles that has multiple \nlayers of textures blended to the client in real time with a remote HTTP server hosting the \ndata. Now, the reader of this gem has everything he or she needs to apply 3D streaming \ntechnology to a next-generation MMO design! \n",
      "content_length": 2465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": " \n \n \nSection 6: Audio \nIntroduction \nA Practical DSP Radio Effect \nEmpowering Your Audio Team with a Great Engine \nReal-Time Sound Synthesis for Rigid Bodies \nIntroduction \nBrian Schmidt, Founder and Executive Director, GameSoundCon; President, Brian \nSchmidt Studios brian@brianschmidtstudios.com \nFor a good deal of its lifetime, advances in game audio have been focused on creating more \nadvanced audio chips and synthesis capabilities using dedicated pieces of hardware for \naudio processing. From the SID and NES chips through Yamaha chips, Sony SPUs, and the \noriginal Xbox audio chip, the trend has been toward more voices, higher fidelity, and \ngreater audio signal processing capabilities in a never-ending goal to create realistic video \ngame sounds. With the current generation and the movement toward a more easily C-\nprogrammable audio system, the emphasis has expanded somewhat—rather than focusing \nsolely on driving more mathematics into the audio signal path, an effort has been made to \nmake those (quite simple) technologies that are available easier to use and manipulate by \nthe composer and the sound designer. \nGame audio development, from a programming perspective, has therefore focused on two \nquite different areas: the very high and the very low level. \nAt the very high level, game audio development was radicalized by the introduction of high-\nlevel authoring tools and matching game audio engines. Prior to the creation of these tools, \nit was clear that one of the largest obstacles to great game audio was the ability for sound \ndesigners and composers (the people with the ―ears‖) to bring their vision to fruition, rather \nthan a lack of cutting-edge sound-processing technology. Often the ideas composers and \nsound designers had were well within the existing capabilities of common custom or third-\nparty game audio engines. Indeed, creating cooler game audio technology was of limited \nvalue, because even if new technologies were available, it was difficult (and programmer \nintensive) to effectively use those technologies. Composers and sound designers weren‘t \neven able to use simple existing technologies effectively, because even the simplest creative \nnotion required creating a detailed description of what they wanted and then going to the \nprogrammer in the hopes that it could be programmed without too much trouble. Such \ncode-driven workflow was not conducive to creativity, because it relied both on programmer \ntime (scarce in any project) and on the ability for the sound designer/composer to \nadequately describe what he or she wanted in a language the programmers could \nunderstand. \nHigh-level tools were introduced broadly around 2002, with the introduction of XACT (Xbox \nAudio Creation Tool) for Xbox and SCREAM for Sony platforms. In these tools—and later in \nplatform-independent tools, such as WWise and FMOD Designer—the sound designer or \ncomposer could use a graphical interface to create game audio content that was then \nplayable by the matching high-level game audio engine. The key element of these content-\ndriven tools was that they allowed, for the first time, highly desirable features to be used in \na way that the composer or sound designer could try, modify, tweak, and otherwise \nexperiment without needing to bother the programmer. \n",
      "content_length": 3314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": " \n \nSo, ironically, audio quality dramatically increased not in response to cool, cutting-edge low-\nlevel audio capabilities, but simply by packaging up the existing technologies into a \nformat/tool chain that could be easily used by the creative audio professional. In essence, \nbetter game audio came from programming better tools with better work-flows and better \nUIs. \nThe gem by Mat Noguchi of Bungie describes the content-driven system used in the Halo \nseries of games across Xbox and Xbox 360. Note the UIs created for the sound designer. \nAlso, pay particular attention to the mixing portion; game audio mixing and post-production \nrepresent some of the biggest issues in current-day game development. \nThe emphasis on high-level tools and content-driven audio systems notwithstanding, the \ncutting edge of game programming has no shortage of low-level problems to be solved. \nAudio signal processing down at the sample level still provides many challenges for great \ngame audio. Growing in popularity in game development is the use of customized digital \nsignal processing (DSP) algorithms to achieve specific effects. \nAudio DSP in games is sometimes compared to pixel shaders in graphics; in fact, the term \n―sound shader‖ is occasionally used. Although game engines have for some time supported \nsomewhat fixed-function DSP in the form of hard-coded resonant filters, \nocclusion/obstruction filtering, and environmental reverb, the advent of CPU-based audio \nsystems has greatly lowered the bar for custom-written DSP tailored for a specific audio \neffect. The DSP effects are sometimes used to take a sound and modify it for a certain \nenvironment/circumstance (such as a reverb or voice effect), or it may be integral to \ncreation of the sound itself (as in a pitch shifter used to create a low dinosaur roar from a \nrecording of an elephant). Audio DSP is also used to process the final output of the audio \nengine using special DSP effects called mastering effects, which are used in virtually every \nother audio/visual medium to put final polish on the sound. The gem by Ian Lewis on \ncreating a run-time radioization effect describes the creation of custom DSP for a particular \nvoice effect used in a popular Xbox title. \nIn addition to audio DSP designed to process existing audio data, further low-level audio \nprogramming challenges lie in creating audio from pure mathematics. Physical modeling is \nsuch a system, where the vibrations that result from the collision of objects are modeled \nand treated as audio data and fed into the audio mix. Further, savvy game programmers \nrecognize the treasure trove of data within the physics simulation code that can be used to \neither create audio or drive parameters of a sophisticated audio engine. The gem by Zhimin \nRen describes such a system, deriving modal synthesis control parameters from the physics \nengine to create tightly correlated audio matching the visual images for impacting and \nrolling interactions. \nSo there remains no shortage of high-level and low-level challenges for game audio \nprogrammers. Better tools that enable composers and sound designers to work more \nefficiently and take advantage of existing technologies in unique and creative ways are \ncontinually undergoing development and improvement. Low-level signal processing in the \nform of off-the-shelf or custom-written DSP provides greater variety and tighter interaction \nwith the visuals. And physical modeling, together with other procedurally generated audio, \nis beginning to show promise in real-world applications. In John Carmack‘s GDC 2004 \nkeynote address, he postulated that, but for a bit more CPU, game audio was ―basically \ndone.‖ We would challenge that assertion as… premature. \n \n6.1. A Practical DSP Radio Effect \nIan Ni-Lewis \nSinglemalt71@gmail.com \n",
      "content_length": 3823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": " \n \nLet‘s say you‘re making a squad-oriented first-person shooter and you want it to have great \nimmersive audio. So you carefully place all of the sound effects in the world, making sure \nthey have realistic distance-based roll-off curves. You calculate the geometry of the game \nmap and make sure each sound gets filtered for obstruction and occlusion. And you set up \nzones for reverberation and get them all carefully cross-faded. Everything sounds realistic. \nBut there‘s a problem, says your lead designer: The gameplay design requires that you be \nable to hear messages from your squadmates. If the squad gets separated, that dialog gets \nrolled off or occluded, and the player can‘t hear what‘s going on. You explain that this is \nrealistic, that people can‘t hear each other when they‘re hundreds of yards away or \nseparated by thick concrete walls, and that changing that would break the immersiveness of \nthe game. The designer is unmoved—and your dialog guy is on his side for once. The dialog \nneeds to be audible everywhere. \nAssuming your FPS isn‘t set too far in the past, a great way to solve this problem is with a \nradio effect. Keep all of the work you did on distance, obstruction, and occlusion for each \ndialog source, because that still sounds great when the source is nearby. But as the direct \nsound from the source gets fainter, you cross-fade in a version of the dialog that is band-\nlimited and distorted (see Figure 6.1.1). \nFigure 6.1.1. Cross-fading of original and distorted dialog based on distance. \n \n \nThis is a great effect—when the source is close by, you get the full environmental audio; but \nwhen it‘s far away, it sounds like it‘s coming to you over the airwaves. \nThe next question, and the topic of this gem, is: How do we get the distorted dialog? One \neasy solution is to just apply a radio effect offline in your favorite audio editing application. \nBut taking the easy way out in this case is going to double your storage budget for audio, \nnot to mention doubling the bandwidth required to render each line of dialog. Plus, it‘s no \nfun. It‘d be a lot more interesting if we could apply the effect in real time. Fortunately, it‘s \nnot too hard to do. \nThe Effect \nI‘m not going to try to explain exactly how signals are affected by radio transmission, \nbecause I can‘t. But we can come up with a pretty convincing approximation by making the \nsignal sound tinny and distorted, with maybe a little static thrown in for good measure. \nCranking Up the Distortion \nDistortion is the most interesting part of the effect from the programmer‘s point of view. We \nwant to emulate clipping, which happens when the signal volume is too high for the output \ndevice to handle. This is insanely easy to do if you don‘t care about quality—just raise the \nvolume and saturate. That lops off the top and bottom of each wave, as you see in Figure \n6.1.2. \n",
      "content_length": 2889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": " \n \nFigure 6.1.2. Distortion of a sine wave due to digital saturation. \n \n \nWith just a little extra gain, this doesn‘t sound all that bad—but it doesn‘t sound right, \neither. It‘s a little harsher and more ―digital‖ than what we really want. We‘re getting noise \nand distortion, but it‘s not the good kind. It‘s the bad kind that makes you think maybe you \nneed to go back and change all your sound levels. This is unsurprising, since all we‘ve done \nso far is digital clipping—the same thing your digital-to-analog converter is going to end up \ndoing if your levels are too hot. \nWhat we really want is something that sounds grungy, but warm—something that won‘t \nirritate our ears or make us think something is wrong with the game. Something that \nsounds nice and warm and analog…. How do we do that? \nTo understand how we go about making nice-sounding distortion, let‘s start by taking a look \nat why our naïve distortion technique sounds so bad. Figure 6.1.3 shows a spectral plot of a \nclean, full-range sine wave with a frequency of just under 1/8 Nyquist. \nFigure 6.1.3. Spectral (frequency) plot of a clean sine wave. \n \n \nFigure 6.1.4 shows the same sine wave with the gain turned up to 1.1. Notice the little \nbumps that start appearing to the right of the original frequency? There‘s the grunge we‘re \nhearing. It‘s harmonic distortion—―distortion‖ because we‘re getting additional frequencies \nthat weren‘t in the source, and ―harmonic‖ because the new frequencies happen to be \nmultiples of the frequency of the original sine wave. Normally, harmonic distortion is exactly \n",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": " \n \nwhat we want. But there‘s a problem here. It‘s hard to see in the previous graph, but watch \nwhat happens when we crank the gain up to 11 (see Figure 6.1.5). \nFigure 6.1.4. Spectral (frequency) plot of a clipped sine wave with gain = 1.1. \n \n \nFigure 6.1.5. Spectral (frequency) plot of a clipped sine wave with gain = 11. \n \n \nIt‘s a classic aliasing pattern: New frequencies are generated above Nyquist, so they wrap \nback over the top of the original frequencies. In the previous graph, only three of the \nfrequencies—the original sine and the largest two harmonics—are below the Nyquist \nfrequency. These three get smaller as you go from left to right. The next longest bars in the \ngraph are between 2× and 3× Nyquist, so they‘re reflected. You can see that they get \nsmaller from right to left. After that, the magnitudes get pretty small and hard to see, but \nat the resolution of this graph, there are still a couple of harmonics that bounced off zero \nand started decreasing from right to left again. \nSo there‘s where the harshness is coming from. The distortion is producing frequencies that \naren‘t band-limited, and that‘s turning into aliasing, and it sounds awful. Let‘s see if we can \nfix that and add some warmth while we‘re at it. \n",
      "content_length": 1254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": " \n \nWhat we‘ve been talking about so far is hard clipping, which boils down to just throwing \naway any sample with a magnitude larger than some threshold value and replacing it with a \nsample at that threshold value (or a negative threshold value). If we plotted a graph of \ninput sample values versus output values for a hard-clipping algorithm, it would look \nsomething like the graph in Figure 6.1.6, with input values on the X-axis and corresponding \noutputs on the Y-axis. This graph is called the transfer function. \nFigure 6.1.6. Transfer function for hard-clipping algorithm. \n \n \nOne easy way to improve on this is to gently reduce the values, rather than chopping them \noff entirely. We‘ll start by choosing a lower threshold than the one we used for hard \nclipping, so we have some headroom to work with. Our new soft-clipping code looks more \nlike this: \nif( input > threshold ) \n      output = ( ( input – threshold ) * ratio ) + threshold; \nelse \n      output = input; \n \nOr, simplifying some: \noffset = ( 1.0f – ratio ) * threshold; \nif( input > threshold ) \n      output = ( input * ratio ) + offset; \nelse \n      output = input; \n \nGraphing the transfer function of the soft clipper gives us something like what you see in \nFigure 6.1.7. \nFigure 6.1.7. Transfer function for softened clipping algorithm. \n",
      "content_length": 1322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": " \n \n \n \nIf you‘re familiar with studio production tools, you might recognize this as the transfer \nfunction for a hard-knee compressor. It‘s a common tool used to implement soft clipping. \nAnd it works, as you can see from the spectrum of our turned-to-11 sine wave run through \nthe soft clipper (see Figure 6.1.8). \nFigure 6.1.8. Spectral plot of hard-knee compressed sine, gain = 11, t = 0.2, r= \n0.4. \n \n \nIt almost works too well, in fact—the aliasing has almost disappeared, but so has much of \nthe harmonic distortion we were after in the first place. Well, maybe we can combine these \ntwo techniques. We could start out by implementing a hard-knee compressor, but instead of \nkeeping the ratio constant, we could make the ratio also vary with the magnitude of the \ninput sample. But now we‘re starting to do some expensive math. If we‘re going to go that \nfar, let‘s not play around. Let‘s go straight for the power tools and use a higher-order \npolynomial. \n",
      "content_length": 966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": " \n \nPolynomials show up in games occasionally. They‘re sometimes used for audio sample rate \nconversion, for instance. In graphics and animation, they show up frequently in the guise of \nparametric splines. The polynomial we need is very similar to a spline, and we‘ll derive it in \nmuch the same way. \nUnfortunately for us, the splines used in graphics are all parameterized on the single \nvariable t, which is usually interpreted as a distance along the spline from the starting point. \nSo we‘ll do this the old-fashioned way, by solving a system of linear equations based on a \nset of constraints. Here‘s what we want our function to look like: \n1. The value of the function at x = 0 is 0, and the slope of the function is 1 to begin \nwith. Things don‘t get interesting until x is greater than or equal to a threshold \nvalue, which we‘ll call t. That means that the value of the function at x = t is t. \n2. The slope at t is still 1. \n3. At x = 1, the slope of the function is the compression ratio, which we‘ll call r. \n4. We probably want the slope to reach r before x = 1. Let‘s define a point k (for knee) \nwhere the slope of the function is r. \nThat‘s four constraints, which means we need a fourth-order polynomial to satisfy them. So \nlet‘s start by defining that function: \nf = ax4 + bx3 + cs2 + dx \nThat‘s the function we‘ll run on each input value (x) to get the output value we‘re looking \nfor. The function itself is pretty simple. The complicated question is, what are the values of \nthe coefficients a, b, c, and d? We get those by solving a system of equations that represent \nour constraints. So let‘s restate those constraints as linear equations in a, b, c, and d, by \nsubstituting x, r, t, and k into our basic equation f(x). The system of equations looks like \nthis: \nAt this point, we‘ll stop pretending to be good at math and turn things over to a symbolic \nsolver (in this case, Maplesoft‘s Maple 13) and ask it to solve for a, b, c, and d in terms of t, \nk, and r. The results are: \n \nYes, it really is that ugly. Fortunately, we only have to calculate the coefficients when t, k, \nor r actually changes, which shouldn‘t be too often. \nOne last thing before we call the distortion finished. It turns out that if we set the \nparameters so that the function sounds nice for normal-range audio, it starts to behave \n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": " \n \nbadly when the amplitude is less than t or greater than k. We‘ll solve this by switching to a \nnormal hard-knee compressor function when the input is higher than t + k and ignoring the \ndistortion function completely when the input is less than t. \nNow we can start to play with this function—or rather, the piecewise function defined by our \nspline, the compressor ratio, and the inflection points t and k: \n \nIn Figure 6.1.9, notice how as the ratio decreases, the tops of the waves begin to look more \nsquared off—but instead of being completely flat, they get rounded off to the point of \nturning inside out. Visually, this effect seems a little odd. But in the audio domain, where a \nwave‘s frequency content is far more important than its shape, it sounds much better. Why? \nTake a look at the spectrum of our turned-up-to-11 sine wave, once we‘ve run its output \nthrough our new function (see Figure 6.1.10). \nFigure 6.1.9a. Transfer function with t = 0.2, k = 0.4, r varying from 0.9 to 0.4. \n \n \nFigure 6.1.9b. Transfer function with t = 0.2, k = 0.4, [0.9,0.4] applied to full-scale \nsine wave. \n \n",
      "content_length": 1112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": " \n \n \nFigure 6.1.10. Spectral plot of distorted sine, gain = 11, t = 0.2, k = 0.4, r = 0.4. \n \n \nThe new harmonics are loud and strong, but the aliased frequencies have all but \ndisappeared. (The aliased frequencies are in fact still present, but at such a low volume that \nthey don‘t show up at the resolution of this graph. For our purposes, they‘ve ceased to \nexist.) \n \nAutomatic Gain Control \nSo we‘ve got a decent-sounding distortion, but it has one major downside: The quality of \nthe distortion is heavily dependent on the volume of the input. That might not seem like a \ndrawback at first—after all, that‘s how distortion works in the real world. But it‘s a \nsignificant problem for game dialog. It‘s not always possible to normalize the volume across \nall dialog samples, so some lines of dialog will sound more distorted than others. Sadly, the \nloudest lines of dialog are invariably the most important or emotional lines—exactly the \nones you don‘t want to get lost in distortion. \nThere are a few different ways of dealing with this problem, but the one I‘ve had the most \nsuccess with is to fiddle with the volume before and after distortion. Decide on an ideal \naverage volume of samples to feed into the distortion effect. Then measure the actual \nvolume (or RMS, which we‘ll discuss in a moment) of your incoming samples. If the \nincoming RMS is lower than the ideal, then crank up the volume. If the incoming RMS is \nhigher, then turn it down. On the way out of the distortion effect, just apply the reverse of \nwhatever you did on the way in, so that the output volume stays about the same. \nFiddling with the volume like this is called automatic gain control, or AGC. It‘s a simple \neffect that‘s very popular in consumer recording devices. It‘s easy to implement. \nFirst, we calculate the root mean square (RMS) of the incoming samples. In theory, this is \nthe square root of the average of the square of all samples, or \n. In \npractice, we don‘t average all samples, because that would give too much weight to past \nvalues and not enough to more recent ones. Instead, we calculate a windowed RMS, which \n",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": " \n \nis the root mean square of a subset of samples, from the current sample n back to some \nprevious sample \n. Of course, we don‘t need to calculate \nthe complete sum for every sample—we just keep a running total. The C code for each \nsample looks something like this: \nfloat* rms, *y; \n \nconst int m; // window length \nfloat window[m]; \n \nfloat MS; // mean square \n \nfloat current = (y[n]* y[n]) / m; \nMS += current;            // add current value \nMS -= window[n % m];      // subtract oldest previous value \nwindow[n % m] = current;  // replace oldest value with current \nvalue \n \nrms[n] = sqrt(MS); // voila, root-mean-square \n \nThis gives an accurate RMS measurement, but if you‘re in a hurry, you can leave out the \nsquare and square root (thus calculating a windowed mean rather than a windowed RMS) \nand still get decent results. The important part is the moving average, which is effectively a \nlow-pass filter on our AGC inputs. \nOnce you‘ve calculated the input RMS, the rest of the AGC is simple. Let T be the target \nvolume. The AGC output is \n. In other words, we calculate the RMS as a \npercentage of the target and multiply each incoming sample by that percentage. \nOn the way out of the distortion effect, we want to readjust the volume to where it was \nbefore. We could just reverse our previous calculation and multiply by \n, but that \nturns out to be a bad idea. The problem is that the distortion effect has a large and not \nalways predictable effect on output volume. The volume of the output is a nonlinear function \nof the inputs. In non-mathematical terms, that means the output volume will be different in \nways that won‘t be easy to understand or correct. And in practical terms, that means your \nsound guy will not be happy with you. \nFortunately, we already have the tools we need to fix this problem. All we need is yet \nanother AGC. This time, instead of using a constant value for the target, we‘ll use the \nincoming RMS that we calculated before. The block diagram looks like Figure 6.1.11. \nFigure 6.1.11. Volume compensation by pre- and post-distortion AGC. \n \n",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": " \n \n \nOne last note on the AGC: As with anything involving division, there are numerical \ninstabilities in this formula. In particular, there‘s a singularity when the input volume is near \nzero. I‘ve dealt with this in two ways: either always adding an epsilon or clamping the input \nvolume to an arbitrary minimum. Both methods work equally well, in my opinion. The \nclamping method gives a little more headroom, so that‘s what I‘ve used in the code that \naccompanies this article. \n \nAdding Static \nA little bit of snap, crackle, and pop is the finishing touch on this effect. Static should be \nsubtle, so you can take some shortcuts here. Blending in a prerecorded noise sound is fine, \nbut unless it‘s a long wave, the loop points will become obvious. Depending on exactly what \nyour sound designer wants, it can be cheaper and more effective just to create your own. \nOne technique that works well: Take your floating-point input value, cast it to an int, \ninvert the bits, and divide by INT_MAX, like so: \nfloat noise = (float)(~(*(int*)& input)) / (float)INT_MAX; \n \nDrop that down to about –24 dB below the main outputs, and it sounds like a mixture of \nstatic and crosstalk. I personally love this effect, but it‘s not for everyone. It has a definite \ndigital-age sound to it, so I wouldn‘t suggest it for games set much earlier than 1990. \nMaking It Tinny \nThe finishing touch that really sells this effect is band-pass filtering. The sound has to be \nthin, like it‘s coming from a very small and not very well-made speaker. It‘s a simple effect \nto achieve. You‘ll want to make the exact parameters configurable, but a bandpass filter \nwith a 24-dB/octave roll off set at about 1 kHz makes a good starting point. \nDesigning this sort of filter is actually quite difficult. Fortunately, the heavy lifting has all \nbeen done long in the past. There is any number of excellent digital filter designs on the \nInternet, free for the taking. I recommend Robert Bristow-Johnson‘s excellent design, which \nis available at www.musicdsp.org/files/EQ-Coefficients.pdf. Try Bristow-Johnson‘s band-pass \nEQ, set to a center frequency of 1 kHz and a Q of 4.5. \n \nPutting It All Together \nFigure 6.1.12 shows the block diagram as implemented in the code accompanying this \narticle. \nFigure 6.1.12. Complete block diagram of radio effect. \n",
      "content_length": 2335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": " \n \n \nThis configuration puts the band-pass filter at the end, so that the sound is easier to fit into \na busy mix. You may want to try other configurations. For instance, if the radio is the main \nor the only sound playing, you‘ll get a fuller sound by putting the filter directly after the \ndistortion instead of at the end. Or you can double the filter for that extra-tinny sound. \nFinally, don‘t forget to add a limiter at the end of your mix. \n \nParameter Animation \nThe best part about applying a radio effect in-game, rather than baking it into your audio \nbeforehand, is that it gives you an opportunity to animate the parameters. Varying the \nsettings of your radio effect, either cyclically over time or dynamically in response to in-\ngame parameters, makes the audio much more organic and unpredictable. For instance: \n \nIncrease the distortion AGC’s gain target as the sound source gets further \nfrom the receiver. The effect is to add another distance/occlusion cue to the sound. \n \nLink the center frequency of the band-pass filter to a low-frequency \noscillator. This is a cheap way to get a phasing effect similar to an out-of-tune AM \nradio station. \n \nAnimate the ratio and knee of the distortion effect. I love this technique \nbecause it adds motion to the sound in a subtle and non-obvious way. Be careful, \nthough: A little of this goes a long way. \nSinusoidal low-frequency oscillators—LFOs—are extremely cheap to run. They require only \ntwo fused multiply-adds per sample and have no real storage needs, which means they can \nbe easily interleaved with other processing. The technique takes advantage of the fact that \nthe cosine and sine functions are derivatives of each other: \nsin(x)\n = cos(x) and cos(x)\n = – sin (x). As long as the frequency is low enough, you \ncan just: \n1. Scale the previous frame‘s sine and cosine values by the per-frame step (2π \n*frequency). \n2. Increment the sine by the scaled cosine value. \n3. Decrement the cosine by the scaled sine value. \nThat‘s all there is to it. This method falls apart at audio frequencies, but for LFOs it‘s \nremarkably stable. \n \nConclusion \nThe effect presented here isn‘t an accurate simulation of real-world electronics. But it‘s \npractical, relatively low cost, and effective. Most important, it‘s configurable and easy to \n",
      "content_length": 2310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": " \n \nuse. The polynomial waveshaper gives it a unique sound, and the dual AGCs make it easy to \ndrop into the mix. It‘s shipped in two triple-A titles that I know of, and I hope to see it ship \nin many more. \n \n6.2. Empowering Your Audio Team with a Great Engine \nMat Noguchi, Bungie \nmatthewn@bungie.com \nMaking award-winning game audio at Bungie isn‘t just about the using best technology or \nhaving the best composers (although that doesn‘t hurt). The best technology will ring flat \ngiven poor audio, and the best music will sound out of place given poor technology. If you \nreally want your game to sing, you need to put audio in the control of your audio team. \nFor the past nine years, with a core set of principles, a lot of code, and even more content, \nBungie has empowered its audio team to make masterpieces. This gem will explore the \naudio engine that drives Halo, from the basic building blocks the sound designers use to the \ninteresting ways the rest of the game interacts with audio. We will also take a peek at the \npost-production process to see how everything comes together. \nAudio Code Building Blocks \nThe sound engine starts with the s_sound_source. \n   enum e_sound_spatialization_mode \n   { \n       _sound_spatialization_mode_none, \n       _sound_spatialization_mode_absolute, \n       _sound_spatialization_mode_relative \n   }; \n \n   struct s_sound_source \n   { \n       e_sound_spatialization_mode spatialization_mode; \n       float scale; \n \n       // only valid if spatialization_mode is absolute. \n       point3d position; \n       quaternion orientation; \n       vector3d translational_velocity; \n   }; \n \nThis structure encompasses all the code-driven behavior of sound. You have your typical \npositional audio parameters, a fade on top of the default volume, some stereo parameters, \nand a single value called scale. What is scale? \nThe scale value is used to parameterize data from the game engine to the audio engine. It \nis normalized to lie within [0, 1], making it simple to use as an input into a function or linear \nrange. Everything that can play a sound in our game exports at least one scale value, if not \nmore. As a simple example, sounds that get generated from particle impacts receive a scale \nnormalized between 0.5 and 1.5 world units/second. A more complex example would be the \n",
      "content_length": 2328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": " \n \nsounds that play when a Banshee banks sharply and forms contrails at the wing. The actual \nscale that gets exported is shown in Figure 6.2.1 in our object editor. \nFigure 6.2.1. An object function from the Warthog for the engine sound. \n \nThis is an example of an object function; it takes various properties exported by an object \nand combines them into a single value that can be sent to the sound system. Incidentally, \nwe drive our shaders in a similar way, although shaders can use more than one input. \nIn general, simple things such as impacts and effects export a single scale. Objects such as \nthe Warthog and Brute can export a combination of multiple scales. \nParameterizing audio with a single value may seem a bit simplistic. However, as we‘ll \nexplore later, we tend to parameterize only a few properties of a sound based on scale, and \nin almost all cases it makes sense to parameterize multiple properties in a coupled fashion. \nFor spatialized audio, we have a separate distance envelope that we‘ll describe in the next \nsection. \n \nSound Parameterization \n",
      "content_length": 1078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": " \n \nGiven that we can send the audio engine interesting data from the game, we need to author \ncontent to use this data (that is, the scale and distance). The audio designers export .AIFF \nfiles, which get converted into the native platform format (XBADPCM for Xbox and XMA2 for \nXbox 360), and they attach in-game metadata through our custom game content files called \ntags. Sound content breaks down into one of two categories: impulse sounds and looping \nsounds. \nImpulse Sounds \nFor impulse sounds, such as impacts, gunshots, and footsteps, we allow the audio designers \nto adjust gain and pitch with the scale shown in Figure 6.2.2. \nFigure 6.2.2. Scale parameter editor. \n \n(Side note: Having your data use units that the audio team understands goes a long way to \nmaking them feel at home with the data they have to work with!) \nFor spatialized audio, we also can specify a distance envelope, as shown in Figure 6.2.3. \nFigure 6.2.3. Distance envelope editor. \n \nFrom the sound source origin to the ―don‘t play distance,‖ the sound is silent. From ―don‘t \nplay‖ to ―attack distance,‖ the sound scales from silence to full volume. Between ―attack \ndistance‖ and ―minimum distance,‖ the sound plays at full volume. And from ―minimum \ndistance‖ to ―maximum distance,‖ the sound scales from full volume back to silence. \nThe audio designers use the attack distance primarily for sound LODs. You can hear this for \nyourself in any Halo 3 level: A sniper rifle firing far away sounds like a muffled echo, while \nthe sniper rifle firing up close has the crisp report of a death machine. See Figure 6.2.4. \nFigure 6.2.4. Distance envelopes for the sniper rifle gunshot. \n",
      "content_length": 1670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": " \n \n \n \nImpulse sounds can also be parameterized based on the total number of instances of that \nsound playing. For example, when glass breaks, it can form a few or a lot of broken glass \nparticles. A lot of glass hitting a concrete floor sounds much different than a little; \nattempting to replicate that sound by playing a lot of the same glass impact sound does not \nwork without a prohibitively large variety of sounds. \nTo combat this, we allow sounds to ―cascade‖ into other sounds as the total number of \nsounds hits a certain threshold. For glass, the sound tag can specify a set of promotion rules \n(see Figure 6.2.5). \nFigure 6.2.5. Broken glass particle promotion rules. \n \n",
      "content_length": 685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": " \n \nThese promotion rules are defined in the order that they should play at run time. For each \nrule, you can specify which kind of sound to play (for example, few glass pieces, many glass \npieces) as well as how many instances of that kind can play before you start the next rule. \nEach rule can also contain a timeout to suppress all sounds from previous rules. \nUsing the rules from Figure 6.2.5, if we played five glass sounds at once, we would play four \ninstances of the breakable_glasspieces_single sounds. When the fifth sound \nplayed, we would play a breakable_glass_few sound and stop the previous four \nbreakable_glasspieces_single sounds. If we then managed to play four more \nbreakable_glass_few sounds in the same way (such that they were all playing at \nonce), we would play a breakable_glass_many sound, stop the previous \nbreakable_glass_few sounds, and then suppress any future glass sound for two \nseconds. \nCascading sounds allow us to have an expansive soundscape for particle impacts without \nplaying a prohibitive number of sounds at once. \nLooping Sounds \nA sound that does not have a fixed lifetime (such as engine sounds, dynamic music, or \nambience) is created using looping sounds. Because looping sounds are dynamic, we allow \ntheir playback to be controlled with a set of events: start, stop, enter alternate state, and \nexit alternate state. (More on alternate state in a bit.) Since these events are really just \nstate transitions, we need just two more bits for playing looping sounds: one bit for whether \nthe loop should be playing and one bit for whether it should be in the alternate state. For \neach event, as well as the steady state of normal playing and alternate playing, the audio \ndesigners can specify a sound. In the steady state when a looping sound is playing, we \nsimply keep playing the loop sound. It‘s usually authored such that it can play forever \nwithout popping. For transition events (start, stop, enter alternate, exit alternate, and stop \nduring alternate), those sounds either can be queued up to play after the loop or can play \non top of the currently playing loop. \nFigure 6.2.6. Looping sound state diagram. \n \n \n(―Alternate‖ is really a way of saying ―cool.‖ During the development of Halo 1, the audio \ndirector Marty O‘Donnell asked for a way to have a cool track for music, so we added the \nalternate loop.) \n",
      "content_length": 2377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": " \n \nDynamic music is implemented with just a few script commands: start, stop, and set \nalternate <on/off>. \nVehicle engines are implemented with looping sounds; however, in order to capture more \nintricacies with how engines sound under various loads (in other words, cruising at a low \nspeed sounds much different than flooring the accelerator), we use something similar to the \ncascade system to select different sounds to play based on scale: the pitch range. (In fact, \nas an implementation note, cascades are implemented referencing pitch ranges.) \nAs the name implies, a pitch range specifies a certain range of pitches to play in (for \nexample, only play this pitch range when the sound is playing from –1200 cents to 1200 \ncents). There are many playback parameters for that pitch range, such as distance \nenvelopes and relative bend. Relative bend is the bend applied to the permutation playing \nfrom this pitch range based on a reference pitch. In the example in Figure 6.2.7, if we were \nplaying the sound with a scale-based pitch of 55 cents, the idle pitch range sounds would \nplay with an actual pitch of –110 cents (pitch – reference pitch). The ―playback bends \nbounds‖ simply clamps the pitch to those bounds before calculating the actual pitch. \nFigure 6.2.7. Pitch range editor. \n \nThis is probably more complicated than it needs to be, since we are basically parameterizing \nthe pitch, then using that to select a pitch range, then converting that back into a relative \nbend to play sounds from that pitch range. But that‘s more a historical artifact than \nanything else, and now the audio designers are used to it. \nAt run time, you can have multiple pitch ranges from a single loop playing at once (see \nFigure 6.2.8). \nFigure 6.2.8. Warthog pitch ranges. Actual gain is displayed as power (gain2). \n",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": " \n \n \n \nThis allows for smooth cross-fading between multiple pitch ranges based on the input scale. \nThe looping sound system has been powerful enough to add novel uses of sound without \nadditional modifications. For example, in Halo 2, we added support for continuous collisions \n(for example, the sound a boulder makes rolling or sliding down a hill) from Havok by \ngenerating a looping sound at run time whenever we registered an object rolling or sliding; \nwe mapped the normal loop to rolling and the alternate loop to sliding so that a single \nobject transitioning between rolling and sliding would have a smooth audio transition \nbetween those states. \nThis kind of flexibility makes it very easy for the audio designers to collaborate with other \nprogrammers without necessarily having to involve an audio programmer. If you can export \na scale value, you can easily add either an impulse or a looping sound to whatever it may \nbe. \n \nMixing \nOne powerful aspect of Bungie‘s audio engine is how well it is integrated into the overall \ngame engine; everything that should make a sound can make a sound, from weapons firing, \nto objects rolling and bouncing, to the various sounds in the HUD based on in-game events. \nOne daunting aspect of the Halo audio engine is that almost everything makes a sound in \nsome way, which means the audio designers have to make a lot of audio content. \nTo make it easier to manage sound across the entirety of a game, we assign every sound a \nsound class, essentially a label we use to define a set of default sound properties, such as \ndistance envelope, Doppler effect multiplier, and so on. The properties in the sound class \nwill be applied to all sounds with that sound class by default, so the audio designers only \nhave to tweak a few sounds here and there. \nListing 6.2.1. A non-exhaustive listing of sound classes \nprojectile_impact \nprojectile_detonation \nprojectile_flyby \n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": " \n \nprojectile_detonation_lod \n \nweapon_fire \nweapon_ready \nweapon_reload \nweapon_empty \n \nobject_impacts \nparticle_impacts \nweapon_fire_lod \n \nunit_footsteps \nunit_dialog \nunit_animation \n \nvehicle_collision \nvehicle_engine \nvehicle_animation \nvehicle_engine_lod \n \nmusic \nambient_nature \nambient_machinery \nambient_stationary \nhuge_ass \n \nmission_dialog \ncinematic_dialog \nscripted_cinematic_foley \n \nWe also use sound classes to control the mix dynamically at run time. For each sound class, \nwe store an additional attenuation to apply at run time—essentially, a sound class mix. \nThese values can be script-driven; for example, during cinematics, we always turn down the \nambience sound classes to silence with the following script call: \n  (sound_class_set_gain \"amb\" 0 0) \n \nWe use a simple LISP-like scripting language. With the sound_class script commands, \nwe use the string as a sound class substring match, so this script command would affect the \ngain (in amplitude) for all sounds that have ―amb‖ in them. If we had a sound class called \n―lambchop‖ it would also affect that, but we don‘t. \nIn addition to manually setting the mix, under certain gameplay conditions, we can activate \na predefined sound class mix. For example, if we have Cortana saying something important \nto you over the radio, we‘ll activate the spoken dialog mix. These mixes, which are \nautomatically activated, fade in and out over time so that the change in volume doesn‘t \npop. The scripted mix and dynamic mix are cumulative; it‘s simple, but that tends to match \nthe expected behavior anyway. \n \n",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": " \n \nPost-Production \nThe bulk of audio production is spent in creating and refining audio content. This is a lot of \nwork, but it‘s fairly straightforward: Create some sound in [insert sound application here], \nplay it in game, tweak it, repeat. However, as the project gets closer to finishing, the audio \nteam has two major tasks left: scoring the game and finalizing the overall mix. \nScoring any particular level is a collaborative process between the composer and the level \ndesigner. The composer works with the level designer to determine music triggers based on \ngameplay, progression, or anything else that can be scripted. (The endgame driving \nsequence of Halo 3 has three triggers: one when you start driving on the collapsing ring, \none after Cortana says ―Charging to 50 percent!‖, and one when you make the final \nWarthog jump into the ship.) Each trigger can specify what looping sound to play, whether \nto use the regular or alternate loop, and when to start and stop. The composer can then \nwork alone to determine the appropriate music to use for the entire level. This collaborative \neffort allows the composer to remain in creative control of the overall score for a level while \nallowing the level designer to provide the necessary hooks in his script to help create a \ndynamic musical score. \nThere is also a chunk of time set aside at the end of production for the audio team to work \nwith finalized content. At this point all the graphics, cinematics, scripts, levels, animations, \nand so on, are locked down; this allows the audio team to polish without needing to worry \nabout further content changes invalidating their work. Once all the sound is finally in place, \nthe audio team then plays through the entire game in a reference 5.1 studio to adjust the \nfinal mix and make sure everything sounds great. \n \nConclusion \nBungie‘s audio engine isn‘t just a powerful engine; it‘s a powerful engine that has continued \nto evolve over time. Many of the concepts and features presented in this gem have been \naround since the first Halo game. Having a mature audio engine means that the entire audio \nteam can iterate the process of making game audio instead of having to reinvent technology \nfrom scratch. Many of the innovations in Bungie‘s audio engine have come from the audio \ndesigners, not just the programmers. In Halo 2, they came up with coupling the \nenvironment ambience loops with the state of the weather so that when a level transitioned \nto rain, so would the ambience. In Halo 3, they suggested the attack portion of the distance \nenvelope to support sound LODs. \nIn other words, Bungie‘s audio engine is not just about technology; it‘s about enabling \neveryone who works on audio to do great things. Any programmer who wants to add sound \nto their feature just needs to use the s_sound_source. Any audio designer can custom \ntailor the playback of any sound with a huge amount of flexibility and functionality. And with \nour mature and proven audio engine, an audio programmer has the framework to add \nfunctionality that can be used right away, in infinite variety. \nThe trifecta of lots of content, a fully integrated sound engine, and an effective audio \nproduction process, combined with Bungie‘s talented audio team, forms an award-winning \ngame audio experience. The numerous accolades Bungie has received for audio for the \nentire Halo series show that our approach to game audio works—and works well. \n \n6.3. Real-Time Sound Synthesis for Rigid Bodies \nZhimin Ren and Ming Lin \n",
      "content_length": 3527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": " \n \nzren@cs.unc.edu \nIn recent 3D games, complex interactions among rigid bodies are ubiquitous. Objects collide \nwith one another, slide on various surfaces, bounce, and roll. The rigid-body dynamic \nsimulation considerably increases the engagement and excitation levels of games. Such \nexamples are shown in Figure 6.3.1. However, without sound induced by these interactions, \nthe synthesized interaction and virtual world are not as realistic, immersive, or convincing \nas they could be. \nFigure 6.3.1. Complicated interactions among rigid bodies are shown in the two \nscenes above. In this gem, we introduce how to automatically synthesize sound \nthat closely correlates to these interactions: impact, rolling, and sliding. \n \n \nAlthough automatically playing back pre-recorded audio is an effective way for developers \nto add realistic sound that corresponds well to some specified interactions (for example, \ncollision), it is not practical to pre-record sounds for all the potential complicated \ninteractions that are controlled by players and triggered at run time. \nSound that is synthesized in real time and based on the ongoing physics simulation can \nprovide a much richer variety of audio effects that correspond much more closely to \ncomplex interactions. \nIn this gem, we explore an approach to synthesize contact sounds induced by different \ntypes of interactions among rigid bodies in real time. We take advantage of common \ncontent resources in games, such as triangle meshes and normal maps, to generate sound \nthat is coherent and consistent with the visual simulation. During the pre-processing stage, \nfor each arbitrary triangle mesh of any sounding object given as an input, we use a modal \nanalysis technique to pre-compute the vibration modes of each object. At run time, we \nclassify contact events reported from physics engines and transform them into an impulse \nor sequence of impulses, which act as excitation to the modal model we obtained during \npre-processing. The impulse generation process takes into consideration visual cues \nretrieved from normal maps. As a result, sound that closely corresponds to the visual \nrendering is automatically generated as the audio hardware mixes the impulse responses of \nthe important modes. \nModal Analysis and Impulse Responses \nIn this section, we give a brief overview on the core sound synthesis processes: modal \nanalysis and modal synthesis. Both of them have been covered in previous Game \nProgramming Gems. More details on modal analysis can be found in Game Programming \nGems 4 [O‘Brien04], and the impulse response to sound calculation (modal synthesis) is \ndescribed in Game Programming Gems 6 [Singer06]. \n",
      "content_length": 2691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": " \n \nFigure 6.3.2 shows the pipeline of the sound synthesis module. \nFigure 6.3.2. In pre-processing, a triangle mesh is converted into a spring-mass \nsystem. Then modal analysis is performed to obtain a bank of vibration modes for \nthe spring-mass system. During run time, impulses are fed to excite the modal \nbank, and sound is generated as a linear combination of the modes. This is the \nmodal synthesis process. \n \n \n \nSpring-Mass System Construction \nIn the content creation process for games, triangle meshes are often used to represent the \n3D objects. In pre-processing, we take these triangle meshes and convert them into spring-\nmass representations that are used for sound synthesis. We consider each vertex of the \ntriangle mesh as a mass particle and the edge between any two vertices as a damped \nspring. The physical properties of the sounding objects are expressed in the spring \nconstants of the edges and the masses of the particles. This conversion is shown in \nEquation (1), where k is the spring constant, Y is the Young‘s Modulus that indicates the \nelasticity of the material, t is the thickness of the object, mi is the mass of particle i, p is the \ndensity, and ai is the area covered by particle i. \nEquation 1  \n \n \nFor more details on the spring-mass system construction, we refer our readers to \n[Raghuvanshi07]. \nSince this spring-mass representation is only used in audio rendering and not graphics \nrendering, the triangle meshes used in this conversion do not necessarily have to be the \nsame as the ones used for graphics rendering. For example, when a large plane can be \nrepresented with two triangles for visual rendering, these two triangles do not carry detailed \ninformation for approximating the sound of a large plane. (This will be explained later.) In \nthis case, we can subdivide this triangle mesh before the spring-mass conversion and use \nthe detailed mesh for further sound computation. On the contrary, sometimes high-\n",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": " \n \ncomplexity triangle meshes are required to represent some visual details, but we are not \nnecessarily able to hear them. In this scenario, we can simplify the meshes first and then \ncontinue with sound synthesis–related computation. \nModal Analysis \nNow that we have a discretized spring-mass representation for an arbitrary triangle mesh, \nwe can perform modal analysis on this representation and pre-compute the vibration modes \nfor this mesh. Vibration of the spring-mass system created from the input mesh can be \ndescribed with an ordinary differential equation (ODE) system as in Equation (2). \nEquation 2  \n \n \nwhere M, C, and K are the mass, damping, and stiffness matrix, respectively. If there are N \nvertices in the triangle mesh, r in Equation (1) is a vector of dimension N, and it represents \nthe displacement of each mass particle from its rest position. Each diagonal element in M \nrepresents the mass of each particle. In our implementation, C adopts Rayleigh damping \napproximation, so it is a linear combination of M and K. The element at row i and column j \nin K represents the spring constant between particle iand particle j. f is the external force \nvector. The resulting ODE system turns into Equation (3). \nEquation 3  \n \n \nwhere M is diagonal and K is real symmetric. Therefore, Equation (3) can be simplified into \na decoupled system after diagonalizing K with K = GDG–1, where D is a diagonal matrix \ncontaining the Eigenvalues of K. \nThe diagonal ODE system that we eventually need to solve is Equation (4). \nEquation 4  \n \n \nwhere Z = G–1r, a linear combination of the original vertex displacement. The general \nsolution to Equation (4) is Equation (5). \nEquation 5  \n \n \n",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": " \n \nwhere λ is the i‘th Eigenvalue of D. With particular initial conditions, we can solve for the \ncoefficient ci and its complex conjugate, \n. The absolute value of ωi‘s imaginary part is the \nfrequency of that mode. Therefore, the vibration of the original triangle mesh is now \napproximated with the linear combination of the mode shapes zi. This linear combination is \ndirectly played out as the synthesized sound. Since only frequencies between 20 Hz to 20 \nkHz are audible to human beings, we discard the modes that are outside this frequency \nrange. \nImpulse Response Calculation \nWhen an object experiences a sudden external force f that lasts for a small duration of time, \nΔt, we say that there is an impulse fΔt applied to the object. f is a vector that contains \nforces on each particle of the spring-mass system. This impulse either causes a resting \nobject to oscillate or changes the way it oscillates; we say that the impulse excites the \noscillation. Mathematically, since the right-hand side of Equation (4) changes, the solution \nof coefficients ci and \nalso changes in response. This is called the impulse response of the \nmodel. \nThe impulse response, or the update rule of ci and \n, for an impulse fΔt, follows the rule \nexpressed in Equation (6): \nEquation 6  \n \n \nwhere gi is the i‘th element in vector G–1f. Whenever an impulse acts on an object, we can \nquickly compute the summation of weighted mode shapes of the sounding object at any \ntime instance onward by plugging Equation (6) into Equation (5). This linear combination is \nwhat we hear directly. With this approach, we generate sound that depends on the sounding \nobjects‘ shape and material, and also the contact position. \nIn conclusion, we can synthesize sound caused by applying impulses on preprocessed 3D \nobjects. In the following section, we show how to convert different contact events in physics \nsimulation into a sequence of impulses that can be used as the excitation for our modal \nmodel. \n \nFrom Physics Engine to Contact Sounds \nWhen any two rigid bodies come into contact during a physics simulation, the physics \nengine is able to detect the collision and provide developers with information pertaining to \nthe contact events. However, directly applying this information as excitation to a sound \nsynthesis module does not generate good-quality sound. We describe a simple yet effective \nscheme that integrates the contact information and data from normal maps to generate \nimpulses that produce sound that closely correlates with visual rendering in games. \nEvent Classification: Transient or Lasting Contacts? \nWe can imagine that transient contacts can be easily approximated with single impulses, \nwhile lasting contacts are more difficult to represent with impulses. Therefore, we handle \n",
      "content_length": 2797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": " \n \nthem differently, and the very first step is to distinguish between a transient and a lasting \ncontact. The way we distinguish the two is very similar to the one covered in [Sreng07]. \nTwo objects are said to be contacting if their models overlap in space at a certain point p, \nand if vp • np <, 0, where vp and np are their relative velocity and contact normal at point \np. Two contacting objects are said to be in lasting contact if vt ≠ 0, where vt is their relative \ntangential velocity. Otherwise, they are in transient contact. \nFrom Transient Contacts to Impulses \nWhen our classifier detects that there are only transient contacts, corresponding single \ninstances of impulses are added to the modal model. The impulse magnitude is scaled with \nthe contact force at the contact point. Developers can specify the scale to control transient \ncontact sound‘s volume. The direction of the impulse is the same as that of the contact \nforce, and it is applied to the nearest neighboring mass particle (in other words, vertex of \nthe original triangle mesh) of the contact point. We keep a k-d tree for fast nearest \nneighbor searches. \nThe following pseudocode gives some more details of this process. \n    IF a transient contact takes place THEN \n          COMPUTE the local coordinates of the contact point \n          FIND the nearest neighbor of the contact in local \nframe \n          CREATE impulse = contactForce * scale \n          CLEAR the buffer that stores impulse information \n          ADD the new impulse to the nearest neighbor vertex \n          UPDATE the coefficients of mode shapes \n    ENDIF \n \n \nFrom Lasting Contacts to Impulses: Using Normal Maps \nWhen our classfier sees a lasting contact (for example, sliding, scraping, and so on), the \ncontact reports from common real-time physics engines fall short for directly providing us \nwith the contact forces for simulating the sliding sound. If we directly add impulses \nmodulated with the contact force, we would not hear continuous sound that corresponds to \nthe continuous sliding motion. Instead, we would ―hear‖ all the discrete impulses that are \napplied to the object to maintain the sliding. One of the problems is the significant gap \nbetween physics simulation rate (about 100 Hz) and audio sampling rate (44,100 Hz). \nWe solve this problem with the use of normal maps, which exist in the majority of 3D \ngames. We generate audio from this commonly used image representation for sound \nrendering as well as by noting the following observations: \n \nNormal maps can add very detailed visual rendering to games, while the underlying \ngeometries are usually simple. However, common physics engines do not see any \ninformation from normal maps for simulation. Therefore, by only using contact \ninformation from physics engines, we miss all the important visual cues that are \noften apparent to players. \n \nNormal maps give us per-pixel normal perturbation data. With normal maps of \ncommon resolutions, a pixel is usually a lot smaller than a triangle of the 3D mesh. \nThis pixel-level information allows us to generate impulses that correspond closely to \nvisual rendering at a much higher sampling rate. This effect would have been \n",
      "content_length": 3214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": " \n \nimpossible to achieve with common 3D meshes even if the physics engine were to \ncalculate the contact force faithfully. \n \nNormal maps are easy to create and exist in almost every 3D game. \nFigure 6.3.3 shows a pen scraping against three normal-mapped flat surfaces. The flat \nplanes no longer sound flat after we take the normal map information as input to our \nimpulse generation for sound synthesis. \nFigure 6.3.3. A pen scraping on three different materials: brick, porcelain, and \nwood. The underlying geometries of the surfaces are all flat planes composed of \ntriangles. With normal maps applied on the surface, the plane looks bumpy. And it \nalso sounds bumpy when we scrape the pen on the surfaces! \n \n \nWe take the normal maps and calculate impulses in the following way. \nImagine an object in sliding contact with another object, whose surface F is shown in Figure \n6.3.4; the contact point traverses the path P within a time step. We look up the normal map \nassociated to F and collect those normals around P. The normals suggest that the high-\nresolution surface looks like f in Figure 6.3.4 and that the contact point is expected to \ntraverse a path P on f. Therefore, besides the momentum along the tangential direction of \nF, the object must also have a time-varying momentum along the normal direction of F, \nnamely pN, where N is the normal vector of F. From simple geometry, we compute its value \nwith Equation (7). \nEquation 7  \n \n \nwhere m is the object‘s mass and vT is the tangential velocity of the object relative to F. The \nimpulse along the normal direction JN that applies on the object is just the change of its \nnormal momentum expressed in Equation (8) \nEquation 8  \n \n \nwhen the object moves from pixel i to pixel j on the normal map. With this formulation, the \nimpulses actually model the force applied by the normal variation on the surface of one \nobject to another, generating sound that naturally correlates with the visual appearance of \nbumps from textures. \n",
      "content_length": 2003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": " \n \nFigure 6.3.4. Impulse calculation. (a) The path P traced by an object sliding \nagainst another object within one physics simulation time step. Each square is a \npixel of the normal map bound to the surface. ni are the normals stored in the \nnormal map around the path. The path lies on the surface F, which is represented \ncoarsely with a low-resolution mesh (here a flat plane). (b) The normal map \nsuggests that the high-resolution surface looks like f, and the object is expected to \ntraverse the path P. (c) The impulse along the normal direction can be recovered \nfrom the geometry configuration of n, N, and VT. \n \n \nAt the end of each time step, we collect impulses that should have taken place in this time \nstep by tracing back the path the object took. We make up these impulses in the immediate \nfollowing time step by adding them into our impulse queue at the end of this time step. The \nfunction for doing this is shown below. \n/** \n* \\param[in] endX the x coordinate of ending position \n* \\param[in] endY the y coordinate of ending position \n* \\param[in] endTime time stamp of the end of this time step \n* \\param[in] elapsedTime time elapsed from last time step \n* \\param[in] tangentialVelocity tagential velocity of the object \n* \\param impulseEventQueue gets updated with the new impulses \n*/ \nvoid CollectImpulses(float endX, float endY, \nfloat endTime, float elapsedTime, Vec3f tangentialVelocity, \nImpulseEventQueue & impulseEventQueue) \n{ \n      float speed = tangentialVelocity.Length(); \n      float traveledDistance = elapsedTime * speed; \n \n// dx is the width of each pixel \n// We assume pixels are square shaped, so dx = dy \n// +1 to ensure nGridTraveled >= 1 \n      int nGridTraveled = int(traveledDistance / dx) + 1; \n \n// Approximate the time used for going through one pixel \n// Divided by 2 to be conservative on not missing a pixel \n      // This can be loosen if performance is an issue \n      float dtBump = ( elapsedTime / nGridTraveled ) / 2; \n      float vX = tangentialVelocity.x, vY = \ntangentialVelocity.y; \n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": " \n \n \n// Trace back to the starting point \n      float x = endX - vX * elapsedTime, y = endY - vY * \nelapsedTime; \n      float startTime = endTime - elapsedTime; \n      float dxTraveled = vX * dtBump, dyTraveled = vY * dtBump; \n \n// Sample along the line segment traveled in the elapsedTime \n      for(float t = startTime; t <= endTime; t += dtBump) \n{ \n            ImpulseEvent impulseEvent; \n \n            // Compute the impulse from the normal map value \n           impulseEvent.impulse = \nGetImpulseAt(x, y, tangentialVelocity); \n           if (impulseEvent.Impulse.Length() == 0) \n                  continue; \n \n            // Update impulseEvent and \n// add this impulse to impulse queue \n            impulseEvent.Time = t; \n            impulseEvent.x = x; \nimpulseEvent.y = y; \n            impulseEventQueue.push_back(impulseEvent); \n            x += dxTraveled; \ny += dyTraveled; \n      } \n} \n \nNotice that we are assuming the object has constant velocity in one time step when we \ntrace back its path. The impulses in the impulse queue updated here are taken out of the \nqueue and added sequentially to the modal model in the next time step. When we ―play \nback‖ these impulses, we look at their time stamps (the variable shown in the code as \nimpulseEvent.Time) and add the excitation exactly the same time in the next time \nstep. Although there will be a delay of one time step, humans are not able to detect it \naccording to [Guski03]. However, humans are able to detect the impression of all the \nimpulses played at different time in one time step. This approach gives a finer \napproximation of the sound responses to the visual cues. \nWe refer our readers to [Ren10] for more details on contact sound synthesis for normal-\nmapped models. \n \nPutting It Together \nThe complete pipeline (as shown in Figure 6.3.5) is composed of the interaction handling \nand sound synthesis modules. The sound synthesis module consists of the modal analysis \nand modal synthesis processes and the interaction handling, which converts a contact event \ninto impulses. The sound synthesis module calculates the mode shapes at audio sampling \nrate (44.1 kHz), while interaction handling runs at the same rate as the physics engine \n(about 100 Hz). However, the impulses generated from normal maps are played back at a \nhigher frequency than the physics engine. \n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": " \n \nFigure 6.3.5. The complete pipeline for the sound synthesis algorithm discussed in \nthis gem. \n \n \n \n \nConclusion \nThis gem describes a pipeline for automatically synthesizing sounds that correspond well to \nthe visual rendering in games. This method utilizes graphics resources such as triangle \nmeshes and normal maps to produce sound for different kinds of interactions. It renders \naudio effects that are tightly related to the visual cues, which otherwise cannot be captured \nat all. The method is general, works well with most game engines, and runs in real time. \n \nReferences \n[Guski03] Guski, Troje. ―Audiovisual Phenomenal Causality.‖ Perception and Psychophysics \n65.5 (2003): 789–800. \n[O‘Brien04] O‘Brien, James F. ―Modal Analysis for Fast, Stable Deformation.‖ Game \nProgramming Gems 4. Boston: Charles River Media, 2004. 287-298. \n[Raghuvanshi07] Raghuvanshi, Lin. ―Physically Based Sound Synthesis for Large-Scale \nVirtual Environments.‖ IEEE Computer Graphics and Applications 27.1 (2007): 14–18. \n[Ren10] Ren, Z., H. Yeh, and Lin. ―Synthesizing Contact Sounds Between Textured Models.‖ \nIEEE Virtual Reality. 2010. \n[Singer06] Singer, Marq. ―Real-Time Sound Generation from Deformable Meshes.‖ Game \nProgramming Gems 6. Boston: Charles River Media, 2006. 541–547. \n[Sreng07] Sreng, J., F. Bergez, J. Legarrec, A. L ecuyer, and C. Andriot. ―Using an Event-\nBased Approach to Improve the Multimodal Rendering of 6dof Virtual Contact.‖ Proceedings \nof the 2007 ACM Symposium on Virtual Reality Software and Technology. 165–173. \n \nSection 7: General Purpose Computing on GPUs \nIntroduction \n",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": " \n \nUsing Heterogeneous Parallel Architectures with OpenCL \nPhysX GPU Rigid Bodies in Batman: Arkham Asylum \nFast GPU Fluid Simulation in PhysX \nIntroduction \nAdam Lake, Sr. Graphics Software Architect, Advanced Visual Computing, Intel \nadam_t_lake@yahoo.com \nThis is a new chapter for the Game Programming Gems series. For the past decade we‘ve \nseen academic literature filled with papers about using GPUs for various non-graphics tasks, \nin particular those that have a high ratio of compute-to-memory-bandwidth density. I \nbelieve we are at a new stage of this development, a time when this work migrates from the \nacademic literature to use in production software. This hypothesis is validated by the \nexciting developments in both hardware and industry APIs supporting development on \nGPUs. The new gold rush is throughput computing, and GPUs will be at the forefront. \nGraphics and gaming will play a critical role in putting this computational horsepower into \nthe hands of researchers, developers, and consumers at a cost that will create new \nindustries, ecosystems, and value for society as a whole. \nThe first article in this section discusses OpenCL, an industry standard for harnessing the \ncomputational horsepower of both CPUs and GPUs in today‘s heterogeneous computing \nenvironment. OpenCL contains APIs and a programming model that allows developers to \nexpress both task and data parallelism in their applications in a cross-platform API. The \nauthor summarizes the basic principles of OpenCL and uses examples to discuss how to use \nOpenCL in a performant way. I‘d expect to see this standard evolve over the next few years \nand to be leveraged for areas of both the game development process and in the game \nengines we ship in the not-too-distant future. \nWe‘ve also included two articles on PhysX, one focused on rigid bodies and the other on \nfluid simulation used in the game Batman: Arkham Asylum. Fluid simulation was used to \nadd smoke and fog, and the rigid body simulation was used to destroy the wall panels in the \nScarecrow levels. The fluid simulation was done by leveraging the CPU to implement a \nparticle system and simulate the fluid with Smoothed Particle Hydrodynamics (SPH). Rigid \nbody simulation is performed using a rigid body simulation implemented on the GPU. \nI hope these gems inspire game developers to think about other interesting ways to exploit \nthe computational resources available in the coming decade of computing! \n \n7.1. Using Heterogeneous Parallel Architectures with OpenCL \nUdeepta Bordoloi, Benedict R. Gaster, and Marc Romankewicz, Advanced Micro \nDevices \nOpen Compute Language (OpenCL) [Munshi09] is an open standard for data-and task-\nparallel programming on CPUs and GPUs. It is developed by the Khronos Group, which is \npart of the Compute Working Group and includes members from AMD, Apple, Electronic \nArts, IBM, Intel, NVIDIA, Qualcomm, and others. OpenCL is a standard to enable parallel \nprogramming on heterogeneous compute platforms. It can provide access to all the \nmachine‘s compute resources, including CPUs and GPUs. \n",
      "content_length": 3099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": " \n \nIn this gem, we give a primer on OpenCL and then introduce a set of general techniques for \nusing it to optimize games. Two examples are used: convolution and histogram. The former \nhighlights many of the techniques for optimizing nested loop data parallelism; the latter \nshowcases methods for accessing memory effectively. \nConvolution is a technique in the field of image processing; it calculates an output pixel as a \nfunction of its corresponding input pixel and the values of the neighboring pixels. This \ntechnique, however, is expensive to compute. Its run time is proportional to the width and \nheight of the image. If W and H are the width and height, respectively, the run time is O(W2 \nH2). The highly data-parallel nature of convolution makes it an excellent candidate for \nleveraging OpenCL optimizations. \nIn game programming, histograms improve the dynamic range of rendered frames through \ntone mapping. Fast on-GPU histogram computation is essential, since the image data \noriginates on the GPU. Historically, histogram computation has been difficult to do on GPU, \nas typical algorithms become limited by the scatter (random write-access) performance. \nHowever, with the advent of large on-chip, user-controlled memories and related atomic \noperations, performance can be dramatically improved. \nEfficient implementations for convolution and histogram calculations broaden the set of \npossibilities for game developers to exploit emerging heterogeneous parallel architectures; \nboth PCs and game consoles have many-core CPUs and GPUs. They also highlight the \ndifficulties that must be overcome to achieve anything like peak performance. \nIn this gem, we discuss a step-by-step optimization toolkit for game developers using \nOpenCL. These optimizations can be used together or individually, but their overall effect \ndepends on the application; there is no magic wand for an algorithm with little parallelism \nor a dataset so small that there is little chance to amortize the associated run-time cost. \nOpenCL Primer \nOpenCL is based on the notion of a host API, which consists of a platform and run-time \nlayer and a C-like language (OpenCL C) for programming compute devices; these devices \ncan range from CPUs to GPUs and other kinds of accelerators. Figure 7.1.1 illustrates this \nmodel, with queues of commands, reading/writing data, and executing kernels for specific \ndevices. The overall system is called a platform. There can be any number of platforms from \ndifferent vendors within a particular system, but only devices within a single platform can \nshare data within what OpenCL calls a context. \nFigure 7.1.1. OpenCL host/device architecture. \n \n",
      "content_length": 2684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": " \n \n \nThe devices are capable of running data- and task-parallel work; a kernel can be executed \nas a function of multidimensional domains of indices. Each element is called a work-item; \nthe total number of indices is defined as the global work-size. The global work-size can be \ndivided into sub-domains, called work-groups, and individual work-items within a group can \ncommunicate through global or locally shared memory. Work-items are synchronized \nthrough barrier or fence operations. Figure 7.1.1 is a representation of the host/device \narchitecture with a single platform, consisting of a GPU and a CPU. \nGiven an enumeration of platforms, we choose one, select a device or devices to create a \ncontext, allocate memory, create device-specific command queues used to submit work to a \nspecific device, and perform computations. Essentially, the platform layer is the gateway to \naccessing specific devices. Given these devices and a corresponding context, the application \nis unlikely to have to refer to the platform layer again. It is the context that drives \ncommunication with, and between, specific devices. A context generally is created from one \nor more devices. A context allows us to: \n \nCreate a command queue. \n \nCreate programs to run on one or more associated devices. \n \nCreate kernels within those programs. \n \nAllocate memory buffers or images, either on the host or on the device(s). (Memory \ncan be copied between the host and device.) \n \nWrite data to the device. \n \nSubmit the kernel (with appropriate arguments) to the command queue for \nexecution. \n \nRead data back to the host from the device. \nThe relationship between context(s), device(s), buffer(s), program(s), kernel(s), and \ncommand queue(s) is best seen by looking at sample code. The following program adds the \nelements of two input buffers, a and b, and stores the results in a single output buffer, o. \nWhile trivial in its application, the sample program is the foundational pattern of all OpenCL \nprograms. \n  #include <CL/cl.hpp> \n  #include <iostream> \n  static char kernelSourceCode[] = \n  \"__kernel void\\n\" \n  \"hello(__global int * inA, __global int * inB, __global int * \nout)\\n\" \n  \"{\\n\" \n  \"    size_t i = get_global_id(0);\\n\" \n  \"    out[i] = inA[i] + inB[i];\\n\" \n  \"}\\n\"; \n  int main(void) { \n      int a[10] = {1,2,3,4,5,6,7,8,9,10}; \n      int b[10] = {1,2,3,4,5,6,7,8,9,10}; \n      int o[10] = {0,0,0,0,0,0,0,0,0,0}; \n      cl::Context context(CL_DEVICE_TYPE_ALL); \n      std::vector<cl::Device> devices = \ncontext.getInfo<CL_CONTEXT_DEVICES>(); \n      cl::CommandQueue queue(context, devices[0]); \n      cl::Program::Sources sources(1, \nstd::make_pair(kernelSourceCode, 0)); \n      cl::Program program(context, sources); \n      program.build(devices); \n",
      "content_length": 2763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": " \n \n      cl::Kernel kernel(program, \"hello\"); \n      cl::Buffer inA(context,CL_MEM_READ_ONLY,10 * sizeof(int)); \n      cl::Buffer inB(context,CL_MEM_READ_ONLY,10 * sizeof(int)); \n      cl::Buffer out(context,CL_MEM_WRITE_ONLY,10 * \nsizeof(int)); \n      queue.enqueueWriteBuffer(inA,CL_TRUE,0,10 * \nsizeof(int),a); \n      queue.enqueueWriteBuffer(inB,CL_TRUE,0,10 * \nsizeof(int),b); \n      kernel.setArg(0,inA); kernel.setArg(1,inB); \nkernel.setArg(2,out); \n      queue.enqueueNDRangeKernel( \n      kernel, cl::NullRange, cl::NDRange(10), cl::NDRange(2) \n  ); \n  queue.enqueueReadBuffer(out,CL_TRUE,0,10 * sizeof(int),o); \n  std::cout << \"{\" ; \n  for (int i = 0; i < 10; i++) { \n       std::cout << o[i] << (i!=9 ? \", \" : \"\") ; \n  } \n  std::cout << \"}\" << std::endl; \n \n} \n \nBecause it is not possible to provide a full introduction to OpenCL in the limited amount of \nspace allocated to this gem, we advise readers new to OpenCL to read through the \nreferences given at the end of the article. For example, many introductory samples are \nshipped with particular implementations, and readers might also like to work though a \ntutorial such as ―hello world‖ [Gaster09]. \n \nTips for Optimizing OpenCL C Kernels \nTwo-dimensional convolution is used to illustrate techniques that can be used to optimize \nOpenCL kernels. \nConvolution Kernel \nThe OpenCL C kernel for convolution is given below. It is almost a replica of a corresponding \nC code for convolution; the only difference is that the C code uses two for loops that \niterate over the output image to initialize the variables xOut and yOut, instead of using \nthe get_global_id call. The output image dimensions are width by height, the input \nimage width is inWidth (equals width+filterWidth-1), and the input height equals \n(height+filterWidth-1). \n__kernel void Convolve(__global float * input, \n                      __constant float * filter, __global float \n* output, \n                      int inWidth, int width, int height, int \nfilterWidth) \n{ \n",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": " \n \n    int yOut = get_global_id(1);//for (int yOut = 0; yOut < \nheight; yOut++) \n    int xOut = get_global_id(0);//for (int xOut = 0; xOut < \nwidth; xOut++) \n    int xInTopLeft = xOut; int yInTopLeft = yOut; \n    float sum = 0; \n    for (int r = 0; r < filterWidth; r++) { \n        int idxFtmp = r * filterWidth; \n        int yIn = yInTopLeft + r; \n        int idxIntmp = yIn * inWidth + xInTopLeft; \n        for (int c = 0; c < filterWidth; c++) \n            sum += filter[idxFtmp+c]*input[idxIntmp+c]; \n    } //for (int r = 0... \n    int idxOut = yOut * width + xOut; \n    output[idxOut] = sum; \n} \n \nFigure 7.1.2 shows the computation time for an output image of size 8192×8192. The test \ncomputer is an AMD Phenom X4 9950 Black Edition with 8 GB RAM. For a filter width of 2, \nthe input image size is 8193×8193; for a filter of width 32, the input image is 8223×8223. \nFor each pixel, the loop runs for (filterWidth)2 times. The computation time increases \napproximately as a function of the square of the filter width. It takes about 14.54 seconds \nfor a 20×20 filter and 3.73 seconds for a 10×10 filter. We first consider unrolling loops to \nimprove performance of this workload. \nFigure 7.1.2. Computation time for various filter widths for an 8192×8192 output \nimage \n \n \n \nLoop Unrolling \nHaving loops in the code comes at a performance cost. For example, consider a 32×32 \nfilter. For each pixel in the output image, the statements in the innermost loop are run \n32×32 = 1024 times. This cost is negligible for small filters, but it becomes significant as \nthe filter width increases. The solution is to reduce the loop count. \n",
      "content_length": 1639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": " \n \nThe following kernel has four iterations of the innermost loop unrolled. A second loop is \nadded to handle the remainder of iterations when filter width is not an even multiple of four. \n__kernel void ConvolveUnroll(...) \n{ \n \n    ... \n    for (int r = 0; r < filterWidth; r++){ \n        ... \n        int c = 0; \n        while (c <= filterWidth-4) { \n            sum += filter[idxFtmp+c] *input[idxIntmp+c]; \n            sum += filter[idxFtmp+c+1]*input[idxIntmp+c+1]; \n            sum += filter[idxFtmp+c+2]*input[idxIntmp+c+2]; \n            sum += filter[idxFtmp+c+3]*input[idxIntmp+c+3]; \n            c += 4; \n        } \n        for (int c1 = c; c1 < filterWidth; c1++) \n            sum += filter[idxFtmp+c1]*input[idxIntmp+c1]; \n    } //for (int r = 0... \n    ... \n} \n \nFigure 7.1.3 shows the results of the unrolled kernel. For larger filters, unrolling helps \nimprove speed by as much as 20 percent. The sawtooth kind of behavior that we see in the \ngraph is due to the iterations that are left over after unrolling and is easily handled by \nunrolling the second inner loop (and substituting it with an if-else statement). \nFigure 7.1.3. Loop unrolling. Unrolling the inner loop (Unroll) and the second \ninner loop (Unroll_If). \n \n \n \nInvariants \nFor the aforementioned kernels, filter widths were passed as an argument. Consider an \napplication where the filter size is constant (for example, 5×5). In this case, the inner loop \ncan be unrolled five times, and the loop condition can be removed. \n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": " \n \n__kernel void DefConvolve(...) \n{ \n    ... \n    for (int r = 0; r < FILTER_WIDTH; r++) { \n        int idxFtmp = r * FILTER_WIDTH; \n        ... \n        for (int c = 0; c < FILTER_WIDTH; c++) \n    ... \n} \n \nAs the filter width is static, FILTER_WIDTH, it can be defined when building the OpenCL \nprogram. The following code shows how to pass in the value of the invariant. \nstd::string sourceStr = FileToString(kernelFileName); \ncl::Program::Sources sources(1, \nstd::make_pair(sourceStr.c_str(), \n                                               \nsourceStr.length())); \nprogram = cl::Program(context, sources); \nchar options[128]; \nsprintf(options, ―-DFILTER_WIDTH=%d‖, param.filterWidth); \nprogram.build(devices, options); \ncl::Kernel kernel = cl::Kernel(program, kernelName.c_str()); \n \nFigure 7.1.4 shows that defining the filter width as an invariant helps the DefConvolve \nkernel gain about 20 percent performance over the Convolve kernel, particularly for small \nkernel sizes. \nFigure 7.1.4. Using the filter width as an invariant in the Convolve and the \nUnroll_If kernels. \n \n \n \nVectorization \nSince the inner loop of the unrolled kernel has four products and four additions, it is \npossible to use one vector (SSE or GPU) packed-multiply and one packed-add to achieve the \nsame results. But how do we use vectors in an OpenCL kernel? AMD‘s OpenCL \nimplementation will try to use ―packed-arithmetic‖ instructions whenever it encounters a \n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": " \n \nvector data type in the kernel. The following kernel body uses the vector type float4. \nNote the additional loop at the end to handle any remaining iterations when filter width is \nnot an even multiple of four. \n__kernel void ConvolveFloat4(...) { \n    ... \n    float4 sum4 = 0; \n    for (int r = 0; r < filterWidth; r++) { \n        ... \n        int c = 0; int c4 = 0; \n        while (c <= filterWidth-4) { \n            float4 filter4 = vload4(c4, filter+idxFtmp); \n            float4 in4     = vload4(c4, input +idxIntmp); \n            sum4 += in4 * filter4; \n            c += 4; c4++; \n        } \n        for (int c1 = c; c1 < filterWidth; c1++) \n            sum4.x += filter[idxFtmp+c1]*input[idxIntmp+c1]; \n    } //for (int r = 0... \n    int idxOut = yOut * width + xOut; \n    output[idxOut] = sum4.x + sum4.y + sum4.z + sum4.w; \n} \n \nThe second inner loop can be unrolled (kernel ConvolveFloat4_If), and invariants can \nbe used for further speedups (kernels DefFloat4 and DefFloat4_If). Figure 7.1.5 \nshows the results for the vectorized kernel. \nFigure 7.1.5. The effect of vectorization. ConvolveFloat4 is the float4 version \nof Convolve; ConvolveFloat4_If has the second inner loop unrolled. \nDefFloat4 and DefFloat4_If use the filter width as an invariant. \n \n \nOptimizing Memory-Bound OpenCL Kernels \nSo far, our optimizations have focused on how to maximize ALU throughput. In the \nfollowing section, we use a histogram calculation to address techniques for optimizing \nkernels limited by memory performance. \nFor simplicity, we assume that a histogram consists of 32-bit-per-pixel images with 256 32-\nbit bins. The algorithm parallelizes the computation over a number of workgroups, each of \nwhich uses a number of sub-histograms stored in on-chip __local memory. \n",
      "content_length": 1781,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": " \n \nNumber of Work-Groups \nWhen choosing the optimal number of work-items and work-groups, some constraints follow \nautomatically from the algorithm, the device architecture, and the size of __local \nmemory. \n__local memory is shared within a single work-group. For the histogram algorithm, it is \ndesirable to have fewer, larger work-groups. This allows many successive threads to \ncontribute to the same __local memory area. Conversely, using more and smaller work-\ngroups would require costly flushing of __local sub-histograms to __global memory \nmore frequently, since the lifetime of a work-group is shorter. \nTo maximize the use of __local memory, the number of work-groups should be as close \nas possible to the number of compute units. Each compute unit typically has a dedicated \nblock of __local memory, and using as many of them concurrently as possible is \nadvantageous. \nThe actual number of compute units, typically in the tens per GPU, can be queried at run \ntime using: \nclGetDeviceInfo( ..., CL_DEVICE_MAX_COMPUTE_UNITS, ... ); \n \n \nWork-Group Size \nA work-group should be at least as big as the basic hardware unit for scheduling. On AMD \nGPUs, a wavefront is a hardware unit of work-items executing concurrently on a given \ncompute unit. OpenCL work-groups are executed as a collection of wavefronts. Wavefront \nsize cannot be queried via OpenCL—on current high-end AMD GPUs, it is 64 work-items. \nAlso, a total of two wavefronts can be scheduled to run on a single compute unit at a time. \nThis sets an absolute minimum for the total work-item number: Multiply two times the \nwavefront size with the number of work-groups. An integer multiple of that number is often \nadvisable, as it will allow more flexibility for run-time scheduling—hiding memory latency, \nallocating register use, and so on. On the other hand, the upper limit of the number of \nthreads is given by the maximum work-group size times the chosen number of work-\ngroups. The maximum work-group size can be queried using: \nclGetDeviceInfo( ..., CL_DEVICE_MAX_WORK_GROUP_SIZE, ... ); \n \nWithin this range, fine-tuning remains an algorithm- and kernel-specific matter. Kernels can \nrequest a particular set of resources. For example, they can request the number of \nregisters, the size of the __local buffer, and the number of instructions. These \nparameters influence how the kernel is scheduled on the GPU at run time. Possible tuning \nknobs include the number of groups, the group size, and the number of work-items. The \nupcoming section on optimal read patterns gives an example. \n__global Read/Write Access Patterns \nFor any implementation of either a parallel or a serial histogram, very few arithmetic \noperations are required. This implies that the first limiting factor for histogram performance \nis the read bandwidth from __global memory, as the input image originates from there. \nFortunately, because the histogram read is order-independent, it is possible to optimize \neach work-item‘s read pattern to that which is best supported by the GPU or CPU. \n",
      "content_length": 3052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": " \n \nThe CPU/GPU memory subsystem and the compiler tool chain are optimized for 128-bit \nquantities—the common shader pixel size of four 32-bit floats. On the GPU, multiples of \nthese quantities are advantageous to allow the memory subsystem to take advantage of the \nwide (typically hundreds of bits) data path to memory. Ideally, all simultaneously executing \nwork-items read adjacent 128-bit quantities, so that these can be combined to larger \nquantities. \nThis leads to the following access pattern: Each thread reads unsigned integer quantities \npacked into four-wide vectors (uint4) quantities, starting at its global work-item index and \ncontinuing with a stride equal to the number of threads, until it reaches the end. For a total \nwork-item number of NT, the resulting pattern is: \nuint4 addr 0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \nThread \n0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \nuint4 addr NT NT+1 NT+2 NT+3 ... 2NT-4 2NT-3 2NT-2 2NT-1 \nThread \n0 \n1 \n2 \n3 \n... NT-4 \nNT-3 \nNT-2 \nNT-1 \n \nAs many of the work-items execute this step at the same time, it results in a large number \nof simultaneous, adjacent read requests, which can be combined into optimal hardware \nunits by the memory subsystem. For example, for NT = 8192, the addresses sent to the \nmemory subsystem result in a tightly aligned pattern: \nThread \n0 1 2 3 4 ... 8189 8190 8192 \nuint4 addr 0 1 2 3 4 ... 8189 8190 8192 \n \nNote that the straightforward choice—a serial access pattern from within each work-item—\nresults in widely dispersed concurrent accesses when issued across all active threads; this is \na worst-case scenario for the GPU. Here, NI is the number of uint4 items per thread: \nuint4 addr 0 1 \n2 \n3 \n... NI-4 \nNI-3 \nNI-2 \nNI-1 \nThread \n0 0 \n0 \n0 \n... 0 \n0 \n0 \n0 \nuint4 addr NI NI+1 NI+2 NI+3 ... 2NI-4 2NI-3 2NI-2 2NI-1 \nThread \n1 1 \n1 \n1 \n... 1 \n1 \n1 \n1 \n \nAs an example, for NT = 8192 and NI = 128, the addresses sent to the memory subsystem \nare widely dispersed: \n",
      "content_length": 1960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": " \n \nThread \n0 1 \n2 \n3 \n4 \n5 \n6 \n7 \n... \nuint4 addr 0 128 256 320 512 640 768 896 ... \n \nOn the CPU, this pattern is nearly ideal, particularly when running only a few CPU work-\nitems with large numbers of NI. Each work-item reads sequentially through its own portion \nof the dataset; this allows prefetching and per-core cache line reads to be most effective. As \na rule of thumb, it is desirable to use per-thread column-major access on the GPU and per-\nthread row-major access on the CPU. Row-major and column-major access can be \nimplemented in a single kernel through different strides—a kernel can switch at run time \nwithout cost. \nBoth patterns are shown in the following code example, where nItemsPerThread is the \noverall number of uint4 elements in the input buffer, divided by the number of threads: \n__kernel void singleBinHistogram (__global uint4 *Image, \n                                  __global uint *Histogram, \n                                  uint nItemsPerThread ) \n{ \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; uint bin = 0; \n    uint val = SOME_PIXEL_VAL; \n#ifdef GPU_PEAK_READ_PERF \n    // with stride, fast \n    for( i=0, idx=get_global_id(0); i<nItemsPerThread; i++, \nidx+=nWItems) { \n#else \n    // serial, slow on GPU, fast on CPU \n    for( i=0, idx=0; i<nItemsPerThread; i++, idx+=1 ) { \n#endif \n        if( Image[idx].x == val ) bin++; \n        if( Image[idx].y == val ) bin++; \n        if( Image[idx].z == val ) bin++; \n        if( Image[idx].w == val ) bin++; \n   } \n \n   Histogram[id] = bin; \n} \n \nWhen executed with at least 8192 work-items and a work-group size of 64, this kernel \nreaches near hardware peak performance (on AMD GPUs), as shown in Figure 7.1.6. \nFigure 7.1.6. Bandwidth per number of work-items. \n",
      "content_length": 1797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": " \n \n \n \n \nGetting around the Scatter Bottleneck \nOn the CPU, reads and randomly indexed writes can be performed at roughly the same \nspeed. The GPU, on the other hand, excels when massively parallel writes can be coalesced \ninto large blocks, just as was shown for reads in the previous section. Also, the total read-\nmodify-write latency to __global memory can be a contributor. For a straightforward \nparallel histogram implementation, where each thread creates a sub-histogram in \n__global memory, as in the following code example, scatter turns out to be a bottleneck. \n__kernel void multiBinHistogram ( __global uint4 *Image, \n                                  __global uint *Histogram, \n                                  uint nItemsPerThread ) { \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; \n \n    for( i=0, idx=get_global_id(0); i<nItemsPerThread; i++, \nidx+=nWItems ) \n    { \n        Histogram[ id * NBINS + Image[idx].x ]++; \n        Histogram[ id * NBINS + Image[idx].y ]++; \n        Histogram[ id * NBINS + Image[idx].z ]++; \n        Histogram[ id * NBINS + Image[idx].w ]++; \n    } \n   ... \n} \n \n \nOptimizing __local Memory Access \nA powerful way to address the GPU scatter bottleneck is to direct scatter into __local \nmemory. This memory is available to all work-items in a work-group, and its lifetime is that \nof the group. \n",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": " \n \nIt is fast for the following reasons: \n \nIt typically is on-chip (cf. clGetDeviceInfo(CL_DEVICE_LOCAL_MEM_TYPE)) \nand very low latency. In essence, it is a user-manageable cache. \n \nIt supports a large number of concurrent data paths: one for each compute unit and \nmore for multiple banks on each compute unit, resulting in an aggregate bandwidth \nup to an order of magnitude higher than __global memory. \n \nIt allows the use of hardware atomics, so that random access RMW cycles can be \nperformed without having to worry about expensive, explicit synchronization \nbetween the global set of threads. \nThe __local memory size per work-group (and by extension, per compute unit) is given \nby: \nclGetDeviceInfo( ..., CL_DEVICE_LOCAL_MEM_SIZE, ... ); \n \nThis gives a total __local size per GPU device as CL_MAX_COMPUTE_UNITS * \nCL_DEVICE_LOCAL_MEM_SIZE. \nA straightforward implementation for the histogram algorithm would be to simply have a \nsub-histogram instance per __local memory and to let all threads in that group \ncontribute to that single instance, as shown in the following example code: \n__kernel void histogramLocal( __global uint4 *Image, \n                              __global uint *Histogram, \n                              __local uint *subhists, \n                              uint nItemsPerThread ) { \n    uint id = get_global_id(0); \n    uint nWItems = get_global_size(0); \n    uint i, idx; uint4 temp; \n    // initialize __local memory \n    ... \n    // scatter loop \n    for( i=0, idx=tid; i<nItemsPerThread; i++, idx+=nWItems ) { \n       temp = Image[idx]; \n \n       atom_inc( subhists + temp.x ); \n       atom_inc( subhists + temp.y ); \n       atom_inc( subhists + temp.z ); \n       atom_inc( subhists + temp.w ); \n    } \n    barrier( CLK_LOCAL_MEM_FENCE ); \n    // reduce sub-histogram and write out to __global for \nfurther reduction \n    ... \n} \n \nThis approach performs very well for randomized input data, since the collective scatter \noperations from all threads are distributed over the __local banks. Unfortunately, real \napplication images can consist of large swaths of identical pixel values—a black background, \nfor example. The resulting simultaneous atomic read-modify-write access by many threads \ninto a single bin quickly leads to severe contention and loss of performance. \n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": " \n \nThis is where __local banks become effective as the hardware provides multiple banks \nthat can be accessed at the same time. On AMD GPUs, addresses of adjacent 32-bit words \nmap to adjacent banks. This pattern repeats every NB addresses, where NB is the number of \navailable banks. \nFor NB=32, the __local bank layout looks like this: \nuint addr 0 \n1 \n2 \n3 \n... 28 29 30 31 \nbank \n0 \n1 \n2 \n3 \n... 28 29 30 31 \nuint addr 32 33 34 34 ... 60 61 62 63 \nbank \n0 \n1 \n2 \n3 \n... 28 29 30 31 \n \nThe goal is to map the set of running threads to as many banks as possible. In the \nhistogram case, an easy way to do this is to maintain NB copies for each bin and use the \nlocal work-item ID to steer simultaneous writes to the same logical bin, but into separate \nbanks. \nThe resulting bins are then combined after the read-scatter phase of the kernel. The optimal \nnumber of copies for each bin can be experimentally determined, but it is less than or equal \nto NB. The value of NB cannot be queried via OpenCL and is obtained from the hardware \nspec. For recent AMD GPUs, it is 32. \n#define NBANKS 32 \n// subhists is of size: (number of histogram bins) * NBANKS \n__kernel void histogramKernel( __global uint4 *Image, \n                               __global uint *Histogram, \n                               __local uint *subhists, \n                               uint nItemsPerWI ) { \n     uint id = get_global_id(0); \n     uint ltid = get_local_id(0); \n     uint nWItems = get_global_size(0); \n \n     uint i, idx; uint4 temp; \n \n     uint4 spread = (uint4) (NBANKS, NBANKS, NBANKS, NBANKS); \n     uint4 offset = (uint4)(ltid, ltid, ltid, ltid); \n     offset &= (uint4)(NBANKS-1, NBANKS-1, NBANKS-1, NBANKS-1); \n \n     // initialize __local memory \n     ... \n     // scatter loop \n     for( i=0, idx=tid; i<nItemsPerWI; i++, idx+=nWItems ) { \n        temp = Image[idx] * spread + offset; \n        atom_inc( subhists + temp.x ); \n        atom_inc( subhists + temp.y ); \n        atom_inc( subhists + temp.z ); \n        atom_inc( subhists + temp.w ); \n     } \n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": " \n \n     barrier( CLK_LOCAL_MEM_FENCE ); \n     // reduce sub-histograms, and write out to __global for \nfurther reduction \n     ... \n} \n \nBy spreading out read-modify-write accesses in this manner, contention is greatly reduced, \nif not removed. The resulting performance gain for a test image consisting of identical pixels \nis an order of magnitude. \n \nOpenCL Command Profiling \nIn the previous sections, a selection of techniques for optimizing OpenCL programs have \nbeen described. But how should we time changes made for performance? In general, \nWindows‘ performance counters work well, and for more detailed profiles, tools such as \nAMD‘s CodeAnalyst Performance Analyzer or GPU PerfStudio help. In addition to these, \nOpenCL provides the ability to query profiling information for specific commands executed \nvia a command queue on a device. \nBy default profiling is not enabled, but it can be explicitly requested at the creation of a \ncommand queue, with the following flag: \nCL_QUEUE_PROFILING_ENABLE \n \nAlternatively, profiling can be enabled for a command queue with the following API call: \nqueue.setProperty(CL_QUEUE_PROFILING_ENABLE, CL_TRUE); \n \nGenerally, commands are enqueued into a queue asynchronously, and the developer must \nuse events to keep track of a command‘s status, as well as to enforce dependencies. Events \nprovide a gateway to a command‘s history: They contain information detailing when the \ncorresponding command was placed in the queue, when it was submitted to the device, and \nwhen it started and ended execution. Access to an event‘s profiling information is through \nthe following API call: \nevent.getProfilingInfo<cl_profiling_info>(); \n \nValid values of the enumeration cl_profiling_info are CL_PROFILING_COMMAND_ \nQUEUED, CL_PROFILING_COMMAND_SUBMIT, CL_PROFILING_COMMAND_START, \nCL_PROFILING_ COMMAND_END. \n \nConclusion \nOpenCL is a technology for describing data- and task-parallel programs that can use all \ncompute resources, including CPUs and GPUs. While it is straightforward to get applications \nto run using OpenCL, it is not always simple to get the expected performance. In this \n",
      "content_length": 2136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": " \n \narticle, we have outlined a number of techniques that can be used to optimize OpenCL \nprograms. These optimizations are not a guarantee for performance; instead, they should \nbe seen as a toolkit to be considered when optimizing an application. \n \nReferences \n[Gaster09] Gaster, Benedict R. ―Introductory Tutorial to OpenCL.‖ August 2009. ATi Stream \nTechnical publications. n.d. \n<http://developer.amd.com/gpu/ATIStreamSDK/pages/TutorialOpenCL.aspx>. \n[Munshi09] Munshi, Aaftab, ed. ―The OpenCL Specification.‖ Version 1.0. Khronos OpenCL \nWorking Group, August 2009. \n \n7.2. PhysX GPU Rigid Bodies in Batman: Arkham Asylum \nRichard Tonge, NVIDIA Corporation, rtonge@nvidia.com \nBen Wyatt and Ben Nicholson, Rocksteady Studios \nOne of the goals of the PC version of Batman: Arkham Asylum was to improve the realism \nand dynamism of the game environment by adding GPU-based physics effects, including \ncloth, fluids, and rigid body–based destruction. With fluids, we were able to add volumetric \nsmoke and fog, which moves around players and other characters as they progress through \nit. With cloth, we were able to bring to life the piles of paper littering the admin areas of \nArkham Asylum. These piles appear as static triangle meshes on other platforms. For rigid \nbody dynamics, we wanted to provide large-scale destruction—for example, to demolish \nlarge concrete walls in the outdoor areas of the game. To believably break up a large wall \nrequires the simulation of thousands of debris pieces colliding with the high triangle count \nenvironment of Batman. To achieve this in addition to running all the gameplay physics, \ngraphics, and other game code on the CPU at high frame rates, it was clear that the effects \nwould have to be run on the GPU. \nFor GPU acceleration of cloth and fluids, we were able to use NVIDIA PhysX, which already \nincludes CUDA implementations of those features. For rigid body simulation, NVIDIA \ndeveloped a GPU rigid body engine (GRB) implementing the subset of PhysX features we \nneeded for Batman. Using GRB, physics calculations run up to three times faster than on \nthe CPU, enabling the large-scale destruction required. This gem describes the design and \nimplementation of GRB and its use in Batman. \nBatman made use of NVIDIA‘s APEX destruction module to author and simulate destructible \ngame objects. APEX is a layer above PhysX that allows specific effects to be authored and \ninserted into games easily. APEX destruction allows the artist to specify how an object \nbreaks, with APEX doing the dirty work of creating, splitting, and destroying PhysX rigid \nbodies when the object is shot at, blown up, or swept away by a tornado. \nIn Batman‘s Scarecrow levels, large wall panels needed to be destroyed, with the debris \nswept up into a tornado centered on the Scarecrow. Wall panels were imported into the \nAPEX destruction tool, where they were broken into pieces and placed into the game using \nUnreal Editor. With the content pipeline in place, our task was to make a GPU rigid body \nengine to accelerate the subset of PhysX rigid body features used by APEX destruction. \nFigure 7.2.1. Destructible wall section being placed in Unreal Editor. \n",
      "content_length": 3200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": " \n \n \n \nRequirements \nWe identified the following requirements: \n \nPerformance. The system must be able to support thousands of rigid bodies \ncolliding and interacting with the game environment in real time. \n \nParallelism. The system must be designed to use thousands of GPU threads to get \ngood GPU performance and allow scaling to future GPUs. \n \nAPI. The API should be similar to PhysX rigid body and support all features and API \ncalls made by APEX destruction. \n \nInteraction with PhysX. The game will use PhysX (CPU) rigid bodies for game-play \nobjects (for example, Batman). GRB should support one-way interaction with PhysX \nbodies. (PhysX bodies should move GRBs but not vice versa.) \n \nShapes. GRB should support the PhysX shapes used in Batman—that is, triangle \nmeshes, convex meshes, spheres, capsules, and boxes. \n \nMulti-threading. The simulation should occur asynchronously to the game thread to \nallow unused GPU resources to be utilized whenever they occur in the frame and \navoid having the game thread wait for GRB to complete. \n \nBuffering. The game should be free to change the scene while the simulation is \nrunning without disrupting the simulation. \n \nStreaming. Batman uses a single scene for the whole game and streams in adjacent \nlevels as the player moves around. This means that additions to the scene have to \nbe fast. \n \nAuthoring. It should be easy to author and tune content. Artists should be able to \nspecify which game objects collide with GRBs and which don‘t. \n \nShape Representation \n",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": " \n \nThe static environment is represented by the original geometry used in the game—triangle \nmeshes, convex meshes, boxes, and so on. Dynamic objects have a sphere-based \nrepresentation made by voxelizing the object geometry. Color Plates 13 and 14 show how \nshapes in Batman are represented by voxel-generated spheres in GRB. Although spheres \nare used to represent the collision geometry of dynamic objects, the dynamics are still done \non rigid body coordinates. \n \nDynamics Algorithm \nOne of the most important requirements was that the algorithm be highly data parallel to \nallow efficient GPU implementation. Initially, the dynamics method from [Harada07] seemed \nlike a good choice. In this method, spring and damper forces are applied at contact points \nto prevent penetration. The forces are integrated explicitly, which means that they can be \ncalculated in parallel. In a sense, this method is maximally parallel, because there are as \nmany threads as contacts. \nUnfortunately, like other penalty methods, this method is hard to tune. In this method, the \nspring stiffness must be set high enough so that penetration is avoided for all incoming \nvelocities. Penalizing penetration with springs injects unwanted energy into the system, so a \ndamping parameter has to be tuned to dissipate the extra energy. Although tuning these \nparameters is reasonable for simple collisions—for example, between two spheres—it \nquickly becomes intractable for multiple simultaneous collisions each having multiple \ncontacts. Also, because the method is an explicit method, the stiffness dictates a maximum \ntime-step size for which the simulation is stable. [Harmon09] addresses some of these \nproblems, but the method is far from real time. \nTo avoid these problems, we decided to use an implicit constraint-based method similar to \nthat used in PhysX. In this method, instead of producing a spring and damper per contact \npoint, the collision detection code produces a set of rules called constraints. These \nconstraints are fed into a constraint solver, which iteratively finds a new set of velocities for \nthe bodies. Its goal is to find velocities that won‘t move the bodies into penetration (violate \nthe constraints) in the current time step. Compared to [Harada07], implementing this \nmethod has the additional complexity of parallelizing a constraint solver. The parallelization \nwill be described in a later section. \n \nPipeline \nThe following is a high-level description of the pipeline. \n \nTransform spheres into world space using body transformation matrices. \n \nFind sphere-sphere overlaps, producing contacts. \n \nFind sphere-mesh overlaps, producing contacts. \n \nEvaluate force fields, such as wind, explosions, tornados, and so on. \n \nUpdate body velocities by applying force fields and gravity. \n \nGenerate constraints from contacts. \n \nSolve constraints producing modified velocities and position corrections. \n \nUpdate body positions using position corrections. \n \nTransform Spheres into World Space \n",
      "content_length": 3017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": " \n \nThe spheres are rigidly attached to the rigid body reference frame. For this reason, in each \nframe the spheres have to be transformed from their local frame to world space. For a \nparticle with position r belonging to a rigid body with transformation matrix M, we calculate \nthe world space position as follows: \nrworld = Mr \n \nSphere-Sphere Collision Detection \nThe dynamic objects are represented by grids of spheres, so we use sphere-sphere tests to \ndetermine the points of contact between them. We use the same size spheres for all the \ndynamic objects in the scene, so the sphere radius is a global parameter ζ. A pair of \nspheres separated by vector rij are overlapping if |rij| ≤2ζ. In this case, the contact normal \nnc, position pc, and separation θc are defined as follows. \n \nIn the case, where the two spheres are in almost the same position (|rij| ≤ ε, where ε is a \nsmall positive number), we use an arbitrary contact normal nc = (1,0,0). Note that when \nthe spheres are penetrating, the ―separation‖ is negative. \n \nSphere-Mesh Collision Detection \nThe static environment is mainly composed of non-convex triangle meshes, so we also have \nto test for sphere-triangle contact. For each sphere and potentially colliding triangle, we first \nneed to check whether the center of the sphere is already behind the triangle. Ignoring \ncontact in this case helps avoid the situation where some of a body‘s spheres are trapped \non one side of a triangle and some on the other, and it also simplifies the computation of \nthe separation. Given sphere center ps, triangle vertices (v0, v1, v2), and triangle normal nt, \nthe sphere center is behind the triangle if: \nnt.ps < nt.v0 \nIf a sphere and triangle survive this test, then we next need to calculate world coordinates \npt of the closest point on the triangle to the center of the sphere ps. For details on how to \ndo this, see [Ericson05]. We generate a contact for a sphere and triangle if |ps – pt| ≤ ζ. \nThe contact normal, position, and separation are given by the following: \nnc = (ps – pt)/|ps – pt| \npc = pt \nθc = |ps – pt| – ζ \nIn the case that the sphere center is on the triangle |ps – pt| ≤ ε, we set the contact normal \nnc to the triangle normal instead. \n",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": " \n \nWe use a uniform grid data structure to avoid having to test every sphere against every \ntriangle in the mesh. This is described later in this gem. \n \nEvaluate Force Fields \nForce fields are used in Batman to blow around debris and implement tornado effects. GRB \nsupports the same interface as PhysX for specifying and evaluating force fields. See \n[Nvidia08] for more details. \n \nCalculate Unconstrained Velocities \nFor a body with velocity v, we step the acceleration due to force fields af and gravity ag \nthrough time step h to get the unconstrained velocity v*. We also apply a global damping \nparameter η. \nv* = (1 – ηh)[v + h(af + ag)] \nEach body‘s inertia matrix I is calculated in its local frame, so we need to transform it to the \nworld frame as follows: \n \nwhere R is the orientation matrix of the body. \n \nGenerate Constraints \nFor each contact, the constraint solver needs a coordinate frame [b0 b1 b2] in which to \napply the constraint forces. We‘ll refer to these three vectors as constraint directions. The \nX-axis is aligned with the contact normal b0 = nc, and the other two axes are arbitrarily \nchosen orthogonal axes in which the friction forces will be applied. \nConsider a contact with position pc, normal nc, and separation θc. Let the position of the \ncontact relative to the center of masses of body a and body b be ra and rb, their final \n(constrained) velocities be \nand \n, their inertia be Ia and Ib, and their \nmasses be ma and mb. \nEach non-penetration constraint has the following form, known as the velocity Signorini \ncondition: \n \nHere, v0 is the component of relative velocity of the two bodies at the contact point along \nthe contact normal, and f0 is the magnitude of the impulse calculated by the constraint \nsolver to satisfy the constraint. The condition ensures that the bodies are either separating \nor resting. Additionally, if the bodies are separating, then the solver must not apply any \n",
      "content_length": 1941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": " \n \nimpulse; and if the bodies are resting, then the solver must apply a non-adhesive impulse. \nThe calculation of the friction impulses will be described later. \nThe constraint solver solves each constraint independently. To do this, it needs to calculate \n, the magnitude of the impulse to apply along direction bj needed to zero the \nrelative velocity along that direction, given initial relative velocity vj. The constant a is the \nreciprocal of the effective mass and is precomputed as follows: \n \nSolver \nThe task of the solver is as follows: Given a set of bodies with unconstrained velocities v* \nand a set of constraints, find impulses to apply to the bodies such that the resulting \nvelocities will (approximately) satisfy the constraints. As with most game physics engines, \nGRB uses an iterative projected Gauss-Seidel solver. Given an unlimited number of \niterations, the constraints will be solved exactly. In Batman, we terminate after four \niterations, so although the velocities may slightly violate the constraints, we can bound the \namount of time the solver will take. \nLet the number of constraints be m. If friction is used, then m is the number of contacts \nmultiplied by three. In this section, we‘ll assume that the constraint directions and all the \nother constraint parameters are concatenated into arrays of length m. The solver iterates \nover the constraints, solving each in isolation. \nSolver algorithm: \nZero f, the impulse accumulator for each constraint \nFor each iteration \nFor i = 0 to m \nCalculate the impulse required to prevent relative motion along \nconstraint direction i \nClamp the accumulated impulse to satisfy the Signorini or \nfriction constraint \nAdd impulse to impulse accumulator \nApply impulse to update the velocities of the constrained bodies \nCalculate Impulse Required to Prevent Relative Motion Along Constraint Direction i \nGiven \nand \n, the linear and angular velocities of the two bodies constrained \nby constraint i, and ra and rb, the position of the contact in the frames of body a and body \nb, the relative velocity along the constraint direction bi is given by the following: \n \n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": " \n \nIn other words, vrel is the velocity of the contact point on body a minus the velocity of the \ncontact point on body b projected onto the direction bi. We can now multiply by the inverse \neffective mass ai to get fd, the additional impulse required to satisfy the constraint in \nisolation: \nfd = aivrel \nClamp the Accumulated Impulse to Satisfy the Signorini Constraint \nConstraint i is either a non-penetration constraint or a friction constraint. For non-\npenetration constraints, we clamp to prevent adhesion as follows: \nfnew = max(0,fi + fd) \nFor friction constraints, we approximate the Coulomb friction cone with a pyramid aligned to \nthe friction constraint directions. This allows us to calculate the friction impulses separately. \nFor a friction constraint with friction coefficient μ, we look up fj, the impulse applied at the \ncorresponding non-penetration constraint, and then clamp to the friction pyramid as \nfollows: \nfnew = max( –μfj, min(fi + fd, μfj)) \nApply Impulse to Update the Velocities of the Constrained Bodies \nWe now apply the clamped impulse to the two bodies involved in the constraint. \n \n \nUpdate Positions \nThe solver calculates a new velocity for each body. To apply velocity (vlin, vang) to a body \nwith position p, orientation quaternion q, and time step h, we use the following: \n \nwhere a = vang/|vang| and θ = |hvang|. \n \nGPU Implementation \nAmdahl‘s law dictates that if you parallelize less than 100 percent of your code, your \nspeedup will be at best the reciprocal of the proportion not parallelized. For example, if 80 \n",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": " \n \npercent of the code is parallelized, the speedup will be no more than 5×, even though the \nGPU might be significantly more than five times faster than the CPU. So our initial aim was \nto port all stages of the GRB pipeline to the GPU. In the end, we ported all pipeline stages \nexcept for force field calculation to the GPU. The reason that we didn‘t port force field \ncalculation is that the PhysX API allows the user to write custom force evaluation kernels in \nC. Linking artist-generated kernels into the GRB CUDA code was not supported with the \navailable tool chain. An advantage to running almost all of the pipeline on the GPU is that \nmost of the data can be kept in the GPU‘s memory. This is an advantage because data \ntransfer between CPU memory and GPU memory introduces latency and unnecessary \nsynchronization with the CPU. \nThe pipeline stages can be divided into those that are easily parallelized and those that are \nnot. The stages in the first category are transform particles, find sphere-sphere contacts, \nfind sphere-mesh contacts, update velocities, generate constraints, and update body \npositions. For example, in transform particles, each GPU thread is assigned a single particle. \nThe calculation of each particle is independent, and as there are typically more than 10,000 \nparticles in Batman, the GPU is well utilized. Similarly, each thread is assigned one particle \nin the collision detection stages. In the sphere-sphere stage, each thread is responsible for \nfinding nearby particles; and in the sphere-triangle stage, each thread looks at nearby \ntriangles. Again, each thread can do this without synchronizing with any other. In order to \nconcatenate the contacts produced by each thread, we allocate memory for a fixed number \nof contacts per thread. We used four contacts per sphere in Batman. Once this fixed number \nof contacts has been generated, further contacts are discarded. Discarding contacts could \nresult in unwanted penetration, but in Batman this is rarely visible. Another way to do this \nwould be to use a scan (prefix sum) operation [Harris07] to allocate space for the contacts. \nHowever, this requires running the collision kernels twice—once to calculate the number of \ncontacts and again to fill in the contact data. For the easily parallelized stages, the CUDA \nport was very quick. In most cases it was as simple as removing an outer for loop, adding \nthe__device__ keyword, and compiling the body of the for loop with the CUDA \ncompiler. Most of the loop bodies compiled with little or no changes. \nThe only pipeline stage that is not easily parallelized is the constraint solver, and that will be \ndiscussed in one of the following sections. \n \nCUDA Voxelizer \nVoxelization is the conversion of an object into a volume representation, stored in a three-\ndimensional array of voxels. A voxel is the three-dimensional equivalent of a pixel, \nrepresenting a small cubical volume. Since GRB represents objects as a collection of \nspheres, we use voxelization to generate an approximate sphere representation of the \nconvex objects generated by APEX destruction. Each covered voxel generates a single \nsphere. \nThe voxelizer isn‘t strictly part of the pipeline because it is only run when dynamic objects \nare first loaded. However, Batman levels are streamed, so object loading has to be as fast \nas possible. For this reason we decided to use a GPU voxelizer. \nInitially, we implemented an OpenGL voxelizer, similar to that described in [Karabassi99]. \nHowever, this required creating a separate OpenGL context, and there was significant \noverhead in transferring data between OpenGL and CUDA, so we decided to write a CUDA \nimplementation instead. \nSince the shapes generated by APEX destruction were all convex meshes, this was \nconsiderably simpler than writing a general-purpose triangle mesh voxelizer. The basic \nalgorithm used was to convert each triangle in the convex mesh into plane equation form. \nThis way, it is simple to determine the distance of a point from any plane by performing a \n",
      "content_length": 4064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": " \n \nsimple dot product. The plane data is sent to the GPU using CUDA constant memory. We \nthen launch a kernel with one thread per voxel. Each thread loops over all the planes, \ncalculating the distance to the plane and keeping track of the minimum distance and sign. If \nthe point is inside all the planes, then we know it is inside the object. By discarding points \nmore than a maximum distance away from the nearest plane, we can modify the thickness \nof the voxelized skin of the object. \nAlthough this algorithm was efficient for large volumes, for GRB we typically needed only a \ncoarse voxelization (perhaps only 83 voxels), which didn‘t generate enough threads to fill \nthe GPU. For this reason, we developed a batched voxelizer that voxelized a whole collection \nof convex shapes in a single kernel invocation. \nOne issue was that the original voxelizer would sometimes generate overlapping spheres for \nthe initially tightly packed groups of convexes generated by APEX destruction, causing the \nobjects to fly away prematurely. This was due to performing the voxelization in the local \nspace of the convex object. By modifying the voxelization to be performed in world space, \nwe ensured that the spheres generated by voxelization fitted together exactly with no \noverlaps. \n \nSpatial Data Structures \nTo avoid having to test each sphere for collision against every other sphere and each sphere \nagainst every triangle in each frame, we use a spatial data structure to cull out all but \nnearby spheres and triangles. Because the spheres are all the same size, we use a uniform \ngrid whose cell size is set to the sphere diameter. So for each sphere, we can find \npotentially overlapping spheres by querying only the 33 cells surrounding it. The triangles \nare also put into a uniform grid, but a separate grid is used to allow the cell size to be tuned \nseparately. When the triangle mesh is loaded, a voxelization process is used to identify \nwhich cells the triangle passes through. Each intersection cell is given a pointer to the \ntriangle. In Batman, triangle meshes are streamed in as the user progresses through the \nlevels. Initially, we updated the triangle grid incrementally each time a new mesh was \nloaded, but we found out that regenerating the whole grid from scratch at each mesh load \nwas more efficient. As the spheres are used to represent moving objects, the sphere grid \nhas to be rebuilt each frame. Like [LeGrand07, Green07] we use a fast radix sort [Satish09] \nto do this. The grid size used for both grids is 643. A simple modulo 64 hash function is used \nso that the world can be bigger than 643 cells. \n \nSolver \nUnlike the other stages, the projected Gauss Seidel solver isn‘t easy to parallelize. The \nreason for this is that each constraint reads and writes the velocities of two bodies, and \neach body can be affected by more than one constraint. For example, consider a stack of \nthree spheres resting on the ground. Each body is affected by two constraints, except for \nthe top body. If the constraints were executed in parallel by separate threads, then each \nbody (except the top body) would be overwritten by two threads. The final value of the \nvelocity of each body would depend on the exact order in which the constraints accessed \nthe body, and the effect of one constraint would not be taken into account in the calculation \nof the other. \nTo get around this problem, the solver has two phases, both of which run on the GPU. The \nfirst divides the constraint list into batches of constraints that can be executed in parallel. In \neach batch, all bodies are modified by just one constraint. The second executes the batches \nin sequence, using one thread for each constraint in each batch. \n",
      "content_length": 3740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": " \n \nWe make the assumption that any system can be solved in 32 or fewer batches. The goal is \nthat each body be modified only once in each batch and that the number of batches used be \nminimal. Each body is given an unsigned integer called a batch bitmap. Bit i set means that \nthe body is modified in batch i. \nInitially, all the batch bitmaps are set to zero. In the first phase, each constraint is \nprocessed by a thread in parallel. Each thread calculates the logical or (disjunction) of the \nbatch bitmaps of its two bodies. If a bit in the result is zero, then it means that batch \ndoesn‘t modify either body, so the constraint can be processed in this batch. So, the thread \nfinds the first zero bit in the result and attempts to atomically set it in the two bodies. If the \natomic set fails (because another thread has allocated the body to the batch in the \nmeantime), then it starts again to find a new bit that is not set for both bodies. If all the \nbits are set in the disjunction of the two bitmaps, then more than 32 batches are required, \nand the constraint is discarded. \n \nConclusion \nTo test the speedup of GRB, we ran the second Scarecrow level from Batman with and \nwithout GRB. When the level is run without GRB, both the gameplay and destruction rigid \nbodies are run on the CPU, and when the level is run with GRB, the game-play bodies are \nsimulated using PhysX on the CPU, but the 1,600 debris chunks are simulated on the GPU \nusing GRB. The following timings are for the 1,600 debris chunks in both cases. In both \ncases, the debris rigid bodies collide with each other, the gameplay rigid bodies, and the \ntens of thousands of triangles that make up the static environment. Because force fields are \nnot simulated on the GPU in either PhysX or GRB, the force field calculation is excluded in \nboth cases. \nThe test was run for the first 900 frames starting at the second-to-last checkpoint of the \nlevel, where a large concrete wall is destroyed and carried away by a force field. The \ntimings are from a representative frame captured after all the chunks have been loosened \nfrom the wall. Because the timings are of the physics pipeline only, they don‘t contain \ngraphics processing time or any other game code. The timings were taken on an Intel Core \n2 Quad running at 2.66 GHz with the graphics running on a GTX260 and the physics running \non a second GTX260. \nGRB physics frame time (1,600 bodies) \n16.5ms \nPhysX physics frame time (1,600 bodies) 49.5ms \nSpeedup \n3.0x \n \nIn a real game, GRB running on a GPU can achieve a threefold performance increase over \nPhysX running on a CPU. \n \nAcknowledgements \nGRB was written by Richard Tonge, Simon Green, Steve Borho, Lihua Zhang, and Bryan \nGaldrikian. We‘d like to thank Adam Moravanszky, Pierre Terdiman, Dilip Sequeira, Mark \nHarris, and all the other people who indirectly contributed to GRB. The GRB Batman \n",
      "content_length": 2895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": " \n \ndestruction content was designed by Dane Johnston and Johnny Costello, and the code was \nintegrated into the game by James Dolan, David Sullins, and Steve Borho. \n \nReferences \n[Ericson05] Ericson, Christer. Real-Time Collision Detection. Morgan Kaufmann, 2005. \n[Erleben04] Erleben, Kenny. ―Stable, Robust, and Versatile Multibody Dynamics Animation.‖ \nPhD thesis. Department of Computer Science, University of Copenhagen, 2004. \n[Green07] Green, Simon. ―CUDA Particles.‖ NVIDIA Corporation. Whitepaper, 2007. \n[Harada07] Harada, Takahiro. ―Real Time Rigid Body Physics on GPUs.‖ GPU Gems 3. \nAddison Wesley, 2007. \n[Harmon09] Harmon, David, et al. ―Asynchronous Contact Mechanics.‖ SIGGRAPH ‘09: ACM \nSIGGRAPH 2009 papers (2009): 1–12. \n[Harris07] Harris, Mark. ―Scan Primitives for GPU Computing.‖ ACM SIGGRAPH/Eurographics \nConference on Graphics Hardware (2007): n.p. \n[Karabassi99] Karabassi, Evaggelia-Aggeliki, et al. ―A Fast Depth-Buffer-Based Voxelization \nAlgorithm.‖ Journal of Graphics Tools 4.4 (1999): n.p. \n[LeGrand07] LeGrand, Scott. ―Broad-Phase Collision Detection with CUDA.‖ GPU Gems 3. \nAddison Wesley, 2007. \n[Nvidia08] ―NVIDIA PhysX SDK 2.8.1 Documentation.‖ NVIDIA Corporation, 2008. \n[Satish09] Satish, Nadathur, et al. ―Designing Efficient Sorting Algorithms for Manycore \nGPUs.‖ Proceedings of the 23rd IEEE International Parallel & Distributed Processing \nSymposium (2009): n.p. \n \n7.3. Fast GPU Fluid Simulation in PhysX \nSimon Schirm and Mark Harris, NVIDIA Corporation \nsschirm@nvidia.com, harrism@nvidia.com \nOver the past several years, physically based simulation has become an important feature \nof games. Effects based on physical simulation of phenomena such as liquid, smoke, \nparticulate debris, cloth, and rigid body dynamics greatly increase the level of realism and \ndetail in a game. These effects can be expensive to simulate using traditional sequential \napproaches, especially in a complex game engine where the CPU is already handling many \nexpensive tasks. On the other hand, there is a lot of data parallelism in many physical \nsimulations, and the GPUs in most modern PCs and consoles are powerful parallel \nprocessors that can be applied to this task. Simulating fluids and particles on the GPU \nenables games to incorporate complex fluids and particle effects with tens of thousands of \nparticles that interact with the player and the game scenery. \nNVIDIA PhysX is a real-time physics engine for games that uses the GPU in concert with the \nCPU to accelerate physically based simulation of particle systems, fluids, cloth, and rigid \n",
      "content_length": 2592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": " \n \nbody dynamics [NVIDIA09a]. The GPU-accelerated portions of the engine are implemented \nusing the C for CUDA language and the CUDA run-time API. The standard C and C++ \nlanguage support in the CUDA compiler makes programming, debugging, and maintaining \nthe complex algorithms used in PhysX much easier and ensures that they can be integrated \nefficiently with the CPU portions of the engine. For more information on CUDA, see \n[NVIDIA09b]. \nIn this gem, we present the algorithms and implementation details for the particle and fluid \nsystems in PhysX. We describe the features of each system and then present an overview of \nthe algorithm and details of the implementation in CUDA. Figure 7.3.1 and Color Plate 15 \nare examples of the results of the techniques described in this gem. \nFigure 7.3.1. Real-time GPU-accelerated Smoothed Particle Hydrodynamics (SPH) \nwater simulation in PhysX. There are more than 60,000 particles in the simulation. \nThe rendering is performed using a screen-space splatting technique. \n \n \n \nParticle Systems \nParticle systems are ubiquitously used in games to create a variety of effects. Particles can \nbe used to represent media ranging from highly granular materials, such as leaves, gravel, \nand sparks, to more continuous media, such as smoke or steam. Particle effects in games \nare typically tailored to interact as lightly as possible with the scene in order to reduce \ncomputational overhead. This is done, for example, by applying pre-computed forces to the \nparticles in a way that doesn‘t require any intersection tests or collision detection. The \nrange of effects that can be achieved is therefore significantly limited. One goal of PhysX is \nto relax these restrictions by moving expensive but highly parallel calculations to the GPU, \nenabling particles to interact with geometrically rich and dynamic scenes. \n \nFluids \nReal-time simulation of fluids can dramatically increase the realism and interactivity of \nsmoke, steam, and liquid effects in games. The PhysX SDK supports the simulation of \ngaseous and liquid fluids by adding support for particle-particle interactions to its particle \n",
      "content_length": 2147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": " \n \nsystems. Inter-particle forces are defined so that the volume the particles occupy is \napproximately conserved over time. \nPhysX fluid simulation is built on top of the particle system component, sharing the same \ncollision algorithm and spatial partitioning data structure. Like particle systems, fluid motion \nis not practically limited to a box domain; particles and fluids can spread throughout a game \nlevel. \n \nRobust Particle Collisions \nPhysX particles are allowed to interact with arbitrary triangle meshes in a game scene, and \ntherefore we need an algorithm that can robustly handle particle collisions with these \nmeshes. This is particularly challenging in the presence of inter-particle forces, which tend \nto literally push particles into all the edge and corner cases. Our algorithm has the following \nproperties. \n \nPenetration and artificial energy loss are minimized so that particles don‘t leak or \nstick as shown in Figure 7.3.2: Left. \nFigure 7.3.2. Left: Particles don’t stick or leak in edge cases. \nRight: Particles don’t fall through static meshes when pushed by dynamic objects. \n \n \nA configurable collision radius is provided. \n \nHigh-velocity collisions are handled. (The collision radius is usually small compared \nto the distance a fast particle travels per time step.) \n \nParticles on thin static objects cannot be pushed through by dynamic objects \npenetrating the static geometry, as shown in Figure 7.3.2: Right. \nThe PhysX particle collision algorithm is based on a mixed approach that makes use of both \ncontinuous and discrete collision detection, as illustrated in Figure 7.3.3. \nFigure 7.3.3. Left: Continuous collision detection (CCD) with multiple shapes. \nRight: A discrete collision detection test between a particle and geometry. \n \n \nThe linear path of the particle motion is tested for intersection with any dynamic or static \nobjects in order to find the first impact location. For dynamic objects, the intersection test \n",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": " \n \nmust be handled with extra care. Each particle‘s original position p is transformed into the \nlocal space of the moving object according to its original pose. The target position q is \ncorrespondingly transformed according to the target pose of the object, as shown in Figure \n7.3.4. Now the linear path defined by the transformed local positions p‘ and q‘ is tested \nagainst the object in its local space. This is just an approximation, since the motion path of \nthe particle observed from the moving dynamic geometry would generally be curved. \nNonetheless, this approximation has the nice property that the test is topologically correct \nwhen multiple compound geometries are involved. Practically, this means that a particle-\nfilled hollow box doesn‘t leak particles when shaken. \nFigure 7.3.4. Left: Continuous collision detection with a dynamic shape. \nRight: The same situation as on the left, but relative to a moving shape. Instead of \nintersecting the curved particle path, a linear approximation is used. \n \n \nIn addition to the continuous collision test, the sphere defined by the target particle position \nand radius is tested for intersections with nearby geometry, as shown in Figure 7.3.3: Right. \nBoth results are combined to calculate an appropriate collision response for the particle. \nThe collision algorithm must gracefully handle cases in which particles could get trapped \nbetween penetrating scene objects, as shown in Figure 7.3.2: Right. In practice, it makes \nsense to give collisions with static objects precedence over collisions with dynamic objects. \nThis is achieved by dividing the collision algorithm into a first pass handling dynamic objects \nand a second pass handling static objects. \nParticles are often continuously pushed against scene geometry by gravity or fluid forces. In \nthese cases, the particle path intersects close to the particle‘s current position. Correct \ncontinuous collision detection will prevent a particle from sliding along object boundaries. \nOne way to overcome this is to correct the velocity of the particle and check its new path for \npotential collisions again for the rest of the time step. This can be repeated until the end of \nthe time step is reached and the final position of the particle is found. For a particle in a \ncorner, this iteration might take forever. To solve this problem, our algorithm remembers \nthe surfaces a particle collided with in the previous time step and uses them in the current \ntime step to correct the particle velocity in a pre-process to collision detection. In practice, it \nturns out that storing two planes— called surface constraints—per particle is enough to \nprovide good results in most cases. \n \nFluid Simulation with Smoothed Particle Hydrodynamics \nTo simulate a fluid, one must compute the mass (and other field values) carried by the fluid \nthrough space over time. There are two fundamental approaches to fluid simulation: \n \nEulerian simulations track fluid field values at fixed locations in space—for example, \non a grid. \n",
      "content_length": 3047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": " \n \n \nLagrangian simulations track fluid field values at locations moving with the flow—for \nexample, by using particles. \nIn PhysX, using particles has several advantages. \n \nParticles are already used in games, which eases the integration. \n \nParticles enable seamless collisions with static geometry and rigid bodies with no \nboundary resampling. \n \nMass conservation is trivially achieved by associating a constant mass with each \nparticle. \nThe approach we use in PhysX is called Smoothed Particle Hydrodynamics (SPH), which is a \nLagrangian simulation technique that was originally developed to simulate the motion of \ngalaxies [Monaghan92] and later applied to free-surface fluid flow simulation. SPH is an \ninterpolation method that allows field values that are defined only at particle positions to be \nevaluated anywhere in space [Müller03]. The method is based on the Navier-Stokes \nequations, which describe fluid motion by relating continuous velocity, density, and pressure \nfields over time. The two equations enforce conservation of mass and momentum. Since \nmass is conserved by using discrete particles, we only use the momentum equation. In \nLagrangian form, it reads as follows: \nEquation 1  \n \n \nAll terms in Equation (1) are derived from local field properties at the location xi of particle i. \nThe mass of each particle is constant over time. \nrepresents the acceleration of the \nparticle, and the three terms on the right-hand side describe contributions to the \nacceleration. \n1. External acceleration g, including gravity and collision impulses. \n2. Force due to the fluid pressure gradient ▽p|xi. \n3. Force due to the viscosity of the fluid, given by the Laplacian of the velocity field \n▽2v|xi weighted by the scalar viscosity of the fluid μ. \nThe pressure and viscosity terms each represent a so-called ―body force,‖ which is a force \nper unit volume. To convert a body force into acceleration using Newton‘s second law, it \nmust be normalized by the mass density ρi at the location of the particle, as shown in \nEquation (1). \nThe particles act as sample points for the continuous physical fields. Radially symmetric \nkernel functions are used to reconstruct smooth fields from the discrete particle samples, \nhence the name Smoothed Particle Hydrodynamics. \nTo evaluate a field value at a certain location, the contributions of the overlapping kernel \nfunctions at each nearby particle are summed up, as shown in Figure 7.3.5. \nCorrespondingly, the pressure gradient and the velocity Laplacian can be derived using the \nkernel functions, as described by [Müller03]. Choosing kernels with a finite non-zero domain \nis crucial to accelerating the search for nearby particles. \nFigure 7.3.5. A one-dimensional field reconstruction formed by the summation of \noverlapping kernels for each particle. \n",
      "content_length": 2828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": " \n \n \n \nTo compute the acceleration on each particle in the fluid, we need to evaluate Equation (1) \nat the particle position by summing up the kernel contributions from all overlapping \nparticles. In practice, this requires evaluating three physical fields: density, pressure, and \nviscosity. This can be done in two passes: one to evaluate the density field and another to \nevaluate pressure and viscosity, which both depend on density. After evaluating Equation \n(1) in this way, we have an acceleration that can be used to integrate the velocity and \nposition of the particle over the time step. \nFor further details on SPH and our application of the method, see [Müller03]. \n \nFluid Simulation Algorithm \nFigure 7.3.6 shows an overview of the fluid simulation pipeline. First, the spatial domain \nthat is covered by the particles is approximated with a set of bounds. These are inserted \ninto the broad phase along with the bounds of static and dynamic objects in the scene. In \nparallel with the broad phase, the SPH phase computes fluid forces and uses them to \nupdate the particle velocity. The particle collision phase uses the output of the broad phase \nand the SPH phase to compute collisions with dynamic and static objects and applies \nvelocity impulses to resolve any penetration. Finally, the positions of all particles are \nupdated using their newly computed velocities. \nFigure 7.3.6. Particle pipeline overview. \n",
      "content_length": 1431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": " \n \n \n \n \nParticle Bounds Generation and the Broad Phase \nTo support efficient collision detection between particles and static geometry and rigid \nbodies present in the scene, we need to avoid testing all possible particle-object pairs for \nintersections. The so-called broad phase collision detection algorithm compares the spatial \nbounds of scene objects to find all potentially colliding pairs. Checking the bounds of all \nindividual particles would overload the broad phase. Therefore, we generate bounds for \ngroups of spatially coherent particles. The particles are associated with a particular particle \ngroup based on a hash grid similar to the one used for finding neighbors in SPH but with \nmuch coarser grid spacing. The particle collision algorithm uses the output of the broad \nphase, a set of pairs of overlapping particle groups and scene objects, to find the \nintersections of individual particles with scene objects. The broad phase can run in parallel \nin a separate CPU thread while the GPU executes the SPH phase, since the result of the \nbroad phase is only needed before the collision phase starts. \nSPH Phase \nThe SPH phase computes the density and force at each particle position xi. The force \ncomputation evaluates the influence of all other particles in a neighborhood around xi. In \norder to find neighboring particles efficiently, we use a hashed regular grid to accelerate \nfinding nearby particles. A hashed grid implementation similar to the one used in PhysX is \ndemonstrated in the ―particles‖ sample in the NVIDIA CUDA SDK [Green09], as described \nby [Teschner03]. \nHashed Grid Data Structure \nBuilding the hashed grid data structure requires two steps. The first step computes a hash \nindex for each particle such that all particles in the same grid cell have the same hash \nindex. The second step sorts all particle data by hash index so that the data for all particles \nin the same hash cell is contiguous in memory. \n",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": " \n \nTo compute the hash index, each particle position (x, y, z) is discretized onto a virtual (and \ninfinite) 3D grid by dividing its coordinates by the grid cell size and rounding down to the \nnearest integer coordinates (i, j, k) = ([x/h], [y/h], [z/h]). These coordinates are then used \nas input to a hash function that computes a single integer hash index. Our hash function is \nsimple; it computes the integer coordinates modulo the grid size and linearizes the resulting \n3D coordinates, as shown in Listing 7.3.1. This simple hash function has the important \nproperty that given a hash index, computing the hash indices of neighboring cells is trivial, \nbecause neighboring cells in the virtual grid map to neighboring cells in the hash grid \n(modulo the grid size). \nListing 7.3.1. The virtual grid hash function. Note that GRID_SIZE must be a \npower of 2. \nint3 gridPosition(float3 pos, float cellSize) \n{ \n \n   int3 gridPos; \n   gridPos.x = floor(pos.x/cellSize) & (GRID_SIZE – 1); \n   gridPos.y = floor(pos.y/cellSize) & (GRID_SIZE – 1); \n   gridPos.z = floor(pos.z/cellSize) & (GRID_SIZE – 1); \n   return gridPos; \n} \nint hash(int3 gridPos) \n{ \n   return (gridPos.z * GRID_SIZE + gridPos.y) * GRID_SIZE + \ngridPos.x; \n} \n \nThe hash values for each particle are computed in a CUDA C kernel that outputs an array of \nhash indices and an array of corresponding particle indices. These are then used as arrays \nof keys and corresponding indices, which we sort using the fast CUDA radix sort of \n[Satish09]. For each hash grid cell, we also store the sorted indices of the first and last \nparticle in the cell, to enable constant-time queries of the particles in each hash cell. After \nthe key-index pairs are sorted, we use the sorted indices to reorder the particle position and \nvelocity arrays. Reordering the data arrays is important because it ensures efficient, \ncoalesced memory accesses in subsequent CUDA computations. \nUsing parallel sorting to build this ―list of lists‖ grid structure is more efficient than building \nit directly (in other words, by appending each particle to a list for each grid cell) in parallel, \nbecause we don‘t know a priori which cells are occupied and how many particles are in each \ncell. An advantage of using a radix sort is that multiple non-interacting fluids can be \ncombined within the same virtual grid at low additional cost, because doubling the number \nof fluids only adds one bit to the sort key length. We simply process the particles of all fluids \ntogether in parallel and append the fluid ID above the most significant bit of the hash index. \nThen when we sort the particles, they will be sorted first by fluid ID and then by hash index. \nThis enables us to do all subsequent steps in the simulation of all fluids in parallel. To \nreduce sorting cost, we sort using only as many bits as we need to represent our chosen \ngrid resolution, which currently is log2 Nf + 18 bits (64×64×64 grid), where Nf is the \nnumber of fluids. \nWith the use of a hash, we can handle an unlimited grid. Therefore, the fluid is not confined \nto a box but can flow anywhere in the scene. Naturally, in a widely dispersed fluid, particles \nfrom far-apart cells may hash to the same index. If many non-interacting particles hash to \nthe same index, we will perform unnecessary work to evaluate their Euclidean distance, but \n",
      "content_length": 3364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": " \n \nin practice this has not been a problem. For more on evaluating hash functions and hash \ntable size, see [Teschner03]. \nComputing Density and Force \nOnce the hashed grid is built, it can be used to quickly find all particles near to each \nparticle. Thus, we can evaluate the density and force by finding the cell each particle resides \nin and iterating over the 27 neighboring cells in 3D. At each neighboring cell, we perform a \nhash table lookup to find the list of particles in each cell. Using these particles, we can \ncompute the sum of the influences of each neighboring particle, as shown in Listing 7.3.2. \nListing 7.3.2. Pseudocode for evaluating a smoothing kernel at a particle’s position \nSumOfSupportFunctions(float3 pos, float cellSize) \n{ \n \n   int3 gridPos = gridPosition(pos, cellSize); \n   for (int i = -1; i <= 1; i++) \n      for (int j = -1; j <= 1; j++) \n         for (int k = -1; k <= 1; k++) { \n             gridPosition += int3(i, j, k); \n             particles = hashTableLookup(hash(gridPosition)) \n             foreach particle p in particles { \n                sum += supportFunction(pos, p.position); \n             } \n         } \n} \n \nThe procedure in Listing 7.3.2 can be applied to both of the steps required to compute the \nbody forces in Equation (1). First, it is used to compute density using the density kernel \nfunction. Then, the density is used to compute pressure and viscosity using the pressure \nand viscosity kernel functions. For details of these kernel functions, see [Müller03]. \nParallel Implementation in CUDA \nThe four SPH steps shown in Figure 7.3.6 are implemented using one CUDA C kernel each \nto compute the particle hash, the density, and the force at each particle, plus a radix sort, \nwhich is implemented using multiple CUDA C kernels as described in [Satish09], and a \nkernel that finds the starting and ending indices of particles in each cell and reorders the \nparticle position and velocity arrays using the sorted indices. \nThe particle hash kernel simply computes the hash index for each particle given its 3D \nposition using the function in Listing 7.3.1. We launch as many GPU threads as there are \nparticles in the scene and compute one hash per thread. After sorting the particles by hash \nindex using the radix sort, we launch another kernel, again with one thread per particle, to \nreorder the particle position and velocity arrays. This kernel also compares each particle‘s \nhash index against those before and after it in the sorted array. If it is different from the \nindex before it, then we know this is the first particle in a cell, so we store this particle‘s \nindex in the sorted array to a ―cell starts‖ array at the cell index. If it is different from the \nindex after it, then it is the end of a cell, so we store its index in a ―cell ends‖ array. \nThe density and force kernels also process one particle per thread. Each thread runs the \nprocedure given in Listing 7.3.2. The density kernel uses the density support function given \n",
      "content_length": 3017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": " \n \nin [Müller03] as its support function to compute a scalar density per particle. The force \nkernel computes two 3D force sums, one using the pressure gradient support function and \none using the viscosity Laplacian support function, both given in [Müller03]. The \nhashTableLookup function in Listing 7.3.2 simply reads the indices at the cell‘s index in \nthe cell start and cell end arrays to get a range of particle indices to read from the sorted \nparticle position and velocity arrays. \nWe make several optimizations to our CUDA C kernels to ensure best performance. First, we \npack together the data for all active particle systems or fluids into large batches so that the \nCUDA kernels can process all particles at once. This improves utilization of the GPU in the \npresence of multiple small particle systems. We add a small amount of padding between the \nend of one particle system‘s data and the beginning of the next to ensure that each particle \nsystem begins on an aligned boundary, and we use structures of arrays (SOA) rather than \narrays of structures for the particle data. This means that instead of having an array of \nparticle structures containing particle position, velocity, and other data, we have an array of \nall particle positions, an array of all velocities, and so on. These optimizations ensure that \nparallel thread accesses to the data arrays are coalesced by the GPU [NVIDIA09b]. Finally, \nthe density and force kernels bind a texture reference to the data arrays so that the \nspatially coherent but non-sequential access pattern of these kernels can benefit from the \ntexture cache hardware on the GPU [NVIDIA09b]. \nParticle Collision Phase \nThe collision phase uses the particle positions and updated velocities from the SPH phase \n(which includes external accelerations, such as gravity) to compute intersections with scene \ngeometry and rigid bodies and to prevent penetration. The collision algorithm is composed \nof the following sequence of stages: \n1. Surface constraint collision \n2. Collision detection with dynamic rigid body shapes \n3. Dynamic collision response \n4. Collision detection with static rigid body shapes \n5. Static collision response \n6. Update velocity and position \nWithin each stage we can treat particles independently, and therefore we typically use one \nCUDA thread per particle. The particle data as well as any temporary per-particle storage is \nlaid out as structures of arrays (SOA) for efficient coalesced memory access. As with the \nSPH stage, in order to improve the utilization of the GPU, each CUDA kernel processes \nmultiple particle systems together in one batch. \nSurface Constraint Collision \nThe first collision CUDA kernel loads up to two surface constraints from the previous time \nstep for each particle, if available. From the current particle position and velocity, we \ncompute a target position. The target position is projected out of the surface to get a \nmotion path that is not in conflict with the surface the particle is in contact with. The target \nposition is then passed on to the next stage. \nCollision Detection with Static and Dynamic Shapes \nIntersection tests for static and dynamic shapes can be handled with the same set of CUDA \nkernels, treating a static shape as a special case of a dynamic shape that doesn‘t move. The \ncollision detection described in this section is therefore executed twice— first for dynamic \nshapes and then for static shapes. \nFor each spatially coherent group of particles, we need to find all relevant intersections with \nall shapes that are provided by the broad phase for this group. To improve execution \n",
      "content_length": 3638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": " \n \ncoherence on the data-parallel GPU architecture, we run a different CUDA kernel for each \nshape type. Our collision algorithm handles collisions with capsules, convex meshes, and \ntriangle mesh shape types, so we have three shape-specific collision kernels. \nEach shape-specific kernel iterates over its shapes by looping over batches of shape data \n(capsule extents and radii, triangles, or planes depending on shape type) that are loaded by \nall threads cooperatively from global device memory into fast on-chip shared memory. \nThen, for each loaded batch, each thread tests one particle for intersection with all shapes \nin the batch and aggregates the contact information. \nIn the case of triangle meshes, an additional optimization improves GPU utilization when \nthere are more triangles than particles in the spatial group. Instead of using one thread per \nparticle, it is more efficient to parallelize across triangles (in other words, run one thread \nper triangle). However, since particles may collide with multiple triangles, we would need to \nstore contact information per particle-triangle pair, which is not memory efficient. Our \napproach is to perform parallel early-out tests for which the results (one bit per test) can be \nstored very efficiently. This is done as follows. For each triangle, all particles are \ninvestigated to find the ones that potentially collide, ruling out the others with a relatively \ncheap conservative intersection test. The per-particle contact generation can then skip \ntriangles for which the early-out test had a negative result. \nCollision Response and Particle Update \nAfter the dynamic shape collision detection, a CUDA kernel computes a collision response \nfrom the aggregated surface contact information and updates particle target positions \naccordingly. After the static shape collision detection, the collision response kernel is \nexecuted again, providing the final particle position. \n \nFluid Rendering \nDisplaying materials in games that are represented as particles requires very different \napproaches depending on the material. Granular materials, such as gravel, leaves, or \nsparks, can be rendered using instanced meshes or billboard techniques, such as impostors. \nGaseous fluids are typically displayed using a billboard per particle and various blending \nmethods. Adding environmental shadowing and self-shadowing improves the realism with \nsome added performance cost. The ―Smoke Particles‖ sample in the NVIDIA CUDA SDK \ndemonstrates this technique [Green09]. Rendering liquids based on particle simulation is \nparticularly challenging, since a smooth surface needs to be extracted. \nScreen-Space Fluid Rendering \nExtracting a surface from a set of points is typically done using the grid-based marching \ncubes algorithm [Lorensen87], which creates the surface by marching through voxels and \ngenerating triangles in cells through which an isosurface is found to pass. This method, \nhowever, doesn‘t provide real-time performance for the resolution and particle count we are \ntargeting. A real-time alternative is the approach described by [Müller07], which simplifies \nthe marching cubes to marching squares to generate a surface mesh in screen space. A \nfurther simplification is to avoid any polygonization of the surface and work entirely at the \npixel level. This is the approach used in the demo shown in Figure 7.3.1 [NVIDIA08]. \nOur screen-space rendering approach is as follows. \n1. Render the scene to a color texture and to the depth buffer. \n2. Render all particles to a depth texture as sphere point sprites. \n3. Render the particle thickness to a ―thickness‖ texture with additive blending. \n",
      "content_length": 3677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": " \n \n4. Smooth the depth texture to avoid a ―particle appearance.‖ \n5. Compute a surface normal texture from the smoothed depth using central \ndifferencing. \n6. Use the normal, depth, and thickness to render the fluid surface into the scene color \ntexture with proper depth testing, reflection, refraction, and Fresnel term. \nThese steps are all implemented in a standard graphics API using pixel and vertex shaders. \nThe smoothing step is very important; without it, the fluid will look like a collection of \ndiscrete particles. There are multiple ways to implement this smoothing. The basic approach \nused in Figure 7.3.1 is to smooth the depth texture using a separable Gaussian blur filter. \nThis is inexpensive but can lead to a somewhat ―blobby‖ appearance. A more sophisticated \napproach is to apply curvature flow, as described by [van der Laan09]. Their technique \nachieves much more smoothing in areas of low curvature, while maintaining sharp details. A \ncomparison of the two techniques is shown in Color Plate 16. \n \nPerformance \nWe measured the performance of the PhysX particle fluid demo shown in Figure 7.3.1 and \nColor Plate 16 on a PC with Intel Core 2 Quad CPU and an NVIDIA GeForce GTX 260 GPU. \nThe demo was run with a screen resolution of 1024×768 and an average of about 60,000 \nparticles. Figure 7.3.7 presents the results. The time taken for rendering and CUDA kernels \nin PhysX is about balanced; on the other hand, the PhysX SDK work executed on the CPU \ncontributes relatively strongly. This can be improved by moving all of the per-particle \nprocessing onto the GPU. The executions of the CUDA kernels take 8.2 ms on average. For \nSPH, the summation passes over particle neighbors that compute density and force take \nmost of the time. For collision, the largest portion is contributed from collisions with the \nstatic triangle mesh. \nFigure 7.3.7. Left: Overall frame run time. Middle, right: Cuda kernel run time for \nSPH and collision phases. The gaps display contributions from remaining kernels \nof the respective phase. ―Radix‖ refers to the radix sort kernels. \n \n \n \nAcknowledgements \nThe CUDA fluid simulation implementation and the ideas behind it are the fruits of the \nefforts of a large team. We would like to thank all the members of the NVIDIA PhysX team \nand others at NVIDIA for their contributions, especially Steve Borho, Curtis Davis, Stefan \nDuthaler, Isha Geigenfeind, Monier Maher, Simon Green, Matthias Müller-Fischer, Ashu \n",
      "content_length": 2478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": " \n \nRege, Miguel Sainz, Richard Tonge, Lihua Zhang, and Jiayuan Zhu. We would also like to \nthank Gareth Ramsey and Eidos Game Studios for the use of Color Plate 15. \n \nReferences \n[Green09] Green, S. ―Particles.‖ and ―Smoke Particles.‖ n.d. NVIDIA CUDA SDK version 2.3. \nn.d. <http://www.nvidia.com/object/cuda_get.html>. \n[Lorensen87] Lorensen, W. E., and H. E. Cline. ―Marching Cubes: A High Resolution 3D \nSurface Construction Algorithm.‖ Computer Graphics 21.4 (July 1987): n.p. \n[Monaghan92] Monaghan, J. J. ―Smoothed Particle Hydrodynamics.‖ Annual Review of \nAstronomy and Astrophysics 30 (1992): 543–574. \n[Müller03] Müller, M., et al. ―Particle-Based Fluid Simulation for Interactive Applications.‖ \nSymposium on Computer Animation (2003): 154–159. \n[Müller07] Müller, M., S. Schirm, and S. Duthaler. ―Screen-Space Meshes.‖ Symposium on \nComputer Animation (2007): 9–15. \n[NVIDIA08] NVIDIA Corporation. ―Fluids: Technology Demo.‖ 2008. NVIDIA. n.d. \n<http://www.nvidia.com/content/graphicsplus/us/download.asp>. \n[NVIDIA09a] NVIDIA Corporation. ―NVIDIA PhysX.‖ 2009. NVIDIA. n.d. \n<http://www.nvidia.com/object/physx_new.html>. \n[NVIDIA09b] NVIDIA Corporation. ―NVIDIA CUDA Programming Guide.‖ 2009. NVIDIA. n.d. \n<http://www.nvidia.com/cuda>. \n[Satish09] Satish, N., M. Harris, and M. Garland. ―Designing Efficient Sorting Algorithms for \nManycore GPUs.‖ Proceedings of IEEE International Parallel and Distributed Processing \nSymposium (2009): to appear. \n[Teschner03] Teschner, M. et al. ―Optimized Spatial Hashing for Collision Detection of \nDeformable Objects.‖ Proceedings of VMV (2003): 47–54. \n[van der Laan09] van der Laan, W., S. Green, and M. Sainz. ―Screen Space Fluid Rendering \nwith Curvature Flow.‖ Proceedings of the 2009 Symposium on Interactive 3D Graphics and \nGames (2009): 91–98. \n \nColor Plate \nColor Plate 1: Final results of the SSAO technique presented in Gem 1.2. The top-left pane \nshows lighting without the ambient occlusion, while the top-right pane shows lighting with \nthe SSAO component mixed in. The final colored result is shown in the bottom image. Here \nthe SSAO samples are very wide, bathing the background area with an effect that would \notherwise only be obtained with a thorough global illumination algorithm. The SSAO term \nadds depth to the scene and helps anchor the characters within the environment. \n",
      "content_length": 2357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": " \n \n \nColor Plate 2: Comparison of large- versus small-area SSAO samples from Gem 1.2. The \nimages show the contrast between the large-area, low-contrast SSAO sampling component \non the bar surface and background in the image on the left and the tighter, higher-contrast \nSSAO samples apparent within the helmet and nooks and crannies found on the \ncharacter’s space suit in the middle image. Final output is on the right. \n \nColor Plate 3: Output of rendering stages and final image from the deferred rendering \ntechnique described in Gem 1.3. Position and material ID (top left), normal and depth G-\nbuffers (bottom left), and a resultant image (right). \n",
      "content_length": 657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": " \n \n \nColor Plate 4: The deferred shading technique presented in Gem 1.2 has three steps: \ngeometry pass, multi-resolution rendering pass, and composite pass. \n \nColor Plate 5: A screenshot of Blur. The SPUs are used to compute position and normal \noffsets to deform the car as it takes damage. More details are in Gem 1.8. \n",
      "content_length": 325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": " \n \n \nColor Plate 6: A screenshot of Blur, showing a final result of the SPU-based lighting system \npresented in Gem 1.8. \n \nColor Plate 7: Test face models while performing basic expressions from the anatomical \nhuman face model presented in Gem 2.1. From left to right: neutral, joy, sadness, surprise, \ndisgust, and fear. \n",
      "content_length": 326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": " \n \n \nColor Plate 8: Segmentation results and piecewise convex approximations of different 3D \nmeshes presented in Gem 2.8. (T number of triangles of the original mesh, D length of the \ndiagonal of its bounding box, α = 0.03 × D and the K number of the ACD clusters.) \n",
      "content_length": 269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": " \n \n \nColor Plates 9 and 10: From Gem 3.4, examples of characters in World of Zoo’s (WOZ’s) \nenvironment. Characters move through the environment in a spatially appropriate and \nconsistent manner when using the technique presented in this gem. \n",
      "content_length": 245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": " \n \n \nColor Plate 11: Example output from Gem 4.8 demonstrating an approach to testing code \ncoverage. © Courtesy of Crytek GmbH and Electronic Arts, Inc. \n \nColor Plate 12: Destructible wall section being placed into Batman in Unreal Editor from \nGem 7.2. \n",
      "content_length": 258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": " \n \n \nColor Plates 13 and 14: Destruction showing sphere representation of rigid bodies and \nrendered geometry from Gem 7.2. \n \nColor Plate 15: Batman: Arkham Asylum with interactive PhysX SPH ground smoke, as \ndescribed in Gem 7.3. \n",
      "content_length": 234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": " \n \n \nColor Plate 16: A close-up of screen-space fluid rendering with depth smoothing via \nGaussian blur (left) and curvature flow (right) from Gem 7.3. The rendering on the right \nalso uses surface Perlin noise to add visual detail. \n \n \n \n",
      "content_length": 241,
      "extraction_method": "Direct"
    }
  ]
}