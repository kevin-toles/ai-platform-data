{
  "metadata": {
    "title": "The DevOps Handbook - Gene Kim",
    "author": "Unknown",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 629,
    "conversion_date": "2025-12-19T17:48:19.024646",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "The DevOps Handbook - Gene Kim.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 3-11)",
      "start_page": 3,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "Development was critical to success. But I still remember the\n\nfirst time I saw the magnitude of the downward spiral that\n\nwould result when these functions worked toward opposing\n\ngoals.\n\nIt was 2006, and I had the opportunity to spend a week with\n\nthe group who managed the outsourced IT Operations of a\n\nlarge airline reservation service. They described the\n\ndownstream consequences of their large, annual software\n\nreleases: each release would cause immense chaos and\n\ndisruption for the outsourcer, as well as customers; there\n\nwould be SLA (service level agreement) penalties, because of the customer-impacting outages; there would be layoffs of the most talented and experienced staff, because of the resulting\n\nprofit shortfalls; there would be much unplanned work and firefighting so that the remaining staff couldn’t work on the ever-growing service request backlogs coming from customers; the contract would be held together by the heroics of middle management; and everyone felt that the contract would be\n\ndoomed to be put out for re-bid in three years.\n\nThe sense of hopelessness and futility that resulted created for me the beginnings of a moral crusade. Development seemed to\n\nalways be viewed as strategic, but IT Operations was viewed as tactical, often delegated away or outsourced entirely, only to return in five years in worse shape than it was first handed over.\n\nFor many years, many of us knew that there must be a better way. I remember seeing the talks coming out of the 2009\n\nVelocity Conference, describing amazing outcomes enabled by architecture, technical practices, and cultural norms that we\n\nnow know as DevOps. I was so excited, because it clearly\n\npointed to the better way that we had all been searching for.\n\nAnd helping spread that word was one of my personal\n\nmotivations to co-author The Phoenix Project. You can\n\nimagine how incredibly rewarding it was to see the broader\n\ncommunity react to that book, describing how it helped them\n\nachieve their own “aha” moments.\n\nJez Humble\n\nMy DevOps “aha” moment was at a start-up in 2000—my first\n\njob after graduating. For some time, I was one of two technical\n\nstaff. I did everything: networking, programming, support, systems administration. We deployed software to production by FTP directly from our workstations.\n\nThen in 2004 I got a job at ThoughtWorks, a consultancy where my first gig was working on a project involving about seventy people. I was on a team of eight engineers whose full- time job was to deploy our software into a production-like environment. In the beginning, it was really stressful. But over\n\na few months we went from manual deployments that took two weeks to an automated deployment that took one hour, where we could roll forward and back in milliseconds using the blue- green deployment pattern during normal business hours.\n\nThat project inspired a lot of the ideas in both the Continuous Delivery (Addison-Wesley, 2000) book and this one. A lot of what drives me and others working in this space is the knowledge that, whatever your constraints, we can always do\n\nbetter, and the desire to help people on their journey.\n\nPatrick Debois\n\nFor me, it was a collection of moments. In 2007 I was working\n\non a data center migration project with some Agile teams. I was jealous that they had such high productivity—able to get so\n\nmuch done in so little time.\n\nFor my next assignment, I started experimenting with Kanban in Operations and saw how the dynamic of the team changed.\n\nLater, at the Agile Toronto 2008 conference I presented my\n\nIEEE paper on this, but I felt it didn’t resonate widely in the Agile community. We started an Agile system administration\n\ngroup, but I overlooked the human side of things.\n\nAfter seeing the 2009 Velocity Conference presentation “10 Deploys per Day” by John Allspaw and Paul Hammond, I was\n\nconvinced others were thinking in a similar way. So I decided to organize the first DevOpsDays, accidently coining the term\n\nDevOps.\n\nThe energy at the event was unique and contagious. When\n\npeople started to thank me because it changed their life for the better, I understood the impact. I haven’t stopped promoting\n\nDevOps since.\n\nJohn Willis\n\nIn 2008, I had just sold a consulting business that focused on\n\nlarge-scale, legacy IT operations practices around configuration management and monitoring (Tivoli) when I\n\nfirst met Luke Kanies (the founder of Puppet Labs). Luke was giving a presentation on Puppet at an O’Reilly open source\n\nconference on configuration management (CM).\n\nAt first I was just hanging out at the back of the room killing time and thinking, “What could this twenty-year-old tell me\n\nabout configuration management?” After all, I had literally been working my entire life at some of the largest enterprises\n\nin the world, helping them architect CM and other operations management solutions. However, about five minutes into his\n\nsession, I moved up to the first row and realized everything I\n\nhad been doing for the last twenty years was wrong. Luke was describing what I now call second generation CM.\n\nAfter his session I had an opportunity to sit down and have\n\ncoffee with him. I was totally sold on what we now call infrastructure as code. However, while we met for coffee, Luke\n\nstarted going even further, explaining his ideas. He started telling me he believed that operations was going to have to\n\nstart behaving like software developers. They were going to\n\nhave to keep their configurations in source control and adopt CI/CD delivery patterns for their workflow. Being the old IT\n\nOperations person at the time, I think I replied to him with something like, “That idea is going to sink like Led Zeppelin\n\nwith Ops folk.” (I was clearly wrong.)\n\nThen about a year later in 2009 at another O’Reilly conference, Velocity, I saw Andrew Clay Shafer give a presentation on Agile\n\nInfrastructure. In his presentation, Andrew showed this iconic\n\npicture of a wall between developers and operations with a metaphorical depiction of work being thrown over the wall. He\n\ncoined this “the wall of confusion.” The ideas he expressed in that presentation codified what Luke was trying to tell me a\n\nyear earlier. That was the light bulb for me. Later that year, I was the only American invited to the original DevOpsDays in\n\nGhent. By the time that event was over, this thing we call\n\nDevOps was clearly in my blood.\n\nClearly, the co-authors of this book all came to a similar epiphany,\n\neven if they came there from very different directions. But there is now an overwhelming weight of evidence that the problems\n\ndescribed above happen almost everywhere, and that the solutions associated with DevOps are nearly universally applicable.\n\nThe goal of writing this book is to describe how to replicate the\n\nDevOps transformations we’ve been a part of or have observed, as well as dispel many of the myths of why DevOps won’t work in\n\ncertain situations. Below are some of the most common myths we\n\nhear about DevOps.\n\nMyth—DevOps is Only for Startups: While DevOps practices have been pioneered by the web-scale, Internet “unicorn”\n\ncompanies such as Google, Amazon, Netflix, and Etsy, each of these organizations has, at some point in their history, risked\n\ngoing out of business because of the problems associated with more traditional “horse” organizations: highly dangerous code\n\nreleases that were prone to catastrophic failure, inability to release\n\nfeatures fast enough to beat the competition, compliance concerns, an inability to scale, high levels of distrust between\n\nDevelopment and Operations, and so forth.\n\nHowever, each of these organizations was able to transform their architecture, technical practices, and culture to create the amazing\n\noutcomes that we associate with DevOps. As Dr. Branden Williams, an information security executive, quipped, “Let there be no more talk of DevOps unicorns or horses but only\n\nthoroughbreds and horses heading to the glue factory.”\n\nMyth—DevOps Replaces Agile: DevOps principles and practices are compatible with Agile, with many observing that DevOps is a\n\nlogical continuation of the Agile journey that started in 2001. Agile often serves as an effective enabler of DevOps, because of its focus on small teams continually delivering high quality code to\n\ncustomers.\n\nMany DevOps practices emerge if we continue to manage our work beyond the goal of “potentially shippable code” at the end of\n\neach iteration, extending it to having our code always in a deployable state, with developers checking into trunk daily, and that we demonstrate our features in production-like\n\nenvironments.\n\nMyth—DevOps is incompatible with ITIL: Many view DevOps as a backlash to ITIL or ITSM (IT Service Management), which was\n\noriginally published in 1989. ITIL has broadly influenced multiple generations of Ops practitioners, including one of the co-authors, and is an ever-evolving library of practices intended to codify the processes and practices that underpin world-class IT Operations,\n\nspanning service strategy, design, and support.\n\nDevOps practices can be made compatible with ITIL process. However, to support the shorter lead times and higher\n\ndeployment frequencies associated with DevOps, many areas of the ITIL processes become fully automated, solving many problems associated with the configuration and release\n\nmanagement processes (e.g., keeping the configuration management database and definitive software libraries up to date). And because DevOps requires fast detection and recovery when service incidents occur, the ITIL disciplines of service\n\ndesign, incident, and problem management remain as relevant as ever.\n\nMyth—DevOps is Incompatible with Information Security and Compliance: The absence of traditional controls (e.g., segregation of duty, change approval processes, manual security reviews at the\n\nend of the project) may dismay information security and compliance professionals.\n\nHowever, that doesn’t mean that DevOps organizations don’t have\n\neffective controls. Instead of security and compliance activities only being performed at the end of the project, controls are integrated into every stage of daily work in the software\n\ndevelopment life cycle, resulting in better quality, security, and compliance outcomes.\n\nMyth—DevOps Means Eliminating IT Operations, or “NoOps”:\n\nMany misinterpret DevOps as the complete elimination of the IT Operations function. However, this is rarely the case. While the nature of IT Operations work may change, it remains as important\n\nas ever. IT Operations collaborates far earlier in the software life cycle with Development, who continues to work with IT Operations long after the code has been deployed into production.\n\nInstead of IT Operations doing manual work that comes from work tickets, it enables developer productivity through APIs and self-serviced platforms that create environments, test and deploy code, monitor and display production telemetry, and so forth. By\n\ndoing this, IT Operations become more like Development (as do QA and Infosec), engaged in product development, where the product is the platform that developers use to safely, quickly, and\n\nsecurely test, deploy, and run their IT services in production.\n\nMyth—DevOps is Just “Infrastructure as Code” or Automation: While many of the DevOps patterns shown in this book require\n\nautomation, DevOps also requires cultural norms and an architecture that allows for the shared goals to be achieved throughout the IT value stream. This goes far beyond just automation. As Christopher Little, a technology executive and one\n\nof the earliest chroniclers of DevOps, wrote, “DevOps isn’t about automation, just as astronomy isn’t about telescopes.”\n\nMyth—DevOps is Only for Open Source Software: Although\n\nmany DevOps success stories take place in organizations using software such as the LAMP stack (Linux, Apache, MySQL, PHP), achieving DevOps outcomes is independent of the technology\n\nbeing used. Successes have been achieved with applications written in Microsoft.NET, COBOL, and mainframe assembly code, as well as with SAP and even embedded systems (e.g., HP LaserJet\n\nfirmware).\n\nSPREADING THE AHA! MOMENT\n\nEach of the authors has been inspired by the amazing innovations happening in the DevOps community and the outcomes they are creating: they are creating safe systems of work, and enabling small teams to quickly and independently develop and validate\n\ncode that can be safely deployed to customers. Given our belief that DevOps is a manifestation of creating dynamic, learning organizations that continually reinforce high-trust cultural norms,\n\nit is inevitable that these organizations will continue to innovate and win in the marketplace.\n\nIt is our sincere hope that The DevOps Handbook will serve as a valuable resource for many people in different ways: a guide for planning and executing DevOps transformations, a set of case\n\nstudies to research and learn from, a chronicle of the history of DevOps, a means to create a coalition that spans Product Owners, Architecture, Development, QA, IT Operations, and Information\n\nSecurity to achieve common goals, a way to get the highest levels of leadership support for DevOps initiatives, as well as a moral imperative to change the way we manage technology organizations\n\nto enable better effectiveness and efficiency, as well as enabling a happier and more humane work environment, helping everyone become lifelong learners—this not only helps everyone achieve their highest goals as human beings, but also helps their\n\norganizations win.",
      "page_number": 3
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-19)",
      "start_page": 12,
      "end_page": 19,
      "detection_method": "topic_boundary",
      "content": "Foreword\n\nIn the past, many fields of engineering have experienced a sort of\n\nnotable evolution, continually “leveling-up” its understanding of\n\nits own work. While there are university curriculums and\n\nprofessional support organizations situated within specific\n\ndisciplines of engineering (civil, mechanical, electrical, nuclear,\n\netc.), the fact is, modern society needs all forms of engineering to\n\nrecognize the benefits of and work in a multidisciplinary way.\n\nThink about the design of a high-performance vehicle. Where does the work of a mechanical engineer end and the work of an\n\nelectrical engineer begin? Where (and how, and when) should\n\nsomeone with domain knowledge of aerodynamics (who certainly would have well-formed opinions on the shape, size, and\n\nplacement of windows) collaborate with an expert in passenger ergonomics? What about the chemical influences of fuel mixture\n\nand oil on the materials of the engine and transmission over the\n\nlifetime of the vehicle? There are other questions we can ask about the design of an automobile, but the end result is the same: success in modern technical endeavors absolutely requires\n\nmultiple perspectives and expertise to collaborate.\n\nIn order for a field or discipline to progress and mature, it needs to reach a point where it can thoughtfully reflect on its origins, seek out a diverse set of perspectives on those reflections, and place that synthesis into a context that is useful for how the community\n\npictures the future.\n\nThis book represents such a synthesis and should be seen as a\n\nseminal collection of perspectives on the (I will argue, still\n\nemerging and quickly evolving) field of software engineering and\n\noperations.\n\nNo matter what industry you are in, or what product or service\n\nyour organization provides, this way of thinking is paramount and\n\nnecessary for survival for every business and technology leader.\n\n—John Allspaw, CTO, Etsy\n\nBrooklyn, NY, August 2016\n\nImagine a World Where Dev and\n\nOps Become DevOps\n\nAn Introduction to The DevOps\n\nHandbook\n\nImagine a world where product owners, Development, QA, IT\n\nOperations, and Infosec work together, not only to help each\n\nother, but also to ensure that the overall organization succeeds. By\n\nworking toward a common goal, they enable the fast flow of\n\nplanned work into production (e.g., performing tens, hundreds, or\n\neven thousands of code deploys per day), while achieving world- class stability, reliability, availability, and security.\n\nIn this world, cross-functional teams rigorously test their hypotheses of which features will most delight users and advance the organizational goals. They care not just about implementing\n\nuser features, but also actively ensure their work flows smoothly and frequently through the entire value stream without causing chaos and disruption to IT Operations or any other internal or external customer.\n\nSimultaneously, QA, IT Operations, and Infosec are always\n\nworking on ways to reduce friction for the team, creating the work systems that enable developers to be more productive and get better outcomes. By adding the expertise of QA, IT Operations,\n\nand Infosec into delivery teams and automated self-service tools and platforms, teams are able to use that expertise in their daily work without being dependent on other teams.\n\nThis enables organizations to create a safe system of work, where small teams are able to quickly and independently develop, test,\n\nand deploy code and value quickly, safely, securely, and reliably to\n\ncustomers. This allows organizations to maximize developer\n\nproductivity, enable organizational learning, create high employee\n\nsatisfaction, and win in the marketplace.\n\nThese are the outcomes that result from DevOps. For most of us,\n\nthis is not the world we live in. More often than not, the system we\n\nwork in is broken, resulting in extremely poor outcomes that fall\n\nwell short of our true potential. In our world, Development and IT\n\nOperations are adversaries; testing and Infosec activities happen\n\nonly at the end of a project, too late to correct any problems found;\n\nand almost any critical activity requires too much manual effort and too many handoffs, leaving us to always be waiting. Not only does this contribute to extremely long lead times to get anything\n\ndone, but the quality of our work, especially production deployments, is also problematic and chaotic, resulting in negative impacts to our customers and our business.\n\nAs a result, we fall far short of our goals, and the whole organization is dissatisfied with the performance of IT, resulting in budget reductions and frustrated, unhappy employees who feel powerless to change the process and its outcomes.† The solution? We need to change how we work; DevOps shows us the best way\n\nforward.\n\nTo better understand the potential of the DevOps revolution, let us\n\nlook at the Manufacturing Revolution of the 1980s. By adopting Lean principles and practices, manufacturing organizations dramatically improved plant productivity, customer lead times, product quality, and customer satisfaction, enabling them to win\n\nin the marketplace.\n\nBefore the revolution, average manufacturing plant order lead times were six weeks, with fewer than 70% of orders being shipped\n\non time. By 2005, with the widespread implementation of Lean practices, average product lead times had dropped to less than\n\nthree weeks, and more than 95% of orders were being shipped on\n\ntime. Organizations that did not implement Lean practices lost market share, and many went out of business entirely.\n\nSimilarly, the bar has been raised for delivering technology\n\nproducts and services—what was good enough in previous decades is not good enough now. For each of the last four decades, the cost\n\nand time required to develop and deploy strategic business capabilities and features has dropped by orders of magnitude.\n\nDuring the 1970s and 1980s, most new features required one to\n\nfive years to develop and deploy, often costing tens of millions of dollars.\n\nBy the 2000’s, because of advances in technology and the\n\nadoption of Agile principles and practices, the time required to develop new functionality had dropped to weeks or months, but\n\ndeploying into production would still require weeks or months, often with catastrophic outcomes.\n\nAnd by 2010, with the introduction of DevOps and the\n\nneverending commoditization of hardware, software, and now the\n\ncloud, features (and even entire startup companies) could be created in weeks, quickly being deployed into production in just\n\nhours or minutes—for these organizations, deployment finally became routine and low risk. These organizations are able to\n\nperform experiments to test business ideas, discovering which ideas create the most value for customers and the organization as\n\na whole, which are then further developed into features that can be rapidly and safely deployed into production.\n\nTable 1. The ever accelerating trend toward faster, cheaper, low-risk delivery of software\n\n(Source: Adrian Cockcroft, “Velocity and Volume (or Speed Wins),” presentation at FlowCon, San Francisco, CA, November 2013.)\n\nToday, organizations adopting DevOps principles and practices often deploy changes hundreds or even thousands of times per\n\nday. In an age where competitive advantage requires fast time to market and relentless experimentation, organizations that are\n\nunable to replicate these outcomes are destined to lose in the\n\nmarketplace to more nimble competitors and could potentially go out of business entirely, much like the manufacturing\n\norganizations that did not adopt Lean principles.\n\nThese days, regardless of what industry we are competing in, the way we acquire customers and deliver value to them is dependent\n\non the technology value stream. Put even more succinctly, as\n\nJeffrey Immelt, CEO of General Electric, stated, “Every industry\n\nand company that is not bringing software to the core of their\n\nbusiness will be disrupted.” Or as Jeffrey Snover, Technical Fellow at Microsoft, said, “In previous economic eras, businesses created\n\nvalue by moving atoms. Now they create value by moving bits.”\n\nIt’s difficult to overstate the enormity of this problem—it affects every organization, independent of the industry we operate in, the\n\nsize of our organization, whether we are profit or non-profit. Now more than ever, how technology work is managed and performed\n\npredicts whether our organizations will win in the marketplace, or\n\neven survive. In many cases, we will need to adopt principles and practices that look very different from those that have successfully\n\nguided us over the past decades. See Appendix 1.\n\nNow that we have established the urgency of the problem that DevOps solves, let us take some time to explore in more detail the\n\nsymptomatology of the problem, why it occurs, and why, without dramatic intervention, the problem worsens over time.\n\nTHE PROBLEM: SOMETHING IN YOUR ORGANIZATION MUST NEED IMPROVEMENT (OR YOU WOULDN’T BE READING THIS BOOK)\n\nMost organizations are not able to deploy production changes in\n\nminutes or hours, instead requiring weeks or months. Nor are they able to deploy hundreds or thousands of changes into production per day; instead, they struggle to deploy monthly or",
      "page_number": 12
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 20-27)",
      "start_page": 20,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "even quarterly. Nor are production deployments routine, instead involving outages and chronic firefighting and heroics.\n\nIn an age where competitive advantage requires fast time to market, high service levels, and relentless experimentation, these organizations are at a significant competitive disadvantage. This is\n\nin large part due to their inability to resolve a core, chronic conflict within their technology organization.\n\nTHE CORE, CHRONIC CONFLICT\n\nIn almost every IT organization, there is an inherent conflict between Development and IT Operations which creates a\n\ndownward spiral, resulting in ever-slower time to market for new products and features, reduced quality, increased outages, and, worst of all, an ever-increasing amount of technical debt.\n\nThe term “technical debt” was first coined by Ward Cunningham. Analogous to financial debt, technical debt describes how decisions we make lead to problems that get increasingly more\n\ndifficult to fix over time, continually reducing our available options in the future—even when taken on judiciously, we still incur interest.\n\nOne factor that contributes to this is the often competing goals of Development and IT Operations. IT organizations are responsible for many things. Among them are the two following goals, which\n\nmust be pursued simultaneously:\n\nRespond to the rapidly changing competitive landscape\n\nProvide stable, reliable, and secure service to the customer\n\nFrequently, Development will take responsibility for responding to changes in the market, deploying features and changes into\n\nproduction as quickly as possible. IT Operations will take responsibility for providing customers with IT service that is stable, reliable, and secure, making it difficult or even impossible\n\nfor anyone to introduce production changes that could jeopardize production. Configured this way, Development and IT Operations have diametrically opposed goals and incentives.\n\nDr. Eliyahu M. Goldratt, one of the founders of the manufacturing management movement, called these types of configuration “the core, chronic conflict”—when organizational measurements and incentives across different silos prevent the achievement of global, organizational goals.‡\n\nThis conflict creates a downward spiral so powerful it prevents the achievement of desired business outcomes, both inside and\n\noutside the IT organization. These chronic conflicts often put technology workers into situations that lead to poor software and service quality, and bad customer outcomes, as well as a daily\n\nneed for workarounds, firefighting, and heroics, whether in Product Management, Development, QA, IT Operations, or Information Security. See Appendix 2.\n\nDOWNWARD SPIRAL IN THREE ACTS\n\nThe downward spiral in IT has three acts that are likely familiar to\n\nmost IT practitioners.\n\nThe first act begins in IT Operations, where our goal is to keep applications and infrastructure running so that our organization\n\ncan deliver value to customers. In our daily work, many of our\n\nproblems are due to applications and infrastructure that are complex, poorly documented, and incredibly fragile. This is the\n\ntechnical debt and daily workarounds that we live with constantly, always promising that we’ll fix the mess when we have a little more time. But that time never comes.\n\nAlarmingly, our most fragile artifacts support either our most important revenue-generating systems or our most critical projects. In other words, the systems most prone to failure are also our most important and are at the epicenter of our most urgent\n\nchanges. When these changes fail, they jeopardize our most important organizational promises, such as availability to customers, revenue goals, security of customer data, accurate\n\nfinancial reporting, and so forth.\n\nThe second act begins when somebody has to compensate for the latest broken promise—it could be a product manager promising a\n\nbigger, bolder feature to dazzle customers with or a business executive setting an even larger revenue target. Then, oblivious to what technology can or can’t do, or what factors led to missing our\n\nearlier commitment, they commit the technology organization to deliver upon this new promise.\n\nAs a result, Development is tasked with another urgent project\n\nthat inevitably requires solving new technical challenges and cutting corners to meet the promised release date, further adding to our technical debt—made, of course, with the promise that we’ll fix any resulting problems when we have a little more time.\n\nThis sets the stage for the third and final act, where everything becomes just a little more difficult, bit by bit—everybody gets a\n\nlittle busier, work takes a little more time, communications\n\nbecome a little slower, and work queues get a little longer. Our work becomes more tightly-coupled, smaller actions cause bigger failures, and we become more fearful and less tolerant of making\n\nchanges. Work requires more communication, coordination, and approvals; teams must wait just a little longer for their dependent work to get done; and our quality keeps getting worse. The wheels\n\nbegin grinding slower and require more effort to keep turning. See Appendix 3.\n\nAlthough it’s difficult to see in the moment, the downward spiral is\n\nobvious when one takes a step back. We notice that production code deployments are taking ever-longer to complete, moving from minutes to hours to days to weeks. And worse, the\n\ndeployment outcomes have become even more problematic, that resulting in an ever-increasing number of customer-impacting outages that require more heroics and firefighting in Operations, further depriving them of their ability to pay down technical debt.\n\nAs a result, our product delivery cycles continue to move slower and slower, fewer projects are undertaken, and those that are, are less ambitious. Furthermore, the feedback on everyone’s work\n\nbecomes slower and weaker, especially the feedback signals from our customers. And, regardless of what we try, things seem to get worse—we are no longer able to respond quickly to our changing\n\ncompetitive landscape, nor are we able to provide stable, reliable service to our customers. As a result, we ultimately lose in the marketplace.\n\nTime and time again, we learn that when IT fails, the entire organization fails. As Steven J. Spear noted in his book The High- Velocity Edge, whether the damages “unfold slowly like a wasting\n\ndisease” or rapidly “like a fiery crash...the destruction can be just as complete.”\n\nWHY DOES THIS DOWNWARD SPIRAL HAPPEN EVERYWHERE?\n\nFor over a decade, the authors of this book have observed this destructive spiral occur in countless organizations of all types and\n\nsizes. We understand better than ever why this downward spiral occurs and why it requires DevOps principles to mitigate. First, as described earlier, every IT organization has two opposing goals, and second, every company is a technology company, whether\n\nthey know it or not.\n\nAs Christopher Little, a software executive and one of the earliest chroniclers of DevOps, said, “Every company is a technology\n\ncompany, regardless of what business they think they’re in. A bank is just an IT company with a banking license.”§\n\nTo convince ourselves that this is the case, consider that the vast majority of capital projects have some reliance upon IT. As the saying goes, “It is virtually impossible to make any business decision that doesn’t result in at least one IT change.”\n\nIn the business and finance context, projects are critical because they serve as the primary mechanism for change inside organizations. Projects are typically what management needs to\n\napprove, budget for, and be held accountable for; therefore, they are the mechanism that achieve the goals and aspirations of the organization, whether it is to grow or even shrink.¶\n\nProjects are typically funded through capital spending (i.e., factories, equipment, and major projects, and expenditures are\n\ncapitalized when payback is expected to take years), of which 50% is now technology related. This is even true in “low tech” industry verticals with the lowest historical spending on technology, such\n\nas energy, metal, resource extraction, automotive, and construction. In other words, business leaders are far more reliant upon the effective management of IT in order to achieve their goals than they think.**\n\nTHE COSTS: HUMAN AND ECONOMIC\n\nWhen people are trapped in this downward spiral for years, especially those who are downstream of Development, they often feel stuck in a system that pre-ordains failure and leaves them powerless to change the outcomes. This powerlessness is often\n\nfollowed by burnout, with the associated feelings of fatigue, cynicism, and even hopelessness and despair.\n\nMany psychologists assert that creating systems that cause feelings of powerlessness is one of the most damaging things we can do to fellow human beings—we deprive other people of their ability to control their own outcomes and even create a culture\n\nwhere people are afraid to do the right thing because of fear of punishment, failure, or jeopardizing their livelihood. This can create the conditions of learned helplessness, where people\n\nbecome unwilling or unable to act in a way that avoids the same problem in the future.\n\nFor our employees, it means long hours, working on weekends,\n\nand a decreased quality of life, not just for the employee, but for everyone who depends on them, including family and friends. It is\n\nnot surprising that when this occurs, we lose our best people\n\n(except for those that feel like they can’t leave, because of a sense\n\nof duty or obligation).\n\nIn addition to the human suffering that comes with the current way of working, the opportunity cost of the value that we could be\n\ncreating is staggering—the authors believe that we are missing out\n\non approximately $2.6 trillion of value creation per year, which is, at the time of this writing, equivalent to the annual economic\n\noutput of France, the sixth-largest economy in the world.\n\nConsider the following calculation—both IDC and Gartner estimated that in 2011, approximately 5% of the worldwide gross\n\ndomestic product($3.1 trillion) was spent on IT (hardware,\n\nservices, and telecom). If we estimate that 50% of that $3.1 trillion was spent on operating costs and maintaining existing systems,\n\nand that one-third of that 50% was spent on urgent and\n\nunplanned work or rework, approximately $520 billion was wasted.\n\nIf adopting DevOps could enable us, through better management\n\nand increased operational excellence, to halve that waste and redeploy that human potential into something that’s five times the\n\nvalue (a modest proposal), we could create $2.6 trillion of value\n\nper year.\n\nTHE ETHICS OF DEVOPS: THERE IS A BETTER WAY\n\nIn the previous sections, we described the problems and the\n\nnegative consequences of the status quo due to the core, chronic\n\nconflict, from the inability to achieve organizational goals, to the\n\ndamage we inflict on fellow human beings. By solving these\n\nproblems, DevOps astonishingly enables us to simultaneously\n\nimprove organizational performance, achieve the goals of all the various functional technology roles (e.g., Development, QA, IT\n\nOperations, Infosec), and improve the human condition.\n\nThis exciting and rare combination may explain why DevOps has generated so much excitement and enthusiasm in so many in such\n\na short time, including technology leaders, engineers, and much of\n\nthe software ecosystem we reside in.\n\nBREAKING THE DOWNWARD SPIRAL WITH DEVOPS\n\nIdeally, small teams of developers independently implement their features, validate their correctness in production-like\n\nenvironments, and have their code deployed into production\n\nquickly, safely and securely. Code deployments are routine and predictable. Instead of starting deployments at midnight on Friday\n\nand spending all weekend working to complete them, deployments occur throughout the business day when everyone is\n\nalready in the office and without our customers even noticing—\n\nexcept when they see new features and bug fixes that delight them. And, by deploying code in the middle of the workday, for the first\n\ntime in decades IT Operations is working during normal business\n\nhours like everyone else.\n\nBy creating fast feedback loops at every step of the process,\n\neveryone can immediately see the effects of their actions.\n\nWhenever changes are committed into version control, fast automated tests are run in production-like environments, giving\n\ncontinual assurance that the code and environments operate as\n\ndesigned and are always in a secure and deployable state.",
      "page_number": 20
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 28-35)",
      "start_page": 28,
      "end_page": 35,
      "detection_method": "topic_boundary",
      "content": "Automated testing helps developers discover their mistakes\n\nquickly (usually within minutes), which enables faster fixes as well\n\nas genuine learning—learning that is impossible when mistakes are discovered six months later during integration testing, when\n\nmemories and the link between cause and effect have long faded. Instead of accruing technical debt, problems are fixed as they are\n\nfound, mobilizing the entire organization if needed, because global\n\ngoals outweigh local goals.\n\nPervasive production telemetry in both our code and production environments ensure that problems are detected and corrected\n\nquickly, confirming that everything is working as intended and customers are getting value from the software we create.\n\nIn this scenario, everyone feels productive—the architecture\n\nallows small teams to work safely and architecturally decoupled from the work of other teams who use self-service platforms that\n\nleverage the collective experience of Operations and Information\n\nSecurity. Instead of everyone waiting all the time, with large amounts of late, urgent rework, teams work independently and\n\nproductively in small batches, quickly and frequently delivering\n\nnew value to customers.\n\nEven high-profile product and feature releases become routine by\n\nusing dark launch techniques. Long before the launch date, we put\n\nall the required code for the feature into production, invisible to everyone except internal employees and small cohorts of real\n\nusers, allowing us to test and evolve the feature until it achieves\n\nthe desired business goal.\n\nAnd, instead of firefighting for days or weeks to make the new\n\nfunctionality work, we merely change a feature toggle or\n\nconfiguration setting. This small change makes the new feature\n\nvisible to ever-larger segments of customers, automatically rolling\n\nback if something goes wrong. As a result, our releases are controlled, predictable, reversible, and low stress.\n\nIt’s not just feature releases that are calmer—all sorts of problems\n\nare being found and fixed early, when they are smaller, cheaper, and easier to correct. With every fix, we also generate\n\norganizational learnings, enabling us to prevent the problem from recurring and enabling us to detect and correct similar problems\n\nfaster in the future.\n\nFurthermore, everyone is constantly learning, fostering a\n\nhypothesis-driven culture where the scientific method is used to ensure nothing is taken for granted—we do nothing without\n\nmeasuring and treating product development and process improvement as experiments.\n\nBecause we value everyone’s time, we don’t spend years building\n\nfeatures that our customers don’t want, deploying code that doesn’t work, or fixing something that isn’t actually the cause of\n\nour problem.\n\nBecause we care about achieving goals, we create long-term teams that are responsible for meeting them. Instead of project teams\n\nwhere developers are reassigned and shuffled around after each\n\nrelease, never receiving feedback on their work, we keep teams intact so they can keep iterating and improving, using those\n\nlearnings to better achieve their goals. This is equally true for the\n\nproduct teams who are solving problems for our external customers, as well as our internal platform teams who are helping\n\nother teams be more productive, safe, and secure.\n\nInstead of a culture of fear, we have a high-trust, collaborative\n\nculture, where people are rewarded for taking risks. They are able to fearlessly talk about problems as opposed to hiding them or\n\nputting them on the backburner—after all, we must see problems\n\nin order to solve them.\n\nAnd, because everyone fully owns the quality of their work, everyone builds automated testing into their daily work and uses\n\npeer reviews to gain confidence that problems are addressed long before they can impact a customer. These processes mitigate risk,\n\nas opposed to approvals from distant authorities, allowing us to\n\ndeliver value quickly, reliably, and securely—even proving to skeptical auditors that we have an effective system of internal\n\ncontrols.\n\nAnd when something does go wrong, we conduct blameless post- mortems, not to punish anyone, but to better understand what\n\ncaused the accident and how to prevent it. This ritual reinforces\n\nour culture of learning. We also hold internal technology conferences to elevate our skills and ensure that everyone is\n\nalways teaching and learning.\n\nBecause we care about quality, we even inject faults into our production environment so we can learn how our system fails in a\n\nplanned manner. We conduct planned exercises to practice large-\n\nscale failures, randomly kill processes and compute servers in production, and inject network latencies and other nefarious acts\n\nto ensure we grow ever more resilient. By doing this, we enable\n\nbetter resilience, as well as organizational learning and improvement.\n\nIn this world, everyone has ownership in their work, regardless of\n\ntheir role in the technology organization They have confidence that their work matters and is meaningfully contributing to\n\norganizational goals, proven by their low-stress work environment\n\nand their organization’s success in the marketplace. Their proof is that the organization is indeed winning in the marketplace.\n\nTHE BUSINESS VALUE OF DEVOPS\n\nWe have decisive evidence of the business value of DevOps. From\n\n2013 through 2016, as part of Puppet Labs’ State Of DevOps\n\nReport, to which authors Jez Humble and Gene Kim contributed, we collected data from over twenty-five thousand technology\n\nprofessionals, with the goal of better understanding the health and\n\nhabits of organizations at all stages of DevOps adoption.\n\nThe first surprise this data revealed was how much high-\n\nperforming organizations using DevOps practices were\n\noutperforming their non–high performing peers in the following areas:\n\nThroughput metrics\n\nCode and change deployments (thirty times more frequent)\n\nCode and change deployment lead time (two hundred times faster)\n\nReliability metrics\n\nProduction deployments (sixty times higher change success rate)\n\nMean time to restore service (168 times faster)\n\nOrganizational performance metrics\n\nProductivity, market share, and profitability goals (two times more likely to exceed)\n\nMarket capitalization growth (50% higher over three years)\n\nIn other words, high performers were both more agile and more\n\nreliable, providing empirical evidence that DevOps enables us to break the core, chronic conflict. High performers deployed code\n\nthirty times more frequently, and the time required to go from\n\n“code committed” to “successfully running in production” was two hundred times faster—high performers had lead times measured\n\nin minutes or hours, while low performers had lead times measured in weeks, months, or even quarters.\n\nFurthermore, high performers were twice as likely to exceed\n\nprofitability, market share, and productivity goals. And, for those\n\norganizations that provided a stock ticker symbol, we found that high performers had 50% higher market capitalization growth\n\nover three years. They also had higher employee job satisfaction, lower rates of employee burnout, and their employees were 2.2\n\ntimes more likely to recommend their organization to friends as a great place to work.†† High performers also had better information security outcomes. By integrating security objectives into all stages\n\nof the development and operations processes, they spent 50% less\n\ntime remediating security issues.\n\nDEVOPS HELPS SCALE DEVELOPER PRODUCTIVITY\n\nWhen we increase the number of developers, individual developer productivity often significantly decreases due to communication,\n\nintegration, and testing overhead. This is highlighted in the\n\nfamous book by Frederick Brook, The Mythical Man-Month,\n\nwhere he explains that when projects are late, adding more developers not only decreases individual developer productivity\n\nbut also decreases overall productivity.\n\nOn the other hand, DevOps shows us that when we have the right architecture, the right technical practices, and the right cultural\n\nnorms, small teams of developers are able to quickly, safely, and\n\nindependently develop, integrate, test, and deploy changes into production. As Randy Shoup, formerly a director of engineering at\n\nGoogle, observed, large organizations using DevOps “have\n\nthousands of developers, but their architecture and practices enable small teams to still be incredibly productive, as if they were\n\na startup.”\n\nThe 2015 State of DevOps Report examined not only “deploys per day” but also “deploys per day per developer.” We hypothesized\n\nthat high performers would be able to scale their number of\n\ndeployments as team sizes grew.\n\nFigure 1. Deployments/day vs. number of developers (Source: Puppet Labs, 2015 State Of DevOps Report.)‡‡\n\nIndeed, this is what we found. Figure 1 shows that in low\n\nperformers, deploys per day per developer go down as team size increases, stays constant for medium performers, and increases\n\nlinearly for high performers.\n\nIn other words, organizations adopting DevOps are able to linearly increase the number of deploys per day as they increase their\n\nnumber of developers, just as Google, Amazon, and Netflix have done.§§\n\nTHE UNIVERSALITY OF THE SOLUTION\n\nOne of the most influential books in the Lean manufacturing movement is The Goal: A Process of Ongoing Improvement\n\nwritten by Dr. Eliyahu M. Goldratt in 1984. It influenced an entire\n\ngeneration of professional plant managers around the world. It was a novel about a plant manager who had to fix his cost and\n\nproduct due date issues in ninety days, otherwise his plant would\n\nbe shut down.\n\nLater in his career, Dr. Goldratt described the letters he received\n\nin response to The Goal. These letters would typically read, “You\n\nhave obviously been hiding in our factory, because you’ve described my life [as a plant manager] exactly…” Most\n\nimportantly, these letters showed people were able to replicate the\n\nbreakthroughs in performance that were described in the book in their own work environments.\n\nThe Phoenix Project: A Novel About IT, DevOps, and Helping\n\nYour Business Win, written by Gene Kim, Kevin Behr, and George Spafford in 2013, was closely modeled after The Goal. It is a novel\n\nthat follows an IT leader who faces all the typical problems that\n\nare endemic in IT organizations: an over-budget, behind-schedule\n\nproject that must get to market in order for the company to survive. He experiences catastrophic deployments; problems with\n\navailability, security, and compliance; and so forth. Ultimately, he\n\nand his team use DevOps principles and practices to overcome those challenges, helping their organization win in the\n\nmarketplace. In addition, the novel shows how DevOps practices\n\nimproved the workplace environment for the team, creating lower stress and higher satisfaction because of greater practitioner\n\ninvolvement throughout the process.\n\nAs with The Goal, there is tremendous evidence of the universality of the problems and solutions described in The Phoenix Project.\n\nConsider some of the statements found in the Amazon reviews: “I find myself relating to the characters in The Phoenix Project...I’ve\n\nprobably met most of them over the course of my career,” “If you\n\nhave ever worked in any aspect of IT, DevOps, or Infosec you will definitely be able to relate to this book,” or “There’s not a\n\ncharacter in The Phoenix Project that I don’t identify with myself\n\nor someone I know in real life… not to mention the problems faced and overcome by those characters.”\n\nIn the remainder of this book, we will describe how to replicate the\n\ntransformation described in The Phoenix Project, as well provide many case studies of how other organizations have used DevOps\n\nprinciples and practices to replicate those outcomes.\n\nTHE DEVOPS HANDBOOK: AN ESSENTIAL GUIDE",
      "page_number": 28
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 36-44)",
      "start_page": 36,
      "end_page": 44,
      "detection_method": "topic_boundary",
      "content": "The purpose of the DevOps Handbook is to give you the theory,\n\nprinciples, and practices you need to successfully start your DevOps initiative and achieve your desired outcomes. This\n\nguidance is based on decades of sound management theory, study\n\nof high-performing technology organizations, work we have done helping organizations transform, and research that validates the\n\neffectiveness of the prescribed DevOps practices. As well as interviews with relevant subject matter experts and analyses of\n\nnearly one hundred case studies presented at the DevOps\n\nEnterprise Summit.\n\nBroken into six parts, this book covers DevOps theories and\n\nprinciples using the Three Ways, a specific view of the\n\nunderpinning theory originally introduced in The Phoenix Project. The DevOps Handbook is for everyone who performs or influences\n\nwork in the technology value stream (which typically includes\n\nProduct Management, Development, QA, IT Operations, and Information Security), as well as for business and marketing\n\nleadership, where most technology initiatives originate.\n\nThe reader is not expected to have extensive knowledge of any of these domains, or of DevOps, Agile, ITIL, Lean, or process\n\nimprovement. Each of these topics is introduced and explained in\n\nthe book as it becomes necessary.\n\nOur intent is to create a working knowledge of the critical concepts\n\nin each of these domains, both to serve as a primer and to\n\nintroduce the language necessary to help practitioners work with all their peers across the entire IT value stream, and to frame\n\nshared goals.\n\nThis book will be of value to business leaders and stakeholders\n\nwho are increasingly reliant upon the technology organization for the achievement of their goals.\n\nFurthermore, this book is intended for readers whose\n\norganizations might not be experiencing all the problems described in the book (e.g., long deployment lead times or painful\n\ndeployments). Even readers in this fortunate position will benefit\n\nfrom understanding DevOps principles, especially those relating to shared goals, feedback, and continual learning.\n\nIn Part I, we present a brief history of DevOps and introduce the\n\nunderpinning theory and key themes from relevant bodies of knowledge that span over decades. We then present the high level\n\nprinciples of the Three Ways: Flow, Feedback, and Continual Learning and Experimentaion.\n\nPart II describes how and where to start, and presents concepts such as value streams, organizational design principles and\n\npatterns, organizational adoption patterns, and case studies.\n\nPart III describes how to accelerate Flow by building the foundations of our deployment pipeline: enabling fast and\n\neffective automated testing, continuous integration, continuous\n\ndelivery, and architecting for low-risk releases.\n\nPart IV discusses how to accelerate and amplify Feedback by\n\ncreating effective production telemetry to see and solve problems,\n\nbetter anticipate problems and achieve goals, enable feedback so\n\nthat Dev and Ops can safely deploy changes, integrate A/B testing\n\ninto our daily work, and create review and coordination processes\n\nto increase the quality of our work.\n\nPart V describes how we accelerate Continual Learning by\n\nestablishing a just culture, converting local discoveries into global\n\nimprovements, and properly reserving time to create organizational learning and improvements.\n\nFinally, in Part VI we describe how to properly integrate security\n\nand compliance into our daily work, by integrating preventative security controls into shared source code repositories and services,\n\nintegrating security into our deployment pipeline, enhancing\n\ntelemetry to better enable detection and recovery, protecting the\n\ndeployment pipeline, and achieving change management\n\nobjectives.\n\nBy codifying these practices, we hope to accelerate the adoption of\n\nDevOps practices, increase the success of DevOps initiatives, and\n\nlower the activation energy required for DevOps transformations.\n\n† This is just a small sample of the problems found in typical IT organizations.\n\n‡ In the manufacturing realm, a similar core, chronic conflict existed: the need to simultaneously ensure on-time shipments to customers and control costs. How this core, chronic conflict was broken is described in Appendix 2.\n\n§ In 2013, the European bank HSBC employed more software developers than Google.\n\n¶ For now, let us suspend the discussion of whether software should be funded as a “project” or a “product.” This\n\nis discussed later in the book.\n\n** For instance, Dr. Vernon Richardson and his colleagues published this astonishing finding. They studied the\n\n10-K SEC filings of 184 public corporations and divided them into three groups: A) firms with material weaknesses with IT-related deficiencies, B) firms with material weaknesses with no IT-related deficiencies, and C) “clean firms” with no material weaknesses. Firms in Group A saw eight times higher CEO turnover than Group C, and there was four times higher CFO turnover in Group A than in Group C. Clearly, IT may matter far more than we typically think.\n\n†† As measured by employee Net Promoter Score (eNPS). This is a significant finding, as research has shown that\n\n“companies with highly engaged workers grew revenues two and a half times as much as those with low engagement levels. And [publicly traded] stocks of companies with a high-trust work environment outperformed market indexes by a factor of three from 1997 through 2011.”\n\n‡‡ Only organizations that are deploying at least once per day are shown.\n\n§§ Another more extreme example is Amazon. In 2011, Amazon was performing approximately seven thousand\n\ndeploys per day. By 2015, they were performing 130,000 deploys per day.\n\nPart\n\nIntroduction\n\nIn Part I of The DevOps Handbook, we will explore how the\n\nconvergence of several important movements in management and\n\ntechnology set the stage for the DevOps movement. We describe\n\nvalue streams, how DevOps is the result of applying Lean principles to the technology value stream, and the Three Ways:\n\nFlow, Feedback, and Continual Learning and Experimentation.\n\nPrimary focuses within these chapters include:\n\nThe principles of Flow, which accelerate the delivery of work\n\nfrom Development to Operations to our customers\n\nThe principles of Feedback, which enable us to create ever safer systems of work\n\nThe principles of Continual Learning and Experimentation foster a high-trust culture and a scientific approach to\n\norganizational improvement risk-taking as part of our daily work\n\nA BRIEF HISTORY\n\nDevOps and its resulting technical, architectural, and cultural\n\npractices represent a convergence of many philosophical and\n\nmanagement movements. While many organizations have\n\ndeveloped these principles independently, understanding that\n\nDevOps resulted from a broad stroke of movements, a\n\nphenomenon described by John Willis (one of the co-authors of\n\nthis book) as the “convergence of DevOps,” shows an amazing\n\nprogression of thinking and improbable connections. There are\n\ndecades of lessons learned from manufacturing, high reliability\n\norganization, high-trust management models, and others that\n\nhave brought us to the DevOps practices we know today.\n\nDevOps is the outcome of applying the most trusted principles from the domain of physical manufacturing and leadership to the\n\nIT value stream. DevOps relies on bodies of knowledge from Lean, Theory of Constraints, the Toyota Production System, resilience engineering, learning organizations, safety culture, human factors, and many others. Other valuable contexts that DevOps draws from include high-trust management cultures, servant leadership,\n\nand organizational change management. The result is world-class quality, reliability, stability, and security at ever lower cost and effort; and accelerated flow and reliability throughout the technology value stream, including Product Management, Development, QA, IT Operations, and Infosec.\n\nWhile the foundation of DevOps can be seen as being derived from Lean, the Theory of Constraints, and the Toyota Kata movement, many also view DevOps as the logical continuation of the Agile\n\nsoftware journey that began in 2001.\n\nTHE LEAN MOVEMENT\n\nTechniques such as Value Stream Mapping, Kanban Boards, and\n\nTotal Productive Maintenance were codified for the Toyota\n\nProduction System in the 1980s. In 1997, the Lean Enterprise\n\nInstitute started researching applications of Lean to other value\n\nstreams, such as the service industry and healthcare.\n\nTwo of Lean’s major tenets include the deeply held belief that\n\nmanufacturing lead time required to convert raw materials into\n\nfinished goods was the best predictor of quality, customer\n\nsatisfaction, and employee happiness, and that one of the best\n\npredictors of short lead times was small batch sizes of work.\n\nLean principles focus on how to create value for the customer through systems thinking by creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual.\n\nTHE AGILE MANIFESTO\n\nThe Agile Manifesto was created in 2001 by seventeen of the leading thinkers in software development. They wanted to create a lightweight set of values and principles against heavyweight software development processes such as waterfall development,\n\nand methodologies such as the Rational Unified Process.\n\nOne key principle was to “deliver working software frequently,\n\nfrom a couple of weeks to a couple of months, with a preference to the shorter timescale,” emphasizing the desire for small batch sizes, incremental releases instead of large, waterfall releases. Other principles emphasized the need for small, self-motivated teams, working in a high-trust management model.\n\nAgile is credited for dramatically increasing the productivity of many development organizations. And interestingly, many of the\n\nkey moments in DevOps history also occurred within the Agile community or at Agile conferences, as described below.\n\nAGILE INFRASTRUCTURE AND VELOCITY MOVEMENT\n\nAt the 2008 Agile conference in Toronto, Canada, Patrick Debois and Andrew Schafer held a “birds of a feather” session on applying\n\nAgile principles to infrastructure as opposed to application code. Although they were the only people who showed up, they rapidly\n\ngained a following of like-minded thinkers, including co-author\n\nJohn Willis.\n\nLater, at the 2009 Velocity conference, John Allspaw and Paul Hammond gave the seminal “10 Deploys per Day: Dev and Ops\n\nCooperation at Flickr” presentation, where they described how they created shared goals between Dev and Ops and used\n\ncontinuous integration practices to make deployment part of everyone’s daily work. According to first hand accounts, everyone\n\nattending the presentation immediately knew they were in the\n\npresence of something profound and of historic significance.\n\nPatrick Debois was not there, but was so excited by Allspaw and Hammond’s idea that he created the first DevOpsDays in Ghent,\n\nBelgium, (where he lived) in 2009. There the term “DevOps” was coined.\n\nTHE CONTINUOUS DELIVERY MOVEMENT\n\nBuilding upon the development discipline of continuous build,\n\ntest, and integration, Jez Humble and David Farley extended the concept to continuous delivery, which defined the role of a\n\n“deployment pipeline” to ensure that code and infrastructure are always in a deployable state, and that all code checked in to trunk\n\ncan be safely deployed into production. This idea was first presented at the 2006 Agile conference, and was also\n\nindependently developed in 2009 by Tim Fitz in a blog post on his website titled “Continuous Deployment.”¶¶\n\nTOYOTA KATA\n\nIn 2009, Mike Rother wrote Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results, which framed\n\nhis twenty-year journey to understand and codify the Toyota Production System. He had been one of the graduate students who\n\nflew with GM executives to visit Toyota plants and helped develop\n\nthe Lean toolkit, but he was puzzled when none of the companies adopting these practices replicated the level of performance\n\nobserved at the Toyota plants.\n\nHe concluded that the Lean community missed the most important practice of all, which he called the improvement kata.\n\nHe explains that every organization has work routines, and the improvement kata requires creating structure for the daily,\n\nhabitual practice of improvement work, because daily practice is\n\nwhat improves outcomes. The constant cycle of establishing desired future states, setting weekly target outcomes, and the\n\ncontinual improvement of daily work is what guided improvement at Toyota.\n\nThe above describes the history of DevOps and relevant\n\nmovements that it draws upon. Throughout the rest of Part I, we look at value streams, how Lean principles can be applied to the\n\ntechnology value stream, and the Three Ways of Flow, Feedback,\n\nand Continual Learning and Experimentation.\n\n¶¶ DevOps also extends and builds upon the practices of infrastructure as code, which was pioneered by Dr. Mark Burgess, Luke Kanies, and Adam Jacob. In infrastructure as code, the work of Operations is automated and treated like application code, so that modern development practices can be applied to the entire development stream. This further enabled fast deployment flow, including continuous integration (pioneered by Grady Booch and integrated as one of the key 12 practices of Extreme Programming), continuous delivery (pioneered by Jez Humble and David Farley), and continuous deployment (pioneered by Etsy, Wealthfront, and Eric Ries’s work at IMVU).",
      "page_number": 36
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 45-53)",
      "start_page": 45,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "1 Agile, Continuous\n\nDelivery, and the Three Ways\n\nIn this chapter, an introduction to the underpinning theory of\n\nLean Manufacturing is presented, as well as the Three Ways, the\n\nprinciples from which all of the observed DevOps behaviors can be\n\nderived.\n\nOur focus here is primarily on theory and principles, describing\n\nmany decades of lessons learned from manufacturing, high- reliability organizations, high-trust management models, and\n\nothers, from which DevOps practices have been derived. The\n\nresulting concrete principles and patterns, and their practical application to the technology value stream, are presented in the\n\nremaining chapters of the book.\n\nTHE MANUFACTURING VALUE STREAM\n\nOne of the fundamental concepts in Lean is the value stream. We will define it first in the context of manufacturing and then extrapolate how it applies to DevOps and the technology value\n\nstream.\n\nKaren Martin and Mike Osterling define value stream in their\n\nbook Value Stream Mapping: How to Visualize Work and Align\n\nLeadership for Organizational Transformation as “the sequence\n\nof activities an organization undertakes to deliver upon a customer\n\nrequest,” or “the sequence of activities required to design,\n\nproduce, and deliver a good or service to a customer, including the\n\ndual flows of information and material.”\n\nIn manufacturing operations, the value stream is often easy to see\n\nand observe: it starts when a customer order is received and the\n\nraw materials are released onto the plant floor. To enable fast and\n\npredictable lead times in any value stream, there is usually a relentless focus on creating a smooth and even flow of work, using techniques such as small batch sizes, reducing work in process\n\n(WIP), preventing rework to ensure we don’t pass defects to downstream work centers, and constantly optimizing our system toward our global goals.\n\nTHE TECHNOLOGY VALUE STREAM\n\nThe same principles and patterns that enable the fast flow of work in physical processes are equally applicable to technology work\n\n(and, for that matter, for all knowledge work). In DevOps, we typically define our technology value stream as the process required to convert a business hypothesis into a technology- enabled service that delivers value to the customer.\n\nThe input to our process is the formulation of a business objective, concept, idea, or hypothesis, and starts when we accept the work in Development, adding it to our committed backlog of work.\n\nFrom there, Development teams that follow a typical Agile or\n\niterative process will likely transform that idea into user stories\n\nand some sort of feature specification, which is then implemented\n\nin code into the application or service being built. The code is then\n\nchecked in to the version control repository, where each change is\n\nintegrated and tested with the rest of the software system.\n\nBecause value is created only when our services are running in\n\nproduction, we must ensure that we are not only delivering fast\n\nflow, but that our deployments can also be performed without\n\ncausing chaos and disruptions such as service outages, service\n\nimpairments, or security or compliance failures.\n\nFOCUS ON DEPLOYMENT LEAD TIME\n\nFor the remainder of this book, our attention will be on deployment lead time, a subset of the value stream described above. This value stream begins when any engineer*** in our value stream (which includes Development, QA, IT Operations, and Infosec) checks a change into version control and ends when that change is successfully running in production, providing value to\n\nthe customer and generating useful feedback and telemetry.\n\nThe first phase of work that includes Design and Development is\n\nakin to Lean Product Development and is highly variable and highly uncertain, often requiring high degrees of creativity and work that may never be performed again, resulting in high variability of process times. In contrast, the second phase of work, which includes Testing and Operations, is akin to Lean Manufacturing. It requires creativity and expertise, and strives to\n\nbe predictable and mechanistic, with the goal of achieving work\n\noutputs with minimized variability (e.g., short and predictable lead times, near zero defects).\n\nInstead of large batches of work being processed sequentially\n\nthrough the design/development value stream and then through the test/operations value stream (such as when we have a large\n\nbatch waterfall process or long-lived feature branches), our goal is\n\nto have testing and operations happening simultaneously with design/development, enabling fast flow and high quality. This\n\nmethod succeeds when we work in small batches and build quality into every part of our value stream.†††\n\nDefining Lead Time vs. Processing Time\n\nIn the Lean community, lead time is one of two measures\n\ncommonly used to measure performance in value streams, with the other being processing time (sometimes known as touch time or task time).‡‡‡\n\nWhereas the lead time clock starts when the request is made and ends when it is fulfilled, the process time clock starts only when\n\nwe begin work on the customer request—specifically, it omits the time that the work is in queue, waiting to be processed (figure 2).\n\nFigure 2. Lead time vs. process time of a deployment operation\n\nBecause lead time is what the customer experiences, we typically focus our process improvement attention there instead of on\n\nprocess time. However, the proportion of process time to lead time serves as an important measure of efficiency—achieving fast flow\n\nand short lead times almost always requires reducing the time our work is waiting in queues.\n\nThe Common Scenario: Deployment Lead Times Requiring Months\n\nIn business as usual, we often find ourselves in situations where\n\nour deployment lead times require months. This is especially\n\ncommon in large, complex organizations that are working with tightly-coupled, monolithic applications, often with scarce\n\nintegration test environments, long test and production environment lead times, high reliance on manual testing, and\n\nmultiple required approval processes.When this occurs, our value stream may look like figure 3:\n\nFigure 3: A technology value stream with a deployment lead time of three months (Source: Damon Edwards, “DevOps Kaizen,” 2015.)\n\nWhen we have long deployment lead times, heroics are required at\n\nalmost every stage of the value stream. We may discover that nothing works at the end of the project when we merge all the\n\ndevelopment team’s changes together, resulting in code that no\n\nlonger builds correctly or passes any of our tests. Fixing each problem requires days or weeks of investigation to determine who\n\nbroke the code and how it can be fixed, and still results in poor\n\ncustomer outcomes.\n\nOur DevOps Ideal: Deployment Lead Times of Minutes\n\nIn the DevOps ideal, developers receive fast, constant feedback on their work, which enables them to quickly and independently\n\nimplement, integrate, and validate their code, and have the code deployed into the production environment (either by deploying\n\nthe code themselves or by others).\n\nWe achieve this by continually checking small code changes into\n\nour version control repository, performing automated and exploratory testing against it, and deploying it into production.\n\nThis enables us to have a high degree of confidence that our changes will operate as designed in production and that any\n\nproblems can be quickly detected and corrected.\n\nThis is most easily achieved when we have architecture that is modular, well encapsulated, and loosely-coupled so that small\n\nteams are able to work with high degrees of autonomy, with\n\nfailures being small and contained, and without causing global disruptions.\n\nIn this scenario, our deployment lead time is measured in\n\nminutes, or, in the worst case, hours. Our resulting value stream map should look something like figure 4:\n\nFigure 4: A technology value stream with a lead time of minutes\n\nOBSERVING “%C/A” AS A MEASURE OF REWORK\n\nIn addition to lead times and process times, the third key metric in the technology value stream is percent complete and accurate\n\n(%C/A). This metric reflects the quality of the output of each step in our value stream. Karen Martin and Mike Osterling state that “the %C/A can be obtained by asking downstream customers what percentage of the time they receive work that is ‘usable as is,’\n\nmeaning that they can do their work without having to correct the information that was provided, add missing information that should have been supplied, or clarify information that should have\n\nand could have been clearer.”\n\nTHE THREE WAYS: THE PRINCIPLES UNDERPINNING DEVOPS\n\nThe Phoenix Project presents the Three Ways as the set of underpinning principles from which all the observed DevOps behaviors and patterns are derived (figure 5).\n\nThe First Way enables fast left-to-right flow of work from Development to Operations to the customer. In order to maximize flow, we need to make work visible, reduce our batch sizes and\n\nintervals of work, build in quality by preventing defects from being\n\npassed to downstream work centers, and constantly optimize for the global goals.\n\nFigure 5: The Three Ways (Source: Gene Kim, “The Three Ways: The Principles Underpinning DevOps,” IT Revolution Press blog, accessed August 9, 2016, http://itrevolution.com/the-three-ways-principles- underpinning-devops/.)\n\nBy speeding up flow through the technology value stream, we\n\nreduce the lead time required to fulfill internal or customer requests, especially the time required to deploy code into the production environment. By doing this, we increase the quality of\n\nwork as well as our throughput, and boost our ability to out- experiment the competition.\n\nThe resulting practices include continuous build, integration, test,\n\nand deployment processes; creating environments on demand;\n\nlimiting work in process (WIP); and building systems and organizations that are safe to change.\n\nThe Second Way enables the fast and constant flow of feedback from right to left at all stages of our value stream. It requires that we amplify feedback to prevent problems from happening again,\n\nor enable faster detection and recovery. By doing this, we create quality at the source and generate or embed knowledge where it is needed—this allows us to create ever-safer systems of work where problems are found and fixed long before a catastrophic failure\n\noccurs.\n\nBy seeing problems as they occur and swarming them until\n\neffective countermeasures are in place, we continually shorten and amplify our feedback loops, a core tenet of virtually all modern process improvement methodologies. This maximizes the opportunities for our organization to learn and improve.\n\nThe Third Way enables the creation of a generative, high-trust culture that supports a dynamic, disciplined, and scientific approach to experimentation and risk-taking, facilitating the\n\ncreation of organizational learning, both from our successes and failures. Furthermore, by continually shortening and amplifying our feedback loops, we create ever-safer systems of work and are\n\nbetter able to take risks and perform experiments that help us learn faster than our competition and win in the marketplace.\n\nAs part of the Third Way, we also design our system of work so\n\nthat we can multiply the effects of new knowledge, transforming local discoveries into global improvements. Regardless of where someone performs work, they do so with the cumulative and\n\ncollective experience of everyone in the organization.",
      "page_number": 45
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 54-61)",
      "start_page": 54,
      "end_page": 61,
      "detection_method": "topic_boundary",
      "content": "CONCLUSION\n\nIn this chapter, we described the concepts of value streams, lead time as one of the key measures of the effectiveness for both manufacturing and technology value streams, and the high-level\n\nconcepts behind each of the Three Ways, the principles that underpin DevOps.\n\nIn the following chapters, the principles for each of the Three Ways are described in greater detail. The first of these principles is Flow, which is focused on how we create the fast flow of work in any value stream, whether it’s in manufacturing or technology\n\nwork. The practices that enable fast flow are described in Part III.\n\n*** Going forward, engineer refers to anyone working in our value stream, not just developers.\n\n††† In fact, with techniques such as test-driven development, testing occurs even before the first line of code is\n\nwritten.\n\n‡‡‡ In this book, the term process time will be favored for the same reason Karen Martin and Mike Osterling cite: “To minimize confusion, we avoid using the term cycle time as it has several definitions synonymous with processing time and pace or frequency of output, to name a few.”\n\n2 The First Way:\n\nThe Principles of Flow\n\nIn the technology value stream, work typically flows from\n\nDevelopment to Operations, the functional areas between our business and our customers. The First Way requires the fast and\n\nsmooth flow of work from Development to Operations, to deliver value to customers quickly. We optimize for this global goal\n\ninstead of local goals, such as Development feature completion\n\nrates, test find/fix ratios, or Ops availability measures.\n\nWe increase flow by making work visible, by reducing batch sizes\n\nand intervals of work, and by building quality in, preventing\n\ndefects from being passed to downstream work centers. By speeding up the flow through the technology value stream, we\n\nreduce the lead time required to fulfill internal and external customer requests, further increasing the quality of our work\n\nwhile making us more agile and able to out-experiment the competition.\n\nOur goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services. Clues on how we do this in the\n\ntechnology value stream can be gleaned from how the Lean\n\nprinciples were applied to the manufacturing value stream.\n\nMAKE OUR WORK VISIBLE\n\nA significant difference between technology and manufacturing\n\nvalue streams is that our work is invisible. Unlike physical\n\nprocesses, in the technology value stream we cannot easily see\n\nwhere flow is being impeded or when work is piling up in front of\n\nconstrained work centers. Transferring work between work\n\ncenters is usually highly visible and slow because inventory must\n\nbe physically moved.\n\nHowever, in technology work the move can be done with a click of\n\na button, such as by re-assigning a work ticket to another team. Because it is so easy, work can bounce between teams endlessly due to incomplete information, or work can be passed onto downstream work centers with problems that remain completely invisible until we are late delivering what we promised to the customer or our application fails in the production environment.\n\nTo help us see where work is flowing well and where work is queued or stalled, we need to make our work as visible as possible.\n\nOne of the best methods of doing this is using visual work boards, such as kanban boards or sprint planning boards, where we can represent work on physical or electronic cards. Work originates on the left (often being pulled from a backlog), is pulled from work center to work center (represented in columns), and finishes when it reaches the right side of the board, usually in a column labeled “done” or “in production.”\n\nFigure 6: An example kanban board, spanning Requirements, Dev, Test, Staging, and In Production (Source: David J. Andersen and Dominica DeGrandis, Kanban for ITOps, training materials for workshop, 2012.)\n\nNot only does our work become visible, we can also manage our work so that it flows from left to right as quickly as possible. Furthermore, we can measure lead time from when a card is placed on the board to when it is moved into the “Done” column.\n\nIdeally, our kanban board will span the entire value stream, defining work as completed only when it reaches the right side of the board (figure 6). Work is not done when Development\n\ncompletes the implementation of a feature—rather, it is only done when our application is running successfully in production, delivering value to the customer.\n\nBy putting all work for each work center in queues and making it visible, all stakeholders can more easily prioritize work in the context of global goals. Doing this enables each work center to\n\nsingle-task on the highest priority work until it is completed, increasing throughput.\n\nLIMIT WORK IN PROCESS (WIP)\n\nIn manufacturing, daily work is typically dictated by a production schedule that is generated regularly (e.g., daily, weekly),\n\nestablishing which jobs must be run based on customer orders, order due dates, parts available, and so forth.\n\nIn technology, our work is usually far more dynamic—this is\n\nespecially the case in shared services, where teams must satisfy\n\nthe demands of many different stakeholders. As a result, daily work becomes dominated by the priority du jour, often with\n\nrequests for urgent work coming in through every communication mechanism possible, including ticketing systems, outage calls,\n\nemails, phone calls, chat rooms, and management escalations.\n\nDisruptions in manufacturing are also highly visible and costly, often requiring breaking the current job and scrapping any\n\nincomplete work in process to start the new job. This high level of\n\neffort discourages frequent disruptions.\n\nHowever, interrupting technology workers is easy, because the consequences are invisible to almost everyone, even though the\n\nnegative impact to productivity may be far greater than in manufacturing. For instance, an engineer assigned to multiple\n\nprojects must switch between tasks, incurring all the costs of having to re-establish context, as well as cognitive rules and goals.\n\nStudies have shown that the time to complete even simple tasks,\n\nsuch as sorting geometric shapes, significantly degrades when\n\nmultitasking. Of course, because our work in the technology value stream is far more cognitively complex than sorting geometric\n\nshapes, the effects of multitasking on process time is much worse.\n\nWe can limit multitasking when we use a kanban board to manage our work, such as by codifying and enforcing WIP (work in\n\nprogress) limits for each column or work center that puts an upper limit on the number of cards that can be in a column.\n\nFor example, we may set a WIP limit of three cards for testing.\n\nWhen there are already three cards in the test lane, no new cards can be added to the lane unless a card is completed or removed\n\nfrom the “in work” column and put back into queue (i.e., putting\n\nthe card back to the column to the left). Nothing can can be worked on until it is represented first in a work card, reinforcing\n\nthat all work must be made visible.\n\nDominica DeGrandis, one of the leading experts on using kanbans in DevOps value streams, notes that “controlling queue size [WIP]\n\nis an extremely powerful management tool, as it is one of the few leading indicators of lead time—with most work items, we don’t\n\nknow how long it will take until it’s actually completed.”\n\nLimiting WIP also makes it easier to see problems that prevent the completion of work.† For instance, when we limit WIP, we find that we may have nothing to do because we are waiting on\n\nsomeone else. Although it may be tempting to start new work (i.e., “It’s better to be doing something than nothing”), a far better\n\naction would be to find out what is causing the delay and help fix that problem. Bad multitasking often occurs when people are\n\nassigned to multiple projects, resulting in many prioritization\n\nproblems.\n\nIn other words, as David J. Andersen, author of Kanban: Successful Evolutionary Change for Your Technology Business,\n\nquipped, “Stop starting. Start finishing.”\n\nREDUCE BATCH SIZES\n\nAnother key component to creating smooth and fast flow is\n\nperforming work in small batch sizes. Prior to the Lean\n\nmanufacturing revolution, it was common practice to manufacture in large batch sizes (or lot sizes), especially for operations where\n\njob setup or switching between jobs was time-consuming or costly. For example, producing large car body panels requires setting\n\nlarge and heavy dies onto metal stamping machines, a process that could take days. When changeover cost is so expensive, we would\n\noften stamp as many panels at a time as possible, creating large\n\nbatches in order to reduce the number of changeovers.\n\nHowever, large batch sizes result in skyrocketing levels of WIP and high levels of variability in flow that cascade through the entire\n\nmanufacturing plant. The result is long lead times and poor quality—if a problem is found in one body panel, the entire batch\n\nhas to be scrapped.\n\nOne of the key lessons in Lean is that in order to shrink lead times and increase quality, we must strive to continually shrink batch\n\nsizes. The theoretical lower limit for batch size is single-piece flow, where each operation is performed one unit at a time.‡\n\nThe dramatic differences between large and small batch sizes can be seen in the simple newsletter mailing simulation described in\n\nLean Thinking: Banish Waste and Create Wealth in Your Corporation by James P. Womack and Daniel T. Jones.\n\nSuppose in our own example we have ten brochures to send and mailing each brochure requires four steps: fold the paper, insert\n\nthe paper into the envelope, seal the envelope, and stamp the envelope.\n\nThe large batch strategy (i.e., “mass production”) would be to sequentially perform one operation on each of the ten brochures. In other words, we would first fold all ten sheets of paper, then\n\ninsert each of them into envelopes, then seal all ten envelopes, and then stamp them.\n\nOn the other hand, in the small batch strategy (i.e., “single-piece flow”), all the steps required to complete each brochure are performed sequentially before starting on the next brochure. In other words, we fold one sheet of paper, insert it into the envelope,\n\nseal it, and stamp it—only then do we start the process over with the next sheet of paper.\n\nThe difference between using large and small batch sizes is\n\ndramatic (see figure 7). Suppose each of the four operations takes ten seconds for each of the ten envelopes. With the large batch size strategy, the first completed and stamped envelope is produced\n\nonly after 310 seconds.\n\nWorse, suppose we discover during the envelope sealing operation that we made an error in the first step of folding—in this case, the\n\nearliest we would discover the error is at two hundred seconds, and we have to refold and reinsert all ten brochures in our batch again.",
      "page_number": 54
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 62-69)",
      "start_page": 62,
      "end_page": 69,
      "detection_method": "topic_boundary",
      "content": "Figure 7: Simulation of “envelope game” (fold, insert, seal, and stamp the envelope)\n\n(Source: Stefan Luyten, “Single Piece Flow: Why mass production isn’t the most efficient way of doing ‘stuff’,” Medium.com, August 8, 2014, https://medium.com/@stefanluyten/single-piece-flow- 5d2c2bec845b#.9o7sn74ns.)\n\nIn contrast, in the small batch strategy the first completed stamped envelope is produced in only forty seconds, eight times faster than the large batch strategy. And, if we made an error in the first step, we only have to redo the one brochure in our batch.\n\nSmall batch sizes result in less WIP, faster lead times, faster detection of errors, and less rework.\n\nThe negative outcomes associated with large batch sizes are just as relevant to the technology value stream as in manufacturing. Consider when we have an annual schedule for software releases, where an entire year’s worth of code that Development has worked\n\non is released to production deployment.\n\nLike in manufacturing, this large batch release creates sudden, high levels of WIP and massive disruptions to all downstream\n\nwork centers, resulting in poor flow and poor quality outcomes. This validates our common experience that the larger the change going into production, the more difficult the production errors are\n\nto diagnose and fix, and the longer they take to remediate.\n\nIn a post on Startup Lessons Learned, Eric Ries states, “The batch size is the unit at which work-products move between stages in a\n\ndevelopment [or DevOps] process. For software, the easiest batch to see is code. Every time an engineer checks in code, they are batching up a certain amount of work. There are many techniques for controlling these batches, ranging from the tiny batches\n\nneeded for continuous deployment to more traditional branch- based development, where all of the code from multiple developers working for weeks or months is batched up and\n\nintegrated together.”\n\nThe equivalent to single piece flow in the technology value stream is realized with continuous deployment, where each change\n\ncommitted to version control is integrated, tested, and deployed into production. The practices that enable this are described in Part IV.\n\nREDUCE THE NUMBER OF HANDOFFS\n\nIn the technology value stream, whenever we have long\n\ndeployment lead times measured in months, it is often because there are hundreds (or even thousands) of operations required to move our code from version control into the production environment. To transmit code through the value stream requires\n\nmultiple departments to work on a variety of tasks, including functional testing, integration testing, environment creation, server administration, storage administration, networking, load\n\nbalancing, and information security.\n\nEach time the work passes from team to team, we require all sorts of communication: requesting, specifying, signaling, coordinating,\n\nand often prioritizing, scheduling, deconflicting, testing, and verifying. This may require using different ticketing or project management systems; writing technical specification documents;\n\ncommunicating via meetings, emails, or phone calls; and using file system shares, FTP servers, and Wiki pages.\n\nEach of these steps is a potential queue where work will wait when we rely on resources that are shared between different value streams (e.g., centralized operations). The lead times for these requests are often so long that there is constant escalation to have\n\nwork performed within the needed timelines.\n\nEven under the best circumstances, some knowledge is inevitably lost with each handoff. With enough handoffs, the work can\n\ncompletely lose the context of the problem being solved or the organizational goal being supported. For instance, a server administrator may see a newly created ticket requesting that user\n\naccounts be created, without knowing what application or service it’s for, why it needs to be created, what all the dependencies are, or whether it’s actually recurring work.\n\nTo mitigate these types of problems, we strive to reduce the number of handoffs, either by automating significant portions of the work or by reorg-anizing teams so they can deliver value to the\n\ncustomer themselves, instead of having to be constantly dependent on others. As a result, we increase flow by reducing the amount of time that our work spends waiting in queue, as well as the amount of non–value-added time. See Appendix 4.\n\nCONTINUALLY IDENTIFY AND ELEVATE OUR CONSTRAINTS\n\nTo reduce lead times and increase throughput, we need to continually identify our system’s constraints and improve its work capacity. In Beyond the Goal, Dr. Goldratt states, “In any value\n\nstream, there is always a direction of flow, and there is always one and only constraint; any improvement not made at that constraint is an illusion.” If we improve a work center that is positioned\n\nbefore the constraint, work will merely pile up at the bottleneck even faster, waiting for work to be performed by the bottlenecked work center.\n\nOn the other hand, if we improve a work center positioned after the bottleneck, it remains starved, waiting for work to clear the bottleneck. As a solution, Dr. Goldratt defined the “five focusing steps”:\n\nIdentify the system’s constraint.\n\nDecide how to exploit the system’s constraint.\n\nSubordinate everything else to the above decisions.\n\nElevate the system’s constraint.\n\nIf in the previous steps a constraint has been broken, go back to step one, but do not allow inertia to cause a system constraint.\n\nIn typical DevOps transformations, as we progress from deployment lead times measured in months or quarters to lead\n\ntimes measured in minutes, the constraint usually follows this progression:\n\nEnvironment creation: We cannot achieve deployments on- demand if we always have to wait weeks or months for production or test environments. The countermeasure is to\n\ncreate environments that are on demand and completely self- serviced, so that they are always available when we need them.\n\nCode deployment: We cannot achieve deployments on\n\ndemand if each of our production code deployments take weeks or months to perform (i.e., each deployment requires 1,300 manual, error-prone steps, involving up to three hundred\n\nengineers). The countermeasure is to automate our deployments as much as possible, with the goal of being completely automated so they can be done self-service by any developer.\n\nTest setup and run: We cannot achieve deployments on demand if every code deployment requires two weeks to set up our test environments and data sets, and another four weeks to\n\nmanually execute all our regression tests. The countermeasure is to automate our tests so we can execute deployments safely and to parallelize them so the test rate can keep up with our\n\ncode development rate.\n\nOverly tight architecture: We cannot achieve deployments on demand if overly tight architecture means that every time\n\nwe want to make a code change we have to send our engineers to scores of committee meetings in order to get permission to\n\nmake our changes. Our countermeasure is to create more\n\nloosely-coupled architecture so that changes can be made safely and with more autonomy, increasing developer\n\nproductivity.\n\nAfter all these constraints have been broken, our constraint will\n\nlikely be Development or the product owners. Because our goal is\n\nto enable small teams of developers to independently develop, test, and deploy value to customers quickly and reliably, this is\n\nwhere we want our constraint to be. High performers, regardless\n\nof whether an engineer is in Development, QA, Ops, or Infosec, state that their goal is to help maximize developer productivity.\n\nWhen the constraint is here, we are limited only by the number of\n\ngood business hypotheses we create and our ability to develop the code necessary to test these hypotheses with real customers.\n\nThe progression of constraints listed above are generalizations of\n\ntypical transformations—techniques to identify the constraint in actual value streams, such as through value stream mapping and\n\nmeasurements, are described later in this book.\n\nELIMINATE HARDSHIPS AND WASTE IN THE VALUE STREAM\n\nShigeo Shingo, one of the pioneers of the Toyota Production System, believed that waste constituted the largest threat to\n\nbusiness viability—the commonly used definition in Lean is “the\n\nuse of any material or resource beyond what the customer requires and is willing to pay for.” He defined seven major types of\n\nmanufacturing waste: inventory, overproduction, extra\n\nprocessing, transportation, waiting, motion, and defects.\n\nMore modern interpretations of Lean have noted that “eliminating\n\nwaste” can have a demeaning and dehumanizing context; instead,\n\nthe goal is reframed to reduce hardship and drudgery in our daily\n\nwork through continual learning in order to achieve the\n\norganization’s goals. For the remainder of this book, the term\n\nwaste will imply this more modern definition, as it more closely matches the DevOps ideals and desired outcomes.\n\nIn the book Implementing Lean Software Development: From\n\nConcept to Cash, Mary and Tom Poppendieck describe waste and hardship in the software development stream as anything that\n\ncauses delay for the customer, such as activities that can be\n\nbypassed without affecting the result.\n\nThe following categories of waste and hardship come from\n\nImplementing Lean Software Development unless otherwise\n\nnoted:\n\nPartially done work: This includes any work in the value\n\nstream that has not been completed (e.g., requirement\n\ndocuments or change orders not yet reviewed) and work that is sitting in queue (e.g., waiting for QA review or server admin\n\nticket). Partially done work becomes obsolete and loses value\n\nas time progresses.\n\nExtra processes: Any additional work that is being\n\nperformed in a process that does not add value to the\n\ncustomer. This may include documentation not used in a downstream work center, or reviews or approvals that do not\n\nadd value to the output. Extra processes add effort and increase lead times.\n\nExtra features: Features built into the service that are not\n\nneeded by the organization or the customer (e.g., “gold\n\nplating”). Extra features add complexity and effort to testing\n\nand managing functionality.\n\nTask switching: When people are assigned to multiple projects and value streams, requiring them to context switch\n\nand manage dependencies between work, adding additional\n\neffort and time into the value stream.\n\nWaiting: Any delays between work requiring resources to wait\n\nuntil they can complete the current work. Delays increase cycle\n\ntime and prevent the customer from getting value.\n\nMotion: The amount of effort to move information or\n\nmaterials from one work center to another. Motion waste can\n\nbe created when people who need to communicate frequently are not colocated. Handoffs also create motion waste and often\n\nrequire additional communication to resolve ambiguities.\n\nDefects: Incorrect, missing, or unclear information, materials, or products create waste, as effort is needed to resolve these\n\nissues. The longer the time between defect creation and defect\n\ndetection, the more difficult it is to resolve the defect.\n\nNonstandard or manual work: Reliance on nonstandard\n\nor manual work from others, such as using non-rebuilding\n\nservers, test environments, and configurations. Ideally, any dependencies on Operations should be automated, self-\n\nserviced, and available on demand.\n\nHeroics: In order for an organization to achieve goals, individuals and teams are put in a position where they must\n\nperform unreasonable acts, which may even become a part of\n\ntheir daily work (e.g., nightly 2:00 a.m. problems in",
      "page_number": 62
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 70-77)",
      "start_page": 70,
      "end_page": 77,
      "detection_method": "topic_boundary",
      "content": "production, creating hundreds of work tickets as part of every software release).§\n\nOur goal is to make these wastes and hardships—anywhere heroics become necessary—visible, and to systematically do what\n\nis needed to alleviate or eliminate these burdens and hardships to\n\nachieve our goal of fast flow.\n\nCONCLUSION\n\nImproving flow through the technology value stream is essential to\n\nachieving DevOps outcomes. We do this by making work visible, limiting WIP, reducing batch sizes and the number of handoffs,\n\ncontinually identifying and evaluating our constraints, and\n\neliminating hardships in our daily work.\n\nThe specific practices that enable fast flow in the DevOps value\n\nstream are presented in Part IV. In the next chapter, we present\n\nThe Second Way: The Principles of Feedback.\n\n† Taiichi Ohno compared enforcing WIP limits to draining water from the river of inventory in order to reveal all\n\nthe problems that obstruct fast flow.\n\n‡ Also known as “batch size of one” or “1x1 flow,” terms that refer to batch size and a WIP limit of one.\n\n§ Although heroics is not included in the Poppendieck categories of waste, it is included here because of how often\n\nit occurs, especially in Operation shared services.\n\n3 The Second Way:\n\nThe Principles of Feedback\n\nWhile the First Way describes the principles that enable the fast\n\nflow of work from left to right, the Second Way describes the principles that enable the reciprocal fast and constant feedback\n\nfrom right to left at all stages of the value stream. Our goal is to create an ever safer and more resilient system of work.\n\nThis is especially important when working in complex systems,\n\nwhen the earliest opportunity to detect and correct errors is typically when a catastrophic event is underway, such as a\n\nmanufacturing worker being hurt on the job or a nuclear reactor\n\nmeltdown in progress.\n\nIn technology, our work happens almost entirely within complex systems with a high risk of catastrophic consequences. As in\n\nmanufacturing, we often discover problems only when large failures are underway, such as a massive production outage or a security breach resulting in the theft of customer data.\n\nWe make our system of work safer by creating fast, frequent, high quality information flow throughout our value stream and our\n\norganization, which includes feedback and feedforward loops. This\n\nallows us to detect and remediate problems while they are smaller,\n\ncheaper, and easier to fix; avert problems before they cause\n\ncatastrophe; and create organizational learning that we integrate\n\ninto future work. When failures and accidents occur, we treat\n\nthem as opportunities for learning, as opposed to a cause for\n\npunishment and blame. To achieve all of the above, let us first\n\nexplore the nature of complex systems and how they can be made\n\nsafer.\n\nWORKING SAFELY WITHIN COMPLEX SYSTEMS\n\nOne of the defining characteristics of a complex system is that it\n\ndefies any single person’s ability to see the system as a whole and\n\nunderstand how all the pieces fit together. Complex systems typically have a high degree of interconnectedness of tightly- coupled components, and system-level behavior cannot be explained merely in terms of the behavior of the system components.\n\nDr. Charles Perrow studied the Three Mile Island crisis and observed that it was impossible for anyone to understand how the\n\nreactor would behave in all circumstances and how it might fail. When a problem was underway in one component, it was difficult\n\nto isolate from the other components, quickly flowing through the paths of least resistance in unpredictable ways.\n\nDr. Sidney Dekker, who also codified some of the key elements of safety culture, observed another characteristic of complex systems: doing the same thing twice will not predictably or necessarily lead to the same result. It is this characteristic that\n\nmakes static checklists and best practices, while valuable,\n\ninsufficient to prevent catastrophes from occurring. See Appendix\n\n5.\n\nTherefore, because failure is inherent and inevitable in complex\n\nsystems, we must design a safe system of work, whether in\n\nmanufacturing or technology, where we can perform work without\n\nfear, confident that any errors will be detected quickly, long before\n\nthey cause catastrophic outcomes, such as worker injury, product\n\ndefects, or negative customer impact.\n\nAfter he decoded the causal mechanism behind the Toyota\n\nProduct System as part of his doctoral thesis at Harvard Business School, Dr. Steven Spear stated that designing perfectly safe systems is likely beyond our abilities, but we can make it safer to work in complex systems when the four following conditions are met:†\n\nComplex work is managed so that problems in design and\n\noperations are revealed\n\nProblems are swarmed and solved, resulting in quick\n\nconstruction of new knowledge\n\nNew local knowledge is exploited globally throughout the organization\n\nLeaders create other leaders who continually grow these types of capabilities\n\nEach of these capabilities are required to work safely in a complex system. In the next sections, the first two capabilities and their\n\nimportance are described, as well as how they have been created\n\nin other domains and what practices enable them in the technology value stream. (The third and fourth capabilities are\n\ndescribed in chapter 4.)\n\nSEE PROBLEMS AS THEY OCCUR\n\nIn a safe system of work, we must constantly test our design and\n\noperating assumptions. Our goal is to increase information flow in our system from as many areas as possible, sooner, faster,\n\ncheaper, and with as much clarity between cause and effect as possible. The more assumptions we can invalidate, the faster we\n\ncan find and fix problems, increasing our resilience, agility, and\n\nability to learn and innovate.\n\nWe do this by creating feedback and feedforward loops into our\n\nsystem of work. Dr. Peter Senge in his book The Fifth Discipline:\n\nThe Art & Practice of the Learning Organization described feedback loops as a critical part of learning organizations and\n\nsystems thinking. Feedback and feedforward loops cause components within a system to reinforce or counteract each other.\n\nIn manufacturing, the absence of effective feedback often\n\ncontribute to major quality and safety problems. In one well- documented case at the General Motors Fremont manufacturing\n\nplant, there were no effective procedures in place to detect\n\nproblems during the assembly process, nor were there explicit procedures on what to do when problems were found. As a result,\n\nthere were instances of engines being put in backward, cars missing steering wheels or tires, and cars even having to be towed\n\noff the assembly line because they wouldn’t start.\n\nIn contrast, in high-performing manufacturing operations there is fast, frequent, and high quality information flow throughout the\n\nentire value stream—every work operation is measured and monitored, and any defects or significant deviations are quickly\n\nfound and acted upon. These are the foundation of what enables quality, safety, and continual learning and improvement.\n\nIn the technology value stream, we often get poor outcomes\n\nbecause of the absence of fast feedback. For instance, in a waterfall\n\nsoftware project, we may develop code for an entire year and get no feedback on quality until we begin the testing phase—or worse,\n\nwhen we release our software to customers. When feedback is this delayed and infrequent, it is too slow to enable us to prevent\n\nundesirable outcomes.\n\nIn contrast, our goal is to create fast feedback and fastforward loops wherever work is performed, at all stages of the technology\n\nvalue stream, encompassing Product Management, Development,\n\nQA, Infosec, and Operations. This includes the creation of automated build, integration, and test processes, so that we can\n\nimmediately detect when a change has been introduced that takes us out of a correctly functioning and deployable state.\n\nWe also create pervasive telemetry so we can see how all our\n\nsystem components are operating in the production environment, so that we can quickly detect when they are not operating as\n\nexpected. Telemetry also allows us to measure whether we are\n\nachieving our intended goals and, ideally, is radiated to the entire value stream so we can see how our actions affect other portions of\n\nthe system as a whole.\n\nFeedback loops not only enable quick detection and recovery of\n\nproblems, but they also inform us on how to prevent these\n\nproblems from occurring again in the future. Doing this increases the quality and safety of our system of work, and creates\n\norganizational learning.\n\nAs Elisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of Explore It!: Reduce Risk and Increase\n\nConfidence with Exploratory Testing, said, “When I headed up quality engineering, I described my job as ‘creating feedback\n\ncycles.’ Feedback is critical because it is what allows us to steer.\n\nWe must constantly validate between customer needs, our intentions and our implementations. Testing is merely one sort of\n\nfeedback.”\n\nSWARM AND SOLVE PROBLEMS TO BUILD NEW KNOWLEDGE\n\nObviously, it is not sufficient to merely detect when the unexpected occurs. When problems occur, we must swarm them,\n\nmobilizing whoever is required to solve the problem.\n\nAccording to Dr. Spear, the goal of swarming is to contain problems before they have a chance to spread, and to diagnose\n\nand treat the problem so that it cannot recur. “In doing so,” he\n\nsays, “they build ever-deeper knowledge about how to manage the systems for doing our work, converting inevitable up-front\n\nignorance into knowledge.”\n\nThe paragon of this principle is the Toyota Andon cord. In da\n\nToyota manufacturing plant, above every work center is a cord\n\nthat every worker and manager is trained to pull when something goes wrong; for example, when a part is defective, when a required\n\npart is not available, or even when work takes longer than documented.‡\n\nWhen the Andon cord is pulled, the team leader is alerted and\n\nimmediately works to resolve the problem. If the problem cannot be resolved within a specified time (e.g., fifty-five seconds), the production line is halted so that the entire organization can be\n\nmobilized to assist with problem resolution until a successful countermeasure has been developed.\n\nInstead of working around the problem or scheduling a fix “when\n\nwe have more time,” we swarm to fix it immediately—this is nearly the opposite of the behavior at the GM Fremont plant described earlier. Swarming is necessary for the following reasons:\n\nIt prevents the problem from progressing downstream, where the cost and effort to repair it increases exponentially and technical debt is allowed to accumulate.\n\nIt prevents the work center from starting new work, which will likely introduce new errors into the system.\n\nIf the problem is not addressed, the work center could potentially have the same problem in the next operation (e.g., fifty-five seconds later), requiring more fixes and work. See Appendix 6.\n\nThis practice of swarming seems contrary to common management practice, as we are deliberately allowing a local problem to disrupt operations globally. However, swarming\n\nenables learning. It prevents the loss of critical information due to",
      "page_number": 70
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 78-85)",
      "start_page": 78,
      "end_page": 85,
      "detection_method": "topic_boundary",
      "content": "fading memories or changing circumstances. This is especially critical in complex systems, where many problems occur because\n\nof some unexpected, idiosyncratic interaction of people, processes, products, places, and circumstances—as time passes, it becomes impossible to reconstruct exactly what was going on when the\n\nproblem occurred.\n\nAs Dr. Spear notes, swarming is part of the “disciplined cycle of real-time problem recognition, diagnosis,...and treatment\n\n(countermeasures or corrective measures in manufacturing vernacular). It [is] the discipline of the Shewhart cycle—plan, do, check, act—popularized by W. Edwards Deming, but accelerated to warp speed.”\n\nIt is only through the swarming of ever smaller problems discovered ever earlier in the life cycle that we can deflect problems before a catastrophe occurs. In other words, when the\n\nnuclear reactor melts down, it is already too late to avert worst outcomes.\n\nTo enable fast feedback in the technology value stream, we must create the equivalent of an Andon cord and the related swarming response. This requires that we also create the culture that makes it safe, and even encouraged, to pull the Andon cord when\n\nsomething goes wrong, whether it is when a production incident occurs or when errors occur earlier in the value stream, such as when someone introduces a change that breaks our continuous\n\nbuild or test processes.\n\nWhen conditions trigger an Andon cord pull, we swarm to solve the problem and prevent the introduction of new work until the issue has been resolved.§ This provides fast feedback for everyone\n\nin the value stream (especially the person who caused the system to fail), enables us to quickly isolate and diagnose the problem,\n\nand prevents further complicating factors that can obscure cause and effect.\n\nPreventing the introduction of new work enables continuous\n\nintegration and deployment, which is single-piece flow in the technology value stream. All changes that pass our continuous build and integration tests are deployed into production, and any changes that cause any tests to fail trigger our Andon cord and are\n\nswarmed until resolved.\n\nKEEP PUSHING QUALITY CLOSER TO THE SOURCE\n\nWe may inadvertently perpetuate unsafe systems of work due to\n\nthe way we respond to accidents and incidents. In complex systems, adding more inspection steps and approval processes actually increases the likelihood of future failures. The effectiveness of approval processes decreases as we push decision-\n\nmaking further away from where the work is performed. Doing so not only lowers the quality of decisions but also increases our cycle time, thus decreasing the strength of the feedback between cause\n\nand effect, and reducing our ability to learn from successes and failures.¶\n\nThis can be seen even in smaller and less complex systems. When\n\ntop-down, bureaucratic command and control systems become ineffective, it is usually because the variance between “who should do something” and “who is actually doing something” is too large,\n\ndue to insufficient clarity and timeliness.\n\nExamples of ineffective quality controls include:\n\nRequiring another team to complete tedious, error-prone, and\n\nmanual tasks that could be easily automated and run as needed by the team who needs the work performed\n\nRequiring approvals from busy people who are distant from the\n\nwork, forcing them to make decisions without an adequate knowledge of the work or the potential implications, or to merely rubber stamp their approvals\n\nCreating large volumes of documentation of questionable detail which become obsolete shortly after they are written\n\nPushing large batches of work to teams and special committees for approval and processing and then waiting for responses\n\nInstead, we need everyone in our value stream to find and fix\n\nproblems in their area of control as part of our daily work. By doing this, we push quality and safety responsibilities and decision-making to where the work is performed, instead of relying on approvals from distant executives.\n\nWe use peer reviews of our proposed changes to gain whatever assurance is needed that our changes will operate as designed. We\n\nautomate as much of the quality checking typically performed by a QA or Information Security department as possible. Instead of developers needing to request or schedule a test to be run, these tests can be performed on demand, enabling developers to quickly\n\ntest their own code and even deploy those changes into production themselves.\n\nBy doing this, we truly make quality everyone’s responsibility as opposed to it being the sole responsibility of a separate department. Information security is not just Information\n\nSecurity’s job, just as availability isn’t merely the job of Operations.\n\nHaving developers share responsibility for the quality of the\n\nsystems they build not only improves outcomes but also accelerates learning. This is especially important for developers as they are typically the team that is furthest removed from the\n\ncustomer. Gary Gruver observes, “It’s impossible for a developer to learn anything when someone yells at them for something they broke six months ago—that’s why we need to provide feedback to everyone as quickly as possible, in minutes, not months.”\n\nENABLE OPTIMIZING FOR DOWNSTREAM WORK CENTERS\n\nIn the 1980s, Designing for Manufacturability principles sought to design parts and processes so that finished goods could be created\n\nwith the lowest cost, highest quality, and fastest flow. Examples include designing parts that are wildly asymmetrical to prevent them from being put on backwards, and designing screw fasteners so that they are impossible to over-tighten.\n\nThis was a departure from how design was typically done, which focused on the external customers but overlooked internal stakeholders, such as the people performing the manufacturing.\n\nLean defines two types of customers that we must design for: the external customer (who most likely pays for the service we are\n\ndelivering) and the internal customer (who receives and processes the work immediately after us). According to Lean, our most important customer is our next step downstream. Optimizing our\n\nwork for them requires that we have empathy for their problems in order to better identify the design problems that prevent fast and smooth flow.\n\nIn the technology value stream, we optimize for downstream work centers by designing for operations, where operational non- functional requirements (e.g., architecture, performance, stability,\n\ntestability, configurability, and security) are prioritized as highly as user features.\n\nBy doing this, we create quality at the source, likely resulting in a\n\nset of codified non-functional requirements that we can proactively integrate into every service we build.\n\nCONCLUSION\n\nCreating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream. We do this by seeing\n\nproblems as they occur, swarming and solving problems to build new knowledge, pushing quality closer to the source, and continually optimizing for downstream work centers.\n\nThe specific practices that enable fast flow in the DevOps value stream are presented in Part IV. In the next chapter, we present\n\nthe Third Way: The Principles of Feedback\n\n† Dr. Spear extended his work to explain the long-lasting successes of other organizations, such as the Toyota\n\nsupplier network, Alcoa, and the US Navy’s Nuclear Power Propulsion Program.\n\n‡ In some of its plants, Toyota has moved to using an Andon button.\n\n§ Astonishingly, when the number of Andon cord pulls drop, plant managers will actually decrease the tolerances\n\nto get an increase in the number of Andon cord pulls in order to continue to enable more learnings and improvements and to detect ever-weaker failure signals.\n\n¶ In the 1700s, the British government engaged in a spectacular example of top-down, bureaucratic command and control, which proved remarkably ineffective. At the time, Georgia was still a colony, and despite the fact that the British government was three thousand miles away and lacked firsthand knowledge of local land chemistry, rockiness, topography, accessibility to water, and other conditions, it tried to plan Georgia’s entire agricultural economy. The results of the attempt were dismal and left Georgia with the lowest levels of prosperity and population in the thirteen colonies.\n\n4 The Third Way:\n\nThe Principles of Continual Learning\n\nand Experimentation\n\nWhile the First Way addresses work flow from left to right and the\n\nSecond Way addresses the reciprocal fast and constant feedback\n\nfrom right to left, the Third Way focuses on creating a culture of continual learning and experimentation. These are the principles\n\nthat enable constant creation of individual knowledge, which is then turned into team and organizational knowledge.\n\nIn manufacturing operations with systemic quality and safety\n\nproblems, work is typically rigidly defined and enforced. For instance, in the GM Fremont plant described in the previous\n\nchapter, workers had little ability to integrate improvements and\n\nlearnings into their daily work, with suggestions for improvement\n\n“apt to meet a brick wall of indifference.”\n\nIn these environments, there is also often a culture of fear and low\n\ntrust, where workers who make mistakes are punished, and those who make suggestions or point out problems are viewed as whistle-blowers and troublemakers. When this occurs, leadership\n\nis actively suppressing, even punishing, learning and improvement, perpetuating quality and safety problems.\n\nIn contrast, high-performing manufacturing operations require\n\nand actively promote learning—instead of work being rigidly\n\ndefined, the system of work is dynamic, with line workers\n\nperforming experiments in their daily work to generate new\n\nimprovements, enabled by rigorous standardization of work\n\nprocedures and documentation of the results.\n\nIn the technology value stream, our goal is to create a high-trust\n\nculture, reinforcing that we are all lifelong learners who must take\n\nrisks in our daily work. By applying a scientific approach to both\n\nprocess improvement and product development, we learn from\n\nour successes and failures, identifying which ideas don’t work and reinforcing those that do. Moreover, any local learnings are rapidly turned into global improvements, so that new techniques\n\nand practices can be used by the entire organization.\n\nWe reserve time for the improvement of daily work and to further accelerate and ensure learning. We consistently introduce stress into our systems to force continual improvement. We even simulate and inject failures in our production services under controlled conditions to increase our resilience.\n\nBy creating this continual and dynamic system of learning, we\n\nenable teams to rapidly and automatically adapt to an ever-\n\nchanging environment, which ultimately helps us win in the marketplace.\n\nENABLING ORGANIZATIONAL LEARNING AND A SAFETY CULTURE",
      "page_number": 78
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 86-93)",
      "start_page": 86,
      "end_page": 93,
      "detection_method": "topic_boundary",
      "content": "When we work within a complex system, by definition it is\n\nimpossible for us to perfectly predict all the outcomes for any\n\naction we take. This is what contributes to unexpected, or even\n\ncatastrophic, outcomes and accidents in our daily work, even\n\nwhen we take precautions and work carefully.\n\nWhen these accidents affect our customers, we seek to understand\n\nwhy it happened. The root cause is often deemed to be human\n\nerror, and the all too common management response is to “name, blame, and shame” the person who caused the problem.† And, either subtly or explicitly, management hints that the person\n\nguilty of committing the error will be punished. They then create more processes and approvals to prevent the error from happening again.\n\nDr. Sidney Dekker, who codified some of the key elements of safety culture and coined the term just culture, wrote, “Responses\n\nto incidents and accidents that are seen as unjust can impede safety investigations, promote fear rather than mindfulness in people who do safety-critical work, make organizations more bureaucratic rather than more careful, and cultivate professional secrecy, evasion, and self-protection.”\n\nThese issues are especially problematic in the technology value stream—our work is almost always performed within a complex system, and how management chooses to react to failures and\n\naccidents leads to a culture of fear, which then makes it unlikely that problems and failure signals are ever reported. The result is that problems remain hidden until a catastrophe occurs.\n\nDr. Ron Westrum was one of the first to observe the importance of\n\norganizational culture on safety and performance. He observed\n\nthat in healthcare organizations, the presence of “generative” cultures was one of the top predictors of patient safety. Dr.\n\nWestrum defined three types of culture:\n\nPathological organizations are characterized by large amounts of fear and threat. People often hoard information, withhold it\n\nfor political reasons, or distort it to make themselves look\n\nbetter. Failure is often hidden.\n\nBureaucratic organizations are characterized by rules and processes, often to help individual departments maintain their\n\n“turf.” Failure is processed through a system of judgment, resulting in either punishment or justice and mercy.\n\nGenerative organizations are characterized by actively seeking\n\nand sharing information to better enable the organization to achieve its mission. Responsibilities are shared throughout the\n\nvalue stream, and failure results in reflection and genuine\n\ninquiry.\n\nFigure 8: The Westrum organizational typology model: how organizations process information (Source: Ron Westrum, “A typology of organisation culture,” BMJ Quality & Safety 13, no. 2 (2004), doi:10.1136/qshc.2003.009522.)\n\nJust as Dr. Westrum found in healthcare organizations, a high- trust, generative culture also predicted IT and organizational\n\nperformance in technology value streams.\n\nIn the technology value stream, we establish the foundations of a generative culture by striving to create a safe system of work.\n\nWhen accidents and failures occur, instead of looking for human error, we look for how we can redesign the system to prevent the\n\naccident from happening again.\n\nFor instance, we may conduct a blameless post-mortem after\n\nevery incident to gain the best understanding of how the accident occurred and agree upon what the best countermeasures are to\n\nimprove the system, ideally preventing the problem from occurring again and enabling faster detection and recovery.\n\nBy doing this, we create organizational learning. As Bethany\n\nMacri, an engineer at Etsy who led the creation of the Morgue tool to help with recording of post-mortems, stated, “By removing\n\nblame, you remove fear; by removing fear, you enable honesty;\n\nand honesty enables prevention.”\n\nDr. Spear observes that the result of removing blame and putting organizational learning in its place is that “organizations become\n\never more self-diagnosing and self-improving, skilled at detecting problems [and] solving them.”\n\nMany of these attributes were also described by Dr. Senge as\n\nattributes of learning organizations. In The Fifth Discipline, he wrote that these characteristics help customers, ensure quality,\n\ncreate competitive advantage and an energized and committed workforce, and uncover the truth.\n\nINSTITUTIONALIZE THE IMPROVEMENT OF DAILY WORK\n\nTeams are often not able or not willing to improve the processes\n\nthey operate within. The result is not only that they continue to suffer from their current problems, but their suffering also grows\n\nworse over time. Mike Rother observed in Toyota Kata that in the\n\nabsence of improvements, processes don’t stay the same—due to chaos and entropy, processes actually degrade over time.\n\nIn the technology value stream, when we avoid fixing our\n\nproblems, relying on daily workarounds, our problems and technical debt accumulates until all we are doing is performing\n\nworkarounds, trying to avoid disaster, with no cycles leftover for doing productive work. This is why Mike Orzen, author of Lean IT,\n\nobserved, “Even more important than daily work is the\n\nimprovement of daily work.”\n\nWe improve daily work by explicitly reserving time to pay down technical debt, fix defects, and refactor and improve problematic\n\nareas of our code and environments—we do this by reserving cycles in each development interval, or by scheduling kaizen\n\nblitzes, which are periods when engineers self-organize into teams to work on fixing any problem they want.\n\nThe result of these practices is that everyone finds and fixes\n\nproblems in their area of control, all the time, as part of their daily\n\nwork. When we finally fix the daily problems that we’ve worked\n\naround for months (or years), we can eradicate from our system the less obvious problems. By detecting and responding to these\n\never-weaker failure signals, we fix problems when it is not only easier and cheaper but also when the consequences are smaller.\n\nConsider the following example that improved workplace safety at Alcoa, an aluminum manufacturer with $7.8 billion in revenue in 1987. Aluminum manufacturing requires extremely high heat,\n\nhigh pressures, and corrosive chemicals. In 1987, Alcoa had a frightening safety record, with 2% of the ninety thousand employee workforce being injured each year—that’s seven injuries\n\nper day. When Paul O’Neill started as CEO, his first goal was to have zero injuries to employees, contractors, and visitors.\n\nO’Neill wanted to be notified within twenty-four hours of anyone\n\nbeing injured on the job—not to punish, but to ensure and promote that learnings were being generated and incorporated to create a safer workplace. Over the course of ten years, Alcoa\n\nreduced their injury rate by 95%.\n\nThe reduction in injury rates allowed Alcoa to focus on smaller problems and weaker failure signals—instead of notifying O’Neill\n\nonly when injuries occurred, they started reporting any close calls as well.‡ By doing this, they improved workplace safety over the subsequent twenty years and have one of the most enviable safety records in the industry.\n\nAs Dr. Spear writes, “Alcoans gradually stopped working around the difficulties, inconveniences, and impediments they\n\nexperienced. Coping, fire fighting, and making do were gradually replaced throughout the organization by a dynamic of identifying opportunities for process and product improvement. As those opportunities were identified and the problems were investigated,\n\nthe pockets of ignorance that they reflected were converted into\n\nnuggets of knowledge.” This helped give the company a greater competitive advantage in the market.\n\nSimilarly, in the technology value stream, as we make our system of work safer, we find and fix problems from ever weaker failure signals. For example, we may initially perform blameless post-\n\nmortems only for customer-impacting incidents. Over time, we may perform them for lesser team-impacting incidents and near misses as well.\n\nTRANSFORM LOCAL DISCOVERIES INTO GLOBAL IMPROVEMENTS\n\nWhen new learnings are discovered locally, there must also be some mechanism to enable the rest of the organization to use and benefit from that knowledge. In other words, when teams or\n\nindividuals have experiences that create expertise, our goal is to convert that tacit knowledge (i.e., knowledge that is difficult to transfer to another person by means of writing it down or\n\nverbalizing) into explicit, codified knowledge, which becomes someone else’s expertise through practice.\n\nThis ensures that when anyone else does similar work, they do so\n\nwith the cumulative and collective experience of everyone in the organization who has ever done the same work. A remarkable example of turning local knowledge into global knowledge is the US Navy’s Nuclear Power Propulsion Program (also known as\n\n“NR” for “Naval Reactors”), which has over 5,700 reactor-years of operation without a single reactor-related casualty or escape of radiation.\n\nThe NR is known for their intense commitment to scripted procedures and standardized work, and the need for incident\n\nreports for any departure from procedure or normal operations to accumulate learnings, no matter how minor the failure signal— they constantly update procedures and system designs based on these learnings.\n\nThe result is that when a new crew sets out to sea on their first deployment, they and their officers benefit from the collective knowledge of 5,700 accident-free reactor-years. Equally\n\nimpressive is that their own experiences at sea will be added to this collective knowledge, helping future crews safely achieve their own missions.\n\nIn the technology value stream, we must create similar mechanisms to create global knowledge, such as making all our blameless post-mortem reports searchable by teams trying to\n\nsolve similar problems, and by creating shared source code repositories that span the entire organization, where shared code, libraries, and configurations that embody the best collective\n\nknowledge of the entire organization can be easily utilized. All these mechanisms help convert individual expertise into artifacts that the rest of the organization can use.\n\nINJECT RESILIENCE PATTERNS INTO OUR DAILY WORK\n\nLower-performing manufacturing organizations buffer themselves from disruptions in many ways—in other words, they bulk up or add flab. For instance, to reduce the risk of a work center being\n\nidle (due to inventory arriving late, inventory that had to be\n\nscrapped, etc.), managers may choose to stockpile more inventory at each work center. However, that inventory buffer also increases WIP, which has all sorts of undesired outcomes, as previously\n\ndiscussed.\n\nSimilarly, to reduce the risk of a work center going down due to\n\nmachinery failure, managers may increase capacity by buying more capital equipment, hiring more people, or even increasing floor space. All these options increase costs.\n\nIn contrast, high performers achieve the same results (or better) by improving daily operations, continually introducing tension to elevate performance, as well as engineering more resilience into their system.\n\nConsider a typical experiment at one of Aisin Seiki Global’s mattress factories, one of Toyota’s top suppliers. Suppose they had\n\ntwo production lines, each capable of producing one hundred units per day. On slow days, they would send all production onto one line, experimenting with ways to increase capacity and identify vulnerabilities in their process, knowing that if\n\noverloading the line caused it to fail, they could send all production to the second line.\n\nBy relentless and constant experimentation in their daily work,\n\nthey were able to continually increase capacity, often without adding any new equipment or hiring more people. The emergent pattern that results from these types of improvement rituals not\n\nonly improves performance but also improves resilience, because the organization is always in a state of tension and change. This process of applying stress to increase resilience was named antifragility by author and risk analyst Nassim Nicholas Taleb.",
      "page_number": 86
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 94-101)",
      "start_page": 94,
      "end_page": 101,
      "detection_method": "topic_boundary",
      "content": "In the technology value stream, we can introduce the same type of tension into our systems by seeking to always reduce deployment lead times, increase test coverage, decrease test execution times,\n\nand even by re-architecting if necessary to increase developer productivity or increase reliability.\n\nWe may also perform Game Day exercises, where we rehearse\n\nlarge scale failures, such as turning off entire data centers. Or we may inject ever-larger scale faults into the production environment (such as the famous Netflix “Chaos Monkey,” which\n\nrandomly kills processes and compute servers in production) to ensure that we’re as resilient as we want to be.\n\nLEADERS REINFORCE A LEARNING CULTURE\n\nTraditionally, leaders were expected to be responsible for setting\n\nobjectives, allocating resources for achieving those objectives, and establishing the right combination of incentives. Leaders also establish the emotional tone for the organizations they lead. In\n\nother words, leaders lead by “making all the right decisions.”\n\nHowever, there is significant evidence that shows greatness is not achieved by leaders making all the right decisions—instead, the\n\nleader’s role is to create the conditions so their team can discover greatness in their daily work. In other words, creating greatness requires both leaders and workers, each of whom are mutually dependent upon each other.\n\nJim Womack, author of Gemba Walks, described the complementary working relationship and mutual respect that\n\nmust occur between leaders and frontline workers. According to Womack, this relationship is necessary because neither can solve problems alone—leaders are not close enough to the work, which\n\nis required to solve any problem, and frontline workers do not have the broader organizational context or the authority to make changes outside of their area of work.§\n\nLeaders must elevate the value of learning and disciplined problem solving. Mike Rother formalized these methods in what he calls the coaching kata. The result is one that mirrors the\n\nscientific method, where we explicitly state our True North goals, such as “sustain zero accidents” in the case of Alcoa, or “double throughput within a year” in the case of Aisin.\n\nThese strategic goals then inform the creation of iterative, shorter term goals, which are cascaded and then executed by establishing target conditions at the value stream or work center level (e.g., “reduce lead time by 10% within the next two weeks”).\n\nThese target conditions frame the scientific experiment: we explicitly state the problem we are seeking to solve, our hypothesis\n\nof how our proposed countermeasure will solve it, our methods for testing that hypothesis, our interpretation of the results, and our use of learnings to inform the next iteration.\n\nThe leader helps coach the person conducting the experiment with\n\nquestions that may include:\n\nWhat was your last step and what happened?\n\nWhat did you learn?\n\nWhat is your condition now?\n\nWhat is your next target condition?\n\nWhat obstacle are you working on now?\n\nWhat is your next step?\n\nWhat is your expected outcome?\n\nWhen can we check?\n\nThis problem-solving approach in which leaders help workers see\n\nand solve problems in their daily work is at the core of the Toyota Production System, of learning organizations, the Improvement\n\nKata, and high-reliability organizations. Mike Rother observes\n\nthat he sees Toyota “as an organization defined primarily by the unique behavior routines it continually teaches to all its\n\nmembers.”\n\nIn the technology value stream, this scientific approach and iterative method guides all of our internal improvement processes,\n\nbut also how we perform experiments to ensure that the products\n\nwe build actually help our internal and external customers achieve their goals.\n\nCONCLUSION\n\nThe principles of the Third Way address the need for valuing organizational learning, enabling high trust and boundary-\n\nspanning between functions, accepting that failures will always\n\noccur in complex systems, and making it acceptable to talk about problems so we can create a safe system of work. It also requires\n\ninstitutionalizing the improvement of daily work, converting local\n\nlearnings into global learnings that can be used by the entire\n\norganization, as well as continually injecting tension into our daily\n\nwork.\n\nAlthough fostering a culture of continual learning and\n\nexperimentation is the principle of the Third Way, it is also\n\ninterwoven into the First and Second Ways. In other words, improving flow and feedback requires an iterative and scientific\n\napproach that includes framing of a target condition, stating a\n\nhypothesis of what will help us get there, designing and conducting experiments, and evaluating the results.\n\nThe results are not only better performance but also increased\n\nresilience, higher job satisfaction, and improved organization adaptability.\n\nPART I CONCLUSION\n\nIn Part I of The DevOps Handbook we looked back at several\n\nmovements in history that helped lead to the development of DevOps. We also looked at the three main principles that form the\n\nfoundation for successful DevOps organizations: the principles of\n\nFlow, Feedback, and Continual Learning and Experimentation. In Part II, we will begin to look at how to start a DevOps movement\n\nin your organization.\n\n† The “name, blame, shame” pattern is part of the Bad Apple Theory criticized by Dr. Sydney Dekker and\n\nextensively discussed in his book The Field Guide to Understanding Human Error.\n\n‡ It is astonishing, instructional, and truly moving to see the level of conviction and passion that Paul O’Neill has\n\nabout the moral responsibility leaders have to create workplace safety.\n\n§ Leaders are responsible for the design and operation of processes at a higher level of aggregation where others\n\nhave less perspective and authority.\n\nPart\n\nIntroduction\n\nHow do we decide where to start a DevOps transformation in our\n\norganization? Who needs to be involved? How should we organize\n\nour teams, protect their work capacity, and maximize their\n\nchances of succeess? These are the questions we aim to answer in Part II of The DevOps Handbook.\n\nIn the following chapters we will walk through the process of\n\ninitiating a DevOps transformation. We begin by evaluating the value streams in our organization, locating a good place to start,\n\nand forming a strategy to create a dedicated transformation team\n\nwith specific improvement goals and eventual expansion. For each value stream being transformed, we identify the work being\n\nperformed and then look at organizational design strategies and organizational archetypes that best support the transformation\n\ngoals.\n\nPrimary focuses in these chapters include:\n\nSelecting which value streams to start with\n\nUnderstanding the work being done in our candidate value\n\nstreams\n\nDesigning our organization and architecture with Conway’s\n\nLaw in mind\n\nEnabling market-oriented outcomes through more effective\n\ncollaboration between functions throughout the value stream\n\nProtecting and enabling our teams\n\nBeginning any transformation is full of uncertainty—we are\n\ncharting a journey to an ideal end state, but where virtually all the\n\nintermediate steps are unknown. These next chapters are intended\n\nto provide a thought process to guide our decisions, provide\n\nactionable steps we can take, and illustrate case studies as examples.\n\n'\n\n5\n\nSelecting Which Value Stream to Start With\n\nChoosing a value stream for DevOps transformation deserves\n\ncareful consideration. Not only does the value stream we choose\n\ndictate the difficulty of our transformation, but it also dictates who will be involved in the transformation. It will affect how we need\n\nto organize into teams and how we can best enable the teams and individuals in them.\n\nAnother challenge was noted by Michael Rembetsy, who helped\n\nlead the DevOps transformation as the Director of Operations at Etsy in 2009. He observed, “We must pick our transformation\n\nprojects carefully—when we’re in trouble, we don’t get very many shots. Therefore, we must carefully pick and then protect those\n\nimprovement projects that will most improve the state of our\n\norganization.”\n\nLet us examine how the Nordstrom team started their DevOps transformation initiative in 2013, which Courtney Kissler, their VP of E-Commerce and Store Technologies, described at the DevOps\n\nEnterprise Summit in 2014 and 2015.",
      "page_number": 94
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 102-110)",
      "start_page": 102,
      "end_page": 110,
      "detection_method": "topic_boundary",
      "content": "Founded in 1901, Nordstrom is a leading fashion retailer that is\n\nfocused on delivering the best possible shopping experience to\n\ntheir customers. In 2015, Nordstrom had annual revenue of $13.5\n\nbillion.\n\nThe stage for Nordstrom’s DevOps journey was likely set in 2011\n\nduring one of their annual board of directors meetings. That year,\n\none of the strategic topics discussed was the need for online\n\nrevenue growth. They studied the plight of Blockbusters, Borders,\n\nand Barnes & Nobles, which demonstrated the dire consequences\n\nwhen traditional retailers were late creating competitive e-\n\ncommerce capabilities—these organizations were clearly at risk of losing their position in the marketplace or even going out of business entirely.†\n\nAt that time, Courtney Kissler was the senior director of Systems Delivery and Selling Technology, responsible for a significant portion of the technology organization, including their in-store systems and online e-commerce site. As Kissler described, “In 2011, the Nordstrom technology organization was very much optimized for cost—we had outsourced many of our technology\n\nfunctions, we had an annual planning cycle with large batch, ‘waterfall’ software releases. Even though we had a 97% success\n\nrate of hitting our schedule, budget, and scope goals, we were ill- equipped to achieve what the five-year business strategy required from us, as Nordstrom started optimizing for speed instead of merely optimizing for cost.”\n\nKissler and the Nordstrom technology management team had to decide where to start their initial transformation efforts. They\n\ndidn’t want to cause upheaval in the whole system. Instead, they wanted to focus on very specific areas of the business so that they\n\ncould experiment and learn. Their goal was to demonstrate early\n\nwins, which would give everyone confidence that these\n\nimprovements could be replicated in other areas of the\n\norganization. How exactly that would be achieved was still\n\nunknown.\n\nThey focused on three areas: the customer mobile application,\n\ntheir in-store restaurant systems, and their digital properties.\n\nEach of these areas had business goals that weren’t being met;\n\nthus, they were more receptive to considering a different way of\n\nworking. The stories of the first two are described below.\n\nThe Nordstrom mobile application had experienced an inauspicious start. As Kissler said, “Our customers were extremely frustrated with the product, and we had uniformly negative reviews when we launched it in the App Store. Worse, the existing structure and processes (aka “the system”) had designed their\n\nprocesses so that they could only release updates twice per year.” In other words, any fixes to the application would have to wait months to reach the customer.\n\nTheir first goal was to enable faster or on-demand releases, providing faster iteration and the ability to respond to customer\n\nfeedback. They created a dedicated product team that was solely dedicated to supporting the mobile application, with the goal of enabling that team to be able to independently implement, test,\n\nand deliver value to the customer. By doing this, they would no longer have to depend on and coordinate with scores of other teams inside Nordstrom. Furthermore, they moved from planning once per year to a continuous planning process. The result was a\n\nsingle prioritized backlog of work for the mobile app based on\n\ncustomer need—gone were all the conflicting priorities when the team had to support multiple products.\n\nOver the following year, they eliminated testing as a separate phase of work, instead integrating it into everyone’s daily work.‡ They doubled the features being delivered per month and halved\n\nthe number of defects—creating a successful outcome.\n\nTheir second area of focus was the systems supporting their in-\n\nstore Café Bistro restaurants. Unlike the mobile app value stream where the business need was to reduce time to market and\n\nincrease feature throughput, the business need here was to decrease cost and increase quality. In 2013, Nordstrom had\n\ncompleted eleven “restaurant re-concepts” which required changes to the in-store applications, causing a number of\n\ncustomer-impacting incidents. Disturbingly, they had planned\n\nforty-four more of these re-concepts for 2014—four times as many as in the previous year.\n\nAs Kissler stated, “One of our business leaders suggested that we\n\ntriple our team size to handle these new demands, but I proposed that we had to stop throwing more bodies at the problem and\n\ninstead improve the way we worked.”\n\nThey were able to identify problematic areas, such as in their work intake and deployment processes, which is where they focused\n\ntheir improvement efforts. They were able to reduce code\n\ndeployment lead times by 60% and reduce the number of production incidents 60% to 90%.\n\nThese successes gave the teams confidence that DevOps principles\n\nand practices were applicable to a wide variety of value streams.\n\nKissler was promoted to VP of E-Commerce and Store Technologies in 2014.\n\nIn 2015, Kissler said that in order for the selling or customer-\n\nfacing technology organization to enable the business to meet their goals, “…we needed to increase productivity in all our\n\ntechnology value streams, not just in a few. At the management level, we created an across-the-board mandate to reduce cycle\n\ntimes by 20% for all customer-facing services.”\n\nShe continued, “This is an audacious challenge. We have many\n\nproblems in our current state—process and cycle times are not consistently measured across teams, nor are they visible. Our first\n\ntarget condition requires us to help all our teams measure, make it visible, and perform experiments to start reducing their process\n\ntimes, iteration by iteration.”\n\nKissler concluded, “From a high level perspective, we believe that techniques such as value stream mapping, reducing our batch\n\nsizes toward single-piece flow, as well as using continuous delivery\n\nand microservices will get us to our desired state. However, while we are still learning, we are confident that we are heading in the\n\nright direction, and everyone knows that this effort has support from the highest levels of management.”\n\nIn this chapter, various models are presented that will enable us to\n\nreplicate the thought processes that the Nordstrom team used to decide which value streams to start with. We will evaluate our\n\ncandidate value streams in many ways, including whether they are\n\na greenfield or brownfield service, a system of engagement or a system of record. We will also estimate the risk/reward balance of\n\ntransforming and assess the likely level of resistance we may get\n\nfrom the teams we would work with.\n\nGREENFIELD VS. BROWNFIELD SERVICES\n\nWe often categorize our software services or products as either\n\ngreenfield or brownfield. These terms were originally used for urban planning and building projects. Greenfield development is\n\nwhen we build on undeveloped land. Brownfield development is\n\nwhen we build on land that was previously used for industrial purposes, potentially contaminated with hazardous waste or\n\npollution. In urban development, many factors can make greenfield projects simpler than brownfield projects—there are no\n\nexisting structures that need to be demolished nor are there toxic materials that need to be removed.\n\nIn technology, a greenfield project is a new software project or\n\ninitiative, likely in the early stages of planning or implementation, where we build our applications and infrastructure anew, with few\n\nconstraints. Starting with a greenfield software project can be\n\neasier, especially if the project is already funded and a team is either being created or is already in place. Furthermore, because\n\nwe are starting from scratch, we can worry less about existing code bases, processes, and teams.\n\nGreenfield DevOps projects are often pilots to demonstrate\n\nfeasibility of public or private clouds, piloting deployment\n\nautomation, and similar tools. An example of a greenfield DevOps project is the Hosted LabVIEW product in 2009 at National Instruments, a thirty-year-old organization with five thousand\n\nemployees and $1 billion in annual revenue. To bring this product\n\nto market quickly, a new team was created and allowed to operate outside of the existing IT processes and explore the use of public\n\nclouds. The initial team included an applications architect, a systems architect, two developers, a system automation developer, an operations lead, and two offshore operations staff. By using\n\nDevOps practices, they were able to deliver Hosted LabVIEW to market in half the time of their normal product introductions.\n\nOn the other end of the spectrum are brownfield DevOps projects,\n\nthese are existing products or services that are already serving customers and have potentially been in operation for years or even decades. Brownfield projects often come with significant amounts\n\nof technical debt, such as having no test automation or running on unsupported platforms. In the Nordstrom example presented earlier in this chapter, both the in-store restaurant systems and e- commerce systems were brownfield projects.\n\nAlthough many believe that DevOps is primarily for greenfield projects, DevOps has been used to successfully transform brownfield projects of all sorts. In fact, over 60% of the\n\ntransformation stories shared at the DevOps Enterprise Summit in 2014 were for brownfield projects. In these cases, there was a large performance gap between what the customer needed and\n\nwhat the organization was currently delivering, and the DevOps transformations created tremendous business benefit.\n\nIndeed, one of the findings in the 2015 State of DevOps Report\n\nvalidated that the age of the application was not a significant predictor of performance; instead, what predicted performance was whether the application was architected (or could be re- architected) for testability and deployability.\n\nTeams supporting brownfield projects may be very receptive to experimenting with DevOps, particularly when there is a\n\nwidespread belief that traditional methods are insufficient to achieve their goals—and especially if there is a strong sense of urgency around the need for improvement.§\n\nWhen transforming brownfield projects, we may face significant impediments and problems, especially when no automated testing exists or when there is a tightly-coupled architecture that prevents\n\nsmall teams from developing, testing, and deploying code independently. How we overcome these issues are discussed throughout this book.\n\nExamples of successful brownfield transformations include:\n\nCSG (2013): In 2013, CSG International had $747 million in revenue and over 3,500 employees, enabling over ninety\n\nthousand customer service agents to provide billing operations and customer care to over fifty million video, voice, and data customers, executing over six billion transactions, and printing\n\nand mailing over seventy million paper bill statements every month. Their initial scope of improvement was bill printing, one of their primary businesses, and involved a COBOL mainframe application and the twenty surrounding technology\n\nplatforms. As part of their transformation, they started performing daily deployments into a production-like environment, and doubled the frequency of customer releases\n\nfrom twice annually to four times annually. As a result, they significantly increased the reliability of the application and reduced code deployment lead times from two weeks to less than one day.\n\nEtsy (2009): In 2009, Etsy had thirty-five employees and was generating $87 million in revenue, but after they “barely\n\nsurvived the holiday retail season,” they started transforming virtually every aspect of how the organization worked, eventually turning the company into one of the most admired DevOps organizations and set the stage for a successful 2015\n\nIPO.\n\nCONSIDER BOTH SYSTEMS OF RECORD AND SYSTEMS OF ENGAGEMENT\n\nThe Gartner research firm has recently popularized the notion of\n\nbimodal IT, referring to the wide spectrum of services that typical enterprises support. Within bimodal IT there are systems of record, the ERP-like systems that run our business (e.g., MRP, HR, financial reporting systems), where the correctness of the\n\ntransactions and data are paramount; and systems of engagement, which are customer-facing or employee-facing systems, such as e-commerce systems and productivity\n\napplications.\n\nSystems of record typically have a slower pace of change and often have regulatory and compliance requirements (e.g., SOX). Gartner\n\ncalls these types of systems “Type 1,” where the organization focuses on “doing it right.”\n\nSystems of engagement typically have a much higher pace of\n\nchange to support rapid feedback loops that enable them to conduct experimentation to discover how to best meet customer needs. Gartner calls these types of systems “Type 2,” where the\n\norganization focuses on “doing it fast.”\n\nIt may be convenient to divide up our systems into these categories; however, we know that the core, chronic conflict between “doing it right” and “doing it fast” can be broken with\n\nDevOps. The data from Puppet Labs’ State of DevOps Reports— following the lessons of Lean manufacturing—shows that high- performing organizations are able to simultaneously deliver\n\nhigher levels of throughput and reliability.\n\nFurthermore, because of how interdependent our systems are, our ability to make changes to any of these systems is limited by the\n\nsystem that is most difficult to safely change, which is almost always a system of record.\n\nScott Prugh, VP of Product Development at CSG, observed, “We’ve\n\nadopted a philosophy that rejects bi-modal IT, because every one of our customers deserve speed and quality. This means that we need technical excellence, whether the team is supporting a 30\n\nyear old mainframe application, a Java application, or a mobile application.”\n\nConsequently, when we improve brownfield systems, we should\n\nnot only strive to reduce their complexity and improve their reliability and stability, we should also make them faster, safer, and easier to change. Even when new functionality is added just to\n\ngreenfield systems of engagement, they often cause reliability problems in the brownfield systems of record they rely on. By making these downstream systems safer to change, we help the entire organization more quickly and safely achieve its goals.\n\nSTART WITH THE MOST SYMPATHETIC AND INNOVATIVE GROUPS",
      "page_number": 102
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 111-118)",
      "start_page": 111,
      "end_page": 118,
      "detection_method": "topic_boundary",
      "content": "Within every organization, there will be teams and individuals with a wide range of attitudes toward the adoption of new ideas. Geoffrey A. Moore first depicted this spectrum in the form of the\n\ntechnology adoption life cycle in Crossing The Chasm, where the chasm represents the classic difficulty of reaching groups beyond the innovators and early adopters (see figure 9).\n\nIn other words, new ideas are often quickly embraced by innovators and early adopters, while others with more conservative attitudes resist them (the early majority, late\n\nmajority, and laggards). Our goal is to find those teams that already believe in the need for DevOps principles and practices, and who possess a desire and demonstrated ability to innovate and improve their own processes. Ideally, these groups will be\n\nenthusiastic supporters of the DevOps journey.\n\nFigure 9: The Technology Adoption Curve (Source: Moore and McKenna, Crossing The Chasm, 15.)\n\nEspecially in the early stages, we will not spend much time trying to convert the more conservative groups. Instead, we will focus our energy on creating successes with less risk-averse groups and\n\nbuild out our base from there (a process that is discussed further in the next section). Even if we have the highest levels of executive sponsorship, we will avoid the big bang approach (i.e., starting\n\neverywhere all at once), choosing instead to focus our efforts in a few areas of the organization, ensuring that those initiatives are successful, and expanding from there.¶\n\nEXPANDING DEVOPS ACROSS OUR ORGANIZATION\n\nRegardless of how we scope our initial effort, we must demonstrate early wins and broadcast our successes. We do this by breaking up our larger improvement goals into small,\n\nincremental steps. This not only creates our improvements faster, it also enables us to discover when we have made the wrong choice of value stream—by detecting our errors early, we can quickly back up and try again, making different decisions armed with our new\n\nlearnings.\n\nAs we generate successes, we earn the right to expand the scope of our DevOps initiative. We want to follow a safe sequence that\n\nmethodically grows our levels of credibility, influence, and\n\nsupport. The following list, adapted from a course taught by Dr. Roberto Fernandez, a William F. Pounds Professor in\n\nManagement at MIT, describes the ideal phases used by change agents to build and expand their coalition and base of support:\n\n1. Find Innovators and Early Adopters: In the beginning,\n\nwe focus our efforts on teams who actually want to help—these are our kindred spirits and fellow travelers who are the first to\n\nvolunteer to start the DevOps journey. In the ideal, these are\n\nalso people who are respected and have a high degree of\n\ninfluence over the rest of the organization, giving our initiative\n\nmore credibility.\n\n2. Build Critical Mass and Silent Majority: In the next\n\nphase, we seek to expand DevOps practices to more teams and\n\nvalue streams with the goal of creating a stable base of support. By working with teams who are receptive to our ideas, even if\n\nthey are not the most visible or influential groups, we expand\n\nour coalition who are generating more successes, creating a “bandwagon effect” that further increases our influence. We\n\nspecifically bypass dangerous political battles that could\n\njeopardize our initiative.\n\n3. Identify the Holdouts: The “holdouts” are the high profile,\n\ninfluential detractors who are most likely to resist (and maybe\n\neven sabotage) our efforts. In general, we tackle this group only after we have achieved a silent majority, when we have\n\nestablished enough successes to successfully protect our initiative.\n\nExpanding DevOps across an organization is no small task. It can\n\ncreate risk to individuals, departments, and the organization as a\n\nwhole. But as Ron van Kemenade, CIO of ING, who helped transform the organization into one of the most admired\n\ntechnology organizations, said, “Leading change requires courage, especially in corporate environments where people are scared and\n\nfight you. But if you start small, you really have nothing to fear.\n\nAny leader needs to be brave enough to allocate teams to do some calculated risk-taking.”\n\nCONCLUSION\n\nPeter Drucker, a leader in the development of management\n\neducation, observed that “little fish learn to be big fish in little\n\nponds.” By choosing carefully where and how to start, we are able to experiment and learn in areas of our organization that create\n\nvalue without jeopardizing the rest of the organization. By doing\n\nthis, we build our base of support, earn the right to expand the use of DevOps in our organization, and gain the recognition and\n\ngratitude of an ever-larger constituency.\n\n† These organizations were sometimes known as the “Killer B’s that are Dying.”\n\n‡ The practice of relying on a stabilization phase or hardening phase at the end of a project often has very poor\n\noutcomes, because it means problems are not being found and fixed as part of daily work and are left unaddressed, potentially snowballing into larger issues.\n\n§ That the services that have the largest potential business benefit are brownfield systems shouldn’t be surprising. After all, these are the systems that are most relied upon and have the largest number of existing customers or highest amount of revenue depending upon them.\n\n¶ Big bang, top-down transformations are possible, such as the Agile transformation at PayPal in 2012 that was led by their vice president of technology, Kirsten Wolberg. However, as with any sustainable and successful transformation, this required the highest level of management support and a relentless, sustained focus on driving the necessary outcomes.\n\n6 Understanding the\n\nWork in Our Value Stream, Making it Visible, and Expanding it Across\n\nthe Organization\n\nOnce we have identified a value stream to which we want to apply DevOps principles and patterns, our next step is to gain a\n\nsufficient understanding of how value is delivered to the customer:\n\nwhat work is performed and by whom, and what steps can we take to improve flow.\n\nIn the previous chapter, we learned about the DevOps\n\ntransformation led by Courtney Kissler and the team at Nordstrom. Over the years, they have learned that one of the most\n\nefficient ways to start improving any value stream is to conduct a\n\nworkshop with all the major stakeholders and perform a value stream mapping exercise—a process (described later in this chapter) designed to help capture all the steps required to create\n\nvalue.\n\nKissler’s favorite example of the valuable and unexpected insights\n\nthat can come from value stream mapping is when they tried to improve the long lead times associated with requests going\n\nthrough the Cosmetics Business Office application, a COBOL\n\nmainframe application that supported all the floor and\n\ndepartment managers of their in-store beauty and cosmetic\n\ndepartments.\n\nThis application allowed department managers to register new\n\nsalespeople for various product lines carried in their stores, so that\n\nthey could track sales commissions, enable vendor rebates, and so\n\nforth.\n\nKissler explained:\n\nI knew this particular mainframe application well—earlier in my career, I supported this technology team, so I know firsthand that for nearly a decade, during each annual planning cycle, we would debate about how we needed to get this application off the mainframe. Of course, like in most\n\norganizations, even when there was full management support, we never seemed to get around to migrating it.\n\nMy team wanted to conduct a value stream mapping exercise to determine whether the COBOL application really was the problem, or maybe there was a larger problem that we needed to address. They conducted a workshop that assembled everyone with any accountability for delivering value to our internal customers, including our business partners, the\n\nmainframe team, the shared service teams, and so forth.\n\nWhat they discovered was that when department managers\n\nwere submitting the ‘product line assignment’ request form, we were asking them for an employee number, which they didn’t have—so they would either leave it blank or put in something\n\nlike ‘I don’t know.’ Worse, in order to fill out the form,\n\ndepartment managers would have to inconveniently leave the\n\nstore floor in order to use a PC in the back office. The end\n\nresult was all this wasted time, with work bouncing back and\n\nforth in the process.\n\nDuring the workshop, the participants conducted several\n\nexperiments, including deleting the employee number field in the\n\nform and letting another department get that information in a\n\ndownstream step. These experiments, conducted with the help of\n\ndepartment managers, showed a four-day reduction in processing\n\ntime. The team later replaced the PC application with an iPad application, which allowed managers to submit the necessary information without leaving the store floor, and the processing\n\ntime was further reduced to seconds.\n\nShe said proudly, “With those amazing improvements, all the\n\ndemands to get this application off the mainframe disappeared. Furthermore, other business leaders took notice and started coming to us with a whole list of further experiments they wanted to conduct with us in their own organizations. Everyone in the business and technology teams were excited by the outcome because they solved a real business problem, and, most\n\nimportantly, they learned something in the process.”\n\nIn the remainder of this chapter, we will go through the following\n\nsteps: identifying all the teams required to create customer value, creating a value stream map to make visible all the required work, and using it to guide the teams in how to better and more quickly create value. By doing this, we can replicate the amazing outcomes\n\ndescribed in this Nordstrom example.\n\nIDENTIFYING THE TEAMS SUPPORTING OUR VALUE STREAM\n\nAs this Nordstrom example demonstrates, in value streams of any complexity, no one person knows all the work that must be\n\nperformed in order to create value for the customer—especially since the required work must be performed by many different\n\nteams, often far removed from each other on the organization charts, geographically, or by incentives.\n\nAs a result, after we select a candidate application or service for\n\nour DevOps initiative, we must identify all the members of the\n\nvalue stream who are responsible for working together to create value for the customers being served. In general, this includes:\n\nProduct owner: the internal voice of the business that\n\ndefines the next set of functionality in the service\n\nDevelopment: the team responsible for developing application functionality in the service\n\nQA: the team responsible for ensuring that feedback loops\n\nexist to ensure the service functions as desired\n\nOperations: the team often responsible for maintaining the production environment and helping ensure that required\n\nservice levels are met\n\nInfosec: the team responsible for securing systems and data\n\nRelease managers: the people responsible for managing and coordinating the production deployment and release processes",
      "page_number": 111
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 119-126)",
      "start_page": 119,
      "end_page": 126,
      "detection_method": "topic_boundary",
      "content": "Technology executives or value stream manager: in Lean literature, someone who is responsible for “ensuring that\n\nthe value stream meets or exceeds the customer [and organizational] requirements for the overall value stream, from\n\nstart to finish”\n\nCREATE A VALUE STREAM MAP TO SEE THE WORK\n\nAfter we identify our value stream members, our next step is to gain a concrete understanding of how work is performed,\n\ndocumented in the form of a value stream map. In our value\n\nstream, work likely begins with the product owner, in the form of a customer request or the formulation of a business hypothesis.\n\nSome time later, this work is accepted by Development, where features are implemented in code and checked in to our version\n\ncontrol repository. Builds are then integrated, tested in a\n\nproduction-like environment, and finally deployed into production, where they (ideally) create value for our customer.\n\nIn many traditional organizations, this value stream will consist of\n\nhundreds, if not thousands, of steps, requiring work from hundreds of people. Because documenting any value stream map\n\nthis complex likely requires multiple days, we may conduct a multi-day workshop, where we assemble all the key constituents\n\nand remove them from the distractions of their daily work.\n\nOur goal is not to document every step and associated minutiae, but to sufficiently understand the areas in our value stream that\n\nare jeopardizing our goals of fast flow, short lead times, and\n\nreliable customer outcomes. Ideally, we have assembled those\n\npeople with the authority to change their portion of the value stream.†\n\nDamon Edwards, co-host of DevOps Café podcast, observed, “In\n\nmy experience, these types of value stream mapping exercises are always an eye-opener. Often, it is the first time when people see\n\nhow much work and heroics are required to deliver value to the customer. For Operations, it may be the first time that they see the\n\nconsequences that result when developers don’t have access to correctly configured environments, which contributes to even\n\nmore crazy work during code deployments. For Development, it\n\nmay be the first time they see all the heroics that are required by Test and Operations in order to deploy their code into production,\n\nlong after they flag a feature as ‘completed.’”\n\nUsing the full breadth of knowledge brought by the teams engaged in the value stream, we should focus our investigation and\n\nscrutiny on the following areas:\n\nPlaces where work must wait weeks or even months, such as getting production-like environments, change approval\n\nprocesses, or security review processes\n\nPlaces where significant rework is generated or received\n\nOur first pass of documenting our value stream should only consist of high-level process blocks. Typically, even for complex\n\nvalue streams, groups can create a diagram with five to fifteen\n\nprocess blocks within a few hours. Each process block should include the lead time and process time for a work item to be processed, as well as the %C/A as measured by the downstream consumers of the output.‡\n\nFigure 10: An example of a value stream map (Source: Humble, Molesky, and O’Reilly, Lean Enterprise, 139.)\n\nWe use the metrics from our value stream map to guide our improvement efforts. In the Nordstrom example, they focused on the low %C/A rates on the request form submitted by department\n\nmanagers due to the absence of employee numbers. In other cases, it may be long lead times or low %C/A rates when delivering correctly configured test environments to Development teams, or\n\nit might be the long lead times required to execute and pass regression testing before each software release.\n\nOnce we identify the metric we want to improve, we should\n\nperform the next level of observations and measurements to better understand the problem and then construct an idealized, future value stream map, which serves as a target condition to achieve by\n\nsome date (e.g., usually three to twelve months).\n\nLeadership helps define this future state and then guides and enables the team to brainstorm hypotheses and countermeasures\n\nto achieve the desired improvement to that state, perform\n\nexperiments to test those hypotheses, and interpret the results to determine whether the hypotheses were correct. The teams keep\n\nrepeating and iterating, using any new learnings to inform the next experiments.\n\nCREATING A DEDICATED TRANSFORMATION TEAM\n\nOne of the inherent challenges with initiatives such as DevOps\n\ntransformations is that they are inevitably in conflict with ongoing business operations. Part of this is a natural outcome of how successful businesses evolve. An organization that has been\n\nsuccessful for any extended period of time (years, decades, or even centuries) has created mechanisms to perpetuate the practices that made them successful, such as product development, order administration, and supply chain operations.\n\nMany techniques are used to perpetuate and protect how current processes operate, such as specialization, focus on efficiency and\n\nrepeatability, bureaucracies that enforce approval processes, and controls to protect against variance. In particular, bureaucracies are incredibly resilient and are designed to survive adverse conditions—one can remove half the bureaucrats, and the process\n\nwill still survive.\n\nWhile this is good for preserving status quo, we often need to change how we work to adapt to changing conditions in the\n\nmarketplace. Doing this requires disruption and innovation, which puts us at odds with groups who are currently responsible for daily operations and the internal bureaucracies, and who will\n\nalmost always win.\n\nIn their book The Other Side of Innovation: Solving the Execution Challenge, Dr. Vijay Govindarajan and Dr. Chris Trimble, both\n\nfaculty members of Dartmouth College’s Tuck School of Business, described their studies of how disruptive innovation is achieved despite these powerful forces of daily operations. They documented how customer-driven auto insurance products were\n\nsuccessfully developed and marketed at Allstate, how the profitable digital publishing business was created at the Wall Street Journal, the development of the breakthrough trail-running\n\nshoe at Timberland, and the development of the first electric car at BMW.\n\nBased on their research, Dr. Govindarajan and Dr. Trimble assert\n\nthat organizations need to create a dedicated transformation team that is able to operate outside of the rest of the organization that is responsible for daily operations (which they call the “dedicated\n\nteam” and “performance engine” respectively).\n\nFirst and foremost, we will hold this dedicated team accountable for achieving a clearly defined, measurable, system-level result\n\n(e.g., reduce the deployment lead time from “code committed into version control to successfully running in production” by 50%). In order to execute such an initiative, we do the following:\n\nAssign members of the dedicated team to be solely allocated to the DevOps transformation efforts (as opposed to “maintain all your current responsibilities, but spend 20% of your time on this new DevOps thing.”).\n\nSelect team members who are generalists, who have skills across a wide variety of domains.\n\nSelect team members who have longstanding and mutually respectful relationships with the rest of the organization.\n\nCreate a separate physical space for the dedicated team, if possible, to maximize communication flow within the team, and creating some isolation from the rest of the organization.\n\nIf possible, we will free the transformation team from many of the rules and policies that restrict the rest of the organization, as National Instruments did, described in the previous chapter. After\n\nall, established processes are a form of institutional memory—we need the dedicated team to create the new processes and learnings required to generate our desired outcomes, creating new institutional memory.\n\nCreating a dedicated team is not only good for the team, but also good for the performance engine. By creating a separate team, we\n\ncreate the space for them to experiment with new practices, protecting the rest of the organization from the potential disruptions and distractions associated with it.\n\nAGREE ON A SHARED GOAL\n\nOne of the most important parts of any improvement initiative is\n\nto define a measurable goal with a clearly defined deadline, between six months and two years in the future. It should require considerable effort but still be achievable. And achievement of the goal should create obvious value for the organization as a whole\n\nand to our customers.\n\nThese goals and the time frame should be agreed upon by the executives and known to everyone in the organization. We also\n\nwant to limit the number of these types of initiatives going on\n\nsimultaneously to prevent us from overly taxing the organizational change management capacity of leaders and the organization. Examples of improvement goals might include:\n\nReduce the percentage of the budget spent on product support and unplanned work by 50%.\n\nEnsure lead time from code check-in to production release is one week or less for 95% of changes.\n\nEnsure releases can always be performed during normal\n\nbusiness hours with zero downtime.\n\nIntegrate all the required information security controls into the deployment pipeline to pass all required compliance\n\nrequirements.\n\nOnce the high-level goal is made clear, teams should decide on a\n\nregular cadence to drive the improvement work. Like product development work, we want transformation work to be done in an iterative, incremental manner. A typical iteration will be in the range of two to four weeks. For each iteration, the teams should\n\nagree on a small set of goals that generate value and makes some progress toward the long-term goal. At the end of each iteration, teams should review their progress and set new goals for the next\n\niteration.\n\nKEEP OUR IMPROVEMENT PLANNING HORIZONS SHORT\n\nIn any DevOps transformation project, we need to keep our\n\nplanning horizons short, just as if we were in a startup doing product or customer development. Our initiative should strive to\n\ngenerate measurable improvements or actionable data within weeks (or, in the worst case, months).\n\nBy keeping our planning horizons and iteration intervals short, we achieve the following:\n\nFlexibility and the ability to reprioritize and replan quickly\n\nDecrease the delay between work expended and improvement realized, which strengthens our feedback loop, making it more likely to reinforce desired behaviors—when improvement\n\ninitiatives are successful, it encourages more investment\n\nFaster learning generated from the first iteration, meaning faster integration of our learnings into the next iteration\n\nReduction in activation energy to get improvements\n\nQuicker realization of improvements that make meaningful\n\ndifferences in our daily work\n\nLess risk that our project is killed before we can generate any demonstrable outcomes\n\nRESERVE 20% OF CYCLES FOR NON-FUNCTIONAL REQUIREMENTS AND REDUCING TECHNICAL DEBT\n\nA problem common to any process improvement effort is how to\n\nproperly prioritize it—after all, organizations that need it most are those that have the least amount of time to spend on\n\nimprovement. This is especially true in technology organizations\n\nbecause of technical debt.",
      "page_number": 119
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 127-136)",
      "start_page": 127,
      "end_page": 136,
      "detection_method": "topic_boundary",
      "content": "Organizations that struggle with financial debt only make interest\n\npayments and never reduce the loan principal, and may eventually\n\nfind themselves in situations where they can no longer service the interest payments. Similarly, organizations that don’t pay down\n\ntechnical debt can find themselves so burdened with daily\n\nworkarounds for problems left unfixed that they can no longer complete any new work. In other words, they are now only making\n\nthe interest payment on their technical debt.\n\nWe will actively manage this technical debt by ensuring that we invest at least 20% of all Development and Operations cycles on\n\nrefactoring, investing in automation work and architecture and\n\nnon-functional requirements (NFRs, sometimes referred to as the “ilities”), such as maintainability, manageability, scalability,\n\nreliability, testability, deployability, and security.\n\nFigure 11: Invest 20% of cycles on those that create positive, user- invisible value (Source: “Machine Learning and Technical Debt with D. Sculley,” Software Engineering Daily podcast, November 17, 2015, http://softwareengineeringdaily.com/2015/11/17/machine-learning- and-technical-debt-with-d-sculley/.)\n\nAfter the near-death experience of eBay in the late 1990s, Marty\n\nCagan, author of Inspired: How To Create Products Customers\n\nLove, the seminal book on product design and management,\n\ncodified the following lesson:\n\nThe deal [between product owners and] engineering goes like this: Product management takes 20% of the team’s capacity\n\nright off the top and gives this to engineering to spend as they\n\nsee fit. They might use it to rewrite, re-architect, or re-factor problematic parts of the code base...whatever they believe is\n\nnecessary to avoid ever having to come to the team and say, ‘we\n\nneed to stop and rewrite [all our code].’ If you’re in really bad shape today, you might need to make this 30% or even more of\n\nthe resources. However, I get nervous when I find teams that think they can get away with much less than 20%.\n\nCagan notes that when organizations do not pay their “20% tax,”\n\ntechnical debt will increase to the point where an organization\n\ninevitably spends all of its cycles paying down technical debt. At some point, the services become so fragile that feature delivery\n\ngrinds to a halt because all the engineers are working on reliability issues or working around problems.\n\nBy dedicating 20% of our cycles so that Dev and Ops can create\n\nlasting countermeasures to the problems we encounter in our daily work, we ensure that technical debt doesn’t impede our\n\nability to quickly and safely develop and operate our services in\n\nproduction. Elevating added pressure of technical debt from workers can also reduce levels of burnout.\n\nCase Study Operation InVersion at LinkedIn (2011)\n\nLinkedIn’s Operation InVersion presents an interesting case\n\nstudy that illustrates the need to pay down technical debt as\n\na part of daily work. Six months after their successful IPO in 2011, LinkedIn continued to struggle with problematic\n\ndeployments that became so painful that they launched Operation InVersion, where they stopped all feature\n\ndevelopment for two months in order to overhaul their\n\ncomputing environments, deployments, and architecture.\n\nLinkedIn was created in 2003 to help users “connect to your network for better job opportunities.” By the end of their first\n\nweek of operation, they had 2,700 members. One year later, they had over one million members, and have grown\n\nexponentially since then. By November 2015, LinkedIn had\n\nover 350 million members, who generate tens of thousands of requests per second, resulting in millions of queries per\n\nsecond on the LinkedIn back-end systems.\n\nFrom the beginning, LinkedIn primarily ran on their homegrown Leo application, a monolithic Java application\n\nthat served every page through servlets and managed JDBC\n\nconnections to various back-end Oracle databases. However, to keep up with growing traffic in their early years,\n\ntwo critical services were decoupled from Leo: the first\n\nhandled queries around the member connection graph entirely in-memory, and the second was member search,\n\nwhich layered over the first.\n\nBy 2010, most new development was occurring in new services, with nearly one hundred services running outside\n\nof Leo. The problem was that Leo was only being deployed\n\nonce every two weeks.\n\nJosh Clemm, a senior engineering manager at LinkedIn,\n\nexplained that by 2010, the company was having significant\n\nproblems with Leo. Despite vertically scaling Leo by adding memory and CPUs, “Leo was often going down in\n\nproduction, it was difficult to troubleshoot and recover, and difficult to release new code….It was clear we needed to ‘Kill\n\nLeo’ and break it up into many small functional and stateless\n\nservices.”\n\nIn 2013, journalist Ashlee Vance of Bloomberg described\n\nhow “when LinkedIn would try to add a bunch of new things\n\nat once, the site would crumble into a broken mess, requiring engineers to work long into the night and fix the\n\nproblems.” By Fall 2011, late nights were no longer a rite of\n\npassage or a bonding activity, because the problems had become intolerable. Some of LinkedIn’s top engineers,\n\nincluding Kevin Scott, who had joined as the LinkedIn VP of\n\nEngineering three months before their initial public offering, decided to completely stop engineering work on new\n\nfeatures and dedicate the whole department to fixing the\n\nsite’s core infrastructure. They called the effort Operation InVersion.\n\nScott launched Operation InVersion as a way to “inject the\n\nbeginnings of a cultural manifesto into his team’s engineering culture. There would be no new feature\n\ndevelopment until LinkedIn’s computing architecture was\n\nrevamped—it’s what the business and his team needed.”\n\nScott described one downside: “You go public, have all the\n\nworld looking at you, and then we tell management that\n\nwe’re not going to deliver anything new while all of\n\nengineering works on this [InVersion] project for the next two\n\nmonths. It was a scary thing.”\n\nHowever, Vance described the massively positive results of Operation InVersion. “LinkedIn created a whole suite of\n\nsoftware and tools to help it develop code for the site.\n\nInstead of waiting weeks for their new features to make their way onto LinkedIn’s main site, engineers could develop a\n\nnew service, have a series of automated systems examine the code for any bugs and issues the service might have\n\ninteracting with existing features, and launch it right to the\n\nlive LinkedIn site...LinkedIn’s engineering corps [now] performs major upgrades to the site three times a day.” By\n\ncreating a safer system of work, the value they created\n\nincluded fewer late night cram sessions, with more time to develop new, innovative features.\n\nAs Josh Clemm described in his article on scaling at\n\nLinkedIn, “Scaling can be measured across many dimensions, including organizational…. [Operation\n\nInVersion] allowed the entire engineering organization to\n\nfocus on improving tooling and deployment, infrastructure, and developer productivity. It was successful in enabling the\n\nengineering agility we need to build the scalable new\n\nproducts we have today….[In] 2010, we already had over 150 separate services. Today, we have over 750 services.”\n\nKevin Scott stated, “Your job as an engineer and your\n\npurpose as a technology team is to help your company win. If you lead a team of engineers, it’s better to take a CEO’s\n\nperspective. Your job is to figure out what it is that your\n\ncompany, your business, your marketplace, your competitive\n\nenvironment needs. Apply that to your engineering team in\n\norder for your company to win.”\n\nBy allowing LinkedIn to pay down nearly a decade of technical debt, Project InVersion enabled stability and safety,\n\nwhile setting the next stage of growth for the company. However, it required two months of total focus on non-\n\nfunctional requirements, at the expense of all the promised\n\nfeatures made to the public markets during an IPO. By finding and fixing problems as part of our daily work, we\n\nmanage our technical debt so that we avoid these “near\n\ndeath” experiences.\n\nINCREASE THE VISIBILITY OF WORK\n\nIn order to be able to know if we are making progress toward our goal, it’s essential that everyone in the organization knows the\n\ncurrent state of work. There are many ways to make the current\n\nstate visible, but what’s most important is that the information we display is up to date, and that we constantly revise what we\n\nmeasure to make sure it’s helping us understand progress toward\n\nour current target conditions.\n\nThe following section discusses patterns that can help create\n\nvisibility and alignment across teams and functions.\n\nUSE TOOLS TO REINFORCE DESIRED BEHAVIOR\n\nAs Christopher Little, a software executive and one of the earliest chroniclers of DevOps, observed, “Anthropologists describe tools\n\nas a cultural artifact. Any discussion of culture after the invention\n\nof fire must also be about tools.” Similarly, in the DevOps value stream, we use tools to reinforce our culture and accelerate\n\ndesired behavior changes.\n\nOne goal is that our tooling reinforces that Development and Operations not only have shared goals, but have a common\n\nbacklog of work, ideally stored in a common work system and\n\nusing a shared vocabulary, so that work can be prioritized globally.\n\nBy doing this, Development and Operations may end up creating a\n\nshared work queue, instead of each silo using a different one (e.g.,\n\nDevelopment uses JIRA while Operations uses ServiceNow). A significant benefit of this is that when production incidents are\n\nshown in the same work systems as development work, it will be\n\nobvious when ongoing incidents should halt other work, especially when we have a kanban board.\n\nAnother benefit of having Development and Operations using a\n\nshared tool is a unified backlog, where everyone prioritizes improvement projects from a global perspective, selecting work\n\nthat has the highest value to the organization or most reduces\n\ntechnical debt. As we identify technical debt, we add it to our prioritized backlog if we can’t address it immediately. For issues\n\nthat remain unaddressed, we can use our “20% time for non-\n\nfunctional requirements” to fix the top items from our backlog.\n\nOther technologies that reinforce shared goals are chat rooms,\n\nsuch as IRC channels, HipChat, Campfire, Slack, Flowdock, and\n\nOpenFire. Chat rooms allow the fast sharing of information (as opposed to filling out forms that are processed through predefined\n\nworkflows), the ability to invite other people as needed, and\n\nhistory logs that are automatically recorded for posterity and can\n\nbe analyzed during post-mortem sessions.\n\nAn amazing dynamic is created when we have a mechanism that\n\nallows any team member to quickly help other team members, or\n\neven people outside their team—the time required to get information or needed work can go from days to minutes. In\n\naddition, because everything is being recorded, we may not need\n\nto ask someone else for help in the future—we simply search for it.\n\nHowever, the rapid communication environment facilitated by\n\nchat rooms can also be a drawback. As Ryan Martens, the founder\n\nand CTO of Rally Software, observes, “In a chat room, if someone doesn’t get an answer in a couple of minutes, it’s totally accepted\n\nand expected that you can bug them again until they get what they\n\nneed.”\n\nThe expectations of immediate response can, of course, lead to\n\nundesired outcomes. A constant barrage of interruptions and\n\nquestions can prevent people from getting necessary work done. As a result, teams may decide that certain types of requests should\n\ngo through more structured and asynchronous tools.\n\nCONCLUSION\n\nIn this chapter, we identified all the teams supporting our value\n\nstream and captured in a value stream map what work is required\n\nin order to deliver value to the customer. The value stream map provides the basis for understanding our current state, including\n\nour lead time and %C/A metrics for problematic areas, and\n\ninforms how we set a future state.\n\nThis enables dedicated transformation teams to rapidly iterate and\n\nexperiment to improve performance. We also make sure that we allocate a sufficient amount of time for improvement, fixing\n\nknown problems and architectural issues, including our non-\n\nfunctional requirements. The case studies from Nordstrom and LinkedIn demonstrate how dramatic improvements can be made\n\nin lead times and quality when we find problems in our value\n\nstream and pay down technical debt.\n\n† Which makes it all the more important that we limit the level of detail being collected—everyone’s time is\n\nvaluable and scarce.\n\n‡ Conversely, there are many examples of using tools in a way that guarantees no behavior changes occur. For\n\ninstance, an organization commits to an agile planning tool but then configures it for a waterfall process, which merely maintains status quo.\n\n7 How to Design Our\n\nOrganization and Architecture with Conway’s Law in\n\nMind\n\nIn the previous chapters, we identified a value stream to start our\n\nDevOps transformation and established shared goals and\n\npractices to enable a dedicated transformation team to improve how we deliver value to the customer.\n\nIn this chapter, we will start thinking about how to organize\n\nourselves to best achieve our value stream goals. After all, how we organize our teams affects how we perform our work. Dr. Melvin\n\nConway performed a famous experiment in 1968 with a contract research organization that had eight people who were\n\ncommissioned to produce a COBOL and an ALGOL compiler. He\n\nobserved, “After some initial estimates of difficulty and time, five\n\npeople were assigned to the COBOL job and three to the ALGOL job. The resulting COBOL compiler ran in five phases, the ALGOL compiler ran in three.”\n\nThese observations led to what is now known as Conway’s Law, which states that “organizations which design systems...are\n\nconstrained to produce designs which are copies of the communication structures of these organizations….The larger an",
      "page_number": 127
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 137-145)",
      "start_page": 137,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "organization is, the less flexibility it has and the more pronounced\n\nthe phenomenon.” Eric S. Raymond, author of the book The\n\nCathedral and the Bazaar: Musings on Linux and Open Source\n\nby an Accidental Revolutionary, crafted a simplified (and now,\n\nmore famous) version of Conway’s Law in his Jargon File: “The\n\norganization of the software and the organization of the software\n\nteam will be congruent; commonly stated as ‘if you have four\n\ngroups working on a compiler, you’ll get a 4-pass compiler.’”\n\nIn other words, how we organize our teams has a powerful effect\n\non the software we produce, as well as our resulting architectural\n\nand production outcomes. In order to get fast flow of work from Development into Operations, with high quality and great customer outcomes, we must organize our teams and our work so\n\nthat Conway’s Law works to our advantage. Done poorly, Conway’s Law will prevent teams from working safely and independently; instead, they will be tightly-coupled together, all waiting on each other for work to be done, with even small changes creating potentially global, catastrophic consequences.\n\nAn example of how Conway’s Law can either impede or reinforce\n\nour goals can be seen in a technology that was developed at Etsy called Sprouter. Etsy’s DevOps journey began in 2009, and is one\n\nof the most admired DevOps organizations, with 2014 revenue of nearly $200 million and a successful IPO in 2015.\n\nOriginally developed in 2007, Sprouter connected people, processes, and technology in ways that created many undesired outcomes. Sprouter, shorthand for “stored procedure router,” was originally designed to help make life easier for the developers and\n\ndatabase teams. As Ross Snyder, a senior engineer at Etsy, said during his presentation at Surge 2011, “Sprouter was designed to\n\nallow the Dev teams to write PHP code in the application, the\n\nDBAs to write SQL inside Postgres, with Sprouter helping them\n\nmeet in the middle.”\n\nSprouter resided between their front-end PHP application and the\n\nPostgres database, centralizing access to the database and hiding\n\nthe database implementation from the application layer. The\n\nproblem was that adding any changes to business logic resulted in\n\nsignificant friction between developers and the database teams. As\n\nSnyder observed, “For nearly any new site functionality, Sprouter\n\nrequired that the DBAs write a new stored procedure. As a result,\n\nevery time developers wanted to add new functionality, they would need something from the DBAs, which often required them to wade through a ton of bureaucracy.” In other words, developers\n\ncreating new functionality had a dependency on the DBA team, which needed to be prioritized, communicated, and coordinated, resulting in work sitting in queues, meetings, longer lead times, and so forth. This is because Sprouter created a tight coupling between the development and database teams, preventing developers from being able to independently develop, test, and deploy their code into production.\n\nAlso, the database stored procedures were tightly-coupled to\n\nSprouter—any time a stored procedure was changed, it required changes to Sprouter too. The result was that Sprouter became an ever-larger single point of failure. Snyder explained that everything was so tightly-coupled and required such a high level of synchronization as a result, that almost every deployment caused a mini-outage.\n\nBoth the problems associated with Sprouter and their eventual solution can be explained by Conway’s Law. Etsy initially had two\n\nteams, the developers and the DBAs, who were each responsible for two layers of the service, the application logic layer and stored\n\nprocedure layer. Two teams working on two layers, as Conway’s Law predicts. Sprouter was intended to make life easier for both\n\nteams, but it didn’t work as expected—when business rules\n\nchanged, instead of changing only two layers, they now needed to make changes to three layers (in the application, in the stored\n\nprocedures, and now in Sprouter). The resulting challenges of coordinating and prioritizing work across three teams significantly\n\nincreased lead times and caused reliability problems.\n\nIn the spring of 2009, as part of what Snyder called “the great Etsy cultural transformation,” Chad Dickerson joined as their new\n\nCTO. Dickerson put into motion many things, including a massive\n\ninvestment into site stability, having developers perform their own deployments into production, as well as beginning a two-year\n\njourney to eliminate Sprouter.\n\nTo do this, the team decided to move all the business logic from the database layer into the application layer, removing the need\n\nfor Sprouter. They created a small team that wrote a PHP Object Relational Mapping (ORM) layer,† enabling the front-end developers to make calls directly to the database and reducing the\n\nnumber of teams required to change business logic from three teams down to one team.\n\nAs Snyder described, “We started using the ORM for any new\n\nareas of the site and migrated small parts of our site from Sprouter to the ORM over time. It took us two years to migrate the entire\n\nsite off of Sprouter. And even though we all grumbled about Sprouter the entire time, it remained in production throughout.”\n\nBy eliminating Sprouter, they also eliminated the problems associated with multiple teams needing to coordinate for business\n\nlogic changes, decreased the number of handoffs, and significantly increased the speed and success of production deployments,\n\nimproving site stability. Furthermore, because small teams could independently develop and deploy their code without requiring\n\nanother team to make changes in other areas of the system,\n\ndeveloper productivity increased.\n\nSprouter was finally removed from production and Etsy’s version control repositories in early 2001. As Snyder said, “Wow, it felt good.”‡\n\nAs Snyder and Etsy experienced, how we design our organization dictates how work is performed, and, therefore, the outcomes we\n\nachieve. Throughout the rest of this chapter we will explore how Conway’s Law can negatively impact the performance of our value\n\nstream, and, more importantly, how we organize our teams to use\n\nConway’s Law to our advantage.\n\nORGANIZATIONAL ARCHETYPES\n\nIn the field of decision sciences, there are three primary types of organizational structures that inform how we design our DevOps\n\nvalue streams with Conway’s Law in mind: functional, matrix, and market. They are defined by Dr. Roberto Fernandez as follows:\n\nFunctional-oriented organizations optimize for expertise,\n\ndivision of labor, or reducing cost. These organizations\n\ncentralize expertise, which helps enable career growth and skill development, and often have tall hierarchical organizational\n\nstructures. This has been the prevailing method of organization\n\nfor Operations (i.e., server admins, network admins, database\n\nadmins, and so forth are all organized into separate groups).\n\nMatrix-oriented organizations attempt to combine functional and market orientation. However, as many who work in or\n\nmanage matrix organizations observe, matrix organizations often result in complicated organizational structures, such as\n\nindividual contributors reporting to two managers or more, and sometimes achieving neither of the goals of functional or\n\nmarket orientation.\n\nMarket-oriented organizations optimize for responding quickly\n\nto customer needs. These organizations tend to be flat, composed of multiple, cross-functional disciplines (e.g.,\n\nmarketing, engineering, etc.), which often lead to potential redundancies across the organization. This is how many\n\nprominent organizations adopting DevOps operate—in extreme examples, such as at Amazon or Netflix, each service team is\n\nsimultaneously responsible for feature delivery and service support.§\n\nWith these three categories of organizations in mind, let’s explore further how an overly functional orientation, especially in\n\nOperations, can cause undesired outcomes in the technology value stream, as Conway’s Law would predict.\n\nPROBLEMS OFTEN CAUSED BY OVERLY FUNCTIONAL ORIENTATION (“OPTIMIZING FOR COST”)\n\nIn traditional IT Operations organizations, we often use functional orientation to organize our teams by their specialties. We put the\n\ndatabase administrators in one group, the network administrators in another, the server administrators in a third, and so forth. One of the most visible consequences of this is long lead times,\n\nespecially for complex activities like large deployments where we must open up tickets with multiple groups and coordinate work handoffs, resulting in our work waiting in long queues at every step.\n\nCompounding the issue, the person performing the work often has little visibility or understanding of how their work relates to any\n\nvalue stream goals (e.g., “I’m just configuring servers because someone told me to.”). This places workers in a creativity and motivation vacuum.\n\nThe problem is exacerbated when each Operations functional area has to serve multiple value streams (i.e., multiple Development teams) who all compete for their scarce cycles. In order for Development teams to get their work done in a timely manner, we\n\noften have to escalate issues to a manager or director, and eventually to someone (usually an executive) who can finally prioritize the work against the global organizational goals instead\n\nof the functional silo goals. This decision must then get cascaded down into each of the functional areas to change the local priorities, and this, in turn, slows down other teams. When every team expedites their work, the net result is that every project ends\n\nup moving at the same slow crawl.\n\nIn addition to long queues and long lead times, this situation results in poor handoffs, large amounts of re-work, quality issues,\n\nbottlenecks, and delays. This gridlock impedes the achievement of\n\nimportant organizational goals, which often far outweigh the desire to reduce costs.¶\n\nSimilarly, functional orientation can also be found with centralized QA and Infosec functions, which may have worked fine (or at least, well enough) when performing less frequent software\n\nreleases. However, as we increase the number of Development teams and their deployment and release frequencies, most functionally-oriented organizations will have difficulty keeping up\n\nand delivering satisfactory outcomes, especially when their work is being performed manually. Now we’ll study how market oriented organizations work.\n\nENABLE MARKET-ORIENTED TEAMS (“OPTIMIZING FOR SPEED”)\n\nBroadly speaking, to achieve DevOps outcomes, we need to reduce the effects of functional orientation (“optimizing for cost”) and enable market orientation (“optimizing for speed”) so we can have\n\nmany small teams working safely and independently, quickly delivering value to the customer.\n\nTaken to the extreme, market-oriented teams are responsible not\n\nonly for feature development, but also for testing, securing, deploying, and supporting their service in production, from idea conception to retirement. These teams are designed to be cross- functional and independent—able to design and run user\n\nexperiments, build and deliver new features, deploy and run their service in production, and fix any defects without manual dependencies on other teams, thus enabling them to move faster.\n\nThis model has been adopted by Amazon and Netflix and is touted\n\nby Amazon as one of the primary reasons behind their ability to move fast even as they grow.\n\nTo achieve market orientation, we won’t do a large, top-down reorganization, which often creates large amounts of disruption, fear, and paralysis. Instead, we will embed the functional\n\nengineers and skills (e.g., Ops, QA, Infosec) into each service team, or provide their capabilities to teams through automated self-service platforms that provide production-like environments, initiate automated tests, or perform deployments.\n\nThis enables each service team to independently deliver value to the customer without having to open tickets with other groups, such as IT Operations, QA, or Infosec.**\n\nMAKING FUNCTIONAL ORIENTATION WORK\n\nHaving just recommended market-orientated teams, it is worth pointing out that it is possible to create effective, high-velocity\n\norganizations with functional orientation. Cross-functional and market-oriented teams are one way to achieve fast flow and reliability, but they are not the only path. We can also achieve our desired DevOps outcomes through functional orientation, as long\n\nas everyone in the value stream views customer and organizational outcomes as a shared goal, regardless of where they reside in the organization.\n\nFigure 12: Functional vs. market orientation\n\nLeft: Functional orientation: all work flows through centralized IT Operations; Right: Market orientation: all product teams can deploy their loosely-coupled components self-service into production. (Source: Humble, Molesky, and O’Reilly, Lean Enterprise, Kindle edition, 4523 & 4592.)\n\nFor example, high performance with a functional-oriented and centralized Operations group is possible, as long as service teams\n\nget what they need from Operations reliably and quickly (ideally on demand) and vice-versa. Many of the most admired DevOps organizations retain functional orientation of Operations,\n\nincluding Etsy, Google, and GitHub.\n\nWhat these organizations have in common is a high-trust culture that enables all departments to work together effectively, where all\n\nwork is transparently prioritized and there is sufficient slack in the system to allow high-priority work to be completed quickly. This is, in part, enabled by automated self-service platforms that build\n\nquality into the products everyone is building.\n\nIn the Lean manufacturing movement of the 1980s, many researchers were puzzled by Toyota’s functional orientation, which\n\nwas at odds with the best practice of having cross-functional,",
      "page_number": 137
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 146-153)",
      "start_page": 146,
      "end_page": 153,
      "detection_method": "topic_boundary",
      "content": "market-oriented teams. They were so puzzled it was called “the second Toyota paradox.”\n\nAs Mike Rother wrote in Toyota Kata, “As tempting as it seems, one cannot reorganize your way to continuous improvement and adaptiveness. What is decisive is not the form of the organization, but how people act and react. The roots of Toyota’s success lie not\n\nin its organizational structures, but in developing capability and habits in its people. It surprises many people, in fact, to find that Toyota is largely organized in a traditional, functional-department\n\nstyle.” It is this development of habits and capabilities in people and the workforce that are the focus of our next sections.\n\nTESTING, OPERATIONS, AND SECURITY AS EVERYONE’S JOB, EVERY DAY\n\nIn high-performing organizations, everyone within the team\n\nshares a common goal—quality, availability, and security aren’t the responsibility of individual departments, but are a part of everyone’s job, every day.\n\nThis means that the most urgent problem of the day may be working on or deploying a customer feature or fixing a Severity 1 production incident. Alternatively, the day may require reviewing\n\na fellow engineer’s change, applying emergency security patches to production servers, or making improvements so that fellow engineers are more productive.\n\nReflecting on shared goals between Development and Operations, Jody Mulkey, CTO at Ticketmaster, said, “For almost 25 years, I used an American football metaphor to describe Dev and Ops. You\n\nknow, Ops is defense, who keeps the other team from scoring, and Dev is offense, trying to score goals. And one day, I realized how flawed this metaphor was, because they never all play on the field\n\nat the same time. They’re not actually on the same team!”\n\nHe continued, “The analogy I use now is that Ops are the offensive linemen, and Dev are the ‘skill’ positions (like the quarterback and\n\nwide receivers) whose job it is to move the ball down the field—the job of Ops is to help make sure Dev has enough time to properly execute the plays.”\n\nA striking example of how shared pain can reinforce shared goals is when Facebook was undergoing enormous growth in 2009. They were experiencing significant problems related to code\n\ndeployments—while not all issues caused customer-impacting issues, there was chronic firefighting and long hours. Pedro Canahuati, their director of production engineering, described a meeting full of Ops engineers where someone asked that all people\n\nnot working on an incident close their laptops, and no one could.\n\nOne of the most significant things they did to help change the\n\noutcomes of deployments was to have all Facebook engineers, engineering managers, and architects rotate through on-call duty for the services they built. By doing this, everyone who worked on the service experienced visceral feedback on the upstream\n\narchitectural and coding decisions they made, which made an enormous positive impact on the downstream outcomes.\n\nENABLE EVERY TEAM MEMBER TO BE A GENERALIST\n\nIn extreme cases of a functionally-oriented Operations\n\norganization, we have departments of specialists, such as network\n\nadministrators, storage administrators, and so forth. When departments over-specialize, it causes siloization, which Dr. Spear\n\ndescribes as when departments “operate more like sovereign\n\nstates.” Any complex operational activity then requires multiple handoffs and queues between the different areas of the\n\ninfrastructure, leading to longer lead times (e.g., because every network change must be made by someone in the networking\n\ndepartment).\n\nBecause we rely upon an ever increasing number of technologies,\n\nwe must have engineers who have specialized and achieved mastery in the technology areas we need. However, we don’t want\n\nto create specialists who are “frozen in time,” only understanding and able to contribute to that one area of the value stream.\n\nOne countermeasure is to enable and encourage every team\n\nmember to be a generalist. We do this by providing opportunities for engineers to learn all the skills necessary to build and run the\n\nsystems they are responsible for, and regularly rotating people\n\nthrough different roles. The term full stack engineer is now commonly used (sometimes as a rich source of parody) to describe\n\ngeneralists who are familiar—at least have a general level of\n\nunderstanding—with the entire application stack (e.g., application code, databases, operating systems, networking, cloud).\n\nTable 2: Specialists vs. Generalists vs. “E-shaped” Staff (experience, expertise, exploration, and execution)\n\n(Source: Scott Prugh, “Continuous Delivery,” ScaledAgileFramework.com, February 14, 2013, http://scaledagileframework.com/continuous-delivery/.)\n\nScott Prugh writes that CSG International has undergone a\n\ntransformation that brings most resources required to build and\n\nrun the product onto one team, including analysis, architecture, development, test, and operations. “By cross-training and growing\n\nengineering skills, generalists can do orders of magnitude more work than their specialist counterparts, and it also improves our\n\noverall flow of work by removing queues and wait time.” This\n\napproach is at odds with traditional hiring practices, but, as Prugh explains, it is well worth it. “Traditional managers will often object\n\nto hiring engineers with generalist skill sets, arguing that they are\n\nmore expensive and that ‘I can hire two server administrators for every multi-skilled operations engineer.’” However, the business\n\nbenefits of enabling faster flow are overwhelming. Furthermore,\n\nas Prugh notes, “[I]nvesting in cross training is the right thing for\n\n[employees’] career growth, and makes everyone’s work more\n\nfun.”\n\nWhen we value people merely for their existing skills or performance in their current role rather than for their ability to\n\nacquire and deploy new skills, we (often inadvertently) reinforce\n\nwhat Dr. Carol Dweck describes as the fixed mindset, where people view their intelligence and abilities as static “givens” that\n\ncan’t be changed in meaningful ways.\n\nInstead, we want to encourage learning, help people overcome learning anxiety, help ensure that people have relevant skills and a\n\ndefined career road map, and so forth. By doing this, we help\n\nfoster a growth mindset in our engineers—after all, a learning organization requires people who are willing to learn. By\n\nencouraging everyone to learn, as well as providing training and support, we create the most sustainable and least expensive way to\n\ncreate greatness in our teams—by investing in the development of\n\nthe people we already have.\n\nAs Jason Cox, Director of Systems Engineering at Disney, described, “Inside of Operations, we had to change our hiring\n\npractices. We looked for people who had ‘curiosity, courage, and candor,’ who were not only capable of being generalists but also\n\nrenegades...We want to promote positive disruption so our\n\nbusiness doesn’t get stuck and can move into the future.” As we’ll see in the next section, how we fund our teams also affects our\n\noutcomes.\n\nFUND NOT PROJECTS, BUT SERVICES AND PRODUCTS\n\nAnother way to enable high-performing outcomes is to create\n\nstable service teams with ongoing funding to execute their own\n\nstrategy and road map of initiatives. These teams have the dedicated engineers needed to deliver on concrete commitments\n\nmade to internal and external customers, such as features, stories, and tasks.\n\nContrast this to the more traditional model where Development\n\nand Test teams are assigned to a “project” and then reassigned to another project as soon as the project is completed and funding\n\nruns out. This leads to all sorts of undesired outcomes, including\n\ndevelopers being unable to see the long-term consequences of decisions they make (a form of feedback) and a funding model\n\nthat only values and pays for the earliest stages of the software life\n\ncycle—which, tragically, is also the least expensive part for successful products or services.††\n\nOur goal with a product-based funding model is to value the\n\nachievement of organizational and customer outcomes, such as revenue, customer lifetime value, or customer adoption rate,\n\nideally with the minimum of output (e.g., amount of effort or time,\n\nlines of code). Contrast this to how projects are typically measured, such as whether it was completed within the promised\n\nbudget, time, and scope.\n\nDESIGN TEAM BOUNDARIES IN ACCORDANCE WITH CONWAY’S LAW\n\nAs organizations grow, one of the largest challenges is maintaining effective communication and coordination between people and\n\nteams. All too often, when people and teams reside on a different\n\nfloor, in a different building, or in a different time zone, creating\n\nand maintaining a shared understanding and mutual trust becomes more difficult, impeding effective collaboration.\n\nCollaboration is also impeded when the primary communication\n\nmechanisms are work tickets and change requests, or worse, when teams are separated by contractual boundaries, such as when\n\nwork is performed by an outsourced team.\n\nAs we saw in the Etsy Sprouter example at the beginning of this chapter, the way we organize teams can create poor outcomes, a\n\nside effect of Conway’s Law. These include splitting teams by\n\nfunction (e.g., by putting developers and testers in different locations or by outsourcing testers entirely) or by architectural\n\nlayer (e.g., application, database).\n\nThese configurations require significant communication and coordination between teams, but still results in a high amount of\n\nrework, disagreements over specifications, poor handoffs, and\n\npeople sitting idle waiting for somebody else.\n\nIdeally, our software architecture should enable small teams to be\n\nindependently productive, sufficiently decoupled from each other\n\nso that work can be done without excessive or unnecessary communication and coordination.\n\nCREATE LOOSELY-COUPLED ARCHITECTURES TO ENABLE DEVELOPER PRODUCTIVITY AND SAFETY\n\nWhen we have a tightly-coupled architecture, small changes can\n\nresult in large scale failures. As a result, anyone working in one\n\npart of the system must constantly coordinate with anyone else\n\nworking in another part of the system they may affect, including navigating complex and bureaucratic change management\n\nprocesses.\n\nFurthermore, to test that the entire system works together requires integrating changes with the changes from hundreds, or\n\neven thousands, of other developers, which may, in turn, have\n\ndependencies on tens, hundreds, or thousands of interconnected systems. Testing is done in scarce integration test environments,\n\nwhich often require weeks to obtain and configure. The result is\n\nnot only long lead times for changes (typically measured in weeks or months) but also low developer productivity and poor\n\ndeployment outcomes.\n\nIn contrast, when we have an architecture that enables small teams of developers to independently implement, test, and deploy\n\ncode into production safely and quickly, we can increase and\n\nmaintain developer productivity and improve deployment outcomes. These characteristics can be found in service-oriented\n\narchitectures (SOAs) first described in the 1990s, in which\n\nservices are independently testable and deployable. A key feature of SOAs is that they’re composed of loosely-coupled services with bounded contexts.‡‡\n\nHaving architecture that is loosely-coupled means that services can update in production independently, without having to update\n\nother services. Services must be decoupled from other services\n\nand, just as important, from shared databases (although they can share a database service, provided they don’t have any common\n\nschemas).",
      "page_number": 146
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 154-162)",
      "start_page": 154,
      "end_page": 162,
      "detection_method": "topic_boundary",
      "content": "Bounded contexts are described in the book Domain Driven\n\nDesign by Eric J. Evans. The idea is that developers should be able to understand and update the code of a service without knowing\n\nanything about the internals of its peer services. Services interact\n\nwith their peers strictly through APIs and thus don’t share data structures, database schemata, or other internal representations of\n\nobjects. Bounded contexts ensure that services are\n\ncompartmentalized and have well-defined interfaces, which also enables easier testing.\n\nRandy Shoup, former Engineering Director for Google App\n\nEngine, observed that “organizations with these types of service- oriented architectures, such as Google and Amazon, have\n\nincredible flexibility and scalability. These organizations have tens\n\nof thousands of developers where small teams can still be incredibly productive.”\n\nKEEP TEAM SIZES SMALL (THE “TWO-PIZZA TEAM” RULE)\n\nConway’s Law helps us design our team boundaries in the context of desired communication patterns, but it also encourages us to\n\nkeep our team sizes small, reducing the amount of inter-team\n\ncommunication and encouraging us to keep the scope of each team’s domain small and bounded.\n\nAs part of its transformation initiative away from a monolithic\n\ncode base in 2002, Amazon used the two-pizza rule to keep team sizes small—a team only as large as can be fed with two pizzas—\n\nusually about five to ten people.\n\nThis limit on size has four important effects:\n\n1. It ensures the team has a clear, shared understanding of the\n\nsystem they are working on. As teams get larger, the amount of communication required for everybody to know what’s going\n\non scales in a combinatorial fashion.\n\n2. It limits the growth rate of the product or service being worked on. By limiting the size of the team, we limit the rate at which their system can evolve. This also helps to ensure the team\n\nmaintains a shared understanding of the system.\n\n3. It decentralizes power and enables autonomy. Each two-pizza\n\nteam (2PT) is as autonomous as possible. The team’s lead,\n\nworking with the executive team, decides on the key business metric that the team is responsible for, known as the fitness\n\nfunction, which becomes the overall evaluation criteria for the\n\nteam’s experiments. The team is then able to act autonomously to maximize that metric.§§\n\n4. Leading a 2PT is a way for employees to gain some leadership\n\nexperience in an environment where failure does not have catastrophic consequences. An essential element of Amazon’s\n\nstrategy was the link between the organizational structure of a\n\n2PT and the architectural approach of a service-oriented architecture.\n\nAmazon CTO Werner Vogels explained the advantages of this\n\nstructure to Larry Dignan of Baseline in 2005. Dignan writes:\n\n“Small teams are fast...and don’t get bogged down in so-called\n\nadministrivia….Each group assigned to a particular business is\n\ncompletely responsible for it….The team scopes the fix, designs it, builds it, implements it and monitors its ongoing use. This\n\nway, technology programmers and architects get direct\n\nfeedback from the business people who use their code or applications—in regular meetings and informal conversations.”\n\nAnother example of how architecture can profoundly improve\n\nproductivity is the API Enablement program at Target, Inc.\n\nCase Study API Enablement at Target (2015)\n\nTarget is the sixth-largest retailer in the US and spends over $1 billion on technology annually. Heather Mickman, a\n\ndirector of development for Target, described the beginnings\n\nof their DevOps journey: “In the bad old days, it used to take ten different teams to provision a server at Target, and when\n\nthings broke, we tended to stop making changes to prevent\n\nfurther issues, which of course makes everything worse.”\n\nThe hardships associated with getting environments and\n\nperforming deployments created significant difficulties for\n\ndevelopment teams, as did getting access to data they needed. As Mickman described:\n\nThe problem was that much of our core data, such as\n\ninformation on inventory, pricing, and stores, was locked up in legacy systems and mainframes. We\n\noften had multiple sources of truths of data, especially between e-commerce and our physical stores, which\n\nwere owned by different teams, with different data\n\nstructures and different priorities....The result was that if a new development team wanted to build something\n\nfor our guests, it would take three to six months to\n\nbuild the integrations to get the data they needed. Worse, it would take another three to six months to do\n\nthe manual testing to make sure they didn’t break\n\nanything critical, because of how many custom point- to-point integrations we had in a very tightly-coupled\n\nsystem. Having to manage the interactions with the\n\ntwenty to thirty different teams, along with all their dependencies, required lots of project managers,\n\nbecause of all the coordination and handoffs. It meant that development was spending all their time waiting in\n\nqueues, instead of delivering results and getting stuff\n\ndone.\n\nThis long lead time for retrieving and creating data in their\n\nsystems of record was jeopardizing important business\n\ngoals, such as integrating the supply chain operations of Target’s physical stores and their e-commerce site, which\n\nnow required getting inventory to stores and customer\n\nhomes. This pushed the Target supply chain well beyond what it was designed for, which was merely to facilitate the\n\nmovement of goods from vendors to distribution centers and\n\nstores.\n\nIn an attempt to solve the data problem, in 2012 Mickman\n\nled the API Enablement team to enable development teams\n\nto “deliver new capabilities in days instead of months.” They wanted any engineering team inside of Target to be able to\n\nget and store the data they needed, such as information on\n\ntheir products or their stores, including operating hours,\n\nlocation, whether there was as Starbucks on-site, and so\n\nforth.\n\nTime constraints played a large role in team selection.\n\nMickman explained that:\n\nBecause our team also needed to deliver capabilities in days, not months, I needed a team who could do the\n\nwork, not give it to contractors—we wanted people\n\nwith kickass engineering skills, not people who knew how to manage contracts. And to make sure our work\n\nwasn’t sitting in queue, we needed to own the entire\n\nstack, which meant that we took over the Ops requirements as well....We brought in many new tools\n\nto support continuous integration and continuous\n\ndelivery. And because we knew that if we succeeded, we would have to scale with extremely high growth, we\n\nbrought in new tools such as the Cassandra database and Kafka message broker. When we asked for\n\npermission, we were told no, but we did it anyway,\n\nbecause we knew we needed it.\n\nIn the following two years, the API Enablement team\n\nenabled fifty-three new business capabilities, including Ship\n\nto Store and Gift Registry, as well as their integrations with Instacart and Pinterest. As Mickman described, “Working\n\nwith Pinterest suddenly became very easy, because we just\n\nprovided them our APIs.”\n\nIn 2014, the API Enablement team served over 1.5 billion\n\nAPI calls per month. By 2015, this had grown to seventeen\n\nbillion calls per month spanning ninety different APIs. To\n\nsupport this capability, they routinely performed eighty\n\ndeployments per week.\n\nThese changes have created major business benefits for\n\nTarget—digital sales increased 42% during the 2014 holiday\n\nseason and increased another 32% in Q2. During the Black Friday weekend of 2015, over 280k in-store pickup orders\n\nwere created. By 2015, their goal is to enable 450 of their\n\n1,800 stores to be able to fulfill e-commerce orders, up from one hundred.\n\n“The API Enablement team shows what a team of\n\npassionate change agents can do,” Mickman says. “And it help set us up for the next stage, which is to expand\n\nDevOps across the entire technology organization.”\n\nCONCLUSION\n\nThrough the Etsy and Target case studies, we can see how\n\narchitecture and organizational design can dramatically improve our outcomes. Done incorrectly, Conway’s Law will ensure that the\n\norganization creates poor outcomes, preventing safety and agility.\n\nDone well, the organization enables developers to safely and\n\nindependently develop, test, and deploy value to the customer.\n\n† Among many things, an ORM abstracts a database, enabling developers to do queries and data manipulation as if they were merely another object in the programming language. Popular ORMs include Hibernate for Java, SQLAlchemy for Python, and ActiveRecord for Ruby on Rails.\n\n‡ Sprouter was one of many technologies used in development and production that Etsy eliminated as part of their\n\ntransformation.\n\n§ However, as will be explained later, equally prominent organizations such as Etsy and GitHub have functional\n\norientation.\n\n¶ Adrian Cockcroft remarked, “For companies who are now coming off of five-year IT outsourcing contracts, it’s like they’ve been frozen in time, during one of the most disruptive times in technology.” In other words, IT\n\noutsourcing is a tactic used to control costs through contractually-enforced stasis, with firm fixed prices that schedule annual cost reductions. However, it often results in organizations being unable to respond to changing business and technology needs.\n\n** For the remainder of this books, we will use service teams interchangeably with feature teams, product teams, development teams, and delivery teams. The intent is to specify the team primarily developing, testing, and securing the code so that value is delivered to the customer.\n\n†† As John Lauderbach, currently VP of Information Technology at Roche Bros. Supermarkets, quipped, “Every\n\nnew application is like a free puppy. It’s not the upfront capital cost that kills you…It’s the ongoing maintenance and support.”\n\n‡‡ These properties are also found in “microservices,” which build upon the principles of SOA. One popular set of\n\npatterns for modern web architecture based on these principles is the “12-factor app.”\n\n§§ In the Netflix culture, one of the seven key values is “highly aligned, loosely-coupled.”\n\n8 How to Get Great\n\nOutcomes by Integrating Operations into the Daily Work of\n\nDevelopment\n\nOur goal is to enable market-oriented outcomes where many small teams can quickly and independently deliver value to the\n\ncustomer. This can be a challenge to achieve when Operations is\n\ncentralized and functionally-oriented, having to serve the needs of many different development teams with potentially wildly\n\ndifferent needs. The result can often be long lead times for needed Ops work, constant reprioritization and escalation, and poor\n\ndeployment outcomes.\n\nWe can create more market-oriented outcomes by better\n\nintegrating Ops capabilities into Dev teams, making both more efficient and productive. In this chapter, we’ll explore many ways to achieve this, both at the organizational level and through daily\n\nrituals. By doing this, Ops can significantly improve the productivity of Dev teams throughout the entire organization, as well as enable better collaboration and organizational outcomes.\n\nAt Big Fish Games, which develops and supports hundreds of\n\nmobile and thousands of PC games and had more than $266\n\nmillion in revenue in 2013, VP of IT Operations Paul Farrall was in\n\ncharge of the centralized Operations organization. He was\n\nresponsible for supporting many different business units that had\n\na great deal of autonomy.\n\nEach of these business units had dedicated development teams\n\nwho often chose wildly different technologies. When these groups\n\nwanted to deploy new functionality, they would have to compete\n\nfor a common pool of scarce Ops resources. Furthermore,\n\neveryone was struggling with unreliable Test and Integration environments, as well as extremely cumbersome release processes.\n\nFarrall thought the best way to solve this problem was by embedding Ops expertise into Development teams. He observed, “When Dev teams had problems with testing or deployment, they needed more than just technology or environments. What they also needed was help and coaching. At first, we embedded Ops engineers and architects into each of the Dev teams, but there\n\nsimply weren’t enough Ops engineers to cover that many teams. We were able to help more teams with what we called an Ops\n\nliaison model and with fewer people.”\n\nFarrall defined two types of Ops liaisons: the business relationship manager and the dedicated release engineer. The business relationship managers worked with product management, line-of- business owners, project management, Dev management, and developers. They became intimately familiar with product group\n\nbusiness drivers and product road maps, acted as advocates for product owners inside of Operations, and helped their product",
      "page_number": 154
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 163-173)",
      "start_page": 163,
      "end_page": 173,
      "detection_method": "topic_boundary",
      "content": "teams navigate the Operations landscape to prioritize and\n\nstreamline work requests.\n\nSimilarly, the dedicated release engineer became intimately\n\nfamiliar with the product’s Development and QA issues, and\n\nhelped them get what they needed from the Ops organization to\n\nachieve their goals. They were familiar with the typical Dev and\n\nQA requests for Ops, and would often execute the needed work\n\nthemselves. As needed, they would also pull in dedicated technical\n\nOps engineers (e.g., DBAs, Infosec, storage engineers, network\n\nengineers), and help determine which self-service tools the entire\n\nOperations group should prioritize building.\n\nBy doing this, Farrall was able to help Dev teams across the organization become more productive and achieve their team goals. Furthermore, he helped the teams prioritize around his global Ops constraints, reducing the number of surprises\n\ndiscovered mid-project and ultimately increasing the overall project throughput.\n\nFarrall notes that both working relationships with Operations and code release velocity were noticeably improved as a result of the changes. He concludes, “The Ops liaison model allowed us to\n\nembed IT Operations expertise into the Dev and Product teams without adding new headcount.”\n\nThe DevOps transformation at Big Fish Games shows how a centralized Operations team was able to achieve the outcomes\n\ntypically associated with market-oriented teams. We can employ the three following broad strategies:\n\nCreate self-service capabilities to enable developers in the service teams to be productive.\n\nEmbed Ops engineers into the service teams.\n\nAssign Ops liaisons to the service teams when embedding Ops\n\nis not possible.\n\nLastly, we describe how Ops engineers can integrate into the Dev team rituals used in their daily work, including daily standups,\n\nplanning, and retrospectives.\n\nCREATE SHARED SERVICES TO INCREASE DEVELOPER PRODUCTIVITY\n\nOne way to enable market-oriented outcomes is for Operations to\n\ncreate a set of centralized platforms and tooling services that any Dev team can use to become more productive, such as getting\n\nproduction-like environments, deployment pipelines, automated testing tools, production telemetry dashboards, and so forth.† By doing this, we enable Dev teams to spend more time building functionality for their customer, as opposed to obtaining all the\n\ninfrastructure required to deliver and support that feature in\n\nproduction.\n\nAll the platforms and services we provide should (ideally) be automated and available on demand, without requiring a\n\ndeveloper to open up a ticket and wait for someone to manually perform work. This ensures that Operations doesn’t become a\n\nbottleneck for their customers (e.g., “We received your work\n\nrequest, and it will take six weeks to manually configure those test environments.”).‡\n\nBy doing this, we enable the product teams to get what they need,\n\nwhen they need it, as well as reduce the need for communications and coordination. As Damon Edwards observed, “Without these\n\nself-service Operations platforms, the cloud is just Expensive Hosting 2.0.”\n\nIn almost all cases, we will not mandate that internal teams use\n\nthese platforms and services—these platform teams will have to\n\nwin over and satisfy their internal customers, sometimes even competing with external vendors. By creating this effective\n\ninternal marketplace of capabilities, we help ensure that the platforms and services we create are the easiest and most\n\nappealing choice available (the path of least resistance).\n\nFor instance, we may create a platform that provides a shared version control repository with pre-blessed security libraries, a\n\ndeployment pipeline that automatically runs code quality and\n\nsecurity scanning tools, which deploys our applications into known, good environments that already have production\n\nmonitoring tools installed on them. Ideally, we make life so much easier for Dev teams that they will overwhelmingly decide that\n\nusing our platform is the easiest, safest, and most secure means to get their applications into production.\n\nWe build into these platforms the cumulative and collective\n\nexperience of everyone in the organization, including QA,\n\nOperations, and Infosec, which helps to create an ever safer system of work. This increases developer productivity and makes\n\nit easy for product teams to leverage common processes, such as\n\nperforming automated testing and satisfying security and\n\ncompliance requirements.\n\nCreating and maintaining these platforms and tools is real product\n\ndevelopment—the customers of our platform aren’t our external customer but our internal Dev teams. Like creating any great\n\nproduct, creating great platforms that everyone loves doesn’t happen by accident. An internal platform team with poor\n\ncustomer focus will likely create tools that everyone will hate and quickly abandon for other alternatives, whether for another\n\ninternal platform team or an external vendor.\n\nDianne Marsh, Director of Engineering Tools at Netflix, states that\n\nher team’s charter is to “support our engineering teams’ innovation and velocity. We don’t build, bake, or deploy anything\n\nfor these teams, nor do we manage their configurations. Instead, we build tools to enable self-service. It’s okay for people to be\n\ndependent on our tools, but it’s important that they don’t become dependent on us.”\n\nOften, these platform teams provide other services to help their\n\ncustomers learn their technology, migrate off of other\n\ntechnologies, and even provide coaching and consulting to help elevate the state of the practice inside the organization. These\n\nshared services also facilitate standardization, which enable engineers to quickly become productive, even if they switch\n\nbetween teams. For instance, if every product team chooses a different toolchain, engineers may have to learn an entirely new\n\nset of technologies to do their work, putting the team goals ahead of the global goals.\n\nIn organizations where teams can only use approved tools, we can start by removing this requirement for a few teams, such as the\n\ntransformation team, so that we can experiment and discover what capabilities make those teams more productive.\n\nInternal shared services teams should continually look for internal\n\ntoolchains that are widely being adopted in the organization, deciding which ones make sense to be supported centrally and made available to everyone. In general, taking something that’s\n\nalready working somewhere and expanding its usage is far more likely to succeed than building these capabilities from scratch.§\n\nEMBED OPS ENGINEERS INTO OUR SERVICE TEAMS\n\nAnother way we can enable more market-oriented outcomes is by\n\nenabling product teams to become more self-sufficient by embedding Operations engineers within them, thus reducing their reliance on centralized Operations. These product teams may also\n\nbe completely responsible for service delivery and service support.\n\nBy embedding Operations engineers into the Dev teams, their priorities are driven almost entirely by the goals of the product\n\nteams they are embedded in—as opposed to Ops focusing inwardly on solving their own problems. As a result, Ops engineers become more closely connected to their internal and external customers.\n\nFurthermore, the product teams often have the budget to fund the hiring of these Ops engineers, although interviewing and hiring decisions will likely still be done from the centralized Operations group, to ensure consistency and quality of staff.\n\nJason Cox said, “In many parts of Disney we have embedded Ops (system engineers) inside the product teams in our business units,\n\nalong with inside Development, Test, and even Information Security. It has totally changed the dynamics of how we work. As Operations Engineers, we create the tools and capabilities that\n\ntransform the way people work, and even the way they think. In traditional Ops, we merely drove the train that someone else built. But in modern Operations Engineering, we not only help build the train, but also the bridges that the trains roll on.”\n\nFor new large Development projects, we may initially embed Ops engineers into those teams. Their work may include helping decide what to build and how to build it, influencing the product\n\narchitecture, helping influence internal and external technology choices, helping create new capabilities in our internal platforms, and maybe even generating new operational capabilities. After the\n\nproduct is released to production, embedded Ops engineers may help with the production responsibilities of the Dev team.\n\nThey will take part in all of the Dev team rituals, such as planning\n\nmeetings, daily standups, and demonstrations where the team shows off new features and decides which ones to ship. As the need for Ops knowledge and capabilities decreases, Ops engineers\n\nmay transition to different projects or engagements, following the general pattern that the composition within product teams changes throughout its life cycle.\n\nThis paradigm has another important advantage: pairing Dev and Ops engineers together is an extremely efficient way to cross-train operations knowledge and expertise into a service team. It can also have the powerful benefit of transforming operations\n\nknowledge into automated code that can be far more reliable and widely reused.\n\nASSIGN AN OPS LIAISON TO EACH SERVICE TEAM\n\nFor a variety of reasons, such as cost and scarcity, we may be unable to embed Ops engineers into every product team. However, we can get many of the same benefits by assigning a designated\n\nliaison for each product team.\n\nAt Etsy, this model is called “designated Ops.” Their centralized Operations group continues to manage all the environments—not\n\njust production environments but also pre-production environments—to help ensure they remain consistent. The designated Ops engineer is responsible for understanding:\n\nWhat the new product functionality is and why we’re building it\n\nHow it works as it pertains to operability, scalability, and observability (diagramming is strongly encouraged)\n\nHow to monitor and collect metrics to ensure the progress,\n\nsuccess, or failure of the functionality\n\nAny departures from previous architectures and patterns, and the justifications for them\n\nAny extra needs for infrastructure and how usage will affect infrastructure capacity\n\nFeature launch plans\n\nFurthermore, just like in the embedded Ops model, this liaison\n\nattends the team standups, integrating their needs into the Operations road map and performing any needed tasks. We rely on these liaisons to escalate any resource contention or\n\nprioritization issue. By doing this, we identify any resource or time conflicts that should be evaluated and prioritized in the context of wider organizational goals.\n\nAssigning Ops liaisons allows us to support more product teams than the embedded Ops model. Our goal is to ensure that Ops is not a constraint for the product teams. If we find that Ops liaisons are stretched too thin, preventing the product teams from\n\nachieving their goals, then we will likely need to either reduce the number of teams each liaison supports or temporarily embed an Ops engineer into specific teams.\n\nINTEGRATE OPS INTO DEV RITUALS\n\nWhen Ops engineers are embedded or assigned as liaisons into\n\nour product teams, we can integrate them into our Dev team rituals. In this section, our goal is to help Ops engineers and other non-developers better understand the existing Development\n\nculture and proactively integrate them into all aspects of planning and daily work. As a result, Operations is better able to plan and radiate any needed knowledge into the product teams, influencing work long before it gets into production. The following sections\n\ndescribe some of the standard rituals used by Development teams using agile methods and how we would integrate Ops engineers into them. By no means are agile practices a prerequisite for this\n\nstep—as Ops engineers, our goal is to discover what rituals the product teams follow, integrate into them, and add value to them.¶\n\nAs Ernest Mueller observed, “I believe DevOps works a lot better if Operations teams adopt the same agile rituals that Dev teams have used—we’ve had fantastic successes solving many problems associated with Ops pain points, as well as integrating better with\n\nDev teams.”\n\nINVITE OPS TO OUR DEV STANDUPS\n\nOne of the Dev rituals popularized by Scrum is the daily standup, a quick meeting where everyone on the team gets together and presents to each other three things: what was done yesterday,\n\nwhat is going to be done today, and what is preventing you from getting your work done.**\n\nThe purpose of this ceremony is to radiate information throughout the team and to understand the work that is being done and is going to be done. By having team members present this information to each other, we learn about any tasks that are\n\nexperiencing roadblocks and discover ways to help each other move our work toward completion. Furthermore, by having managers present, we can quickly resolve prioritization and\n\nresource conflicts.\n\nA common problem is that this information is compartmentalized within the Development team. By having Ops engineers attend,\n\nOperations can gain an awareness of the Development team’s activities, enabling better planning and preparation—for instance, if we discover that the product team is planning a big feature rollout in two weeks, we can ensure that the right people and\n\nresources are available to support the rollout. Alternatively, we may highlight areas where closer interaction or more preparation is needed (e.g., creating more monitoring checks or automation\n\nscripts). By doing this, we create the conditions where Operations can help solve our current team problems (e.g., improving performance by tuning the database, instead of optimizing code) or future problems before they turn into a crisis (e.g., creating\n\nmore integration test environments to enable performance testing).\n\nINVITE OPS TO OUR DEV RETROSPECTIVES\n\nAnother widespread agile ritual is the retrospective. At the end of each development interval, the team discusses what was\n\nsuccessful, what could be improved, and how to incorporate the successes and improvements in future iterations or projects. The team comes up with ideas to make things better and reviews\n\nexperiments from the previous iteration. This is one of the primary mechanisms where organizational learning and the development of countermeasures occurs, with resulting work implemented immediately or added to the team’s backlog.\n\nHaving Ops engineers attend our project team retrospectives means they can also benefit from any new learnings. Furthermore,\n\nwhen there is a deployment or release in that interval, Operations should present the outcomes and any resulting learnings, creating\n\nfeedback into the product team. By doing this, we can improve\n\nhow future work is planned and performed, improving our outcomes. Examples of feedback that Operations can bring to a\n\nretrospective include:\n\n“Two weeks ago, we found a monitoring blind-spot and agreed\n\non how to fix it. It worked. We had an incident last Tuesday,\n\nand we were able to quickly detect and correct it before any customers were impacted.”\n\n“Last week’s deployment was one of the most difficult and\n\nlengthy we’ve had in over a year. Here are some ideas on how it can be improved.”\n\n“The promotion campaign we did last week was far more\n\ndifficult than we thought it would be, and we should probably not make an offer like that again. Here are some ideas on other\n\noffers we can make to achieve our goals.”\n\n“During the last deployment, the biggest problem we had was our firewall rules are now thousands of lines long, making it\n\nextremely difficult and risky to change. We need to re-architect\n\nhow we prevent unauthorized network traffic.”\n\nFeedback from Operations helps our product teams better see and\n\nunderstand the downstream impact of decisions they make. When\n\nthere are negative outcomes, we can make the changes necessary to prevent them in the future. Operations feedback will also likely\n\nidentify more problems and defects that should be fixed—it may\n\neven uncover larger architectural issues that need to be addressed.\n\nThe additional work identified during project team retrospectives\n\nfalls into the broad category of improvement work, such as fixing\n\ndefects, refactoring, and automating manual work. Product managers and project managers may want to defer or deprioritize\n\nimprovement work in favor of customer features.",
      "page_number": 163
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 174-182)",
      "start_page": 174,
      "end_page": 182,
      "detection_method": "topic_boundary",
      "content": "However, we must remind everyone that improvement of daily\n\nwork is more important than daily work itself, and that all teams\n\nmust have dedicated capacity for this (e.g., reserving 20% of all cycles for improvement work, scheduling one day per week or one\n\nweek per month, etc.). Without doing this, the productivity of the\n\nteam will almost certainly grind to a halt under the weight of its own technical and process debt.\n\nMAKE RELEVANT OPS WORK VISIBLE ON SHARED KANBAN BOARDS\n\nOften, Development teams will make their work visible on a project board or kanban board. It’s far less common, however, for\n\nwork boards to show the relevant Operations work that must be\n\nperformed in order for the application to run successfully in production, where customer value is actually created. As a result,\n\nwe are not aware of necessary Operations work until it becomes an urgent crisis, jeopardizing deadlines or creating a production\n\noutage.\n\nBecause Operations is part of the product value stream, we should put the Operations work that is relevant to product delivery on the\n\nshared kanban board. This enables us to more clearly see all the\n\nwork required to move our code into production, as well as keep track of all Operations work required to support the product.\n\nFurthermore, it enables us to see where Ops work is blocked and\n\nwhere work needs escalation, highlighting areas where we may need improvement.\n\nKanban boards are an ideal tool to create visibility, and visibility is\n\na key component in properly recognizing and integrating Ops work into all the relevant value streams. When we do this well, we\n\nachieve market-oriented outcomes, regardless of how we’ve drawn\n\nour organization charts.\n\nCONCLUSION\n\nThroughout this chapter, we explored ways to integrate\n\nOperations into the daily work of Development, and looked at how\n\nto make our work more visible to Operations. To accomplish this, we explored three broad strategies, including creating self-service\n\ncapabilities to enable developers in service teams to be productive,\n\nembedding Ops engineers into the service teams, and assigning Ops liaisons to the service teams when embedding Ops engineers\n\nwas not possible. Lastly, we described how Ops engineers can\n\nintegrate with the Dev team through inclusion in their daily work, including daily standups, planning, and retrospectives.\n\nPART II CONCLUSION\n\nIn Part II: Where to Start, we explored a variety of ways to think\n\nabout DevOps transformations, including how to choose where to start, relevant aspects of architecture and organizational design,\n\nand how to organize our teams. We also explored how to integrate\n\nOps into all aspects of Dev planning and daily work.\n\nIn Part III: The First Way, The Technical Practices of Flow, we\n\nwill now start to explore how to implement the specific technical\n\npractices to realize the principles of flow, which enable the fast flow of work from Development to Operations without causing\n\nchaos and disruption downstream.\n\n† The terms platform, shared service, and toolchain will be used interchangeably in this book.\n\n‡ Ernest Mueller observed, “At Bazaarvoice, the agreement was that these platform teams that make tools accept\n\nrequirements, but not work from other teams.”\n\n§ After all, designing a system upfront for re-use is a common and expensive failure mode of many enterprise\n\narchitectures.\n\n¶ However, if we discover that the entire Development organization merely sits at their desks all day without ever\n\ntalking to each other, we may have to find a different way to engage them, such as buying them lunch, starting a book club, taking turns doing “lunch and learn” presentations, or having conversations to discover what everyone’s biggest problems are, so that we can figure out how we can make their lives better.\n\n** Scrum is an agile development methodology, described as “a flexible, holistic product development strategy where a development team works as a unit to reach a common goal.” It was first fully described by Ken Schwaber and Mike Beedle in the book Agile Software Development with Scrum. In this book, we use the term “agile development” or “iterative development” to encompass the various techniques used by special methodologies such as Agile and Scrum.\n\nPart\n\nIntroduction\n\nIn Part III, our goal is to create the technical practices and\n\narchitecture required to enable and sustain the fast flow of work\n\nfrom Development into Operations without causing chaos and\n\ndisruption to the production environment or our customers. This means we need to reduce the risk associated with deploying and\n\nreleasing changes into production. We will do this by\n\nimplementing a set of technical practices known as continuous delivery.\n\nContinuous delivery includes creating the foundations of our\n\nautomated deployment pipeline, ensuring that we have automated tests that constantly validate that we are in a deployable state,\n\nhaving developers integrate their code into trunk daily, and architecting our environments and code to enable low-risk\n\nreleases. Primary focuses within these chapters include:\n\nCreating the foundation of our deployment pipeline\n\nEnabling fast and reliable automated testing\n\nEnabling and practicing continuous integration and testing\n\nAutomating, enabling, and architecting for low-risk releases\n\nImplementing these practices reduces the lead time to get\n\nproduction-like environments, enables continuous testing that\n\ngives everyone fast feedback on their work, enables small teams to\n\nsafely and independently develop, test, and deploy their code into\n\nproduction, and makes production deployments and releases a\n\nroutine part of daily work.\n\nFurthermore, integrating the objectives of QA and Operations into\n\neveryone’s daily work reduces firefighting, hardship, and toil,\n\nwhile making people more productive and increasing joy in the\n\nwork we do. We not only improve outcomes, but our organization\n\nis better able to win in the marketplace.\n\n9 Create the\n\nFoundations of Our Deployment Pipeline\n\nIn order to create fast and reliable flow from Dev to Ops, we must\n\nensure that we always use production-like environments at every stage of the value stream. Furthermore, these environments must\n\nbe created in an automated manner, ideally on demand from scripts and configuration information stored in version control,\n\nand entirely self-serviced, without any manual work required from\n\nOperations. Our goal is to ensure that we can re-create the entire production environment based on what’s in version control.\n\nAll too often, the only time we discover how our applications\n\nperform in anything resembling a production-like environment is during production deployment—far too late to correct problems\n\nwithout the customer being adversely impacted. An illustrative example of the spectrum of problems that can be caused by\n\ninconsistently built applications and environments is the Enterprise Data Warehouse program led by Em Campbell-Pretty at a large Australian telecommunications company in 2009. Campbell-Pretty became the general manager and business\n\nsponsor for this $200 million program, inheriting responsibility for all the strategic objectives that relied upon this platform.\n\nIn her presentation at the 2014 DevOps Enterprise Summit,\n\nCampbell-Pretty explained, “At the time, there were ten streams of\n\nwork in progress, all using waterfall processes, and all ten streams\n\nwere significantly behind schedule. Only one of the ten streams\n\nhad successfully reached User Acceptance Testing [UAT] on\n\nschedule, and it took another six months for that stream to\n\ncomplete UAT, with the resulting capability falling well short of\n\nbusiness expectations. This under-performance was the main\n\ncatalyst for the department’s Agile transformation.”\n\nHowever, after using Agile for nearly a year, they experienced only\n\nsmall improvements, still falling short of their needed business outcomes. Campbell-Pretty held a program-wide retrospective and asked, “After reflecting on all our experiences over the last release,\n\nwhat are things we could do that would double our productivity?”\n\nThroughout the project, there was grumbling about the “lack of business engagement.” However, during the retrospective, “improve availability of environments” was at the top of the list. In hindsight, it was obvious—Development teams needed provisioned environments in order to begin work, and were often\n\nwaiting up to eight weeks.\n\nThey created a new integration and build team that was\n\nresponsible for “building quality into our processes, instead of trying to inspect quality after the fact.” It was initially comprised of database administrators (DBAs) and automation specialists tasked with automating their environment creation process. The team quickly made a surprising discovery: only 50% of the source code in their development and test environments matched what\n\nwas running in production.\n\nCampbell-Pretty observed, “Suddenly, we understood why we\n\nencountered so many defects each time we deployed our code into\n\nnew environments. In each environment, we kept fixing forward,\n\nbut the changes we made were not being put back into version\n\ncontrol.”\n\nThe team carefully reverse-engineered all the changes that had\n\nbeen made to the different environments and put them all into\n\nversion control. They also automated their environment creation\n\nprocess so they could repeatedly and correctly spin up\n\nenvironments.\n\nCampbell-Pretty described the results, noting that “the time it took to get a correct environment went from eight weeks to one day. This was one of the key adjustments that allowed us to hit our objectives concerning our lead time, the cost to deliver, and the number of escaped defects that made it into production.”\n\nCampbell-Pretty’s story shows the variety of problems that can be\n\ntraced back to inconsistently constructed environments and changes not being systematically put back into version control.\n\nThroughout the remainder of this chapter, we will discuss how to build the mechanisms that will enable us to create environments on demand, expand the use of version control to everyone in the value stream, make infrastructure easier to rebuild than to repair, and ensure that developers run their code in production-like environments along every stage of the software development life\n\ncycle.\n\nENABLE ON DEMAND CREATION OF DEV, TEST, AND PRODUCTION ENVIRONMENTS\n\nAs seen in the enterprise data warehouse example above, one of the major contributing causes of chaotic, disruptive, and\n\nsometimes even catastrophic software releases, is the first time we ever get to see how our application behaves in a production-like\n\nenvironment with realistic load and production data sets is during the release.† In many cases, development teams may have requested test environments in the early stages of the project.\n\nHowever, when there are long lead times required for Operations\n\nto deliver test environments, teams may not receive them soon enough to perform adequate testing. Worse, test environments are\n\noften mis-configured or are so different from our production environments that we still end up with large production problems\n\ndespite having performed pre-deployment testing.\n\nIn this step, we want developers to run production-like environments on their own workstations, created on demand and\n\nself-serviced. By doing this, developers can run and test their code\n\nin production-like environments as part of their daily work, providing early and constant feedback on the quality their work.\n\nInstead of merely documenting the specifications of the\n\nproduction environment in a document or on a wiki page, we create a common build mechanism that creates all of our\n\nenvironments, such as for development, test, and production. By doing this, anyone can get production-like environments in\n\nminutes, without opening up a ticket, let alone having to wait weeks.‡",
      "page_number": 174
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 183-190)",
      "start_page": 183,
      "end_page": 190,
      "detection_method": "topic_boundary",
      "content": "To do this requires defining and automating the creation of our known, good environments, which are stable, secure, and in a risk-\n\nreduced state, embodying the collective knowledge of the organization. All our requirements are embedded, not in\n\ndocuments or as knowledge in someone’s head, but codified in our automated environment build process.\n\nInstead of Operations manually building and configuring the\n\nenvironment, we can use automation for any or all of the\n\nfollowing:\n\nCopying a virtualized environment (e.g., a VMware image, running a Vagrant script, booting an Amazon Machine Image\n\nfile in EC2)\n\nBuilding an automated environment creation process that starts from “bare metal” (e.g., PXE install from a baseline\n\nimage)\n\nUsing “infrastructure as code” configuration management tools (e.g., Puppet, Chef, Ansible, Salt, CFEngine, etc.)\n\nUsing automated operating system configuration tools (e.g.,\n\nSolaris Jumpstart, Red Hat Kickstart, Debian preseed)\n\nAssembling an environment from a set of virtual images or containers (e.g., Vagrant, Docker)\n\nSpinning up a new environment in a public cloud (e.g., Amazon\n\nWeb Services, Google App Engine, Microsoft Azure), private\n\ncloud, or other PaaS (platform as a service, such as OpenStack or Cloud Foundry, etc.).\n\nBecause we’ve carefully defined all aspects of the environment\n\nahead of time, we are not only able to create new environments\n\nquickly, but also ensure that these environments will be stable, reliable, consistent, and secure. This benefits everyone.\n\nOperations benefits from this capability, to create new\n\nenvironments quickly, because automation of the environment creation process enforces consistency and reduces tedious, error-\n\nprone manual work. Furthermore, Development benefits by being able to reproduce all the necessary parts of the production\n\nenvironment to build, run, and test their code on their\n\nworkstations. By doing this, we enable developers to find and fix many problems, even at the earliest stages of the project, as\n\nopposed to during integration testing or worse, in production.\n\nBy providing developers an environment they fully control, we enable them to quickly reproduce, diagnose, and fix defects, safely\n\nisolated from production services and other shared resources. They can also experiment with changes to the environments, as\n\nwell as to the infrastructure code that creates it (e.g., configuration\n\nmanagement scripts), further creating shared knowledge between Development and Operations.§\n\nCREATE OUR SINGLE REPOSITORY OF TRUTH FOR THE ENTIRE SYSTEM\n\nIn the previous step, we enabled the on demand creation of the\n\ndevelopment, test, and production environments. Now we must ensure that all parts of our software system.\n\nFor decades, comprehensive use of version control has increasingly become a mandatory practice of individual developers and development teams.¶ A version control system records changes to files or sets of files stored within the system. This can be source code, assets, or other documents that may be\n\npart of a software development project. We make changes in groups called commits or revisions. Each revision, along with metadata such as who made the change and when, is stored within the system in one way or another, allowing us to commit,\n\ncompare, merge, and restore past revisions to objects to the repository. It also minimizes risks by establishing a way to revert objects in production to previous versions. (In this book, the\n\nfollowing terms will be used interchangeably: checked in to version control, committed into version control, code commit, change commit, commit.)\n\nWhen developers put all their application source files and configurations in version control, it becomes the single repository of truth that contains the precise intended state of the system.\n\nHowever, because delivering value to the customer requires both our code and the environments they run in, we need our environments in version control as well. In other words, version control is for everyone in our value stream, including QA,\n\nOperations, Infosec, as well as developers. By putting all production artifacts into version control, our version control repository enables us to repeatedly and reliably reproduce all\n\ncomponents of our working software system—this includes our applications and production environment, as well as all of our pre- production environments.\n\nTo ensure that we can restore production service repeatedly and predictably (and, ideally, quickly) even when catastrophic events\n\noccur, we must check in the following assets to our shared version control repository:\n\nAll application code and dependencies (e.g., libraries, static\n\ncontent, etc.)\n\nAny script used to create database schemas, application reference data, etc.\n\nAll the environment creation tools and artifacts described in the previous step (e.g., VMware or AMI images, Puppet or Chef\n\nrecipes, etc.)\n\nAny file used to create containers (e.g., Docker or Rocket definition or composition files)\n\nAll supporting automated tests and any manual test scripts\n\nAny script that supports code packaging, deployment, database\n\nmigration, and environment provisioning\n\nAll project artifacts (e.g., requirements documentation, deployment procedures, release notes, etc.)\n\nAll cloud configuration files (e.g., AWS Cloudformation templates, Microsoft Azure Stack DSC files, OpenStack HEAT)\n\nAny other script or configuration information required to create infrastructure that supports multiple services (e.g., enterprise service buses, database management systems, DNS zone files, configuration rules for firewalls, and other networking devices).**\n\nWe may have multiple repositories for different types of objects and services, where they are labelled and tagged alongside our\n\nsource code. For instance, we may store large virtual machine images, ISO files, compiled binaries, and so forth in artifact repositories (e.g., Nexus, Artifactory). Alternatively, we may put them in blob stores (e.g., Amazon S3 buckets) or put Docker\n\nimages into Docker registries, and so forth.\n\nIt is not sufficient to merely be able to re-create any previous state of the production environment; we must also be able to re-create\n\nthe entire pre-production and build processes as well. Consequently, we need to put into version control everything relied upon by our build processes, including our tools (e.g.,\n\ncompilers, testing tools) and the environments they depend upon.††\n\nIn Puppet Labs’ 2014 State of DevOps Report, the use of version\n\ncontrol by Ops was the highest predictor of both IT performance and organizational performance. In fact, whether Ops used version control was a higher predictor for both IT performance\n\nand organizational performance than whether Dev used version control.\n\nThe findings from Puppet Labs' 2014 State of DevOps Report\n\nunderscores the critical role version control plays in the software development process. We now know when all application and environment changes are recorded in version control, it enables us to not only quickly see all changes that might have contributed to\n\na problem, but also provides the means to roll back to a previous known, running state, allowing us to more quickly recover from failures.\n\nBut why does using version control for our environments predict IT and organizational performance better than using version control for our code?\n\nBecause in almost all cases, there are orders of magnitude more configurable settings in our environment than in our code.\n\nConsequently, it is the environment that needs to be in version control the most.‡‡\n\nVersion control also provides a means of communication for\n\neveryone working in the value stream—having Development, QA, Infosec, and Operations able to see each other’s changes helps reduce surprises, creates visibility into each other’s work, and helps build and reinforce trust. See Appendix 7.\n\nMAKE INFRASTRUCTURE EASIER TO REBUILD THAN TO REPAIR\n\nWhen we can quickly rebuild and re-create our applications and environments on demand, we can also quickly rebuild them\n\ninstead of repairing them when things go wrong. Although this is something that almost all large-scale web operations do (i.e., more than one thousand servers), we should also adopt this practice even if we have only one server in production.\n\nBill Baker, a distinguished engineer at Microsoft, quipped that we used to treat servers like pets: “You name them and when they get sick, you nurse them back to health. [Now] servers are [treated]\n\nlike cattle. You number them and when they get sick, you shoot them.”\n\nBy having repeatable environment creation systems, we are able to easily increase capacity by adding more servers into rotation (i.e., horizontal scaling). We also avoid the disaster that inevitably\n\nresults when we must restore service after a catastrophic failure of irreproducible infrastructure, created through years of undocumented and manual production changes.\n\nTo ensure consistency of our environments, whenever we make production changes (configuration changes, patching, upgrading, etc.), those changes need to be replicated everywhere in our\n\nproduction and pre-production environments, as well as in any newly created environments.\n\nInstead of manually logging into servers and making changes, we\n\nmust make changes in a way that ensures all changes are replicated everywhere automatically and that all our changes are put into version control.\n\nWe can rely on our automated configuration systems to ensure consistency (e.g., Puppet, Chef, Ansible, Salt, Bosh, etc.), or we can create new virtual machines or containers from our automated\n\nbuild mechanism and deploy them into production, destroying the old ones or taking them out of rotation.§§\n\nThe latter pattern is what has become known as immutable\n\ninfrastructure, where manual changes to the production environment are no longer allowed—the only way production changes can be made is to put the changes into version control\n\nand re-create the code and environments from scratch. By doing this, no variance is able to creep into production.\n\nTo prevent uncontrolled configuration variances, we may disable remote logins to production servers¶¶ or routinely kill and replace production instances, ensuring that manually-applied production\n\nchanges are removed. This action motivates everyone to put their changes in the correct way through version control. By applying such measures, we are systematically reducing the ways our infrastructure can drift from our known, good states (e.g.,\n\nconfiguration drift, fragile artifacts, works of art, snowflakes, and so forth).\n\nAlso, we must keep our pre-production environments up to date— specifically, we need developers to stay running on our most current environment. Developers will often want to keep running on older environments because they fear environment updates\n\nmay break existing functionality. However, we want to update them frequently so we can find problems at the earliest part of the life cycle.***\n\nMODIFY OUR DEFINITION OF DEVELOPMENT “DONE” TO INCLUDE RUNNING IN PRODUCTION-LIKE ENVIRONMENTS\n\nNow that our environments can be created on demand and\n\neverything is checked in to version control, our goal is to ensure\n\nthat these environments are being used in the daily work of Development. We need to verify that our application runs as\n\nexpected in a production-like environment long before the end of\n\nthe project or before our first production deployment.",
      "page_number": 183
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 191-198)",
      "start_page": 191,
      "end_page": 198,
      "detection_method": "topic_boundary",
      "content": "Most modern software development methodologies prescribe\n\nshort and iterative development intervals, as opposed to the big\n\nbang approach (e.g., the waterfall `model). In general, the longer the interval between deployment, the worse the outcomes. For\n\nexample, in the Scrum methodology a sprint is a time-boxed\n\ndevelopment interval (typically one month or less) within which we are required to be done, widely defined as when we have\n\n“working and potentially shippable code.”\n\nOur goal is to ensure that Development and QA are routinely integrating the code with production-like environments at increasingly frequent intervals throughout the project.††† We do this by expanding the definition of “done” beyond just correct code functionality (addition in bold text): at the end of each\n\ndevelopment interval, we have integrated, tested, working and potentially shippable code, demonstrated in a production-\n\nlike environment.\n\nIn other words, we will only accept development work as done when it can be successfully built, deployed, and confirmed that it\n\nruns as expected in a production-like environment, instead of\n\nmerely when a developer believes it to be done—ideally, it runs under a production-like load with a production-like dataset, long\n\nbefore the end of a sprint. This prevents situations where a feature\n\nis called done merely because a developer can run it successfully on their laptop but nowhere else.\n\nBy having developers write, test, and run their own code in a\n\nproduction-like environment, the majority of the work to successfully integrate our code and environments happens during\n\nour daily work, instead of at the end of the release. By the end of\n\nour first interval, our application can be demonstrated to run\n\ncorrectly in a production-like environment, with the code and\n\nenvironment having been integrated together many times over,\n\nideally with all the steps automated (no manual tinkering required).\n\nBetter yet, by the end of the project, we will have successfully\n\ndeployed and run our code in production-like environments hundreds or even thousands of times, giving us confidence that\n\nmost of our production deployment problems have been found\n\nand fixed.\n\nIdeally, we use the same tools, such as monitoring, logging, and\n\ndeployment, in our pre-production environments as we do in\n\nproduction. By doing this, we have familiarity and experience that will help us smoothly deploy and run, as well as diagnose and fix,\n\nour service when it is in production.\n\nBy enabling Development and Operations to gain a shared mastery of how the code and environment interact, and practicing\n\ndeployments early and often, we significantly reduce the\n\ndeployment risks that are associated with production code releases. This also allows us to eliminate an entire class of\n\noperational and security defects and architectural problems that are usually caught too late in the project to fix.\n\nCONCLUSION\n\nThe fast flow of work from Development to Operations requires\n\nthat anyone can get production-like environments on demand. By allowing developers to use production-like environments even at\n\nthe earliest stages of a software project, we significantly reduce the\n\nrisk of production problems later. This is one of many practices\n\nthat demonstrate how Operations can make developers far more\n\nproductive. We enforce the practice of developers running their code in production-like environments by incorporating it into the\n\ndefinition of “done.”\n\nFurthermore, by putting all production artifacts into version control, we have a “single source of truth” that allows us to re-\n\ncreate the entire production environment in a quick, repeatable,\n\nand documented way, using the same development practices for Operations work as we do for Development work. And by making\n\nproduction infrastructure easier to rebuild than to repair, we make resolving problems easier and faster, as well as making it easier to\n\nexpand capacity.\n\nHaving these practices in place sets the stage for enabling comprehensive test automation, which is explored in the next\n\nchapter.\n\n† In this context, environment is defined as everything in the application stack except for the application,\n\nincluding the databases, operating systems, networking, virtualization, and all associated configurations.\n\n‡ Most developers want to test their code, and they have often gone to extreme lengths to obtain test environments to do so. Developers have been known to reuse old test environments from previous projects (often years old) or ask someone who has a reputation of being able to find one—they just won’t ask where it came from, because, invariably, someone somewhere is now missing a server.\n\n§ Ideally, we should be finding errors before integration testing when is too late in the testing cycle to create fast\n\nfeedback for developers. If we are unable to do so, we likely have an architectural issue that needs to be addressed. Designing our systems for testability, to include the ability to discover most defects using a non- integrated virtual environment on a development workstation, is a key part of creating an architecture that supports fast flow and feedback.\n\n¶ The first version control system was likely UPDATE on the CDC6600 (1969). Later came SCCS (1972), CMS on\n\nVMS (1978), RCS (1982), and so forth.\n\n** One may observe that version control fulfills some of the ITIL constructs of the Definitive Media Library (DML)\n\nand Configuration Management Database (CMDB), inventorying everything required to re-create the production environment.\n\n†† In future steps, we will also check in to version control all the supporting infrastructure we build, such as the\n\nautomated test suites and our continuous integration and deployment pipeline infrastructure.\n\n‡‡ Anyone who has done a code migration for an ERP system (e.g., SAP, Oracle Financials, etc.) may recognize the following situation: When a code migration fails, it is rarely due to a coding error. Instead, it’s far more likely\n\nthat the migration failed due to some difference in the environments, such as between Development and QA or QA and Production.\n\n§§ At Netflix, the average age of Netflix AWS instance is twenty-four days, with 60% being less than one week old.\n\n¶¶ Or allow it only in emergencies, ensuring that a copy of the console log is automatically emailed to the\n\noperations team.\n\n*** The entire application stack and environment can be bundled into containers, which can enable\n\nunprecedented simplicity and speed across the entire deployment pipeline.\n\n††† The term integration has many slightly different usages in Development and Operations. In Development,\n\nintegration typically refers to code integration, which is the integration of multiple code branches into trunk in version control. In continuous delivery and DevOps, integration testing refers to the testing of the application in a production-like environment or integrated test environment.\n\n10Enable Fast and Reliable Automated Testing\n\nAt this point, Development and QA are using production-like\n\nenvironments in their daily work, and we are successfully\n\nintegrating and running our code into a production-like\n\nenvironment for every feature that is accepted, with all changes\n\nchecked in to version control. However, we are likely to get undesired outcomes if we find and fix errors in a separate test\n\nphase, executed by a separate QA department only after all\n\ndevelopment has been completed. And, if testing is only performed a few times a year, developers learn about their\n\nmistakes months after they introduced the change that caused the error. By then, the link between cause and effect has likely faded,\n\nsolving the problem requires firefighting and archaeology, and,\n\nworst of all, our ability to learn from the mistake and integrate it into our future work is significantly diminished.\n\nAutomated testing addresses another significant and unsettling\n\nproblem. Gary Gruver observes that “without automated testing, the more code we write, the more time and money is required to test our code—in most cases, this is a totally unscalable business\n\nmodel for any technology organization.”\n\nAlthough Google now undoubtedly exemplifies a culture that\n\nvalues automated testing at scale, this wasn’t always the case. In\n\n2005, when Mike Bland joined the organization, deploying to\n\nGoogle.com was often extremely problematic, especially for the\n\nGoogle Web Server (GWS) team.\n\nAs Bland explains, “The GWS team had gotten into a position in\n\nthe mid 2000s where it was extremely difficult to make changes to\n\nthe web server, a C++ application that handled all requests to\n\nGoogle’s home page and many other Google web pages. As\n\nimportant and prominent as Google.com was, being on the GWS\n\nteam was not a glamorous assignment—it was often the dumping\n\nground for all the different teams who were creating various\n\nsearch functionality, all of whom were developing code independently of each other. They had problems such as builds and tests taking too long, code being put into production without\n\nbeing tested, and teams checking in large, infrequent changes that conflicted with those from other teams.”\n\nThe consequences of this were large—search results could have errors or become unacceptably slow, affecting thousands of search queries on Google.com. The potential result was not only loss of revenue, but customer trust.\n\nBland describes how it affected developers deploying changes,\n\n“Fear became the mind-killer. Fear stopped new team members\n\nfrom changing things because they didn’t understand the system. But fear also stopped experienced people from changing things because they understood it all too well.”† Bland was part of the group that was determined to solve this problem.\n\nGWS team lead Bharat Mediratta believed automated testing would help. As Bland describes, “They created a hard line: no changes would be accepted into GWS without accompanying\n\nautomated tests. They set up a continuous build and religiously\n\nkept it passing. They set up test coverage monitoring and ensured\n\nthat their level of test coverage went up over time. They wrote up\n\npolicy and testing guides, and insisted that contributors both\n\ninside and outside the team follow them.”\n\nThe results were startling. As Bland notes, “GWS quickly became\n\none of the most productive teams in the company, integrating\n\nlarge numbers of changes from different teams every week while\n\nmaintaining a rapid release schedule. New team members were\n\nable to make productive contributions to this complex system\n\nquickly, thanks to good test coverage and code health. Ultimately, their radical policy enabled the Google.com home page to quickly expand its capabilities and thrive in an amazingly fast-moving and\n\ncompetitive technology landscape.”\n\nBut GWS was still a relatively small team in a large and growing\n\ncompany. The team wanted to expand these practices across the entire organization. Thus, the Testing Grouplet was born, an informal group of engineers who wanted to elevate automated testing practices across the entire organization. Over the next five years, they helped replicate this culture of automated testing across all of Google.‡\n\nNow when any Google developer commits code, it is automatically run against a suite of hundreds of thousands of automated tests. If\n\nthe code passes, it is automatically merged into trunk, ready to be deployed into production. Many Google properties build hourly or daily, then pick which builds to release; others adopt a continuous “Push on Green” delivery philosophy.\n\nThe stakes are higher than ever—a single code deployment error at Google can take down every property, all at the same time (such as\n\na global infrastructure change or when a defect is introduced into a core library that every property depends upon).\n\nEran Messeri, an engineer in the Google Developer Infrastructure\n\ngroup, notes, “Large failures happen occasionally. You’ll get a ton\n\nof instant messages and engineers knocking on your door. [When the deployment pipeline is broken,] we need to fix it right away,\n\nbecause developers can no longer commit code. Consequently, we want to make it very easy to roll back.”\n\nWhat enables this system to work at Google is engineering\n\nprofessionalism and a high-trust culture that assumes everyone wants to do a good job, as well as the ability to detect and correct\n\nissues quickly. Messeri explains, “There are no hard policies at\n\nGoogle, such as, ‘If you break production for more than ten projects, you have an SLA to fix the issue within ten minutes.’\n\nInstead, there is mutual respect between teams and an implicit agreement that everyone does whatever it takes to keep the\n\ndeployment pipeline running. We all know that one day, I’ll break your project by accident; the next day, you may break mine.”\n\nWhat Mike Bland and the Testing Grouplet team achieved has\n\nmade Google one of the most productive technology organizations\n\nin the world. By 2013, automated testing and continuous integration at Google enabled over four thousand small teams to\n\nwork together and stay productive, all simultaneously developing, integrating, testing, and deploying their code into production. All\n\ntheir code is in a single, shared repository, made up of billions of files, all being continuously built and integrated, with 50% of their",
      "page_number": 191
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 199-209)",
      "start_page": 199,
      "end_page": 209,
      "detection_method": "topic_boundary",
      "content": "code being changed each month. Some other impressive statistics on their performance include:\n\n40,000 code commits/day\n\n50,000 builds/day (on weekdays, this may exceed 90,000)\n\n120,000 automated test suites\n\n75 million test cases run daily\n\n100+ engineers working on the test engineering, continuous\n\nintegration, and release engineering tooling to increase developer productivity (making up 0.5% of the R&D workforce)\n\nIn the remainder of this chapter, we will go through the\n\ncontinuous integration practices required to replicate these outcomes.\n\nCONTINUOUSLY BUILD, TEST, AND INTEGRATE OUR CODE AND ENVIRONMENTS\n\nOur goal is to build quality into our product, even at the earliest\n\nstages, by having developers build automated tests as part of their daily work. This creates a fast feedback loop that helps developers\n\nfind problems early and fix them quickly, when there are the fewest constraints (e.g., time, resources).\n\nIn this step, we create automated test suites that increase the\n\nfrequency of integration and testing of our code and our environments from periodic to continuous. We do this by building\n\nour deployment pipeline, which will perform integration of our\n\ncode and environments and trigger a series of tests every time a new change is put into version control.§ (See figure 13.)\n\nThe deployment pipeline, first defined by Jez Humble and David Farley in their book Continuous Delivery: Reliable Software\n\nReleases Through Build, Test, and Deployment Automation, ensures that all code checked in to version control is automatically\n\nbuilt and tested in a production-like environment. By doing this, we find any build, test, or integration errors as soon as a change is\n\nintroduced, enabling us to fix them immediately. Done correctly,\n\nthis allows us to always be assured that we are in a deployable and shippable state.\n\nTo achieve this, we must create automated build and test\n\nprocesses that run in dedicated environments. This is critical for the following reasons:\n\nOur build and test process can run all the time, independent of\n\nthe work habits of individual engineers.\n\nA segregated build and test process ensures that we understand all the dependencies required to build, package, run, and test\n\nour code (i.e., removing the “it worked on the developer’s\n\nlaptop, but it broke in production” problem).\n\nWe can package our application to enable the repeatable\n\ninstallation of code and configurations into an environment\n\n(e.g., on Linux RPM, yum, npm; on Windows, OneGet; alternatively framework-specific packaging systems can be used, such as EAR and WAR files for Java, gems for Ruby,\n\netc.).\n\nInstead of putting our code in packages, we may choose to package our applications into deployable containers (e.g.,\n\nDocker, Rkt, LXD, AMIs).\n\nEnvironments can be made more production-like in a way that is consistent and repeatable (e.g., compilers are removed from\n\nthe environment, debugging flags are turned off, etc.).\n\nOur deployment pipeline validates after every change that our\n\ncode successfully integrates into a production-like environment. It becomes the platform through which testers request and certify builds during acceptance testing and usability testing, and it will run automated performance and security validations.\n\n>\n\nFigure 13: The deployment pipeline (Source: Humble and Farley, Continuous Delivery, 3.)\n\nFurthermore, it will be used to self-service builds to UAT (user acceptance testing), integration testing, and security testing environments. In future steps, as we evolve the deployment\n\npipeline, it will also be used to manage all activities required to take our changes from version control to deployment.\n\nA variety of tools have been designed to provide deployment\n\npipeline functionality, many of them open source (e.g., Jenkins, ThoughtWorks Go, Concourse, Bamboo, Microsoft Team Foundation Server, TeamCity, Gitlab CI, as well as cloud-based solutions such as Travis CI and Snap).¶\n\nWe begin the deployment pipeline by running the commit stage, which builds and packages the software, runs automated unity\n\ntests, and performs additional validation such as static code analysis, duplication and test coverage analysis, and checking style.** If successful, this triggers the acceptance stage, which automatically deploys the packages created in the commit stage into a production-like environment and runs the automated acceptance tests.\n\nOnce changes are accepted into version control, we want to package our code only once, so that the same packages are used to deploy code throughout our entire deployment pipeline. By doing this, code will be deployed into our integrated test and staging\n\nenvironments in the same way that it is deployed into production. This reduces variances that can avoid downstream errors that are difficult to diagnose (e.g., using different compilers, compiler flags, library versions, or configurations).††\n\nThe goal of the deployment pipeline is to provide everyone in the value stream, especially developers, the fastest possible feedback\n\nthat a change has taken us out of a deployable state. This could be a change to our code, to any of our environments, to our automated tests, or even to the deployment pipeline infrastructure\n\n(e.g., a Jenkins configuration setting).\n\nAs a result, our deployment pipeline infrastructure becomes as foundational for our development processes as our version control\n\ninfrastructure. Our deployment pipeline also stores the history of each code build, including information about which tests were performed on which build, which builds have been deployed to which environment, and what the test results were. In\n\ncombination with the information in our version control history,\n\nwe can quickly determine what caused our deployment pipeline to break and, likely, how to fix the error.\n\nThis information also helps us fulfill evidence requirements for audit and compliance purposes, with evidence being automatically generated as part of daily work.\n\nNow that we have a working deployment pipeline infrastructure, we must create our continuous integration practices, which require three capabilities:\n\nA comprehensive and reliable set of automated tests that validate we are in a deployable state.\n\nA culture that “stops the entire production line” when our validation tests fail.\n\nDevelopers working in small batches on trunk rather than\n\nlong-lived feature branches.\n\nIn the next section, we describe why fast and reliable automated testing is needed and how to build it.\n\nBUILD A FAST AND RELIABLE AUTOMATED VALIDATION TEST SUITE\n\nIn the previous step, we started to create the automated testing infrastructure that validates that we have a green build (i.e.,\n\nwhatever is in version control is in a buildable and deployable state). To underscore why we need to perform this integration and testing step continuously, consider what happens when we only\n\nperform this operation periodically, such as during a nightly build process.\n\nSuppose we have a team of ten developers, with everyone checking their code into version control daily, and a developer introduces a change that breaks our nightly build and test job. In this scenario,\n\nwhen we discover the next day that we no longer have a green build, it will take minutes, or more likely hours, for our development team to figure out which change caused the problem, who introduced it, and how to fix it.\n\nWorse, suppose the problem wasn’t caused by a code change, but was due to a test environment issue (e.g., an incorrect configuration setting somewhere). The development team may\n\nbelieve that they fixed the problem because all the unit tests pass, only to discover that the tests will still fail later that night.\n\nFurther complicating the issue, ten more changes will have been checked in to version control by the team that day. Each of these changes has the potential to introduce more errors that could break our automated tests, further increasing the difficulty of\n\nsuccessfully diagnosing and fixing the problem.\n\nIn short, slow and periodic feedback kills. Especially for larger development teams. The problem becomes even more daunting\n\nwhen we have tens, hundreds, or even thousands of other developers checking their changes into version control each day. The result is that our builds and automated tests are frequently\n\nbroken, and developers even stop checking their changes into version control (“Why bother, since the builds and tests are always broken?”). Instead they wait to integrate their code at the end of\n\nthe project, resulting in all the undesired outcomes of large batch size, big bang integrations, and production deployments.‡‡\n\nTo prevent this scenario, we need fast automated tests that run within our build and test environments whenever a new change is introduced into version control. In this way we can find and fix any problems immediately, as the Google Web Server example\n\ndemonstrated. By doing this, we ensure our batches remains small, and, at any given point in time, we remain in a deployable state.\n\nIn general, automated tests fall into one of the following categories, from fastest to slowest:\n\nUnit tests: These typically test a single method, class, or function in isolation, providing assurance to the developer that their code operates as designed. For many reasons, including the need to keep our tests fast and stateless, unit tests often\n\n“stub out” databases and other external dependencies (e.g., functions are modified to return static, predefined values, instead of calling the real database).§§\n\nAcceptance tests: These typically test the application as a whole to provide assurance that a higher level of functionality operates as designed (e.g., the business acceptance criteria for\n\na user story, the correctness of an API), and that regression errors have not been introduced (i.e., we broke functionality that was previously operating correctly). Humble and Farley\n\ndefine the difference between unit and acceptance testing as, “The aim of a unit test is to show that a single part of the application does what the programmer intends it to....The objective of acceptance tests is to prove that our application\n\ndoes what the customer meant it to, not that it works the way its programmers think it should.” After a build passes our unit tests, our deployment pipeline runs it against our acceptance\n\ntests. Any build that passes our acceptance tests is then typically made available for manual testing (e.g., exploratory testing, UI testing, etc.), as well as for integration testing.\n\nIntegration tests: Integration tests are where we ensure that our application correctly interacts with other production applications and services, as opposed to calling stubbed out\n\ninterfaces. As Humble and Farley observe, “much of the work in the SIT environment involves deploying new versions of each of the applications until they all cooperate. In this situation the smoke test is usually a fully fledged set of\n\nacceptance tests that run against the whole application.” Integration tests are performed on builds that have passed our unit and acceptance tests. Because integration tests are often\n\nbrittle, we want to minimize the number of integration tests and find as many of our defects as possible during unit and acceptance testing. The ability to use virtual or simulated versions of remote services when running acceptance tests\n\nbecomes an essential architectural requirement.\n\nWhen facing deadline pressures, developers may stop creating\n\nunit tests as part of their daily work, regardless of how we’ve defined ‘done.’ To detect this, we may choose to measure and\n\nmake visible our test coverage (as a function of number of classes,\n\nlines of code, permutations, etc.), maybe even failing our validation test suite when it drops below a certain level (e.g., when less than 80% of our classes have unit tests).¶¶\n\nMartin Fowler observes that, in general, “a ten-minute build [and\n\ntest process] is perfectly within reason…[We first] do the\n\ncompilation and run tests that are more localized unit tests with the database completely stubbed out. Such tests can run very fast,\n\nkeeping within the ten minute guideline. However any bugs that\n\ninvolve larger scale interactions, particularly those involving the real database, won’t be found. The second stage build runs a\n\ndifferent suite of tests [acceptance tests] that do hit the real database and involve more end-to-end behavior. This suite may\n\ntake a couple of hours to run.”\n\nCATCH ERRORS AS EARLY IN OUR AUTOMATED TESTING AS POSSIBLE\n\nA specific design goal of our automated test suite is to find errors as early in the testing as possible. This is why we run faster-\n\nrunning automated tests (e.g., unit tests) before slower-running\n\nautomated tests (e.g., acceptance and integration tests), which are both run before any manual testing.\n\nAnother corollary of this principle is that any errors should be\n\nfound with the fastest category of testing possible. If most of our errors are found in our acceptance and integration tests, the\n\nfeedback we provide to developers is orders of magnitude slower than with unit tests—and integration testing requires using scarce\n\nand complex integration test environments, which can only be\n\nused by one team at a time, further delaying feedback.\n\nFurthermore, not only are errors detected during integration\n\ntesting difficult and time-consuming for developers to reproduce,\n\neven validating that it has been fixed is difficult (i.e., a developer\n\ncreates a fix but then needs to wait four hours to learn whether the\n\nintegration tests now pass).\n\nTherefore, whenever we find an error with an acceptance or integration test, we should create a unit test that could find the\n\nerror faster, earlier, and cheaper. Martin Fowler described the\n\nnotion of the “ideal testing pyramid,” where we are able to catch most of our errors using our unit tests. (See figure 14.) In contrast,\n\nin many testing programs the inverse is true, where most of the\n\ninvestment is in manual and integration testing.\n\nFigure 14: The ideal and non-ideal automated testing pyramids (Source: Martin Fowler, “TestPyramid.”)\n\nIf we find that unit or acceptance tests are too difficult and\n\nexpensive to write and maintain, it’s likely that we have an architecture that is too tightly-coupled, where strong separation\n\nbetween our module boundaries no longer exist (or maybe never existed). In this case, we will need to create a more loosely-\n\ncoupled system so modules can be independently tested without\n\nintegration environments. Acceptance test suites for even the most\n\ncomplex applications that run in minutes are possible.\n\nENSURE TESTS RUN QUICKLY (IN PARALLEL, IF NECESSARY)\n\nBecause we want our tests to run quickly, we need to design our\n\ntests to run in parallel, potentially across many different servers.\n\nWe may also want to run different categories of tests in parallel. For example, when a build passes our acceptance tests, we may\n\nrun our performance testing in parallel with our security testing,\n\nas shown in figure 15. We may or may not allow manual exploratory testing until the build has passed all our automated\n\ntests—which enables faster feedback, but may also allow manual testing on builds that will eventually fail.\n\nWe make any build that passes all our automated tests available to\n\nuse for exploratory testing, as well as for other forms of manual or\n\nresource-intensive testing (such as performance testing). We want to do all such testing as frequently as possible and practical, either\n\ncontinually or on a schedule.",
      "page_number": 199
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 210-221)",
      "start_page": 210,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": "Figure 15: Running automated and manual tests in parallel (Source: Humble and Farley, Continuous Delivery, Kindle edition, location 3868.)\n\nAny tester (which includes all our developers) should use the\n\nlatest build that has passed all the automated tests, as opposed to\n\nwaiting for developers to flag a specific build as ready to test. By doing this, we ensure that testing happens as early in the process\n\nas possible.\n\nWRITE OUR AUTOMATED TESTS BEFORE WE WRITE THE CODE (“TEST-DRIVEN DEVELOPMENT”)\n\nOne of the most effective ways to ensure we have reliable\n\nautomated testing, is to write those tests as part of our daily work,\n\nusing techniques such as test-driven development (TDD) and acceptance test-driven development (ATDD). This is when we\n\nbegin every change to the system by first writing an automated test that validates the expected behavior fails, and then we write\n\nthe code to make the tests pass.\n\nThis technique was developed by Kent Beck in the late 1990s as part of Extreme Programming, and has the following three steps:\n\n1. Ensure the tests fail. “Write a test for the next bit of\n\nfunctionality you want to add.” Check in.\n\n2. Ensure the tests pass. “Write the functional code until the test\n\npasses.”Check in.\n\n3. “Refactor both new and old code to make it well\n\nstructured.”Ensure the tests pass. Check in again.\n\nThese automated test suites are checked in to version control\n\nalongside our code, which provides a living, up-to-date\n\nspecification of the system. Developers wishing to understand how to use the system can look at this test suite to find working examples of how to use the system’s API.***\n\nAUTOMATE AS MANY OF OUR MANUAL TESTS AS POSSIBLE\n\nOur goal is to find as many code errors through our automated\n\ntest suites, reducing our reliance on manual testing. In her 2013\n\npresentation at Flowcon titled “On the Care and Feeding of Feedback Cycles,” Elisabeth Hendrickson observed, “Although\n\ntesting can be automated, creating quality cannot. To have\n\nhumans executing tests that should be automated is a waste of human potential.”\n\nBy doing this, we enable all our testers (which, of course, includes\n\ndevelopers) work on high-value activities that cannot be automated, such as exploratory testing or improving the test\n\nprocess itself.\n\nHowever, merely automating all our manual tests may create undesired outcomes—we do not want automated tests that are\n\nunreliable or generate false positives (i.e., tests that should have\n\npassed because the code is functionally correct but failed due to problems such as slow performance, causing timeouts,\n\nuncontrolled starting state, or unintended state due to using\n\ndatabase stubs or shared test environments).\n\nUnreliable tests that generate false positives create significant\n\nproblems—they waste valuable time (e.g., forcing developers to re-\n\nrun the test to determine whether there is actually a problem), increase the overall effort of running and interpreting our test\n\nresults, and often result in stressed developers ignoring test\n\nresults entirely or turning off the automated tests in favor of focusing on creating code.\n\nThe result is always the same: we detect the problems later, the\n\nproblems are more difficult to fix, and our customers have worse outcomes, which in turn creates stress across the value stream.\n\nTo mitigate of this, a small number of reliable, automated tests are\n\nalmost always preferable over a large number of manual or unreliable automated tests. Therefore, we focus on automating\n\nonly the tests that genuinely validate the business goals we are\n\ntrying to achieve. If abandoning a test results in production defects, we should add it back to our manual test suite, with the\n\nideal of eventually automating it.\n\nAs Gary Gruver, formerly VP of Quality Engineering, Release Engineering, and Operations for Macys.com, described observes,\n\n“For a large retailer e-commerce site, we went from running 1,300\n\nmanual tests that we ran every ten days to running only ten automated tests upon every code commit—it’s far better to run a\n\nfew tests that we trust than to run tests that aren’t reliable. Over\n\ntime, we grew this test suite to having hundreds of thousands of\n\nautomated tests.”\n\nIn other words, we start with a small number of reliable\n\nautomated tests and add to them over time, creating an ever-\n\nincreasing level of assurance that we will quickly detect any changes to the system that take us out of a deployable state.\n\nINTEGRATE PERFORMANCE TESTING INTO OUR TEST SUITE\n\nAll too often, we discover that our application performs poorly\n\nduring integration testing or after it has been deployed to production. Performance problems are often difficult to detect,\n\nsuch as when things slow down over time, going unnoticed until it is too late (e.g., database queries without an index). And many\n\nproblems are difficult to solve, especially when they are caused by\n\narchitectural decisions we made or unforeseen limitations of our networking, database, storage, or other systems.\n\nOur goal is to write and run automated performance tests that\n\nvalidate our performance across the entire application stack (code, database, storage, network, virtualization, etc.) as part of the\n\ndeployment pipeline, so we detect problems early, when the fixes\n\nare cheapest and fastest.\n\nBy understanding how our application and environments behave\n\nunder a production-like load, we can do a far better job at capacity\n\nplanning, as well as detecting conditions such as:\n\nWhen our database query times grow non-linearly (e.g., we\n\nforget to turn on database indexing, and page load goes from\n\none hundred minutes to thirty seconds).\n\nWhen a code change causes the number of database calls,\n\nstorage use, or network traffic to increase ten-fold.\n\nWhen we have acceptance tests that are able to be run in parallel,\n\nwe can use them as the basis of our performance tests. For\n\ninstance, suppose we run an e-commerce site and have identified “search” and “checkout” as two high-value operations that must\n\nperform well under load. To test this, we may run thousands of\n\nparallel search acceptance tests simultaneously with thousands of parallel checkout tests.\n\nDue to the large amount of compute and I/O that is required to\n\nrun performance tests, creating a performance testing environment can easily be more complex than creating the\n\nproduction environment for the application itself. Because of this,\n\nwe may build our performance testing environment at the start of any project and ensure that we dedicate whatever resources are\n\nrequired to build it early and correctly.\n\nTo find performance problems early, we should log performance results and evaluate each performance run against previous\n\nresults. For instance, we might fail the performance tests if\n\nperformance deviates more than 2% from the previous run.\n\nINTEGRATE NON-FUNCTIONAL REQUIREMENTS TESTING INTO OUR TEST SUITE\n\nIn addition to testing that our code functions as designed and it\n\nperforms under production-like loads, we also want to validate every other attribute of the system we care about. These are often\n\ncalled non-functional requirements, which include availability,\n\nscalability, capacity, security, and so forth.\n\nMany of these requirements are fulfilled by the correct\n\nconfiguration of our environments, so we must also build automated tests to validate that our environments have been built\n\nand configured properly. For example, we want to enforce the\n\nconsistency and correctness of the following, which many non- functional requirements rely upon (e.g., security, performance,\n\navailability):\n\nSupporting applications, databases, libraries, etc.\n\nLanguage interpreters, compilers, etc.\n\nOperating systems (e.g., audit logging enabled, etc.)\n\nAll dependencies\n\nWhen we use infrastructure as code configuration management tools (e.g., Puppet, Chef, Ansible, Salt, Bosh), we can use the same\n\ntesting frameworks that we use to test our code to also test that\n\nour environments are configured and operating correctly (e.g., encoding environment tests into cucumber or gherkin tests).\n\nFurthermore, similar to how we run analysis tools on our\n\napplication in our deployment pipeline (e.g., static code analysis, test coverage analysis), we should run tools that analyze the code\n\nthat constructs our environments (e.g., Foodcritic for Chef, puppet-lint for Puppet). We should also run any security\n\nhardening checks as part of our automated tests to ensure that\n\neverything is configured securely and correctly (e.g., server-spec).\n\nAt any point in time, our automated tests can validate that we have a green build and that we are in a deployable state. Now, we\n\nmust create an Andon cord so that when someone breaks the\n\ndeployment pipeline, we take all necessary steps to get back into a\n\ngreen build state.\n\nPULL OUR ANDON CORD WHEN THE DEPLOYMENT PIPELINE BREAKS\n\nWhen we have a green build in our deployment pipeline, we have a\n\nhigh degree of confidence that our code and environment will\n\noperate as designed when we deploy our changes into production.\n\nIn order to keep our deployment pipeline in a green state, we will\n\ncreate a virtual Andon Cord, similar to the physical one in the\n\nToyota Production System. Whenever someone introduces a change that causes our build or automated tests to fail, no new\n\nwork is allowed to enter the system until the problem is fixed. And\n\nif someone needs help to resolve the problem, they can bring in whatever help they need, as in the Google example at the\n\nbeginning of this chapter.\n\nWhen our deployment pipeline is broken, at a minimum, we notify the entire team of the failure, so anyone can either fix the problem\n\nor roll-back the commit. We may even configure the version\n\ncontrol system to prevent further code commits until the first stage (i.e., builds and unit tests) of the deployment pipeline is back\n\nin a green state. If the problem was due to an automated test\n\ngenerating a false positive error, the offending test should either be rewritten or removed.††† Every member of the team should be empowered to roll back the commit to get back into a green state.\n\nRandy Shoup, former engineering director for Google App Engine, wrote about the importance of bringing the deployment back into\n\na green state. “We prioritize the team goals over individual goals—\n\nwhenever we help someone move their work forward, we help the entire team. This applies whether we’re helping someone fix the\n\nbuild or an automated test, or even performing a code review for\n\nthem. And of course, we know that they’ll do the same for us, when we need help. This system worked without a lot of formality\n\nor policy—everyone knew that our job was not just ‘write code,’ but it was to ‘run a service.’ This is why we prioritized all quality\n\nissues, especially those related to reliability and scaling, at the\n\nhighest level, treating them as a Priority 0 ‘show-stopper’ problems. From a systems perspective, these practices keep us\n\nfrom slipping backwards.”\n\nWhen later stages of the deployment pipeline fail, such as acceptance tests or performance tests, instead of stopping all new\n\nwork, we will have developers and testers on-call who are\n\nresponsible for fixing these problems immediately. They should also create new tests that run at an earlier stage in the deployment\n\npipeline to catch any future regressions. For example, if we\n\ndiscover a defect in our acceptance tests, we should write a unit test to catch the problem. Similarly, if we discover a defect in\n\nexploratory testing, we should write a unit or acceptance test.\n\nTo increase the visibility of automated test failures, we should create highly visible indicators so that the entire team can see\n\nwhen our build or automated tests are failing. Many teams have\n\ncreated highly visible build lights that get mounted on a wall, indicating the current build status, or other fun ways of telling the\n\nteam the build is broken, including lava lamps, playing a voice\n\nsample or song, klaxons, traffic lights, and so forth.\n\nIn many ways, this step is more challenging than creating our\n\nbuilds and test servers—those were purely technical activities, whereas this step requires changing human behavior and\n\nincentives. However, continuous integration and continuous delivery require these changes, as we explore in the next section.\n\nWHY WE NEED TO PULL THE ANDON CORD\n\nThe consequence of not pulling the Andon cord and immediately fixing any deployment pipeline issues results in the all too familiar\n\nproblem where it becomes ever more difficult to bring our applications and environment back into a deployable state.\n\nConsider the following situation:\n\nSomeone checks in code that breaks the build or our\n\nautomated tests, but no one fixes it.\n\nSomeone else checks in another change onto the broken build, which also doesn’t pass our automated tests—but no one sees\n\nthe failing test results which would have enabled us to see the\n\nnew defect, let alone fix it.\n\nOur existing tests don’t run reliably, so we are very unlikely to\n\nbuild new tests. (Why bother? We can’t even get the current\n\ntests to run.)\n\nWhen this happens, our deployments to any environment become\n\nas unreliable as when we had no automated tests or were using a\n\nwaterfall method, where the majority of our problems are being\n\ndiscovered in production. The inevitable outcome of this vicious\n\ncycle is that we end up where we started, with an unpredictable\n\n“stabilization phase” that takes weeks or months where our whole team is plunged into crisis, trying to get all our tests to pass,\n\ntaking shortcuts because of deadline pressures, and adding to our technical debt.‡‡‡\n\nCONCLUSION\n\nIn this chapter, we have created a comprehensive set of automated\n\ntests to confirm that we have a green build that is still in a passing\n\nand deployable state. We have organized our test suites and\n\ntesting activities into a deployment pipeline. We have also created the cultural norm of doing whatever it takes to get back into a\n\ngreen build state if someone introduces a change that breaks any\n\nof our automated tests.\n\nBy doing this, we set the stage for implementing continuous\n\nintegration, which allows many small teams to independently and\n\nsafely develop, test, and deploy code into production, delivering\n\nvalue to customers.\n\n† Bland described that at Google, one of the consequences of having so many talented developers was that it\n\ncreated “imposter syndrome,” a term coined by psychologists to informally describe people who are unable to internalize their accomplishments. Wikipedia states that “despite external evidence of their competence, those exhibiting the syndrome remain convinced that they are frauds and do not deserve the success they have achieved. Proof of success is dismissed as luck, timing, or as a result of deceiving others into thinking they are more intelligent and competent than they believe themselves to be.”\n\n‡ They created training programs, pushed the famous Testing on the Toilet newsletter (which they posted in the\n\nbathrooms), the Test Certified roadmap and certification program, and led multiple “fix-it” days (i.e., improvement blitzes), which helped teams improve their automated testing processes so they could replicate the amazing outcomes that the GWS team was able to achieve.\n\n§ In Development, continuous integration often refers to the continuous integration of multiple code branches into trunk and ensuring that it passes unit tests. However, in the context of continuous delivery and DevOps, continuous integration also mandates running on production-like environments and passing acceptance and integration tests. Jez Humble and David Farley disambiguate these by calling the latter CI+. In this book, continuous integration will always refer to CI+ practices.\n\n¶ If we create containers in our deployment pipeline and have an architecture such as microservices, we can enable each developer to build immutable artifacts where developers assemble and run all the service components in an environment identical to production on their workstation. This enables developers to build and run more tests on their workstation instead of on testing servers, giving us even faster feedback on their work.\n\n** We may even require that these tools are run before changes are accepted into version control (e.g., get pre-\n\ncommit hooks). We may also run these tools within the developer integrated development environment (IDE;\n\nwhere the developer edits, compiles, and runs code), which creates an even faster feedback loop.\n\n†† We can also use containers, such as Docker, as the packaging mechanism. Containers enable the capability to\n\nwrite once, run anywhere. These containers are created as part of our build process and can be quickly deployed and run in any environment. Because the same container is run in every environment, we help enforce the consistency of all our build artifacts.\n\n‡‡ It is exactly this problem that led to the development of continuous integration practices.\n\n§§ There is a broad category of architectural and testing techniques used to handle the problems of tests requiring input from external integration points, including “stubs,” “mocks,” “service virtualization,” and so forth. This becomes even more important for acceptance and integration testing, which place far more reliance on external states.\n\n¶¶ We should do this only when our teams already value automated testing—this type of metric is easily gamed by\n\ndevelopers and managers.\n\n*** Nachi Nagappan, E. Michael Maximilien, and Laurie Williams (from Microsoft Research, IBM Almaden Labs, and North Carolina State University, respectively) conducted a study that showed teams using TDD produced code 60%–90% better in terms of defect density than non-TDD teams, while taking only 15%–35% longer.\n\n††† If the process for rolling back the code is not well-known, a potential countermeasure is to schedule a pair\n\nprogrammed rollback, so that it can be better documented.\n\n‡‡‡ This is sometimes called the water-Scrum-fall anti-pattern, which refers to when an organization claims to be using Agile-like practices, but, in reality, all testing and defect fixing are performed at the end of the project.\n\n11Enable and Practice Continuous Integration\n\nIn the previous chapter, we created the automated testing\n\npractices to ensure that developers get fast feedback on the quality\n\nof their work. This becomes even more important as we increase\n\nthe number of developers and the number of branches they work\n\non in version control.\n\nThe ability to “branch” in version control systems was created\n\nprimarily to enable developers to work on different parts of the\n\nsoftware system in parallel, without the risk of individual developers checking in changes that could destabilize or introduce errors into trunk (sometimes also called master or mainline).†\n\nHowever, the longer developers are allowed to work in their branches in isolation, the more difficult it becomes to integrate\n\nand merge everyone’s changes back into trunk. In fact, integrating\n\nthose changes becomes exponentially more difficult as we increase the number of branches and the number of changes in each code branch.\n\nIntegration problems result in a significant amount of rework to get back into a deployable state, including conflicting changes that\n\nmust be manually merged or merges that break our automated or manual tests, usually requiring multiple developers to successfully",
      "page_number": 210
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 222-229)",
      "start_page": 222,
      "end_page": 229,
      "detection_method": "topic_boundary",
      "content": "resolve. And because integration has traditionally been done at the\n\nend of the project, when it takes far longer then planned, we are\n\noften forced to cut corners to make the release date.\n\nThis causes another downward spiral: when merging code is\n\npainful, we tend to do it less often, making future merges even\n\nworse. Continuous integration was designed to solve this problem\n\nby making merging into trunk a part of everyone’s daily work.\n\nThe surprising breadth of problems that continuous integration\n\nsolves, as well as the solutions themselves, are exemplified in Gary\n\nGruver’s experience as the director of engineering for HP’s\n\nLaserJet Firmware division, which builds the firmware that runs\n\nall their scanners, printers, and multifunction devices.\n\nThe team consisted of four hundred developers distributed across the US, Brazil, and India. Despite the size of their team, they were\n\nmoving far too slowly. For years, they were unable to deliver new features as quickly as the business needed.\n\nGruver described the problem thusly: “Marketing would come to us with a million ideas to dazzle our customer, and we’d just tell them, ‘Out of your list, pick the two things you’d like to get in the next six to twelve months.’”\n\nThey were only completing two firmware releases per year, with the majority of their time spent porting code to support new products. Gruver estimated that only 5% of their time was spent creating new features—the rest of the time was spent on non-\n\nproductive work associated with their technical debt, such as managing multiple code branches and manual testing, as shown below:\n\n20% on detailed planning (Their poor throughput and high\n\nlead times were misattributed to faulty estimation, and so,\n\nhoping to get a better answer, they were asked to estimate the\n\nwork in greater detail.)\n\n25% spent porting code, all maintained on separate code\n\nbranches\n\n10% spent integrating their code between developer branches\n\n15% spent completing manual testing\n\nGruver and his team created a goal of increasing the time spent on innovation and new functionality by a factor of ten. The team hoped this goal could be achieved through:\n\nContinuous integration and trunk-based development\n\nSignificant investment in test automation\n\nCreation of a hardware simulator so tests could be run on a virtual platform\n\nThe reproduction of test failures on developer workstations\n\nA new architecture to support running all printers off a common build and release\n\nBefore this, each product line would require a new code branch, with each model having a unique firmware build with capabilities defined at compile time.‡ The new architecture would have all developers working in a common code base, with a single firmware release supporting all LaserJet models built off of trunk,\n\nwith printer capabilities being established at runtime in an XML configuration file.\n\nFour years later, they had one codebase supporting all twenty-four\n\nHP LaserJet product lines being developed on trunk. Gruver admits trunk-based development requires a big mindset shift.\n\nEngineers thought trunk-based development would never work,\n\nbut once they started, they couldn’t imagine ever going back. Over the years we’ve had several engineers leave HP, and they would\n\ncall me to tell me about how backward development was in their new companies, pointing out how difficult it is to be effective and\n\nrelease good code when there is no feedback that continuous integration gives them.\n\nHowever, trunk-based development required them to build more\n\neffective automated testing. Gruver observed, “Without automated\n\ntesting, continuous integration is the fastest way to get a big pile of junk that never compiles or runs correctly.” In the beginning, a\n\nfull manual testing cycle required six weeks.\n\nIn order to have all firmware builds automatically tested, they invested heavily in their printer simulators and created a testing\n\nfarm in six weeks—within a few years two thousand printer simulators ran on six racks of servers that would load the\n\nfirmware builds from their deployment pipeline. Their continuous\n\nintegration (CI) system ran their entire set of automated unit, acceptance, and integration tests on builds from trunk, just as\n\ndescribed in the previous chapter. Furthermore, they created a culture that halted all work anytime a developer broke the\n\ndeployment pipeline, ensuring that developers quickly brought the system back into a green state.\n\nAutomated testing created fast feedback that enabled developers to quickly confirm that their committed code actually worked.\n\nUnit tests would run on their workstations in minutes, three levels of automated testing would run on every commit as well as every\n\ntwo and four hours. The final full regression testing would run every twenty-four hours. During this process, they:\n\nReduced the build to one build per day, eventually doing ten to\n\nfifteen builds per day\n\nWent from around twenty commits per day performed by a\n\n“build boss” to over one hundred commits per day performed by individual developers\n\nEnabled developers to change or add 75k–100k lines of code\n\neach day\n\nReduced regression test times from six weeks to one day\n\nThis level of productivity could never have been supported prior to adopting continuous integration, when merely creating a green\n\nbuild required days of heroics. The resulting business benefits were astonishing:\n\nTime spent on driving innovation and writing new features\n\nincreased from 5% of developer time to 40%.\n\nOverall development costs were reduced by approximately 40%.\n\nPrograms under development were increased by about 140%.\n\nDevelopment costs per program were decreased by 78%.\n\nWhat Gruver’s experience shows is that, after comprehensive use\n\nof version control, continuous integration is one of the most\n\ncritical practices that enable the fast flow of work in our value stream, enabling many development teams to independently\n\ndevelop, test, and deliver value. Nevertheless, continuous integration remains a controversial practice. The remainder of this\n\nchapter describes the practices required to implement continuous integration, as well as how to overcome common objections.\n\nSMALL BATCH DEVELOPMENT AND WHAT HAPPENS WHEN WE COMMIT CODE TO TRUNK INFREQUENTLY\n\nAs described in the previous chapters, whenever changes are\n\nintroduced into version control that cause our deployment pipeline to fail, we quickly swarm the problem to fix it, bringing\n\nour deployment pipeline back into a green state. However,\n\nsignificant problems result when developers work in long-lived private branches (also known as “feature branches”), only merging\n\nback into trunk sporadically, resulting in a large batch size of changes. As described in the HP LaserJet example, what results is\n\nsignificant chaos and rework in order to get their code into a\n\nreleasable state.\n\nJeff Atwood, founder of the Stack Overflow site and author of the\n\nCoding Horror blog, observes that while there are many\n\nbranching strategies, they can all be put on the following spectrum:\n\nOptimize for individual productivity: Every single person on the project works in their own private branch. Everyone\n\nworks independently, and nobody can disrupt anyone else’s work; however, merging becomes a nightmare. Collaboration becomes almost comically difficult—every person’s work has to\n\nbe painstakingly merged with everyone else’s work to see even the smallest part of the complete system.\n\nOptimize for team productivity: Everyone works in the\n\nsame common area. There are no branches, just a long, unbroken straight line of development. There’s nothing to understand, so commits are simple, but each commit can break\n\nthe entire project and bring all progress to a screeching halt.\n\nAtwood’s observation is absolutely correct—stated more precisely, the required effort to successfully merge branches back together\n\nincreases exponentially as the number of branches increase. The problem lies not only in the rework this “merge hell” creates, but also in the delayed feedback we receive from our deployment pipeline. For instance, instead of performance testing against a\n\nfully integrated system happening continuously, it will likely happen only at the end of our process.\n\nFurthermore, as we increase the rate of code production as we add\n\nmore developers, we increase the probability that any given change will impact someone else and increase the number of developers who will be impacted when someone breaks the\n\ndeployment pipeline.\n\nHere is one last troubling side effect of large batch size merges: when merging is difficult, we become less able and motivated to\n\nimprove and refactor our code, because refactorings are more\n\nlikely to cause rework for everyone else. When this happens, we are more reluctant to modify code that has dependencies\n\nthroughout the codebase, which is (tragically) where we may have the highest payoffs.\n\nThis is how Ward Cunningham, developer of the first wiki, first\n\ndescribed technical debt: when we do not aggressively refactor our codebase, it becomes more difficult to make changes and to maintain over time, slowing down the rate at which we can add\n\nnew features. Solving this problem was one of the primary reasons behind the creation of continuous integration and trunk-based development practices, to optimize for team productivity over individual productivity.\n\nADOPT TRUNK-BASED DEVELOPMENT PRACTICES\n\nOur countermeasure to large batch size merges is to institute continuous integration and trunk-based development practices,\n\nwhere all developers check in their code to trunk at least once per day. Checking code in this frequently reduces our batch size to the work performed by our entire developer team in a single day. The more frequently developers check in their code to trunk, the\n\nsmaller the batch size and the closer we are to the theoretical ideal of single-piece flow.\n\nFrequent code commits to trunk means we can run all automated\n\ntests on our software system as a whole and receive alerts when a change breaks some other part of the application or interferes with the work of another developer. And because we can detect merge\n\nproblems when they are small, we can correct them faster.\n\nWe may even configure our deployment pipeline to reject any commits (e.g., code or environment changes) that take us out of a\n\ndeployable state. This method is called gated commits, where the deployment pipeline first confirms that the submitted change will successfully merge, build as expected, and pass all the automated tests before actually being merged into trunk. If not, the developer\n\nwill be notified, allowing corrections to be made without impacting anyone else in the value stream.\n\nThe discipline of daily code commits also forces us to break our\n\nwork down into smaller chunks while still keeping trunk in a working, releasable state. And version control becomes an integral mechanism of how the team communicates with each other—\n\neveryone has a better shared understanding of the system, is aware of the state of the deployment pipeline, and can help each other when it breaks. As a result, we achieve higher quality and\n\nfaster deployment lead times.\n\nHaving these practices in place, we can now again modify our definition of “done” (addition in bold text): “At the end of each\n\ndevelopment interval, we must have integrated, tested, working, and potentially shippable code, demonstrated in a production-like environment, created from trunk using a one-click process, and validated with automated tests.”\n\nAdhering to this revised definition of done helps us further ensure the ongoing testability and deployability of the code we’re producing. By keeping our code in a deployable state, we are able\n\nto eliminate the common practice of having a separate test and stabilization phase at the end of the project.",
      "page_number": 222
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 230-237)",
      "start_page": 230,
      "end_page": 237,
      "detection_method": "topic_boundary",
      "content": "Case Study Continuous Integration at Bazaarvoice (2012)\n\nErnest Mueller, who helped engineer the DevOps\n\ntransformation at National Instruments, later helped transform the development and release processes at Bazaarvoice in 2012. Bazaarvoice supplies customer generated content (e.g., reviews, ratings) for thousands of\n\nretailers, such as Best Buy, Nike, and Walmart.\n\nAt that time, Bazaarvoice had $120 million in revenue and was preparing for an IPO.§ The business was primarily driven by the Bazaarvoice Conversations application, a monolithic Java application comprised of nearly five million lines of code dating back to 2006, spanning fifteen thousand\n\nfiles. The service ran on 1,200 servers across four data centers and multiple cloud service providers.\n\nPartially as a result of switching to an Agile development\n\nprocess and to two-week development intervals, there was a tremendous desire to increase release frequency from their current ten-week production release schedule. They had\n\nalso started to decouple parts of their monolithic application, breaking it down into microservices.\n\nTheir first attempt at a two-week release schedule was in\n\nJanuary of 2012. Mueller observed, “It didn’t go well. It caused massive chaos, with forty-four production incidents filed by our customers. The major reaction from management was basically ‘Let’s not ever do that again.’”\n\nMueller took over the release processes shortly afterward, with the goal of doing bi-weekly releases without causing customer downtime. The business objectives for releasing\n\nmore frequently included enabling faster A/B testing (described in upcoming chapters) and increasing the flow of features into production. Mueller identified three core\n\nproblems:\n\nLack of test automation made any level of testing during the two-week intervals inadequate to prevent large-scale\n\nfailures.\n\nThe version control branching strategy allowed developers to check in new code right up to the production release.\n\nThe teams running microservices were also performing independent releases, which were often causing issues during the monolith release or vice versa.\n\nMueller concluded that the monolithic Conversations application deployment process needed to be stabilized,\n\nwhich required continuous integration. In the six weeks that followed, developers stopped doing feature work to focus instead on writing automated testing suites, including unit tests in JUnit, regression tests in Selenium, and getting a\n\ndeployment pipeline running in TeamCity. “By running these tests all the time, we felt like we could make changes with some level of safety. And most importantly, we could\n\nimmediately find when someone broke something, as opposed to discovering it only after it’s in production.”\n\nThey also changed to a trunk/branch release model, where every two weeks they created a new dedicated release branch, with no new commits allowed to that branch unless\n\nthere was an emergency—all changes would be worked through a sign-off process, either per-ticket or per-team through their internal wiki. That branch would go through a QA process, which would then be promoted into production.\n\nThe improvements to predictability and quality of the releases were startling:\n\nJanuary 2012 release: forty-four customer incidents (continuous integration effort begins)\n\nMarch 6, 2012 release: five days late, five customer incidents\n\nMarch 22, 2012 release: on time, one customer incident\n\nApril 5, 2012 release: on time, zero customer incidents\n\nMueller further described how successful this effort was:\n\nWe had such success with releases every two weeks, we went to weekly releases, which required almost no\n\nchanges from the engineering teams. Because releases became so routine, it was as simple as\n\ndoubling the number of releases on the calendar and\n\nreleasing when the calendar told us to. Seriously, it was almost a non-event. The majority of changes\n\nrequired were in our customer service and marketing\n\nteams, who had to change their processes, such as changing the schedule of their weekly customer emails\n\nto make sure customers knew that feature changes\n\nwere coming. After that, we started working toward our\n\nnext goals, which eventually led to speeding up our testing times from three plus hours to less than an\n\nhour, reducing the number of environments from four\n\nto three (Dev, Test, Production, eliminating Staging), and moving to a full continuous delivery model where\n\nwe enable fast, one-click deployments.\n\nCONCLUSION\n\nTrunk-based development is likely the most controversial practice\n\ndiscussed in this book. Many engineers will not believe that it’s\n\npossible, even those that prefer working uninterrupted on a private branch without having to deal with other developers.\n\nHowever, the data from Puppet Labs’ 2015 State of DevOps\n\nReport is clear: trunk-based development predicts higher throughput and better stability, and even higher job satisfaction\n\nand lower rates of burnout.\n\nWhile convincing developers may be difficult at first, once they see the extraordinary benefits, they will likely become lifetime\n\nconverts, as the HP LaserJet and Bazaarvoice examples illustrate.\n\nContinuous integration practices set the stage for the next step, which is automating the deployment process and enabling low-\n\nrisk releases.\n\n† Branching in version control has been used in many ways, but is typically used to divide work between team\n\nmembers by release, promotion, task, component, technology platforms, and so forth.\n\n‡ Compile flags (#define and #ifdef) were used to enable/disable code execution for presence of copiers, paper size\n\nsupported, and so on.\n\n§ The production release was delayed due to their (successful) IPO.\n\n12Automate and Enable Low-Risk Releases\n\nChuck Rossi is the director of release engineering at Facebook.\n\nOne of his responsibilities is overseeing the daily code push. In\n\n2012, Rossi described their process as follows: “Starting around 1\n\np.m., I switch over to ‘operations mode’ and work with my team to\n\nget ready to launch the changes that are going out to Facebook.com that day. This is the more stressful part of the job\n\nand really relies heavily on my team’s judgment and past\n\nexperience. We work to make sure that everyone who has changes going out is accounted for and is actively testing and supporting\n\ntheir changes.”\n\nJust prior to the production push, all developers with changes\n\ngoing out must be present and check in on their IRC chat channel\n\n—any developers not present have their changes automatically removed from the deployment package. Rossi continued, “If everything looks good and our test dashboards and canary tests† are green, we push the big red button and the entire Facebook.com server fleet gets the new code delivered. Within twenty minutes, thousands and thousands of machines are up on new code with no visible impact to the people using the site.”‡\n\nLater that year, Rossi doubled their software release frequency to\n\ntwice daily. He explained that the second code push gave\n\nengineers not on the US West Coast the ability to “move and ship\n\nas quickly as any other engineer in the company,” and also gave\n\neveryone a second opportunity each day to ship code and launch\n\nfeatures.\n\nFigure 16: Number of developers deploying per week at Facebook (Source: Chuck Rossi, “Ship early and ship twice as often.”)\n\nKent Beck, the creator of the Extreme Programming methodology, one of the leading proponents of Test Driven Development, and technical coach at Facebook, further comments on the their code release strategy in an article posted on his Facebook page: “Chuck\n\nRossi made the observation that there seem to be a fixed number of changes Facebook can handle in one deployment. If we want\n\nmore changes, we need more deployments. This has led to a steady increase in deployment pace over the past five years, from weekly to daily to thrice daily deployments of our PHP code and from six to four to two week cycles for deploying our mobile apps. This improvement has been driven primarily by the release engineering team.”\n\nBy using continuous integration and making code deployment a\n\nlow-risk process, Facebook has enabled code deployment to be a\n\npart of everyone’s daily work and sustain developer productivity.\n\nThis requires that code deployment be automated, repeatable, and\n\npredictable. In the practices described in the book so far, even\n\nthough our code and environments have been tested together,\n\nmost likely we are not deploying to production very often because\n\ndeployments are manual, time-consuming, painful, tedious, and\n\nerror-prone, and they often involve an inconvenient and\n\nunreliable handoff between Development and Operations.\n\nAnd because it is painful, we tend to do it less and less frequently, resulting in another self-reinforcing downward spiral. By deferring production deployments, we accumulate ever-larger\n\ndifferences between the code to be deployed and what’s running in production, increasing the deployment batch size. As deployment batch size grows, so does the risk of unexpected outcomes associated with the change, as well as the difficulty fixing them.\n\nIn this chapter, we reduce the friction associated with production deployments, ensuring that they can be performed frequently and easily, either by Operations or Development. We do this by extending our deployment pipeline.\n\nInstead, of merely continually integrating our code in a production-like environment, we will enable the promotion into\n\nproduction of any build that passes our automated test and validation process, either on demand (i.e., at the push of a button) or automatically (i.e., any build that passes all the tests is automatically deployed).\n\nBecause of the number of practices presented, extensive footnotes are provided with numerous examples and additional information,\n\nwithout interrupting the presentation of concepts in the chapter.\n\nAUTOMATE OUR DEPLOYMENT PROCESS\n\nAchieving outcomes like those at Facebook requires that we have\n\nan automated mechanism that deploys our code into production. Especially if we have a deployment process that has existed for\n\nyears, we need to fully document the steps in the deployment process, such as in a value stream mapping exercise, which we can\n\nassemble in a workshop or document incrementally (e.g., in a\n\nwiki).\n\nOnce we have the process documented, our goal is to simplify and\n\nautomate as many of the manual steps as possible, such as:\n\nPackaging code in ways suitable for deployment\n\nCreating pre-configured virtual machine images or containers\n\nAutomating the deployment and configuration of middleware\n\nCopying packages or files onto production servers\n\nRestarting servers, applications, or services\n\nGenerating configuration files from templates\n\nRunning automated smoke tests to make sure the system is working and correctly configured\n\nRunning testing procedures",
      "page_number": 230
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 238-247)",
      "start_page": 238,
      "end_page": 247,
      "detection_method": "topic_boundary",
      "content": "Scripting and automating database migrations\n\nWhere possible, we will re-architect to remove steps, particularly those that take a long time to complete. We also want to not only\n\nreduce our lead times but also the number of handoffs as much as possible in order to reduce errors and loss of knowledge.\n\nHaving developers focus on automating and optimizing the\n\ndeployment process can lead to significant improvements in deployment flow, such as ensuring that small application\n\nconfiguration changes no longer need new deployments or new\n\nenvironments.\n\nHowever, this requires that Development works closely with Operations to ensure that all the tools and processes we co-create\n\ncan be used downstream, as opposed to alienating Operations or reinventing the wheel.\n\nMany tools that provide continuous integration and testing also\n\nsupport the ability to extend the deployment pipeline so that validated builds can be promoted into production, typically after\n\nthe production acceptance tests are performed (e.g., the Jenkins\n\nBuild Pipeline plugin, ThoughtWorks Go.cd and Snap CI, Microsoft Visual Studio Team Services, and Pivotal Concourse).\n\nThe requirements for our deployment pipeline include:\n\nDeploying the same way to every environment: By\n\nusing the same deployment mechanism for every environment (e.g., development, test, and production), our production\n\ndeployments are likely to be far more successful, since we know that it has been successfully performed many times already\n\nearlier in the pipeline.\n\nSmoke testing our deployments: During the deployment\n\nprocess, we should test that we can connect to any supporting\n\nsystems (e.g., databases, message buses, external services) and run a single test transaction through the system to ensure that\n\nour system is performing as designed. If any of these tests fail, we should fail the deployment.\n\nEnsure we maintain consistent environments: In\n\nprevious steps, we created a single-step environment build process so that the development, test, and production\n\nenvironments had a common build mechanism. We must\n\ncontinually ensure that these environments remain synchronized.\n\nOf course, when any problems occur during deployment, we pull\n\nthe Andon cord and swarm the problem until the problem is resolved, just as we do when our deployment pipeline fails in any\n\nof the earlier steps.\n\nCase Study Daily Deployments at CSG International (2013)\n\nCSG International runs one of the largest bill printing\n\noperations in the US. Scott Prugh, their chief architect and VP of Development, in an effort to improve the predictability\n\nand reliability of their software releases, doubled their\n\nrelease frequency from two per year to four per year (halving their deployment interval from twenty-eight weeks to fourteen weeks).\n\nAlthough the Development teams were using continuous integration to deploy their code into test environments daily,\n\nthe production releases were being performed by the Operations team. Prugh observed, “It was as if we had a ‘practice team’ that practiced daily (or even more frequently)\n\nin low-risk test environments, perfecting their processes and tools. But our production ‘game team’ got very few attempts to practice, only twice per year. Worse, they were practicing in the high-risk production environments, which were often\n\nvery different than the pre-production environments with different constraints—the development environments were missing many production assets such as security, firewalls,\n\nload balancers, and a SAN.”\n\nTo solve this problem, they created a Shared Operations Team (SOT) that was responsible for managing all the\n\nenvironments (development, test, production) performing daily deployments into those development and test environments, as well as doing production deployments and\n\nreleases every fourteen weeks. Because the SOT was doing deployments every day, any problems they encountered that were left unfixed would simply occur again the next day. This created tremendous motivation to automate tedious or error-\n\nprone manual steps and to fix any issues that could potentially happen again. Because the deployments were performed nearly one hundred times before the production\n\nrelease, most problems were found and fixed long before then.\n\nDoing this revealed problems that were previously only\n\nexperienced by the Ops team, which were then problems for\n\nthe entire value stream to solve. The daily deployments enabled daily feedback on which practices worked and\n\nwhich didn’t.\n\nThey also focused on making all their environments look as similar as possible, including the restricted security access\n\nrights and load balancers. Prugh writes, “We made non- production environments as similar to production as possible, and we sought to emulate production constraints in\n\nas many ways as possible. Early exposure to production- class environments altered the designs of the architecture to make them friendlier in these constrained or different environments. Everyone gets smarter from this approach.”\n\nPrugh also observes:\n\n“We have experienced many cases where changes to\n\ndatabase schemas are either 1) handed off to a DBA team for them to ‘go and figure it out’ or 2) automated tests that run on unrealistically small data sets (i.e., “100’s of MB vs.\n\n100’s of GBs”), which led to production failures. In our old way of working, this would become a late-night blame game between teams trying to unwind the mess. We created a development and deployment process that removed the\n\nneed for handoffs to DBAs by cross-training developers, automating schema changes, and executing them daily. We created realistic load testing against sanitized customer\n\ndata, ideally running migrations every day. By doing this, we run our service hundreds of times with realistic scenarios before seeing actual production traffic.”§\n\nTheir results were astonishing. By doing daily deployments and doubling the frequency of production releases, the\n\nnumber of production incidents went down by 91%, MTTR went down by 80%, and the deployment lead time required for the service to run in production in a “fully hands-off state” went from fourteen days to one day.\n\nPrugh reported that deployments became so routine that the Ops team was playing video games by the end of the first day. In addition to deployments going more smoothly for Dev\n\nand Ops, in 50% of the cases the customer received the value in half the time, underscoring how more frequent deployments can be good for Development, QA, Operations,\n\nand the customer.\n\nFigure 17: Daily deployments and increasing release frequency resulted in decrease in # of production incidents and MTTR (Source: “DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),” YouTube video, 29:39, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=tKdIHCL0DUg.)\n\nENABLE AUTOMATED SELF-SERVICE DEPLOYMENTS\n\nConsider the following quote from Tim Tischler, Director of Operations Automation at Nike, Inc., that describes the common experience of a generation of developers: “As a developer, there\n\nhas never been a more satisfying point in my career than when I wrote the code, when I pushed the button to deploy it, when I could see the production metrics confirm that it actually worked in\n\nproduction, and when I could fix it myself if it didn’t.”\n\nDevelopers’ ability to self-deploy code into production, to quickly see happy customers when their feature works, and to quickly fix\n\nany issues without having to open up a ticket with Operations has diminished over the last decade—in part as a result of a need for control and oversight, perhaps driven by security and compliance\n\nrequirements.\n\nThe resulting common practice is for Operations to perform code deployments, because separation of duties is a widely accepted\n\npractice to reduce the risk of production outages and fraud. However, to achieve DevOps outcomes, our goal is to shift our reliance to other control mechanisms that can mitigate these risks equally or even more effectively, such as through automated\n\ntesting, automated deployment, and peer review of changes.\n\nThe Puppet Labs’ 2013 State of DevOps Report, which surveyed\n\nover four thousand technology professionals, found that there was no statistically significant difference in the change success rates between organizations where Development deployed code and those where Operations deployed code.\n\nIn other words, when there are shared goals that span Development and Operations, and there is transparency, responsibility, and accountability for deployment outcomes, it\n\ndoesn’t matter who performs the deployment. In fact, we may even have other roles, such as testers or project managers, able to deploy to certain environments so they can get their own work\n\ndone quickly, such as setting up demonstrations of specific features in test or UAT environments.\n\nTo better enable fast flow, we want a code promotion process that\n\ncan be performed by either Development or Operations, ideally without any manual steps or handoffs. This affects the following steps:\n\nBuild: Our deployment pipeline must create packages from version control that can be deployed to any environment, including production.\n\nTest: Anyone should be able to run any or all of our automated test suite on their workstation or on our test systems.\n\nDeploy: Anybody should be able to deploy these packages to any environment where they have access, executed by running scripts that are also checked in to version control.\n\nThese are the practices that enable deployments to be performed successfully, regardless of who is performing the deployment.\n\nINTEGRATE CODE DEPLOYMENT INTO THE DEPLOYMENT PIPELINE\n\nOnce the code deployment process is automated, we can make it part of the deployment pipeline. Consequently, our deployment automation must provide the following capabilities:\n\nEnsure that packages created during the continuous integration process are suitable for deployment into production\n\nShow the readiness of production environments at a glance\n\nProvide a push-button, self-service method for any suitable version of the packaged code to be deployed into production\n\nRecord automatically, for auditing and compliance purposes, which commands were run on which machines when, who authorized it, and what the output was\n\nRun a smoke test to ensure the system is operating correctly and the configuration settings, including items such as database connection strings, are correct\n\nProvide fast feedback for the deployer so they can quickly determine whether their deployment was successful (e.g., did the deployment succeed, is the application performing as\n\nexpected in production, etc.)\n\nOur goal is ensure that deployments are fast—we don’t want to\n\nhave to wait hours to determine whether our code deployment succeeded or failed and then need hours to deploy any needed code fixes. Now that we have technologies such as containers, it is possible to complete even the most complex deployments in\n\nseconds or minutes. In Puppet Labs’ 2014 State of DevOps Report, the data showed that high performers had deployment\n\nlead times measured in minutes or hours, while the lowest\n\nperformers had deployment lead times measured in months.\n\nFigure 18: High performers had much faster deployment lead times and much faster time to restore production service after incidents (Source: Puppet Labs, 2014 State of DevOps Report.)\n\nBy building this capability, we now have a “deploy code” button\n\nthat allows us to safely and quickly promote changes to our code\n\nand our environments into production through our deployment pipeline.\n\nCase Study Etsy—Self-Service Developer Deployment, an Example of Continuous Deployment (2014)\n\nUnlike at Facebook where deployments are managed by release engineers, at Etsy deployments are performed by\n\nanyone who wants to perform a deployment, such as\n\nDevelopment, Operations, or Infosec. The deployment process at Etsy has become so safe and routine that new\n\nengineers will perform a production deployment on their first\n\nday at work—as have Etsy board members and even dogs!\n\nAs Noah Sussman, a test architect at Etsy, wrote, “By the time 8am rolls around on a normal business day, 15 or so\n\npeople and dogs are starting to queue up, all of them\n\nexpecting to collectively deploy up to 25 changesets before the day is done.”\n\nEngineers who want to deploy their code first go to a chat\n\nroom, where engineers add themselves to the deploy queue, see the deployment activity in progress, see who else is in\n\nthe queue, broadcast their activities, and get help from other\n\nengineers when they need it. When it’s an engineer’s turn to deploy, they are notified in the chat room.\n\nThe goal at Etsy has been to make it easy and safe to\n\ndeploy into production with the fewest number of steps and the least amount of ceremony. Likely before the developer\n\neven checks in code, they will run on their workstation all\n\n4,500 unit tests, which takes less than one minute. All calls to external systems, such as databases, have been stubbed\n\nout.\n\nAfter they check their changes in to trunk in version control, over seven thousand automated trunk tests are instantly run\n\non their continuous integration (CI) servers. Sussman writes, “Through trial-and-error, we’ve settled on about 11 minutes\n\nas the longest that the automated tests can run during a\n\npush. That leaves time to re-run the tests once during a deployment [if someone breaks something and needs to fix\n\nit], without going too far past the 20 minute time limit.”",
      "page_number": 238
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 248-255)",
      "start_page": 248,
      "end_page": 255,
      "detection_method": "topic_boundary",
      "content": "If all the tests were run sequentially, Sussman states that\n\n“the 7,000 trunk tests would take about half an hour to\n\nexecute. So we split these tests up into subsets, and distribute those onto the 10 machines in our Jenkins [CI]\n\ncluster....Splitting up our test suite and running many tests in parallel, gives us the desired 11 minute runtime.”\n\nFigure 19: The Deployinator console at Etsy (Source: Erik Kastner, “Quantum of Deployment,” CodeasCraft.com, May 20, 2010, https://codeascraft.com/2010/05/20/quantum-of-deployment/.)\n\nThe next tests to run are the smoke tests, which are system\n\nlevel tests that run cURL to execute PHPUnit test cases. Following these tests, the functional tests are run, which\n\nexecute end-to-end GUI-driven tests on a live server—this\n\nserver is either their QA environment or staging environment (nicknamed “Princess”), which is actually a production\n\nserver that has been taken out of rotation, ensuring that it\n\nexactly matches the production environment.\n\nOnce it is an engineer’s turn to deploy, Erik Kastner writes,\n\n“you go to Deployinator [an internally developed tool, see\n\nfigure 19] and push the button to get it on QA. From there it visits Princess....Then, when it’s ready to go live, you hit the\n\n“Prod” button and soon your code is live, and everyone in IRC [chat channel] knows who pushed what code, complete\n\nwith a link to the diff. For anyone not on IRC, there’s the\n\nemail that everyone gets with the same information.”\n\nIn 2009, the deployment process at Etsy was a cause of\n\nstress and fear. By 2011, it had become a routine operation,\n\nhappening twenty-five to fifty times per day, helping engineers get their code quickly into production, delivering\n\nvalue to their customers.\n\nDECOUPLE DEPLOYMENTS FROM RELEASES\n\nIn the traditional launch of a software project, releases are driven by our marketing launch date. On the prior evening, we deploy our\n\ncompleted software (or as close to complete as we could get) into\n\nproduction. The next morning, we announce our new capabilities to the world, start taking orders, deliver the new functionality to\n\ncustomer, etc.\n\nHowever, all too often things don’t go according to plan. We may experience production loads that we never tested or designed for,\n\ncausing our service to fail spectacularly, both for our customers\n\nand our organization. Worse, restoring service may require a painful rollback process or an equally risky fix forward operation,\n\nwhere we make changes directly in production, this can all be a\n\ntruly miserable experience for workers. When everything is finally\n\nworking, everyone breathes a sigh of relief, grateful that production deployments and releases don’t happen more often.\n\nOf course, we know that we need to be deploying more frequently\n\nto achieve our desired outcome of smooth and fast flow, not less\n\nfrequently. To enable this, we need to decouple our production deployments from our feature releases. In practice, the terms\n\ndeployment and release are often used interchangeably. However, they are two distinct actions that serve two very different\n\npurposes:\n\nDeployment is the installation of a specified version of software to a given environment (e.g., deploying code into an integration\n\ntest environment or deploying code into production).\n\nSpecifically, a deployment may or may not be associated with a release of a feature to customers.\n\nRelease is when we make a feature (or set of features) available\n\nto all our customers or a segment of customers (e.g., we enable the feature to be used by 5% of our customer base). Our code\n\nand environments should be architected in such a way that the\n\nrelease of functionality does not require changing our application code.¶\n\nIn other words, when we conflate deployment and release, it\n\nmakes it difficult to create accountability for successful outcomes —decoupling these two activities allows us to empower\n\nDevelopment and Operations to be responsible for the success of\n\nfast and frequent deployments, while enabling product owners to be responsible for the successful business outcomes of the release\n\n(i.e., was building and launching the feature worth our time).\n\nThe practices described so far in this book ensure that we are\n\ndoing fast and frequent production deployments throughout feature development, with the goal of reducing the risk and impact\n\nof deployment errors. The remaining risk is release risk, which is\n\nwhether the features we put into production achieve the desired customer and business outcomes.\n\nIf we have extremely long deployment lead times, this dictates\n\nhow frequently we can release new features to the marketplace. However, as we become able to deploy on demand, how quickly\n\nwe expose new functionality to customers becomes a business and\n\nmarketing decision, not a technical decision. There are two broad categories of release patterns we can use:\n\nEnvironment-based release patterns: This is where we\n\nhave two or more environments that we deploy into, but only one environment is receiving live customer traffic (e.g., by\n\nconfiguring our load balancers). New code is deployed into a\n\nnon-live environment, and the release is performed moving traffic to this environment. These are extremely powerful\n\npatterns, because they typically require little or no change to\n\nour applications. These patterns include blue-green deployments, canary releases, and cluster immune systems,\n\nall of which will be discussed shortly.\n\nApplication-based release patterns: This is where we modify our application so that we can selectively release and\n\nexpose specific application functionality by small configuration\n\nchanges. For instance, we can implement feature flags that progressively expose new functionality in production to the\n\ndevelopment team, all internal employees, 1% of our customers, or, when we are confident that the release will\n\noperate as designed, our entire customer base. As discussed\n\nearlier, this enables a technique called dark launching, where we stage all the functionality to be launched in production and\n\ntest it with production traffic before our release. For instance,\n\nwe may invisibly test our new functionality with production traffic for weeks before our launch in order to expose problems\n\nso that they can be fixed before our actual launch.\n\nENVIRONMENT-BASED RELEASE PATTERNS\n\nDecoupling deployments from our releases dramatically changes\n\nhow we work. We no longer have to perform deployments in the middle of the night or on weekends to lower the risk of negatively\n\nimpacting customers. Instead, we can do deployments during\n\ntypical business hours, enabling Ops to finally have normal working hours, just like everyone else.\n\nThis section focuses on environment-based release patterns,\n\nwhich require no changes to application code. We do this by having multiple environments to deploy into, but only one of them\n\nreceives live customer traffic. By doing this, we can significantly\n\ndecrease the risk associated with production releases and reduce the deployment lead time.\n\nThe Blue-Green Deployment Pattern\n\nThe simplest of the three patterns is called blue-green\n\ndeployment. In this pattern, we have two production environments: blue and green. At any time, only one of these is\n\nserving customer traffic—in figure 20, the green environment is\n\nlive.\n\nFigure 20: Blue-green deployment patterns (Source: Humble and North, Continuous Delivery, 261.)\n\nTo release a new version of our service, we deploy to the inactive\n\nenvironment where we can perform our testing without\n\ninterrupting the user experience. When we are confident that everything is functioning as designed, we execute our release by\n\ndirecting traffic to the blue environment. Thus, blue becomes live and green becomes staging. Roll back is performed by sending customer traffic back to the green environment.**\n\nThe blue-green deployment pattern is simple, and it is extremely easy to retrofit onto existing systems. It also has incredible\n\nbenefits, such as enabling the team to perform deployments\n\nduring normal business hours and conduct simple changeovers (e.g., changing a router setting, changing a symlink) during off-\n\npeak times. This alone can dramatically improve the work\n\nconditions for the team performing the deployment.\n\nDealing with Database Changes\n\nHaving two versions of our application in production creates problems when they depend upon a common database—when the\n\ndeployment requires database schema changes or adding,\n\nmodifying, or deleting tables or columns, the database cannot support both versions of our application. There are two general\n\napproaches to solving this problem:\n\nCreate two databases (i.e., a blue and green database):\n\nEach version—blue (old) and green (new)—of the application has its own database. During the release, we put the blue\n\ndatabase into read-only mode, perform a backup of it, restore\n\nonto the green database, and finally switch traffic to the green environment. The problem with this pattern is that if we need\n\nto roll back to the blue version, we can potentially lose\n\ntransactions if we don’t manually migrate them from the green version first.\n\nDecouple database changes from application changes:\n\nInstead of supporting two databases, we decouple the release of database changes from the release of application changes by\n\ndoing two things: First, we make only additive changes to our\n\ndatabase, we never mutate existing database objects, and second, we make no assumptions in our application about\n\nwhich database version will be in production. This is very different than how we’ve been traditionally trained to think\n\nabout databases, where we avoid duplicating data. The process\n\nof Decoupling database changes from application changes was used by IMVU (among others) around 2009, enabling them to\n\ndo fifty deployments per day, some of which required database changes.††\n\nCase Study Dixons Retail—Blue-Green Deployment for Point-Of-Sale System (2008)\n\nDan North and Dave Farley, co-authors of Continuous\n\nDelivery, were working on a project for Dixons Retail, a large British retailer involving thousands of point-of-sale (POS)\n\nsystems that resided in hundreds of retail stores and\n\noperating under a number of different customer brands.\n\nAlthough blue-green deployments are mostly associated\n\nwith online web services, North and Farley used this pattern\n\nto significantly reduce the risk and changeover times for POS upgrades.\n\nTraditionally, upgrading POS systems are a big bang,\n\nwaterfall project: the POS clients and the centralized server are upgraded at the same time, which requires extensive\n\ndowntime (often an entire weekend), as well as significant\n\nnetwork bandwidth to push out the new client software to all the retail stores. When things don’t go entirely according to\n\nplan, it can be incredibly disruptive to store operations.\n\nFor this upgrade, there was not enough network bandwidth to upgrade all the POS systems simultaneously, which made\n\nthe traditional strategy impossible. To solve this problem,\n\nthey used the blue-green strategy and created two production versions of the centralized server software,\n\nenabling them to simultaneously support the old and new\n\nversions of the POS clients.\n\nAfter they did this, weeks before the planned POS upgrade,\n\nthey started sending out new versions of client POS\n\nsoftware installers to the retail stores over the slow network links, deploying the new software onto the POS systems in\n\nan inactive state. Meanwhile, the old version kept running as normal.",
      "page_number": 248
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 256-263)",
      "start_page": 256,
      "end_page": 263,
      "detection_method": "topic_boundary",
      "content": "When all the POS clients had everything staged for the\n\nupgrade (the upgraded client and server had tested together successfully, and new client software had been deployed to\n\nall clients), the store managers were empowered to decide\n\nwhen to release the new version.\n\nDepending on their business needs, some managers\n\nwanted to use the new features immediately and released\n\nright away, while others wanted to wait. In either case, whether releasing features immediately or waiting, itwas\n\nsignificantly better for the managers than having the centralized IT department choose for them when the release\n\nwould occur.\n\nThe result was a significantly smoother and faster release,\n\nhigher satisfaction from the store managers, and far less disruption to store operations. Furthermore, this application\n\nof blue-green deployments to thick-client PC applications demonstrates how DevOps patterns can be universally\n\napplied to different technologies, often in very surprising\n\nways but with the same fantastic outcomes.\n\nThe Canary and Cluster Immune System Release Patterns\n\nThe blue-green release pattern is easy to implement and can dramatically increase the safety of software releases. There are\n\nvariants of this pattern that can further improve safety and\n\ndeployment lead times using automation, but with the potential trade-off of additional complexity.\n\nThe canary release pattern automates the release process of\n\npromoting to successively larger and more critical environments\n\nas we confirm that the code is operating as designed.\n\nThe term canary release comes from the tradition of coal miners bringing caged canaries into mines to provide early detection of\n\ntoxic levels of carbon monoxide. If there wastoo much gas in the\n\ncave, it would kill the canaries before it killed the miners, alerting them to evacuate.\n\nIn this pattern, when we perform a release, we monitor how the\n\nsoftware in each environment is performing. When something appears to be going wrong, we roll back; otherwise, we deploy to the next environment.‡‡\n\nFigure 21 shows the groups of environments Facebook created to support this release pattern:\n\nA1 group: Production servers that only serve internal employees\n\nA2 group: Production servers that only serve a small\n\npercentage of customers and are deployed when certain acceptance criteria have been met (either automated or\n\nmanual)\n\nA3 group: The rest of the production servers, which are deployed after the software running in the A2 cluster meets\n\ncertain acceptance criteria\n\nFigure 21: The canary release pattern (Source: Humble and Farley, Continuous Delivery, 263.)\n\nThe cluster immune system expands upon the canary release\n\npattern by linking our production monitoring system with our\n\nrelease process and by automating the roll back of code when the\n\nuser-facing performance of the production system deviates outside of a predefined expected range, such as when the conversion rates\n\nfor new users drops below our historical norms of 15%–20%.\n\nThere are two significant benefits to this type of safeguard. First, we protect against defects that are hard to find through automated\n\ntests, such as a web page change that renders some critical page\n\nelement invisible (e.g., CSS change). Second, we reduce the time\n\nrequired to detect and respond to the degraded performance created by our change.§§\n\nAPPLICATION-BASED PATTERNS TO ENABLE SAFER RELEASES\n\nIn the previous section, we created environment-based patterns\n\nthat allowed us to decouple our deployments from our releases by\n\nusing multiple environments and by switching between which\n\nenvironment was live, which can be entirely implemented at the\n\ninfrastructure level.\n\nIn this section, we describe application-based release patterns that we can implement in our code, allowing even greater flexibility in\n\nhow we safely release new features to our customer, often on a\n\nper-feature basis. Because application-based release patterns are\n\nimplemented in the application, these require involvement from\n\nDevelopment.\n\nImplement Feature Toggles\n\nThe primary way we enable application-based release patterns is\n\nby implementing feature toggles, which provide us with the\n\nmechanism to selectively enable and disable features without\n\nrequiring a production code deployment. Feature toggles can also control which features are visible and available to specific user\n\nsegments (e.g., internal employees, segments of customers).\n\nFeature toggles are usually implemented by wrapping application logic or UI elements with a conditional statement, where the\n\nfeature is enabled or disabled based on a configuration setting\n\nstored somewhere. This can be as simple as an application\n\nconfiguration file (e.g., configuration files in JSON, XML), or it\n\nmight be through a directory service or even a web service specifically designed to manage feature toggling.¶¶\n\nFeature toggles also enable us to do the following:\n\nRoll back easily: Features that create problems or\n\ninterruptions in production can be quickly and safely disabled\n\nby merely changing the feature toggle setting. This is especially\n\nvaluable when deployments are infrequent—switching off one\n\nparticular stakeholder’s features is usually much easier than\n\nrolling back an entire release.\n\nGracefully degrade performance: When our service\n\nexperiences extremely high loads that would normally require\n\nus to increase capacity or, worse, risk having our service fail in\n\nproduction, we can use feature toggles to reduce the quality of service. In other words, we can increase the number of users\n\nwe serve by reducing the level of functionality delivered (e.g.,\n\nreduce the number of customers who can access a certain\n\nfeature, disable CPU-intensive features such as\n\nrecommendations, etc.).\n\nIncrease our resilience through a service-oriented\n\narchitecture: If we have a feature that relies on another\n\nservice that isn’t complete yet, we can still deploy our feature\n\ninto production but hide it behind a feature toggle. When that service finally becomes available, we can toggle the feature on.\n\nSimilarly, when a service we rely upon fails, we can turn off the\n\nfeature to prevent calls to the downstream service while\n\nkeeping the rest of the application running.\n\nTo ensure that we find errors in features wrapped in feature\n\ntoggles, our automated acceptance tests should run with all\n\nfeature toggles on. (We should also test that our feature toggling\n\nfunctionality works correctly too!)\n\nFeature toggles enable the decoupling of code deployments and\n\nfeature releases, later in the book we use feature toggles to enable\n\nhypothesis-driven development and A/B testing, furthering our ability to achieve our desired business outcomes.\n\nPerform Dark Launches\n\nFeature toggles allow us to deploy features into production\n\nwithout making them accessible to users, enabling a technique\n\nknown as dark launching. This is where we deploy all the\n\nfunctionality into production and then perform testing of that\n\nfunctionality while it is still invisible to customers. For large or\n\nrisky changes, we often do this for weeks before the production launch, enabling us to safely test with the anticipated production-\n\nlike loads.\n\nFor instance, suppose we dark launch a new feature that poses significant release risk, such as new search features, account\n\ncreation processes, or new database queries. After all the code is in\n\nproduction, keeping the new feature disabled, we may modify user\n\nsession code to make calls to new functions—instead of displaying\n\nthe results to the user, we simply log or discard the results.\n\nFor example, we may have 1% of our online users make invisible\n\ncalls to a new feature scheduled to be launched to see how our new\n\nfeature behaves under load. After we find and fix any problems,\n\nwe progressively increase the simulated load by increasing the frequency and number of users exercising the new functionality.\n\nBy doing this, we are able to safely simulate production-like loads,\n\ngiving us confidence that our service will perform as it needs to.\n\nFurthermore, when we launch a feature, we can progressively roll\n\nout the feature to small segments of customers, halting the release\n\nif any problems are found. That way, we minimize the number of\n\ncustomers who are given a feature only to have it taken away\n\nbecause we find a defect or are unable to maintain the required\n\nperformance.\n\nIn 2009, when John Allspaw was VP of Operations at Flickr, he\n\nwrote to the Yahoo! executive management team about their dark\n\nlaunch process, saying that it “increases everyone’s confidence almost to the point of apathy, as far as fear of load-related issues\n\nare concerned. I have no idea how many code deploys there were\n\nmade to production on any given day in the past 5 years...because\n\nfor the most part I don’t care, because those changes made in\n\nproduction have such a low chance of causing issues. When they have caused issues, everyone on the Flickr staff can find on a\n\nwebpage when the change was made, who made the change, and exactly (line-by-line) what the change was.”***\n\nLater, when we have built adequate production telemetry in our\n\napplication and environments, we can also enable faster feedback\n\ncycles to validate our business assumptions and outcomes\n\nimmediately after we deploy the feature into production.\n\nBy doing this, we no longer wait until a big bang release to test\n\nwhether customers want to use the functionality we build. Instead,\n\nby the time we announce and release our big feature, we have\n\nalready tested our business hypotheses and run countless\n\nexperiments to continually refine our product with real customers, which helps us validate that the features will achieve the desired\n\ncustomer outcomes.\n\nCase Study Dark Launch of Facebook Chat (2008)\n\nFor nearly a decade, Facebook has been one of the most\n\nwidely visited Internet sites, as measured by pages viewed\n\nand unique site users. In 2008, it had over seventy million\n\ndaily active users, which created a challenge for the team that was developing the new Facebook Chat functionality.†††\n\nEugene Letuchy, an engineer on the Chat team, wrote about how the number of concurrent users presented a huge\n\nsoftware engineering challenge: “The most resource-\n\nintensive operation performed in a chat system is not\n\nsending messages. It is rather keeping each online user\n\naware of the online-idle-offline states of their friends, so that conversations can begin.”\n\nImplementing this computationally-intensive feature was one\n\nof the largest technical undertakings ever at Facebook and took almost a year to complete.‡‡‡ Part of the complexity of the project was due to the wide variety of technologies\n\nneeded to achieve the desired performance, including C++,\n\nJavaScript, and PHP, as well as their first use of Erlang in\n\ntheir back-end infrastructure.\n\nThroughout the course of the year-long endeavor, the Chat\n\nteam checked their code in to version control, where it would\n\nbe deployed into production at least once per day. At first,\n\nthe Chat functionality was visible only to the Chat team. Later, it was made visible to all internal employees, but it\n\nwas completely hidden from external Facebook users\n\nthrough Gatekeeper, the Facebook feature toggling service.\n\nAs part of their dark launch process, every Facebook user\n\nsession, which runs JavaScript in the user browser, had a\n\ntest harness loaded into it—the chat UI elements were\n\nhidden, but the browser client would send invisible test chat\n\nmessages to the back-end chat service that was already in",
      "page_number": 256
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 264-271)",
      "start_page": 264,
      "end_page": 271,
      "detection_method": "topic_boundary",
      "content": "production, enabling them to simulate production-like loads\n\nthroughout the entire project, allowing them to find and fix performance problems long before the customer release.\n\nBy doing this, every Facebook user was part of a massive\n\nload testing program, which enabled the team to gain confidence that their systems could handle realistic\n\nproduction-like loads. The Chat release and launch required\n\nonly two steps: modifying the Gatekeeper configuration\n\nsetting to make the Chat feature visible to some portion of\n\nexternal users, and having Facebook users load new\n\nJavaScript code that rendered the Chat UI and disabled the invisible test harness. If something went wrong, the two\n\nsteps would be reversed. When the launch day of Facebook\n\nChat arrived, it was surprisingly successful and uneventful,\n\nseeming to scale effortlessly from zero to seventy million\n\nusers overnight. During the release, they incrementally\n\nenabled the chat functionality to ever-larger segments of the customer population—first to all internal Facebook\n\nemployees, then to 1% of the customer population, then to\n\n5%, and so forth. As Letuchy wrote, “The secret for going\n\nfrom zero to seventy million users overnight is to avoid doing\n\nit all in one fell swoop.”\n\nSURVEY OF CONTINUOUS DELIVERY AND CONTINUOUS DEPLOYMENT IN PRACTICE\n\nIn Continuous Delivery, Jez Humble and David Farley define the\n\nterm continuous delivery. The term continuous deployment was\n\nfirst mentioned by Tim Fitz in his blog post “Continuous\n\nDeployment at IMVU: Doing the impossible fifty times a day.”\n\nHowever, in 2015, during the construction of The DevOps\n\nHandbook, Jez Humble commented, “In the last five years, there\n\nhas been confusion around the terms continuous delivery versus\n\ncontinuous deployment—and, indeed, my own thinking and definitions have changed since we wrote the book. Every\n\norganization should create their variations, based on what they\n\nneed. The key thing we should care about is not the form, but the\n\noutcomes: deployments should be low-risk, push-button events we\n\ncan perform on demand.”\n\nHis updated definitions of continuous delivery and continuous\n\ndeployment are as follows:\n\nWhen all developers are working in small batches on trunk, or\n\neveryone is working off trunk in short-lived feature branches\n\nthat get merged to trunk regularly, and when trunk is always\n\nkept in a releasable state, and when we can release on demand\n\nat the push of a button during normal business hours, we are doing continuous delivery. Developers get fast feedback when\n\nthey introduce any regression errors, which include defects,\n\nperformance issues, security issues, usability issues, etc. When\n\nthese issues are found, they are fixed immediately so that trunk\n\nis always deployable.\n\nIn addition to the above, when we are deploying good builds\n\ninto production on a regular basis through self-service (being\n\ndeployed by Dev or by Ops)—which typically means that we are deploying to production at least once per day per developer, or\n\nperhaps even automatically deploying every change a\n\ndeveloper commits—this is when we are engaging in\n\ncontinuous deployment.\n\nDefined this way, continuous delivery is the prerequisite for\n\ncontinuous deployment—just as continuous integration is a prerequisite for continuous delivery. Continuous deployment is\n\nlikely applicable in the context of web services that are delivered\n\nonline. However, continuous delivery is applicable in almost every\n\ncontext where we desire deployments and releases that have high\n\nquality, fast lead times and have highly predictable, low-risk\n\noutcomes, including for embedded systems, COTS products, and mobile apps.\n\nAt Amazon and Google, most teams practice continuous delivery,\n\nalthough some perform continuous deployment— thus, there is considerable variation between teams in how frequently they\n\ndeploy code and how deployments are performed. Teams are\n\nempowered to choose how to deploy based on the risks they are\n\nmanaging. For example, the Google App Engine team often\n\ndeploys once per day, while the Google Search property deploys several times per week.\n\nSimilarly, most of the cases studies presented in this book are also\n\ncontinuous delivery, such as the embedded software running on\n\nHP LaserJet printers, the CSG bill printing operations running on twenty technology platforms including a COBOL mainframe\n\napplication, Facebook, and Etsy. These same patterns can be used\n\nfor software that runs on mobile phones, ground control stations\n\nthat control satellites, and so forth.\n\nCONCLUSION\n\nAs the Facebook, Etsy, and CSG examples have shown, releases\n\nand deployments do not have to be high-risk, high-drama affairs\n\nthat require tens or hundreds of engineers to work around the\n\nclock to complete. Instead, they can be made entirely routine and\n\na part of everyone’s daily work.\n\nBy doing this, we can reduce our deployment lead times from\n\nmonths to minutes, allowing our organizations to quickly deliver\n\nvalue to our customer without causing chaos and disruption.\n\nFurthermore, by having Dev and Ops work together, we can finally\n\nmake Operations work humane.\n\n† A canary release test is when software is deployed to a small group of production servers to make sure nothing\n\nterrible happens to them with live customer traffic.\n\n‡ The Facebook front-end codebase is primarily written in PHP. In 2010, to increase site performance, the PHP code was converted into C++ by their internally developed HipHop compiler, which was then compiled into a 1.5 GB executable. This file was then copied onto production servers using BitTorrent, enabling the copy operation to be completed in fifteen minutes.\n\n§ In their experiments, they found that SOT teams were successful, regardless of whether they were managed by\n\nDevelopment or Operations, as long as the teams were staffed with the right people and were dedicated to SOT success.\n\n¶ Operation Desert Shield may serve as an effective metaphor. Starting on August 7, 1990, thousands of men and materials were safely deployed over four months into the production theater, culminating in a single, multi- disciplinary, highly coordinated release.\n\n** Other ways that we can implement the blue-green pattern include setting up multiple Apache/NGINX web\n\nservers to listen on different physical or virtual interfaces; employing multiple virtual roots on Windows IIS servers bound to different ports; using different directories for every version of the system, with a symbolic link determining which one is live (e.g., as Capistrano does for Ruby on Rails); running multiple versions of services or middleware concurrently, with each listening on different ports; using two different data centers and switching traffic between the data centers, instead of using them merely as hot- or warm-spares for disaster recovery purposes (incidentally, by routinely using both environments, we are continually ensuring that our disaster recovery process works as designed); or using different availability zones in the cloud.\n\n†† This pattern is also commonly referred to as the expand/contract pattern, which Timothy Fitz described when he said, “We do not change (mutate) database objects, such as columns or tables. Instead, we first expand, by adding new objects, then, later, contract by removing the old ones.” Furthermore, increasingly, there are technologies that enable virtualization, versioning, labeling, and roll back of databases, such as Redgate, Delphix, DBMaestro, and Datical, as well as open source tools, such as DBDeploy, that make database changes dramatically safer and faster.\n\n‡‡ Note that canary releases require having multiple versions of our software running in production\n\nsimultaneously. However, because each additional version we have in production creates additional complexity to manage, we should keep the number of versions to a minimum. This may require the use of the expand/contract database pattern described earlier.\n\n§§ The cluster immune system was first documented by Eric Ries while working at IMVU. This functionality is\n\nalso supported by Etsy in their Feature API library, as well as by Netflix.\n\n¶¶ One sophisticated example of such a service is Facebook’s Gatekeeper, an internally developed service that dynamically selects which features are visible to specific users based on demographic information such as location, browser type, and user profile data (age, gender, etc.). For instance, a particular feature could be configured so that it is only accessible by internal employees, 10% of their user base, or only users between the\n\nages of twenty-five and thirty-five. Other examples include the Etsy Feature API and the Netflix Archaius library.\n\n*** Similarly, as Chuck Rossi, Director of Release Engineering at Facebook, described, “All the code supporting\n\nevery feature we’re planning to launch over the next six months has already been deployed onto our production servers. All we need to do is turn it on.”\n\n††† By 2015, Facebook had over one billion active users, growing 17% over the previous year.\n\n‡‡‡ This problem has a worst-case computational characteristic of O(n3). In other words, the compute time increases exponentially as the function of the number of online users, the size of their friend lists, and the frequency of online/offline state change.\n\n13Architect for Low-Risk Releases\n\nAlmost every well-known DevOps exemplar has had near-death\n\nexperiences due to architectural problems, such as in the stories\n\npresented about LinkedIn, Google, eBay, Amazon, and Etsy. In\n\neach case, they were able to successfully migrate to a more\n\nsuitable architecture that addressed their current problems and organizational needs.\n\nThis is the principle of evolutionary architecture—Jez Humble\n\nobserves that architecture of “any successful product or organization will necessarily evolve over its life cycle.” Before his\n\ntenure at Google, Randy Shoup served as chief engineer and distinguished architect at eBay from 2004 to 2011. He observes\n\nthat “both eBay and Google are each on their fifth entire rewrite of\n\ntheir architecture from top to bottom.”\n\nHe reflects, “Looking back with 20/20 hindsight, some technology [and architectural choices] look prescient and others look shortsighted. Each decision most likely best served the\n\norganizational goals at the time. If we had tried to implement the 1995 equivalent of micro-services out of the gate, we would have likely failed, collapsing under our own weight and probably taking the entire company with us.”†\n\nThe challenge is how to keep migrating from the architecture we\n\nhave to the architecture we need. In the case of eBay, when they\n\nneeded to re-architect, they would first do a small pilot project to\n\nprove to themselves that they understood the problem well\n\nenough to even undertake the effort. For instance, when Shoup’s\n\nteam was planning on moving certain portions of the site to full-\n\nstack Java in 2006, they looked for the area that would get them\n\nthe biggest bang for their buck by sorting the site pages by revenue\n\nproduced. They chose the highest revenue areas, stopping when\n\nthere was not enough of a business return to justify the effort.\n\nWhat Shoup’s team did at eBay is a textbook example of evolutionary design, using a technique called the strangler application pattern—instead of “ripping out and replacing” old\n\nservices with architectures that no longer support our organizational goals, we put the existing functionality behind an API and avoid making further changes to it. All new functionality is then implemented in the new services that use the new desired architecture, making calls to the old system when necessary.\n\nThe strangler application pattern is especially useful for helping\n\nmigrate portions of a monolithic application or tightly-coupled services to one that is more loosely-coupled. All too often, we find\n\nourselves working within an architecture that has become too tightly-coupled and too interconnected, often having been created years (or decades) ago.\n\nThe consequences of overly tight architectures are easy to spot: every time we attempt to commit code in to trunk or release code in to production, we risk creating global failures (e.g., we break\n\neveryone else’s tests and functionality, or the entire site goes down). To avoid this, every small change requires enormous\n\namounts of communication and coordination over days or weeks,\n\nas well as approvals from any group that could potentially be\n\naffected. Deployments become problematic as well—the number\n\nof changes that are batched together for each deployment grows,\n\nfurther complicating the integration and test effort, and increasing\n\nthe already high likelihood of something going wrong.\n\nEven deploying small changes may require coordinating with\n\nhundreds (or even thousands) of other developers, with any one of\n\nthem able to create a catastrophic failure, potentially requiring\n\nweeks to find and fix the problem. (This results in another\n\nsymptom: “My developers spend only 15% of their time coding— the rest of their time is spent in meetings.”)\n\nThese all contribute to an extremely unsafe system of work, where small changes have seemingly unknowable and catastrophic consequences. It also often contributes to a fear of integrating and\n\ndeploying our code, and the self-reinforcing downward spiral of deploying less frequently.\n\nFrom an enterprise architecture perspective, this downward spiral is the consequence of the Second Law of Architectural Thermodynamics, especially in large, complex organizations.\n\nCharles Betz, author of Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler’s Children, observes, “[IT project owners]\n\nare not held accountable for their contributions to overall system entropy.” In other words, reducing our overall complexity and increasing the productivity of all our development teams is rarely the goal of an individual project.",
      "page_number": 264
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 272-279)",
      "start_page": 272,
      "end_page": 279,
      "detection_method": "topic_boundary",
      "content": "In this chapter, we will describe steps we can take to reverse the downward spiral, review the major architectural archetypes,\n\nexamine the attributes of architectures that enable developer productivity, testability, deployability, and safety, as well as\n\nevaluate strategies that allow us to safely migrate from whatever\n\ncurrent architecture we have to one that better enables the achievement of our organizational goals.\n\nAN ARCHITECTURE THAT ENABLES PRODUCTIVITY, TESTABILITY, AND SAFETY\n\nIn contrast to a tightly-coupled architecture that can impede\n\neveryone’s productivity and ability to safely make changes, a loosely-coupled architecture with well-defined interfaces that\n\nenforce how modules connect with each other promotes productivity and safety. It enables small, productive, two-pizza\n\nteams that are able to make small changes that can be safely and\n\nindependently deployed. And because each service also has a well- defined API, it enables easier testing of services and the creation\n\nof contracts and SLAs between teams.\n\nFigure 22: Google cloud datastore (Source: Shoup, “From the Monolith to Micro-services.”)\n\nAs Randy Shoup describes, “This type of architecture has served Google extremely well—for a service like Gmail, there’s five or six\n\nother layers of services underneath it, each very focused on a very specific function. Each service is supported by a small team, who\n\nbuilds it and runs their functionality, with each group potentially\n\nmaking different technology choices. Another example is the Google Cloud Datastore service, which is one of the largest NoSQL\n\nservices in the world—and yet it is supported by a team of only about eight people, largely because it is based on layers upon\n\nlayers of dependable services built upon each other.”\n\nThis kind of service-oriented architecture allows small teams to work on smaller and simpler units of development that each team\n\ncan deploy independently, quickly, and safely. Shoup notes,\n\n“Organizations with these types of architectures, such as Google and Amazon, show how it can impact organizational structures,\n\n[creating] flexibility and scalability. These are both organizations\n\nwith tens of thousands of developers, where small teams can still\n\nbe incredibly productive.”\n\nARCHITECTURAL ARCHETYPES: MONOLITHS VS. MICROSERVICES\n\nAt some point in their history, most DevOps organizations were hobbled by tightly-coupled, monolithic architectures that—while\n\nextremely successful at helping them achieve product/market fit— put them at risk of organizational failure once they had to operate\n\nat scale (e.g., eBay’s monolithic C++ application in 2001, Amazon’s monolithic OBIDOS application in 2001, Twitter’s\n\nmonolithic Rails front-end in 2009, and LinkedIn’s monolithic\n\nLeo application in 2011). In each of these cases, they were able to re-architect their systems and set the stage not only to survive, but\n\nalso to thrive and win in the marketplace.\n\nMonolithic architectures are not inherently bad—in fact, they are often the best choice for an organization early in a product life\n\ncycle. As Randy Shoup observes, “There is no one perfect architecture for all products and all scales. Any architecture meets\n\na particular set of goals or range of requirements and constraints,\n\nsuch as time to market, ease of developing functionality, scaling, etc. The functionality of any product or service will almost\n\ncertainly evolve over time—it should not be surprising that our\n\narchitectural needs will change as well. What works at scale 1x rarely works at scale 10x or 100x.”\n\nTable 3: Architectural archetypes\n\n(Source: Shoup, “From the Monolith to Micro-services.”)\n\nThe major architectural archetypes are shown in table 3, each row\n\nindicates a different evolutionary need for an organization, with each column giving the pros and cons of each of the different archetypes. As the table shows, a monolithic architecture that\n\nsupports a startup (e.g., rapid prototyping of new features, and potential pivots or large changes in strategies) is very different\n\nfrom an architecture that needs hundreds of teams of developers, each of whom must be able to independently deliver value to the\n\ncustomer. By supporting evolutionary architectures, we can ensure that our architecture always serves the current needs of the organization.\n\nCase Study Evolutionary Architecture at Amazon (2002)\n\nOne of the most studied architecture transformations occurred at Amazon. In an interview with ACM Turing\n\nAward-winner and Microsoft Technical Fellow Jim Gray, Amazon CTO Werner Vogels explains that Amazon.com started in 1996 as a “monolithic application, running on a\n\nweb server, talking to a database on the back end. This application, dubbed Obidos, evolved to hold all the business logic, all the display logic, and all the functionality that Amazon eventually became famous for: similarities,\n\nrecommendations, Listmania, reviews, etc.”\n\nAs time went by, Obidos grew too tangled, with complex sharing relationships meaning individual pieces could not be\n\nscaled as needed. Vogels tells Gray that this meant “many things that you would like to see happening in a good software environment couldn’t be done anymore; there were\n\nmany complex pieces of software combined into a single system. It couldn’t evolve anymore.”\n\nDescribing the thought process behind the new desired\n\narchitecture, he tells Gray, “We went through a period of serious introspection and concluded that a service-oriented\n\narchitecture would give us the level of isolation that would allow us to build many software components rapidly and\n\nindependently.”\n\nVogels notes, “The big architectural change that Amazon went through in the past five years [from 2001–2005] was to\n\nmove from a two-tier monolith to a fully-distributed, decentralized, services platform serving many different applications. A lot of innovation was necessary to make this happen, as we were one of the first to take this approach.”\n\nThe lessons from Vogel’s experience at Amazon that are important to our understanding of architecture shifts include the following:\n\nLesson 1: When applied rigorously, strict service orientation is an excellent technique to achieve isolation; you achieve a level of ownership and control that was not\n\nseen before.\n\nLesson 2: Prohibiting direct database access by clients makes performing scaling and reliability improvements to\n\nyour service state possible without involving your clients.\n\nLesson 3: Development and operational process greatly\n\nbenefits from switching to service-orientation. The services model has been a key enabler in creating teams that can innovate quickly with a strong customer focus. Each service has a team associated with it, and that team is completely\n\nresponsible for the service—from scoping out the functionality to architecting, building, and operating it.\n\nThe extent to which applying these lessons enhances developer productivity and reliability is breathtaking. In 2011, Amazon was performing approximately fifteen thousands\n\ndeployments per day. By 2015, they were performing nearly 136,000 deployments per day.\n\nUSE THE STRANGLER APPLICATION PATTERN TO SAFELY EVOLVE OUR ENTERPRISE ARCHITECTURE\n\nThe term strangler application was coined by Martin Fowler in 2004 after he was inspired by seeing massive strangler vines\n\nduring a trip to Australia, writing, “They seed in the upper branches of a fig tree and gradually work their way down the tree until they root in the soil. Over many years they grow into fantastic and beautiful shapes, meanwhile strangling and killing\n\nthe tree that was their host.”\n\nIf we have determined that our current architecture is too tightly- coupled, we can start safely decoupling parts of the functionality\n\nfrom our existing architecture. By doing this, we enable teams supporting the decoupled functionality to independently develop, test, and deploy their code into production with autonomy and\n\nsafety, and reduce architectural entropy.\n\nAs described earlier, the strangler application pattern involves placing existing functionality behind an API, where it remains\n\nunchanged, and implementing new functionality using our desired architecture, making calls to the old system when necessary. When we implement strangler applications, we seek to access all\n\nservices through versioned APIs, also called versioned services or immutable services.\n\nVersioned APIs enable us to modify the service without impacting the callers, which allows the system to be more loosely-coupled—if we need to modify the arguments, we create a new API version and migrate teams who depend on our service to the new version.\n\nAfter all, we are not achieving our re-architecting goals if we allow our new strangler application to get tightly-coupled into other services (e.g., connecting directly to another service’s database).\n\nIf the services we call do not have cleanly-defined APIs, we should build them or at least hide the complexity of communicating with such systems within a client library that has a cleanly defined API.\n\nBy repeatedly decoupling functionality from our existing tightly- coupled system, we move our work into a safe and vibrant ecosystem where developers can be far more productive resulting\n\nin the legacy application shrinking in functionality. It might even disappear entirely as all the needed functionality migrates to our new architecture.\n\nBy creating strangler applications, we avoid merely reproducing existing functionality in some new architecture or technology— often, our business processes are far more complex than necessary\n\ndue to the idiosyncrasies of the existing systems, which we will end up replicating. (By researching the user, we can often re- engineer the process so that we can design a far simpler and more streamlined means to achieving the business goal.)‡\n\nAn observation from Martin Fowler underscores this risk: “Much of my career has involved rewrites of critical systems. You would",
      "page_number": 272
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 280-289)",
      "start_page": 280,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": "think such a thing is easy—just make the new one do what the old one did. Yet they are always much more complex than they seem, and overflowing with risk. The big cut-over date looms, and the\n\npressure is on. While new features (there are always new features) are liked, old stuff has to remain. Even old bugs often need to be added to the rewritten system.”\n\nAs with any transformation, we seek to create quick wins and deliver early incremental value before continuing to iterate. Up- front analysis helps us identify the smallest possible piece of work\n\nthat will usefully achieve a business outcome using the new architecture.\n\nCase Study Strangler Pattern at Blackboard Learn (2011)\n\nBlackboard Inc. is one of the pioneers of providing technology for educational institutions, with annual revenue of approximately $650 million in 2011. At that time, the\n\ndevelopment team for their flagship Learn product, packaged software that was installed and run on-premise at their customer sites, was living with the daily consequences of a legacy J2EE codebase that went back to 1997. As David Ashman, their chief architect, observes, “we still have\n\nfragments of Perl code still embedded throughout our\n\ncodebase.”\n\nIn 2010, Ashman was focused on the complexity and\n\ngrowing lead times associated with the old system,\n\nobserving that “our build, integration, and testing processes kept getting more and more complex and error prone. And\n\nthe larger the product got, the longer our lead times and the\n\nworse the outcomes for our customers. To even get\n\nfeedback from our integration process would require twenty- four to thirty-six hours.”\n\nFigure 23: Blackboard Learn code repository: before Building Blocks (Source: “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4.)\n\nHow this started to impact developer productivity was made\n\nvisible to Ashman in graphs generated from their source code repository going all the way back to 2005.\n\nIn figure 24, the top graph represents the number of lines of\n\ncode in the monolithic Blackboard Learn code repository; the bottom graph represents the number of code commits. The\n\nproblem that became evident to Ashman was that the\n\nnumber of code commits started to decrease, objectively showing the increasing difficulty of introducing code\n\nchanges, while the number of lines of code continued to\n\nincrease. Ashman noted, “To me, it said we needed to do\n\nsomething, otherwise the problems would keep getting\n\nworse, with no end in sight.”\n\nAs a result, in 2012 Ashman focused on implementing a\n\ncode re-architecturing project that used the strangler pattern.\n\nThe team accomplished this by creating what they internally called Building Blocks, which allowed developers to work in\n\nseparate modules that were decoupled from the monolithic\n\ncodebase and accessed through fixed APIs. This enabled them to work with far more autonomy, without having to\n\nconstantly communicate and coordinate with other development teams.\n\nFigure 24: Blackboard Learn code repository: after Building Blocks (Source: “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds.” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4.)\n\nWhen Building Blocks were made available to developers, the size of the monolith source code repository began to\n\ndecrease (as measured by number of lines of code).\n\nAshman explained that this was because developers were\n\nmoving their code into the Building Block modules source\n\ncode repository. “In fact,” Ashman reported, “every\n\ndeveloper given a choice would work in the Building Block codebase, where they could work with more autonomy and\n\nfreedom and safety.”\n\nThe graph above shows the connection between the exponential growth in the number of lines of code and the\n\nexponential growth of the number of code commits for the\n\nBuilding Blocks code repositories. The new Building Blocks codebase allowed developers to be more productive, and\n\nthey made the work safer because mistakes resulted in small, local failures instead of major catastrophes that\n\nimpacted the global system.\n\nAshman concluded, “Having developers work in the Building Blocks architecture made for impressive improvements in\n\ncode modularity, allowing them to work with more\n\nindependence and freedom. In combination with the updates to our build process, they also got faster, better feedback on\n\ntheir work, which meant better quality. ”\n\nCONCLUSION\n\nTo a large extent, the architecture that our services operate within dictates how we test and deploy our code. This was validated in\n\nPuppet Labs’ 2015 State of DevOps Report, showing that\n\narchitecture is one of the top predictors of the productivity of the engineers that work within it and of how changes can be quickly\n\nand safely made.\n\nBecause we are often stuck with architectures that were optimized\n\nfor a different set of organizational goals, or for an era long-\n\npassed, we must be able to safely migrate from one architecture to another. The case studies presented in this chapter, as well as the\n\nAmazon case study previously presented, describe techniques like the strangler pattern that can help us migrate between\n\narchitectures incrementally, enabling us to adapt to the needs of\n\nthe organization.\n\nPART III CONCLUSION\n\nWithin the previous chapters of Part III, we have implemented the\n\narchitecture and technical practices that enable the fast flow of work from Dev to Ops, so that value can be quickly and safely\n\ndelivered to customers.\n\nIn Part IV: The Second Way, The Technical Practices of Feedback, we will create the architecture and mechanisms to enable the\n\nreciprocal fast flow of feedback from right to left, to find and fix\n\nproblems faster, radiate feedback, and ensure better outcomes from our work. This enables our organization to further increase\n\nthe rate at which it can adapt.\n\n† eBay’s architecture went through the following phases: Perl and files (v1, 1995), C++ and Oracle (v2, 1997), XSL\n\nand Java (v3, 2002), full-stack Java (v4, 2007), Polyglot microservices (2013+).\n\n‡ The strangler application pattern involves incrementally replacing a whole system, usually a legacy system, with a completely new one. Conversely, branching by abstraction, a term coined by Paul Hammant, is a technique where we create an abstraction layer between the areas that we are changing. This enables evolutionary design of the application architecture while allowing everybody to work off trunk/master and practice continuous integration.\n\nPart\n\nIntroduction\n\nIn Part III, we described the architecture and technical practices\n\nrequired to create fast flow from Development into Operations.\n\nNow in Part IV, we describe how to implement the technical\n\npractices of the Second Way, which are required to create fast and continuous feedback from Operations to Development.\n\nBy doing this, we shorten and amplify feedback loops so that we\n\ncan see problems as they occur and radiate this information to everyone in the value stream. This allows us to quickly find and fix\n\nproblems earlier in the software development life cycle, ideally\n\nlong before they cause a catastrophic failure.\n\nFurthermore, we will create a system of work where knowledge\n\nacquired downstream in Operations is integrated into the upstream work of Development and Product Management. This allows us to quickly create improvements and learnings, whether\n\nit’s from a production issue, a deployment issue, early indicators of problems, or our customer usage patterns.\n\nAdditionally, we will create a process that allows everyone to get feedback on their work, makes information visible to enable\n\nlearning, and enables us to rapidly test product hypotheses,\n\nhelping us determine if the features we are building are helping us\n\nachieve our organizational goals.\n\nWe will also demonstrate how to create telemetry from our build,\n\ntest, and deploy processes, as well as from user behavior,\n\nproduction issues and outages, audit issues, and security breaches.\n\nBy amplifying signals as part of our daily work, we make it\n\npossible to see and solve problems as they occur, and we grow safe\n\nsystems of work that allow us to confidently make changes and\n\nrun product experiments, knowing we can quickly detect and\n\nremediate failures. We will do all of this by exploring the\n\nfollowing:\n\nCreating telemetry to enable seeing and solving problems\n\nUsing our telemetry to better anticipate problems and achieve goals\n\nIntegrating user research and feedback into the work of product teams\n\nEnabling feedback so Dev and Ops can safely perform\n\ndeployments\n\nEnabling feedback to increase the quality of our work through\n\npeer reviews and pair programming\n\nThe patterns in this chapter help reinforce the common goals of Product Management, Development, QA, Operations, and Infosec, and encourage them to share in the responsibility of ensuring that services run smoothly in production and collaborate on the improvement of the system as a whole. Where possible, we want to link cause to effect. The more assumptions we can invalidate,\n\nthe faster we can discover and fix problems, but also the greater\n\nour ability to learn and innovate.\n\nThroughout the following chapters, we will implement feedback\n\nloops, enabling everyone to work together toward shared goals, to\n\nsee problems as they occur, enable quick detection and recovery,\n\nand ensure that features not only operate as designed in\n\nproduction, but also achieve organizational goals and support\n\norganizational learning.\n\n14Create Telemetry to Enable Seeing and Solving Problems\n\nA fact of life in Operations is that things go wrong—small changes\n\nmay result in many unexpected outcomes, including outages and\n\nglobal failures that impact all our customers. This is the reality of\n\noperating complex systems; no single person can see the whole system and understand how all the pieces fit together.\n\nWhen production outages and other problems occur in our daily work, we don’t often have the information we need to solve the\n\nproblem. For example, during an outage we may not be able to\n\ndetermine whether the issue is due to a failure in our application (e.g., defect in the code), in our environment (e.g., a networking\n\nproblem, server configuration problem), or something entirely external to us (e.g., a massive denial of service attack).\n\nIn Operations, we may deal with this problem with the following rule of thumb: When something goes wrong in production, we just\n\nreboot the server. If that doesn’t work, reboot the server next to it. If that doesn’t work, reboot all the servers. If that doesn’t work, blame the developers, they’re always causing outages.\n\nIn contrast, the Microsoft Operations Framework (MOF) study in 2001 found that organizations with the highest service levels",
      "page_number": 280
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 290-298)",
      "start_page": 290,
      "end_page": 298,
      "detection_method": "topic_boundary",
      "content": "rebooted their servers twenty times less frequently than average\n\nand had five times fewer “blue screens of death.” In other words,\n\nthey found that the best-performing organizations were much\n\nbetter at diagnosing and fixing service incidents, in what Kevin\n\nBehr, Gene Kim, and George Spafford called a “culture of\n\ncausality” in The Visible Ops Handbook. High performers used a\n\ndisciplined approach to solving problems, using production\n\ntelemetry to understand possible contributing factors to focus\n\ntheir problem solving, as opposed to lower performers who would\n\nblindly reboot servers.\n\nTo enable this disciplined problem-solving behavior, we need to design our systems so that they are continually creating telemetry, widely defined as “an automated communications process by\n\nwhich measurements and other data are collected at remote points and are subsequently transmitted to receiving equipment for monitoring.” Our goal is to create telemetry within our applications and environments, both in our production and pre- production environments as well as in our deployment pipeline.\n\nMichael Rembetsy and Patrick McDonnell described how\n\nproduction monitoring was a critical part of Etsy’s DevOps transformation that started in 2009. This was because they were\n\nstandardizing and transitioning their entire technology stack to the LAMP stack (Linux, Apache, MySQL, and PHP), abandoning a myriad of different technologies being used in production that were increasingly difficult to support.\n\nAt the 2012 Velocity Conference, McDonnell described how much risk this created, “We were changing some of our most critical\n\ninfrastructure, which, ideally, customers would never notice. However, they’d definitely notice if we screwed something up. We\n\nneeded more metrics to give us confidence that we weren’t\n\nactually breaking things while we were doing these big changes,\n\nboth for our engineering teams and for team members in the non-\n\ntechnical areas, such as marketing.”\n\nMcDonnell explained further, “We started collecting all our server\n\ninformation in a tool called Ganglia, displaying all the information\n\ninto Graphite, an open source tool we invested heavily into. We\n\nstarted aggregating metrics together, everything from business\n\nmetrics to deployments. This is when we modified Graphite with\n\nwhat we called ‘our unparalleled and unmatched vertical line\n\ntechnology’ that overlaid onto every metric graph when deployments happened. By doing this, we could more quickly see any unintended deployment side effects. We even started putting\n\nTV screens all around the office so that everyone could see how our services were performing.”\n\nBy enabling developers to add telemetry to their features as part of their daily work, they created enough telemetry to help make deployments safe. By 2011, Etsy was tracking over two hundred thousand production metrics at every layer of the application stack (e.g., application features, application health, database, operating system, storage, networking, security, etc.) with the top\n\nthirty most important business metrics prominently displayed on their “deploy dashboard.” By 2014, they were tracking over eight hundred thousand metrics, showing their relentless goal of instrumenting everything and making it easy for engineers to do so.\n\nAs Ian Malpass, an engineer at Etsy, quipped, “If Engineering at\n\nEtsy has a religion, it’s the Church of Graphs. If it moves, we track it. Sometimes we’ll draw a graph of something that isn’t moving\n\nyet, just in case it decides to make a run for it….Tracking everything is key to moving fast, but the only way to do it is to\n\nmake tracking anything easy....We enable engineers to track what they need to track, at the drop of a hat, without requiring time-\n\nsucking configuration changes or complicated processes.”\n\nOne of the findings of the 2015 State of DevOps Report was that\n\nhigh performers could resolve production incidents 168 times faster than their peers, with the median high performer having a\n\nMTTR measured in minutes, while the median low performer had an MTTR measured in days. The top two technical practices that\n\nenabled fast MTTR were the use of version control by Operations and having telemetry and proactive monitoring in the production\n\nenvironment.\n\nFigure 25: Incident resolution time for high, medium, and low performers (Source: Puppet Labs, 2014 State of DevOps Report.)\n\nAs was created at Etsy, our goal in this chapter is to ensure that we always have enough telemetry so that we can confirm that our\n\nservices are correctly operating in production. And when problems do occur, make it possible to quickly determine what is going\n\nwrong and make informed decisions on how best to fix it, ideally\n\nlong before customers are impacted. Furthermore, telemetry is what enables us to assemble our best understanding of reality and\n\ndetect when our understanding of reality is incorrect.\n\nCREATE OUR CENTRALIZED TELEMETRY INFRASTRUCTURE\n\nOperational monitoring and logging is by no means new—multiple\n\ngenerations of Operations engineers have used and customized monitoring frameworks (e.g., HP OpenView, IBM Tivoli, and BMC\n\nPatrol/BladeLogic) to ensure the health of production systems.\n\nData was typically collected through agents that ran on servers or through agent-less monitoring (e.g., SNMP traps or polling based\n\nmonitors). There was often a graphical user interface (GUI) front end, and back-end reporting was often augmented through tools\n\nsuch as Crystal Reports.\n\nSimilarly, the practices of developing applications with effective logging and managing the resulting telemetry are not new—a\n\nvariety of mature logging libraries exist for almost all\n\nprogramming languages.\n\nHowever, for decades we have ended up with silos of information, where Development only creates logging events that are\n\ninteresting to developers, and Operations only monitors whether the environments are up or down. As a result, when inopportune\n\nevents occur, no one can determine why the entire system is not operating as designed or which specific component is failing,\n\nimpeding our ability to bring our system back to a working state.\n\nIn order for us to see all problems as they occur, we must design\n\nand develop our applications and environments so that they\n\ngenerate sufficient telemetry, allowing us to understand how our system is behaving as a whole. When all levels of our application stack have monitoring and logging, we enable other important\n\ncapabilities, such as graphing and visualizing our metrics, anomaly detection, proactive alerting and escalation, etc.\n\nIn The Art of Monitoring, James Turnbull describes a modern monitoring architecture, which has been developed and used by Operations engineers at web-scale companies (e.g., Google,\n\nAmazon, Facebook). The architecture often consisted of open source tools, such as Nagios and Zenoss, that were customized and deployed at a scale that was difficult to accomplish with licensed\n\ncommercial software at the time. This architecture has the following components:\n\nData collection at the business logic, application, and\n\nenvironments layer: In each of these layers, we are creating telemetry in the form of events, logs, and metrics. Logs may be stored in application-specific files on each server (e.g.,\n\n/var/log/httpd-error.log), but preferably we want all our logs sent to a common service that enables easy centralization, rotation, and deletion. This is provided by most operating systems, such as syslog for Linux, the Event Log for Windows,\n\netc. Furthermore, we gather metrics at all layers of the application stack to better understand how our system is behaving. At the operating system level, we can collect metrics\n\nsuch as CPU, memory, disk, or network usage over time using tools like collectd, Ganglia, etc. Other tools that collect performance information include AppDynamics, New Relic, and Pingdom.\n\nAn event router responsible for storing our events and metrics: This capability potentially enables visualization, trending, alerting, anomaly detection, and so forth. By\n\ncollecting, storing, and aggregating all our telemetry, we better\n\nenable further analysis and health checks. This is also where we store configurations related to our services (and their\n\nsupporting applications and environments) and is likely where we do threshold-based alerting and health checks.†\n\nOnce we have centralized our logs, we can transform them into\n\nmetrics by counting them in the event router—for example, a log event such as “child pid 14024 exit signal Segmentation fault” can be counted and summarized as a single segfault metric across our\n\nentire production infrastructure.\n\nBy transforming logs into metrics, we can now perform statistical operations on them, such as using anomaly detection to find\n\noutliers and variances even earlier in the problem cycle. For instance, we might configure our alerting to notify us if we went from “ten segfaults last week” to “thousands of segfaults in the last hour,” prompting us to investigate further.\n\nIn addition to collecting telemetry from our production services and environments, we must also collect telemetry from our\n\ndeployment pipeline when important events occur, such as when our automated tests pass or fail and when we perform deployments to any environment. We should also collect telemetry on how long it takes us to execute our builds and tests. By doing\n\nthis, we can detect conditions that could indicate problems, such as if the performance test or our build takes twice as long as normal, allowing us to find and fix errors before they go into\n\nproduction.\n\nFigure 26: Monitoring framework (Source: Turnbull, The Art of Monitoring, Kindle edition, chap. 2.)\n\nFurthermore, we should ensure that it is easy to enter and retrieve information from our telemetry infrastructure. Preferably, everything should be done through self-service APIs, as opposed\n\nto requiring people to open up tickets and wait to get reports.\n\nIdeally, we will create telemetry that tells us exactly when anything of interest happens, as well as where and how. Our\n\ntelemetry should also be suitable for manual and automated analysis and should be able to be analyzed without having the application that produced the logs on hand. As Adrian Cockcroft\n\npointed out, “Monitoring is so important that our monitoring systems need to be more available and scalable than the systems being monitored.”\n\nFrom here on, the term telemetry will be used interchangeably with metrics, which includes all event logging and metrics created\n\nby our services at all levels of our application stack and generated from all our production and pre-production environments, as well as from our deployment pipeline.\n\nCREATE APPLICATION LOGGING TELEMETRY THAT HELPS PRODUCTION\n\nNow that we have a centralized telemetry infrastructure, we must ensure that the applications we build and operate are creating sufficient telemetry. We do this by having Dev and Ops engineers create production telemetry as part of their daily work, both for\n\nnew and existing services.\n\nScott Prugh, Chief Architect and Vice President of Development at\n\nCSG, said, “Every time NASA launches a rocket, it has millions of automated sensors reporting the status of every component of this valuable asset. And yet, we often don’t take the same care with software—we found that creating application and infrastructure\n\ntelemetry to be one of the highest return investments we’ve made. In 2014, we created over one billion telemetry events per day, with over one hundred thousand code locations instrumented.”\n\nIn the applications we create and operate, every feature should be instrumented—if it was important enough for an engineer to implement, it is certainly important enough to generate enough",
      "page_number": 290
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 299-306)",
      "start_page": 299,
      "end_page": 306,
      "detection_method": "topic_boundary",
      "content": "production telemetry so that we can confirm that it is operating as designed and that the desired outcomes are being achieved.‡\n\nEvery member of our value stream will use telemetry in a variety of ways. For example, developers may temporarily create more telemetry in their application to better diagnose problems on their workstation, while Ops engineers may use telemetry to diagnose a\n\nproduction problem. In addition, Infosec and auditors may review the telemetry to confirm the effectiveness of a required control, and a product manager may use them to track business outcomes,\n\nfeature usage, or conversion rates.\n\nTo support these various usage models, we have different logging levels, some of which may also trigger alerts, such as the following:\n\nDEBUG level: Information at this level is about anything that happens in the program, most often used during debugging. Often, debug logs are disabled in production but temporarily\n\nenabled during troubleshooting.\n\nINFO level: Information at this level consists of actions that\n\nare user-driven or system specific (e.g., “beginning credit card transaction”).\n\nWARN level: Information at this level tells us of conditions\n\nthat could potentially become an error (e.g., a database call taking longer than some predefined time). These will likely initiate an alert and troubleshooting, while other logging messages may help us better understand what led to this\n\ncondition.\n\nERROR level: Information at this level focuses on error conditions (e.g., API call failures, internal error conditions).\n\nFATAL level: Information at this level tells us when we must terminate (e.g., a network daemon can’t bind a network socket).\n\nChoosing the right logging level is important. Dan North, a former ThoughtWorks consultant who was involved in several projects in which the core continuous delivery concepts took shape, observes,\n\n“When deciding whether a message should be ERROR or WARN, imagine being woken up at 4 a.m. Low printer toner is not an ERROR.”\n\nTo help ensure that we have information relevant to the reliable and secure operations of our service, we should ensure that all potentially significant application events generate logging entries,\n\nincluding those provided on this list assembled by Anton A. Chuvakin, a research VP at Gartner’s GTP Security and Risk Management group:\n\nAuthentication/authorization decisions (including logoff)\n\nSystem and data access\n\nSystem and application changes (especially privileged changes)\n\nData changes, such as adding, editing, or deleting data\n\nInvalid input (possible malicious injection, threats, etc.)\n\nResources (RAM, disk, CPU, bandwidth, or any other resource\n\nthat has hard or soft limits)\n\nHealth and availability\n\nStartups and shutdowns\n\nFaults and errors\n\nCircuit breaker trips\n\nDelays\n\nBackup success/failure\n\nTo make it easier to interpret and give meaning to all these log\n\nentries, we should (ideally) create logging hierarchical categories,\n\nsuch as for non-functional attributes (e.g., performance, security) and for attributes related to features (e.g., search, ranking).\n\nUSE TELEMETRY TO GUIDE PROBLEM SOLVING\n\nAs described in the beginning of this chapter, high performers use\n\na disciplined approach to solving problems. This is in contrast to the more common practice of using rumor and hearsay, which can\n\nlead to the unfortunate metric of mean time until declared\n\ninnocent—how quickly can we convince everyone else that we didn’t cause the outage.\n\nWhen there is a culture of blame around outages and problems,\n\ngroups may avoid documenting changes and displaying telemetry where everyone can see them to avoid being blamed for outages.\n\nOther negative outcomes due to lack of public telemetry include a\n\nhighly charged political atmosphere, the need to deflect accusations, and, worse, the inability to create institutional\n\nknowledge around how the incidents occurred and the learnings\n\nneeded to prevent these errors from happening again in the future.§\n\nIn contrast, telemetry enables us to use the scientific method to formulate hypotheses about what is causing a particular problem\n\nand what is required to solve it. Examples of questions we can\n\nanswer during problem resolution include:\n\nWhat evidence do we have from our monitoring that a problem\n\nis actually occurring?\n\nWhat are the relevant events and changes in our applications and environments that could have contributed to the problem?\n\nWhat hypotheses can we formulate to confirm the link between\n\nthe proposed causes and effects?\n\nHow can we prove which of these hypotheses are correct and successfully effect a fix?\n\nThe value of fact-based problem solving lies not only in\n\nsignificantly faster MTTR (and better customer outcomes), but also in its reinforcement of the perception of a win/win\n\nrelationship between Development and Operations.\n\nENABLE CREATION OF PRODUCTION METRICS AS PART OF DAILY WORK\n\nTo enable everyone to be able to find and fix problems in their\n\ndaily work, we need to enable everyone to create metrics in their\n\ndaily work that can be easily created, displayed, and analyzed. To do this, we must create the infrastructure and libraries necessary\n\nto make it as easy as possible for anyone in Development or\n\nOperations to create telemetry for any functionality they build. In the ideal, it should be as easy as writing one line of code to create a\n\nnew metric that shows up in a common dashboard where everyone\n\nin the value stream can see it.\n\nThis was the philosophy that guided the development of one of the\n\nmost widely used metrics libraries, called StatsD, which was\n\ncreated and open-sourced at Etsy. As John Allspaw described, “We designed StatsD to prevent any developer from saying, ‘It’s too\n\nmuch of a hassle to instrument my code.’ Now they can do it with one line of code. It was important to us that for a developer,\n\nadding production telemetry didn’t feel as difficult as doing a\n\ndatabase schema change.”\n\nStatsD can generate timers and counters with one line of code (in Ruby, Perl, Python, Java, and other languages) and is often used\n\nin conjunction with Graphite or Grafana, which renders metric events into graphs and dashboards.\n\nFigure 27: One line of code to generate telemetry using StatsD and Graphite at Etsy (Source: Ian Malpass, “Measure Anything, Measure Everything.”)\n\nFigure 27 above shows an example of how a single line of code\n\ncreates a user login event (in this case, one line of PHP code: “StatsD::increment(“login.successes”)). The resulting graph shows\n\nthe number of successful and failed logins per minute, and\n\noverlaid on the graph are vertical lines that represent a production deployment.\n\nWhen we generate graphs of our telemetry, we will also overlay\n\nonto them when production changes occur, because we know that the significant majority of production issues are caused by\n\nproduction changes, which include code deployments. This is part\n\nof what allows us to have a high rate of change, while still preserving a safe system of work.\n\nAlternative libraries to StatsD that allow developers to generate\n\nproduction telemetry can be easily aggregated and analyzed include JMX and codahale metrics. Other tools that create metrics\n\ninvaluable for problem solving include New Relic, AppDynamics,\n\nand Dynatrace. Tools such as munin and collectd can be used to create similar functionality.¶\n\nBy generating production telemetry as part of our daily work, we create an ever-improving capability to not only see problems as\n\nthey occur, but also to design our work so that problems in design\n\nand operations can be revealed, allowing an increasing number of metrics to be tracked, as we saw in the Etsy case study.\n\nCREATE SELF-SERVICE ACCESS TO TELEMETRY AND INFORMATION RADIATORS\n\nIn the previous steps, we enabled Development and Operations to\n\ncreate and improve production telemetry as part of their daily work. In this step, our goal is to radiate this information to the rest\n\nof the organization, ensuring that anyone who wants information\n\nabout any of the services we are running can get it without needing production system access or privileged accounts, or\n\nhaving to open up a ticket and wait for days for someone to\n\nconfigure the graph for them.\n\nBy making telemetry fast, easy to get, and sufficiently centralized,\n\neveryone in the value stream can share a common view of reality.\n\nTypically, this means that production metrics will be radiated on web pages generated by a centralized server, such as Graphite or\n\nany of the other technologies described in the previous section.\n\nWe want our production telemetry to be highly visible, which means putting it in central areas where Development and\n\nOperations work, thus allowing everyone who is interested to see\n\nhow our services are performing. At a minimum, this includes\n\neveryone in our value stream, such as Development, Operations, Product Management, and Infosec.\n\nThis is often referred to as an information radiator, defined by\n\nthe Agile Alliance as “the generic term for any of a number of handwritten, drawn, printed, or electronic displays which a team\n\nplaces in a highly visible location, so that all team members as well\n\nas passers-by can see the latest information at a glance: count of automated tests, velocity, incident reports, continuous integration\n\nstatus, and so on. This idea originated as part of the Toyota\n\nProduction System.”\n\nBy putting information radiators in highly visible places, we\n\npromote responsibility among team members, actively\n\ndemonstrating the following values:\n\nThe team has nothing to hide from its visitors (customers,\n\nstakeholders, etc.)\n\nThe team has nothing to hide from itself: it acknowledges and confronts problems\n\nNow that we possess the infrastructure to create and radiate\n\nproduction telemetry to the entire organization, we may also\n\nchoose to broadcast this information to our internal customers and even to our external customers. For example, we might do this\n\nby creating publicly-viewable service status pages so that customers can learn how the services they depend upon are\n\nperforming.\n\nAlthough there may be some resistance to providing this amount of transparency, Ernest Mueller describes the value of doing so:",
      "page_number": 299
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 307-314)",
      "start_page": 307,
      "end_page": 314,
      "detection_method": "topic_boundary",
      "content": "One of the first actions I take when starting in an organization\n\nis to use information radiators to communicate issues and detail the changes we are making—this is usually extremely\n\nwell-received by our business units, who were often left in the\n\ndark before. And for Development and Operations groups who must work together to deliver a service to others, we need that\n\nconstant communication, information, and feedback.\n\nWe may even extend this transparency further—instead of trying to keep customer-impacting problems a secret, we can broadcast\n\nthis information to our external customers. This demonstrates\n\nthat we value transparency, thereby helping to build and earn customers’ trust.** See Appendix 10.\n\nCase Study Creating Self-Service Metrics at LinkedIn (2011)\n\nAs described in Part III, LinkedIn was created in 2003 to help users connect “to your network for better job\n\nopportunities.” By November 2015, LinkedIn had over 350\n\nmillion members generating tens of thousands of requests per second, resulting in millions of queries per second on\n\nthe LinkedIn back-end systems.\n\nPrachi Gupta, Director of Engineering at LinkedIn, wrote in 2011 about the importance of production telemetry: “At\n\nLinkedIn, we emphasize making sure the site is up and our members have access to complete site functionality at all\n\ntimes. Fulfilling this commitment requires that we detect and\n\nrespond to failures and bottlenecks as they start happening.\n\nThat’s why we use these time-series graphs for site\n\nmonitoring to detect and react to incidents within minutes...This monitoring technique has proven to be a\n\ngreat tool for engineers. It lets us move fast and buys us\n\ntime to detect, triage, and fix problems.”\n\nHowever, in 2010, even though there was an incredibly large\n\nvolume of telemetry being generated, it was extremely\n\ndifficult for engineers to get access to the data, let alone analyze it. Thus began Eric Wong’s summer intern project at\n\nLinkedIn, which turned into the production telemetry initiative\n\nthat created InGraphs.\n\nWong wrote, “To get something as simple as CPU usage of\n\nall the hosts running a particular service, you would need to\n\nfile a ticket and someone would spend 30 minutes putting it [a report] together.”\n\nAt the time, LinkedIn was using Zenoss to collect metrics,\n\nbut as Wong explains, “Getting data from Zenoss required digging through a slow web interface, so I wrote some\n\npython scripts to help streamline the process. While there\n\nwas still manual intervention in setting up metric collection, I was able to cut down the time spent navigating Zenoss’\n\ninterface.”\n\nOver the course of the summer, he continued to add functionality to InGraphs so that engineers could see exactly\n\nwhat they wanted to see, adding the ability to make\n\ncalculations across multiple datasets, view week-over-week trending to compare historical performance, and even define\n\ncustom dashboards to pick exactly which metrics would be\n\ndisplayed on a single page.\n\nIn writing about the outcomes of adding functionality to\n\nInGraphs and the value of this capability, Gupta notes, “The\n\neffectiveness of our monitoring system was highlighted in an instant where our InGraphs monitoring functionality tied to a\n\nmajor web-mail provider started trending downwards and the\n\nprovider realized they had a problem in their system only after we reached out to them!”\n\nWhat started off as a summer internship project is now one\n\nof the most visible parts of LinkedIn operations. InGraphs has been so successful that the real-time graphs are\n\nfeatured prominently in the company’s engineering offices\n\nwhere visitors can’t fail to see them.\n\nFIND AND FILL ANY TELEMETRY GAPS\n\nWe have now created the infrastructure necessary to quickly\n\ncreate production telemetry throughout our entire application stack and radiate it throughout our organization.\n\nIn this step, we will identify any gaps in our telemetry that impede\n\nour ability to quickly detect and resolve incidents—this is especially relevant if Dev and Ops currently have little (or no)\n\ntelemetry. We will use this data later to better anticipate problems,\n\nas well as to enable everyone to gather the information they need to make better decisions to achieve organizational goals.\n\nAchieving this requires that we create enough telemetry at all\n\nlevels of the application stack for all our environments, as well as for the deployment pipelines that support them. We need metrics\n\nfrom the following levels:\n\nBusiness level: Examples include the number of sales transactions, revenue of sales transactions, user signups, churn\n\nrate, A/B testing results, etc.\n\nApplication level: Examples include transaction times, user response times, application faults, etc.\n\nInfrastructure level (e.g., database, operating system,\n\nnetworking, storage): Examples include web server traffic, CPU load, disk usage, etc.\n\nClient software level (e.g., JavaScript on the client\n\nbrowser, mobile application): Examples include application errors and crashes, user measured transaction\n\ntimes, etc.\n\nDeployment pipeline level: Examples include build pipeline status (e.g., red or green for our various automated\n\ntest suites), change deployment lead times, deployment\n\nfrequencies, test environment promotions, and environment status.\n\nBy having telemetry coverage in all of these areas, we will be able\n\nto see the health of everything that our service relies upon, using data and facts instead of rumors, finger-pointing, blame, and so\n\nforth.\n\nFurther, we better enable detection of security-relevant events by\n\nmonitoring any application and infrastructure faults (e.g., abnormal program terminations, application errors and\n\nexceptions, and server and storage errors). Not only does this\n\ntelemetry better inform Development and Operations when our services are crashing, but these errors are often indicators that a\n\nsecurity vulnerability is being actively exploited.\n\nBy detecting and correcting problems earlier, we can fix them while they are small and easy to fix, with fewer customers\n\nimpacted. Furthermore, after every production incident, we should identify any missing telemetry that could have enabled\n\nfaster detection and recovery; or, better yet, we can identify these\n\ngaps during feature development in our peer review process.\n\nAPPLICATION AND BUSINESS METRICS\n\nAt the application level, our goal is to ensure that we are generating telemetry not only around application health (e.g.,\n\nmemory usage, transaction counts, etc.), but also to measure to\n\nwhat extent we are achieving our organizational goals (e.g., number of new users, user login events, user session lengths,\n\npercent of users active, how often certain features are being used,\n\nand so forth).\n\nFor example, if we have a service that is supporting e-commerce,\n\nwe want to ensure that we have telemetry around all of the user\n\nevents that lead up to a successful transaction that generates revenue. We can then instrument all the user actions that are\n\nrequired for our desired customer outcomes.\n\nThese metrics will vary according to different domains and\n\norganizational goals. For instance, for e-commerce sites, we may want to maximize the time spent on the site; however, for search\n\nengines, we may want to reduce the time spent on the site, since long sessions may indicate that users are having difficulty finding\n\nwhat they’re looking for.\n\nIn general, business metrics will be part of a customer acquisition\n\nfunnel, which is the theoretical steps a potential customer will take to make a purchase. For instance, in an e-commerce site, the\n\nmeasurable journey events include total time on site, product link clicks, shopping cart adds, and completed orders.\n\nEd Blankenship, Senior Product Manager for Microsoft Visual\n\nStudio Team Services, describes, “Often, feature teams will define their goals in an acquisition funnel, with the goal of their feature\n\nbeing used in every customer’s daily work. Sometimes they’re\n\ninformally described as ‘tire kickers,’ ‘active users,’ ‘engaged\n\nusers,’ and ‘deeply engaged users,’ with telemetry supporting each\n\nstage.”\n\nOur goal is to have every business metric be actionable—these top\n\nmetrics should help inform how to change our product and be\n\namenable to experimentation and A/B testing. When metrics aren’t actionable, they are likely vanity metrics that provide little\n\nuseful information—these we want to store, but likely not display,\n\nlet alone alert on.\n\nIdeally, anyone viewing our information radiators will be able to\n\nmake sense of the information we are showing in the context of\n\ndesired organizational outcomes, such as goals around revenue,\n\nuser attainment, conversion rates, etc. We should define and link\n\neach metric to a business outcome metric at the earliest stages of\n\nfeature definition and development, and measure the outcomes\n\nafter we deploy them in production. Furthermore, doing this helps product owners describe the business context of each feature for\n\neveryone in the value stream.\n\nFigure 28: Amount of user excitement of new features in user forum posts after deployments (Source: Mike Brittain, “Tracking Every Release,” CodeasCraft.com, December 8, 2010, https://codeascraft.com/2010/12/08/track-every-release/.)\n\nFurther business context can be created by being aware of and\n\nvisually displaying time periods relevant to high-level business\n\nplanning and operations, such as high transaction periods associated with peak holiday selling seasons, end-of-quarter\n\nfinancial close periods, or scheduled compliance audits. This\n\ninformation may be used as a reminder to avoid scheduling risky\n\nchanges when availability is critical or avoid certain activities\n\nwhen audits are in progress.\n\nBy radiating how customers interact with what we build in the\n\ncontext of our goals, we enable fast feedback to feature teams so\n\nthey can see whether the capabilities we are building are actually\n\nbeing used and to what extent they are achieving business goals.\n\nAs a result, we reinforce the cultural expectations that\n\ninstrumenting and analyzing customer usage is also a part of our\n\ndaily work, so we better understand how our work contributes to\n\nour organizational goals.\n\nINFRASTRUCTURE METRICS\n\nJust as we did for application metrics, our goal for production and non-production infrastructure is to ensure that we are generating\n\nenough telemetry so that if a problem occurs in any environment,\n\nwe can quickly determine whether infrastructure is a contributing\n\ncause of the problem. Furthermore, we must be able to pinpoint\n\nexactly what in the infrastructure is contributing to the problem (e.g., database, operating system, storage, networking, etc.).\n\nWe want to make as much infrastructure telemetry visible as\n\npossible, across all the technology stakeholders, ideally organized\n\nby service or application. In other words, when something goes wrong with something in our environment, we need to know\n\nexactly what applications and services could be or are being affected.††\n\nIn decades past, creating links between a service and the\n\nproduction infrastructure it depended on was often a manual\n\neffort (such as ITIL CMDBs or creating configuration definitions\n\ninside alerting tools in tools such as Nagios). However,\n\nincreasingly these links are now registered automatically within our services, which are then dynamically discovered and used in\n\nproduction through tools such as Zookeeper, Etcd, Consul, etc.",
      "page_number": 307
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 315-322)",
      "start_page": 315,
      "end_page": 322,
      "detection_method": "topic_boundary",
      "content": "These tools enable services to register themselves, storing\n\ninformation that other services need to interact with it (e.g., IP\n\naddress, port numbers, URIs). This solves the manual nature of the ITIL CMDB and is absolutely necessary when services are\n\nmade up of hundreds (or thousands or even millions) of nodes, each with dynamically assigned IP addresses.‡‡\n\nRegardless of how simple or complex our services are, graphing\n\nour business metrics alongside our application and infrastructure\n\nmetrics allow us to detect when things go wrong. For instance, we\n\nmay see that new customer signups drop to 20% of daily norms,\n\nand then immediately also see that all our database queries are taking five times longer than normal, enabling us to focus our\n\nproblem solving.\n\nFurthermore, business metrics create context for our\n\ninfrastructure metrics, enabling Development and Operations to better work together toward common goals. As Jody Mulkey, CTO\n\nof Ticketmaster/LiveNation, observes, “Instead of measuring\n\nOperations against the amount of downtime, I find it’s much\n\nbetter to measure both Dev and Ops against the real business\n\nconsequences of downtime: how much revenue should we have attained, but didn’t.”§§\n\nNote that in addition to monitoring our production services, we\n\nalso need telemetry for those services in our pre-production environments (e.g., development, test, staging, etc.). Doing this\n\nenables us to find and fix issues before they go into production,\n\nsuch as detecting when we have ever-increasing database insert\n\ntimes due to a missing table index.\n\nOVERLAYING OTHER RELEVANT INFORMATION ONTO OUR METRICS\n\nEven after we have created our deployment pipeline that allows us\n\nto make small and frequent production changes, changes still inherently create risk. Operational side effects are not just\n\noutages, but also significant disruptions and deviations from\n\nstandard operations.\n\nTo make changes visible, we make work visible by overlaying all\n\nproduction deployment activities on our graphs. For instance, for\n\na service that handles a large number of inbound transactions,\n\nproduction changes can result in a significant settling period,\n\nwhere performance degrades substantially as all cache lookups\n\nmiss.\n\nTo better understand and preserve quality of service, we want to\n\nunderstand how quickly performance returns to normal, and if\n\nnecessary, take steps to improve performance.\n\nSimilarly, we want to overlay other useful operational activities,\n\nsuch as when the service is under maintenance or being backed\n\nup, in places where we may want to display or suppress alerts.\n\nCONCLUSION\n\nThe improvements enabled by production telemetry from Etsy\n\nand LinkedIn show us how critical it is to see problems as they occur, so we can search out the cause and quickly remedy the\n\nsituation. By having all elements of our service emitting telemetry\n\nthat can be analyzed, whether it is in our application, database, or\n\nin our environment, and making that telemetry widely available,\n\nwe can find and fix problems long before they cause something\n\ncatastrophic, ideally long before a customer even notices that\n\nsomething is wrong. The result is not only happier customers, but, by reducing the amount of firefighting and crises when things go\n\nwrong, we have a happier and more productive workplace with\n\nless stress and lower levels of burnouts.\n\n† Example tools include Sensu, Nagios, Zappix, LogsStash, Splunk, Sumo Logic, Datadog, and Riemann.\n\n‡ A variety of application logging libraries exist that make it easy for developers to create useful telemetry, and we should choose one that allows us to send all our application logs to the centralized logging infrastructure that we created in the previous section. Popular examples include rrd4j and log4j for Java, and log4r and ruby-cabin for Ruby.\n\n§ In 2004, Gene Kim, Kevin Behr and George Spafford described this as a symptom of lacking a “culture of\n\ncausality,” noting that high-performing organizations recognize that 80% of all outages are caused by change and 80% of MTTR is spent trying to determine what changed.\n\n¶ A whole other set of tools to aid in monitoring, aggregation, and collection include Splunk, Zabbix, Sumo Logic, DataDog, as well as Nagios, Cacti, Sensu, RRDTool, Netflix Atlas, Riemann, and others. Analysts often call this broad category of tools “application performance monitors.”\n\n** Creating a simple dashboard should be part of creating any new product or service—automated tests should\n\nconfirm that both the service and dashboard are working correctly, helping both our customers and our ability to safely deploy code.\n\n†† Exactly as an ITIL Configuration Management Database (CMDB) would prescribe.\n\n‡‡ Consul may be of specific interest, as it creates an abstraction layer that easily enables service mapping, monitoring, locks, and key-value configuration stores, as well as host clustering and failure detection.\n\n§§ This could be the cost of production downtime or the costs associated with a late feature. In product\n\ndevelopment terms, the second metric is known as cost of delay, and is key to making effective prioritization decisions.\n\n15Analyze Telemetry to Better Anticipate Problems and Achieve Goals\n\nAs we saw in the previous chapter, we need sufficient production\n\ntelemetry in our applications and infrastructure to see and solve problems as they occur. In this chapter, we will create tools that\n\nallow us to discover variances and ever-weaker failure signals hidden in our production telemetry so we can avert catastrophic\n\nfailures. Numerous statistical techniques will be presented, along\n\nwith case studies demonstrating their use.\n\nA great example of analyzing telemetry to proactively find and fix\n\nproblems before customers are impacted can be seen at Netflix, a\n\nglobal provider of streaming films and television series. Netflix had revenue of $6.2 billion from seventy-five million subscribers\n\nin 2015. One of their goals is to provide the best experience to those watching videos online around the world, which requires a\n\nrobust, scalable, and resilient delivery infrastructure. Roy Rapoport describes one of the challenges of managing the Netflix cloud-based video delivery service: “Given a herd of cattle that should all look and act the same, which cattle look different from\n\nthe rest? Or more concretely, if we have a thousand-node stateless compute cluster, all running the same software and subject to the\n\nsame approximate traffic load, our challenge is to find any nodes\n\nthat don’t look like the rest of the nodes.”\n\nOne of the statistical techniques that the team used at Netflix in\n\n2012 was outlier detection, defined by Victoria J. Hodge and Jim\n\nAustin of the University of York as detecting “abnormal running\n\nconditions from which significant performance degradation may\n\nwell result, such as an aircraft engine rotation defect or a flow\n\nproblem in a pipeline.”\n\nRapoport explains that Netflix “used outlier detection in a very\n\nsimple way, which was to first compute what was the ‘current\n\nnormal’ right now, given population of nodes in a compute cluster.\n\nAnd then we identified which nodes didn’t fit that pattern, and removed those nodes from production.”\n\nRapoport continues, “We can automatically flag misbehaving\n\nnodes without having to actually define what the ‘proper’ behavior is in any way. And since we’re engineered to run resiliently in the cloud, we don’t tell anyone in Operations to do something— instead, we just kill the sick or misbehaving compute node, and then log it or notify the engineers in whatever form they want.”\n\nBy implementing the Server Outlier Detection process, Rapoport states, Netflix has “massively reduced the effort of finding sick servers, and, more importantly, massively reduced the time\n\nrequire Rapoport states d to fix them, resulting in improved service quality. The benefit of using these techniques to preserve employee sanity, work/life balance, and service quality cannot be overstated.” The work done at Netflix highlights one very specific way we can use telemetry to mitigate problems before they impact our customer.\n\nThroughout this chapter we will explore many statistical and\n\nvisualization techniques (including outlier detection) that we can\n\nuse to analyze our telemetry to better anticipate problems. This\n\nenables us to solve problems faster, cheaper, and earlier than ever,\n\nbefore our customer or anyone in our organization is impacted;\n\nfurthermore, we will also create more context for our data to help\n\nus make better decisions and achieve our organizational goals.\n\nUSE MEANS AND STANDARD DEVIATIONS TO DETECT POTENTIAL PROBLEMS\n\nOne of the simplest statistical techniques that we can use to analyze a production metric is computing its mean (or average) and standard deviations. By doing this, we can create a filter that detects when this metric is significantly different from its norm, and even configure our alerting so that we can take corrective\n\naction (e.g., notify on-call production staff at 2 a.m. to investigate when database queries are significantly slower than average).\n\nWhen critical production services have problems, waking people at 2 a.m. may be the right thing to do. However, when we create alerts that are not actionable or are false-positives, we’ve\n\nunnecessarily woken up people in the middle of the night. As John Vincent, an early leader in the DevOps movement, observed, “Alert fatigue is the single biggest problem we have right now…We need to be more intelligent about our alerts or we’ll all go insane.”\n\nWe create better alerts by increasing the signal-to-noise ratio,\n\nfocusing on the variances or outliers that matter. Suppose we are analyzing the number of unauthorized login attempts per day. Our\n\ncollected data has a Gaussian distribution (i.e., normal or bell\n\ncurve distribution) that matches the graph in the figure 29. The vertical line in the middle of the bell curve is the mean, and the\n\nfirst, second, and third standard deviations indicated by the other vertical lines contain 68%, 95%, and 99.7% of the data,\n\nrespectively.\n\nFigure 29: Standard deviations (σ) & mean (µ) with Gaussian distribution (Source: Wikipedia’s “Normal Distribution” entry, https://en.wikipedia.org/wiki/Normal_distribution.)\n\nA common use of standard deviations is to periodically inspect the\n\ndata set for a metric and alert if it has significantly varied from the mean. For instance, we may set an alert for when the number of\n\nunauthorized login attempts per day is three standard deviations greater than the mean. Provided that this data set has Gaussian\n\ndistribution, we would expect that only 0.3% of the data points\n\nwould trigger the alert.\n\nEven this simple type of statistical analysis is valuable, because no one had to define a static threshold value, something which is\n\ninfeasible if we are tracking thousands or hundreds of thousands of production metrics.\n\nFor the remainder of this book, we will use the terms telemetry, metric, and data sets interchangeably—in other words, a metric\n\n(e.g., “page load times”) will map to a data set (e.g., 2 ms, 8 ms, 11 ms, etc.), the term used by statisticians to describe a matrix of\n\ndata points where each column represents a variable of which statistical operations are performed.\n\nINSTRUMENT AND ALERT ON UNDESIRED OUTCOMES\n\nTom Limoncelli, co-author of The Practice of Cloud System\n\nAdministration: Designing and Operating Large Distributed\n\nSystems and a former Site Reliability Engineer at Google, relates the following story on monitoring: “When people ask me for\n\nrecommendations on what to monitor, I joke that in an ideal world, we would delete all the alerts we currently have in our\n\nmonitoring system. Then, after each user-visible outage, we’d ask\n\nwhat indicators would have predicted that outage and then add those to our monitoring system, alerting as needed. Repeat. Now\n\nwe only have alerts that prevent outages, as opposed to being bombarded by alerts after an outage already occurred.”\n\nIn this step, we will replicate the outcomes of such an exercise.\n\nOne of the easiest ways to do this is to analyze our most severe incidents in the recent past (e.g., 30 days) and create a list of\n\ntelemetry that could have enabled earlier and faster detection and\n\ndiagnosis of the problem, as well as easier and faster confirmation that an effective fix had been implemented.\n\nFor instance, if we had an issue where our NGINX web server\n\nstopped responding to requests, we would look at the leading",
      "page_number": 315
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 323-330)",
      "start_page": 323,
      "end_page": 330,
      "detection_method": "topic_boundary",
      "content": "indicators that could have warned us earlier that we were starting\n\nto deviate from standard operations, such as:\n\nApplication level: increasing web page load times, etc.\n\nOS level: server free memory running low, disk space running\n\nlow, etc.\n\nDatabase level: database transaction times taking longer\n\nthan normal, etc.\n\nNetwork level: number of functioning servers behind the\n\nload balancer dropping, etc.\n\nEach of these metrics is a potential precursor to a production incident. For each, we would configure our alerting systems to\n\nnotify them when they deviate sufficiently from the mean, so that we can take corrective action.\n\nBy repeating this process on ever-weaker failure signals, we find\n\nproblems ever earlier in the life cycle, resulting in fewer customer impacting incidents and near misses. In other words, we are\n\npreventing problems as well as enabling quicker detection and\n\ncorrection.\n\nPROBLEMS THAT ARISE WHEN OUR TELEMETRY DATA HAS NON-GAUSSIAN DISTRIBUTION\n\nUsing means and standard deviations to detect variance can be extremely useful. However, using these techniques on many of the\n\ntelemetry data sets that we use in Operations will not generate the\n\ndesired results. As Dr. Toufic Boubez observes, “Not only will we get wakeup calls at 2 a.m., we’ll get them at 2:37 a.m., 4:13 a.m.,\n\n5:17 a.m. This happens when the underlying data that we’re monitoring doesn’t have a Gaussian distribution.”\n\nIn other words, when the distribution of the data set does not have\n\nthe Gaussian bell curve described earlier, the properties associated with standard deviations do not apply. For example, consider the scenario in which we are monitoring the number of file downloads\n\nper minute from our website. We want to detect periods when we have unusually high numbers of downloads, such as when our download rate is greater than three standard deviations from our\n\naverage, so that we can proactively add more capacity.\n\nFigure 30 shows our number of simultaneous downloads per minute over time, with a bar overlaid on top. When the bar is\n\nblack, the number of downloads within a given period (sometimes called a “sliding window”) is at least three standard deviations from the average. Otherwise, it is gray.\n\nFigure 30: Downloads per minute: over-alerting when using “3 standard deviation” rule (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nThe obvious problem that the graph shows is that we are alerting almost all of the time. This is because in almost any given period of time, we have instances when the download count exceeds our\n\nthree standard deviation threshold.\n\nTo confirm this, when we create a histogram (see figure 31) that shows the frequency of downloads per minute, we can see that it\n\ndoes not have the classic, symmetrical bell curve shape. Instead, it is obvious that the distribution is skewed toward the lower end, showing that the majority of the time we have very few downloads\n\nper minute but that download counts frequently spike three standard deviations higher.\n\nFigure 31: Downloads per minute: histogram of data showing non- Gaussian distribution (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nMany production data sets are non-Gaussian distribution. Dr. Nicole Forsgren explains, “In Operations, many of our data sets have what we call ‘chi squared’ distribution. Using standard\n\ndeviations for this data not only results in over- or under-alerting, but it also results in nonsensical results.” She continues, “When you compute the number of simultaneous downloads that are three standard deviations below the mean, you end up with a\n\nnegative number, which obviously doesn’t make sense.”\n\nOver-alerting causes Operations engineers to be woken up in the\n\nmiddle of the night for protracted periods of time, even when there are few actions that they can appropriately take. The problem associated with under-alerting is just as significant. For instance, suppose we are monitoring the number of completed\n\ntransactions, and the completed transaction count drops by 50% in the middle of the day due to a software component failure. If this is still within three standard deviations of the mean, no alert\n\nwill be generated, meaning that our customers will discover the\n\nproblem before we do, at which point the problem may be much more difficult to solve.\n\nFortunately, there are techniques we can use to detect anomalies in even non-Gaussian data sets, which are described next.\n\nCase Study Auto-Scaling Capacity at Netflix (2012)\n\nAnother tool developed at Netflix to increase service quality, Scryer, addresses some of the shortcomings of Amazon Auto Scaling (AAS), which dynamically increases and\n\ndecreases AWS compute server counts based on workload data. Scryer works by predicting what customer demands will be based on historical usage patterns and provisions the\n\nnecessary capacity.\n\nScryer addressed three problems with AAS. The first was dealing with rapid spikes in demand. Because AWS instance\n\nstartup times can be ten to forty-five minutes, additional compute capacity was often delivered too late to deal with spikes in demand. The second problem was that after outages, the rapid decrease in customer demand led to AAS\n\nremoving too much compute capacity to handle future incoming demand. The third problem was that AAS didn’t factor in known usage traffic patterns when scheduling\n\ncompute capacity.\n\nFigure 32: Netflix customer viewing demand for five days (Source: Daniel Jacobson, Danny Yuan, and Neeraj Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine,” The Netflix Tech Blog, November 5, 2013, http://techblog.netflix.com/2013/11/scryer-netflixs-predictive- auto-scaling.html.)\n\nNetflix took advantage of the fact that their consumer viewing patterns were surprisingly consistent and predictable, despite not having Gaussian distributions.\n\nBelow is a chart reflecting customer requests per second throughout the work week, showing regular and consistent customer viewing patterns Monday through Friday.\n\nScryer uses a combination of outlier detections to throw out spurious data points and then uses techniques such as Fast Fourier Transform (FFT) and linear regression to smooth the data while preserving legitimate traffic spikes that recur in\n\ntheir data. The result is that Netflix can forecast traffic demand with surprising accuracy.\n\nFigure 33: Netflix Scryer forecasting customer traffic and the resulting AWS schedule of compute resources (Source: Jacobson, Yuan, Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine.”)\n\nOnly months after first using Scryer in production, Netflix significantly improved their customer viewing experience, improved service availability, and reduced Amazon EC2\n\ncosts.\n\nUSING ANOMALY DETECTION TECHNIQUES\n\nWhen our data does not have Gaussian distribution, we can still find noteworthy variances using a variety of methods. These\n\ntechniques are broadly categorized as anomaly detection, often defined as “the search for items or events which do not conform to an expected pattern.” Some of these capabilities can be found\n\ninside our monitoring tools, while others may require help from people with statistical skills.\n\nTarun Reddy, VP of Development and Operations at Rally\n\nSoftware, actively advocates this active collaboration between Operations and statistics, observing, “To better enable service\n\nquality, we put all our production metrics into Tableau, a\n\nstatistical analysis software package. We even have an Ops\n\nengineer trained in statistics who writes R code (another statistical\n\npackage)—this engineer has her own backlog, filled with requests\n\nfrom other teams inside the company who want to find variance ever earlier, before it causes an even larger variance that could\n\naffect our customers.”\n\nOne of the statistical techniques we can use is called smoothing, which is especially suitable if our data is a time series, meaning\n\neach data point has a time stamp (e.g., download events,\n\ncompleted transaction events, etc.). Smoothing often involves using moving averages (or rolling averages), which transform our\n\ndata by averaging each point with all the other data within our\n\nsliding window. This has the effect of smoothing out short-term fluctuations and highlighting longer-term trends or cycles.†\n\nAn example of this smoothing effect is shown in the figure 34. The\n\nblack line represents the raw data, while the blue line indicates the thirty day moving average (i.e., the average of the trailing thirty days).‡\n\nFigure 34: Autodesk share price and thirty day moving average filter (Source: Jacobson, Yuan, Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine.”)",
      "page_number": 323
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 331-338)",
      "start_page": 331,
      "end_page": 338,
      "detection_method": "topic_boundary",
      "content": "More exotic filtering techniques exist, such as Fast Fourier\n\nTransforms, which has been widely used in image processing, and\n\nthe Kolmogorov-Smirnov test (found in Graphite and Grafana), which is often used to find similarities or differences in\n\nperiodic/seasonal metric data.\n\nWe can expect that a large percentage of telemetry concerning user data will have periodic/seasonal similarities—web traffic,\n\nretail transactions, movie watching, and many other user\n\nbehaviors have very regular and surprisingly predictable daily, weekly, and yearly patterns. This enables us to be able to detect\n\nsituations that vary from historical norms, such as when our order transaction rate on a Tuesday afternoon drops to 50% of our\n\nweekly norms.\n\nBecause of the usefulness of these techniques in forecasting, we\n\nmay be able to find people in the Marketing or Business Intelligence departments with the knowledge and skills necessary\n\nto analyze this data. We may want to seek these people out and explore working together to identify shared problems and use\n\nimproved anomaly detection and incident prediction to solve them.§\n\nCase Study Advanced Anomaly Detection (2014)\n\nAt Monitorama in 2014, Dr. Toufic Boubez described the power of using anomaly detection techniques, specifically\n\nhighlighting the effectiveness of the Komogorov-Smirnov\n\ntest, a technique that is often used in statistics to determine whether two data sets differ significantly and is found in the\n\npopular Graphite and Grafana tool. The purpose of\n\npresenting this case study here is not as a tutorial, but to\n\ndemonstrate how a class of statistical techniques can be used in our work, as well as how it’s likely being used in our\n\norganizations in completely different applications.\n\nFigure 35 shows the number of transactions per minute at an e-commerce site. Note the weekly periodicity of the\n\ngraph, with transaction volume dropping on the weekends.\n\nBy visual inspection, we can see that something peculiar seems to happen on the fourth week when normal\n\ntransaction volume doesn’t return to normal levels on Monday. This suggests an event we should investigate.\n\nFigure 35: Transaction volume: under-alerting using “3 standard deviation” rule (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nUsing the three standard deviation rule would only alert us twice, missing the critical Monday dropoff in transaction\n\nvolume. Ideally, we would also want to be alerted that the\n\ndata has drifted from our expected Monday pattern.\n\n“Even saying ‘Kolmogorov-Smirnov’ is a great way to impress everyone,” Dr. Boubez jokes. “But what Ops\n\nengineers should tell statisticians is that these types of non-\n\nparametric techniques are great for Operations data, because it makes no assumptions about normality or any\n\nother probability distribution, which is crucial for us to understand what’s going on in our very complex systems.\n\nThese techniques compare two probability distributions,\n\nallowing us to compare periodic or seasonal data, which helps us find variances in data that varies from day to day or\n\nweek to week.”\n\nFigure 36, on the following page, shows is the same data set with the K-S filter applied, with the third area highlighting the\n\nanomalous Monday where transaction volume didn’t return\n\nto normal levels. This would have alerted us of a problem in our system that would have been virtually impossible to\n\ndetect using visual inspection or using standard deviations.\n\nIn this scenario, this early detection could prevent a customer impacting event, as well as better enable us to\n\nachieve our organizational goals.\n\nFigure 36: Transaction volume: using Kolmogorov-Smirnov test to alert on anomalies (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nCONCLUSION\n\nIn this chapter, we explored several different statistical techniques\n\nthat can be used to analyze our production telemetry so we can\n\nfind and fix problems earlier than ever, often when they are still small and long before they cause catastrophic outcomes. This\n\nenables us to find ever-weaker failure signals that we can then act\n\nupon, creating an ever safer system of work, as well as increasing our ability to achieve our goals.\n\nSpecific case studies were presented, including how Netflix used\n\nthese techniques to proactively remove compute servers from production and auto-scale their compute infrastructure. We also\n\ndiscussed how to use a moving average and the Kolmogorov-\n\nSmirnov filter, both of which can be found in popular telemetry\n\ngraphing tools.\n\nIn the next chapter, we will describe how to integrate production telemetry into the daily work of Development in order to make\n\ndeployments safer and improve the system as a whole.\n\n† Smoothing and other statistical techniques are also used to manipulate graphic and audio files. For instance,\n\nimage smoothing (or blurring) as each pixel is replaced by the average of all its neighbors.\n\n‡ Other examples of smoothing filters include weighted moving averages or exponential smoothing (which linearly\n\nor exponentially weight more recent data points over older data points, respectively), and so forth.\n\n§ Tools we can using to solve these types of problems include Microsoft Excel (which remains one of the easiest\n\nand fastest ways to manipulate data for one-time purposes), as well as statistical packages such as SPSS, SAS, and the open source R project, now one of the most widely used statistical packages. Many other tools have been created, including several that Etsy has open-sourced, such as Oculus, which finds graphs with similar shapes that may indicate correlation; Opsweekly, which tracks alert volumes and frequencies; and Skyline, which attempts to identify anomalous behavior in system and application graphs.\n\n16Enable Feedback So Development and\n\nOperations Can Safely Deploy Code\n\nIn 2006, Nick Galbreath was VP of Engineering at Right Media,\n\nresponsible for both the Development and Operations departments for an online advertising platform that displayed and\n\nserved over ten billion impressions daily.\n\nGalbreath described the competitive landscape they operated in:\n\nIn our business, ad inventory levels were extremely dynamic,\n\nso we needed to respond to market conditions within minutes. This meant that Development had to be able to quickly make\n\ncode changes and get them into production as soon as possible,\n\notherwise we would lose to faster competitors. We found that having a separate group for testing, and even deployment, was simply too slow. We had to integrate all these functions into\n\none group, with shared responsibilities and goals. Believe it or not, our biggest challenge was getting developers to overcome their fear of deploying their own code!\n\nThere is an interesting irony here: Dev often complains about Ops being afraid to deploy code. But in this case, when given the power\n\nto deploy their own code, developers became just as afraid to\n\nperform code deployments.\n\nThe fear of deploying code that was shared by both Dev and Ops at\n\nRight Media is not unusual. However, Galbreath observed that\n\nproviding faster and more frequent feedback to engineers\n\nperforming deployments (whether Dev or Ops), as well as\n\nreducing the batch size of their work, created safety and then\n\nconfidence.\n\nAfter observing many teams go through this transformation,\n\nGalbreath describes their progression as follows:\n\nWe start with no one in Dev or Ops being willing to push the “deploy code” button that we’ve built that automates the entire code deployment process, because of the paralyzing fear of being the first person to potentially bring all of the production\n\nsystems down. Eventually, when someone is brave enough to volunteer to push their code into production, inevitably, due to incorrect assumptions or production subtleties that weren’t fully appreciated, the first production deployment doesn’t go smoothly—and because we don’t have enough production telemetry, we only find out about the problems when\n\ncustomers tell us.\n\nTo fix the problem, our team urgently fixes the code and pushes it\n\ninto production, but this time with more production telemetry added to our applications and environment. This way, we can actually confirm that our fix restored service correctly, and we’ll be able to detect this type of problem before a customer tells us next time.\n\nLater, more developers start to push their own code into\n\nproduction. And because we’re working in a complex system, we’ll\n\nstill probably break something in production, but this time we’ll be\n\nable to quickly see what functionality broke, and quickly decide\n\nwhether to roll back or fix-forward, resolving the problem. This is\n\na huge victory for the entire team and everyone celebrates—we’re\n\nnow on a roll.\n\nHowever, the team wants to improve the outcomes of their\n\ndeployments, so developers proactively get more peer reviews of\n\ntheir code changes (described in chapter 18), and everyone helps\n\neach other write better automated tests so we can find errors before deployment. And because everyone now knows that the smaller our production changes, the fewer problems we will have,\n\ndevelopers start checking ever-smaller increments of code more frequently into the deployment pipeline, ensuring that their change is working successfully in production before moving to their next change.\n\nWe are now deploying code more frequently than ever, and service stability is better than ever too. We have re-discovered that the secret to smooth and continuous flow is making small, frequent changes that anyone can inspect and easily understand.\n\nGalbreath observes that the above progression benefits everyone, including Development, Operations, and Infosec. “As the person\n\nwho is also responsible for security, it’s reassuring to know that we can deploy fixes into production quickly, because changes are going into production throughout the entire day. Furthermore, it always amazes me how interested every engineer becomes in\n\nsecurity when you find problems in their code that they are responsible for and that they can quickly fix themselves.”",
      "page_number": 331
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 339-346)",
      "start_page": 339,
      "end_page": 346,
      "detection_method": "topic_boundary",
      "content": "The Right Media story shows that it is not enough to merely automate the deployment process—we must also integrate the\n\nmonitoring of production telemetry into our deployment work, as well as establish the cultural norms that everyone is equally\n\nresponsible for the health of the entire value stream.\n\nIn this chapter, we create the feedback mechanisms that enable us\n\nto improve the health of the value stream at every stage of the service life cycle, from product design through development and\n\ndeployment and into operation and eventually retirement. By doing this, we ensure that our services are “production ready,”\n\neven at the earliest stages of the project, as well as integrating the learnings from each release and production problem into our\n\nfuture work, resulting in better safety and productivity for\n\neveryone.\n\nUSE TELEMETRY TO MAKE DEPLOYMENTS SAFER\n\nIn this step, we ensure that we are actively monitoring our production telemetry when anyone performs a production\n\ndeployment, as was illustrated in the Right Media story. This allows whoever is doing the deployment, be it Dev or Ops, to\n\nquickly determine whether features are operating as designed\n\nafter the new release is running in production. After all, we should never consider our code deployment or production change to be\n\ndone until it is operating as designed in the production environment.\n\nWe do this by actively monitoring the metrics associated with our\n\nfeature during our deployment to ensure we haven’t inadvertently\n\nbroken our service—or worse, that we broke another service. If our change breaks or impairs any functionality, we quickly work to\n\nrestore service, bringing in whoever else is required to diagnose and fix the issue.†\n\nAs described in Part III, our goal is to catch errors in our\n\ndeployment pipeline before they get into production. However, there will still be errors that we don’t detect, and we rely on\n\nproduction telemetry to quickly restore service. We may choose to\n\nturn off broken features with feature toggles (which is often the easiest and least risky option since it involves no deployments to\n\nproduction), or fix forward (i.e., make code changes to fix the defect, which are then pushed into production through the\n\ndeployment pipeline), or roll back (e.g., switch back to the previous release by using feature toggles or by taking broken\n\nservers out of rotation using the blue-green or canary release\n\npatterns, etc.)\n\nAlthough fixing forward can often be dangerous, it can be extremely safe when we have automated testing and fast\n\ndeployment processes, and sufficient telemetry that allows us to quickly confirm whether everything is functioning correctly in\n\nproduction.\n\nFigure 37 shows a deployment of PHP code change at Etsy that generated a spike in PHP runtime warnings—in this case, the\n\ndeveloper quickly noticed the problem within minutes, and\n\ngenerated a fix and deployed it into production, resolving the issue in less than ten minutes.\n\nBecause production deployments are one of the top causes of\n\nproduction issues, each deployment and change event is overlaid\n\nonto our metric graphs to ensure that everyone in the value stream\n\nis aware of relevant activity, enabling better communication and\n\ncoordination, as well as faster detection and recovery.\n\nFigure 37: Deployment to Etsy.com causes PHP run-time warnings and is quickly fixed (Source: Mike Brittain, “Tracking Every Release.”)\n\nDEV SHARES PAGER ROTATION DUTIES WITH OPS\n\nEven when our production deployments and releases go\n\nflawlessly, in any complex service we will still have unexpected problems, such as incidents and outages that happen at\n\ninopportune times (every night at 2 a.m.). Left unfixed, these can cause recurring problems and suffering for Ops engineers downstream, especially when these problems are not made visible\n\nto the upstream engineers responsible for creating the problem.\n\nEven if the problem results in a defect being assigned to the feature team, it may be prioritized below the delivery of new\n\nfeatures. The problem may keep recurring for weeks, months, or even years, causing continual chaos and disruption in Operations. This is an example of how upstream work centers can locally\n\noptimize for themselves but actually degrade performance for the entire value stream.\n\nTo prevent this from happening, we will have everyone in the\n\nvalue stream share the downstream responsibilities of handling operational incidents. We can do this by putting developers, development managers, and architects on pager rotation, just as\n\nPedro Canahuati, Facebook Director of Production Engineering, did in 2009. This ensures everyone in the value stream gets visceral feedback on any upstream architectural and coding decisions they make.\n\nBy doing this, Operations doesn’t struggle, isolated and alone with code-related production issues; instead, everyone is helping find the proper balance between fixing production defects and\n\ndeveloping new functionality, regardless of where we reside in the value stream. As Patrick Lightbody, SVP of Product Management at New Relic, observed in 2011, “We found that when we woke up\n\ndevelopers at 2 a.m., defects were fixed faster than ever.”\n\nOne side effect of this practice is that it helps Development management see that business goals are not achieved simply\n\nbecause features have been marked as “done.” Instead, the feature is only done when it is performing as designed in production, without causing excessive escalations or unplanned work for either Development or Operations.‡\n\nThis practice is equally applicable for market-oriented teams, responsible for both developing the feature and running it in\n\nproduction, and for functionally-oriented teams. As Arup Chakrabarti, Operations Engineering Manager at PagerDuty, observed during a 2014 presentation, “It’s becoming less and less\n\ncommon for companies to have dedicated on-call teams; instead, everyone who touches production code and environments is expected to be reachable in the event of downtime.”\n\nRegardless of how we’ve organized our teams, the underlying principles remain the same: when developers get feedback on how their applications perform in production, which includes fixing it when it breaks, they become closer to the customer, this creates a\n\nbuy-in that everyone in the value stream benefits from.\n\nHAVE DEVELOPERS FOLLOW WORK DOWNSTREAM\n\nOne of the most powerful techniques in interaction and user\n\nexperience design (UX) is contextual inquiry. This is when the product team watches a customer use the application in their natural environment, often working at their desk. Doing so often uncovers startling ways that customers struggle with the\n\napplication, such as requiring scores of clicks to perform simple tasks in their daily work, cutting and pasting text from multiple screens, or writing down notes on paper. All of these are examples\n\nof compensatory behaviors and workarounds for usability issues.\n\nThe most common reaction for developers after participating in a customer observation is dismay, often stating “how awful it was\n\nseeing the many ways we have been inflicting pain on our\n\ncustomers.” These customer observations almost always result in significant learning and a fervent desire to improve the situation\n\nfor the customer.\n\nOur goal is to use this same technique to observe how our work affects our internal customers. Developers should follow their\n\nwork downstream, so they can see how downstream work centers must interact with their product to get it running into production.§\n\nDevelopers want to follow their work downstream—by seeing\n\ncustomer difficulties firsthand, they make better and more informed decisions in their daily work.\n\nBy doing this, we create feedback on the non-functional aspects of\n\nour code—all the elements that are not related to the customer- facing feature—and identify ways that we can improve deployability, manageability, operability, and so on.\n\nUX observation often has a powerful impact on the observers. When describing his first customer observation, Gene Kim, the founder and CTO at Tripwire for thirteen years and co-author of\n\nthis book, said:\n\nOne of the worst moments of my professional career was in 2006 when I spent an entire morning watching one of our\n\ncustomers use our product. I was watching him perform an operation that we expected customers to do weekly, and, to our extreme horror, we discovered that it required sixty-three\n\nclicks. This person kept apologizing, saying things like, “Sorry, there’s probably a better way to do this.”\n\nUnfortunately, there wasn’t a better way to do that operation.\n\nAnother customer described how initial product setup took\n\n1,300 steps. Suddenly, I understood why the job of managing our product was always assigned to the newest engineer on the team—no one wanted the job of running our product. That was\n\none of the reasons I helped create the UX practice at my company, to help atone for the pain we were inflicting on our customers.\n\nUX observation enables the creation of quality at the source and results in far greater empathy for fellow team members in the value stream. Ideally, UX observation helps us as we create\n\ncodified non-functional requirements to add to our shared backlog of work, eventually allowing us to proactively integrate them into every service we build, which is an important part of creating a DevOps work culture.¶\n\nHAVE DEVELOPERS INITIALLY SELF- MANAGE THEIR PRODUCTION SERVICE\n\nEven when Developers are writing and running their code in production-like environments in their daily work, Operations may\n\nstill experience disastrous production releases because it is the first time we actually see how our code behaves during a release and under true production conditions. This result occurs because\n\noperational learnings often occur too late in the software life cycle.\n\nLeft unaddressed, the result is often production software that is difficult to operate. As an anonymous Ops engineer once said, “In\n\nour group, most system administrators lasted only six months. Things were always breaking in production, the hours were insane, and application deployments were painful beyond belief— the worst part was pairing the application server clusters, which\n\nwould take us six hours. During each moment, we all felt like the developers personally hated us.”\n\nThis can be an outcome of not having enough Ops engineers to support all the product teams and the services we already have in production, which can happen in both functionally- and market- oriented teams.\n\nOne potential countermeasure is to do what Google does, which is have Development groups self-manage their services in\n\nproduction before they become eligible for a centralized Ops group to manage. By having developers be responsible for deployment and production support, we are far more likely to have a smooth transition to Operations.**\n\nTo prevent the possibility of problematic, self-managed services going into production and creating organizational risk, we may define launch requirements that must be met in order for services\n\nto interact with real customers and be exposed to real production traffic. Furthermore, to help the product teams, Ops engineers should act as consultants to help them make their services\n\nproduction-ready.\n\nBy creating launch guidance, we help ensure that every product team benefits from the cumulative and collective experience of the\n\nentire organization, especially Operations. Launch guidance and requirements will likely include the following:\n\nDefect counts and severity: Does the application actually\n\nperform as designed?\n\nType/frequency of pager alerts: Is the application generating an unsupportable number of alerts in production?",
      "page_number": 339
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 347-354)",
      "start_page": 347,
      "end_page": 354,
      "detection_method": "topic_boundary",
      "content": "Monitoring coverage: Is the coverage of monitoring sufficient to restore service when things go wrong?\n\nSystem architecture: Is the service loosely-coupled enough to support a high rate of changes and deployments in production?\n\nDeployment process: Is there a predictable, deterministic, and sufficiently automated process to deploy code into production?\n\nProduction hygiene: Is there evidence of enough good production habits that would allow production support to be managed by anyone else?\n\nSuperficially, these requirements may appear similar to traditional production checklists we have used in the past. However, the key differences are we require effective monitoring to be in place,\n\ndeployments to be reliable and deterministic, and an architecture that supports fast and frequent deployments.\n\nIf any deficiencies are found during the review, the assigned Ops\n\nengineer should help the feature team resolve the issues or even help re-engineer the service if necessary, so that it can be easily deployed and managed in production.\n\nAt this time, we may also want to learn whether this service is subject to any regulatory compliance objectives or if it is likely to\n\nbe in the future:\n\nDoes the service generate a significant amount of revenue?\n\n(For example, if it is more than 5% of total revenue of a publicly-held US corporation, it is a “significant account” and\n\nin-scope for compliance with Section 404 of the Sarbanes-\n\nOxley Act of 2002 [SOX].)\n\nDoes the service have high user traffic or have high outage/impairment costs? (i.e., do operational issues risk\n\ncreating availability or reputational risk?)\n\nDoes the service store payment cardholder information, such as credit card numbers, or personally identifiable information,\n\nsuch as Social Security numbers or patient care records? Are\n\nthere other security issues that could create regulatory, contractual obligation, privacy, or reputation risk?\n\nDoes the service have any other regulatory or contractual\n\ncompliance requirements associated with it, such as US export regulations, PCI-DSS, HIPAA, and so forth?\n\nThis information helps ensure that we effectively manage not only\n\nthe technical risks associated with this service, but also any potential security and compliance risks. It also provides essential\n\ninput into the design of the production control environment.\n\nFigure 38: The “Service Handback” at Google (Source: “SRE@Google: Thousands of DevOps Since 2004,” YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n\nBy integrating operability requirements into the earliest stages of\n\nthe development process and having Development initially self- manage their own applications and services, the process of\n\ntransitioning new services into production becomes smoother,\n\nbecoming far easier and more predictable to complete. However, for services already in production, we need a different mechanism\n\nto ensure that Operations is never stuck with an unsupportable service in production. This is especially relevant for functionally-\n\noriented Operations organizations.\n\nIn this step, we may create a service handback mechanism—in\n\nother words, when a production service becomes sufficiently fragile, Operations has the ability to return production support\n\nresponsibility back to Development.\n\nWhen a service goes back into a developer-managed state, the role\n\nof Operations shifts from production support to consultation,\n\nhelping the team make the service production-ready.\n\nThis mechanism serves as our pressure escape valve, ensuring that\n\nwe never put Operations in a situation where they are trapped into\n\nmanaging a fragile service while an ever-increasing amount of technical debt buries them and amplifies a local problem into a\n\nglobal problem. This mechanism also helps ensure that\n\nOperations has enough capacity to work on improvement work and preventive projects.\n\nThe hand-back remains a long-standing practice at Google and is\n\nperhaps one of the best demonstrations of the mutual respect between Dev and Ops engineers. By doing this, Development is\n\nable to quickly generate new services, with Ops engineers joining the team when the services become strategically important to the\n\ncompany and, in rare cases, handing them back when they become too troublesome to manage in production.†† The following case study of Site Reliability Engineering at Google describes how the\n\nHand-off Readiness Review and Launch Readiness Review\n\nprocesses evolved, and the benefits that resulted.\n\nCase Study The Launch and Hand-off Readiness Review at Google (2010)\n\nOne of the many surprising facts about Google is that they\n\nhave a functional orientation for their Ops engineers, who are referred to as “Site Reliability Engineers” (SRE), a term coined by Ben Treynor Sloss in 2004.‡‡ That year, Treynor\n\nSloss started off with a staff of seven SREs that grew to over\n\n1,200 SREs by 2014. As Treynor Sloss said, “If Google ever\n\ngoes down, it’s my fault.” Treynor Sloss has resisted creating a single sentence definition of what SREs are, but,\n\nhe once described SREs as “what happens when a software engineer is tasked with what used to be called operations.”\n\nEvery SRE reports to Treynor Sloss’s organization to help\n\nensure consistency of quality of staffing and hiring, and they are embedded into product teams across Google (which\n\nalso provide their funding). However, SREs are still so\n\nscarce they are assigned only to the product teams that have the highest importance to the company or those that\n\nmust comply with regulatory requirements. Furthermore,\n\nthose services must have low operational burden. Products that don’t meet the necessary criteria remain in a developer-\n\nmanaged state.\n\nEven when new products become important enough to the company to warrant being assigned an SRE, developers still\n\nmust have self-managed their service in production for at\n\nleast six months before it becomes eligible to have an SRE assigned to the team.\n\nTo help ensure that these self-managed product teams can\n\nstill benefit from the collective experience of the SRE organization, Google created two sets of safety checks for\n\ntwo critical stages of releasing new services called the\n\nLaunch Readiness Review and the Hand-Off Readiness Review (LRR and HRR, respectively).\n\nThe LRR must be performed and signed off on before any\n\nnew Google service is made publicly available to customers and receives live production traffic, while the HRR is\n\nperformed when the service is transitioned to an Ops-\n\nmanaged state, usually months after the LRR. The LRR and HRR checklists are similar, but the HRR is far more stringent\n\nand has higher acceptance standards, while the LRR is self-\n\nreported by the product teams.\n\nAny product team going through an LRR or HRR has an\n\nSRE assigned to them to help them understand the\n\nrequirements and to help them achieve those requirements. The LRR and HRR launch checklists have evolved over time\n\nso every team can benefit from the collective experiences of\n\nall previous launches, whether successful or unsuccessful. Tom Limoncelli noted during his “SRE@Google: Thousands\n\nof DevOps Since 2004” presentation in 2012, “Every time we\n\ndo a launch, we learn something. There will always be some people who are less experienced than others doing releases\n\nand launches. The LRR and HRR checklists are a way to\n\ncreate that organizational memory.”\n\nRequiring product teams to self-manage their own services\n\nin production forces Development to walk in the shoes of\n\nOps, but guided by the LRR and HRR, which not only makes service transition easier and more predictable, but also\n\nhelps create empathy between upstream and downstream work centers.\n\nFigure 39: The “Launch readiness review and hand-offs readiness review” at Google (Source: “SRE@Google: Thousands of DevOps Since 2004,” YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n\nLimoncelli noted, “In the best case, product teams have been using the LRR checklist as a guideline, working on\n\nfulfilling it in parallel with developing their service, and\n\nreaching out to SREs to get help when they need it.”\n\nFurthermore, Limoncelli observed, “The teams that have the\n\nfastest HRR production approval are the ones that worked\n\nwith SREs earliest, from the early design stages up until launch. And the great thing is, it’s always easy to get an\n\nSRE to volunteer to help with your project. Every SRE sees\n\nvalue in giving advice to project teams early, and will likely volunteer a few hours or days to do just that.”\n\nThe practice of SREs helping product teams early is an\n\nimportant cultural norm that is continually reinforced at Google. Limoncelli explained, “Helping product teams is a\n\nlong-term investment that will pay off many months later\n\nwhen it comes time to launch. It is a form of ‘good citizenship’ and ‘community service’ that is valued, it is\n\nroutinely considered when evaluating engineers for SRE\n\npromotions.”\n\nCONCLUSION\n\nIn this chapter, we discussed the feedback mechanisms that\n\nenable us to improve our service at every stage of our daily work,\n\nwhether it is deploying changes into production, fixing code when things go wrong and engineers are paged, having developers\n\nfollow their work downstream, creating non-functional requirements that help development teams write more\n\nproduction-ready code, or even handing problematic services back\n\nto be self-managed by Development.\n\nBy creating these feedback loops, we make production\n\ndeployments safer, increase the production readiness of code\n\ncreated by Development, and help create a better working relationship between Development and Operations by reinforcing\n\nshared goals, responsibilities, and empathy.\n\nIn the next chapter, we explore how telemetry can enable hypothesis-driven development and A/B testing to perform\n\nexperiments that help us achieve our organizational goals and win\n\nin the marketplace.\n\n† By doing this, along with the required architecture, we “optimize for MTTR, instead of MTBF,” a popular DevOps maxim to describe our desire to optimize for recovering from failures quickly, as opposed to attempting to prevent failures.\n\n‡ ITIL defines warranty as when a service can run in production reliably without intervention for a predefined period of time (e.g., two weeks). This definition of warranty should ideally be integrated into our collective",
      "page_number": 347
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 355-362)",
      "start_page": 355,
      "end_page": 362,
      "detection_method": "topic_boundary",
      "content": "definition of “done.”\n\n§ By following work downstream, we may uncover ways to help improve flow, such as automating complex, manual steps (e.g., pairing application server clusters that require six hours to successfully complete); performing packaging of code once instead of creating it multiple times at different stages of QA and Production deployment; working with testers to automate manual test suites, thus removing a common bottleneck for more frequent deployment; and creating more useful documentation instead of having someone decipher developer application notes to build packaged installers.\n\n¶ More recently, Jeff Sussna attempted to further codify how to better achieve UX goals in what he calls “digital conversations,” which are intended to help organizations understand the customer journey as a complex system, broadening the context of quality. The key concepts include designing for service, not software; minimizing latency and maximizing strength of feedback; designing for failure and operating to learn; using Operations as an input to design; and seeking empathy.\n\n** We further increase the likelihood of production problems being fixed by ensuring that the Development teams\n\nremain intact, and not disbanded after the project is complete.\n\n†† In organizations with project-based funding, there may be no developers to hand the service back to, as the\n\nteam has already been disbanded or may not have the budget or time to take on service responsibility. Potential countermeasures include holding an improvement blitz to improve the service, temporarily funding or staffing improvement efforts, or retiring the service.\n\n‡‡ In this book, we use the term “Ops engineer,” but the two terms, “Ops Engineer” and “Site Reliability\n\nEngineer,” are intended to be interchangeable.\n\n17Integrate Hypothesis- Driven Development and A/B Testing into Our Daily Work\n\nAll too often in software projects, developers work on features for\n\nmonths or years, spanning multiple releases, without ever confirming whether the desired business outcomes are being met,\n\nsuch as whether a particular feature is achieving the desired results or even being used at all.\n\nWorse, even when we discover that a given feature isn’t achieving\n\nthe desired results, making corrections to the feature may be out- prioritized by other new features, ensuring that the under-\n\nperforming feature will never achieve its intended business goal.\n\nIn general, Jez Humble observes, “the most inefficient way to test a business model or product idea is to build the complete product\n\nto see whether the predicted demand actually exists.”\n\nBefore we build a feature, we should rigorously ask ourselves, “Should we build it, and why?” We should then perform the cheapest and fastest experiments possible to validate through user\n\nresearch whether the intended feature will actually achieve the desired outcomes. We can use techniques such as hypothesis- driven development, customer acquisition funnels, and A/B\n\ntesting, concepts we explore throughout this chapter. Intuit, Inc.\n\nprovides a dramatic example of how organizations use these\n\ntechniques to create products that customers love, to promote\n\norganizational learning, and to win in the marketplace.\n\nIntuit is focused on creating business and financial management\n\nsolutions to simplify life for small businesses, consumers, and\n\naccounting professionals. In 2012, they had $4.5 billion in revenue\n\nand 8,500 employees, with flagship products that include QuickBooks, TurboTax, Mint, and, until recently, Quicken.†\n\nScott Cook, the founder of Intuit, has long advocated building a\n\nculture of innovation, encouraging teams to take an experimental\n\napproach to product development and exhorting leadership to\n\nsupport them. As he said, “Instead of focusing on the boss’s vote… the emphasis is on getting real people to really behave in real\n\nexperiments, and basing your decisions on that.” This is the epitome of a scientific approach to product development.\n\nCook explained that what is needed is “a system where every employee can do rapid, high-velocity experiments….Dan Maurer runs our consumer division....[which] runs the TurboTax website. When he took over, we did about seven experiments a year.”\n\nHe continued, “By installing a rampant innovation culture [in 2010], they now do 165 experiments in the three months of the [US] tax season. Business result? [The] conversion rate of the\n\nwebsite is up 50 percent…. The folks [team members] just love it, because now their ideas can make it to market.”\n\nAside from the effect on the website conversion rate, one of the most surprising elements of this story is that TurboTax performed production experiments during their peak traffic seasons. For\n\ndecades, especially in retailing, the risk of revenue-impacting\n\noutages during the holiday season were so high that we would\n\noften put into place a change freeze from mid-October to mid-\n\nJanuary.\n\nHowever, by making software deployments and releases fast and\n\nsafe, the TurboTax team made online user experimentation and\n\nany required production changes a low-risk activity that could be\n\nperformed during the highest traffic and revenue generating\n\nperiods.\n\nThis highlights the notion that the period when experimentation\n\nhas the highest value is during peak traffic seasons. Had the TurboTax team waited until April 16th, the day after the US tax filing deadline, to implement these changes, the company could have lost many of its prospective customers, and even some of its existing customers, to the competition.\n\nThe faster we can experiment, iterate, and integrate feedback into\n\nour product or service, the faster we can learn and out-experiment the competition. And how quickly we can integrate our feedback depends on our ability to deploy and release software.\n\nThe Intuit example shows that the Intuit TurboTax team was able to make this situation work for them and won in the marketplace as a result.\n\nA BRIEF HISTORY OF A/B TESTING\n\nAs the Intuit TurboTax story highlights, an extremely powerful user research technique is defining the customer acquisition\n\nfunnel and performing A/B testing. A/B testing techniques were pioneered in direct response marketing, which is one of the two\n\nmajor categories of marketing strategies. The other is called mass marketing or brand marketing and often relies on placing as\n\nmany ad impressions in front of people as possible to influence\n\nbuying decisions.\n\nIn previous eras, before email and social media, direct response marketing meant sending thousands of postcards or flyers via\n\npostal mail, and asking prospects to accept an offer by calling a telephone number, returning a postcard, or placing an order.\n\nIn these campaigns, experiments were performed to determine\n\nwhich offer had the highest conversion rates. They experimented with modifying and adapting the offer, re-wording the offer,\n\nvarying the copywriting styles, design and typography, packaging,\n\nand so forth, to determine which was most effective in generating the desired action (e.g., calling a phone number, ordering a\n\nproduct).\n\nEach experiment often required doing another design and print run, mailing out thousands of offers, and waiting weeks for\n\nresponses to come back. Each experiment typically cost tens of thousands of dollars per trial and required weeks or months to\n\ncomplete. However, despite the expense, iterative testing easily\n\npaid off if it significantly increased conversion rates (e.g., the percentage of respondents ordering a product going from 3%–\n\n12%).\n\nWell-documented cases of A/B testing include campaign fundraising, Internet marketing, and the Lean Startup\n\nmethodology. Interestingly, it has also been used by the British\n\ngovernment to determine which letters were most effective in collecting overdue tax revenue from delinquent citizens.‡\n\nINTEGRATING A/B TESTING INTO OUR FEATURE TESTING\n\nThe most commonly used A/B technique in modern UX practice\n\ninvolves a website where visitors are randomly selected to be shown one of two versions of a page, either a control (the “A”) or a\n\ntreatment (the “B”). Based on statistical analysis of the subsequent behavior of these two cohorts of users, we demonstrate whether\n\nthere is a significant difference in the outcomes of the two,\n\nestablishing a causal link between the treatment (e.g., a change in a feature, design element, background color) and the outcome\n\n(e.g., conversion rate, average order size).\n\nFor example, we may conduct an experiment to see whether modifying the text or color on a “buy” button increases revenue or\n\nwhether slowing down the response time of a website (by introducing an artificial delay as the treatment) reduces revenue.\n\nThis type of A/B testing allows us to establish a dollar value on\n\nperformance improvements.\n\nSometimes, A/B tests are also known as online controlled experiments and split tests. It’s also possible to run experiments\n\nwith more than one variable. This allows us to see how the variables interact, a technique known as multivariate testing.\n\nThe outcomes of A/B tests are often startling. Ronny Kohavi,\n\nDistinguished Engineer and General Manager of the Analysis and Experimentation group at Microsoft, observed that after\n\n“evaluating well-designed and executed experiments that were\n\ndesigned to improve a key metric, only about one-third were\n\nsuccessful at improving the key metric!” In other words, two- thirds of features either have a negligible impact or actually make\n\nthings worse. Kohavi goes on to note that all these features were originally thought to be reasonable, good ideas, further elevating\n\nthe need for user testing over intuition and expert opinions.\n\nThe implications of the Kohavi data are staggering. If we are not performing user research, the odds are that two-thirds of the\n\nfeatures we are building deliver zero or negative value to our\n\norganization, even as they make our codebase ever more complex, thus increasing our maintenance costs over time and making our\n\nsoftware more difficult to change. Furthermore, the effort to build these features is often made at the expense of delivering features\n\nthat would deliver value (i.e., opportunity cost). Jez Humble joked, “Taken to an extreme, the organization and customers\n\nwould have been better off giving the entire team a vacation,\n\ninstead of building one of these non–value-adding features.”\n\nOur countermeasure is to integrate A/B testing into the way we design, implement, test, and deploy our features. Performing\n\nmeaningful user research and experiments ensures that our efforts help achieve our customer and organizational goals, and help us\n\nwin in the marketplace.\n\nINTEGRATE A/B TESTING INTO OUR RELEASE\n\nFast and iterative A/B testing is made possible by being able to\n\nquickly and easily do production deployments on demand, using\n\nfeature toggles and potentially delivering multiple versions of our code simultaneously to customer segments. Doing this requires\n\nuseful production telemetry at all levels of the application stack.\n\nBy hooking into our feature toggles, we can control which percentage of users see the treatment version of an experiment.\n\nFor example, we may have one-half of our customers be our treatment group and one-half get shown the following: “Similar items link on unavailable items in the cart.” As part of our\n\nexperiment, we compare the behavior of the control group (no offer made) against the treatment group (offer made), perhaps measuring number of purchases made in that session.\n\nEtsy open-sourced their experimentation framework Feature API (formerly known as the Etsy A/B API), which not only supports A/B testing but also online ramp-ups, enabling throttling\n\nexposure to experiments. Other A/B testing products include Optimizely, Google Analytics, etc.\n\nIn a 2014 interview with Kendrick Wang of Apptimize, Lacy\n\nRhoades at Etsy described their journey: “Experimentation at Etsy comes from a desire to make informed decisions, and ensure that when we launch features for our millions of members, they work. Too often, we had features that took a lot of time and had to be\n\nmaintained without any proof of their success or any popularity among users. A/B testing allows us to...say a feature is worth working on as soon as it’s underway.”\n\nINTEGRATING A/B TESTING INTO OUR FEATURE PLANNING",
      "page_number": 355
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 363-370)",
      "start_page": 363,
      "end_page": 370,
      "detection_method": "topic_boundary",
      "content": "Once we have the infrastructure to support A/B feature release and testing, we must ensure that product owners think about each\n\nfeature as a hypothesis and use our production releases as experiments with real users to prove or disprove that hypothesis. Constructing experiments should be designed in the context of the\n\noverall customer acquisition funnel. Barry O’Reilly, co-author of Lean Enterprise: How High Performance Organizations Innovate at Scale, described how we can frame hypotheses in feature development in the following form:\n\nWe Believe that increasing the size of hotel images on the booking page\n\nWill Result in improved customer engagement and conversion\n\nWe Will Have Confidence To Proceed When we see a 5%\n\nincrease in customers who review hotel images who then proceed to book in forty-eight hours.\n\nAdopting an experimental approach to product development\n\nrequires us to not only break down work into small units (stories or requirements), but also validate whether each unit of work is delivering the expected outcomes. If it does not, we modify our\n\nroad map of work with alternative paths that will actually achieve those outcomes.\n\nCase Study\n\nDoubling Revenue Growth through Fast Release Cycle Experimentation at Yahoo! Answers (2010)\n\nThe faster we can iterate and integrate feedback into the product or service we are delivering to customers, the faster\n\nwe can learn and the bigger the impact we can create. How dramatically outcomes can be affected by faster cycle times was evident at Yahoo! Answers as they went from one release every six weeks to multiple releases every week.\n\nIn 2009, Jim Stoneham was General Manager of the Yahoo! Communities group that included Flickr and Answers. Previously, he had been primarily responsible for Yahoo!\n\nAnswers, competing against other Q&A companies such as Quora, Aardvark, and Stack Exchange.\n\nAt that time, Answers had approximately 140 million monthly visitors, with over twenty million active users answering questions in over twenty different languages. However, user growth and revenue had flattened, and user engagement\n\nscores were declining.\n\nStoneham observes that “Yahoo Answers was and continues to be one of the biggest social games on the\n\nInternet; tens of millions of people are actively trying to ‘level up’ by providing quality answers to questions faster than the next member of the community. There were many\n\nopportunities to tweak the game mechanic, viral loops, and other community interactions. When you’re dealing with these human behaviors, you’ve got to be able to do quick iterations and testing to see what clicks with people.”\n\nHe continues, “These [experiments] are the things that Twitter, Facebook, and Zynga did so well. Those\n\norganizations were doing experiments at least twice per\n\nweek—they were even reviewing the changes they made before their deployments, to make sure they were still on track. So here I am, running [the] largest Q&A site in the\n\nmarket, wanting to do rapid iterative feature testing, but we can’t release any faster than once every 4 weeks. In contrast, the other people in the market had a feedback loop\n\n10x faster than us.”\n\nStoneham observed that as much as product owners and developers talk about being metrics-driven, if experiments\n\nare not performed frequently (daily or weekly), the focus of daily work is merely on the feature they’re working on, as opposed to customer outcomes.\n\nAs the Yahoo! Answers team was able to move to weekly deployments, and later multiple deployments per week, their ability to experiment with new features increased\n\ndramatically. Their astounding achievements and learnings over the next twelve months of experimentation included increased monthly visits of 72%, increased user engagement of threefold, and the team doubled their\n\nrevenue. To continue their success, the team focused on optimizing the following top metrics:\n\nTime to first answer: How quickly was an answer posted to a user question?\n\nTime to best answer: How quickly did the user community\n\naward a best answer?\n\nUpvotes per answer: How many times was an answer upvoted by the user community?\n\nAnswers/week/person: How many answers were users creating?\n\nSecond search rate: How often did visitors have to search again to get an answer? (Lower is better.)\n\nStoneham concluded, “This was exactly the learning that we\n\nneeded to win in the marketplace—and it changed more than our feature velocity. We transformed from a team of employees to a team of owners. When you move at that\n\nspeed, and are looking at the numbers and the results daily, your investment level radically changes.”\n\nCONCLUSION\n\nSuccess requires us to not only deploy and release software quickly, but also to out-experiment our competition. Techniques such as hypothesis-driven development, defining and measuring\n\nout customer acquisition funnel, and A/B testing allow us to perform user-experiments safely and easily, enabling us to unleash creativity and innovation, and create organizational\n\nlearning. And, while succeeding is important, the organizational learning that comes from experimentation also gives employees ownership of business objectives and customer satisfaction. In the next chapter, we examine and create review and coordination\n\nprocesses as a way to increase the quality of our current work.\n\n† In 2016, Intuit sold the Quicken business to the private equity firm H.I.G. Capital.\n\n‡ There are many other ways to conduct user research before embarking on development. Among the most\n\ninexpensive methods include performing surveys, creating prototypes (either mock-ups using tools such as Balsamiq or interactive versions written in code), and performing usability testing. Alberto Savoia, Director of Engineering at Google, coined the term pretotyping for the practice of using prototypes to validate whether we are building the right thing. User research is so inexpensive and easy relative to the effort and cost of building a\n\nuseless feature in code that, in almost every case, we shouldn’t prioritize a feature without some form of validation.\n\n18Create Review and\n\nCoordination Processes\n\nto Increase Quality of Our Current Work\n\nIn the previous chapters, we created the telemetry necessary to see\n\nand solve problems in production and at all stages of our\n\ndeployment pipeline, and created fast feedback loops from customers to help enhance organizational learning—learning that\n\nencourages ownership and responsibility for customer satisfaction and feature performance, which helps us succeed.\n\nOur goal in this chapter is to enable Development and Operations to\n\nreduce the risk of production changes before they are made. Traditionally, when we review changes for deployment, we tend to\n\nrely heavily on reviews, inspections, and approvals just prior to deployment. Frequently those approvals are given by external teams\n\nwho are often too far removed from the work to make informed decisions on whether a change is risky or not, and the time required\n\nto get all the necessary approvals also lengthens our change lead\n\ntimes.\n\nThe peer review process at GitHub is a striking example of how\n\ninspection can increase quality, make deployments safe, and be\n\nintegrated into the flow of everyone’s daily work. They pioneered the process called pull request, one of the most popular forms of peer\n\nreview that span Dev and Ops.\n\nScott Chacon, CIO and co-founder of GitHub, wrote on his website\n\nthat pull requests are the mechanism that lets engineers tell others\n\nabout changes they have pushed to a repository on GitHub. Once a\n\npull request is sent, interested parties can review the set of changes,\n\ndiscuss potential modifications, and even push follow-up commits if\n\nnecessary. Engineers submitting a pull request will often request a\n\n“+1,” “+2,” or so forth, depending on how many reviews they need,\n\nor “@mention” engineers that they’d like to get reviews from.\n\nAt GitHub, pull requests are also the mechanism used to deploy code\n\ninto production through a collective set of practices they call\n\n“GitHub Flow”—it’s how engineers request code reviews, gather and\n\nintegrate feedback, and announce that code will be deployed to production (i.e., “master” branch).\n\nFigure 40: Comments and suggestions on a GitHub pull request (Source: Scott Chacon, “GitHub Flow,” ScottChacon.com, August 31, 2011, http://scottchacon.com/2011/08/31/github-flow.html.)\n\nGitHub Flow is composed of five steps:\n\n1. To work on something new, the engineer creates a descriptively\n\nnamed branch off of master (e.g., “new-oauth2-scopes”).\n\n2. The engineer commits to that branch locally, regularly pushing\n\ntheir work to the same named branch on the server.\n\n3. When they need feedback or help, or when they think the branch\n\nis ready for merging, they open a pull request.\n\n4. When they get their desired reviews and get any necessary\n\napprovals of the feature, the engineer can then merge it into\n\nmaster.\n\n5. Once the code changes are merged and pushed to master, the\n\nengineer deploys them into production.\n\nThese practices, which integrate review and coordination into daliy work, have allowed GitHub to quickly and reliably deliver features to\n\nmarket with high quality and security. For example, in 2012 they performed an amazing 12,602 deployments. In particular, on August 23rd, after a company-wide summit where many exciting ideas were brainstormed and discussed, the company had their busiest\n\ndeployment day of the year, with 563 builds and 175 successful deployments into production, all made possible through the pull request process.\n\nThroughout this chapter we will integrate practices, such as those used at GitHub, to shift our reliance away from periodic inspections and approvals, and moving to integrated peer review performed continually as a part of our daily work. Our goal is to ensure that\n\nDevelopment, Operations, and Infosec are continuously collaborating so that changes we make to our systems will operate\n\nreliably, securely, safely, and as designed.",
      "page_number": 363
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 371-378)",
      "start_page": 371,
      "end_page": 378,
      "detection_method": "topic_boundary",
      "content": "THE DANGERS OF CHANGE APPROVAL PROCESSES\n\nThe Knight Capital failure is one of the most prominent software\n\ndeployment errors in recent memory. A fifteen minute deployment\n\nerror resulted in a $440 million trading loss, during which the\n\nengineering teams were unable to disable the production services.\n\nThe financial losses jeopardized the firm’s operations and forced the\n\ncompany to be sold over the weekend so they could continue\n\noperating without jeopardizing the entire financial system.\n\nJohn Allspaw observed that when high-profile incidents occur, such\n\nas the Knight Capital deployment accident, there are typically two counterfactual narratives for why the accident occurred.†\n\nThe first narrative is that the accident was due to a change control failure, which seems valid because we can imagine a situation where better change control practices could have detected the risk earlier and prevented the change from going into production. And if we\n\ncouldn’t prevent it, we might have taken steps to enable faster detection and recovery.\n\nThe second narrative is that the accident was due to a testing failure. This also seems valid, with better testing practices we could have identified the risk earlier and canceled the risky deployment, or we could have at least taken steps to enable faster detection and recovery.\n\nThe surprising reality is that in environments with low-trust, command-and-control cultures, the outcomes of these types of change control and testing countermeasures often result in an increased likelihood that problems will occur again, potentially with even worse outcomes.\n\nGene Kim (co-author of this book) describes his realization that\n\nchange and testing controls can potentially have the opposite effect\n\nthan intended as “one of the most important moments of my\n\nprofessional career. This ‘aha’ moment was the result of a\n\nconversation in 2013 with John Allspaw and Jez Humble about the\n\nKnight Capital accident, making me question some of my core\n\nbeliefs that I’ve formed over the last ten years, especially having\n\nbeen trained as an auditor.”\n\nHe continues, “However upsetting it was, it was also a very\n\nformative moment for me. Not only did they convince me that they\n\nwere correct, we tested these beliefs in the 2014 State of DevOps\n\nReport, which led to some astonishing findings that reinforce that building high-trust cultures is likely the largest management challenge of this decade.”\n\nPOTENTIAL DANGERS OF “OVERLY CONTROLLING CHANGES”\n\nTraditional change controls can lead to unintended outcomes, such as contributing to long lead times, and reducing the strength and immediacy of feedback from the deployment process. In order to understand how this happens, let us examine the controls we often put in place when change control failures occur:\n\nAdding more questions that need to be answered to the change\n\nrequest form\n\nRequiring more authorizations, such as one more level of management approval (e.g., instead of merely the VP of Operations approving, we now require that the CIO also approve)\n\nor more stakeholders (e.g., network engineering, architecture\n\nreview boards, etc.)\n\nRequiring more lead time for change approvals so that change\n\nrequests can be properly evaluated\n\nThese controls often add more friction to the deployment process by\n\nmultiplying the number of steps and approvals, and increasing batch\n\nsizes and deployment lead times, which we know reduces the\n\nlikelihood of successful production outcomes for both Dev and Ops.\n\nThese controls also reduce how quickly we get feedback from our\n\nwork.\n\nOne of the core beliefs in the Toyota Production System is that “people closest to a problem typically know the most about it.” This becomes more pronounced as the work being performed and the system the work occurs in become more complex and dynamic, as is typical in DevOps value streams. In these cases, creating approval steps from people who are located further and further away from the work may actually reduce the likelihood of success. As has been\n\nproven time and again, the further the distance between the person doing the work (i.e., the change implementer) and the person deciding to do the work (i.e., the change authorizer), the worse the outcome.\n\nIn Puppet Labs’ 2014 State of DevOps Report, one of the key findings was that high-performing organizations relied more on peer review and less on external approval of changes. Figure 41 shows\n\nthat the more organizations rely on change approvals, the worse their IT performance in terms of both stability (mean time to restore service and change fail rate) and throughput (deployment lead times, deployment frequency).\n\nIn many organizations, change advisory boards serve an important\n\nrole in coordinating and governing the delivery process, but their job\n\nshould not be to manually evaluate every change, nor does ITIL\n\nmandate such a practice.\n\nTo understand why this is the case, consider the predicament of\n\nbeing on a change advisory board, reviewing a complex change\n\ncomposed of hundreds of thousands of lines of code changes, and\n\ncreated by hundreds of engineers.\n\nAt one extreme, we cannot reliably predict whether a change will be\n\nsuccessful either by reading a hundred-word description of the\n\nchange or by merely validating that a checklist has been completed. At the other extreme, painfully scrutinizing thousands of lines of code changes is unlikely to reveal any new insights. Part of this is the nature of making changes inside of a complex system. Even the engineers who work inside the codebase as part of their daily work are often surprised by the side effects of what should be low-risk changes.\n\nFigure 41: Organizations that rely on peer review outperform those with change approvals (Source: Puppet Labs, DevOps Survey Of Practice 2014)\n\nFor all these reasons, we need to create effective control practices that more closely resemble peer review, reducing our reliance on external bodies to authorize our changes. We also need to coordinate and schedule changes effectively. We explore both of these in the\n\nnext two sections.\n\nENABLE COORDINATION AND SCHEDULING OF CHANGES\n\nWhenever we have multiple groups working on systems that share dependencies, our changes will likely need to be coordinated to ensure that they don’t interfere with each other (e.g., marshaling, batching, and sequencing the changes). In general, the more loosely- coupled our architecture, the less we need to communicate and\n\ncoordinate with other component teams—when the architecture is\n\ntruly service-oriented, teams can make changes with a high degree of\n\nautonomy, where local changes are unlikely to create global\n\ndisruptions.\n\nHowever, even in a loosely-coupled architecture, when many teams\n\nare doing hundreds of independent deployments per day, there may\n\nbe a risk of changes interfering with each other (e.g., simultaneous\n\nA/B tests). To mitigate these risks, we may use chat rooms to\n\nannounce changes and proactively find collisions that may exist.\n\nFor more complex organizations and organizations with more\n\ntightly-coupled architectures, we may need to deliberately schedule\n\nour changes, where representatives from the teams get together, not to authorize changes, but to schedule and sequence their changes in order to minimize accidents.\n\nHowever, certain areas, such as global infrastructure changes (e.g., core network switch changes) will always have a higher risk associated with them. These changes will always require technical countermeasures, such as redundancy, failover, comprehensive testing, and (ideally) simulation.\n\nENABLE PEER REVIEW OF CHANGES\n\nInstead of requiring approval from an external body prior to deployment, we may require engineers to get peer reviews of their changes. In Development, this practice has been called code review, but it is equally applicable to any change we make to our applications or environments, including servers, networking, and databases.‡ The goal is to find errors by having fellow engineers close to the work scrutinize our changes. This review improves the\n\nquality of our changes, which also creates the benefits of cross-\n\ntraining, peer learning, and skill improvement.\n\nA logical place to require reviews is prior to committing code to\n\ntrunk in source control, where changes could potentially have a\n\nteam-wide or global impact. At a minimum, fellow engineers should\n\nreview our change, but for higher risk areas, such as database\n\nchanges or business-critical components with poor automated test\n\ncoverage, we may require further review from a subject matter\n\nexpert (e.g., information security engineer, database engineer) or\n\nmultiple reviews (e.g., “+2” instead of merely “+1”).\n\nThe principle of small batch sizes also applies to code reviews. The larger the size of the change that needs to be reviewed, the longer it takes to understand and the larger the burden on the reviewing engineer. As Randy Shoup observed, “There is a non-linear relationship between the size of the change and the potential risk of\n\nintegrating that change—when you go from a ten line code change to a one hundred line code, the risk of something going wrong is more than ten times higher, and so forth.” This is why it’s so essential for developers to work in small, incremental steps rather than on long- lived feature branches.\n\nFurthermore, our ability to meaningfully critique code changes goes down as the change size goes up. As Giray Özil tweeted, “Ask a programmer to review ten lines of code, he’ll find ten issues. Ask him to do five hundred lines, and he’ll say it looks good.”\n\nGuidelines for code reviews include:\n\nEveryone must have someone to review their changes (e.g., to the code, environment, etc.) before committing to trunk.\n\nEveryone should monitor the commit stream of their fellow team\n\nmembers so that potential conflicts can be identified and\n\nreviewed.\n\nDefine which changes qualify as high risk and may require review\n\nfrom a designated subject matter expert (e.g., database changes, security-sensitive modules such as authentication, etc.).§\n\nIf someone submits a change that is too large to reason about\n\neasily—in other words, you can’t understand its impact after\n\nreading through it a couple of times, or you need to ask the\n\nsubmitter for clarification—it should be split up into multiple,\n\nsmaller changes that can be understood at a glance.\n\nTo ensure that we are not merely rubber stamping reviews, we may also want to inspect the code review statistics to determine the number of proposed changes approved versus not approved, and perhaps sample and inspect specific code reviews.\n\nCode reviews come in various forms:\n\nPair programming: programmers work in pairs (see section\n\nbelow)\n\n“Over-the-shoulder”: One developer looks over the author’s shoulder as the latter walks through the code.\n\nEmail pass-around: A source code management system emails code to reviewers automatically after the code is checked in.\n\nTool-assisted code review: Authors and reviewers use specialized tools designed for peer code review (e.g., Gerrit, GitHub pull requests, etc.) or facilities provided by the source code repositories (e.g., GitHub, Mercurial, Subversion, as well as",
      "page_number": 371
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 379-386)",
      "start_page": 379,
      "end_page": 386,
      "detection_method": "topic_boundary",
      "content": "other platforms such as Gerrit, Atlassian Stash, and Atlassian\n\nCrucible).\n\nClose scrutiny of changes in many forms is effective in locating\n\nerrors previously overlooked. Code reviews can facilitate increased\n\ncode commits and production deployments, and support trunk-\n\nbased deployment and continuous delivery at scale, as we will see in\n\nthe following case study.\n\nCase Study\n\nCode Reviews at Google (2010)\n\nGoogle is an excellent example of a company that employees trunk-based development and continuous delivery at scale. As noted earlier in this book, Eran Messeri described that in 2013\n\nthe processes at Google enabled over thirteen thousand developers to work off of trunk on a single source code tree, performing over 5,500 code commits per week, resulting in hundreds of production deployments per week. In 2010, there were 20+ changes being checked in to trunk every minute, resulting in 50% of the codebase being changed every month.\n\nThis requires considerable discipline from Google team\n\nmembers and mandatory code reviews, which cover the following areas:\n\nCode readability for languages (enforces style guide)\n\nOwnership assignments for code sub-trees to maintain consistency and correctness\n\nCode transparency and code contributions across teams\n\nFigure 42 shows how code review lead times are affected by\n\nthe change size. On the x-axis is the size of the change, and\n\non the y-axis is the lead time required for code review process.\n\nIn general, the larger the change submitted for code reviews,\n\nthe longer the lead time required to get the necessary sign\n\noffs. And the data points in the upper-left corner represent the\n\nmore complex and potentially risky changes that required\n\nmore deliberation and discussion.\n\nFigure 42: Size of change vs. lead time for reviews at Google (Source: Ashish Kumar, “Development at the Speed and Scale of Google,” presentation at QCon, San Francisco, CA, 2010, https://qconsf.com/sf2010/dl/qcon-sanfran-\n\n2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf.)\n\nWhile he was working as a Google engineering director, Randy Shoup started a personal project to solve a technical problem that the organization was facing. He said, “I worked on that project for weeks and finally got around to asking a\n\nsubject matter expert to review my code. It was nearly three thousand lines of code, which took the reviewer days of work to go through. He told me, ‘Please don’t do that to me again.’ I was grateful that this engineer took the time to do that. That\n\nwas also when I learned how to make code reviews a part of\n\nmy daily work.”\n\nPOTENTIAL DANGERS OF DOING MORE MANUAL TESTING AND CHANGE FREEZES\n\nNow that we have created peer reviews that reduce our risk, shorten\n\nlead times associated with change approval processes, and enable\n\ncontinuous delivery at scale, such as we saw in the Google case\n\nstudy, let us examine the effects of how testing countermeasure can\n\nsometimes backfire. When testing failures occur, our typical reaction\n\nis to do more testing. However, if we are merely performing more testing at the end of the project, we may worsen our outcomes.\n\nThis is especially true if we are doing manual testing, because manual testing is naturally slower and more tedious than automated testing and performing “additional testing” often has the consequence of taking significantly longer to test, which means we are deploying less frequently, thus increasing our deployment batch size. And we know from both theory and practice that when we increase our deployment batch size, our change success rates go down and our incident counts and MTTR go up—the opposite of the outcome we want.\n\nInstead of performing testing on large batches of changes that are scheduled around change freeze periods, we want to fully integrate testing our daily work as part of the smooth and continual flow into production, and increase our deployment frequency. By doing this, we build in quality, which allows us to test, deploy, and release in ever smaller batch sizes.\n\nENABLE PAIR PROGRAMMING TO IMPROVE ALL OUR CHANGES\n\nPair programming is when two engineers work together at the same\n\nworkstation, a method popularized by Extreme Programming and\n\nAgile in the early 2000s. As with code reviews, this practice started\n\nin Development but is equally applicable to the work that any\n\nengineer does in our value stream. In this book, we will use the term\n\npairing and pair programming interchangeably, to indicate that the\n\npractice is not just for developers.\n\nIn one common pattern of pairing, one engineer fills the role of the\n\ndriver, the person who actually writes the code, while the other engineer acts as the navigator, observer, or pointer, the person who reviews the work as it is being performed. While reviewing, the observer may also consider the strategic direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the tactical aspects of completing the task, using the observer as a safety net and guide. When the two have differing specialties, skills are transferred as an automatic side effect, whether it’s through ad- hoc training or by sharing techniques and workarounds.\n\nAnother pair programming pattern reinforces test-driven development (TDD) by having one engineer write the automated test and the other engineer implement the code. Jeff Atwood, one of the founders of Stack Exchange, wrote, “I can’t help wondering if pair programming is nothing more than code review on steroids….The advantage of pair programming is its gripping immediacy: it is impossible to ignore the reviewer when he or she is sitting right next\n\nto you.”\n\nHe continued, “Most people will passively opt out [of reviewing\n\ncode] if given the choice. With pair programming, that’s not\n\npossible. Each half of the pair has to understand the code, right then\n\nand there, as it’s being written. Pairing may be invasive, but it can\n\nalso force a level of communication that you’d otherwise never\n\nachieve.”\n\nDr. Laurie Williams performed a study in 2001 that showed “paired\n\nprogrammers are 15% slower than two independent individual\n\nprogrammers, while ‘error-free’ code increased from 70% to 85%.\n\nSince testing and debugging are often many times more costly than\n\ninitial programming, this is an impressive result. Pairs typically\n\nconsider more design alternatives than programmers working alone and arrive at simpler, more maintainable designs; they also catch design defects early.” Dr. Williams also reported that 96% of her respondents stated that they enjoyed their work more when they programmed in pairs than when they programmed alone.¶\n\nPair programming has the additional benefit of spreading knowledge throughout the organization and increasing information flow within the team. Having more experienced engineers review while the less experienced engineer codes is also an effective way to teach and be taught.\n\nCase Study Pair Programming Replacing Broken Code Review Processes at Pivotal Labs (2011)\n\nElisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of Explore It!: Reduce Risk and Increase Confidence with Exploratory Testing, has spoken extensively about making every team responsible for their own quality, as\n\nopposed to making separate departments responsible. She\n\nargues that doing so not only increase quality, but significantly\n\nincreases the flow of work into production.\n\nIn her 2015 DevOps Enterprise Summit presentation, she\n\ndescribed how in 2011, there were two accepted methods of\n\ncode review at Pivotal: pair programming (which ensured that\n\nevery line of code was inspected by two people) or a code\n\nreview process that was managed by Gerrit (which ensured\n\nthat every code commit had two designated people “+1” the\n\nchange before it was allowed into trunk).\n\nThe problem Hendrickson observed with the Gerrit code review process was that it would often take an entire week for developers to receive their required reviews. Worse, skilled developers were experiencing the “frustrating and soul crushing experience of not being able to get simple changes into the codebase, because we had inadvertently created intolerable bottlenecks.”\n\nHendrickson lamented that “the only people who had the ability to ‘+1’ the changes were senior engineers, who had many other responsibilities and often didn’t care as much about the fixes the more junior developers were working on or their productivity. It created a terrible situation—while you were waiting for your changes to get reviewed, other developers were checking in their changes. So for a week, you would have to merge all their code changes onto your laptop, re-run all the tests to ensure that everything still worked, and (sometimes) you’d have to resubmit your changes for review again!”\n\nTo fix the problem and eliminate all of these delays, they\n\nended up dismantling the entire Gerrit code review process,\n\ninstead requiring pair programming to implement code\n\nchanges into the system. By doing this, they reduced the\n\namount of time required to get code reviewed from weeks to\n\nhours.\n\nHendrickson is quick to note that code reviews work fine in\n\nmany organizations, but it requires a culture that values\n\nreviewing code as highly as it values writing the code in the\n\nfirst place. When that culture is not yet in place, pair\n\nprogramming can serve as a valuable interim practice.\n\nEVALUATING THE EFFECTIVENESS OF PULL REQUEST PROCESSES\n\nBecause the peer review process is an important part of our control environment, we need to be able to determine whether it is working effectively or not. One method is to look at production outages and examine the peer review process for any relevant changes.\n\nAnother method comes from Ryan Tomayko, CIO and co-founder of GitHub and one of the inventors of the pull request process. When asked to describe the difference between a bad pull request and a good pull request, he said it has little to do with the production outcome. Instead, a bad pull request is one that doesn’t have enough context for the reader, having little or no documentation of what the change is intended to do. For example, a pull request that merely has the following text: “Fixing issue #3616 and #3841.”**\n\nThat was an actual internal GitHub pull request, which Tomayko critiqued, “This was probably written by a new engineer here. First off, no specific engineers were specifically @mentioned—at a\n\nminimum, the engineer should have mentioned their mentor or a\n\nsubject matter expert in the area that they’re modifying to ensure\n\nthat someone appropriate reviews their change. Worse, there isn’t\n\nany explanation of what the changes actually are, why it’s important,\n\nor exposing any of the implementer’s thinking.”\n\nOn the other hand, when asked to describe a great pull request that\n\nindicates an effective review process, Tomayko quickly listed off the\n\nessential elements: there must be sufficient detail on why the change\n\nis being made, how the change was made, as well as any identified\n\nrisks and resulting countermeasures.\n\nTomayko also looks for good discussion of the change, enabled by all the context that the pull request provided—often, there will be additional risks pointed out, ideas on better ways to implement the desired change, ideas on how to better mitigate the risk, and so\n\nforth. And if something bad or unexpected happens upon deployment, it is added to the pull request, with a link to the corresponding issue. All discussion happens without placing blame; instead, there is a candid conversation on how to prevent the problem from recurring in the future.\n\nAs an example, Tomayko produced another internal GitHub pull request for a database migration. It was many pages long, with lengthy discussions about the potential risks, leading up to the following statement by the pull request author: “I am pushing this now. Builds are now failing for the branch, because of a missing column in the CI servers. (Link to Post-Mortem: MySQL outage)”\n\nThe change submitter then apologized for the outage, describing what conditions and mistaken assumptions led to the accident, as well as a list of proposed countermeasures to prevent it from happening again. This was followed by pages and pages of",
      "page_number": 379
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 387-394)",
      "start_page": 387,
      "end_page": 394,
      "detection_method": "topic_boundary",
      "content": "discussion. Reading through the pull request, Tomayko smiled,\n\n“Now that is a great pull request.”\n\nAs described above, we can evaluate the effectiveness of our peer\n\nreview process by sampling and examining pull requests, either from\n\nthe entire population of pull requests or those that are relevant to\n\nproduction incidents.\n\nFEARLESSLY CUT BUREAUCRATIC PROCESSES\n\nSo far, we have discussed peer review and pair programming processes that enable us to increase the quality of our work without relying on external approvals for changes. However, many companies still have long-standing processes for approval that require months to navigate. These approval processes can significantly increase lead times, not only preventing us from delivering value quickly to customers, but potentially increasing the risk to our organizational objectives. When this happens, we must re-engineer our processes so that we can achieve our goals more quickly and safely.\n\nAs Adrian Cockcroft observed, “A great metric to publish widely is how many meetings and work tickets are mandatory to perform a release—the goal is to relentlessly reduce the effort required for engineers to perform work and deliver it to the customer.”\n\nSimilarly, Dr. Tapabrata Pal, technical fellow at Capital One, described a program at Capital One called Got Goo?, which involves a dedicated team removing obstacles—including tools, processes, and approvals—that impede work completion. Jason Cox, Senior Director of Systems Engineering at Disney, described in his\n\npresentation at the DevOps Enterprise Summit in 2015 a program\n\ncalled Join The Rebellion that aimed to remove toil and obstacles\n\nfrom daily work.\n\nAt Target in 2012, a combination of the Technology Enterprise\n\nAdoption Process and Lead Architecture Review Board (TEAP-LARB\n\nprocess) resulted in complicated, long approval times for anyone\n\nattempting to bring in new technology. The TEAP form needed to be\n\nfilled out by anyone wanting to propose new technologies to be\n\nadopted, such as a new database or monitoring technologies. These\n\nproposals were evaluated, and those deemed appropriate were put\n\nonto the monthly LARB meeting agenda.\n\nHeather Mickman and Ross Clanton, Director of Development and Director of Operations at Target, Inc., respectively, were helping to lead the DevOps movement at Target. During their DevOps initiative, Mickman had identified a technology needed to enable an initiative from the lines of business (in this case, Tomcat and Cassandra). The decision from the LARB was that Operations could not support it at the time. However, because Mickman was so convinced that this technology was essential, she proposed that her Development team be responsible for service support as well as integration, availability, and security, instead of relying on the Operations team.\n\n“As we went through the process, I wanted to better understand why the TEAP-LARB process took so long to get through, and I used the technique of ‘the five why’s’....Which eventually led to the question of why TEAP-LARB existed in the first place. The surprising thing was that no one knew, outside of a vague notion that we needed some sort of governance process. Many knew that there had been some sort of disaster that could never happen again years ago, but\n\nno one could remember exactly what that disaster was, either,”\n\nMickman observed.\n\nMickman concluded that this process was not necessary for her\n\ngroup if they were responsible for the operational responsibilities of\n\nthe technology she was introducing. She added, “I let everyone know\n\nthat any future technologies that we would support wouldn’t have to\n\ngo through the TEAP-LARB process, either.”\n\nThe outcome was that Cassandra was successfully introduced inside\n\nTarget and eventually widely adopted. Furthermore, the TEAP-\n\nLARB process was eventually dismantled. Out of appreciation, her\n\nteam awarded Mickman the Lifetime Achievement Award for removing barriers to get technology work done within Target.\n\nCONCLUSION\n\nIn this chapter, we discussed how to integrate practices into our daily work that increase the quality of our changes and reduce the risk of poor deployment outcomes, reducing our reliance on approval processes. Case studies from GitHub and Target show that these practices not only improve our outcomes, but also significantly reduce lead times and increase developer productivity. To do this kind of work requires a high-trust culture.\n\nConsider a story that John Allspaw told about a newly hired junior engineer: The engineer asked if it was okay to deploy a small HTML change, and Allspaw responded, “I don’t know, is it?” He then asked “Did you have someone review your change? Do you know who the best person to ask is for changes of this type? Did you do everything you absolutely could to assure yourself that this change operates in\n\nproduction as designed? If you did, then don’t ask me—just make\n\nthe change!”\n\nBy responding this way, Allspaw reminded the engineer that she was\n\nsolely responsibility for the quality of her change—if she did\n\neverything she felt she could to give herself confidence that the\n\nchange would work, then she didn’t need to ask anyone for approval,\n\nshe should make the change.\n\nCreating the conditions that enable change implementers to fully\n\nown the quality of their changes is an essential part of the high-trust,\n\ngenerative culture we are striving to build. Furthermore, these\n\nconditions enable us to create an ever-safer system of work, where we are all helping each other achieve our goals, spanning whatever boundaries necessary to get there.\n\nPART IV CONCLUSION\n\nPart IV has shown us that by implementing feedback loops we can\n\nenable everyone to work together toward shared goals, see problems\n\nas they occur, and, with quick detection and recovery, ensure that\n\nfeatures not only operate as designed in production, but also achieve\n\norganizational goals and organizational learning. We have also\n\nexamined how to enable shared goals spanning Dev and Ops so that\n\nthey can improve the health of the entire value stream.\n\nWe are now ready to enter Part V: The Third Way, The Technical\n\nPractices of Learning, so we can create opportunities for learning that happen earlier and ever more quickly and cheaply, and so that we can unleash a culture of innovation and experimentation that enables everyone to do meaningful work that helps our organization succeed.\n\n† Counterfactual thinking is a term used in psychology that involves the human tendency to create possible\n\nalternatives to life events that have already occurred. In reliability engineering, it often involves narratives of the “system as imagined” as opposed to the “system in reality.”\n\n‡ In this book, the terms code review and change review will be used interchangeably.\n\n§ Incidentally, a list of high-risk areas of code and environments has likely already been created by the change\n\nadvisory board.\n\n¶ Some organizations may require pair programming, while in others, engineers find someone to pair program with\n\nwhen working in areas where they want more scrutiny (such as before checking in) or for challenging tasks. Another common practice is to set pairing hours for a subset of the working day, perhaps four hours from mid- morning to mid-afternoon.\n\n** Gene Kim is grateful to Shawn Davenport, James Fryman, Will Farr, and Ryan Tomayko at GitHub for discussing\n\nthe differences between good and bad pull requests.\n\nPart\n\nIntroduction\n\nIn Part III, The First Way: The Technical Practices of Flow, we\n\ndiscussed implementing the practices required to create fast flow\n\nin our value stream. In Part IV, The Second Way: The Technical\n\nPractices of Feedback, our goal was to create as much feedback as possible, from as many areas in our system as possible—sooner,\n\nfaster, and cheaper.\n\nIn Part V, The Third Way: The Technical Practices of Learning, we present the practices that create opportunities for learning, as\n\nquickly, frequently, cheaply, and as soon as possible. This includes\n\ncreating learnings from accidents and failures, which are inevitable when we work within complex systems, as well as\n\norganizing and designing our systems of work so that we are constantly experimenting and learning, continually making our\n\nsystems safer. The results include higher resilience and an ever- growing collective knowledge of how our system actually works, so\n\nthat we are better able to achieve our goals.\n\nIn the following chapters, we will institutionalize rituals that\n\nincrease safety, continuous improvement, and learning by doing the following:\n\nEstablish a just culture to make safety possible\n\nInject production failures to create resilience\n\nConvert local discoveries into global improvements\n\nReserve time to create organizational improvements and\n\nlearning\n\nWe will also create mechanisms so that any new learnings\n\ngenerated in one area of the organization can be rapidly used\n\nacross the entire organization, turning local improvements into\n\nglobal advancements. In this way, we not only learn faster than\n\nour competition, helping us win in the marketplace, but also\n\ncreate a safer, more resilient work culture that people are excited to be a part of and that helps them achieve their highest potential.\n\n19Enable and Inject Learning into Daily Work\n\nWhen we work within a complex system, it is impossible for us to\n\npredict all the outcomes for the actions we take. This contributes\n\nto unexpected and sometimes catastrophic accidents, even when\n\nwe use static precautionary tools, such as checklists and runbooks,\n\nwhich codify our current understanding of the system.\n\nTo enable us to safely work within complex systems, our\n\norganizations must become ever better at self-diagnostics and self-\n\nimprovement and must be skilled at detecting problems, solving them, and multiplying the effects by making the solutions\n\navailable throughout the organization. This creates a dynamic system of learning that allows us to understand our mistakes and\n\ntranslate that understanding into actions that prevent those\n\nmistakes from recurring in the future.\n\nThe result is what Dr. Steven Spear describes as resilient organizations, who are “skilled at detecting problems, solving them, and multiplying the effect by making the solutions available\n\nthroughout the organization.” These organizations can heal themselves. “For such an organization, responding to crises is not idiosyncratic work. It is something that is done all the time. It is this responsiveness that is their source of reliability.”",
      "page_number": 387
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 395-403)",
      "start_page": 395,
      "end_page": 403,
      "detection_method": "topic_boundary",
      "content": "A striking example of the incredible resilience that can result from\n\nthese principles and practices was seen on April 21, 2011, when the\n\nentire Amazon AWS US-EAST availability zone went down, taking\n\ndown virtually all of their customers who depended on it, including Reddit and Quora.† However, Netflix was a surprising exception, seemingly unaffected by this massive AWS outage.\n\nFollowing the event, there was considerable speculation about\n\nhow Netflix kept their services running. A popular theory was\n\nsince Netflix was one of the largest customers of Amazon Web\n\nServices, it was given some special treatment that allowed them to\n\nkeep running. However, a Netflix Engineering blog post explained that it was their architectural design decisions in 2009 enabled their exceptional resilience.\n\nBack in 2008, Netflix’s online video delivery service ran on a monolithic J2EE application hosted in one of their data centers. However, starting in 2009, they began re-architecting this system to be what they called cloud native—it was designed to run entirely in the Amazon public cloud and to be resilient enough to survive significant failures.\n\nOne of their specific design objectives was to ensure Netflix\n\nservices kept running, even if an entire AWS availability zone went\n\ndown, such as happened with US-EAST. To do this required that their system be loosely-coupled, with each component having aggressive timeouts to ensure that failing components didn’t bring the entire system down.Instead, each feature and component was designed to gracefully degrade. For example, during traffic surges that created CPU-usage spikes, instead of showing a list of movies\n\npersonalized to the user, they would show static content, such as\n\ncached or un-personalized results, which required less\n\ncomputation.\n\nFurthermore, the blog post explained that, in addition to\n\nimplementing these architectural patterns, they also built and had\n\nbeen running a surprising and audacious service called Chaos\n\nMonkey, which simulated AWS failures by constantly and\n\nrandomly killing production servers. They did so because they\n\nwanted all “engineering teams to be used to a constant level of\n\nfailure in the cloud” so that services could “automatically recover\n\nwithout any manual intervention.”\n\nIn other words, the Netflix team ran Chaos Monkey to gain assurance that they had achieved their operational resilience objectives, constantly injecting failures into their pre-production and production environments.\n\nAs one might expect, when they first ran Chaos Monkey in their production environments, services failed in ways they never could\n\nhave predicted or imagined—by constantly finding and fixing these issues during normal working hours, Netflix engineers quickly and iteratively created a more resilient service, while simultaneously creating organizational learnings (during normal\n\nworking hours!) that enabled them to evolve their systems far beyond their competition.\n\nChaos Monkey is just one example of how learning can be integrated into daily work. The story also shows how learning\n\norganizations think about failures, accidents, and mistakes—as an opportunity for learning and not something to be punished. This chapter explores how to create a system of learning and how to\n\nestablish a just culture, as well as how to routinely rehearse and deliberately create failures to accelerate learning.\n\nESTABLISH A JUST, LEARNING CULTURE\n\nOne of the prerequisites for a learning culture is that when accidents occur (which they undoubtedly will), the response to\n\nthose accidents is seen as “just.” Dr. Sidney Dekker, who helped codify some of the key elements of safety culture and coined the\n\nterm just culture, writes, “When responses to incidents and accidents are seen as unjust, it can impede safety investigations,\n\npromoting fear rather than mindfulness in people who do safety-\n\ncritical work, making organizations more bureaucratic rather than more careful, and cultivating professional secrecy, evasion, and\n\nself-protection.”\n\nThis notion of punishment is present, either subtly or prominently, in the way many managers have operated during the\n\nlast century. The thinking goes, in order to achieve the goals of the organization, leaders must command, control, establish\n\nprocedures to eliminate errors, and enforce compliance of those\n\nprocedures.\n\nDr. Dekker calls this notion of eliminating error by eliminating the\n\npeople who caused the errors the Bad Apple Theory. He asserts\n\nthat this is invalid, because “human error is not our cause of troubles; instead, human error is a consequence of the design of\n\nthe tools that we gave them.”\n\nIf accidents are not caused by “bad apples,” but rather are due to inevitable design problems in the complex system that we created,\n\nthen instead of “naming, blaming, and shaming” the person who caused the failure, our goal should always be to maximize\n\nopportunities for organizational learning, continually reinforcing that we value actions that expose and share more widely the\n\nproblems in our daily work. This is what enables us to improve the quality and safety of the system we operate within and reinforce\n\nthe relationships between everyone who operates within that\n\nsystem.\n\nBy turning information into knowledge and building the results of the learning into our systems, we start to achieve the goals of a\n\njust culture, balancing the needs for safety and accountability. As John Allspaw, CTO of Etsy, states, “Our goal at Etsy is to view\n\nmistakes, errors, slips, lapses, and so forth with a perspective of learning.”\n\nWhen engineers make mistakes and feel safe when giving details\n\nabout it, they are not only willing to be held accountable, but they\n\nare also enthusiastic in helping the rest of the company avoid the same error in the future. This is what creates organizational\n\nlearning. On the other hand, if we punish that engineer, everyone is dis-incentivized to provide the necessary details to get an\n\nunderstanding of the mechanism, pathology, and operation of the failure, which guarantees that the failure will occur again.\n\nTwo effective practices that help create a just, learning-based\n\nculture are blameless post-mortems and the controlled\n\nintroduction of failures into production to create opportunities to practice for the inevitable problems that arise within complex\n\nsystems. We will first look at blameless post-mortems and follow that with an exploration of why failure can be a good thing.\n\nSCHEDULE BLAMELESS POST-MORTEM MEETINGS AFTER ACCIDENTS OCCUR\n\nTo help enable a just culture, when accidents and significant\n\nincidents occur (e.g., failed deployment, production issue that affected customers), we should conduct a blameless post-mortem after the incident has been resolved.‡ Blameless post-mortems, a term coined by John Allspaw, help us examine “mistakes in a way that focuses on the situational aspects of a failure’s mechanism\n\nand the decision-making process of individuals proximate to the failure.”\n\nTo do this, we schedule the post-mortem as soon as possible after\n\nthe accident occurs and before memories and the links between cause and effect fade or circumstances change. (Of course, we wait\n\nuntil after the problem has been resolved so as not to distract the\n\npeople who are still actively working on the issue.)\n\nIn the blameless post-mortem meeting, we will do the following:\n\nConstruct a timeline and gather details from multiple\n\nperspectives on failures, ensuring we don’t punish people for\n\nmaking mistakes\n\nEmpower all engineers to improve safety by allowing them to give detailed accounts of their contributions to failures\n\nEnable and encourage people who do make mistakes to be the experts who educate the rest of the organization on how not to\n\nmake them in the future\n\nAccept that there is always a discretionary space where humans can decide to take action or not, and that the judgment of those\n\ndecisions lies in hindsight\n\nPropose countermeasures to prevent a similar accident from happening in the future and ensure these countermeasures are\n\nrecorded with a target date and an owner for follow-up\n\nTo enable us to gain this understanding, the following\n\nstakeholders need to be present at the meeting:\n\nThe people involved in decisions that may have contributed to the problem\n\nThe people who identified the problem\n\nThe people who responded to the problem\n\nThe people who diagnosed the problem\n\nThe people who were affected by the problem\n\nAnd anyone else who is interested in attending the meeting.\n\nOur first task in the blameless post-mortem meeting is to record our best understanding of the timeline of relevant events as they\n\noccurred. This includes all actions we took and what time (ideally supported by chat logs, such as IRC or Slack), what effects we observed (ideally in the form of the specific metrics from our\n\nproduction telemetry, as opposed to merely subjective narratives), all investigation paths we followed, and what resolutions were considered.\n\nTo enable these outcomes, we must be rigorous about recording details and reinforcing a culture that information can be shared\n\nwithout fear of punishment or retribution. Because of this, especially for our first few post-mortems, it may be helpful to have the meeting led by a trained facilitator who wasn’t involved in the\n\naccident.\n\nDuring the meeting and the subsequent resolution, we should explicitly disallow the phrases “would have” or “could have,” as\n\nthey are counterfactual statements that result from our human tendency to create possible alternatives to events that have already occurred.\n\nCounterfactual statements, such as “I could have...” or “If I had known about that, I should have…,” frame the problem in terms of the system as imagined instead of in terms of the system that actually exists, which is the context we need to restrict ourselves\n\nto. See Appendix 8.\n\nOne of the potentially surprising outcomes of these meetings is\n\nthat people will often blame themselves for things outside of their control or question their own abilities. Ian Malpass, an engineer at Etsy observes, “In that moment when we do something that causes the entire site to go down, we get this ‘ice water down the spine’\n\nfeeling, and likely the first thought through our head is, ‘I suck and I have no idea what I’m doing.’ We need to stop ourselves from doing that, as it is route to madness, despair, and feelings of being\n\nan imposter, which is something that we can’t let happen to good engineers. The better question to focus on is, ‘Why did it make sense to me when I took that action?’”\n\nIn the meeting, we must reserve enough time for brainstorming and deciding which countermeasures to implement. Once the\n\ncountermeasures have been identified, they must be prioritized and given an owner and timeline for implementation. Doing this further demonstrates that we value improvement of our daily work more than daily work itself.\n\nDan Milstein, one of the principal engineers at Hubspot, writes that he begins all blameless post-mortem meetings “by saying, ‘We’re trying to prepare for a future where we’re as stupid as we\n\nare today.’” In other words, it is not acceptable to have a countermeasure to merely “be more careful” or “be less stupid”— instead, we must design real countermeasures to prevent these\n\nerrors from happening again.\n\nExamples of such countermeasures include new automated tests to detect dangerous conditions in our deployment pipeline, adding\n\nfurther production telemetry, identifying categories of changes that require additional peer review, and conducting rehearsals of this category of failure as part of regularly scheduled Game Day\n\nexercises.\n\nPUBLISH OUR POST-MORTEMS AS WIDELY AS POSSIBLE\n\nAfter we conduct a blameless post-mortem meeting, we should widely announce the availability of the meeting notes and any\n\nassociated artifacts (e.g., timelines, IRC chat logs, external communications). This information should (ideally) be placed in a centralized location where our entire organization can access it\n\nand learn from the incident. Conducting post-mortems is so\n\nimportant that we may even prohibit production incidents from being closed until the post-mortem meeting has been completed.\n\nDoing this helps us translate local learnings and improvements into global learnings and improvements. Randy Shoup, former engineering director for Google App Engine, describes how\n\ndocumentation of post-mortem meetings can have tremendous value to others in the organization, “As you can imagine at Google, everything is searchable. All the post-mortem documents are in places where other Googlers can see them. And trust me, when\n\nany group has an incident that sounds similar to something that happened before, these post-mortem documents are among the first documents being read and studied.”§\n\nWidely publishing post-mortems and encouraging others in the organization to read them increases organizational learning, and it also becoming increasingly commonplace for online service\n\ncompanies to publish post-mortems for customer-impacting outages. This often significantly increases the transparency we have with our internal and external customers, which in turn increases their trust in us.\n\nThis desire to conduct as many blameless post-mortem meetings as necessary at Etsy led to some problems—over the course of four\n\nyears, Etsy accumulated a large number of post-mortem meeting notes in wiki pages, which became increasingly difficult to search, save, and collaborate from.\n\nTo help with this issue, they developed a tool called Morgue to easily record aspects of each accident, such as the incident MTTR and severity, better address time zones (which became relevant as more Etsy employees were working remotely), and include other",
      "page_number": 395
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 404-412)",
      "start_page": 404,
      "end_page": 412,
      "detection_method": "topic_boundary",
      "content": "data, such as rich text in Markdown format, embedded images, tags, and history.\n\nMorgue was designed to make it easy for the team to record:\n\nWhether the problem was due to a scheduled or an unscheduled incident\n\nThe post-mortem owner\n\nRelevant IRC chat logs (especially important for 3 a.m. issues\n\nwhen accurate note-taking may not happen)\n\nRelevant JIRA tickets for corrective actions and their due dates (information particularly important to management)\n\nLinks to customer forum posts (where customers complain about issues)\n\nAfter developing and using Morgue, the number of recorded post- mortems at Etsy increased significantly compared to when they used wiki pages, especially for P2, P3, and P4 incidents (i.e., lower\n\nseverity problems). This result reinforced the hypothesis that if they made it easier to document post-mortems through tools such as Morgue, more people would record and detail the outcomes of their post-mortem meetings, enabling more organizational\n\nlearning.\n\nDr. Amy C. Edmondson, Novartis Professor of Leadership and Management at Harvard Business School and co-author of\n\nBuilding the Future: Big Teaming for Audacious Innovation, writes:\n\nAgain, the remedy—which does not necessarily involve much time and expense—is to reduce the stigma of failure. Eli Lilly has done this since the early 1990s by holding ‘failure parties’\n\nto honor intelligent, high-quality scientific experiments that fail to achieve the desired results. The parties don’t cost much, and redeploying valuable resources—particularly scientists—to new projects earlier rather than later can save hundreds of\n\nthousands of dollars, not to mention kickstart potential new discoveries.\n\nDECREASE INCIDENT TOLERANCES TO FIND EVER-WEAKER FAILURE SIGNALS\n\nInevitably, as organizations learn how to see and solve problems efficiently, they need to decrease the threshold of what constitutes a problem in order to keep learning. To do this, we seek to amplify weak failure signals. For example, as described in chapter 4, when\n\nAlcoa was able to reduce the rate of workplace accidents so that they were no longer commonplace, Paul O’Neill, CEO of Alcoa, started to be notified of accident near-misses in addition to actual\n\nworkplace accidents.\n\nDr. Spear summarizes O’Neill’s accomplishments at Alcoa when he writes, “Though it started by focusing on problems related to\n\nworkplace safety, it soon found that safety problems reflected process ignorance and that this ignorance would also manifest\n\nitself in other problems such as quality, timeliness, and yield\n\nversus scrap.”\n\nWhen we work within complex systems, this need to amplify weak\n\nfailure signals is critical to averting catastrophic failures. The way\n\nNASA handled failure signals during the space shuttle era serves\n\nas an illustrative example: In 2003, sixteen days into the\n\nColumbia space shuttle mission, it exploded as it re-entered the earth’s atmosphere. We now know that a piece of insulating foam\n\nhad broken off the external fuel tank during takeoff.\n\nHowever, prior to Columbia’s re-entry, a handful of mid-level NASA engineers had reported this incident, but their voices had\n\ngone unheard. They observed the foam strike on video monitors\n\nduring a post-launch review session and immediately notified NASA’s managers, but they were told that the foam issue was\n\nnothing new. Foam dislodgement had damaged shuttles in\n\nprevious launches, but had never resulted in an accident. It was considered a maintenance problem and not acted upon until it was\n\ntoo late.\n\nMichael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson wrote in a 2006 article for Harvard Business Review how NASA\n\nculture contributed to this problem. They describe how organizations are typically structured in one of two models: a\n\nstandardized model, where routine and systems govern\n\neverything, including strict compliance with timelines and budgets, or an experimental model, where every day every\n\nexercise and every piece of new information is evaluated and\n\ndebated in a culture that resembles a research and design (R&D) laboratory.\n\nThey observe, “Firms get into trouble when they apply the wrong\n\nmind-set to an organization [which dictates how they respond to ambiguous threats or, in the terminology of this book, weak\n\nfailure signals]....By the 1970s, NASA had created a culture of\n\nrigid standardization, promoting to Congress the space shuttle as\n\na cheap and reusable spacecraft.” NASA favored strict process\n\ncompliance instead of an experimental model where every piece of\n\ninformation needed to be evaluated as it occured without bias. The absence of continuous learning and experimentation had dire\n\nconsequences. The authors conclude that it is culture and mind-\n\nset that matters, not just “being careful”—as they write, “vigilance alone will not prevent ambiguous threats [weak failure signals]\n\nfrom turning into costly (and sometimes tragic) failures.”\n\nOur work in the technology value stream, like space travel, should be approached as a fundamentally experimental endeavor and\n\nmanaged that way. All work we do is a potentially important hypothesis and a source of data, rather than a routine application\n\nand validation of past practice. Instead of treating technology\n\nwork as entirely standardized, where we strive for process compliance, we must continually seek to find ever-weaker failure\n\nsignals so that we can better understand and manage the system\n\nwe operate in.\n\nREDEFINE FAILURE AND ENCOURAGE CALCULATED RISK-TAKING\n\nLeaders of an organization, whether deliberately or inadvertently,\n\nreinforce the organizational culture and values through their\n\nactions. Audit, accounting, and ethics experts have long observed that the “tone at the top” predicts the likelihood of fraud and other\n\nunethical practices. To reinforce our culture of learning and\n\ncalculated risk-taking, we need leaders to continually reinforce that everyone should feel both comfortable with and responsible\n\nfor surfacing and learning from failures.\n\nOn failures, Roy Rapoport from Netflix observes, “What the 2014\n\nState of DevOps Report proved to me is that high-performing\n\nDevOps organizations will fail and make mistakes more often. Not only is this okay, it’s what organizations need! You can even see it\n\nin the data: if high performers are performing thirty times more frequently but with only half the change failure rate, they’re\n\nobviously having more failures.”\n\nHe continues, “I was talking with a co-worker about a massive\n\noutage we just had at Netflix—it was caused by, frankly, a dumb mistake. In fact, it was caused by an engineer who had taken down\n\nNetflix twice in the last eighteen months. But, of course, this is a person we’d never fire. In that same eighteen months, this\n\nengineer moved the state of our operations and automation\n\nforward not by miles but by light-years. That work has enabled us to do deployments safely on a daily basis, and has personally\n\nperformed huge numbers of production deployments.”\n\nHe concludes, “DevOps must allow this sort of innovation and the resulting risks of people making mistakes. Yes, you’ll have more\n\nfailures in production. But that’s a good thing, and should not be\n\npunished.”\n\nINJECT PRODUCTION FAILURES TO ENABLE RESILIENCE AND LEARNING\n\nAs we saw in the chapter introduction, injecting faults into the\n\nproduction environment (such as Chaos Monkey) is one way we\n\ncan increase our resilience. In this section, we describe the processes involved in rehearsing and injecting failures into our\n\nsystem to confirm that we have designed and architected our\n\nsystems properly, so that failures happen in specific and\n\ncontrolled ways. We do this by regularly (or even continuously)\n\nperforming tests to make certain that our systems fail gracefully.\n\nAs Michael Nygard, author of Release It! Design and Deploy\n\nProduction-Ready Software, comments, “Like building crumple\n\nzones into cars to absorb impacts and keep passengers safe, you can decide what features of the system are indispensable and build\n\nin failure modes that keep cracks away from those features. If you do not design your failure modes, then you will get whatever\n\nunpredictable—and usually dangerous—ones happen to emerge.”\n\nResilience requires that we first define our failure modes and then\n\nperform testing to ensure that these failure modes operate as designed. One way we do this is by injecting faults into our\n\nproduction environment and rehearsing large-scale failures so we are confident we can recover from accidents when they occur,\n\nideally without even impacting our customers.\n\nThe 2012 story about Netflix and the Amazon AWS-EAST outage presented in the introduction is just one example. An even more\n\ninteresting example of resilience at Netflix was during the “Great\n\nAmazon Reboot of 2014,” when nearly 10% of the entire Amazon EC2 server fleet had to be rebooted to apply an emergency Xen\n\nsecurity patch. As Christos Kalantzis of Netflix Cloud Database\n\nEngineering recalled, “When we got the news about the emergency EC2 reboots, our jaws dropped. When we got the list of how many\n\nCassandra nodes would be affected, I felt ill.”But, Kalantzis\n\ncontinues, “Then I remembered all the Chaos Monkey exercises we’ve gone through. My reaction was, ‘Bring it on!’”\n\nOnce again, the outcomes were astonishing. Of the 2,700+\n\nCassandra nodes used in production, 218 were rebooted, and twenty-two didn’t reboot successfully. As Kalantzis and Bruce\n\nWong from Netflix Chaos Engineering wrote, “Netflix experienced\n\n0 downtime that weekend. Repeatedly and regularly exercising failure, even in the persistence [database] layer, should be part of\n\nevery company’s resilience planning. If it wasn’t for Cassandra’s\n\nparticipation in Chaos Monkey, this story would have ended much differently.”\n\nEven more surprising, not only was no one at Netflix working\n\nactive incidents due to failed Cassandra nodes, no one was even in the office—they were in Hollywood at a party celebrating an\n\nacquisition milestone. This is another example demonstrating that\n\nproactively focusing on resilience often means that a firm can handle events that may cause crises for most organizations in a manner that is routine and mundane.¶ See Appendix 9.\n\nINSTITUTE GAME DAYS TO REHEARSE FAILURES\n\nIn this section, we describe specific disaster recovery rehearsals called Game Days, a term popularized by Jesse Robbins, one of\n\nthe founders of the Velocity Conference community and co-\n\nfounder of Chef, for the work he did at Amazon, where he was responsible for programs to ensure site availability and was widely\n\nknown internally as the “Master of Disaster.”The concept of Game\n\nDays comes from the discipline of resilience engineering. Robbins defines resilience engineering as “an exercise designed to increase\n\nresilience through large-scale fault injection across critical\n\nsystems.”\n\nRobbins observes that “whenever you set out to engineer a system at scale, the best you can hope for is to build a reliable software\n\nplatform on top of components that are completely unreliable. That puts you in an environment where complex failures are both\n\ninevitable and unpredictable.”\n\nConsequently, we must ensure that services continue to operate when failures occur, potentially throughout our system, ideally\n\nwithout crisis or even manual intervention. As Robbins quips, “a\n\nservice is not really tested until we break it in production.”\n\nOur goal for Game Day is to help teams simulate and rehearse accidents to give them the ability to practice. First, we schedule a\n\ncatastrophic event, such as the simulated destruction of an entire data center, to happen at some point in the future. We then give\n\nteams time to prepare, to eliminate all the single points of failure,\n\nand to create the necessary monitoring procedures, failover procedures, etc.\n\nOur Game Day team defines and executes drills, such as\n\nconducting database failovers (i.e., simulating a database failure and ensuring that the secondary database works) or turning off an\n\nimportant network connection to expose problems in the defined\n\nprocesses. Any problems or difficulties that are encountered are identified, addressed, and tested again.\n\nAt the scheduled time, we then execute the outage. As Robbins\n\ndescribes, at Amazon they “would literally power off a facility—\n\nwithout notice—and then let the systems fail naturally and [allow]\n\nthe people to follow their processes wherever they led.”\n\nBy doing this, we start to expose the latent defects in our system,\n\nwhich are the problems that appear only because of having\n\ninjected faults into the system. Robbins explains, “You might discover that certain monitoring or management systems crucial\n\nto the recovery process end up getting turned off as part of the\n\nfailure you’ve orchestrated. [Or] you would find some single points of failure you didn’t know about that way.” These exercises\n\nare then conducted in an increasingly intense and complex way\n\nwith the goal of making them feel like just another part of an average day.\n\nBy executing Game Days, we progressively create a more resilient\n\nservice and a higher degree of assurance that we can resume operations when inopportune events occur, as well create more\n\nlearnings and a more resilient organization.\n\nAn excellent example of simulating disaster is Google’s Disaster Recovery Program (DiRT). Kripa Krishnan is a technical program\n\ndirector at Google, and, at the time of this writing, has led the\n\nprogram for over seven years. During that time, they’ve simulated an earthquake in Silicon Valley, which resulted in the entire\n\nMountain View campus being disconnected from Google; major\n\ndata centers having complete loss of power; and even aliens attacking cities where engineers resided.\n\nAs Krishnan wrote, “An often-overlooked area of testing is\n\nbusiness process and communications. Systems and processes are highly intertwined, and separating testing of systems from testing\n\nof business processes isn’t realistic: a failure of a business system",
      "page_number": 404
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 413-420)",
      "start_page": 413,
      "end_page": 420,
      "detection_method": "topic_boundary",
      "content": "will affect the business process, and conversely a working system\n\nis not very useful without the right personnel.”\n\nSome of the learnings gained during these disasters included:\n\nWhen connectivity was lost, the failover to the engineer\n\nworkstations didn’t work\n\nEngineers didn’t know how to access a conference call bridge or the bridge only had capacity for fifty people or they needed a\n\nnew conference call provider who would allow them to kick off\n\nengineers who had subjected the entire conference to hold music\n\nWhen the data centers ran out of diesel for the backup\n\ngenerators, no one knew the procedures for making emergency purchases through the supplier, resulting in someone using a\n\npersonal credit card to purchase $50,000 worth of diesel.\n\nBy creating failure in a controlled situation, we can practice and create the playbooks we need. One of the other outputs of Game\n\nDays is that people actually know who to call and know who to talk\n\nto—by doing this, they develop relationships with people in other departments so they can work together during an incident,\n\nturning conscious actions into unconscious actions that are able to become routine.\n\nCONCLUSION\n\nTo create a just culture that enables organizational learning, we\n\nhave to re-contextualize so-called failures. When treated properly, errors that are inherent in complex systems can create a dynamic\n\nlearning environment where all of the shareholders feel safe\n\nenough to come forward with ideas and observations, and where groups rebound more readily from projects that don’t perform as\n\nexpected.\n\nBoth blameless post-mortems and injecting production failures reinforce a culture that everyone should feel both comfortable\n\nwith and responsible for surfacing and learning from failures. In\n\nfact, when we sufficiently reduce the number of accidents, we decrease our tolerance so that we can keep learning. As Peter\n\nSenge is known to say, “The only sustainable competitive\n\nadvantage is an organization’s ability to learn faster than the competition.”\n\n† In January 2013 at re:Invent, James Hamilton, VP and Distinguished Engineer for Amazon Web Services said that the US East region had more than ten data centers all by itself, and added that a typical data center has between fifty thousand and eighty thousand servers. By this math, the 2011 EC2 outage affected customers on more than half a million servers.\n\n‡ This practice has also been called blameless post-incident reviews as well as post-event retrospectives. There is\n\nalso a noteworthy similarity to the routine retrospectives that are a part of many iterative and agile development practices.\n\n§ We may also choose to extend the philosophies of Transparent Uptime to our post-mortem reports and, in\n\naddition to making a service dashboard available to the public, we may choose to publish (maybe sanitized) post-mortem meetings to the public. Some of the most widely admired public post-mortems include those posted by the Google App Engine team after a significant 2010 outage, as well as the post-mortem of the 2015 Amazon DynamoDB outage. Interestingly, Chef publishes their post-mortem meeting notes on their blog, as well as recorded videos of the actual post-mortem meetings.\n\n¶ Specific architectural patterns that they implemented included fail fasts (setting aggressive timeouts such that failing components don’t make the entire system crawl to a halt), fallbacks (designing each feature to degrade or fall back to a lower quality representation), and feature removal (removing non-critical features when they run slowly from any given page to prevent them from impacting the member experience). Another astonishing example of the resilience that the Netflix team created beyond preserving business continuity during the AWS outage, was that Netflix went over six hours into the AWS outage before declaring a Sev 1 incident, assuming that AWS service would eventually be restored (i.e., “AWS will come back… it usually does, right?”). Only after six hours into the outage did they activate any business continuity procedures.\n\n20Convert Local Discoveries into Global Improvements\n\nIn the previous chapter, we discussed developing a safe learning\n\nculture by encouraging everyone to talk about mistakes and\n\naccidents through blameless post-mortems. We also explored\n\nfinding and fixing ever-weaker failure signals, as well as reinforcing and rewarding experimentation and risk-taking.\n\nFurthermore, we helped make our system of work more resilient by proactively scheduling and testing failure scenarios, making\n\nour systems safer by finding latent defects and fixing them.\n\nIn this chapter, we will create mechanisms that make it possible for new learnings and improvements discovered locally to be\n\ncaptured and shared globally throughout the entire organization, multiplying the effect of global knowledge and improvement. By\n\ndoing this, we elevate the state of the practice of the entire\n\norganization so that everyone doing work benefits from the cumulative experience of the organization.\n\nUSE CHAT ROOMS AND CHAT BOTS TO AUTOMATE AND CAPTURE ORGANIZATIONAL KNOWLEDGE\n\nMany organizations have created chat rooms to facilitate fast\n\ncommunication within teams. However, chat rooms are also used\n\nto trigger automation.\n\nThis technique was pioneered in the ChatOps journey at GitHub.\n\nThe goal was to put automation tools into the middle of the\n\nconversation in their chat rooms, helping create transparency and\n\ndocumentation of their work. As Jesse Newland, a systems\n\nengineer at GitHub, describes, “Even when you’re new to the\n\nteam, you can look in the chat logs and see how everything is\n\ndone. It’s as if you were pair-programming with them all the\n\ntime.”\n\nThey created Hubot, a software application that interacted with the Ops team in their chat rooms, where it could be instructed to\n\nperform actions merely by sending it a command (e.g., “@hubot deploy owl to production”). The results would also be sent back into the chat room.\n\nHaving this work performed by automation in the chat room (as opposed to running automated scripts via command line) had numerous benefits, including:\n\nEveryone saw everything that was happening.\n\nEngineers on their first day of work could see what daily work looked like and how it was performed.\n\nPeople were more apt to ask for help when they saw others helping each other.\n\nRapid organizational learning was enabled and accumulated.\n\nFurthermore, beyond the above tested benefits, chat rooms\n\ninherently record and make all communications public; in\n\ncontrast, emails are private by default, and the information in\n\nthem cannot easily be discovered or propagated within an\n\norganization.\n\nIntegrating our automation into chat rooms helps document and\n\nshare our observations and problem solving as an inherent part of\n\nperforming our work. This reinforces a culture of transparency\n\nand collaboration in everything we do.\n\nThis is also an extremely effective way of converting local learning\n\nto global knowledge. At Github, all the Operations staff worked remotely—in fact, no two engineers worked in the same city. As Mark Imbriaco, former VP of Operations at GitHub, recalls, “There was no physical water cooler at GitHub. The chat room was the water cooler.”\n\nGithub enabled Hubot to trigger their automation technologies,\n\nincluding Puppet, Capistrano, Jenkins, resque (a Redis-backed library for creating background jobs), and graphme (which generates graphs from Graphite).\n\nActions performed through Hubot included checking the health of services, doing puppet pushes or code deployments into production, and muting alerts as services went into maintenance mode. Actions that were performed multiple times, such as pulling up the smoke test logs when a deployment failed, taking\n\nproduction servers out of rotation, reverting to master for production front-end services, or even apologizing to the engineers who were on call, also became Hubot actions.†\n\nSimilarly, commits to the source code repository and the commands that trigger production deployments both emit\n\nmessages to the chat room. Additionally, as changes move through the deployment pipeline, their status is posted in the chat room.\n\nA typical quick chat room exchange might look like:\n\n“@sr: @jnewland, how do you get that list of big repos?\n\ndisk_hogs or something?”\n\n“@jnewland: /disk-hogs”\n\nNewland observes that certain questions that were previously asked during the course of a project are rarely asked now. For\n\nexample, engineers may ask each other, “How is that deploy\n\ngoing?” or “Are you deploying that, or should I?” or “How does the load look?”\n\nAmong all the benefits that Newland describes, which include\n\nfaster onboarding of newer engineers and making all engineers more productive, the result that he felt was most important was\n\nthat Ops work became more humane as Ops engineers were enabled to discover problems and help each other quickly and\n\neasily.\n\nGitHub created an environment for collaborative local learning\n\nthat could be transformed into learnings across the organization. Throughout the rest of this chapter we will explore ways to create\n\nand accelerate the spread of new organizational learnings.\n\nAUTOMATE STANDARDIZED PROCESSES IN SOFTWARE FOR RE-USE\n\nAll too often, we codify our standards and processes for architecture, testing, deployment, and infrastructure management\n\nin prose, storing them in Word documents that are uploaded somewhere. The problem is that engineers who are building new\n\napplications or environments often don’t know that these documents exist, or they don’t have the time to implement the\n\ndocumented standards. The result is they create their own tools\n\nand processes, with all the disappointing outcomes we’d expect: fragile, insecure, and unmaintainable applications and\n\nenvironments that are expensive to run, maintain, and evolve.\n\nInstead of putting our expertise into Word documents, we need to transform these documented standards and processes, which\n\nencompass the sum of our organizational learnings and knowledge, into an executable form that makes them easier to\n\nreuse. One of the best ways we can make this knowledge re-usable\n\nis by putting it into a centralized source code repository, making the tool available for everyone to search and use.\n\nJustin Arbuckle was chief architect at GE Capital in 2013 when he\n\nsaid, “We needed to create a mechanism that would allow teams to easily comply with policy—national, regional, and industry\n\nregulations across dozens of regulatory frameworks, spanning thousands of applications running on tens of thousands of servers\n\nin tens of data centers.”\n\nThe mechanism they created was called ArchOps, which “enabled\n\nour engineers to be builders, not bricklayers. By putting our design standards into automated blueprints that were able to be\n\nused easily by anyone, we achieved consistency as a byproduct.”\n\nBy encoding our manual processes into code that is automated\n\nand executed, we enable the process to be widely adopted,\n\nproviding value to anyone who uses them. Arbuckle concluded that “the actual compliance of an organization is in direct\n\nproportion to the degree to which its policies are expressed as code.”\n\nBy making this automated process the easiest means to achieve\n\nthe goal, we allow practices to be widely adopted—we may even consider turning them into shared services supported by the\n\norganization.\n\nCREATE A SINGLE, SHARED SOURCE CODE REPOSITORY FOR OUR ENTIRE ORGANIZATION\n\nA firm-wide, shared source code repository is one of the most powerful mechanisms used to integrate local discoveries across\n\nthe entire organization. When we update anything in the source\n\ncode repository (e.g., a shared library), it rapidly and automatically propagates to every other service that uses that\n\nlibrary, and it is integrated through each team’s deployment pipeline.\n\nGoogle is one of the largest examples of using an organization-\n\nwide shared source code repository. By 2015, Google had a single shared source code repository with over one billion files and over\n\ntwo billion lines of code. This repository is used by every one of their twenty-five thousand engineers and spans every Google",
      "page_number": 413
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 421-429)",
      "start_page": 421,
      "end_page": 429,
      "detection_method": "topic_boundary",
      "content": "property, including Google Search, Google Maps, Google Docs, Google+, Google Calendar, Gmail, and YouTube.‡\n\nOne of the valuable results of this is that engineers can leverage the diverse expertise of everyone in the organization. Rachel Potvin, a Google engineering manager overseeing the Developer\n\nInfrastructure group, told Wired that every Google engineer can access “a wealth of libraries” because “almost everything has already been done.”\n\nFurthermore, as Eran Messeri, an engineer in the Google Developer Infrastructure group, explains, one of the advantages of using a single repository is that it allows users to easily access all\n\nof the code in its most up-to-date form, without the need for coordination.\n\nWe put into our shared source code repository not only source\n\ncode, but also other artifacts that encode knowledge and learning, including:\n\nConfiguration standards for our libraries, infrastructure, and environments (Chef recipes, Puppet manifests, etc.)\n\nDeployment tools\n\nTesting standards and tools, including security\n\nDeployment pipeline tools\n\nMonitoring and analysis tools\n\nTutorials and standards\n\nEncoding knowledge and sharing it through this repository is one of the most powerful mechanisms we have for propagating\n\nknowledge. As Randy Shoup describes, “The most powerful mechanism for preventing failures at Google is the single code repository. Whenever someone checks in anything into the repo, it\n\nresults in a new build, which always uses the latest version of everything. Everything is built from source rather than dynamically linked at runtime—there is always a single version of a library that is the current one in use, which is what gets statically\n\nlinked during the build process.”\n\nTom Limoncelli is the co-author of The Practice of Cloud System Administration: Designing and Operating Large Distributed\n\nSystems and a former Site Reliability Engineer at Google. In his book, he states that the value of having a single repository for an entire organization is so powerful it is difficult to even explain.\n\nYou can write a tool exactly once and have it be usable for all projects. You have 100% accurate knowledge of who depends on a library; therefore, you can refactor it and be 100% sure of\n\nwho will be affected and who needs to test for breakage. I could probably list one hundred more examples. I can’t express in words how much of a competitive advantage this is for Google.\n\nAt Google, every library (e.g., libc, OpenSSL, as well internally developed libraries such as Java threading libraries) has an owner who is responsible for ensuring that the library not only compiles,\n\nbut also successfully passes the tests for all projects that depend upon it, much like a real-world librarian. That owner is also responsible for migrating each project from one version to the next.\n\nConsider the real-life example of an organization that runs eighty- one different versions of the Java Struts framework library in\n\nproduction—all but one of those versions have critical security vulnerabilities, and maintaining all those versions, each with its own quirks and idiosyncrasies, creates significant operational burden and stress. Furthermore, all this variance makes\n\nupgrading versions risky and unsafe, which in turn discourages developers from upgrading. And the cycle continues.\n\nThe single source repository solves much of this problem, as well\n\nas having automated tests that allow teams to migrate to new versions safely and confidently.\n\nIf we are not able to build everything off a single source tree, we must find another means to maintain known good versions of the libraries and their dependencies. For instance, we may have an organization-wide repository such as Nexus, Artifactory, or a\n\nDebian or RPM repository, which we must then update where there are known vulnerabilities, both in these repositories and in production systems.\n\nSPREAD KNOWLEDGE BY USING AUTOMATED TESTS AS DOCUMENTATION AND COMMUNITIES OF PRACTICE\n\nWhen we have shared libraries being used across the organization, we should enable rapid propagation of expertise and\n\nimprovements. Ensuring that each of these libraries has significant amounts of automated testing included means these\n\nlibraries become self-documenting and show other engineers how to use them.\n\nThis benefit will be nearly automatic if we have test-driven development (TDD) practices in place, where automated tests are written before we write the code. This discipline turns our test\n\nsuites into a living, up-to-date specification of the system. Any engineer wishing to understand how to use the system can look at the test suite to find working examples of how to use the system’s API.\n\nIdeally, each library will have a single owner or a single team supporting it, representing where knowledge and expertise for the library resides. Furthermore, we should (ideally) only allow one\n\nversion to be used in production, ensuring that whatever is in production leverages the best collective knowledge of the organization.\n\nIn this model, the library owner is also responsible for safely migrating each group using the repository from one version to the next. This in turn requires quick detection of regression errors\n\nthrough comprehensive automated testing and continuous integration for all systems that use the library.\n\nIn order to more rapidly propagate knowledge, we can also create\n\ndiscussion groups or chat rooms for each library or service, so anyone who has questions can get responses from other users, who are often faster to respond than the developers.\n\nBy using this type of communication tool instead of having isolated pockets of expertise spread throughout the organization, we facilitate an exchange of knowledge and experience, ensuring\n\nthat workers are able to help each other with problems and new patterns.\n\nDESIGN FOR OPERATIONS THROUGH CODIFIED NON-FUNCTIONAL REQUIREMENTS\n\nWhen Development follows their work downstream and participates in production incident resolution activities, the\n\napplication becomes increasingly better designed for Operations. Furthermore, as we start to deliberately design our code and application so that it can accommodate fast flow and deployability, we will likely identify a set of non-functional requirements that we\n\nwill want to integrate into all of our production services.\n\nImplementing these non-functional requirements will enable our\n\nservices to be easy to deploy and keep running in production, where we can quickly detect and correct problems, and ensure it degrades gracefully when components fail. Examples of non- functional requirements include ensuring that we have:\n\nSufficient production telemetry in our applications and environments\n\nThe ability to accurately track dependencies\n\nServices that are resilient and degrade gracefully\n\nForward and backward compatibility between versions\n\nThe ability to archive data to manage the size of the production data set\n\nThe ability to easily search and understand log messages across services\n\nThe ability to trace requests from users through multiple services\n\nSimple, centralized runtime configuration using feature flags\n\nand so forth\n\nBy codifying these types of non-functional requirements, we make it easier for all our new and existing services to leverage the\n\ncollective knowledge and experience of the organization. These are all responsibilities of the team building the service.\n\nBUILD REUSABLE OPERATIONS USER STORIES INTO DEVELOPMENT\n\nWhen there is Operations work that cannot be fully automated or\n\nmade self-service, our goal is to make this recurring work as repeatable and deterministic as possible. We do this by standardizing the needed work, automating as much as possible,\n\nand documenting our work so that we can best enable product teams to better plan and resource this activity.\n\nInstead of manually building servers and then putting them into\n\nproduction according to manual checklists, we should automate as much of this work as possible. Where certain steps cannot be\n\nautomated (e.g., manually racking a server and having another\n\nteam cable it), we should collectively define the handoffs as clearly as possible to reduce lead times and errors. This will also enable\n\nus to better plan and schedule these steps in the future. For\n\ninstance, we can use tools such as Rundeck to automate and\n\nexecute workflows, or work ticket systems such as JIRA or\n\nServiceNow.\n\nIdeally, for all our recurring Ops work we will know the following:\n\nwhat work is required, who is needed to perform it, what the steps\n\nto complete it are, and so forth. For instance, “We know a high- availability rollout takes fourteen steps, requiring work from four\n\ndifferent teams, and the last five times we performed this, it took\n\nan average of three days.”\n\nJust as we create user stories in Development that we put into the\n\nbacklog and then pull into work, we can create well-defined “Ops\n\nuser stories” that represent work activities that can be reused across all our projects (e.g., deployment, capacity, security, etc.).\n\nBy creating these well defined Ops user stories, we expose\n\nrepeatable IT Operations work in a manner where it shows up alongside Development work, enabling better planning and more\n\nrepeatable outcomes.\n\nENSURE TECHNOLOGY CHOICES HELP ACHIEVE ORGANIZATIONAL GOALS\n\nWhen one of our goals is to maximize developer productivity and we have service-oriented architectures, small service teams can\n\npotentially build and run their service in whatever language or\n\nframework that best serves their specific needs. In some cases, this is what best enables us to achieve our organizational goals.\n\nHowever, there are scenarios when the opposite occurs, such as\n\nwhen expertise for a critical service resides only in one team, and\n\nonly that team can make changes or fix problems, creating a\n\nbottleneck. In other words, we may have optimized for team\n\nproductivity but inadvertently impeded the achievement of organizational goals.\n\nThis often happens when we have a functionally-oriented\n\nOperations group that is responsible for any aspect of service support. In these scenarios, to ensure that we enable the deep skill\n\nsets in specific technologies, we want to make sure that Operations\n\ncan influence which components are used in production, or give them the ability to not be responsible for unsupported platforms.\n\nIf we do not have a list of technologies that Operations will\n\nsupport, collectively generated by Development and Operations, we should systematically go through the production infrastructure\n\nand services, as well as all their dependencies that are currently\n\nsupported, to find which ones are creating a disproportionate amount of failure demand and unplanned work. Our goal is to\n\nidentify the technologies that:\n\nImpede or slow down the flow of work\n\nDisproportionately create high levels of unplanned work\n\nDisproportionately create large numbers of support requests\n\nAre most inconsistent with our desired architectural outcomes (e.g. throughput, stability, security, reliability, business\n\ncontinuity)\n\nBy removing these problematic infrastructures and platforms from the technologies supported by Ops, we enable them to focus\n\non infrastructure that best helps achieve the global goals of the\n\norganization.\n\nAs Tom Limoncelli describes, “When I was at Google, we had one official compiled language, one official scripting language, and one\n\nofficial UI language. Yes, other languages were supported in some\n\nway or another, but sticking with ‘the big three’ meant support libraries, tools, and an easier way to find collaborators.”§ These standards were also reinforced by the code review process, as well\n\nas what languages were supported by their internal platforms.\n\nIn a presentation that he gave with Olivier Jacques and Rafael\n\nGarcia at the 2015 DevOps Enterprise Summit, Ralph Loura, CIO\n\nof HP, stated:\n\nInternally, we described our goal as creating “buoys, not\n\nboundaries.” Instead of drawing hard boundaries that\n\neveryone has to stay within, we put buoys that indicate deep areas of the channel where you’re safe and supported. You can\n\ngo past the buoys as long as you follow the organizational\n\nprinciples. After all, how are we ever going to see the next innovation that helps us win if we’re not exploring and testing\n\nat the edges? As leaders, we need to navigate the channel, mark the channel, and allow people to explore past it.\n\nCase Study Standardizing a New Technology Stack at Etsy (2010)\n\nIn many organizations adopting DevOps, a common story developers tell is, “Ops wouldn’t provide us what we needed,\n\nso we just built and supported it ourselves.” However, in the",
      "page_number": 421
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 430-437)",
      "start_page": 430,
      "end_page": 437,
      "detection_method": "topic_boundary",
      "content": "early stages of the Etsy transformation, technology\n\nleadership took the opposite approach, significantly reducing\n\nthe number of supported technologies in production.\n\nIn 2010, after a nearly disastrous peak holiday season, the\n\nEtsy team decided to massively reduce the number of\n\ntechnologies used in production, choosing a few that the entire organization could fully support and eradicating the rest.¶\n\nTheir goal was to standardize and very deliberately reduce the supported infrastructure and configurations. One of the\n\nearly decisions was to migrate Etsy’s entire platform to PHP\n\nand MySQL. This was primarily a philosophical decision rather than a technological decision—they wanted both Dev\n\nand Ops to be able to understand the full technology stack so that everyone could contribute to a single platform, as\n\nwell as enable everyone to be able to read, rewrite, and fix\n\neach other’s code. Over the next several years, as Michael Rembetsy, who was Etsy’s Director of Operations at the\n\ntime, recalls, “We retired some great technologies, taking\n\nthem entirely out of production,” including lighttpd, Postgres, MongoDB, Scala, CoffeeScript, Python, and many others.\n\nSimilarly, Dan McKinley, a developer on the feature team\n\nthat introduced MongoDB into Etsy in 2010, writes on his blog that all the benefits of having a schema-less database\n\nwere negated by all the operational problems the team had\n\nto solve. These included problems concerning logging, graphing, monitoring, production telemetry, and backups and\n\nrestoration, as well as numerous other issues that\n\ndevelopers typically do not need to concern themselves\n\nwith. The result was to abandon MongoDB, porting the new\n\nservice to use the already supported MySQL database infrastructure.\n\nCONCLUSION\n\nThe techniques described in this chapter enable every new\n\nlearning to be incorporated into the collective knowledge of the organization, multiplying its effect. We do this by actively and\n\nwidely communicating new knowledge, such as through chat\n\nrooms and through technology such as architecture as code, shared source code repositories, technology standardization, and\n\nso forth. By doing this, we elevate the state of the practice of not\n\njust Dev and Ops, but also the entire organization, so everyone who performs work does so with the cumulative experience of the\n\nentire organization.\n\n† Hubot often performed tasks by calling shell scripts, which could then be executed from the chat room\n\nanywhere, including from an engineer’s phone.\n\n‡ The Chrome and Android projects reside in a separate source code repository, and certain algorithms that are\n\nkept secret, such as PageRank, are available only to certain teams.\n\n§ Google used C++ as their official compiled language, Python (and later Go) as their official scripting language,\n\nand Java and JavaScript via Google Web Toolkit as their official UI languages.\n\n¶ At that time, Etsy used PHP, lighttp, Postgres, MongoDB, Scala, CoffeeScript, Python, as well as many other\n\nplatforms and languages.\n\n21Reserve Time to Create Organizational Learning and Improvement\n\nOne of the practices that forms part of the Toyota Production\n\nSystem is called the improvement blitz (or sometimes a kaizen\n\nblitz), defined as a dedicated and concentrated period of time to\n\naddress a particular issue, often over the course of a several days. Dr. Spear explains, “...blitzes often take this form: A group is\n\ngathered to focus intently on a process with problems…The blitz lasts a few days, the objective is process improvement, and the\n\nmeans are the concentrated use of people from outside the process\n\nto advise those normally inside the process.”\n\nSpear observes that the output of the improvement blitz team will\n\noften be a new approach to solving a problem, such as new layouts of equipment, new means of conveying material and information,\n\na more organized workspace, or standardized work. They may also\n\nleave behind a to-do list of changes to be made down the road.\n\nAn example of a DevOps improvement blitz is the Monthly Challenge program at the Target DevOps Dojo. Ross Clanton, Director of Operations at Target, is responsible for accelerating\n\nthe adoption of DevOps. One of his primary mechanisms for this is the Technology Innovation Center, more popularly known as the DevOps Dojo.\n\nThe DevOps Dojo occupies about eighteen thousand square feet of\n\nopen office space, where DevOps coaches help teams from across\n\nthe Target technology organization elevate the state of their\n\npractice. The most intensive format is what they call “30-Day\n\nChallenges,” where internal development teams come in for a\n\nmonth and work together with dedicated Dojo coaches and\n\nengineers. The team brings their work with them, with the goal of\n\nsolving an internal problem they have been struggling with and to\n\ncreate a breakthrough in thirty days.\n\nThroughout the thirty days, they work intensively with the Dojo\n\ncoaches on the problem—planning, working, and doing demos in two-day sprints. When the 30-Day Challenge is complete, the internal teams return to their lines of business, not only having\n\nsolved a significant problem, but bringing their new learnings back to their teams.\n\nClanton describes, “We currently have capacity to have eight teams doing 30-Day Challenges concurrently, so we are focused on the most strategic projects of the organization. So far, we’ve had some of our most critical capabilities come through the Dojo,\n\nincluding teams from Point Of Sale (POS), Inventory, Pricing, and Promotion.”\n\nBy having full-time assigned Dojo staff and being focused on only one objective, teams going through a 30-Day Challenge make incredible improvements.\n\nRavi Pandey, a Target development manager who went through this program, explains, “In the old days, we would have to wait six weeks to get a test environment. Now, we get it in minutes, and we’re working side by side with Ops engineers who are helping us\n\nincrease our productivity and building tooling for us to help us\n\nachieve our goals.” Clanton expands on this idea, “It is not\n\nuncommon for teams to achieve in days what would usually take\n\nthem three to six months. So far, two hundred learners have come\n\nthrough the Dojo, having completed fourteen challenges.”\n\nThe Dojo also supports less intensive engagement models,\n\nincluding Flash Builds, where teams come together for one- to\n\nthree-day events, with the goal of shipping a minimal viable\n\nproduct (MVP) or a capability by the end of the event. They also\n\nhost Open Labs every two weeks, where anyone can visit the Dojo\n\nto talk to the Dojo coaches, attend demos, or receive training.\n\nIn this chapter, we will describe this and other ways of reserving time for organizational learning and improvement, further institutionalizing the practice of dedicating time for improving daily work.\n\nINSTITUTIONALIZE RITUALS TO PAY DOWN TECHNICAL DEBT\n\nIn this section, we schedule rituals that help enforce the practice\n\nof reserving Dev and Ops time for improvement work, such as non-functional requirements, automation, etc. One of the easiest ways to do this is to schedule and conduct day- or week-long improvement blitzes, where everyone on a team (or in the entire organization) self-organizes to fix problems they care about—no feature work is allowed. It could be a problematic area of the code,\n\nenvironment, architecture, tooling, and so forth. These teams span the entire value stream, often combining Development,\n\nOperations, and Infosec engineers. Teams that typically don’t\n\nwork together combine their skills and effort to improve a chosen area and then demonstrate their improvement to the rest of the\n\ncompany.\n\nIn addition to the Lean-oriented terms kaizen blitz and improvement blitz, the technique of dedicated rituals for\n\nimprovement work has also been called spring or fall cleanings\n\nand ticket queue inversion weeks. Other terms have also been used, such as hack days, hackathons, and 20% innovation time.\n\nUnfortunately, these specific rituals sometimes focus on product innovation and prototyping new market ideas, rather than on\n\nimprovement work, and worse, they are often restricted to developers—which is considerably different than the goals of an\n\nimprovement blitz.†\n\nOur goal during these blitzes is not to simply experiment and\n\ninnovate for the sake of testing out new technologies, but to improve our daily work, such as solving our daily workarounds.\n\nWhile experiments can also lead to improvements, improvement blitzes are very focused on solving specific problems we encounter\n\nin our daily work.\n\nWe may schedule week-long improvement blitzes that prioritize Dev and Ops working together toward improvement goals. These\n\nimprovement blitzes are simple to administer: One week is\n\nselected where everyone in the technology organization works on an improvement activity at the same time. At the end of the\n\nperiod, each team makes a presentation to their peers that discusses the problem they were tackling and what they built. This\n\npractice reinforces a culture in which engineers work across the entire value stream to solve problems. Furthermore, it reinforces\n\nfixing problems as part of our daily work and demonstrates that we value paying down technical debt.\n\nWhat makes improvement blitzes so powerful is that we are\n\nempowering those closest to the work to continually identify and solve their own problems. Consider for a moment that our\n\ncomplex system is like a spider web, with intertwining strands that are constantly weakening and breaking. If the right combination\n\nof strands breaks, the entire web collapses. There is no amount of\n\ncommand-and-control management that can direct workers to fix each strand one by one. Instead, we must create the organizational\n\nculture and norms that lead to everyone continually finding and fixing broken strands as part of our daily work. As Dr. Spear\n\nobserves, “No wonder then that spiders repair rips and tears in the web as they occur, not waiting for the failures to accumulate.”\n\nA great example of the success of the improvement blitz concept is\n\ndescribed by Mark Zuckerberg, Facebook CEO. In an interview\n\nwith Jessica Stillman of Inc.com, he says, “Every few months we have a hackathon, where everyone builds prototypes for new ideas\n\nthey have. At the end, the whole team gets together and looks at everything that has been built. Many of our most successful\n\nproducts came out of hackathons, including Timeline, chat, video, our mobile development framework and some of our most\n\nimportant infrastructure like the HipHop compiler.”\n\nOf particular interest is the HipHop PHP compiler. In 2008,\n\nFacebook was facing significant capacity problems, with over one- hundred million active users and rapidly growing, creating\n\ntremendous problems for the entire engineering team. During a hack day, Haiping Zhao, Senior Server Engineer at Facebook,\n\nstarted experimenting with converting PHP code to compilable\n\nC++ code, with the hope of significantly increasing the capacity of\n\ntheir existing infrastructure. Over the next two years, a small team\n\nwas assembled to build what became known as the HipHop compiler, converting all Facebook production services from\n\ninterpreted PHP to compiled C++ binaries. HipHop enabled Facebook’s platform to handle six times higher production loads\n\nthan the native PHP.\n\nIn an interview with Cade Metz of Wired, Drew Paroski, one of the engineers who worked on the project, noted, “There was a\n\nmoment where, if HipHop hadn’t been there, we would have been\n\nin hot water. We would probably have needed more machines to serve the site than we could have gotten in time. It was a Hail\n\nMary pass that worked out.”\n\nLater, Paroski and fellow engineers Keith Adams and Jason Evans decided that they could beat the performance of the HipHop\n\ncompiler effort and reduce some of its limitations that reduced developer productivity. The resulting project was the HipHop\n\nvirtual machine project (“HHVM”), taking a just-in-time\n\ncompilation approach. By 2012, HHVM had completely replaced the HipHop compiler in production, with nearly twenty engineers\n\ncontributing to the project.\n\nBy performing regularly scheduled improvement blitzes and hack weeks, we enable everyone in the value stream to take pride and\n\nownership in the innovations they create, and we continually integrate improvements into our system, further enabling safety,\n\nreliability, and learning.",
      "page_number": 430
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 438-445)",
      "start_page": 438,
      "end_page": 445,
      "detection_method": "topic_boundary",
      "content": "ENABLE EVERYONE TO TEACH AND LEARN\n\nA dynamic culture of learning creates conditions so that everyone can not only learn, but also teach, whether through traditional didactic methods (e.g., people taking classes, attending training) or more experiential or open methods (e.g., conferences,\n\nworkshops, mentoring). One way that we can foster this teaching and learning is to dedicate organizational time to it.\n\nSteve Farley, VP of Information Technology at Nationwide\n\nInsurance, said, “We have five thousand technology professionals, who we call ‘associates.’ Since 2011, we have been committed to create a culture of learning—part of that is something we call\n\nTeaching Thursday, where each week we create time for our associates to learn. For two hours, each associate is expected to teach or learn. The topics are whatever our associates want to learn about—some of them are on technology, on new software\n\ndevelopment or process improvement techniques, or even on how to better manage their career. The most valuable thing any associate can do is mentor or learn from other associates.”\n\nAs has been made evident throughout this book, certain skills are becoming increasingly needed by all engineers, not just by developers. For instance, it is becoming more important for all\n\nOperations and Test engineers to be familiar with Development techniques, rituals, and skills, such as version control, automated testing, deployment pipelines, configuration management, and\n\ncreating automation. Familiarity with Development techniques helps Operations engineers remain relevant as more technology value streams adopt DevOps principles and patterns.\n\nAlthough the prospect of learning something new may be intimidating or cause a sense of embarrassment or shame, it\n\nshouldn’t. After all, we are all lifelong learners, and one of the best ways to learn is from our peers. Karthik Gaekwad, who was part of the National Instruments DevOps transformation, said, “For\n\nOperations people who are trying to learn automation, it shouldn’t be scary—just ask a friendly developer, because they would love to help.”\n\nWe can help further help teach skills through our daily work by jointly performing code reviews that include both parties so that we learn by doing, as well as by having Development and Operations work together to solve small problems. For instance,\n\nwe might have Development show Operations how to authenticate an application, and login and run automated tests against various parts of the application to ensure that critical components are\n\nworking correctly (e.g., key application functionality, database transactions, message queues). We would then integrate this new automated test into our deployment pipeline and run it periodically, sending the results to our monitoring and alerting\n\nsystems so that we get earlier detection when critical components fail.\n\nAs Glenn O’Donnell from Forrester Research quipped in his 2014 DevOps Enterprise Summit presentation, “For all technology professionals who love innovating, love change, there is a wonderful and vibrant future ahead of us.”\n\nSHARE YOUR EXPERIENCES FROM DEVOPS CONFERENCES\n\nIn many cost-focused organizations, engineers are often discouraged from attending conferences and learning from their\n\npeers. To help build a learning organization, we should encourage our engineers (both from Development and Operations) to attend conferences, give talks at them, and, when necessary, create and organize internal or external conferences themselves.\n\nDevOpsDays remains one of the most vibrant self-organized conference series today. Many DevOps practices have been shared and promulgated at these events. It has remained free or nearly\n\nfree, supported by a vibrant community of practitioner communities and vendors.\n\nThe DevOps Enterprise Summit was created in 2014 for technology leaders to share their experiences adopting DevOps principles and practices in large, complex organizations. The program is organized primarily around experience reports from\n\ntechnology leaders on the DevOps journey, as well as subject matter experts on topics selected by the community.\n\nCase Study Internal Technology Conferences at Nationwide Insurance, Capital One, and Target (2014)\n\nAlong with attending external conferences, many\n\ncompanies, including those described in this section, have internal conferences for their technology employees.\n\nNationwide Insurance is a leading provider of insurance and\n\nfinancial services, and operates in heavily regulated industries. Their many offerings include auto and\n\nhomeowners insurance, and they are the top provider of public-sector retirement plans and pet insurance. As of 2014, $195 billion in assets, with $24 billion in revenue.\n\nSince 2005, Nationwide has been adopting Agile and Lean principles to elevate the state of practice for their five thousand technology professionals, enabling grassroots\n\ninnovation.\n\nSteve Farley, VP of Information Technology, remembers, “Exciting technology conferences were starting to appear\n\naround that time, such as the Agile national conference. In 2011, the technology leadership at Nationwide agreed that we should create a technology conference, called TechCon.\n\nBy holding this event, we wanted to create a better way to teach ourselves, as well as ensure that everything had a Nationwide context, as opposed to sending everyone to an external conference.”\n\nCapital One, one of the largest banks in the US with over $298 billion in assets and $24 billion in revenue in 2015, held their first internal software engineering conference in\n\n2015 as part of their goal to create a world-class technology organization. The mission was to promote a culture of sharing and collaboration, and to build relationships between\n\nthe technology professionals and enable learning. The conference had thirteen learning tracks and fifty-two sessions, and over 1,200 internal employees attended.\n\nDr. Tapabrata Pal, a technical fellow at Capital One and one of the organizers of the event, describes, “We even had an expo hall, where we had twenty-eight booths, where internal\n\nCapital One teams were showing off all the amazing\n\ncapabilities they were working on. We even decided very deliberately that there would be no vendors there, because we wanted to keep the focus on Capital One goals.”\n\nTarget is the sixth-largest retailer in the US, with $72 billion in revenue in 2014 and 1,799 retail stores and 347,000 employees worldwide. Heather Mickman, a director of\n\nDevelopment, and Ross Clanton have held six internal DevOpsDays events since 2014 and have over 975 followers inside their internal technology community,\n\nmodeled after the DevOpsDays held at ING in Amsterdam in 2013.‡\n\nAfter Mickman and Clanton attended the DevOps Enterprise\n\nSummit in 2014, they held their own internal conference, inviting many of the speakers from outside firms so that they could re-create their experience for their senior leadership.\n\nClanton describes, “2015 was the year when we got executive attention and when we built up momentum. After that event, tons of people came up to us, asking how they could get involved and how they could help.”\n\nCREATE INTERNAL CONSULTING AND COACHES TO SPREAD PRACTICES\n\nCreating an internal coaching and consulting organization is a method commonly used to spread expertise across an organization. This can come in many different forms. At Capital\n\nOne, designated subject matter experts hold office hours where anyone can consult with them, ask questions, etc.\n\nEarlier in the book, we began the story of how the Testing Grouplet built a world-class automated testing culture at Google starting in 2005. Their story continues here, as they try to improve\n\nthe state of automated testing across all of Google by using dedicated improvement blitzes, internal coaches, and even an internal certification program.\n\nBland said, at that time, there was a 20% innovation time policy at Google, enabling developers to spend roughly one day per week on a Google-related project outside of their primary area of\n\nresponsibility. Some engineers chose to form grouplets, ad hoc teams of like-minded engineers who wanted to pool their 20% time, allowing them to do focused improvement blitzes.\n\nA testing grouplet was formed by Bharat Mediratta and Nick Lesiecki, with the mission of driving the adoption of automated testing across Google. Even though they had no budget or formal authority, as Mike Bland described, “There were no explicit\n\nconstraints put upon us, either. And we took advantage of that.”\n\nThey used several mechanisms to drive adoption, but one of the\n\nmost famous was Testing on the Toilet (or TotT), their weekly testing periodical. Each week, they published a newsletter in nearly every bathroom in nearly every Google office worldwide. Bland said, “The goal was to raise the degree of testing knowledge\n\nand sophistication throughout the company. It’s doubtful an online-only publication would’ve involved people to the same\n\ndegree.”\n\nBland continues, “One of the most significant TotT episodes was\n\nthe one titled, ‘Test Certified: Lousy Name, Great Results,’ because\n\nit outlined two initiatives that had significant success in advancing\n\nthe use of automated testing.”\n\nTest Certified (TC) provided a road map to improve the state of automated testing. As Bland describes, “It was intended to hack\n\nthe measurement-focused priorities of Google culture...and to\n\novercome the first, scary obstacle of not knowing where or how to start. Level 1 was to quickly establish a baseline metric, Level 2\n\nwas setting a policy and reaching an automated test coverage goal,\n\nand Level 3 was striving towards a long-term coverage goal.”\n\nThe second capability was providing Test Certified mentors to any\n\nteam who wanted advice or help, and Test Mercenaries (i.e., a full-\n\ntime team of internal coaches and consultants) to work hands-on with teams to improve their testing practices and code quality. The\n\nMercenaries did so by applying the Testing Grouplet’s knowledge,\n\ntools, and techniques to a team’s own code, using TC as both a guide and a goal. Bland was eventually a leader of the Testing\n\nGrouplet from 2006 to 2007, and a member of the Test Mercenaries from 2007 to 2009.\n\nBland continues, “It was our goal to get every team to TC Level 3,\n\nwhether they were enrolled in our program our not. We also\n\ncollaborated closely with the internal testing tools teams, providing feedback as we tackled testing challenges with the\n\nproduct teams. We were boots on the ground, applying the tools we built, and eventually, we were able to remove ‘I don’t have time\n\nto test’ as a legitimate excuse.”\n\nHe continues, “The TC levels exploited the Google metrics-driven culture—the three levels of testing were something that people\n\ncould discuss and brag about at performance review time. The\n\nTesting Grouplet eventually got funding for the Test Mercenaries,\n\na staffed team of full-time internal consultants. This was an\n\nimportant step, because now management was fully onboard, not with edicts, but by actual funding.”\n\nAnother important construct was leveraging company-wide “Fixit”\n\nimprovement blitzes. Bland describes Fixits as “when ordinary engineers with an idea and a sense of mission recruit all of Google\n\nengineering for one-day, intensive sprints of code reform and tool\n\nadoption.” He organized four company-wide Fixits, two pure Testing Fixits and two that were more tools-related, the last\n\ninvolving more than one hundred volunteers in over twenty offices in thirteen countries. He also led the Fixit Grouplet from 2007 to\n\n2008.\n\nThese Fixits, as Bland describes means that we should provide\n\nfocused missions at critical points in time to generate excitement and energy, which helps advance the state-of-the-art. This will\n\nhelp the long-term culture change mission reach a new plateau with every big, visible effort.\n\nThe results of the testing culture are self-evident in the amazing\n\nresults Google has achieved, presented throughout the book.\n\nCONCLUSION\n\nThis chapter described how we can institute rituals that help\n\nreinforce the culture that we are all lifelong learners and that we\n\nvalue the improvement of daily work over daily work itself. We do this by reserving time to pay down technical debt, create forums\n\nthat allow everyone to learn from and teach each other, both",
      "page_number": 438
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 446-453)",
      "start_page": 446,
      "end_page": 453,
      "detection_method": "topic_boundary",
      "content": "inside our organization and outside it. And we make experts\n\navailable to help internal teams, either by coaching or consulting\n\nor even just holding office hours to answer questions.\n\nBy having everyone help each other learn in our daily work, we\n\nout-learn the competition, helping us win in the marketplace. But\n\nalso we help each other achieve our full potential as human beings.\n\nCONCLUSION TO PART V\n\nThroughout Part V, we explored the practices that help create a\n\nculture of learning and experimentation in your organization. Learning from incidents, creating shared repositories, and sharing\n\nlearnings is essential when we work in complex systems, helping\n\nto make our work culture more just and our systems safer and more resilient.\n\nIn Part VI, we’ll explore how to extend flow, feedback, and\n\nlearning and experimentation by using them to simultaneously help us achieve our Information Security goals.\n\n† From here on, the terms “hack week” and “hackathon” are used interchangeably with “improvement blitz,” and\n\nnot in the context of “you can work on whatever you want.”\n\n‡ Incidentally, the first Target internal DevOpsDays event was modeled after the first ING DevOpsDays that was\n\norganized by Ingrid Algra, Jan-Joost Bouwman, Evelijn Van Leeuwen, and Kris Buytaert in 2013, after some of the ING team attended the 2013 Paris DevOpsDays.\n\n22Information Security as Everyone’s Job, Every Day\n\nOne of the top objections to implementing DevOps principles and\n\npatterns has been, “Information security and compliance won’t let\n\nus.” And yet, DevOps may be one of the best ways to better\n\nintegrate information security into the daily work of everyone in\n\nthe technology value stream.\n\nWhen Infosec is organized as a silo outside of Development and\n\nOperations, many problems arise. James Wickett, one of the\n\ncreators of the Gauntlt security tool and organizer of DevOpsDays Austin and the Lonestar Application Security conference,\n\nobserved:\n\nOne interpretation of DevOps is that it came from the need to enable developers productivity, because as the number of\n\ndevelopers grew, there weren’t enough Ops people to handle all\n\nthe resulting deployment work. This shortage is even worse in Infosec—the ratio of engineers in Development, Operations, and Infosec in a typical technology organization is 100:10:1.\n\nWhen Infosec is that outnumbered, without automation and integrating information security into the daily work of Dev and Ops, Infosec can only do compliance checking, which is the opposite of security engineering—and besides, it also makes\n\neveryone hate us.\n\nJames Wickett and Josh Corman, former CTO of Sonatype and\n\nrespected information security researcher, have written about\n\nincorporating information security objectives into DevOps, a set of\n\npractices and principles termed Rugged DevOps. Similar ideas\n\nwere created by Dr. Tapabrata Pal, Director and Platform\n\nEngineering Technical Fellow at Capital One, and the Capital One\n\nteam, who describe their processes as DevOpsSec, where Infosec is\n\nintegrated into all stages of the SDLC. Rugged DevOps traces\n\nsome of its history to Visible Ops Security, written by Gene Kim,\n\nPaul Love, and George Spafford.\n\nThroughout The DevOps Handbook, we have explored how to fully integrate the QA and Operations objectives throughout our entire technology value stream. In this chapter, we describe how\n\nto similarly integrate Infosec objectives into our daily work, where we can increase developer and operational productivity, increase safety, and increase our security.\n\nINTEGRATE SECURITY INTO DEVELOPMENT ITERATION DEMONSTRATIONS\n\nOne of our goals is to have feature teams engaged with Infosec as early as possible, as opposed to primarily engaging at the end of the project. One way we can do this is by inviting Infosec to the product demonstrations at the end of each development interval so that they can better understand the team goals in the context of\n\norganizational goals, observe their implementations as they are being built, and provide guidance and feedback at the earliest\n\nstages of the project, when there is the most amount of time and\n\nfreedom to make corrections.\n\nJustin Arbuckle, former chief architect at GE Capital, observes,\n\n“When it came to information security and compliance, we found\n\nthat blockages at the end of the project were much more expensive\n\nthan at the beginning—and Infosec blockages were among the\n\nworst. ‘Compliance by demonstration’ became one of the rituals\n\nwe used to shift all this complexity earlier in the process.”\n\nHe continues, “By having Infosec involved throughout the creation\n\nof any new capability, we were able to reduce our use of static\n\nchecklists dramatically and rely more on using their expertise throughout the entire software development process.”\n\nThis helped the organization achieve its goals. Snehal Antani, former CIO of Enterprise Architecture at GE Capital Americas, described their top three key business measurements were “development velocity (i.e., speed of delivering features to\n\nmarket), failed customer interactions (i.e., outages, errors), and compliance response time (i.e., lead time from audit request to delivery of all quantitative and qualitative information required to fulfill the request).”\n\nWhen Infosec is an assigned part of the team, even if they are only being kept informed and observing the process, they gain the business context they need to make better risk-based decisions. Furthermore, Infosec is able to help feature teams learn what is\n\nrequired to meet security and compliance objectives.\n\nINTEGRATE SECURITY INTO DEFECT TRACKING AND POST-MORTEMS\n\nWhen possible, we want to track all open security issues in the same work tracking system that Development and Operations are\n\nusing, ensuring the work is visible and can be prioritized against all other work. This is very different from how Infosec has\n\ntraditionally worked, where all security vulnerabilities are stored in a GRC (governance, risk, and compliance) tool that only Infosec\n\nhas access to. Instead, we will put any needed work in the systems\n\nthat Dev and Ops use.\n\nIn a presentation at the 2012 Austin DevOpsDays, Nick Galbreath, who headed up Information Security at Etsy for many years,\n\ndescribes how they treated security issues, “We put all security issues into JIRA, which all engineers use in their daily work, and\n\nthey were either ‘P1’ or ‘P2,’ meaning that they had to be fixed immediately or by the end of the week, even if the issue is only an\n\ninternally-facing application.”\n\nFurthermore, he states, “Any time we had a security issue, we\n\nwould conduct a post-mortem, because it would result in better educating our engineers on how to prevent it from happening\n\nagain in the future, as well as a fantastic mechanism for transferring security knowledge to our engineering teams.”\n\nINTEGRATE PREVENTIVE SECURITY CONTROLS INTO SHARED SOURCE CODE REPOSITORIES AND SHARED SERVICES\n\nIn chapter 20, we created a shared source code repository that makes it easy for anyone to discover and reuse the collective\n\nknowledge of our organization—not only for our code, but also for our toolchains, deployment pipeline, standards, etc. By doing this,\n\nanyone can benefit from the cumulative experience of everyone in the organization.\n\nNow we will add to our shared source code repository any\n\nmechanisms or tools that help enable us to ensure our\n\napplications and environments are secure. We will add libraries that are pre-blessed by security to fulfill specific Infosec\n\nobjectives, such as authentication and encryption libraries and services. Because everyone in the DevOps value stream uses\n\nversion control for anything they build or support, putting our information security artifacts there makes it much easier to\n\ninfluence the daily work of Dev and Ops, because anything we\n\ncreate is available, searchable, and reusable. Version control also serves as a omni-directional communication mechanism to keep\n\nall parties aware of changes being made.\n\nIf we have a centralized shared services organization, we may also collaborate with them to create and operate shared security-\n\nrelevant platforms, such as authentication, authorization, logging, and other security and auditing services that Dev and Ops require.\n\nWhen engineers use one of these predefined libraries or services,\n\nthey won’t need to schedule a separate security design review for that module; they’ll be using the guidance we’ve created\n\nconcerning configuration hardening, database security settings, key lengths, and so forth.\n\nTo further increase the likelihood that the services and libraries\n\nwe provide will be used correctly, we can provide security training\n\nto Dev and Ops, as well as review what they’ve created to help\n\nensure that security objectives are being implemented correctly,\n\nespecially for teams using these tools for the first time.\n\nUltimately, our goal is to provide the security libraries or services that every modern application or environment requires, such as\n\nenabling user authentication, authorization, password management, data encryption, and so forth. Furthermore, we can\n\nprovide Dev and Ops with effective security-specific configuration settings for the components they use in their application stacks,\n\nsuch as for logging, authentication, and encryption. We may\n\ninclude items such as:\n\nCode libraries and their recommended configurations (e.g., 2FA [two-factor authentication library], bcrypt password\n\nhashing, logging)\n\nSecret management (e.g., connection settings, encryption keys) using tools such as Vault, sneaker, Keywhiz, credstash,\n\nTrousseau, Red October, etc.\n\nOS packages and builds (e.g., NTP for time syncing, secure versions of OpenSSL with correct configurations, OSSEC or\n\nTripwire for file integrity monitoring, syslog configuration to\n\nensure logging of critical security into our centralized ELK stack)\n\nBy putting all these into our shared source code repository, we\n\nmake it easy for any engineer to correctly create and use logging and encryption standards in their applications and environments, with no further work from us.",
      "page_number": 446
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 454-462)",
      "start_page": 454,
      "end_page": 462,
      "detection_method": "topic_boundary",
      "content": "We should also collaborate with Ops teams to create a base cookbook or build image of our OS, databases, and other\n\ninfrastructure (e.g., NGINX, Apache, Tomcat), showing they are in a known, secure, and risk-reduced state. Our shared repository not only becomes the place where we can get the latest versions,\n\nbut also becomes a place where we can collaborate with other engineers and monitor and alert on changes made to security- sensitive modules.\n\nINTEGRATE SECURITY INTO OUR DEPLOYMENT PIPELINE\n\nIn previous eras, in order to harden and secure our application, we would start our security review after development was completed. Often, the output of this review would be hundreds of pages of\n\nvulnerabilities in a PDF, which we’d give to Development and Operations, which would be completely un-addressed due to project due date pressure or problems being found too late in the software life cycle to be easily corrected.\n\nIn this step, we will automate as many of our information security tests as possible, so that they run alongside all our other automated tests in our deployment pipeline, being performed\n\n(ideally) upon every code commit by Dev or Ops, and even in the earliest stages of a software project.\n\nOur goal is to provide both Dev and Ops with fast feedback on their work so that they are notified whenever they commit changes that are potentially insecure. By doing this, we enable them to quickly detect and correct security problems as part of their daily\n\nwork, which enables learning and prevents future errors.\n\nIdeally, these automated security tests will be run in our deployment pipeline alongside the other static code analysis tools.\n\nTools such as Gauntlt have been designed to integrate into the deployment pipelines, which run automated security tests on our applications, our application dependencies, our environment, etc.\n\nRemarkably, Gauntlt even puts all its security tests in Gherkin syntax test scripts, which is widely used by developers for unit and functional testing. Doing this puts security testing in a framework\n\nthey are likely already familiar with. This also allows security tests to easily run in a deployment pipeline on every committed change, such as static code analysis, checking for vulnerable dependencies, or dynamic testing.\n\nFigure 43: Jenkins running automated security testing (Source: James Wicket and Gareth Rushgrove, “Battle-tested code without the battle,” Velocity 2014 conference presentation, posted to Speakerdeck.com, June 24, 2014, https://speakerdeck.com/garethr/battle-tested-code-without-the- battle.)\n\nBy doing this, we provide everyone in the value stream with the fastest possible feedback about the security of what they are\n\ncreating, enabling Dev and Ops engineers to find and fix issues quickly.\n\nENSURE SECURITY OF THE APPLICATION\n\nOften, Development testing focuses on the correctness of functionality, looking at positive logic flows. This type of testing is\n\noften referred to as the happy path, which validates user journeys (and sometimes alternative paths) where everything goes as expected, with no exceptions or error conditions.\n\nOn the other hand, effective QA, Infosec, and Fraud practitioners will often focus on the sad paths, which happen when things go wrong, especially in relation to security-related error conditions. (These types of security-specific conditions are often jokingly\n\nreferred to as the bad paths.)\n\nFor instance, suppose we have an e-commerce site with a\n\ncustomer input form that accepts credit card numbers as part of generating a customer order. We want to define all the sad and bath paths required to ensure that invalid credit cards are properly rejected to prevent fraud and security exploits, such as\n\nSQL injections, buffer overruns, and other undesirable outcomes.\n\nInstead of performing these tests manually, we would ideally generate them as part of our automated unit or functional tests so\n\nthat they can be run continuously in our deployment pipeline. As part of our testing, we will want to include the following:\n\nStatic analysis: This is testing that we perform in a non- runtime environment, ideally in the deployment pipeline. Typically, a static analysis tool will inspect program code for all possible run-time behaviors and seek out coding flaws, back\n\ndoors, and potentially malicious code (this is sometimes known as “testing from the inside-out”). Examples of tools include\n\nBrakeman, Code Climate, and searching for banned code functions (e.g., “exec()”).\n\nDynamic analysis: As opposed to static testing, dynamic analysis consists of tests executed while a program is in operation. Dynamic tests monitor items such as system\n\nmemory, functional behavior, response time, and overall performance of the system. This method (sometimes known as “testing from the outside-in”) is similar to the manner in which a malicious third party might interact with an application.\n\nExamples include Arachni and OWASP ZAP (Zed Attack Proxy).† Some types of penetration testing can also be performed in an automated fashion and should be included as\n\npart of dynamic analysis using tools such as Nmap and Metasploit. Ideally, we should perform automated dynamic testing during the automated functional testing phase of our deployment pipeline, or even against our services while they\n\nare in production. To ensure correct security handling, tools like OWASP ZAP can be configured to attack our services through a web browser proxy and inspect the network traffic\n\nwithin our test harness.\n\nDependency scanning: Another type of static testing we would normally perform at build time inside of our deployment\n\npipeline involves inventorying all our dependencies for binaries and executables, and ensuring that these dependencies, which we often don’t have control over, are free of vulnerabilities or malicious binaries. Examples include Gemnasium and bundler\n\naudit for Ruby, Maven for Java, and the OWASP Dependency- Check.\n\nSource code integrity and code signing: All developers should have their own PGP key, perhaps created and managed in a system such as keybase.io. All commits to version control\n\nshould be signed —that is straightforward to configure using the open source tools gpg and git. Furthermore, all packages created by the CI process should be signed, and their hash\n\nrecorded in the centralized logging service for audit purposes.\n\nFurthermore, we should define design patterns to help developers write code to prevent abuse, such as putting in rate limits for our\n\nservices and graying out submit buttons after they have being pressed. OWASP publishes a great deal of useful guidance such as the Cheat Sheet series, which includes:\n\nHow to store passwords\n\nHow to handle forgotten passwords\n\nHow to handle logging\n\nHow to prevent cross-site scripting (XSS) vulnerabilities\n\nCase Study Static Security Testing at Twitter (2009)\n\nThe “10 Deploys per Day: Dev and Ops Cooperation at Flickr” presentation by John Allspaw and Paul Hammond is\n\nfamous for catalyzing the Dev and Ops community in 2009. The equivalent for the information security community is likely the presentation that Justin Collins, Alex Smolen, and Neil Matatall gave on their information security\n\ntransformation work at Twitter at the AppSecUSA conference in 2012.\n\nTwitter had many challenges due to hyper-growth. For years, the famous Fail Whale error page would be displayed when Twitter did not have sufficient capacity to keep up with user demand, showing a graphic of a whale being lifted by\n\neight birds. The scale of user growth was breathtaking— between January and March 2009, the number of active Twitter users went from 2.5 million to 10 million.\n\nTwitter also had security problems during this period. In early 2009, two serious security breaches occurred. First, in January the @BarackObama Twitter account was hacked.\n\nThen in April, the Twitter administrative accounts were compromised through a brute-force dictionary attack. These events led the Federal Trade Commission to judge that Twitter was misleading its users into believing that their\n\naccounts were secure and issued an FTC consent order.\n\nThe consent order required that Twitter comply within sixty\n\ndays by instituting a set of processes that were to be enforced for the following twenty years and would do the following:\n\nDesignate an employee or employees to be responsible for\n\nTwitter’s information security plan\n\nIdentify reasonably foreseeable risks, both internal and external, that could lead to an intrusion incident and create and implement a plan to address these risks‡\n\nMaintain the privacy of user information, not just from\n\noutside sources but also internally, with an outline of\n\npossible sources of verification and testing of the security and correctness of these implementations\n\nThe group of engineers assigned to solve this problem had\n\nto integrate security into the daily work of Dev and Ops and close the security holes that allowed the breaches to happen\n\nin the first place.\n\nIn their previously mentioned presentation, Collins, Smolen, and Matatall identified several problems they needed to\n\naddress:\n\nPrevent security mistakes from being repeated: They found that they were fixing the same defects and\n\nvulnerabilities over and over again. They needed to modify\n\nthe system of work and automation tools to prevent the issues from happening again.\n\nIntegrate security objectives into existing developer\n\ntools: They identified early on that the major source of vulnerabilities were code issues. They couldn’t run a tool\n\nthat generated a huge PDF report and then email it to\n\nsomeone in Development or Operations. Instead, they needed to provide the developer who had created the\n\nvulnerability with the exact information needed to fix it.\n\nPreserve trust of Development: They needed to earn and maintain the trust of Development. That meant they needed\n\nto know when they sent Development false positives, so\n\nthey could fix the error that prompted the false positive and\n\navoid wasting Development’s time.\n\nMaintain fast flow through Infosec through automation: Even when code vulnerability scanning was automated,\n\nInfosec still had to do lots of manual work and waiting. They\n\nhad to wait for the scan to complete, get back the big stack of reports, interpret the reports, and then find the person\n\nresponsible for fixing it. And when the code changed, it had\n\nto be done all over again. By automating the manual work, they did fewer dumb “button-pushing” tasks, enabling them\n\nto use more creativity and judgment.\n\nMake everything security related self-service, if possible: They trusted that most people wanted to do the\n\nright thing, so it was necessary to provide them with all the\n\ncontext and information they needed to fix any issues.\n\nTake a holistic approach to achieving Infosec\n\nobjectives: Their goal was to do analysis from all the\n\nangles: source code, the production environment, and even what their customers were seeing.\n\nThe first big breakthrough for the Infosec team occured\n\nduring a company-wide hack week when they integrated static code analysis into the Twitter build process. The team\n\nused Brakeman, which scans Ruby on Rails applications for vulnerabilities. The goal was to integrate security scanning\n\ninto the earliest stages of the Development process, not just\n\nwhen the code was committed into the source code repo.\n\nFigure 44: Number of Brakeman security vulnerabilities detected\n\nThe results of integrating security testing into the development process were breathtaking. Over the years, by\n\ncreating fast feedback for developers when they write\n\ninsecure code and showing them how to fix the vulnerabilities, Brakeman has reduced the rate of\n\nvulnerabilities found by 60%, as shown in figure 44. (The\n\nspikes are usually associated with new releases of Brakeman.)\n\nThis cases study illustrates just how necessary it is to\n\nintegrate security into the daily work and tools of DevOps and how effectively it can work. Doing so mitigates security\n\nrisk, reduces the probability of vulnerabilities in the system, and helps teach developers to write more secure code.\n\nENSURE SECURITY OF OUR SOFTWARE SUPPLY CHAIN",
      "page_number": 454
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 463-470)",
      "start_page": 463,
      "end_page": 470,
      "detection_method": "topic_boundary",
      "content": "Josh Corman observed that as developers “we are no longer\n\nwriting customized software—instead, we assemble what we need\n\nfrom open source parts, which has become the software supply chain that we are very much reliant upon.” In other words, when\n\nwe use components or libraries —either commercial or open source —in our software, we not only inherit their functionality, but also\n\nany security vulnerabilities they contain.\n\nWhen selecting software, we detect when our software projects are relying on components or libraries that have known\n\nvulnerabilities, and help developers choose the components they\n\nuse deliberately and with due care, selecting those components (e.g., open source projects) that have a demonstrated history of\n\nquickly fixing software vulnerabilities. We also look for multiple\n\nversions of the same library being used across our production landscape, particularly the presence of older versions of libraries\n\nwhich contain known vulnerabilities.\n\nExamining cardholder data breaches shows how important the security of open source components we choose can be. Since\n\n2008, the annual Verizon PCI Data Breach Investigation Report\n\n(DBIR) has been the most authoritative voice on data breaches where cardholder data was lost or stolen. In the 2014 report, they\n\nstudied over eighty-five thousand breaches to better understand\n\nwhere attacks were coming from, how cardholder data was stolen, and factors leading to the breach.\n\nThe DBIR found that ten vulnerabilities (i.e., CVEs) accounted for\n\nalmost 97% of the exploits used in studied cardholder data breaches in 2014. Of these ten vulnerabilities, eight of them were\n\nover ten years old.\n\nThe 2015 Sonatype State of the Software Supply Chain Report\n\nfurther analyzed the vulnerability data from the Nexus Central Repository. In 2015, this repository provided the build artifacts for\n\nover 605,000 open source projects, servicing over seventeen\n\nbillion download requests of artifacts and dependencies primarily for the Java platform, originating from 106,000 organizations.\n\nThe report included these startling findings:\n\nThe typical organization relied upon 7,601 build artifacts (i.e.,\n\nsoftware suppliers or components) and used 18,614 different versions (i.e., software parts).\n\nOf those components being used, 7.5% had known\n\nvulnerabilities, with over 66% of those vulnerabilities being over two years old without having been resolved.\n\nThe last statistic confirms another information security study by\n\nDr. Dan Geer and Josh Corman, which showed that of the open source projects with known vulnerabilities registered in the\n\nNational Vulnerability Database, only 41% were ever fixed and\n\nrequired, on average, 390 days to publish a fix. For those vulnerabilities that were labeled at the highest severity (i.e., those scored as CVSS level 10), fixes required 224 days.§\n\nENSURE SECURITY OF THE ENVIRONMENT\n\nIn this step, we should do whatever is required to help ensure that the environments are in a hardened, risk-reduced state. Although\n\nwe may have created known, good configurations already, we\n\nmust put in monitoring controls to ensure that all production\n\ninstances match these known good states.\n\nWe do this by generating automated tests to ensure that all appropriate settings have been correctly applied for configuration\n\nhardening, database security settings, key lengths, and so forth. Furthermore, we will use tests to scan our environments for known vulnerabilities.¶\n\nAnother category of security verification is understanding actual environments (i.e., “as they actually are”). Examples of tools for\n\nthis include Nmap to ensure that only expected ports are open and\n\nMetasploit to ensure that we’ve adequately hardened our environments against known vulnerabilities, such as scanning\n\nwith SQL injection attacks. The output of these tools should be put\n\ninto our artifact repository and compared with the previous version as part of our functional testing process. Doing this will\n\nhelp us detect any undesirable changes as soon as they occur.\n\nCase Study 18F Automating Compliance for the Federal Government with Compliance Masonry\n\nUS Federal Government agencies were projected to spend\n\nnearly $80 billion on IT in 2016, supporting the mission of all\n\nthe executive branch agencies. Regardless of agency, to take any system from “dev complete” to “live in production”\n\nrequires obtaining an Authority to Operate (ATO) from a\n\nDesignated Approving Authority (DAA). The laws and policies that govern complience in government are\n\ncomprised of tens of documents that together number over\n\nfour thousand pages, littered with acronyms such as FISMA,\n\nFedRAMP, and FITARA. Even for systems that only require low levels of confidentiality, integrity, and availability, over\n\none hundred controls must be implemented, documented,\n\nand tested. It typically takes between eight and fourteen months for an ATO to be granted following “dev complete.”\n\nThe 18F team in the federal government’s General Services\n\nAdministration has taken a multi-pronged approach to solving this problem. Mike Bland explains, “18F was created\n\nwithin the General Services Administration to capitalize on\n\nthe momentum generated by the Healthcare.gov recovery to reform how the government builds and buys software.”\n\nOne 18F effort is a platform as a service called Cloud.gov,\n\ncreated from open source components. Cloud.gov runs on AWS GovCloud at present. Not only does the platform\n\nhandle many of the operational concerns delivery teams\n\nmight otherwise have to take care of, such as logging, monitoring, alerting, and service lifecycle management, it\n\nalso handles the bulk of compliance concerns. By running\n\non this platform, a large majority of the controls that government systems must implement can be taken care of\n\nat the infrastructure and platform level. Then, only the remaining controls that are in scope at the application layer\n\nhave to be documented and tested, significantly reducing\n\nthe compliance burden and the time it takes to receive an ATO.\n\nAWS GovCloud has already been approved for use for\n\nfederal government systems of all types, including those which require high levels of confidentiality, integrity, and\n\navailability. By the time you read this book, it is expected that\n\nCloud.gov will be approved for all systems that require moderate levels of confidentiality, integrity, and availability.**\n\nFurthermore, the Cloud.gov team is building a framework to\n\nautomate the creation of system security plans (SSPs), which are “comprehensive descriptions of the system’s\n\narchitecture, implemented controls, and general security\n\nposture…[which are] often incredibly complex, running several hundred pages in length.” They developed a\n\nprototype tool called compliance masonry so that SSP data\n\nis stored in machine-readable YAML and then turned into GitBooks and PDFs automatically.\n\n18F is dedicated to working in the open and publishes its\n\nwork open source in the public domain. You can find compliance masonry and the components that make up\n\nCloud.gov in 18F’s GitHub repositories—you can even stand\n\nup your own instance of Cloud.gov. The work on open documentation for SSPs is being done in close partnership\n\nwith the OpenControl community.\n\nINTEGRATE INFORMATION SECURITY INTO PRODUCTION TELEMETRY\n\nMarcus Sachs, one of the Verizon Data Breach researchers, observed in 2010, “Year after year, in the vast majority of\n\ncardholder data breaches, the organization detected the security\n\nbreach months or quarters after the breach occurred. Worse, the way the breach was detected was not an internal monitoring\n\ncontrol, but was far more likely someone outside of the\n\norganization, usually a business partner or the customer who\n\nnotices fraudulent transactions. One of the primary reasons for this is that no one in the organization was regularly reviewing the\n\nlog files.”\n\nIn other words, internal security controls are often ineffective in successfully detecting breaches in a timely manner, either because\n\nof blind spots in our monitoring or because no one in our\n\norganization is examining the relevant telemetry in their daily work.\n\nIn chapter 14, we discussed creating a culture in Dev and Ops\n\nwhere everyone in the value stream is creating production telemetry and metrics, making them visible in prominent public\n\nplaces so that everyone can see how our services are performing in\n\nproduction. Furthermore, we explored the necessity of relentlessly seeking ever-weaker failure signals so that we can find and fix\n\nproblems before they result in a catastrophic failure.\n\nHere, we deploy the monitoring, logging, and alerting required to fulfill our information security objectives throughout our\n\napplications and environments, as well as ensure that it is\n\nadequately centralized to facilitate easy and meaningful analysis and response.\n\nWe do this by integrating our security telemetry into the same\n\ntools that Development, QA, and Operations are using, giving everyone in the value stream visibility into how their application\n\nand environments are performing in a hostile threat environment\n\nwhere attackers are constantly attempting to exploit vulnerabilities, gain unauthorized access, plant backdoors,\n\ncommit fraud, perform denials-of-service, and so forth.\n\nBy radiating how our services are being attacked in the production\n\nenvironment, we reinforce that everyone needs to be thinking about security risks and designing countermeasures in their daily\n\nwork.\n\nCREATING SECURITY TELEMETRY IN OUR APPLICATIONS\n\nIn order to detect problematic user behavior that could be an indicator or enabler of fraud and unauthorized access, we must\n\ncreate the relevant telemetry in our applications.\n\nExamples may include:\n\nSuccessful and unsuccessful user logins\n\nUser password resets\n\nUser email address resets\n\nUser credit card changes\n\nFor instance, as an early indicator of brute-force login attempts to\n\ngain unauthorized access, we might display the ratio of\n\nunsuccessful login attempts to successful logins. And, of course, we should create alerting around important events to ensure we\n\ncan detect and correct issues quickly.\n\nCREATING SECURITY TELEMETRY IN OUR ENVIRONMENT\n\nIn addition to instrumenting our application, we also need to\n\ncreate sufficient telemetry in our environments so that we can detect early indicators of unauthorized access, especially in the\n\ncomponents that are running on infrastructure that we do not\n\ncontrol (e.g., hosting environments, in the cloud).\n\nWe need to monitor and potentially alert on items, including the\n\nfollowing:\n\nOS changes (e.g., in production, in our build infrastructure)\n\nSecurity group changes\n\nChanges to configurations (e.g., OSSEC, Puppet, Chef,\n\nTripwire)\n\nCloud infrastructure changes (e.g., VPC, security groups, users and privileges)\n\nXSS attempts (i.e., “cross-site scripting attacks”)\n\nSQLi attempts (i.e., “SQL injection attacks”)\n\nWeb server errors (e.g., 4XX and 5XX errors)\n\nWe also want to confirm that we’ve correctly configured our\n\nlogging so that all telemetry is being sent to the right place. When we detect attacks, in addition to logging that it happened, we may\n\nalso choose to block access and store information about the source\n\nto aid us in choosing the best mitigation actions.\n\nCase Study Instrumenting the Environment at Etsy (2010)",
      "page_number": 463
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 471-478)",
      "start_page": 471,
      "end_page": 478,
      "detection_method": "topic_boundary",
      "content": "In 2010, Nick Galbreath was director of engineering at Etsy\n\nand responsible for information security, fraud control, and privacy. Galbreath defined fraud as when “the system works\n\nincorrectly, allowing invalid or un-inspected input into the\n\nsystem, causing financial loss, data loss/theft, system downtime, vandalism, or an attack on another system.”\n\nTo achieve these goals, Galbreath did not create a separate\n\nfraud control or information security department; instead, he embedded those responsibilities throughout the DevOps\n\nvalue stream.\n\nGalbreath created security-related telemetry that were displayed alongside all the other more Dev and Ops oriented\n\nmetrics, which every Etsy engineer routinely saw:\n\nAbnormal production program terminations (e.g.,\n\nsegmentation faults, core dumps, etc.): “Of particular\n\nconcern was why certain processes kept dumping core\n\nacross our entire production environment, triggered from\n\ntraffic coming from the one IP address, over and over again. Of equal concern were those HTTP ‘500 Internal Server\n\nErrors.’ These are indicators that a vulnerability was being\n\nexploited to gain unauthorized access to our systems, and\n\nthat a patch needs to be urgently applied.”\n\nDatabase syntax error: “We were always looking for\n\ndatabase syntax errors inside our code—these either\n\nenabled SQL Injection attacks or were actual attacks in\n\nprogress. For this reason, we had zero-tolerance for\n\ndatabase syntax errors in our code, because it remains one\n\nof the leading attack vectors used to compromise systems.”\n\nIndications of SQL injection attacks: “This was a\n\nridiculously simple test—we’d merely alert whenever\n\n‘UNION ALL’ showed up in user-input fields, since it almost\n\nalways indicates a SQL injection attack. We also added unit tests to make sure that this type of uncontrolled user input\n\ncould never be allowed into our database queries.”\n\nFigure 45: Developers would see SQL injection attempts in Graphite at Etsy (Source: “DevOpsSec: Appling DevOps Priciples to Security, DevOpsDays Austin 2012,” SlideShare.net, posted by Nick Galbreath, April 12, 2012, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops- principles-to-security.)\n\nFigure 45 is an example of a graph that every developer\n\nwould see, which shows the number of potential SQL\n\ninjection attacks that were attempted in the production\n\nenvironment. As Galbreath observed, “Nothing helps developers understand how hostile the operating\n\nenvironment is than seeing their code being attacked in real-\n\ntime.”\n\nGalbreath observed, “One of the results of showing this\n\ngraph was that developers realized that they were being\n\nattacked all the time! And that was awesome, because it\n\nchanged how developers thought about the security of their\n\ncode as they were writing the code.”\n\nPROTECT OUR DEPLOYMENT PIPELINE\n\nThe infrastructure that supports our continuous integration and\n\ncontinuous deployment processes also presents a new surface area\n\nvulnerable to attack. For instance, if someone compromises the\n\nservers running deployment pipeline that has the credentials for\n\nour version control system, it could enable someone to steal source code. Worse, if the deployment pipeline has write access,\n\nan attacker could also inject malicious changes into our version\n\ncontrol repository, and, therefore, inject malicious changes into\n\nour application and services.\n\nAs Jonathan Claudius, former Senior Security Tester at TrustWave\n\nSpiderLabs, observed, “Continuous build and test servers are\n\nawesome, and I use them myself. But I started thinking about\n\nways to use CI/CD as a way to inject malicious code. Which led to\n\nthe question of where would be a good place to hide malicious code? The answer was obvious: in the unit tests. No one actually\n\nlooks at the unit tests, and they’re run every time someone\n\ncommits code to the repo.”\n\nThis demonstrates that in order to adequately protect the integrity\n\nof our applications and environments, we must also mitigate the\n\nattack vectors on our deployment pipeline. Risks include\n\ndevelopers introducing code that enables unauthorized access\n\n(which we’ve mitigated through controls such as code testing, code\n\nreviews, and penetration testing) and unauthorized users gaining\n\naccess to our code or environment (which we’ve mitigated through\n\ncontrols such as ensuring configurations match known, good\n\nstates, and effective patching).\n\nHowever, in order to protect our continuous build, integration, or\n\ndeployment pipeline, our mitigation strategies may include:\n\nHardening continuous build and integration servers and\n\nensuring we can reproduce them in an automated manner, just\n\nas we would for infrastructure that supports customer-facing\n\nproduction services, to prevent our continuous build and integration servers from being compromised\n\nReviewing all changes introduced into version control, either\n\nthrough pair programming at commit time or by a code review process between commit and merge into trunk, to prevent\n\ncontinuous integration servers from running uncontrolled code\n\n(e.g., unit tests may contain malicious code that allows or\n\nenables unauthorized access)\n\nInstrumenting our repository to detect when test code contains\n\nsuspicious API calls (e.g., unit tests accessing the filesystem or\n\nnetwork) is checked in to the repository, perhaps quarantining\n\nit and triggering an immediate code review\n\nEnsuring every CI process runs on its own isolated container or\n\nVM\n\nEnsuring the version control credentials used by the CI system\n\nare read-only\n\nCONCLUSION\n\nThroughout this chapter we have described ways to integrate\n\ninformation security objectives into all stages of our daily work.\n\nWe do this by integrating security controls into the mechanisms\n\nwe’ve already created, ensuring that all on-demand environments\n\nare also in a hardened, risk-reduced state—by integrating security testing into the deployment pipeline and ensuring the creation of\n\nsecurity telemetry in pre-production and production\n\nenvironments. By doing so, we enable developer and operational\n\nproductivity to increase while simultaneously increasing our\n\noverall safety. Our next step is to protect the deployment pipeline.\n\n† The Open Web Application Security Project (OWASP) is a non-profit organization focused on improving the\n\nsecurity of software.\n\n‡ Strategies for managing these risks include providing employee training and management; rethinking the design of information systems, including network and software; and instituting processes designed to prevent, detect, and respond to attacks.\n\n§ Tools that can help ensure the integrity of our software dependencies include OWASP Dependency Check and\n\nSonatype Nexus Lifecycle.\n\n¶ Examples of tools that can help with security correctness testing (i.e., “as it should be”) include automated\n\nconfiguration management systems (e.g., Puppet, Chef, Ansible, Salt), as well as tools such as ServerSpec and the Netflix Simian Army (e.g., Conformity Monkey, Security Monkey, etc.).\n\n** These approvals are known as FedRAMP JAB P-ATOs.\n\n23Protecting the Deployment Pipeline\n\nThroughout this chapter we will look at how to protect our\n\ndeployment pipeline, as well as how to acheive security and\n\ncompliance objectives in our control environment, including\n\nchange management and separation of duty.\n\nINTEGRATE SECURITY AND COMPLIANCE INTO CHANGE APPROVAL PROCESSES\n\nAlmost any IT organization of any significant size will have\n\nexisting change management processes, which are the primary controls to reduce operations and security risks. Compliance\n\nmanager and security managers place reliance on change\n\nmanagement processes for compliance requirements, and they typically require evidence that all changes have been appropriately authorized.\n\nIf we have constructed our deployment pipeline correctly so that\n\ndeployments are low-risk, the majority of our changes won’t need to go through a manual change approval process, because we will have placed our reliance on controls such as automated testing and proactive production monitoring.\n\nIn this step, we will do what is required to ensure that we can\n\nsuccessfully integrate security and compliance into any existing\n\nchange management process. Effective change management\n\npolicies will recognize that there are different risks associated with\n\ndifferent types of changes and that those changes are all handled\n\ndifferently. These processes are defined in ITIL, which breaks\n\nchanges down into three categories:\n\nStandard changes: These are lower-risk changes that follow\n\nan established and approved process, but can also be pre-\n\napproved. They include monthly updates of application tax\n\ntables or country codes, website content and styling changes, and certain types of application or operating system patches that have a well-understood impact. The change proposer does\n\nnot require approval before deploying the change, and change deployments can be completely automated and should be logged so there is traceability.\n\nNormal changes: These are higher-risk changes that require review or approval from the agreed upon change authority. In many organizations, this responsibility is inappropriately\n\nplaced on the change advisory board (CAB) or emergency change advisory board (ECAB), which may lack the required\n\nexpertise to understand the full impact of the change, often leading to unacceptably long lead times. This problem is especially relevant for large code deployments, which may contain hundreds of thousands (or even millions) of lines of new code, submitted by hundreds of developers over the course\n\nof several months. In order for normal changes to be authorized, the CAB will almost certainly have a well-defined request for change (RFC) form that defines what information is\n\nrequired for the go/no-go decision. The RFC form usually\n\nincludes the desired business outcomes, planned utility and warranty,† a business case with risks and alternatives, and a proposed schedule.‡\n\nUrgent changes: These are emergency, and, consequently,\n\npotentially high risk, changes that must be put into production\n\nimmediately (e.g., urgent security patch, restore service). These\n\nchanges often require senior management approval, but allow\n\ndocumentation to be performed after the fact. A key goal of\n\nDevOps practices is to streamline our normal change process\n\nsuch that it is also suitable for emergency changes.\n\nRE-CATEGORIZE THE MAJORITY OF OUR LOWER RISK CHANGES AS STANDARD CHANGES\n\nIdeally, by having a reliable deployment pipeline in place, we will have already earned a reputation for fast, reliable, and non- dramatic deployments. At this point, we should seek to gain agreement from Operations and the relevant change authorities that our changes have been demonstrated to be low risk enough to\n\nbe defined as standard changes, pre-approved by the CAB. This enables us to deploy into production without need for further approval, although the changes should still be properly recorded.\n\nOne way to support an assertion that our changes are low risk is to show a history of changes over a significant time period (e.g., months or quarters) and provide a complete list of production\n\nissues during that same period. If we can show high change",
      "page_number": 471
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 479-487)",
      "start_page": 479,
      "end_page": 487,
      "detection_method": "topic_boundary",
      "content": "success rates and low MTTR, we can assert that we have a control environment that is effectively preventing deployment errors, as\n\nwell as prove that we can effectively and quickly detect and correct any resulting problems.\n\nEven when our changes are categorized as standard changes, they\n\nstill need to be visual and recorded in our change management\n\nsystems (e.g., Remedy or ServiceNow). Ideally, deployments will be performed automatically by our configuration management\n\nand deployment pipeline tools (e.g., Puppet, Chef, Jenkins) and the results will be automatically recorded. By doing this, everyone\n\nin our organization (DevOps or not) will have visibility into our changes in addition to all the other changes happening in the\n\norganization.\n\nWe may automatically link these change request records to\n\nspecific items in our work planning tools (e.g., JIRA, Rally, LeanKit, ThoughtWorks Mingle), allowing us to create more\n\ncontext for our changes, such as linking to feature defects, production incidents, or user stories. This can be accomplished in\n\na lightweight way by including ticket numbers from planning tools in the comments associated with version control check ins.§ By doing this, we can trace a production deployment to the changes\n\nin version control and, from there, trace them further back to the planning tool tickets.\n\nCreating this traceability and context should be easy and should\n\nnot create an overly onerous or time-consuming burden for engineers. Linking to user stories, requirements, or defects is\n\nalmost certainly sufficient—any further detail, such as opening a ticket for each commit to version control, is likely not useful, and\n\nthus unnecessary and undesired, as it will impose a significant level of friction on their daily work.\n\nWHAT TO DO WHEN CHANGES ARE CATEGORIZED AS NORMAL CHANGES\n\nFor those changes that we cannot get classified as standard\n\nchanges, they will be considered normal changes and will require approval from at least a subset of the CAB before deployment. In\n\nthis case, our goal is still to ensure that we can deploy quickly, even if it is not fully automated.\n\nIn this case, we must ensure that any submitted change requests\n\nare as complete and accurate as possible, giving the CAB\n\neverything they need to properly evaluate our change—after all, if our change request is malformed or incomplete, it will be bounced\n\nback to us, increasing the time required for us to get into production and casting doubt on whether we actually understand\n\nthe goals of the change management process.\n\nWe can almost certainly automate the creation of complete and accurate RFCs, populating the ticket with details of exactly what is\n\nto be changed. For instance, we could automatically create a\n\nServiceNow change ticket with a link to the JIRA user story, along with the build manifests and test output from our deployment\n\npipeline tool and links to the Puppet/Chef scripts that will be run.\n\nBecause our submitted changes will be manually evaluated by people, it is even more important that we describe the context of\n\nthe change. This includes identifying why we are making the\n\nchange (e.g., providing a link to the features, defects, or incidents),\n\nwho is affected by the change, and what is going to be changed.\n\nOur goal is to share the evidence and artifacts that give us\n\nconfidence that the change will operate in production as designed. Although RFCs typically have free-form text fields, we should\n\nprovide links to machine-readable data to enable others to integrate and process our data (e.g., links to JSON files).\n\nIn many toolchains, this can be done in a compliant and fully\n\nautomated way. For example, ThoughtWorks’ Mingle and Go can automatically link this information together, such as a list of\n\ndefects fixed and new features completed that are associated with\n\nthe change, and put it into an RFC.\n\nUpon submission of our RFC, the relevant members of the CAB will review, process, and approve these changes as they would any\n\nother submitted change request. If all goes well, the change authorities will appreciate the thoroughness and detail of our\n\nsubmitted changes, because we have allowed them to quickly validate the correctness of the information we’ve provided (e.g.,\n\nviewing the links to artifacts from our deployment pipeline tools).\n\nHowever, our goal should be to continually show an exemplary track record of successful changes, so we can eventually gain their\n\nagreement that our automated changes can be safely classified as standard changes.\n\nCase Study Automated Infrastructure Changes as Standard Changes at Salesforce.com (2012)\n\nSalesforce was founded in 2000 with the aim of making customer relationship management easily available and\n\ndeliverable as a service. Salesforce’s offerings were widely adopted by the marketplace, leading to a successful IPO in 2004. By 2007, the company had over fifty-nine thousand\n\nenterprise customers, processing hundreds of millions of transactions per day, with annual revenue of $497 million.\n\nHowever, around that same time, their ability to develop and\n\nrelease new functionality to their customers seemed to grind to a halt. In 2006, they had four major customer releases, but in 2007 they were only able to do one customer release\n\ndespite having hired more engineers. The result was that the number of features delivered per team kept decreasing and the days between major releases kept increasing.\n\nAnd because the batch size of each release kept getting larger, the deployment outcomes also kept getting worse. Karthik Rajan, then VP of Infrastructure Engineering, reports in a 2013 presentation that 2007 marked “the last year when\n\nsoftware was created and shipped using a waterfall process and when we made our shift to a more incremental delivery process.”\n\nAt the 2014 DevOps Enterprise Summit, Dave Mangot and Reena Mathew described the resulting multi-year DevOps transformation that started in 2009. According to Mangot\n\nand Mathew, by implementing DevOps principles and practices, the company reduced their deployment lead times from six days to five minutes by 2013. As a result, they were able to scale capacity more easily, allowing them to process\n\nover one billion transactions per day.\n\nOne of the main themes of the Salesforce transformation was to make quality engineering everyone’s job, regardless\n\nof whether they were part of Development, Operations, or Infosec. To do this, they integrated automated testing into all stages of the application and environment creation, as well\n\nas into the continuous integration and deployment process, and created the open source tool Rouster to conduct functional testing of their Puppet modules.\n\nThey also started to routinely perform destructive testing, a term used in manufacturing to refer to performing prolonged endurance testing under the most severe operating conditions until the component being tested is destroyed.\n\nThe Salesforce team started routinely testing their services under increasingly higher loads until the service broke, which helped them understand their failure modes and make\n\nappropriate corrections. Unsurprisingly, the result was significantly higher service quality with normal production loads.\n\nInformation Security also worked with Quality Engineering at the earliest stages of their project, continually collaborating in critical phases such as architecture and test design, as\n\nwell as properly integrating security tools into the automated testing process.\n\nFor Mangot and Mathew, one of the key successes from all\n\nthe repeatability and rigor they designed into the process was being told by their change management group that “infrastructure changes made through Puppet would now be treated as ‘standard changes,’ requiring far less or even no\n\nfurther approvals from the CAB.” Furthermore, they noted\n\nthat “manual changes to infrastructure would still require approvals.”\n\nBy doing this, they had not only integrated their DevOps processes with the change management process, but also created further motivation to automate the change process\n\nfor more of their infrastructure.\n\nREDUCE RELIANCE ON SEPARATION OF DUTY\n\nFor decades, we have used separation of duty as one of our primary controls to reduce the risk of fraud or mistakes in the\n\nsoftware development process. It has been the accepted practice in most SDLCs to require developer changes to be submitted to a code librarian, who would review and approve the change before\n\nIT Operations promoted the change into production.\n\nThere are plenty of other less contentious examples of separation of duty in Ops work, such as server administrators ideally being\n\nable to view logs but not delete or modify them, in order to prevent someone with privileged access from deleting evidence of fraud or other issues.\n\nWhen we did production deployments less frequently (e.g., annually) and when our work was less complex, compartmentalizing our work and doing hand-offs were tenable\n\nways of conducting business. However, as complexity and deployment frequency increase, performing production deployments successfully increasingly requires everyone in the value stream to quickly see the outcomes of their actions.\n\nSeparation of duty often can impede this by slowing down and reducing the feedback engineers receive on their work. This prevents engineers from taking full responsibility for the quality of\n\ntheir work and reduces a firm’s ability to create organizational learning.\n\nConsequently, wherever possible, we should avoid using separation of duties as a control. Instead, we should choose controls such as pair programming, continuous inspection of code check-ins, and code review. These controls can give us the\n\nnecessary reassurance about the quality of our work. Furthermore, by putting these controls in place, if separation of duties is required, we can show that we achieve equivalent outcomes with\n\nthe controls we have created.\n\nCase Study PCI Compliance and a Cautionary Tale of Separating Duties at Etsy (2014)\n\nBill Massie is a development manager at Etsy and is responsible for the payment application called ICHT (an\n\nabbreviation for “I Can Haz Tokens”). ICHT takes customer credit orders through a set of internally-developed payment processing applications that handle online order entry by\n\ntaking customer-entered cardholder data, tokenizing it, communicating with the payment processor, and completing the order transaction.¶\n\nBecause the scope of the Payment Card Industry Data Security Standards (PCI DSS) cardholder data environment (CDE) is “the people, processes and technology that store,\n\nprocess or transmit cardholder data or sensitive authentication data,” including any connected system components, the ICHT application has in scope for the PCI\n\nDSS.\n\nTo contain the PCI DSS scope, the ICHT application is physically and logically separated from the rest of the Etsy\n\norganization and is managed by a completely separate application team of developers, database engineers, networking engineers, and ops engineers. Each team\n\nmember is issued two laptops: one for ICHT (which are configured differently to meet the DSS requirements, as well as being locked in a safe when not in use) and one for the rest of Etsy.\n\nBy doing this, they were able to decouple the CDE environment from the rest of the Etsy organization, limiting\n\nthe scope of the PCI DSS regulations to one segregated area. The systems that form the CDE are separated (and managed differently) from the rest of Etsy’s environments at the physical, network, source code, and logical infrastructure\n\nlevels. Furthermore, the CDE is built and operated by a cross-functional team that is solely responsible for the CDE.\n\nThe ICHT team had to modify their continuous delivery\n\npractices in order to accommodate the need for code approvals. According to Section 6.3.2 of the PCI DSS v3.1, teams should review:\n\nAll custom code prior to release to production or customers in order to identify any potential coding vulnerability (using either manual or automated processes) as follows:\n\nAre code changes reviewed by individuals other than the originating code author, and by individuals knowledgeable about code-review techniques and secure coding practices?\n\nDo code reviews ensure code is developed according to secure coding guidelines?\n\nAre appropriate corrections implemented prior to release?\n\nAre code review results reviewed and approved by management prior to release?\n\nTo fulfill this requirement, the team initially decided to designate Massie as the change approver responsible for deploying any changes into production. Desired\n\ndeployments would be flagged in JIRA, and Massie would mark them as reviewed and approved, and manually deploy them into the ICHT production.\n\nThis has enabled Etsy to meet their PCI DSS requirements and get their signed Report of Compliance from their assessors. However, with regard to the team, significant\n\nproblems have resulted.\n\nMassie observes that one troubling side effect “is a level of ‘compartmentalization’ that is happening in the ICHT team\n\nthat no other group is having at Etsy. Ever since we implemented separation of duty and other controls required\n\nby the PCI DSS compliance, no one can be a full-stack\n\nengineer in this environment.”\n\nAs a result, while the rest of the Development and Operations teams at Etsy work together closely and deploy",
      "page_number": 479
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 488-496)",
      "start_page": 488,
      "end_page": 496,
      "detection_method": "topic_boundary",
      "content": "changes smoothly and with confidence, Massie notes that\n\n“within our PCI environment, there is fear and reluctance\n\naround deployment and maintenance because no one has visibility outside their portion of the software stack. The\n\nseemingly minor changes we made to the way we work\n\nseem to have created an impenetrable wall between developers and ops, and creates an undeniable tension that\n\nno one at Etsy has had since 2008. Even if you have confidence in your portion, it’s impossible to get confidence\n\nthat someone else’s change isn’t going to break your part of\n\nthe stack.”\n\nThis case study shows that compliance is possible in organizations using DevOps. However, the potentially\n\ncautionary tale here is that all the virtues that we associate with high-performing DevOps teams are fragile—even a\n\nteam that has shared experiences with high trust and shared\n\ngoals can begin to struggle when low trust control mechanisms are put into place.\n\nENSURE DOCUMENTATION AND PROOF FOR AUDITORS AND COMPLIANCE OFFICERS\n\nAs technology organizations increasingly adopt DevOps patterns,\n\nthere is more tension than ever between IT and audit. These new\n\nDevOps patterns challenge traditional thinking about auditing, controls, and risk mitigation.\n\nAs Bill Shinn, a principal security solutions architect at Amazon\n\nWeb Services, observes, “DevOps is all about bridging the gap\n\nbetween Dev and Ops. In some ways, the challenge of bridging the gap between DevOps and auditors and compliance officers is even\n\nlarger. For instance, how many auditors can read code and how\n\nmany developers have read NIST 800-37 or the Gramm-Leach- Bliley Act? That creates a gap of knowledge, and the DevOps\n\ncommunity needs to help bridge that gap.”\n\nCase Study Proving Compliance in Regulated Environments (2015)\n\nHelping large enterprise customers show that they can still\n\ncomply with all relevant laws and regulations is among Bill Shinn’s responsibilities as a principal security solutions\n\narchitect at Amazon Web Services. Over the years, he has\n\nspent time with over one thousand enterprise customers, including Hearst Media, GE, Phillips, and Pacific Life, who\n\nhave publicly referenced their use of public clouds in highly\n\nregulated environments.\n\nShinn notes, “One of the problems is that auditors have\n\nbeen trained in methods that aren’t very suitable for DevOps\n\nwork patterns. For example, if an auditor saw an environment with ten thousand productions servers, they\n\nhave been traditionally trained to ask for a sample of one\n\nthousand servers, along with screenshot evidence of asset management, access control settings, agent installations,\n\nserver logs, and so forth.”\n\n“That was fine with physical environments,” Shinn continues.\n\n“But when infrastructure is code, and when auto-scaling\n\nmakes servers appear and disappear all the time, how do you sample that? You run into the same problems when you\n\nhave a deployment pipeline, which is very different than the traditional software development process, where one group\n\nwrites the code and another group deploys that code into\n\nproduction.”\n\nHe explains, “In audit fieldwork, the most commonplace methods of gathering evidence are still screenshots and\n\nCSV files filled with configuration settings and logs. Our goal is to create alternative methods of presenting the data that\n\nclearly show auditors that our controls are operating and\n\neffective.”\n\nTo help bridge that gap, he has teams work with auditors in\n\nthe control design process. They use an iterative approach,\n\nassigning a single control for each sprint to determine what is needed in terms of audit evidence. This has helped\n\nensure that auditors get the information they need when the\n\nservice is in production, entirely on demand.\n\nShinn states that the best way to accomplish this is to “send\n\nall data into our telemetry systems, such as Splunk or\n\nKibana. This way auditors can get what they need, completely self-serviced. They don’t need to request a data\n\nsample—instead, they log into Kibana, and then search for\n\naudit evidence they need for a given time range. Ideally, they’ll see very quickly that there’s evidence to support that\n\nour controls are working.”\n\nShinn continues, “With modern audit logging, chat rooms,\n\nand deployment pipelines, there’s unprecedented visibility\n\nand transparency into what’s happening in production, especially compared to how Operations used to be done,\n\nwith far lower probability of errors and security flaws being introduced. So, the challenge is to turn all that evidence into\n\nsomething an auditor recognizes.”\n\nThat requires deriving the engineering requirements from the actual regulations. Shinn explains, “To discover what\n\nHIPAA requires from an information security perspective,\n\nyou have to look into the forty-five CFR Part 160 legislation, go into Subparts A and C of Part 164. Even then, you need\n\nto keep reading until you get into ‘technical safeguards and\n\naudit controls.’ Only there will you see that what is required is that we need to determine activities that will be tracked\n\nand audited relevant to Patient Healthcare Information,\n\ndocument and implement those controls, select tools, and then finally review and capture the appropriate information.”\n\nShinn continues, “How to fulfill that requirement is the\n\ndiscussion that needs to be happening between compliance and regulatory officers, and the security and DevOps teams,\n\nspecifically around how to prevent, detect, and correct\n\nproblems. Sometimes they can be fulfilled in a configuration setting in version control. Other times, it’s a monitoring\n\ncontrol.”\n\nShinn gives an example: “We may choose to implement one of those controls using AWS CloudWatch, and we can test\n\nthat the control is operating with one command line.\n\nFurthermore, we need to show where the logs are going—in\n\nthe ideal, we push all this into our logging framework, where\n\nwe can link the audit evidence with the actual control requirement.”\n\nTo help solve this problem, the DevOps Audit Defense\n\nToolkit describes the end-to-end narrative of the compliance\n\nand audit process for a fictitious organization (Parts Unlimited from The Phoenix Project). It starts by describing\n\nthe entity’s organizational goals, business processes, top risks, and resulting control environment, as well as how\n\nmanagement could successfully prove that controls exist\n\nand are effective. A set of audit objections is also presented, as well as how to overcome them.\n\nThe document describes how controls could be designed in\n\na deployment pipeline to mitigate the stated risks, and provides examples of control attestations and control\n\nartifacts to demonstrate control effectiveness. It was\n\nintended to be general to all control objectives, including in support of accurate financial reporting, regulatory\n\ncompliance (e.g., SEC SOX-404, HIPAA, FedRAMP, EU\n\nModel Contracts, and the proposed SEC Reg-SCI regulations), contractual obligations (e.g., PCI DSS, DOD\n\nDISA), and effective and efficient operations.\n\nCase Study Relying on Production Telemetry for ATM Systems\n\nMary Smith (a pseudonym) heads up the DevOps initiative\n\nfor the consumer banking property of a large US financial\n\nservices organization. She made the observation that\n\ninformation security, auditors, and regulators often put too much reliance on code reviews to detect fraud. Instead, they\n\nshould be relying on production monitoring controls in\n\naddition to using automated testing, code reviews, and approvals, to effectively mitigate the risks associated with\n\nerrors and fraud.\n\nShe observed:\n\nMany years ago, we had a developer who planted a\n\nbackdoor in the code that we deploy to our ATM cash\n\nmachines. They were able to put the ATMs into maintenance mode at certain times, allowing them to\n\ntake cash out of the machines. We were able to detect\n\nthe fraud very quickly, and it wasn’t through a code review. These types of backdoors are difficult, or even\n\nimpossible, to detect when the perpetrators have\n\nsufficient means, motive, and opportunity.\n\nHowever, we quickly detected the fraud during our\n\nregularly operations review meeting when someone\n\nnoticed that ATMs in a city were being put into maintenance mode at unscheduled times. We found\n\nthe fraud even before the scheduled cash audit\n\nprocess, when they reconcile the amount of cash in the ATMs with authorized transactions.\n\nIn this case study, the fraud occurred despite separation of\n\nduties between Development and Operations and a change approval process, but was quickly detected and corrected\n\nthrough effective production telemetry.\n\nCONCLUSION\n\nThroughout this chapter, we have discussed practices that make\n\ninformation security everyone’s job, where all of our information security objectives are integrated into the daily work of everyone\n\nin the value stream. By doing this, we significantly improve the\n\neffectiveness of our controls, so that we can better prevent security breaches, as well as detect and recover from them faster. And we\n\nsignificantly reduce the work associated with preparing and\n\npassing compliance audits.\n\nPART VI CONCLUSION\n\nThroughout the previous chapters, we explored how to take\n\nDevOps principles and apply them to Information Security, helping us achieve our goals, and making sure security is a part of\n\neveryone’s job, every day. Better security ensures that we are\n\ndefensible and sensible with our data, that we can recover from security problems before they become catastrophic, and, most\n\nimportantly, that we can make the security of our systems and data better than ever.\n\n† ITIL defines utility as “what the service does,” while warranty is defined as “how the service is delivered and can\n\nbe used to determine whether a service is ‘fit for use.’”\n\n‡ To further manage risk changes, we may also have defined rules, such as certain changes can only be\n\nimplemented by a certain group or individual (e.g., only DBAs can deploy database schema changes). Traditionally, the CAB meetings have been held weekly, where the change requests are approved and scheduled. From ITIL version 3 onward, it is acceptable for changes to be approved electronically in a just-in- time fashion through a change management tool. It also specifically recommends that “standard changes should be identified early on when building the Change Management process to promote efficiency. Otherwise, a Change Management implementation can create unnecessarily high levels of administration and resistance to the Change Management process.”\n\n§ The term ticket is used generically to indicate any uniquely identifiable work item.\n\n¶ The authors thank Bill Massie and John Allspaw for spending an entire day with Gene Kim sharing their\n\ncompliance experience.\n\nA Call to Action",
      "page_number": 488
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 497-504)",
      "start_page": 497,
      "end_page": 504,
      "detection_method": "topic_boundary",
      "content": "Conclusion to the DevOps\n\nHandbook\n\nWe have come to the end of a detailed exploration of both the\n\nprinciples and technical practices of DevOps. At a time when every\n\ntechnology leader is challenged with enabling security, reliability,\n\nand agility, and at a time when security breaches, time to market,\n\nand massive technology transformation is taking place, DevOps\n\noffers a solution. Hopefully, this book has provided an in-depth understanding of the problem and a road map to creating relevant\n\nsolutions.\n\nAs we have explored throughout The DevOps Handbook, we know that, left unmanaged, an inherent conflict can exist between\n\nDevelopment and Operations that creates ever-worsening problems,which results in slower time to market for new products and features, poor quality, increased outages and technical debt, reduced engineering productivity, as well as increased employee dissatisfaction and burnout.\n\nDevOps principles and patterns enable us to break this core, chronic conflict. After reading this book, we hope you see how a DevOps transformation can enable the creation of dynamic\n\nlearning organizations, achieving the amazing outcomes of fast flow and world-class reliability and security, as well as increased competitiveness and employee satisfaction.\n\nDevOps requires potentially new cultural and management norms, and changes in our technical practices and architecture. This\n\nrequires a coalition that spans business leadership, Product\n\nManagement, Development, QA, IT Operations, Information\n\nSecurity, and even Marketing, where many technology initiatives\n\noriginate. When all these teams work together, we can create a\n\nsafe system of work, enabling small teams to quickly and\n\nindependently develop and validate code that can be safely\n\ndeployed to customers. This results in maximizing developer\n\nproductivity, organizational learning, high employee satisfaction,\n\nand the ability to win in the marketplace.\n\nOur goal in writing this book was to sufficiently codify DevOps\n\nprinciples and practices so that the amazing outcomes achieved within the DevOps community could be replicated by others. We hope to accelerate the adoption of DevOps initiatives and support\n\ntheir successful implementations while lowering the activation energy required for them to be completed.\n\nWe know the dangers of postponing improvements and settling for daily work-arounds, as well as the difficulties of changing how we prioritize and perform our daily work. Furthermore, we understand the risks and effort required to get organizations to embrace a different way of working, as well as the perception that DevOps is another passing fad, soon to replaced by the next\n\nbuzzword.\n\nWe assert that DevOps is transformational to how we perform\n\ntechnology work, just as Lean forever transformed how manufacturing work was performed in the 1980s. Those that adopt DevOps will win in the marketplace, at the expense of those that do not. They will create energized and continually learning\n\norganizations that out-perform and out-innovate their competitors.\n\nBecause of this, DevOps is not just a technology imperative, but also an organizational imperative. The bottom line is, DevOps is\n\napplicable and relevant to any and all organizations that must increase flow of planned work through the technology\n\norganization, while maintaining quality, reliability, and security\n\nfor our customers.\n\nOur call to action is this: no matter what role you play in your organization, start finding people around you who want to change\n\nhow work is performed. Show this book to others and create a coalition of like-minded thinkers to break out of the downward\n\nspiral. Ask organizational leaders to support these efforts, or, better yet, sponsor and lead these efforts yourself.\n\nFinally, since you’ve made it this far, we have a dirty secret to\n\nreveal. In many of our case studies, following the achievement of\n\nthe breakthrough results presented, many of the change agents were promoted—but, in some cases, there was later a change of\n\nleadership which resulted in many of the people involved leaving, accompanied by a rolling back of the organizational changes they\n\nhad created.\n\nWe believe it’s important not to be cynical about this possibility. The people involved in these transformations knew up front that\n\nwhat they were doing had a high chance of failure, and they did it\n\nanyway. In doing so, perhaps most importantly, they inspired the rest of us by showing us what can be done. Innovation is\n\nimpossible without risk taking, and if you haven’t managed to upset at least some people in management, you’re probably not\n\ntrying hard enough. Don’t let your organization’s immune system deter or distract you from your vision. As Jesse Robbins,\n\npreviously “master of disaster” at Amazon, likes to say, “Don’t fight stupid, make more awesome.”\n\nDevOps benefits all of us in the technology value stream, whether\n\nwe are Dev, Ops, QA, Infosec, Product Owners, or customers. It brings joy back to developing great products, with fewer death\n\nmarches. It enables humane work conditions with fewer weekends and missed holidays with our loved ones. It enables teams to work\n\ntogether to survive, learn, thrive, delight our customers, and help\n\nour organization succeed.\n\nWe sincerely hope The DevOps Handbook helps you achieve these goals.\n\nAppendices\n\nAPPENDIX 1 THE CONVERGENCE OF DEVOPS\n\nWe believe that DevOps is benefiting from an incredible\n\nconvergence of management movements, which are all mutually reinforcing and can help create a powerful coalition to transform\n\nhow organizations develop and deliver IT products and services.\n\nJohn Willis named this “the Convergence of DevOps.” The various elements of this convergence are described below in approximate\n\nchronological order. (Note that these descriptions are not intended to be an exhaustive description, but merely enough to\n\nshow the progression of thinking and the rather improbable\n\nconnections that led to DevOps.)\n\nTHE LEAN MOVEMENT\n\nThe Lean Movement started in the 1980s as an attempt to codify the Toyota Production System with the popularization of techniques such as Value Stream Mapping, kanban boards, and\n\nTotal Productive Maintenance.\n\nTwo major tenets of Lean were the deeply held belief that lead time (i.e., the time required to convert raw materials into finished goods) was the best predictor of quality, customer satisfaction,\n\nand employee happiness; and that one of the best predictors of\n\nshort lead times was small batch sizes, with the theoretical ideal\n\nbeing “single piece flow” (i.e., “1x1” flow: inventory of 1, batch size\n\nof 1).\n\nLean principles focus on creating value for the customer—thinking\n\nsystematically, creating constancy of purpose, embracing scientific\n\nthinking, creating flow and pull (versus push), assuring quality at\n\nthe source, leading with humility, and respecting every individual.\n\nTHE AGILE MOVEMENT\n\nStarted in 2001, the Agile Manifesto was created by seventeen of the leading thinkers in software development, with the goal of\n\nturning lightweight methods such as DP and DSDM into a wider movement that could take on heavyweight software development processes such as waterfall development and methodologies such as the Rational Unified Process.\n\nA key principle was to “deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.” Two other principles focus on the need for\n\nsmall, self-motivated teams, working in a high-trust management model and an emphasis on small batch sizes. Agile is also\n\nassociated with a set of tools and practices such as Scrum, Standups, and so on.\n\nTHE VELOCITY CONFERENCE MOVEMENT\n\nStarted in 2007, the Velocity Conference was created by Steve Souders, John Allspaw, and Jesse Robbins to provide a home for the IT Operations and Web Performance tribe. At the Velocity\n\n2009 conference, John Allspaw and Paul Hammond gave the\n\nseminal “10 Deploys per Day: Dev and Ops Cooperation at Flickr.”\n\nTHE AGILE INFRASTRUCTURE MOVEMENT\n\nAt the 2008 Agile Toronto conference, Patrick Dubois and Andrew\n\nSchafer held a “birds of a feather” session on applying Agile\n\nprinciples to infrastructure as opposed to application code. They\n\nrapidly gained a following of like-minded thinkers, including John\n\nWillis. Later, Dubois was so excited by Allspaw and Hammond’s\n\n“10 Deploys per Day: Dev and Ops Cooperation at Flickr”\n\npresentation that he created the first DevOpsDays in Ghent,\n\nBelgium, in 2009, coining the word “DevOps.”\n\nTHE CONTINUOUS DELIVERY MOVEMENT\n\nBuilding upon the Development discipline of continuous build, test, and integration, Jez Humble and David Farley extended the concept of continuous delivery, which included a “deployment\n\npipeline” to ensure that code and infrastructure are always in a deployable state and that all code checked in to truck is deployed into production.\n\nThis idea was first presented at Agile 2006 and was also\n\nindependently developed by Tim Fitz in a blog post titled “Continuous Deployment.”\n\nTHE TOYOTA KATA MOVEMENT\n\nIn 2009, Mike Rother wrote Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results, which described learnings over his twenty-year journey to understand\n\nand codify the causal mechanisms of the Toyota Production\n\nSystem. Toyota Kata describes the “unseen managerial routines and thinking that lie behind Toyota’s success with continuous\n\nimprovement and adaptation… and how other companies develop similar routines and thinking in their organizations.”\n\nHis conclusion was that the Lean community missed the most\n\nimportant practice of all, which he described as the Improvement\n\nKata. He explains that every organization has work routines, and the critical factor in Toyota was making improvement work\n\nhabitual, and building it into the daily work of everyone in the organization. The Toyota Kata institutes an iterative, incremental,\n\nscientific approach to problem solving in the pursuit of a shared organizational true north.\n\nTHE LEAN STARTUP MOVEMENT\n\nIn 2011, Eric Ries wrote The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically\n\nSuccessful Businesses, codifying his lessons learned at IMVU, a Silicon Valley startup, which built upon the work of Steve Blank in\n\nThe Four Steps to the Epiphany as well as continuous deployment\n\ntechniques. Eric Ries also codified related practices and terms including Minimum Viable Product, the build-measure-learn\n\ncycle, and many continuous deployment technical patterns.\n\nTHE LEAN UX MOVEMENT\n\nIn 2013, Jeff Gothelf wrote Lean UX: Applying Lean Principles to Improve User Experience, which codified how to improve the\n\n“fuzzy front end” and explained how product owners can frame business hypotheses, experiment, and gain confidence in those\n\nbusiness hypotheses before investing time and resources in the",
      "page_number": 497
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 505-513)",
      "start_page": 505,
      "end_page": 513,
      "detection_method": "topic_boundary",
      "content": "resulting features. By adding Lean UX, we now have the tools to fully optimize the flow between business hypotheses, feature\n\ndevelopment, testing, deployment, and service delivery to the customer.\n\nTHE RUGGED COMPUTING MOVEMENT\n\nIn 2011, Joshua Corman, David Rice, and Jeff Williams examined the apparent futility of securing applications and environments\n\nlate in the life cycle. In response, they created a philosophy called “Rugged Computing,” which attempts to frame the non-functional\n\nrequirements of stability, scalability, availability, survivability, sustainability, security, supportability, manageability, and\n\ndefensibility.\n\nBecause of the potential for high release rates, DevOps can put\n\nincredible pressure on QA and Infosec, because when deploy rates go from monthly or quarterly to hundreds or thousands daily, no\n\nlonger are two week turnaround times from Infosec or QA tenable. The Rugged Computing movement posited that the current\n\napproach to fighting the vulnerable industrial complex being employed by most information security programs is hopeless.\n\nAPPENDIX 2 THEORY OF CONSTRAINTS AND CORE, CHRONIC CONFLICTS\n\nThe Theory of Constraints body of knowledge extensively\n\ndiscusses the use of creating core conflict clouds (often referred to\n\nas “C3”). Here is the conflict cloud for IT:\n\nFigure 46: The core, chronic conflict facing every IT organization\n\nDuring the 1980s, there was a very well-known core, chronic\n\nconflict in manufacturing. Every plant manager had two valid business goals: protect sales and reduce costs. The problem was\n\nthat in order to protect sales, sales management was incentivized\n\nto increase inventory to ensure that it was always possible to fulfill customer demand.\n\nOn the other hand, in order to reduce cost, production\n\nmanagement was incentivized to decrease inventory to ensure that money was not tied up in work in progress that wasn’t\n\nimmediately shippable to the customer in the form of fulfilled sales.\n\nThey were able to break the conflict by adopting Lean principles,\n\nsuch as reducing batch sizes, reducing work in process, and\n\nshortening and amplifying feedback loops. This resulted in dramatic increases in plant productivity, product quality, and\n\ncustomer satisfaction.\n\nThe principles behind DevOps work patterns are the same as those\n\nthat transformed manufacturing, allowing us to optimize the IT\n\nvalue stream, converting business needs into capabilities and services that provide value for our customers.\n\nAPPENDIX 3 TABULAR FORM OF DOWNWARD SPIRAL\n\nThe columnar form of the downward spiral depicted in The Phoenix Project is shown below:\n\nTable 4: The Downward Spiral\n\nAPPENDIX 4 THE DANGERS OF HANDOFFS AND QUEUES\n\nThe problem with high amounts of queue time is exacerbated\n\nwhen there are many handoffs, because that is where queues are created. Figure 47 shows wait time as a function of how busy a resource at a work center is. The asymptotic curve shows why a\n\n“simple thirty-minute change” often takes weeks to complete— specific engineers and work centers often become problematic bottlenecks when they operate at high utilization. As a work center approaches 100% utilization, any work required from it will\n\nlanguish in queues and won’t be worked on without someone expediting/escalating.\n\nFigure 47: Queue size and wait times as function of percent utilization (Source: Kim, Behr, and Spafford, The Phoenix Project, ePub edition, 557.)\n\nIn figure 47, the x-axis is the percent busy for a given resource at a work center, and the y-axis is the approximate wait time (or, more precisely stated, the queue length). What the shape of the line\n\nshows is that as resource utilization goes past 80%, wait time goes through the roof.\n\nIn The Phoenix Project, here’s how Bill and his team realized the devastating consequences of this property on lead times for the commitments they were making to the project management office:\n\nI tell them about what Erik told me at MRP-8, about how wait times depend upon resource utilization. “The wait time is the ‘percentage of time busy’ divided by the ‘percentage of time\n\nidle.’ In other words, if a resource is fifty percent busy, then it’s fifty percent idle. The wait time is fifty percent divided by fifty percent, so one unit of time. Let’s call it one hour.\n\nSo, on average, our task would wait in the queue for one hour before it gets worked.\n\n“On the other hand, if a resource is ninety percent busy, the wait time is ‘ninety percent divided by ten percent,’ or nine hours. In other words, our task would wait in queue nine times longer than if the resource were fifty percent idle.”\n\nI conclude, “So…For the Phoenix task, assuming we have seven handoffs, and that each of those resources is busy ninety percent of the time, the tasks would spend in queue a total of\n\nnine hours times the seven steps…”\n\n“What? Sixty-three hours, just in queue time?” Wes says, incredulously. “That’s impossible!”\n\nPatty says with a smirk, “Oh, of course. Because it’s only thirty seconds of typing, right?”\n\nBill and team realize that their “simple thirty-minute task” actually requires seven handoffs (e.g., server team, networking team, database team, virtualization team, and, of course, Brent,\n\nthe “rockstar” engineer).\n\nAssuming that all work centers were 90% busy, the figure shows\n\nus that the average wait time at each work center is nine hours— and because the work had to go through seven work centers, the total wait time is seven times that: sixty-three hours.\n\nIn other words, the total % of value added time (sometimes known as process time) was only 0.16% of the total lead time (thirty minutes divided by sixty-three hours). That means that for 99.8% of our total lead time, the work was simply sitting in queue,\n\nwaiting to be worked on.\n\nAPPENDIX 5 MYTHS OF INDUSTRIAL SAFETY\n\nDecades of research into complex systems shows that\n\ncountermeasures are based on several myths. In “Some Myths about Industrial Safety,” by Denis Besnard and Erik Hollnagel, they are summarized as such:\n\nMyth 1: “Human error is the largest single cause of accidents and incidents.”\n\nMyth 2: “Systems will be safe if people comply with the\n\nprocedures they have been given.”\n\nMyth 3: “Safety can be improved by barriers and protection; more layers of protection results in higher safety.”\n\nMyth 4: “Accident analysis can identify the root cause (the ‘truth’) of why the accident happened.”\n\nMyth 5: “Accident investigation is the logical and rational identification of causes based on facts.”\n\nMyth 6: “Safety always has the highest priority and will never\n\nbe compromised.”\n\nThe differences between what is myth and what is true are shown below:\n\nTable 5: Two Stories\n\nAPPENDIX 6 THE TOYOTA ANDON CORD\n\nMany ask how can any work be completed if the Andon cord is being pulled over five thousand times per day? To be precise, not\n\nevery Andon cord pull results in stopping the entire assembly line. Rather, when the Andon cord is pulled, the team leader overseeing the specified work center has fifty seconds to resolve the problem.\n\nIf the problem has not been resolved by the time the fifty seconds\n\nis up, the partially assembled vehicle will cross a physically drawn line on the floor, and the assembly line will be stopped.\n\nFigure 48: The Toyota Andon cord\n\nAPPENDIX 7 COTS SOFTWARE\n\nCurrently, in order to get complex COTS (commercial off-the- shelf) software (e.g., SAP, IBM WebSphere, Oracle WebLogic) into\n\nversion control, we may have to eliminate the use of graphical\n\npoint-and-click vendor installer tools. To do that, we need to discover what the vendor installer is doing, and we may need to do\n\nan install on a clean server image, diff the file system, and put\n\nthose added files into version control. Files that don’t vary by environment are put into one place (“base install”), while\n\nenvironment-specific files are put into their own directory (“test”\n\nor “production”). By doing this, software install operations\n\nbecome merely a version control operation, enabling better\n\nvisibility, repeatability, and speed.\n\nWe may also have to transform any application configuration\n\nsettings so that they are in version control. For instance, we may\n\ntransform application configurations that are stored in a database into XML files and vice versa.\n\nAPPENDIX 8 POST-MORTEM MEETINGS\n\nA sample agenda of the post-mortem meeting is shown below:\n\nAn initial statement will be made by the meeting leader or facilitator to reinforce that this meeting is a blameless post-\n\nmortem and that we will not focus on past events or speculate\n\non “would haves” or “could haves.” Facilitators might read the “Retrospective Prime Directive” from the website\n\nRetrospective.com.\n\nFurthermore, the facilitator will remind everyone that any countermeasures must be assigned to someone, and if the\n\ncorrective action does not warrant being a top priority when\n\nthe meeting is over, then it is not a corrective action. (This is to prevent the meeting from generating a list of good ideas that\n\nare never implemented.)\n\nThose at the meeting will reach an agreement on the complete timeline of the incident, including when and who detected the\n\nissue, how it was discovered (e.g., automated monitoring,",
      "page_number": 505
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 514-521)",
      "start_page": 514,
      "end_page": 521,
      "detection_method": "topic_boundary",
      "content": "manual detection, customer notified us), when service was\n\nsatisfactorily restored, and so forth. We will also integrate into\n\nthe timeline all external communications during the incident.\n\nWhen we use the word “timeline,” it may evoke the image of a\n\nlinear set of steps of how we gained an understanding of the problem and eventually fixed it. In reality, especially in\n\ncomplex systems, there will likely be many events that contributed to the accident, and many troubleshooting paths\n\nand actions will have been taken in an effort to fix it. In this\n\nactivity, we seek to chronicle all of these events and the perspectives of the actors and establish hypotheses concerning\n\ncause and effect where possible.\n\nThe team will create a list of all the factors which contributed to the incident, both human and technical. They may then sort\n\nthem into categories, such as ‘design decision,’ ‘remediation,’\n\n‘discovering there was a problem,’ and so forth. The team will use techniques such as brainstorming and the ‘infinite hows’ to\n\ndrill down on contributing factors they deem particularly\n\nimportant to discover deeper levels of contributing factors. All perspectives should be included and respected—nobody should\n\nbe permitted to argue with or deny the reality of a contributing\n\nfactor somebody else has identified. It’s important for the post- mortem facilitator to ensure that sufficient time is spent on this\n\nactivity, and that the team doesn’t try and engage in convergent behavior such as trying to identify one or more ‘root causes.’\n\nThose at the meeting will reach an agreement on the list of\n\ncorrective actions that will be made top priorities after the\n\nmeeting. Assembling this list will require brainstorming and\n\nchoosing the best potential actions to either prevent the issue\n\nfrom occurring or enable faster detection or recovery. Other\n\nways to improve the systems may also be included.\n\nOur goal is to identify the smallest number of incremental steps to achieve the desired outcomes, as opposed to “big bang”\n\nchanges, which not only take longer to implement, but delay\n\nthe improvements we need.\n\nWe will also generate a separate list of lower priority ideas and\n\nassign an owner. If similar problems occur in the future, these ideas may serve as the foundation for crafting future\n\ncountermeasures.\n\nThose at the meeting will reach an agreement on the incident metrics and their organizational impact. For example, we may\n\nchoose to measure our incidents by the following metrics:\n\n▹ Event severity: How severe was this issue? This directly relates to the impact on the service and our\n\ncustomers.\n\n▹ Total downtime: How long were customers unable\n\nto use the service to any degree?\n\n▹ Time to detect: How long did it take for us or our\n\nsystems to know there was a problem?\n\n▹ Time to resolve: How long after we knew there was a\n\nproblem did it take for us to restore service?\n\nBethany Macri from Etsy observed, “Blamelessness in a post-\n\nmortem does not mean that no one takes responsibility. It means\n\nthat we want to find out what the circumstances were that allowed\n\nthe person making the change or who introduced the problem to\n\ndo this. What was the larger environment….The idea is that by removing blame, you remove fear, and by removing fear, you get\n\nhonesty.”\n\nAPPENDIX 9 THE SIMIAN ARMY\n\nAfter the 2011 AWS EAST Outage, Netflix had numerous\n\ndiscussions about engineering their systems to automatically deal\n\nwith failure. These discussions have evolved into a service called “Chaos Monkey.”\n\nSince then, Chaos Monkey has evolved into a whole family of\n\ntools, known internally as the “Netflix Simian Army,” to simulate increasingly catastrophic levels of failures:\n\nChaos Gorilla: simulates the failure of an entire AWS\n\navailability zone\n\nChaos Kong: simulates failure of entire AWS regions, such as\n\nNorth America or Europe\n\nOther member of the Simian Army now include:\n\nLatency Monkey: induces artificial delays or downtime in their RESTful client-server communication layer to simulate\n\nservice degradation and ensure that dependent services respond appropriately\n\nConformity Monkey: finds and shuts down AWS instances\n\nthat don’t adhere to best-practices (e.g., when instances don’t\n\nbelong to an auto-scaling group or when there is no escalation\n\nengineer email address listed in the service catalog)\n\nDoctor Monkey: taps into health checks that run on each instance and finds unhealthy instances and proactively shuts\n\nthem down if owners don’t fix the root cause in time\n\nJanitor Monkey: ensures that their cloud environment is running free of clutter and waste; searches for unused\n\nresources and disposes of them\n\nSecurity Monkey: an extension of Conformity Monkey; finds and terminates instances with security violations or\n\nvulnerabilities, such as improperly configured AWS security\n\ngroups\n\nAPPENDIX 10 TRANSPARENT UPTIME\n\nLenny Rachitsky wrote about the benefits of what he called\n\n“transparent uptime”:\n\n1. Your support costs go down as your users are able to self-\n\nidentify system wide problems without calling or emailing your\n\nsupport department. Users will no longer have to guess whether their issues are local or global, and can more quickly\n\nget to the root of the problem before complaining to you.\n\n2. You are better able to communicate with your users during\n\ndowntime events, taking advantage of the broadcast nature of\n\nthe Internet versus the one-to-one nature of email and the\n\nphone. You spend less time communicating the same thing over and over and more time resolving the issue.\n\n3. You create a single and obvious place for your users to come to\n\nwhen they are experiencing downtime. You save your users’ time currently spent searching forums, Twitter, or your blog.\n\n4. Trust is the cornerstone of any successful SaaS adoption. Your\n\ncustomers are betting their business and their livelihoods on your service or platform. Both current and prospective\n\ncustomers require confidence in your service. Both need to\n\nknow they won’t be left in the dark, alone and uninformed, when you run into trouble. Real time insight into unexpected\n\nevents is the best way to build this trust. Keeping them in the\n\ndark and alone is no longer an option.\n\n5. It’s only a matter of time before every serious SaaS provider\n\nwill be offering a public health dashboard. Your users will\n\ndemand it.\n\nAdditional Resources\n\nMany of the common problems faced by IT organizations are discussed in the first half of the book The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business\n\nWin by Gene Kim, Kevin Behr, and George Spafford.\n\nThis video shows a speech Paul O’Neill gave on his tenure as CEO of Alcoa, including the\n\ninvestigation he took part in after a teenage worker was killed at one of Alcoa’s plants: https://www.youtube.com/watch?v=tC2ucDs_XJY .\n\nFor more on value stream mapping, see Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation by Karen Martin and Mike\n\nOsterling.\n\nFor more on ORMs, visit Stack Overflow:\n\nhttp://stackoverflow.com/questions/1279613/what-is-an-orm-and-where-can-i-learn- more-about-it .\n\nAn excellent primer on many agile development rituals and how to use them in IT\n\nOperations work can be found in a series of posts written on the Agile Admin blog:\n\nhttp://theagileadmin.com/2011/02/21/scrum-for-operations-what-is-scrum/ .\n\nFor more information on architecting for fast builds, see Daniel Worthington-Bodart’s\n\nblog post “Crazy Fast Build Times (or When 10 Seconds Starts to Make You Nervous)”: http://dan.bodar.com/2012/02/28/crazy-fast-build-times-or-when-10-seconds-starts-to-\n\nmake-you-nervous/ .\n\nFor more details on performance testing at Facebook, along with some detailed\n\ninformation on Facebook’s release process, check out Chuck Rossi’s presentation “The\n\nFacebook Release Process”: http://www.infoq.com/presentations/Facebook-Release-\n\nProcess .\n\nMany more variants of dark launching can be found in chapter 8 of The Practice of Cloud\n\nSystem Administration: Designing and Operating Large Distributed Systems, Volume 2 by Thomas A. Limoncelli, Strata R. Chalup, and Christina J. Hogan.\n\nThere is an excellent technical discussion of feature toggles here:\n\nhttp://martinfowler.com/articles/feature-toggles.html .\n\nReleases are discussed in more detail in The Practice of Cloud System Administration:\n\nDesigning and Operating Large Distributed Systems, Volume 2 by Thomas A. Limoncelli,\n\nStrata R. Chalup, and Christina J. Hogan; Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation by Jez Humble and David\n\nFarley; and Release It! Design and Deploy Production-Ready Software by Michael T. Nygard.\n\nA description of the circuit breaker pattern can be found here: http://martinfowler.com/bliki/CircuitBreaker.html .\n\nFor more on the cost of delay see The Principles of Product Development Flow: Second Generation Lean Product Development by Donald G. Reinertsen.\n\nA further discussion on staying ahead of failures for the Amazon S3 service can be found here: https://qconsf.com/sf2010/dl/qcon-sanfran- 2009/slides/JasonMcHugh_AmazonS3ArchitectingForResiliencyInTheFaceOfFailures.pdf\n\n.\n\nFor an excellent guide on conducting user research, see Lean UX: Applying Lean\n\nPrinciples to Improve User Experience by Jeff Gothelf and Josh Seiden.\n\nWhich Test Won? is a site that displays hundreds of real-life A/B tests and asks the viewer to guess which variant performed better, reinforcing the key that unless we actually test,\n\nwe’re merely guessing. Visit it here: http://whichtestwon.com/ .\n\nA list of architectural patterns can be found in Release It! Design and Deploy Production- Ready Software by Michael T. Nygard.\n\nAn example of published Chef post-mortem meeting notes can be found here: https://www.chef.io/blog/2014/08/14/cookbook-dependency-api-postmortem/ . A video of the meeting can be found here: https://www.youtube.com/watch?v=Rmi1Tn5oWfI .\n\nA current schedule of upcoming DevOpsDays can be found on the DevOpsDays website: http://www.devopsdays.org/ . Instructions on organizing a new DevOpsDays can be\n\nfound on the DevOpsDay Organizing Guide page: http://www.devopsdays.org/pages/organizing/ .\n\nMore on using tools to manage secrets can be found in Noah Kantrowitz’s post “Secrets\n\nManagement and Chef” on his blog: https://coderanger.net/chef-secrets/ .\n\nJames Wickett and Gareth Rushgrove have put all their examples of secure pipelines on\n\nthe GitHub website: https://github.com/secure-pipeline .\n\nThe National Vulnerability Database website and XML data feeds can be found at:\n\nhttps://nvd.nist.gov/ .\n\nA concrete scenario involving integration between Puppet and ThoughtWorks’ Go and\n\nMingle (a project management application) can be found in a Puppet Labs blog post by\n\nAndrew Cunningham and Andrew Myers and edited by Jez Humble: https://puppetlabs.com/blog/a-deployment-pipeline-for-infrastructure .\n\nPreparing and passing compliance audits is further explored in Jason Chan’s 2015 presentation “SEC310: Splitting the Check on Compliance and Security: Keeping Developers and Auditors Happy in the Cloud”: https://www.youtube.com/watch?\n\nv=Io00_K4v12Y&feature=youtu.be .\n\nThe story of how application configuration settings were transformed by Jez Humble and\n\nDavid Farley for Oracle WebLogic was described in the book Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation. Mirco Hering described a more generic approach to this process here:\n\nhttp://notafactoryanymore.com/2015/10/19/devops-for-systems-of-record-a-new-hope- preview-of-does-talk/ .\n\nA sample list of DevOps operational requirements can be found here: http://blog.devopsguys.com/2013/12/19/the-top-ten-devops-operational-requirements/ .",
      "page_number": 514
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 522-529)",
      "start_page": 522,
      "end_page": 529,
      "detection_method": "topic_boundary",
      "content": "Endnotes\n\nINTRODUCTION\n\nBefore the revolution… Eliyahu M. Goldratt, Beyond the Goal: Eliyahu Goldratt Speaks on the Theory of Constraints (Your Coach in a Box) (Prince Frederick, Maryland: Gildan Media, 2005), Audiobook.\n\nPut even more… Jeff Immelt, “GE CEO Jeff Immelt: Let’s Finally End the Debate over Whether We Are in a Tech Bubble,” Business Insider, December 9, 2015,\n\nhttp://www.businessinsider.com/ceo-of-ge-lets-finally-end-the-debate-over-whether-we- are-in-a-tech-bubble-2015-12 .\n\nOr as Jeffrey… “Weekly Top 10: Your DevOps Flavor,” Electric Cloud, April 1, 2016, http://electric-cloud.com/blog/2016/04/weekly-top-10-devops-flavor/ .\n\nDr. Eliyahu M. Goldratt… Goldratt, Beyond the Goal.\n\nAs Christopher Little… Christopher Little, personal correspondence with Gene Kim, 2010.\n\nAs Steven J. Spear… Steven J. Spear, The High-Velocity Edge: How Market Leaders Leverage Operational Excellence to Beat the Competition (New York, NY: McGraw Hill\n\nEducation), Kindle edition, chap. 3.\n\nIn 2013, the… Chris Skinner, “Banks have bigger development shops than Microsoft,” Chris Skinner’s Blog, accessed July 28, 2016, http://thefinanser.com/2011/09/banks-have-bigger- development-shops-than-microsoft.html/ .\n\nProjects are typically… Nico Stehr and Reiner Grundmann, Knowledge: Critical Concepts,\n\nVolume 3 (London: Routledge, 2005), 139.\n\nDr. Vernon Richardson… A. Masli, V. Richardson, M. Widenmier, and R. Zmud, “Senior Executive’s IT Management Responsibilities: Serious IT Deficiencies and CEO-CFO Turnover,” MIS Quaterly (published electronically June 21, 2016).\n\nConsider the following… “IDC Forecasts Worldwide IT Spending to Grow 6% in 2012, Despite Economic Uncertainty,” Business Wire, September 10, 2012, http://www.businesswire.com/news/home/20120910005280/en/IDC-Forecasts- Worldwide-Spending-Grow-6-2012 .\n\nThe first surprise… Nigel Kersten, IT Revolution, and PwC, 2015 State of DevOps Report (Portland, OR: Puppet Labs, 2015), https://puppet.com/resources/white-paper/2015-state- of-devops-report?_ga=1.6612658.168869.1464412647&link=blog .\n\nThis is highlighted… Frederick P. Brooks, Jr., The Mythical Man-Month: Essays on Software Engineering, Anniversary Edition (Upper Saddle River, NJ: Addison-Wesley, 1995).\n\nAs Randy Shoup… Gene Kim, Gary Gruver, Randy Shoup, and Andrew Phillips, “Exploring the Uncharted Territory of Microservices,” XebiaLabs.com, webinar, February 20, 2015,\n\nhttps://xebialabs.com/community/webinars/exploring-the-uncharted-territory-of- microservices/ .\n\nThe 2015 State… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nAnother more extreme… “Velocity 2011: Jon Jenkins, ‘Velocity Culture’,” YouTube video, 15:13, posted by O’Reilly, June 20, 2011, https://www.youtube.com/watch?v=dxk8b9rSKOo ;\n\n“Transforming Software Development,” YouTube video, 40:57, posted by Amazon Web Service, April 10, 2015, https://www.youtube.com/watch?v=YCrhemssYuI&feature=youtu.be .\n\nLater in his… Eliyahu M. Goldratt, Beyond the Goal.\n\nAs with The… JGFLL, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 4, 2013, http://www.amazon.com/review/R1KSSPTEGLWJ23 ; Mark L Townsend, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 2, 2013, http://uedata.amazon.com/gp/customer-\n\nreviews/R1097DFODM12VD/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI ; Scott Van Den Elzen, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 13, 2013, http://uedata.amazon.com/gp/customer- reviews/R2K95XEH5OL3Q5/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI .\n\nPART I INTRODUCTION\n\nOne key principle… Kent Beck, et al., “Twelve Principles of Agile Software,” AgileManifesto.org, 2001, http://agilemanifesto.org/principles.html .\n\nHe concluded that… Mike Rother, Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results (New York: McGraw Hill, 2010), Kindle edition, Part III.\n\nCHAPTER 1\n\nKaren Martin and… Karen Martin and Mike Osterling, Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation (New York:\n\nMcGraw Hill, 2013), Kindle edition, chap 1.\n\nIn this book… Ibid., chap. 3.\n\nKaren Martin and… Ibid.\n\nCHAPTER 2\n\nStudies have shown… Joshua S. Rubinstein, David E. Meyer, and Jeffrey E. Evans, “Executive Control of Cognitive Processes in Task Switching,” Journal of Experimental Psychology: Human Perception and Performance 27, no. 4 (2001): 763-797, doi: 10.1037//0096- 1523.27.4.763, http://www.umich.edu/~bcalab/documents/RubinsteinMeyerEvans2001.pdf .\n\nDominica DeGrandis, one… “DOES15—Dominica DeGrandis—The Shape of Uncertainty,” YouTube video, 22:54, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=Gp05i0d34gg .\n\nTaiichi Ohno compared… Sami Bahri, “Few Patients-In-Process and Less Safety Scheduling; Incoming Supplies are Secondary,” The Deming Institute Blog, August 22, 2013,\n\nhttps://blog.deming.org/2013/08/fewer-patients-in-process-and-less-safety-scheduling- incoming-supplies-are-secondary/ .\n\nIn other words… Meeting between David J. Andersen and team at Motorola with Daniel S. Vacanti, February 24, 2004; story retold at USC CSSE Research Review with Barry Boehm in March 2004.\n\nThe dramatic differences… James P. Womack and Daniel T. Jones, Lean Thinking: Banish Waste and Create Wealth in Your Corporation (New York: Free Press, 2010), Kindle edition, chap. 1.\n\nThere are many… Eric Ries, “Work in small batches,” StartupLessonsLearned.com, February\n\n20, 2009, http://www.startuplessonslearned.com/2009/02/work-in-small-batches.html .\n\nIn Beyond the… Goldratt, Beyond the Goal.\n\nAs a solution… Eliyahu M. Goldratt, The Goal: A Process of Ongoing Improvement (Great Barrington, MA: North River Press, 2014), Kindle edition, “Five Focusing Steps.”\n\nShigeo Shingo, one… Shigeo Shingo, A Study of the Toyota Production System: From an Industrial Engineering Viewpoint (London: Productivity Press, 1989); “The 7 Wastes (Seven forms of Muda),” BeyondLean.com, accessed July 28, 2016, http://www.beyondlean.com/7- wastes.html .\n\nIn the book… Mary Poppendieck and Tom Poppendieck, Implementing Lean Software: From Concept to Cash, (Upper Saddle River, NJ: Addison-Wesley, 2007), 74.\n\nThe following categories… Adapted from Damon Edwards, “DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,” Slideshare.net, posted by dev2ops, May 4, 2015, http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards .\n\nCHAPTER 3\n\nDr. Charles Perrow… Charles Perrow, Normal Accidents: Living with High Risk Technologies (Princeton, NJ: Princeton University Press, 1999).\n\nDr. Sidney Dekker… Dr. Sidney Dekker, The Field Guide to Understanding Human Error (Lund University, Sweden: Ashgate, 2006).\n\nAfter he decoded… Spear, The High-Velocity Edge, chap. 8.\n\nDr. Spear extended… Ibid.\n\nDr. Peter Senge… Peter M. Senge, The Fifth Discipline: The Art & Practice of the Learning Organization (New York: Doubleday, 2006), Kindle edition, chap. 5.\n\nIn one well-documented… “NUMMI,” This American Life, March 26, 2010, http://www.thisamericanlife.org/radio-archives/episode/403/transcript .\n\nAs Elisabeth Hendrickson… “DOES15 - Elisabeth Hendrickson - Its All About Feedback,” YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=r2BFTXBundQ .\n\n“In doing so… Spear, The High-Velocity Edge, chap. 1.\n\nAs Dr. Spear… Ibid., chap. 4.\n\nExamples of ineffective… Jez Humble, Joanne Molesky, and Barry O’Reilly, Lean Enterprise: How High Performance Organizations Innovate at Scale (Sebastopol, CA: O’Reilly Media, 2015), Kindle edition, Part IV.\n\nIn the 1700s… Dr. Thomas Sowell, Knowledge and Decisions (New York: Basic Books, 1980), 222.\n\nAs Gary Gruver… Gary Gruver, personal correspondence with Gene Kim, 2014.\n\nCHAPTER 4\n\nFor instance, in… Paul Adler, “Time-and-Motion Regained,” Harvard Business Review, January-February 1993, https://hbr.org/1993/01/time-and-motion-regained .\n\nThe “name, blame… Dekker, The Field Guide to Understanding Human Error, chap. 1.\n\nDr. Sidney Dekker… “Just Culture: Balancing Safety and Accountability,” Lund University, Human Factors & System Safety website, November 6, 2015,\n\nhttp://www.humanfactors.lth.se/sidney-dekker/books/just-culture/ .\n\nHe observed that… Ron Westrum, “The study of information flow: A personal journey,” Proceedings of Safety Science 67 (August 2014): 58-63, https://www.researchgate.net/publication/261186680_ The_study_of_information_flow_A_personal_journey .\n\nJust as Dr. Westrum… Nicole Forsgren Velasquez, Gene Kim, Nigel Kersten, and Jez Humble, 2014 State of DevOps Report (Portland, OR: Puppet Labs, IT Revolution Press, and ThoughtWorks, 2014), http://puppetlabs.com/2014-devops-report .\n\nAs Bethany Macri… Bethany Macri, “Morgue: Helping Better Understand Events by Building\n\na Post Mortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nDr. Spear observes… Spear, The High-Velocity Edge, chap. 1.\n\nIn The Fifth… Senge, The Fifth Discipline, chap. 1.\n\nMike Rother observed… Mike Rother, Toyota Kata, 12.\n\nThis is why… Mike Orzen, personal correspondence with Gene Kim, 2012.\n\nConsider the following… “Paul O’Neill,” Forbes, October 11, 2001, http://www.forbes.com/2001/10/16/poneill.html .\n\nIn 1987, Alcoa… Spear, The High-Velocity Edge, chap. 4.\n\nAs Dr. Spear… Ibid.\n\nA remarkable example… Ibid., chap. 5.\n\nThis process of… Nassim Nicholas Taleb, Antifragile: Things That Gain from Disorder (Incerto), (New York: Random House, 2012).\n\nAccording to Womack… Jim Womack, Gemba Walks (Cambridge, MA: Lean Enterprise Institute, 2011), Kindle edition, location 4113.\n\nMike Rother formalized… Rother, Toyota Kata, Part IV.\n\nMike Rother observes… Ibid., Conclusion.\n\nCHAPTER 5\n\nTherefore, we must… Michael Rembetsy and Patrick McDonnell, “Continuously Deploying Culture [at Etsy],” Slideshare.net, October 4, 2012, posted by Patrick McDonnel.bl, http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at- etsy-14588485 .\n\nIn 2015, Nordstrom… “Nordstrom, Inc.,” company profile on Vault.com, http://www.vault.com/company-profiles/retail/nordstrom,-inc/company-overview.aspx .\n\nThe stage for… Courtney Kissler, “DOES14 - Courtney Kissler - Nordstrom - Transforming to\n\na Culture of Continuous Improvement,” YouTube video, 29:59, posted by DevOps Enterprise Summit 2014, October 29, 2014, https://www.youtube.com/watch?v=0ZAcsrZBSlo .\n\nThese organizations were… Tom Gardner, “Barnes & Noble, Blockbuster, Borders: The Killer B’s Are Dying,” The Motley Fool, July 21, 2010, http://www.fool.com/investing/general/2010/07/21/barnes-noble-blockbuster-borders-the- killer-bs-are.aspx .\n\nAs Kissler described… Kissler, “DOES14 - Courtney Kissler - Nordstrom.”\n\nAs Kissler said… Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n\nAs Kissler stated… Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n\nIn 2015, Kissler… Ibid.\n\nShe continued, “This… Ibid.\n\nKissler concluded, “From… Ibid.\n\nAn example of… Ernest Mueller, “Business model driven cloud adoption: what NI Is doing in the cloud,” Slideshare.net, June 28, 2011, posted by Ernest Mueller, http://www.slideshare.net/mxyzplk/business-model-driven-cloud-adoption-what-ni-is-\n\ndoing-in-the-cloud .\n\nAlthough many believe… Unpublished calculation by Gene Kim after the 2014 DevOps Enterprise Summit.\n\nIndeed, one of… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nCSG (2013): In… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments,” Slideshare.net, November 14, 2014, posted by DevOps Enterprise Summit, http://www.slideshare.net/DevOpsEnterpriseSummit/scott-prugh .\n\nEtsy (2009): In… Rembetsy and McDonnell, “Continuously Deploying Culture [at Etsy].”\n\nThe Gartner research… Bernard Golden, “What Gartner’s Bimodal IT Model Means to Enterprise CIOs,” CIO Magazine, January 27, 2015,\n\nhttp://www.cio.com/article/2875803/cio-role/what-gartner-s-bimodal-it-model-means-to- enterprise-cios.html .\n\nSystems of record… Ibid.\n\nSystems of engagement… Ibid.\n\nThe data from… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nScott Prugh, VP… Scott Prugh, personal correspondence with Gene Kim, 2014.\n\nGeoffrey A. Moore… Geoffrey A. Moore and Regis McKenna, Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers (New York: HarperCollins,\n\n2009), 11.\n\nBig bang, top-down… Linda Tucci, “Four Pillars of PayPal’s ‘Big Bang’ Agile Transformation,” TechTarget, August 2014, http://searchcio.techtarget.com/feature/Four- pillars-of-PayPals-big-bang-Agile-transformation .\n\nThe following list… “Creating High Velocity Organizations,” description of course by Roberto Fernandez and Steve Spear, MIT Sloan Executive Education website, accessed May 30, 2016, http://executive.mit.edu/openenrollment/program/organizational-development-high- velocity-organizations .\n\nBut as Ron van Kemenade… Ron Van Kemande, “Nothing Beats Engineering Talent: The\n\nAgile Transformation at ING,” presentation at the DevOps Enterprise Summit, London, UK, June 30-July 1, 2016.\n\nPeter Drucker, a… Leigh Buchanan, “The Wisdom of Peter Drucker from A to Z,” Inc., November 19, 2009, http://www.inc.com/articles/2009/11/drucker.html .\n\nCHAPTER 6\n\nOver the years… Kissler, “DOES14 - Courtney Kissler - Nordstrom.”\n\nKissler explained:… Ross Clanton and Michael Ducy, interview of Courtney Kissler and Jason Josephy, “Continuous Improvement at Nordstrom,” The Goat Farm, podcast audio, June 25, 2015, http://goatcan.do/2015/06/25/the-goat-farm-episode-7-continuous-improvement-at- nordstrom/ .\n\nShe said proudly… Ibid.\n\nTechnology executives or… Brian Maskell, “What Does This Guy Do? Role of Value Stream Manager,” Maskell, July 3, 2015, http://blog.maskell.com/? p=2106http://www.lean.org/common/display/?o=221 .\n\nDamon Edwards observed… Damon Edwards, “DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,” Slideshare.net, posted by dev2ops, May 4, 2015, http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards .\n\nIn their book… Vijay Govindarajan and Chris Trimble, The Other Side of Innovation: Solving the Execution Challenge (Boston, MA: Harvard Business Review, 2010) Kindle edition.\n\nBased on their… Ibid., Part I.\n\nAfter the near-death… Marty Cagan, Inspired: How to Create Products Customers Love (Saratoga, CA: SVPG Press, 2008), 12.\n\nCagan notes that… Ibid.\n\nSix months after… Ashlee Vance, “LinkedIn: A Story About Silicon Valley’s Possibly Unhealthy Need for Speed,” Bloomberg, April 30, 2013, http://www.bloomberg.com/bw/articles/2013-04-29/linkedin-a-story-about-silicon-valleys- possibly-unhealthy-need-for-speed .\n\nLinkedIn was created… “LinkedIn started back in 2003 — LinkedIn - A Brief History,”\n\nSlideshare.net, posted by Josh Clemm, November 9, 2015, http://www.slideshare.net/joshclemm/how-linkedin-scaled-a-brief-history/3- LinkedIn_started_back_in_2003 .\n\nOne year later… Jonas Klit Nielsen, “8 Years with LinkedIn – Looking at the Growth [Infographic],” MindJumpers.com, May 10, 2011, http://www.mindjumpers.com/blog/2011/05/linkedin-growth-infographic/ .\n\nBy November 2015… “LinkedIn started back in 2003,” Slideshare.net.\n\nThe problem was… “From a Monolith to Microservices + REST: The Evolution of LinkedIn’s Architecture,” Slideshare.net, posted by Karan Parikh, November 6,\n\n2014,http://www.slideshare.net/parikhk/restli-and-deco .\n\nJosh Clemm, a… “LinkedIn started back in 2003,” Slideshare.net.\n\nIn 2013, journalist… Vance, “LinkedIn: A Story About,” Bloomberg.\n\nScott launched Operation… “How I Structured Engineering Teams at LinkedIn and AdMob for Success,” First Round Review, 2015,http://firstround.com/review/how-i-structured- engineering-teams-at-linkedin-and-admob-for-success/ .\n\nScott described one… Ashlee Vance, “Inside Operation InVersion, the Code Freeze that Saved LinkedIn,” Bloomberg, April 11, 2013, http://www.bloomberg.com/news/articles/2013-04- 10/inside-operation-inversion-the-code-freeze-that-saved-linkedin .\n\nHowever, Vance described… Vance, “LinkedIn: A Story About,” Bloomberg.",
      "page_number": 522
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 530-538)",
      "start_page": 530,
      "end_page": 538,
      "detection_method": "topic_boundary",
      "content": "As Josh Clemm… “LinkedIn started back in 2003,” Slideshare.net.\n\nKevin Scott stated… “How I Structured Engineering Teams,” First Round Review.\n\nAs Christopher Little… Christopher Little, personal correspondence with Gene Kim, 2011.\n\nAs Ryan Martens… Ryan Martens, personal correspondence with Gene Kim, 2013.\n\nCHAPTER 7\n\nHe observed, “After… Dr. Melvin E. Conway, “How Do Committees Invent?” MelConway.com, http://www.melconway.com/research/committees.html , previously published in Datamation, April 1968.\n\nThese observations led… Ibid.\n\nEric S. Raymond, author… Eric S. Raymond, “Conway’s Law,” catb.org, accessed May 31, 2016, http://catb.org/~esr/jargon/ .\n\nEtsy’s DevOps journey… Sarah Buhr, “Etsy Closes Up 86 Percent on First Day of Trading,”\n\nTech Crunch, April 16, 2015, http://techcrunch.com/2015/04/16/etsy-stock-surges-86- percent-at-close-of-first-day-of-trading-to-30-per-share/ .\n\nAs Ross Snyder… “Scaling Etsy: What Went Wrong, What Went Right,” Slideshare.net, posted by Ross Snyder, October 5, 2011, http://www.slideshare.net/beamrider9/scaling-etsy- what-went-wrong-what-went-right .\n\nAs Snyder observed… Ibid.\n\nIn other words… Sean Gallagher, “When ‘Clever’ Goes Wrong: How Etsy Overcame Poor Architectural Choices,” Arstechnica, October 3, 2011, http://arstechnica.com/business/2011/10/when-clever-goes-wrong-how-etsy-overcame- poor-architectural-choices/ .\n\nSnyder explained that… “Scaling Etsy” Slideshare.net.\n\nEtsy initially had… Ibid.\n\nIn the spring… Ibid.\n\nAs Snyder described… Ross Snyder, “Surge 2011—Scaling Etsy: What Went Wrong, What Went Right,” YouTube video, posted by Surge Conference, December 23, 2011, https://www.youtube.com/watch?v=eenrfm50mXw .\n\nAs Snyder said… Ibid.\n\nSprouter was one… “Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012,\n\nhttp://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at- etsy-14588485 .\n\nThey are defined… “Creating High Velocity Organizations,” description of course by Roberto Fernandez and Steven Spear.\n\nAdrian Cockcroft remarked… Adrian Cockcroft, personal correspondence with Gene Kim, 2014.\n\nIn the Lean… Spear, The High-Velocity Edge, chap. 8.\n\nAs Mike Rother… Rother, Toyota Kata, 250.\n\nReflecting on shared… “DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,” YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=USYrDaPEFtM .\n\nHe continued, “The… Ibid.\n\nPedro Canahuati, their… Pedro Canahuati, “Growing from the Few to the Many: Scaling the Operations Organization at Facebook,” InfoQ, December 16, 2013, http://www.infoq.com/presentations/scaling-operations-facebook .\n\nWhen departments over-specialize… Spear, The High-Velocity Edge, chap. 1.\n\nScott Prugh writes… Scott Prugh, “Continuous Delivery,” Scaled Agile Framework, updated February 14, 2013, http://www.scaledagileframework.com/continuous-delivery/ .\n\n“By cross-training… Ibid.\n\n“Traditional managers will… Ibid.\n\nFurthermore, as Prugh… Ibid.\n\nWhen we value… Dr. Carol Dweck, “Carol Dweck Revisits the ‘Growth Mindset,’” Education Week, September 22, 2015, http://www.edweek.org/ew/articles/2015/09/23/carol-dweck- revisits-the-growth-mindset.html .\n\nAs Jason Cox… Jason Cox, “Disney DevOps: To Infinity and Beyond,” presentation at DevOps Enterprise Summit 2014, San Francisco, CA, October 2014.\n\nAs John Lauderbach… John Lauderbach, personal conversation with Gene Kim, 2001.\n\nThese properties are… Tony Mauro, “Adopting Microservices at Netflix: Lessons for Architectural Design,” NGINX, February 19, 2015, https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/ .; Adam Wiggins, “The Twelve-Factor App,” 12Factor.net, January 30, 2012, http://12factor.net/ .\n\nRandy Shoup, former… “Exploring the Uncharted Territory of Microservices,” YouTube video, 56:50, posted by XebiaLabs, Inc., February 20, 2015, https://www.youtube.com/watch?v=MRa21icSIQk .\n\nAs part of… Humble, O’Reilly, and Molesky, Lean Enterprise, Part III.\n\nIn the Netflix… Reed Hastings, “Netflix Culture: Freedom and Responsibility,” Slideshare.net, August 1, 2009, http://www.slideshare.net/reed2001/culture-1798664 .\n\nAmazon CTO Werner… Larry Dignan, “Little Things Add Up,” Baseline, October 19, 2005, http://www.baselinemag.com/c/a/Projects-Management/Profiles-Lessons-From-the- Leaders-in-the-iBaselinei500/3 .\n\nTarget is the… Heather Mickman and Ross Clanton, “DOES15 - Heather Mickman & Ross Clanton - (Re)building an Engineering Culture: DevOps at Target,” YouTube video, 33:39, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch? v=7s-VbB1fG5o .\n\nAs Mickman described… Ibid.\n\nIn an attempt… Ibid.\n\nBecause our team… Ibid.\n\nIn the following… Ibid.\n\nThese changes have… Ibid.\n\nThe API Enablement… Ibid.\n\nCHAPTER 8\n\nAt Big Fish… “Big Fish Celebrates 11th Consecutive Year of Record Growth,” BigFishGames.com, January 28, 2014, http://pressroom.bigfishgames.com/2014-01-28-Big- Fish-Celebrates-11th-Consecutive-Year-of-Record-Growth .\n\nHe observed that… Paul Farrall, personal correspondence with Gene Kim, January 2015.\n\nFarrall defined two… Ibid., 2014.\n\nHe concludes, “The… Ibid.\n\nErnest Mueller observed… Ernest Mueller, personal correspondence with Gene Kim, 2014.\n\nAs Damon Edwards… Edwards, “DevOps Kaizen.”\n\nDianne Marsh, Director… “Dianne Marsh ‘Introducing Change while Preserving Engineering Velocity,” YouTube video, 17:37, posted by Flowcon, November 11, 2014,\n\nhttps://www.youtube.com/watch?v=eW3ZxY67fnc .\n\nJason Cox said… Jason Cox, “Disney DevOps.”\n\nAt Etsy, this… “devopsdays Minneapolis 2015 - Katherine Daniels - DevOps: The Missing Pieces,” YouTube video, 33:26, posted by DevOps Minneapolis, July 13, 2015, https://www.youtube.com/watch?v=LNJkVw93yTU .\n\nAs Ernest Mueller… Ernest Mueller, personal correspondence with Gene Kim, 2015.\n\nScrum is an agile… Hirotaka Takeuchi and Ikujiro Nonaka, “New Product Development Game,” Harvard Business Review (January 1986): 137-146.\n\nCHAPTER 9\n\nIn her presentation… Em Campbell-Pretty, “DOES14 - Em Campbell-Pretty - How a Business Exec Led Agile, Lead, CI/CD,” YouTube video, 29:47, posted by DevOps Enterprise Summit,\n\nApril 20, 2014, https://www.youtube.com/watch?v=-4pIMMTbtwE .\n\nCampbell-Pretty became… Ibid.\n\nThey created a… Ibid.\n\nCampbell-Pretty observed… Ibid.\n\nCampbell-Pretty described… Ibid.\n\nThe first version… “Version Control History,” PlasticSCM.com, accessed May 31, 2016, https://www.plasticscm.com/version-control-history.html .\n\nA version control… Jennifer Davis and Katherine Daniels, Effective DevOps: Building a Culture of Collaboration, Affinity, and Tooling at Scale (Sebastopol, CA: O’Reilly Media, 2016), 37.\n\nBill Baker, a… Simon Sharwood, “Are Your Servers PETS or CATTLE?,” The Register, March 18 2013, http://www.theregister.co.uk/2013/03/18/servers_pets_or_cattle_cern/ .\n\nAt Netflix, the… Jason Chan, “OWASP AppSecUSA 2012: Real World Cloud Application Security,” YouTube video, 37:45, posted by Christiaan008, December 10, 2012, https://www.youtube.com/watch?v=daNA0jXDvYk .\n\nThe latter pattern… Chad Fowler, “Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components,” ChadFowler.com, June 23, 2013,\n\nhttp://chadfowler.com/2013/06/23/immutable-deployments.html .\n\nThe entire application… John Willis, “Docker and the Three Ways of DevOps Part 1: The First Way—Systems Thinking,” Docker, May 26, 2015, https://blog.docker.com/2015/05/docker-\n\nthree-ways-devops/ .\n\nCHAPTER 10\n\nGary Gruver, former… Gary Gruver, personal correspondence with Gene Kim, 2014.\n\nThey had problems… “DOES15 - Mike Bland - Pain Is Over, If You Want It,” Slideshare.net, posted by Gene Kim, November 18, 2015, http://www.slideshare.net/ITRevolution/does15- mike-bland-pain-is-over-if-you-want-it-55236521 .\n\nBland describes how… Ibid.\n\nBland described that… Ibid.\n\nAs Bland describes… Ibid.\n\nAs Bland notes… Ibid.\n\nOver the next… Ibid.\n\nEran Messeri, an… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?,” presentation at the GOTO Conference, Aarhus, Denmark, October 2, 2013.\n\nMesseri explains, “There… Ibid.\n\nAll their code… Ibid.\n\nSome of the… Ibid.\n\nIn Development, continuous… Jez Humble and David Farley, personal correspondence with Gene Kim, 2012.\n\nThe deployment pipeline… Jez Humble and David Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation (Upper Saddle River, NJ: Addison-Wesly, 2011), 3.\n\nHumble and Farley… Ibid., 188.\n\nAs Humble and… Ibid., 258.\n\nMartin Fowler observes… Martin Fowler, “Continuous Integration,” MartinFowler.com, May 1, 2006, http://www.martinfowler.com/articles/continuousIntegration.html .\n\nMartin Fowler described… Martin Fowler, “TestPyramid,” MartinFowler.com, May 1, 2012, http://martinfowler.com/bliki/TestPyramid.html .\n\nThis technique was .. Martin Fowler, “Test Driven Development,” MartinFowler.com, March 5, 2005, http://martinfowler.com/bliki/TestDrivenDevelopment.html .\n\nNachi Nagappan, E. Michael… Nachiappan Nagappan, E. Michael Maximilien, Thirumalesh Bhat, and Laurie Williams, “Realizing quality improvement through test driven development: results and experiences of four industrial teams,” Empir Software Engineering, 13, (2008): 289-302, http://research.microsoft.com/en-us/groups/ese/nagappan_tdd.pdf .\n\nIn her 2013… Elisabeth Hendrickson, “On the Care and Feeding of Feedback Cycles,” Slideshare.net, posted by Elisabeth Hendrickson, November 1, 2013, http://www.slideshare.net/ehendrickson/care-and-feeding-of-feedback-cycles .\n\nHowever, merely automating… “Decreasing false positives in automated testing,” Slideshare.net, posted by Sauce Labs, March 24, 2015, http://www.slideshare.net/saucelabs/decreasing-false-positives-in-automated-testing .; Martin Fowler, “Eradicating Non-determinism in Tests,” MartinFowler.com, April 14, 2011, http://martinfowler.com/articles/nonDeterminism.html .\n\nAs Gary Gruver… Gary Gruver, “DOES14 - Gary Gruver - Macy’s - Transforming Traditional Enterprise Software Development Processes,” YouTube video, 27:24, posted by DevOps Enterprise Summit 2014, October 29, 2014, https://www.youtube.com/watch?v=- HSSGiYXA7U .\n\nRandy Shoup, former… Randy Shoup, “The Virtuous Cycle of Velocity: What I Learned About Going Fast at eBay and Google by Randy Shoup,” YouTube video, 30:05, posted by Flowcon, December 26, 2013, https://www.youtube.com/watch?v=EwLBoRyXTOI .\n\nThis is sometimes… David West, “Water scrum-fall is-reality_of_agile_for_most,” Slideshare.net, posted by harsoft, April 22, 2013, http://www.slideshare.net/harsoft/water- scrumfall-isrealityofagileformost .\n\nCHAPTER 11\n\nThe surprising breadth… Gene Kim, “The Amazing DevOps Transformation of the HP LaserJet Firmware Team (Gary Gruver),” ITRevolution.com, 2013, http://itrevolution.com/the-amazing-devops-transformation-of-the-hp-laserjet-firmware- team-gary-gruver/ .\n\nGruver described this… Ibid.\n\nCompile flags (#define… Ibid.\n\nGruver admits trunk-based… Gary Gruver and Tommy Mouser, Leading the Transformation: Applying Agile and DevOps Principles at Scale (Portland, OR: IT Revolution Press), 60.\n\nGruver observed, “Without… Kim, “The Amazing DevOps Transformation ”\n\nITRevolution.com.\n\nJeff Atwood, founder… Jeff Atwood, “Software Branching and Parallel Universes,” CodingHorror.com, October 2, 2007, http://blog.codinghorror.com/software-branching- and-parallel-universes/ .\n\nThis is how… Ward Cunningham, “Ward Explains Debt Metaphor,” c2.com, 2011, http://c2.com/cgi/wiki?WardExplainsDebtMetaphor .\n\nErnest Mueller, who… Ernest Mueller, “2012: A Release Odyssey,” Slideshare.net, posted by Ernest Mueller, March 12, 2014, http://www.slideshare.net/mxyzplk/2012-a-release-odyssey .\n\nAt that time… “Bazaarvoice, Inc. Announces Its Financial Results for the Fourth Fiscal Quarter and Fiscal Year Ended April 30, 2012,” BasaarVoice.com, June 6, 2012, http://investors.bazaarvoice.com/releasedetail.cfm?ReleaseID=680964 .\n\nMueller observed, “It… Ernest Mueller, “DOES15 - Ernest Mueller - DevOps Transformations At National Instruments and…,” YouTube video, 34:14, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=6Ry40h1UAyE .\n\n“By running these… Ibid.\n\nMueller further described… Ibid.\n\nHowever, the data… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nCHAPTER 12\n\nIn 2012, Rossi… Chuck Rossi, “Release engineering and push karma: Chuck Rossi,” post on Chuck Rossi’s Facebook page, April 5, 2012, https://www.facebook.com/notes/facebook- engineering/release-engineering-and-push-karma-chuck-rossi/10150660826788920 .\n\nJust prior to… Ryan Paul, “Exclusive: a behind-the-scenes look at Facebook release engineering,” Ars Technica, April 5, 2012, http://arstechnica.com/business/2012/04/exclusive-a-behind-the-scenes-look-at-facebook- release-engineering/1/ .\n\nRossi continued, “If… Chuck Rossi, “Release engineering and push karma.”\n\nThe Facebook front-end… Paul, “Exclusive: a behind-the-scenes look at Facebook release engineering,” Ars Technica.\n\nHe explained that… Chuck Rossi, “Ship early and ship twice as often,” post on Chuck Rossi’s Facebook page, August 3, 2012, https://www.facebook.com/notes/facebook-\n\nengineering/ship-early-and-ship-twice-as-often/10150985860363920 .\n\nKent Beck, the .. Kent Beck, “Slow Deployment Causes Meetings,” post on Kent Beck’s Facebook page, November 19, 2015), https://www.facebook.com/notes/kent-beck/slow- deployment-causes-meetings/1055427371156793?_rdr=p .\n\nScott Prugh, their… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.”\n\nPrugh observed, “It… Ibid.\n\nPrugh writes, “We… Ibid.\n\nPrugh also observes:… Ibid.\n\nIn their experiments… Puppet Labs and IT Revolution Press, 2013 State of DevOps Report (Portland, OR: Puppet Labs, 2013), http://www.exin-library.com/Player/eKnowledge/2013- state-of-devops-report.pdf .\n\nPrugh reported that… Scott Prugh and Erica Morrison, “DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),” YouTube video, 29:39, posted by\n\nDevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch? v=tKdIHCL0DUg .\n\nConsider the following… Tim Tischler, personal conversation with Gene Kim, FlowCon 2013.\n\nIn practice, the… Puppet Labs and IT Revolution Press, 2013 State of DevOps Report.\n\nIn Puppet Labs’… Velasquez, Kim, Kersten, and Humble, 2014 State of DevOps Report.\n\nThe deployment process… Chad Dickerson, “Optimizing for developer happiness,” CodeAsCraft.com, June 6, 2011, https://codeascraft.com/2011/06/06/optimizing-for- developer-happiness/ .\n\nAs Noah Sussman… Noah Sussman and Laura Beth Denker, “Divide and Conquer,” CodeAsCraft.com, April 20, 2011, https://codeascraft.com/2011/04/20/divide-and-concur/ .\n\nSussman writes, “Through… Ibid.\n\nIf all the tests… Ibid.\n\nOnce it is an… Erik Kastner, “Quantum of Deployment,” CodeAsCraft.com, May 20, 2010, https://codeascraft.com/2010/05/20/quantum-of-deployment/ .\n\nThis technique was… Timothy Fitz, “Continuous Deployment at IMVU: Doing the impossible fifty times a day,” TimothyFitz.com, February 10, 2009, http://timothyfitz.com/2009/02/10/continuous-deployment-at-imvu-doing-the-impossible-\n\nfifty-times-a-day/ .\n\nThis pattern is… Fitz, “Continuous Deployment,” TimothyFitz.com.; Michael Hrenko, “DOES15 - Michael Hrenko - DevOps Insured By Blue Shield of California,” YouTube video, 42:24, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=NlgrOT24UDw .\n\nDan North and Dave… Humble and Farley, Continuous Delivery, 265.\n\nThe cluster immune… Eric Ries, The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses (New York: Random House, 2011), Audiobook.\n\nOne sophisticated example… Andrew ‘Boz’ Bosworth, “Building and testing at Facebook,”\n\npost on Boz Facebook page, August 8, 2012, https://www.facebook.com/notes/facebook- engineering/building-and-testing-at-facebook/10151004157328920 ; “Etsy’s Feature flagging API used for operational rampups and A/B testing,” GitHub.com, https://github.com/etsy/feature ; “Library for configuration management API,” GitHub.com, https://github.com/Netflix/archaius .\n\nIn 2009, when… John Allspaw, “Convincing management that cooperation and collaboration was worth it,” KitchenSoap.com, January 5, 2012, http://www.kitchensoap.com/2012/01/05/convincing-management-that-cooperation-and- collaboration-was-worth-it/ .\n\nSimilarly, as Chuck… Rossi, “Release engineering and push karma.”\n\nFor nearly a decade… Emil Protalinski, “Facebook passes 1.55B monthly active users and 1.01B daily active users,” Venture Beat, November 4, 2015, http://venturebeat.com/2015/11/04/facebook-passes-1-55b-monthly-active-users-and-1-01- billion-daily-active-users/ .\n\nBy 2015, Facebook… Ibid.\n\nEugene Letuchy, an… Eugene Letuchy, “Facebook Chat,” post on Eugene Letuchy’s Facebook page, May 3, 2008, http://www.facebook.com/note.php? note_id=14218138919&id=944554719 .\n\nImplementing this computationally-intensive… Ibid.\n\nAs Letuchy wrote… Ibid.\n\nHowever, in 2015… Jez Humble, personal correspondence with Gene Kim, 2014.\n\nHis updated definitions… Ibid.\n\nAt Amazon and… Ibid.",
      "page_number": 530
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 539-546)",
      "start_page": 539,
      "end_page": 546,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 13\n\nThis is the… Jez Humble, “What is Continuous Delivery,” ContinuousDelivery.com, accessed May 28, 2016, https://continuousdelivery.com/ .\n\nHe observes that… Kim, Gruver, Shoup, and Phillips, “Exploring the Uncharted Territory of Microservices.”\n\nHe reflects, “Looking… Ibid.\n\neBay’s architecture went… Shoup, “From Monolith to Micro-services.”\n\nCharles Betz, author… Charles Betz, Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler’s Children (Witham, MA: Morgan Kaufmann, 2011), 300.\n\nAs Randy Shoup… Randy Shoup, “From the Monolith to Micro-services,” Slideshare.net, posted by Randy Shoup, October 8, 2014, http://www.slideshare.net/RandyShoup/goto- aarhus2014-enterprisearchitecturemicroservices .\n\nShoup notes, “Organizations… Ibid.\n\nAs Randy Shoup observes… Ibid.\n\nOne of the most… Werner Vogels, “A Conversation with Werner Vogels,” acmqueque 4, no. 4\n\n(2006): 14-22, http://queue.acm.org/detail.cfm?id=1142065 .\n\nVogels tells Gray… Ibid.\n\nDescribing the thought… Ibid.\n\nVogels notes, “The… Ibid.\n\nIn 2011, Amazon… John Jenkins, “Velocity 2011: Jon Jenkins, “Velocity Culture,”” YouTube video, 15:13, posted by O’Reilly, June 20, 2011, {https://www.youtube.com/watch? v=dxk8b9rSKOo .\n\nBy 2015, they… Ken Exner, “Transforming Software Development,” YouTube video, 40:57, posted by Amazon Web Services, April 10, 2015, https://www.youtube.com/watch? v=YCrhemssYuI&feature=youtu.be .\n\nThe term strangler… Martin Fowler, “StranglerApplication,” MartinFowler.com, June 29, 2004, http://www.martinfowler.com/bliki/StranglerApplication.html .\n\nWhen we implement… Boris Lublinsky, “Versioning in SOA,” The Architecture Journal, April 2007, https://msdn.microsoft.com/en-us/library/bb491124.aspx .\n\nThe strangler application… Paul Hammant, “Introducing Branch by Abstraction,” PaulHammant.com, April 26, 2007, http://paulhammant.com/blog/branch_by_abstraction.html .\n\nAn observation from… Martin Fowler, “StranglerApplication,” MartinFowler.com, June 29, 2004, http://www.martinfowler.com/bliki/StranglerApplication.html .\n\nBlackboard Inc., is… Gregory T. Huang, “Blackboard CEO Jay Bhatt on the Global Future of Edtech,” Xconomy, June 2, 2014, http://www.xconomy.com/boston/2014/06/02/blackboard-ceo-jay-bhatt-on-the-global- future-of-edtech/ .\n\nAs David Ashman… David Ashman, “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4 .\n\nIn 2010, Ashman… Ibid.\n\nHow this started… David Ashman, personal correspondence with Gene Kim, 2014.\n\nAshman noted, “To… Ibid.\n\n“In fact,” Ashman… Ibid.\n\nAshman concluded, “Having… Ibid.\n\nCHAPTER 14\n\nIn Operations, we… Kim, Behr, and Spafford, The Visible Ops Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Eugene, OR: IT Process Institute, 2004), Kindle edition, Introduction.\n\nIn contrast, the… Ibid.\n\nIn other words… Ibid.\n\nTo enable this… “Telemetry,” Wikipedia, last modified May 5, 2016, https://en.wikipedia.org/wiki/Telemetry .\n\nMcDonnell described how… Michael Rembetsy and Patrick McDonnell, “Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012, http://www.slideshare.net/mcdonnps/continuously- deploying-culture-scaling-culture-at-etsy-14588485 .\n\nMcDonnell explained further… Ibid.\n\nBy 2011, Etsy… John Allspaw, personal conversation with Gene Kim, 2014.\n\nAs Ian Malpass… Ian Malpass, “Measure Anything, Measure Everything,” CodeAsCraft.com, February 15, 2011, http://codeascraft.com/2011/02/15/measure-anything-measure- everything/ .\n\nOne of the findings… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nThe top two… “2014 State Of DevOps Findings! Velocity Conference,” Slideshare.net, posted by Gene Kim, June 30, 2014, http://www.slideshare.net/realgenekim/2014-state-of-devops- findings-velocity-conference .\n\nIn The Art… James Turnbull, The Art of Monitoring (Seattle, WA: Amazon Digital Services, 2016), Kindle edition, Introduction.\n\nThe resulting capability… “Monitorama - Please, no more Minutes, Milliseconds, Monoliths or Monitoring Tools,” Slideshare.net, posted by Adrian Cockcroft, May 5, 2014,\n\nhttp://www.slideshare.net/adriancockcroft/monitorama-please-no-more .\n\nScott Prugh, Chief… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.”\n\nTo support these… Brice Figureau, “The 10 Commandments of Logging,” Mastersen’s Blog, January 13, 2013, http://www.masterzen.fr/2013/01/13/the-10-commandments-of-logging/ .\n\nChoosing the right… Dan North, personal correspondence with Gene Kim, 2016.\n\nTo help ensure… Anton Chuvakin, “LogLogic/Chuvakin Log Checklist,” republished with permission, 2008, http://juliusdavies.ca/logging/llclc.html .\n\nIn 2004, Kim… Kim, Behr, and Spafford, The Visible Ops Handbook, Introduction.\n\nThis was the… Dan North, “Ops and Operability,” SpeakerDeck.com, February 25, 2016, https://speakerdeck.com/tastapod/ops-and-operability .\n\nAs John Allspaw… John Allspaw, personal correspondence with Gene Kim, 2011.\n\nThis is often… “Information Radiators,” AgileAlliance.com, accessed May 31, 2016, https://www.agilealliance.org/glossary/incremental-radiators/ .\n\nAlthough there may… Ernest Mueller, personal correspondence with Gene Kim, 2014.\n\nPrachi Gupta, Director… Prachi Gupta, “Visualizing LinkedIn’s Site Performance,” LinkedIn\n\nEngineering blog, June 13, 2011, https://engineering.linkedin.com/25/visualizing-linkedins- site-performance .\n\nThus began Eric… Eric Wong, “Eric the Intern: the Origin of InGraphs,” LinkedIn, June 30, 2011, http://engineering.linkedin.com/32/eric-intern-origin-ingraphs .\n\nWong wrote, “To… Ibid.\n\nAt the time… Ibid.\n\nIn writing about… Gupta, “Visualizing LinkedIn’s Site Performance.”\n\nEd Blankenship, Senior… Ed Blankenship, personal correspondence with Gene Kim, 2016.\n\nHowever, increasingly these… Mike Burrows, “The Chubby lock service for loosely-coupled\n\ndistributed systems,” OSDI’06: Seventh Symposium on Operating System Design and Implementation, November 2006, http://static.googleusercontent.com/media/research.google.com/en//archive/chubby- osdi06.pdf .\n\nConsul may be… Jeff Lindsay, “Consul Service Discovery with Docker,” Progrium.com, August 20, 2014, http://progrium.com/blog/2014/08/20/consul-service-discovery-with- docker .\n\nAs Jody Mulkey… Jody Mulkey, “DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,” YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=USYrDaPEFtM .\n\nCHAPTER 15\n\nIn 2015, Netflix… Netflix Letter to Shareholders, January 19, 2016,\n\nhttp://files.shareholder.com/downloads/NFLX/2432188684x0x870685/C6213FF9-5498- 4084-A0FF-74363CEE35A1/Q4_15_Letter_to_Shareholders_-_COMBINED.pdf .\n\nRoy Rapoport describes… Roy Rapoport, personal correspondence with Gene Kim, 2014.\n\nOne of the statistical… Victoria Hodge and Jim Austin, “A Survey of Outlier Detection\n\nMethodologies,” Artificial Intelligence Review 22, no. 2 (October 2004): 85-126, http://www.geo.upm.es/postgrado/CarlosLopez/\n\npapers/Hodge+Austin_OutlierDetection_AIRE381.pdf .\n\nRapoport explains that… Roy Rapoport, personal correspondence with Gene Kim, 2014.\n\nRapoport continues, “We… Ibid.\n\nRapoport states that… Ibid.\n\nAs John Vincent… Toufic Boubez, “Simple math for anomaly detection toufic boubez -\n\nmetafor software - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-\n\nboubez-metafor-software-monitorama-pdx-20140505 .\n\nTom Limoncelli, co-author… Tom Limoncelli, “Stop monitoring whether or not your service is up!,” EverythingSysAdmin.com, November 27, 2013,\n\nhttp://everythingsysadmin.com/2013/11/stop-monitoring-if-service-is-up.html .\n\nAs Dr. Toufic… Toufic Boubez, “Simple math for anomaly detection toufic boubez - metafor\n\nsoftware - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-boubez-\n\nmetafor-software-monitorama-pdx-20140505 .\n\nDr. Nicole Forsgren… Dr. Nicole Forsgren, personal correspondence with Gene Kim, 2015.\n\nScryer works by… Daniel Jacobson, Danny Yuan, and Neeraj Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine,” The Netflix Tech Blog, November 5, 2013,\n\nhttp://techblog.netflix.com/2013/11/scryer-netflixs-predictive-auto-scaling.html .\n\nThese techniques are… Varun Chandola, Arindam Banerjee, and Vipin Kumar, “Anomaly detection: A survey,” ACM Computing Surveys 41, no. 3 (July 2009): article no. 15,\n\nhttp://doi.acm.org/10.1145/1541880.1541882 .\n\nTarun Reddy, VP… Tarun Reddy, personal interview with Gene Kim, Rally headquarters,\n\nBoulder, CO, 2014.\n\nAt Monitorama in 2014… “Kolmogorov-Smirnov Test,” Wikipedia, last modified May 19,\n\n2016, http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test .\n\nEven saying Kilmogorov-Smirnov… ”Simple math for anomaly detection toufic boubez -\n\nmetafor software - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection -toufic-\n\nboubez-metafor-software-monitorama-pdx-20140505.\n\nCHAPTER 16\n\nIn 2006, Nick… Mark Walsh, “Ad Firms Right Media, AdInterax Sell To Yahoo,” MediaPost,\n\nOctober 18, 2006, http://www.mediapost.com/publications/article/49779/ad-firms-right-\n\nmedia-adinterax-sell-to-yahoo.html?edition= .\n\nGalbreath described the… Nick Galbreath, personal conversation with Gene, 2013.\n\nHowever, Galbreath observed… Nick Galbreath, “Continuous Deployment - The New #1 Security Feature, from BSildesLA 2012,” Slideshare.net, posted by Nick Galbreath, Aug 16,\n\n2012, http://www.slideshare.net/nickgsuperstar/continuous-deployment-the-new-1- security-feature .\n\nAfter observing many… Ibid.\n\nGalbreath observes that… Ibid.\n\nAs Patrick Lightbody… “Volocity 2011: Patrick Lightbody, ‘From Inception to Acquisition,’”\n\nYouTube video, 15:28, posted by O’Reilly, June 17, 2011, https://www.youtube.com/watch? v=ShmPod8JecQ.\n\nAs Arup Chakrabarti… Arup Chakrabarti, “Common Ops Mistakes,” presentation at Heavy Bit Industries, June 3, 2014, http://www .heavybit.com/library/video/common-ops-\n\nmistakes/\n\nMore recently, Jeff… ”From Design Thinking to DevOps and Back Again: Unifying Design & Operations,” Vimeo video, 21:19, posted by William Evans, June 5, 2015,\n\nhttps://vimeo.com/129939230.\n\nAs an anonymous… Anonymous, personal conversation with Gene Kim, 2005.\n\nLaunch guidance and… Tom Limoncelli, “SRE@Google: Thousands Of DevOps Since 2004,” YouTube video of USENIX Association Talk, NYC, posted by USENIX, 45:57, posted January\n\n12, 2012, http://www.youtube.com/watch?v=iIuTnhdTzK .\n\nAs Treynor Sloss has… Ben Treynor, “Keys to SRE” (presentation, Usenix SREcon14, Santa\n\nClara, CA, May 30, 2014), https://www.usenix.org/conference/srecon14/technical- sessions/presentation/keys-sre .\n\nTreynor Sloss has resisted… Ibid.\n\nEven when new… Limoncelli, “SRE@Google.”\n\nTom Limoncelli noted… Ibid.\n\nLimoncelli noted, “In… Ibid.\n\nFurthermore, Limoncelli observed… Tom Limoncelli, personal correspondence with Gene Kim, 2016.\n\nLimoncelli explained, “Helping… Ibid., 2015.\n\nCHAPTER 17\n\nIn general, Jez… Humble, O’Reilly and Molesky, Lean Enterprise, Part II.\n\nIn 2012, they… Intuit, Inc., “2012 Annual Report: Form 10-K,” July 31, 2012,\n\nhttp://s1.q4cdn.com/018592547/files/doc_financials/ 2012/INTU_2012_7_31_10K_r230_at_09_13_12_FINAL_and_Camera_Ready.pdf .\n\nCook explained that… Scott Cook, “Leadership in an Agile Age: An Interview with Scott Cook,” Intuit.com, April 20, 2011, https://web.archive.org/web/20160205050418/\n\nhttp://network.intuit.com/2011/04/20/leadership-in-the-agile-age/\n\nHe continued, “By… Ibid.\n\nIn previous eras… “Direct Marketing,” Wikipedia, last modified May 28, 2016, https://en.wikipedia.org/wiki/Direct_marketing .\n\nInterestingly, it has… Freakonomics, “Fighting Poverty With Actual Evidence: Full Transcript,” Freakonomics.com, November 27, 2013,\n\nhttp://freakonomics.com/2013/11/27/fighting-poverty-with-actual-evidence-full-transcript/ .\n\nRonny Kohavi, Distinguished… Ron Kohavi, Thomas Crook, and Roger Longbotham, “Online Experimentation at Microsoft,” (paper presented at the Fifteenth ACM SIGKDD International\n\nConference on Knowledge Discovery and Data Mining, Paris, France, 2009), http://www.exp-platform.com/documents/exp_dmcasestudies.pdf .\n\nKohavi goes on… Ibid.\n\nJez Humble joked… Jez Humble, personal correspondence with Gene Kim, 2015.\n\nIn a 2014… Wang, Kendrick, “Etsy’s Culture Of Continuous Experimentation and A/B Testing Spurs Mobile Innovation,” Apptimize.com, January 30, 2014,\n\nhttp://apptimize.com/blog/2014/01/etsy-continuous-innovation-ab-testing/ .\n\nBarry O’Reilly, co-author… Barry O’Reilly, “How to Implement Hypothesis-Driven\n\nDevelopment,” BarryOReilly.com, October 21, 2013,\n\nhttp://barryoreilly.com/2013/10/21/how-to-implement-hypothesis-driven-development/ .\n\nIn 2009, Jim… Gene Kim, “Organizational Learning and Competitiveness: Revisiting the\n\n“Allspaw/Hammond 10 Deploys Per Day at Flickr” Story,” ITRevolution.com, 2015, http://itrevolution.com/organizational-learning-and-competitiveness-a-different-view-of-\n\nthe-allspawhammond-10-deploys-per-day-at-flickr-story/ .\n\nStoneham observes that… Ibid.\n\nHe continues, “These… Ibid.\n\nTheir astounding achievements… Ibid.\n\nStoneham concluded, “This… Ibid.\n\nCHAPTER 18\n\nOnce a pull… Scott Chacon, “Github Flow,” ScottChacon.com, August 31, 2011,\n\nhttp://scottchacon.com/2011/08/31/github-flow.html .\n\nFor example, in… Jake Douglas, “Deploying at Github,” GitHub.com, August 29, 2012, https://github.com/blog/1241-deploying-at-github .\n\nA fifteen minute… John Allspaw, “Counterfactual Thinking, Rules, and the Knight Capital Accident,” KitchenSoap.com, October 29, 2013,\n\nhttp://www.kitchensoap.com/2013/10/29/counterfactuals-knight-capital/ .\n\nOne of the core… Bradley Staats and David M. Upton, “Lean Knowledge Work,” Harvard\n\nBusiness Review, October 2011, https://hbr.org/2011/10/lean-knowledge-work .\n\nIn the 2014… Velasquez, Kim, Kersten, and Humble, 2014 State of DevOps Report.\n\nAs Randy Shoup… Randy Shoup, personal interview with Gene Kim, 2015.\n\nAs Giary Özil… Giray Özil, Twitter post, February 27, 2013, 10:42 a.m.,\n\nhttps://twitter.com/girayozil/status/306836785739210752 .\n\nAs noted earlier… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the\n\nSame Continuous Build?,” (2013), http://scribes.tweetscriber.com/realgenekim/206 .\n\nIn 2010, there… John Thomas and Ashish Kumar, “Welcome to the Google Engineering Tools\n\nBlog,” Google Engineering Tools blog, posted May 3, 2011, http://google-\n\nengtools.blogspot.com/2011/05/welcome-to-google-engineering-tools.html .\n\nThis requires considerable… Ashish Kumar, “Development at the Speed and Scale of Google,”\n\n(presentation at QCon, San Francisco, CA, 2010), https://qconsf.com/sf2010/dl/qcon- sanfran-2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf .\n\nHe said, “I… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nJeff Atwood, one… Jeff Atwood, “Pair Programming vs. Code Reviews,” CodingHorror.com,\n\nNovember 18, 2013, http://blog.codinghorror.com/pair-programming-vs-code-reviews/ .\n\nHe continued, “Most… Ibid.\n\nDr. Laurie Williams performed… “Pair Programming,” ALICE Wiki page, last modified April 4, 2014, http://euler.math.uga.edu/wiki/index.php?title=Pair_programming .\n\nShe argues that… Elisabeth Hendrickson, “DOES15 - Elisabeth Hendrickson - Its All About Feedback,” YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015,\n\nhttps://www.youtube.com/watch?v=r2BFTXBundQ .\n\nIn her 2015… Ibid.\n\nThe problem Hendrickson… Ibid.\n\nWorse, skilled developers… Ibid.\n\nHendrickson lamented that… Ibid.",
      "page_number": 539
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 547-558)",
      "start_page": 547,
      "end_page": 558,
      "detection_method": "topic_boundary",
      "content": "That was an actual… Ryan Tomayko and Shawn Davenport, personal interview with Gene\n\nKim, 2013.\n\nIt is many… Ibid.\n\nReading through the… Ibid.\n\nAdrian Cockcroft observed… Adrian Cockcroft, interview by Michael Ducy and Ross Clanton,\n\n“Adrian Cockcroft of Battery Ventures – the Goat Farm – Episode 8,” The Goat Farm, podcast audio, July 31, 2015, http://goatcan.do/2015/07/31/adrian-cockcroft-of-battery-\n\nventures-the-goat-farm-episode-8/ .\n\nSimilarly, Dr. Tapabrata Pal… Tapabrata Pal, “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit,\n\nJanuary 4, 2016, https://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nJason Cox, Senior… Jason Cox, “Disney DevOps.”\n\nAt Target in… Ross Clanton and Heather Mickman, ‘DOES14 - Ross Clanton and Heather Mickman - DevOps at Target,” YouTube video, 29:20, posted by DevOps Enterprise Summit\n\n2014, October 29, 2014, https://www.youtube.com/watch?v=exrjV9V9vhY .\n\n“As we went… Ibid.\n\nShe added, “I… Ibid.\n\nConsider a story… John Allspaw and Jez Humble, personal correspondence with Gene Kim,\n\n2014.\n\nCHAPTER 19\n\nThe result is… Spear, The High-Velocity Edge, chap. 1.\n\n“For such an… Ibid., chap. 10.\n\nA striking example… Julianne Pepitone, “Amazon EC2 Outage Downs Reddit, Quora,” CNN Money, April 22, 2011,\n\nhttp://money.cnn.com/2011/04/21/technology/amazon_server_outage .\n\nIn January 2013… Timothy Prickett Morgan, “A Rare Peek Into The Massive Scale of AWS,”\n\nEnterprise Tech, November 14, 2014, http://www.enterprisetech.com/2014/11/14/rare-peek- massive-scale-aws/ .\n\nHowever, a Netflix… Adrian Cockcroft, Cory Hicks, and Greg Orzell, “Lessons Netflix Learned from the AWS Outage,” The Netflix Tech Blog, April 29, 2011,\n\nhttp://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html .\n\nThey did so… Ibid.\n\nDr. Sidney Dekker… Sidney Dekker, Just Culture: Balancing Safety and Accountability\n\n(Lund University, Sweden: Ashgate Publishing Company, 2007), 152.\n\nHe asserts that… “DevOpsDays Brisbane 2014 - Sidney Decker - System Failure, Human Error: Who’s to Blame?” Vimeo video, 1:07:38, posted by info@devopsdays.org, 2014,\n\nhttps://vimeo.com/102167635 .\n\nAs John Allspaw… Jenn Webb, interview with John Allspaw, “Post-Mortems, Sans Finger-\n\nPointing,” The O’Reilly Radar Postcast, podcast audio, August 21, 2014, http://radar.oreilly.com/2014/08/postmortems-sans-finger-pointing-the-oreilly-radar-\n\npodcast.html .\n\nBlameless post-mortems, a… John Allspaw, “Blameless PostMortems and a Just Culture,”\n\nCodeAsCraft.com, May 22, 2012, http://codeascraft.com/2012/05/22/blameless- postmortems/ .\n\nIan Malpass, an… Ian Malpass, “DevOpsDays Minneapolis 2014 -- Ian Malpass, Fallible humans,” YouTube video, 35:48, posted by DevOps Minneapolis, July 20, 2014,\n\nhttps://www.youtube.com/watch?v=5NY-SrQFrBU .\n\nDan Milstein, one… Dan Milstein, “Post-Mortems at HubSpot: What I Learned from 250\n\nWhys,” HubSpot, June 1, 2011, http://product.hubspot.com/blog/bid/64771/Post-Mortems- at-HubSpot-What-I-Learned-From-250-Whys .\n\nRandy Shoup, former… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nWe may also… “Post-Mortem for February 24, 2010 Outage,” Google App Engine website, March 4, 2010, https://groups.google.com/forum/#!topic/google-appengine/p2QKJ0OSLc8\n\n; “Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US- East Region,” Amazon Web Services website, accessed May 28, 2016,\n\nhttps://aws.amazon.com/message/5467D2/ .\n\nThis desire to… Bethany Macri, “Morgue: Helping Better Understand Events by Building a\n\nPost Mortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nFor example, as… Spear, The High-Velocity Edge, chap. 4.\n\nDr. Amy C. Edmondson… Amy C. Edmondson, “Strategies for Learning from Failure,”\n\nHarvard Business Review, April 2011, https://hbr.org/2011/04/strategies-for-learning- from-failure .\n\nDr. Spear summarizes… Ibid.\n\nWe now know… Ibid., chap. 3.\n\nHowever, prior to… Michael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson, “Facing Ambiguous Threats,” Harvard Business Review, November 2006,\n\nhttps://hbr.org/2006/11/facing-ambiguous-threats/ar/1 .\n\nThey describe how… Ibid.\n\nThey observe, “Firms… Ibid.\n\nThe authors conclude… Ibid.\n\nOn failures, Roy… Roy Rapoport, personal correspondence with Gene Kim, 2012.\n\nHe continues, “I… Ibid.\n\nHe concludes, “DevOps… Ibid.\n\nAs Michael Nygard… Michael T. Nygard, Release It!: Design and Deploy Production-Ready\n\nSoftware (Pragmatic Bookshelf: Raleigh, NC, 2007), Kindle edition, Part I.\n\nAn even more… Jeff Barr, “EC2 Maintenance Update,” AWS Blog, September 25, 2014,\n\nhttps://aws.amazon.com/blogs/aws/ec2-maintenance-update/ .\n\nAs Christos Kalantzis… Bruce Wong and Christos Kalantzis, “A State of Xen - Chaos Monkey & Cassandra,” The Netflix Tech Blog, October 2, 2014,\n\nhttp://techblog.netflix.com/2014/10/a-state-of-xen-chaos-monkey-cassandra.html .\n\nBut, Kalantzis continues… Ibid.\n\nAs Kalantzis and… Ibid.\n\nEven more surprising… Roy Rapoport, personal correspondence with Gene Kim, 2015.\n\nSpecific architectural patterns… Adrian Cockcroft, personal correspondence with Gene Kim, 2012.\n\nIn this section… Jesse Robbins, “GameDay: Creating Resiliency Through Destruction - LISA11,” Slideshare.net, posted by Jesse Robbins, December 7, 2011,\n\nhttp://www.slideshare.net/jesserobbins/ameday-creating-resiliency-through-destruction .\n\nRobbins defines resilience… Ibid.\n\nJesse Robbins observes… Jesse Robbins, Kripa Krishnan, John Allspaw, and Tom Limoncelli, “Resilience Engineering: Learning to Embrace Failure,” amcqueue 10, no. 9 (September 13,\n\n2012): https://queue.acm.org/detail.cfm?id=2371297 .\n\nAs Robbins quips… Ibid.\n\nAs Robbins describes… Ibid.\n\nRobbins explains, “You… Ibid.\n\nDuring that time… “Kripa Krishnan: ‘Learning Continuously From Failures’ at Google,” YouTube video, 21:35, posted by Flowcon, November 11, 2014,\n\nhttps://www.youtube.com/watch?v=KqqS3wgQum0 .\n\nKrishnan wrote, “An… Kripa Krishnan, “Weathering the Unexpected,” Communications of\n\nthe ACM 55, no. 11 (November 2012): 48-52, http://cacm.acm.org/magazines/2012/11/156583-weathering-the-unexpected/abstract .\n\nSome of the learnings… Ibid.\n\nAs Peter Senge… Widely attributed to Peter Senge.\n\nCHAPTER 20\n\nAs Jesse Newland… Jesse Newland, “ChatOps at GitHub,” SpeakerDeck.com, February 7,\n\n2013, https://speakerdeck.com/jnewland/chatops-at-github .\n\nAs Mark Imbriaco… Mark Imbriaco, personal correspondence with Gene Kim, 2015.\n\nThey enabled Hubot… Newland, “ChatOps at GitHub.”\n\nHubot often performed… Ibid.\n\nNewland observes that… Ibid.\n\nInstead of putting… Leon Osterweil, “Software processes are software too,” paper presented\n\nat International Conference on Software Engineering, Monterey, CA, 1987, http://www.cs.unibo.it/cianca/wwwpages/ids/letture/Osterweil.pdf .\n\nJustin Arbuckle was… Justin Arbuckle, “What Is ArchOps: Chef Executive Roundtable” (2013).\n\nWhat resulted was… Ibid.\n\nArbuckle’s conclusion was… Ibid.\n\nBy 2015, Google… Cade Metz, “Google Is 2 Billion Lines of Code—and It’s All in One Place,”\n\nWired, September 16, 2015, http://www.wired.com/2015/09/google-2-billion-lines- codeand-one-place/ .\n\nThe Chrome and… Ibid.\n\nRachel Potvin, a… Ibid.\n\nFurthermore, as Eran… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?” (2013),\n\nhttp://scribes.tweetscriber.com/realgenekim/206 .\n\nAs Randy Shoup… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nTom Limoncelli, co-author… Tom Limoncelli, “Yes, you can really work from HEAD,” EverythingSysAdmin.com, March 15, 2014, http://everythingsysadmin.com/2014/03/yes-\n\nyou-really-can-work-from-head.html .\n\nTom Limoncelli describes… Tom Limoncelli, “Python is better than Perl6,” EverythingSysAdmin.com, January 10, 2011,\n\nhttp://everythingsysadmin.com/2011/01/python-is-better-than-perl6.html .\n\nGoogle used C++… “Which programming languages does Google use internally?,” Quora.com\n\nforum, accessed May 29, 2016, https://www.quora.com/Which-programming-languages- does-Google-use-internally .; “When will Google permit languages other than Python, C++,\n\nJava and Go to be used for internal projects?,” Quora.com forum, accessed May 29, 2016, https://www.quora.com/When-will-Google-permit-languages-other-than-Python-C-Java-\n\nand-Go-to-be-used-for-internal-projects/answer/Neil-Kandalgaonkar .\n\nIn a presentation… Ralph Loura, Olivier Jacques, and Rafael Garcia, “DOES15 - Ralph Loura,\n\nOlivier Jacques, & Rafael Garcia - Breaking Traditional IT Paradigms to…,” YouTube video, 31:07, posted by DevOps Enterprise Summit, November 16, 2015,\n\nhttps://www.youtube.com/watch?v=q9nNqqie_sM .\n\nIn many organizations… Michael Rembetsy and Patrick McDonnell, “Continuously\n\nDeploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012, http://www.slideshare.net/mcdonnps/continuously-\n\ndeploying-culture-scaling-culture-at-etsy-14588485 .\n\nAt that time, Etsy… Ibid.\n\nOver the next… Ibid.\n\nSimilarly, Dan McKinley… Dan McKinley, “Why MongoDB Never Worked Out at Etsy,” McFunley.com, December 26, 2012, http://mcfunley.com/why-mongodb-never-worked-out-\n\nat-etsy .\n\nCHAPTER 21\n\nOne of the… “Kaizen,” Wikipedia, last modified May 12, 2016,\n\nhttps://en.wikipedia.org/wiki/Kaizen .\n\nDr. Spear explains… Spear, The High-Velocity Edge, chap. 8.\n\nSpear observes that… Ibid.\n\nClanton describes, “We… Mickman and Clanton, “(Re)building an Engineering Culture.”\n\nRavi Pandey, a… Ravi Pandey, personal correspondence with Gene Kim, 2015.\n\nClanton expands on… Mickman and Clanton, “(Re)building an Engineering Culture.”\n\nIn addition to… Hal Pomeranz, “Queue Inversion Week,” Righteous IT, February 12, 2009, https://righteousit.wordpress.com/2009/02/12/queue-inversion-week/ .\n\nAs Dr. Spear… Spear, The High-Velocity Edge, chap. 3.\n\nIn an interview with Jessica… Jessica Stillman, “Hack Days: Not Just for Facebookers,” Inc.,\n\nFebruary 3, 2012, http://www.inc.com/jessica-stillman/hack-days-not-just-for- facebookers.html .\n\nIn 2008, Facebook… AP, “Number of active users at Facebook over the years,” Yahoo! News, May 1, 2013, https://www.yahoo.com/news/number-active-users-facebook-over-\n\n230449748.html?ref=gs .\n\nDuring a hack… Haiping Zhao, “HipHop for PHP: Move Fast,” post on Haiping Zhao’s\n\nFacebook page, February 2, 2010, https://www.facebook.com/notes/facebook- engineering/hiphop-for-php-move-fast/280583813919 .\n\nIn an interview with Cade… Cade Metz, “How Three Guys Rebuilt the Foundation of Facebook,” Wired, June 10, 2013,\n\nhttp://www.wired.com/wiredenterprise/2013/06/facebook-hhvm-saga/all/ .\n\nSteve Farley, VP… Steve Farley, personal correspondence with Gene Kim, January 5, 2016.\n\nKarthik Gaekwad, who… “Agile 2013 Talk: How DevOps Change Everything,” Slideshare.net,\n\nposted by Karthik Gaekwad, August 7, 2013, http://www.slideshare.net/karthequian/howdevops\n\nchangeseverythingagile2013karthikgaekwad/ .\n\nAs Glenn O’Donnell… Glenn O’Donnell, “DOES14 - Glenn O’Donnell - Forrester - Modern\n\nServices Demand a DevOps Culture Beyond Apps,” YouTube video, 12:20, posted by DevOps Enterprise Summit 2014, November 5, 2014, https://www.youtube.com/watch?\n\nv=pvPWKuO4_48 .\n\nAs of 2014… Nationwide, 2014 Annual Report, https://www.nationwide.com/about-\n\nus/nationwide-annual-report-2014.jsp .\n\nSteve Farley, VP… Steve Farley, personal correspondence with Gene Kim, 2016.\n\nCapital One, one… “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nDr. Tapabrata Pal… Tapabrata Pal, personal correspondence with Gene Kim, 2015.\n\nTarget is the… “Corporate Fact Sheet,” Target company website, accessed June 9, 2016,\n\nhttps://corporate.target.com/press/corporate .\n\nIncidentally, the first… Evelijn Van Leeuwen and Kris Buytaert, “DOES15 - Evelijn Van\n\nLeeuwen and Kris Buytaert - Turning Around the Containership,” YouTube video, 30:28, posted by DevOps Enterprise Summit, December 21, 2015,\n\nhttps://www.youtube.com/watch?v=0GId4AMKvPc .\n\nClanton describes, “2015… Mickman and Clanton, “(Re)building an Engineering Culture.”\n\nAt Capital One… “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nBland explains that… Bland, “DOES15 - Mike Bland - Pain Is Over, If You Want It.”\n\nEven though they… Ibid.\n\nThey used several… Ibid.\n\nBland described, “The… Ibid.\n\nBland continues, “One… Ibid.\n\nAs Bland describes… Ibid.\n\nBland continues, “It… Ibid.\n\nHe continues, “The… Ibid.\n\nBland describes Fixits… Mike Bland, “Fixits, or I Am the Walrus,” Mike-Bland.com, October 4, 2011, https://mike-bland.com/2011/10/04/fixits.html .\n\nThese Fixits, as… Ibid.\n\nCHAPTER 22\n\nOne of the top… James Wickett, “Attacking Pipelines--Security meets Continuous Delivery,”\n\nSlideshare.net, posted by James Wickett, June 11, 2014,\n\nhttp://www.slideshare.net/wickett/attacking-pipelinessecurity-meets-continuous-delivery .\n\nJames Wickett, one… Ibid.\n\nSimilar ideas were… Tapabrata Pal, “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nJustin Arbuckle, former… Justin Arbuckle, personal interview with Gene Kim, 2015.\n\nHe continues, “By… Ibid.\n\nThis helped the… Snehal Antani, “IBM Innovate DevOps Keynote,” YouTube video, 47:57,\n\nposted by IBM DevOps, June 12, 2014, https://www.youtube.com/watch?v=s0M1P05-6Io .\n\nIn a presentation… Nick Galbreath, “DevOpsSec: Appling DevOps Principles to Security,\n\nDevOpsDays Austin 2012,” Slideshare, posted by Nick Galbreath, April 12, 2012, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security .\n\nFurthermore, he states… Ibid.\n\nFurthermore, we should… “OWASP Cheat Sheet Series,” OWASP.org, last modified March 2, 2016, https://www.owasp.org/index.php/OWASP_Cheat_Sheet_Series .\n\nThe scale of… Justin Collins, Alex Smolen, and Neil Matatall, “Putting to your Robots to Work V1.1,” Slideshare.net, posted by Neil Matatall, April 24, 2012,\n\nhttp://www.slideshare.net/xplodersuv/sf-2013-robots/ .\n\nIn early 2009… “What Happens to Companies That Get Hacked? FTC Cases,” Giant Bomb\n\nforum, posted by SuicidalSnowman, July 2012, http://www.giantbomb.com/forums/off- topic-31/what-happens-to-companies-that-get-hacked-ftc-case-540466/ .\n\nIn their previously… Collins, Smolen, and Matatall, “Putting to your Robots to Work V1.1.”\n\nThe first big… Twitter Engineering, “Hack Week @ Twitter,” Twitter blog, January 25, 2012,\n\nhttps://blog.twitter.com/2012/hack-week-twitter .\n\nJosh Corman observed… Josh Corman and John Willis, “Immutable Awesomeness - Josh\n\nCorman and John Willis at DevOps Enterprise Summit 2015,” YouTube video, 34:25, posted by Sonatype, October 21, 2015, https://www.youtube.com/watch?v=-S8-lrm3iV4 .\n\nIn the 2014… Verizon, ”2014 Data Breach Investigations Report,” (Verizon Enterprise Solutions, 2014), https://dti.delaware.gov/pdfs/rp_Verizon-DBIR-2014_en_xg.pdf .\n\nIn 2015, this… “2015 State of the Software Supply Chain Report: Hidden Speed Bumps on the Way to ‘Continuous,’” (Fulton, MD: Sonatype, Inc, 2015),\n\nhttp://cdn2.hubspot.net/hubfs/1958393/White_Papers/2015_State_\n\nof_the_Software_Supply_Chain_Report-.pdf?t=1466775053631 .\n\nThe last statistic… Dan Geer and Joshua Corman, “Almost Too Big to Fail,” ;login:: The\n\nUsenix Magazine, 39, no. 4 (August 2014): 66-68, https://www.usenix.org/system/files/login/articles/15_geer_0.pdf .\n\nUS Federal Government… Wyatt Kash, “New details released on proposed 2016 IT spending,” FedScoop, February 4, 2015, http://fedscoop.com/what-top-agencies-would-\n\nspend-on-it-projects-in-2016 .\n\nAs Mike Bland… Bland, “DOES15 - Mike Bland - Pain Is Over, If You Want It.”\n\nFurthermore, the Cloud.gov… Mossadeq Zia, Gabriel Ramírez, Noah Kunin, “Compliance\n\nMasonry: Bulding a risk management platform, brick by brick,” 18F, April 15, 2016, https://18f.gsa.gov/2016/04/15/compliance-masonry-buildling-a-risk-management-\n\nplatform/ .\n\nMarcus Sachs, one… Marcus Sachs, personal correspondence with Gene Kim, 2010.\n\nWe need to… “VPC Best Configuration Practices,” Flux7 blog, January 23, 2014, http://blog.flux7.com/blogs/aws/vpc-best-configuration-practices .\n\nIn 2010, Nick… Nick Galbreath, “Fraud Engineering, from Merchant Risk Council Annual Meeting 2012,” Slideshare.net, posted by Nick Galbreath, May 3, 2012,\n\nhttp://www.slideshare.net/nickgsuperstar/fraud-engineering .\n\nOf particular concern… Nick Galbreath, “DevOpsSec: Appling DevOps Principles to Security,\n\nDevOpsDays Austin 2012,” Slideshare.net, posted by Nick Galbreath, April 12, 2013, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security .\n\nWe were always… Ibid.\n\nThis was a ridiculously… Ibid.\n\nAs Galbreath observed… Ibid.\n\nGalbreath observed, “One… Ibid.\n\nAs Jonathan Claudius… Jonathan Claudius, “Attacking Cloud Services with Source Code,”\n\nSpeakerdeck.com, posted by Jonathan Claudius, April 16, 2013, https://speakerdeck.com/claudijd/attacking-cloud-services-with-source-code .\n\nCHAPTER 23\n\nITIL defines utility… Axelos, ITIL Service Transition (ITIL Lifecycle Suite) (Belfast, Ireland: TSO, 2011), 48.\n\nSalesforce was founded… Reena Matthew and Dave Mangot, “DOES14 - Reena Mathew and Dave Mangot - Salesforce,” Slideshare.net, posted by ITRevolution, October 29, 2014,\n\nhttp://www.slideshare.net/ITRevolution/does14-reena-matthew-and-dave-mangot- salesforce .\n\nBy 2007, the… Dave Mangot and Karthik Rajan, “Agile.2013.effecting.a.dev ops.transformation.at.salesforce,” Slideshare.net, posted by Dave Mangot, August 12, 2013,\n\nhttp://www.slideshare.net/dmangot/agile2013effectingadev-opstransformationatsalesforce .\n\nKarthik Rajan, then… Ibid.\n\nAt the 2014… Matthew and Mangot, “DOES14 - Salesforce.”\n\nFor Mangot and… Ibid.\n\nFurthermore, they noted… Ibid.\n\nBill Massie is… Bill Massie, personal correspondence with Gene Kim, 2014.\n\nBecause the scope… “Glossary,” PCI Security Standards Council website, accessed May 30,\n\n2016, https://www.pcisecuritystandards.org/pci_security/glossary .\n\nAre code review… PCI Security Standards Council, Payment Card Industry (PCI) Data\n\nSecurity Stands: Requirements and Security Assessment Procedures, Version 3.1 (PCI Security Standards Council, 2015), Section 6.3.2.\n\nhttps://webcache.googleusercontent.com/search? q=cache:hpRe2COzzdAJ:https://www.cisecuritystandards.org/documents/PCI_DSS_v3-\n\n1_SAQ_D_Merchant_rev1-1.docx+&cd=2&hl=en&ct=clnk&gl=us .\n\nTo fulfill this… Bill Massie, personal correspondence with Gene Kim, 2014.\n\nMassie observes that… Ibid.\n\nAs a result… Ibid.\n\nAs Bill Shinn… Bill Shinn, “DOES15 - Bill Shinn - Prove it! The Last Mile for DevOps in\n\nRegulated Organizations,” Slideshare.net, posted by ITRevolution, November 20, 2015, http://www.slideshare.net/ITRevolution/does15-bill-shinn-prove-it-the-last-mile-for-\n\ndevops-in-regulated-organizations .\n\nHelping large enterprise… Ibid.\n\nShinn notes, “One… Ibid.\n\n“That was fine… Ibid.\n\nHe explains, “In… Ibid.\n\nShinn states that… Ibid.\n\nShinn continues, “With… Ibid.\n\nThat requires deriving… Ibid.\n\nShinn continues, “How… Ibid.\n\nShinn gives an… Ibid.\n\nTo help solve… James DeLuccia, Jeff Gallimore, Gene Kim, and Byron Miller, DevOps Audit\n\nDefense Toolkit (Portland, OR: IT Revolution, 2015), http://itrevolution.com/devops-and-\n\nauditors-the-devops-audit-defense-toolkit .\n\nShe made the… Mary Smith (a pseudonym), personal correspondence with Gene Kim, 2013\n\nShe observed:… Ibid., 2014.\n\nCONCLUSION\n\nAs Jesse Robbins… “Hacking Culture at VelocityConf,” Slideshare.net, posted by Jesse Robbins, June 28, 2012, http://www.slideshare.net/jesserobbins/hacking-culture-at-\n\nvelocityconf .\n\nAPPENDIX\n\nThe Lean movement started… Ries, The Lean Startup.\n\nA key principal… Kent Beck et al., “Twelve Principles of Agile Software,” AgileManifesto.org,\n\n2001, http://agilemanifesto.org/principles.html .\n\nBuilding upon the… Humble and Farley, Continuous Delivery.\n\nThis idea was… Fitz, “Continuous Deployment at IMVU.”\n\nToyota Kata describes… Rother, Toyota Kata, Introduction.\n\nHis conclusion was… Ibid..\n\nIn 2011, Eric… Ries, The Lean Startup.\n\nIn The Phoenix… Kim, Behr, and Spafford, The Phoenix Project, 365.\n\nMyth 1: “Human… Denis Besnard and Erik Hollnagel, Some Myths about Industrial Safety(Paris, Centre De Recherche Sur Les Risques Et Les Crises Mines, 2012), 3,\n\nhttp://gswong.com/?wpfb_dl=31 .\n\nMyth 2: “Systems… Ibid., 4.\n\nMyth 3: “Safety… Ibid., 6.\n\nMyth 4: “Accident… Ibid., 8.\n\nMyth 5: “Accident… Ibid., 9.\n\nMyth 6: Safety… Ibid., 11.\n\nRather, when the… John Shook, “Five Missing Pieces in Your Standardized Work (Part 3 of 3),” Lean.org, October 27, 2009, http://www.lean.org/shook/DisplayObject.cfm?o=1321 .\n\nTime to resolve… “Post Event Retrospective - Part 1,” Rally Blogs, accessed May 31, 2016,\n\nhttps://www.rallydev.com/blog/engineering/post-event-retrospective-part-i .\n\nBethany Macri, from… “Morgue: Helping Better Understand events by Building a Post\n\nMortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nThese discussions have… Cockcroft, Hicks, and Orzell, “Lessons Netflix Learned.”\n\nSince then, Chaos… Ibid.\n\nLenny Rachitsky wrote… Lenny Rachitsky, “7 Keys to a Successful Public Health\n\nDashboard,” Transparent Uptime, December 1, 2008, http://www.transparentuptime.com/2008/11/rules-for-successful-public-health.html .",
      "page_number": 547
    },
    {
      "number": 66,
      "title": "Segment 66 (pages 559-567)",
      "start_page": 559,
      "end_page": 567,
      "detection_method": "topic_boundary",
      "content": "Index\n\nSymbols Numbers A B C D E F G H I J K L M N O P Q R S T U V W Y Z\n\nNote: Figures are indicated with f; footnotes are indicated with n\n\nSYMBOLS %C/A, 11, 65\n\nNUMBERS 2PT. See two-pizza team\n\n18F team, 325–326 2013 State of DevOps Report, 159–160\n\nA AAS. See Amazon Auto Scaling\n\nAdams, Keith, 302 Agile\n\nInfrastructure and Velocity Movement, 5\n\nInfrastructure Movement, 354 Manifesto, 4–5 Movement, 354 principles, xxii–xxiii\n\nAisin Seiki Global, 43–44\n\nAlcoa, 41–42, 279\n\nAlgra, Ingrid, 305n\n\nAllspaw, John\n\nAgile Infrastructure and Velocity Movement, 5\n\ndark launches, 173–174\n\ndeployment failures, 251–252\n\nlearning culture, 273–274\n\nproduction metrics, 204\n\nVelocity Movement, 354\n\nAllstate, 66\n\nAmazon\n\ncontinuous delivery, 176 deploys per day, xxxivn evolutionary architecture, 184–185 market-oriented organization, 81 service-oriented architecture, 90–91\n\nAmazon Auto Scaling, 221–222 Amazon AWS\n\ncompliance in regulated environments, 342–344 resilience, 271–273, 281–282 service outage, 271n\n\nAndon cord\n\nconsequences of not pulling, 140\n\nillustrated, 361f swarming, 31, 32 virtual, 138–140 and work stoppages, 360–361\n\nAntani, Snehal, 314 APIs\n\nenablement, 91–93 Feature API, 245\n\nservice interactions using, 89\n\nversioned, 185\n\napplication logging, 201–203, 201n\n\napplication-based release patterns, 171–175\n\nArbuckle, Justin, 290, 314\n\narchitecture, evolutionary\n\nAmazon case study, 184–185\n\narchitectural archetypes, 183f\n\nBlackboard Learn case study, 186–189, 187f, 188f\n\ncode repository, 187f, 188f\n\ndecoupling functionality, 186\n\ndescription of, 179–180\n\nimmutable services, 185 loosely-coupled architecture, 181–182 monoliths vs microservices, 182–185 Second Law of Architectural Thermodynamics, 180–181 service-oriented architecture, 182 strangler application pattern, 180, 185–189\n\ntightly-coupled architecture, 180–181, 185 versioned APIs, 185 versioned services, 185\n\narchitecture, loosely-coupled, 89–93, 181–182, 254–255\n\narchitecture, monitoring, 198–199 architecture, service-oriented, 89, 90–91, 182 Ashman, David, 187\n\nATDD. See development, acceptance test-driven Atwood, Jeff, 147, 259–260 Austin, Jim, 215–216 automated environment build process\n\nassets to check into version control repository, 116 automated configuration systems, 118\n\nbenefits of automation, 114–115 common build mechanisms, 113\n\ncritical role of version control, 117 environment consistency, 118\n\nenvironment development on demand, 113–115\n\nenvironment re-build vs repair, 118 environments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120 standardization, 114\n\ntesting, 113 testing environments, 113\n\nuses of automation, 114\n\nversion control as predictor of organizational performance, 117 version control systems, 115–118\n\nautomated validation test suite\n\nacceptance test-driven development, 134–135\n\nacceptance tests, 131, 132 analysis tools, 138\n\nautomating manual tests, 135–136\n\ncode configuration management tools, 138 environment validation, 137–138\n\nerror detection, 132–133 fast testing, 132, 133–134\n\nfeedback, 130 green builds, 129–130\n\nideal vs non-ideal testing, 133f\n\nintegration tests, 131, 132 non-functional requirements testing, 137–138\n\nperformance testing, 136–137 test types, 130–131\n\ntest-driven development, 134–135 testing in parallel, 133–134, 134f\n\nunit tests, 130–131, 132–133\n\nunreliable test, 135\n\nautomation. See automated environment build process;\n\ndeployment process automation; testing, automated\n\nB Baker, Bill, 118\n\nBarnes & Noble, 51 batch sizes\n\ncontinuous deployment, 20\n\nerror management, 19 large, 19–20\n\nsingle-piece flow, 19, 20 small, 18–20\n\nsmall batch strategy, 19\n\nsmall vs large, 20f\n\nBazaarvoice, 97n, 149–151\n\nBeck, Kent, 134, 154 Beedle, Mike, 102n\n\nBehr, Kevin, 195, 203n Besnard, Denis, 359–360\n\nBetz, Charles, 180–181, 313n\n\nBig Fish Games, 95–97 bimodal IT, 56\n\nBlackboard Learn\n\ncase study, 187f, 188f\n\nPerl, 187\n\nstrangler application pattern, 186–189\n\nblameless post-mortems\n\ncountermeasures, 276 goals of, 274–275\n\noutcomes of, 275–276 publicizing, 277–278\n\nsample agenda, 362–364\n\nstakeholders present, 275 transparent uptime, 277n\n\nBland, Mike, 123–126, 124n, 306–307, 325 Blank, Steve, 355\n\nBlankenship, Ed, 210\n\nblitz\n\ngoals, 301\n\nimprovement, 299 kaizen, 299\n\nBlockbuster, 51 blue-green deployment pattern\n\ndeployment, 166–169, 166f, 167n\n\nFarley, David, 168–169 low-risk releases, 166–169, 166f\n\nRuby on Rails, 167n\n\nBMW, 67\n\nBohmer, Richard M. J., 279 Booch, Grady, 6n\n\nBorders, 51 Boubez, Toufic, 219, 224–226\n\nBouwman, Jan-Joost, 305n Brakeman, 322, 322f\n\nbrownfield services. See services, brownfield Building Blocks, 188–189\n\nbuild-measure-learn cycle, 355 bureaucratic organizations, 39 Burgess, Mark, 6n\n\nbusiness logic\n\nchanges to, 78, 79 coordinating changes to, 79 moving to application layer, 79\n\nbusiness relationship manager, 96 Buytaert, Kris, 305n\n\nC C++\n\neBay, 179n, 182 Facebook, 153n, 175, 302\n\nGoogle, 296n Google Web Server, 123\n\nCagan, Marty, 70\n\nCampbell-Pretty, Em, 111–112 Canahuati, Pedro, 85 canary release pattern, 153n, 169–171, 170f, 170n canary tests, 153\n\nCapitol One, 304–306 case studies\n\nAmazon, 184–185\n\nAmazon AWS, 271–273, 344–345 anomaly detection techniques, 224–226 ATM systems, 344–345 Bazaarvoice, 149–151\n\nBig Fish Games, 95–97\n\nBlackboard Learn, 186–189, 187f, 188f Capitol One, 304–306\n\nCSG International, 157–159 Dixons Retail, 168–169 Etsy, 77–80, 162–164, 297–298, 328–330, 339–341\n\nFacebook, 153–155, 174–175 Federal Government, 325–326 Google, 237–239, 257–258 Google Web Server, 123–126\n\nHP, 144–146 Intuit, 241–248 LinkedIn, 71–73, 207–208\n\nNationwide Insurance, 304–306 Netflix, 215–216, 221–222, 271–273 Nordstrom, 51–55, 61–62 Pivotal Labs, 260–261\n\nRight Media, 227–229 Salesforce.com, 337–338 Target, 91–93, 299–300, 304–306\n\nTwitter, 320–323 Yahoo! Answers, 246–248\n\nChacon, Scott, 249 Chakrabarti, Arup, 232\n\nchange approval processes. See also code reviews\n\ncase study, 249–251 change advisory boards, 253\n\nchange control failures, 252–253 change freezes, 258–259 change review lead times, 258f code reviews, 255–258\n\ncoordination and scheduling of changes, 254–255\n\ncounterfactual thinking, 251, 251n cutting bureaucratic processes, 263–264\n\ndangers of, 251–252 email pass-around, 257 engineer roles, 259 GitHub Flow, 250\n\nGoogle case study, 257–258 guidelines for code reviews, 256 in a loosely-coupled architecture, 254–255\n\nmanual testing, 258–259 over-the-shoulder, 256 pair programming, 256, 259–263 peer reviews, 249–251, 253, 254f, 255–258\n\nPivotal Labs case study, 260–261 pull requests, 250, 250f, 261–263 review steps, 250–251\n\nsmall batch sizes, 255–256 test-driven development, 259–260 tool-assisted, 257 traditional change controls, 252–254\n\ntypes of code reviews, 256–257\n\nChaos Monkey, 272–273, 281–282, 364 chat rooms, 74\n\nChatOps, 287–289 Chuvakin, Anton A., 202 CI. See continuous integration Clanton, Ross, 263, 299–300, 305\n\nClaudius, Jonathan, 330 Clemm, Josh, 71, 72 Cloud.gov, 325–326\n\ncluster immune system, 171n",
      "page_number": 559
    },
    {
      "number": 67,
      "title": "Segment 67 (pages 568-575)",
      "start_page": 568,
      "end_page": 575,
      "detection_method": "topic_boundary",
      "content": "cluster immune system release pattern, 169, 170–171 coaching kata, 45 Cockcoft, Adrian, 82n, 200, 263\n\ncode\n\ncommits, 148 configuration management tools, 138\n\ndeployment, 22, 160–162 deployment process automation, 160 deployment process changes, 154\n\ninfrastructure as, 6n merging, 143–144 migration, 117n packaging, 128\n\nrepositories, 187f, 188f, 290–292, 315–317 re-use, 289–290 signing, 319–320\n\nsource code integrity, 319–320\n\ncode commits, 148 code reviews. See also change approval processes\n\nchange review lead times, 258f\n\nemail pass-around, 257 Google case study, 257–258 guidelines for, 256\n\nover-the-shoulder, 256 pair programming, 256 small batch sizes, 255–256 tool-assisted, 257\n\ntypes of, 256–257\n\ncollective knowledge, 42–43 compliance\n\naudit and compliance documentation and proof, 341–345\n\nregulatory compliance objectives, 235–236 security and compliance and change approval processes, 333– 335\n\nconfiguration management tools, 116n constraints\n\nbottlenecks, 22\n\ncode deployment, 22 environment creation, 22 overly tight architecture, 23 test setup and run, 22–23\n\ncontinual experimentation and learning, 37–46 continuous delivery\n\ndeployment pipeline, 127–129, 127f\n\nGoogle, 176 low-risk releases, 175–177\n\nContinuous Delivery Movement, 5–6, 354 continuous deployment, 6, 20, 175–177\n\ncontinuous integration case study, 149–151 code merging, 143–144\n\ndescription of, 144–146 Dev vs DevOps, 126n frequent code commits, 148 gated commits, 148\n\nintegration problems, 143 large batch development, 147–148 and trunk-based development practices, 148–151\n\nand version control, 148–149 Convergence of DevOps, 353–356 Conway, Melvin, 77 Conway’s Law, 77–78, 88\n\nCook, Scott, 242 core chronic conflict, xxiv–xxvi, xxvn core conflict cloud, 356–357, 356f\n\nCorman, Josh, 313, 323 cost of delay, 213n COTS software, 361 Cox, Jason, 87, 99–100, 263\n\nCSG International\n\nbrownfield services, 56 cross-training, 86–87\n\ndaily deployments, 157–159\n\nCunningham, Ward, 148\n\nD dark launches, 173–175 dashboards, 207n data sets. See telemetry\n\nDebois, Patrick, 5 dedicated release engineer, 96 DeGrandis, Dominica, 18 Dekker, Sidney\n\njust culture, 273 safety culture, 28, 38\n\ndependency scanning\n\nJava, 319\n\nRuby on Rails, 319\n\nDeployinator, 163–164, 163f deployment\n\nautomated self-service, 159–160\n\nblue-green pattern, 166–169, 166f, 167n change, 124\n\ncode, 22, 154, 160–162\n\nconsistency, 156\n\ncontinuous, 20, 175–177 daily, 157–159\n\ndecoupling from releases, 164–175\n\ndefined, 164 on demand, 165\n\nfast, 161, 161f flow, 156\n\nissues, 78\n\nlead time, 8–11, 9f, 165 making safer, 229–230\n\noverlay of production deployment activities, 213\n\npace, 154 pipeline requirements, 156–157\n\nprocess automation, 155–164, 159f\n\nself-service developer, 162–164 speed and success, 79\n\ntool, 163–164, 163f\n\ndeployment lead time\n\ndesign and development, 8\n\nlead time vs processing time, 9–10, 9f\n\nLean Manufacturing, 9 Lean Product Development, 8\n\nlong, 10, 10f, 165 short, 10–11, 11f\n\ntesting and operations, 9\n\nworkflow, 9\n\ndeployment pipeline\n\nbreakdown, 138–140\n\ncontainers in, 128n\n\ncontinuous delivery, 127–129, 127f\n\nand information security, 330–331\n\ndeployment pipeline protection\n\nAmazon AWS case study, 342–344\n\naudit and compliance documentation and proof, 341–345\n\ncategories of changes, 334–335, 334n compliance in regulated environments, 341–345\n\ndestructive testing, 338 Etsy case study, 339–341\n\nnormal changes, 334, 336–338\n\nproduction telemetry for ATM systems, 344–345 Salesforce case study, 337–338\n\nsecurity and compliance and change approval processes, 333–\n\n335 separation of duties, 338–341\n\nstandard changes, 334, 335–336\n\nurgent changes, 334–335\n\ndeployment process automation\n\nautomated self-service deployments, 159–160\n\nautomating manual steps, 155–156 code deployment as part of deployment pipeline, 160–162\n\ncode promotion processes, 160 CSG International case study, 157–159\n\ndeployment consistency, 156\n\ndeployment flow, 156 deployment pipeline requirements, 156–157\n\nenvironment consistency, 157, 158\n\nEtsy case study, 162–164 fast deployments, 161, 161f\n\nlead time reduction, 156\n\nMTTR, 158, 159f, 161f\n\nprocess documentation, 155\n\nproduction incident decrease, 158, 159f\n\nself-service developer deployment, 162–164 Shared Operations team, 157\n\nsmoke testing, 156, 163\n\ndeployment vs releases, 164–175\n\nDev in production-like environments\n\nassets to check into version control repository, 116 automated configuration systems, 118\n\nbenefits of automation, 114–115\n\ncommon build mechanisms, 113 critical role of version control, 117\n\nenvironment consistency, 118\n\nenvironment development on demand, 113–115 environment re-build vs repair, 118\n\nenvironments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120 standardization, 114\n\ntesting, 113\n\ntesting environments, 113 uses of automation, 114\n\nversion control as predictor of organizational performance, 117\n\nversion control systems, 115–118 Development. See entries under Dev\n\ndevelopment, acceptance test-driven, 134–135\n\ndevelopment, test-driven\n\nautomated testing, 134–135, 293\n\nhandling defect density, 135n\n\nand low-risk releases, 154 and pair programming, 259–260\n\ntesting before code writing, 9n development, trunk-based, 143–151\n\ndevelopment, waterfall, 5\n\nDevOps\n\nAgile Infrastructure and Velocity Movement, 5\n\nAgile Manifesto, 4–5\n\nbusiness value of, xxxii–xxxiii Continuous Delivery Movement, 5–6\n\ndefined, 4\n\nDevOpsDays, 5 downward spiral in, xxx–xxxii\n\nethics of, xxix–xxxv\n\nhistory of, 3–6 Lean Movement, 4\n\nrevolution, xxii team engagement, 101n\n\nThe Three Ways, 11–12\n\nToyota Kata movement, 6\n\nDevOps myths\n\nLAMP stack, xvi\n\nMySQL, xvi PHP, xvi\n\nDevOps transformation\n\nbimodal IT, 56 chat rooms, 74\n\nexpanding DevOps, 58–59\n\ngreenfield vs brownfield services, 54–56\n\nleveraging innovators, 57–58\n\nLinkedIn case study, 71–73 making work visible, 73\n\nmanaging technical debt, 69–71\n\nphases of initiatives, 59 rapid communication environment, 74\n\nreinforcing desired behavior, 73–74\n\nshared tools, 73–74 shared work queue, 73–74\n\nsystems of engagement, 56–57\n\nsystems of record, 56 technical debt, 69–71, 70f\n\ntechnology adoption curve, 58f\n\ntransformation team, 66–73\n\nDevOpsDays, 5, 305n\n\nDignan, Larry, 91 Disney, 87, 99–100\n\nDixons Retail, 168–169\n\ndocumentation\n\nautomated tests as, 293\n\nprocess, 155\n\ndownward spiral, xxvi–xxviii, xxx–xxxii, 357, 357f Drucker, Peter, 60\n\nDweck, Carol, 87\n\nE eBay, 70, 179–180, 179n, 182\n\ne-commerce sites\n\nanomaly detection techniques, 224–226 application security, 318\n\nmetrics sources, 210",
      "page_number": 568
    },
    {
      "number": 68,
      "title": "Segment 68 (pages 576-583)",
      "start_page": 576,
      "end_page": 583,
      "detection_method": "topic_boundary",
      "content": "Nordstrom, 51–55\n\nTarget, 91–93\n\nEdmondson, Amy C., 278, 279\n\nEdwards, Damon, 64, 97\n\nemployee Net Promotor Score, xxxiiin environment consistency, 157, 158\n\nenvironment security, 324–326, 324n\n\nenvironments\n\nconsistency, 157, 158\n\ncreation restraints, 22\n\ndefinition of, 113n rapid communication environment, 74\n\nvalidation, 137–138\n\nenvironments, automated build\n\nassets to check into version control repository, 116\n\nautomated configuration systems, 118 benefits of automation, 114–115\n\ncommon build mechanisms, 113\n\ncritical role of version control, 117 environment consistency, 118\n\nenvironment development on demand, 113–115\n\nenvironment re-build vs repair, 118 environments stored in version control, 115–116\n\nimmutable infrastructure, 119\n\nmetadata, 115 new definition of finished development, 119–121\n\nquick environment development, 112\n\nshared version control repository, 115–118 sprints, 119–120\n\nstandardization, 114 testing, 113\n\ntesting environments, 113\n\nuses of automation, 114 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nenvironments, production-like\n\nassets to check into version control repository, 116\n\nautomated configuration systems, 118\n\nbenefits of automation, 114–115 common build mechanisms, 113\n\ncritical role of version control, 117\n\ndevelopment on demand, 113–115 environment consistency, 118\n\nenvironment development on demand, 113–115 environment re-build vs repair, 118\n\nenvironments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120\n\nstandardization, 114 testing, 113\n\ntesting environments, 113\n\nuses of automation, 114 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nerrors\n\ndetection, 28, 114n, 132–133\n\nmanagement, 19\n\nEtsy\n\nbrownfield services, 56\n\ncase study, 77–80, 162–164 code deployment, 162n\n\ndesignated Ops, 100–101\n\nDevOps transformation, 51 functional-oriented organizations, 84\n\nLAMP stack at, 196\n\nmetrics library, 204–206 Morgue, 277–278\n\nMySQL at, 196, 297–298\n\norganizational learning, 40 PHP, 196, 297, 297n\n\nproduction monitoring, 196–198 programming languages used, 297n\n\npublicizing post-mortems, 277\n\nPython, 297, 297n security telemetry, 328–330\n\nseparation of duties, 339–341\n\nSprouter, 78–80, 88 technology stack standardization, 297–298\n\nEvans, Eric J., 89\n\nEvans, Jason, 302 Extreme Programming, 134, 154\n\nF Facebook\n\nC++, 153n, 175, 302\n\ncanary release pattern, 170\n\ncase study, 153–155 dark launches, 174–175\n\nfront-end codebase, 153n\n\nGatekeeper, 172n\n\nJavaScript, 175 PHP, 153n, 154, 175, 302\n\nshared pain, 85\n\ntechnical debt, 302\n\nFarley, David\n\nautomated testing, 126–127\n\nblue-green deployment pattern, 168–169 continuous delivery, 175–176, 354\n\nContinuous Delivery Movement, 5–6 continuous integration, 126n\n\ninfrastructure as code, 6n\n\nFarley, Steve, 303, 305 Farrall, Paul, 95–97\n\nfast release cycle experimentation\n\ncase study, 246–248 Feature API, 245\n\nhistory of, 243\n\nintegrating into feature planning, 245–248 integrating into feature testing, 244–245\n\nintegrating into release, 245\n\noutcomes of, 244 user research, 244–245\n\nYahoo! Answers, 246–248\n\nfast testing, 132, 133–134\n\nfeature toggles, 171–173, 172n, 175, 229–230\n\nfeatures\n\nconsequences of new, 57\n\nextra, 24\n\nplanning, 245–248 testing, 244–245\n\nFederal Government agencies, 325–326\n\nfeedback\n\nautomated build, integration, and test processes, 30\n\nautomated testing, 126, 130\n\nerror detection, 28 ineffective quality controls, 32–34\n\nmechanisms for production telemetry, 229\n\nOps and market-oriented outcomes, 103 optimizing for downstream work centers, 34–35\n\nproblem prevention, 30 problem visibility, 29–30\n\nproduction telemetry, 229\n\nQA automation, 33–34 safety in complex systems, 27–29\n\nswarming, 30–32\n\nfeed-forward loops, 29, 30 Fernandez, Roberto, 59, 80\n\nfirst stories, 360f\n\nFitz, Tim\n\ncontinuous delivery, 354\n\ncontinuous deployment, 6, 175–177\n\nexpand/contract pattern, 168n\n\nfix forward, 230\n\nfixed mindset, 87 Flickr, 173–174\n\nForsgren, Nicole, 220\n\nFowler, Martin, 132, 185, 186\n\nG Gaekwad, Karthik, 303\n\nGalbreath, Nick, 227–229, 328–330\n\nGame Days, 282–284\n\nGanglia, 196 Garcia, Rafael, 297\n\nGatekeeper, 172n, 175\n\nGauntlt, 313, 317 General Motors Fremont plant, 29, 31, 37\n\ngenerative organizations, 39–40 GitHub\n\nfunctional-oriented organizations, 84\n\nGitHub Flow, 250 organizational knowledge, 287–289\n\npeer reviews, 249–251\n\nGitHub Flow, 250 goal setting, 68\n\nGoogle\n\nautomated testing, 125n C++, 296n\n\ncode reviews, 257–258\n\ncontinuous delivery, 176 Disaster Recovery Program (DiRT), 284\n\nfunctional-oriented organizations, 84 imposter syndrome, 124n\n\nJava, 296n\n\nJavaScript, 296n launch and hand-off readiness reviews, 237–239\n\nproduction service, 234\n\nprogramming languages used, 297n publicizing post-mortems, 277\n\nPython, 296\n\nservice-oriented architecture, 90 source code repository, 291–292\n\nGoogle App Engine, 139\n\nGoogle Cloud Datastore, 181f, 182 Google Web Server\n\nAndon cord, 138 automated testing, 123–126\n\nC++, 123\n\ncase study, 123–126 Fixit Grouplet, 307\n\nTesting Grouplet, 124–126, 306–307\n\nGovindarajan, Vijay, 66 Grafana, 204, 224\n\nGraphite, 196, 204, 224\n\nGray, Jim, 184 greenfield services. See services, greenfield\n\ngrowth mindset, 87\n\nGruver, Gary\n\nautomated testing, 123, 136 continuous integration, 144–146\n\nineffective quality controls, 34\n\nGupta, Prachi, 207–208\n\nGWS. See Google Web Server\n\nH Hammant, Paul, 186n\n\nHammond, Paul, 5, 354\n\nHand-Off Readiness Review, 237–239, 239f\n\nhandoffs\n\ndangers of, 358–359\n\nloss of knowledge, 21\n\nreducing batch size, 21\n\nworkflow management, 21\n\nhardening phase, 53n\n\nHendrickson, Elisabeth, 30, 135, 260–261\n\nheroics, 10, 25, 25n high-trust culture, 37–38\n\nHipHop compiler, 302\n\nHodge, Victoria J., 215–216\n\nHollnagel, Erik, 359–360\n\nHP, 144–146\n\nHRR. See Hand-Off Readiness Review Humble, Jez\n\nautomated testing, 126–127\n\ncontinuous delivery, 175–176\n\nContinuous Delivery Movement, 5–6\n\ncontinuous integration, 126n dangers of change approval processes, 252\n\nevolutionary architecture, 179\n\ninfrastructure as code, 6n\n\nuser research, 244\n\nhypothesis-driven development, 241–248\n\nI ICHT, 339–341\n\nImbriaco, Mark, 288\n\nimposter syndrome, 124n improvement blitz\n\norganizational learning and improvement, 299\n\nSpear, Steven, 299\n\nToyota Production System, 299\n\nimprovement goal examples, 68\n\nimprovement kata, 6, 355 industrial safety, 359–360",
      "page_number": 576
    },
    {
      "number": 69,
      "title": "Segment 69 (pages 584-591)",
      "start_page": 584,
      "end_page": 591,
      "detection_method": "topic_boundary",
      "content": "information radiator, 206–208\n\ninformation security\n\n18F team, 325–326\n\napplication security, 318–323\n\nautomated security testing, 318f\n\nbad paths, 318 Brakeman, 322, 322f\n\nbuild images, 317\n\nCloud.gov, 325–326\n\ncode signing, 319–320\n\ncreating security telemetry, 327–330 data breaches, 323–324\n\nand defect tracking and post-mortems, 315\n\ndependency scanning, 319\n\nand the deployment pipeline, 317–318, 330–331\n\ndynamic analysis, 319\n\nenvironment security, 324–326 Etsy case study, 328–330\n\nFederal Government case study, 325–326\n\nGauntlt, 313, 317\n\nGraphite, 329f\n\nhappy paths, 318\n\nintegrating into production telemetry, 326–327 inviting InfoSec to product demonstrations, 314\n\nJava, 324\n\nMetasploit, 325\n\nNmap, 325\n\npreventive security controls, 315–317 Ruby on Rails, 322\n\nrugged DevOps, 313\n\nsad paths, 318\n\nsecurity libraries, 316\n\nshared code repositories and services, 315–317\n\nsoftware supply chain security, 323–324 source code integrity, 319–320\n\nSQL injection attempts, 329, 329f\n\nstatic analysis, 319, 320–323\n\nTwitter case study, 320–323\n\nvalue stream, 63\n\nInfosec. See information security\n\ninfrastructure as code, 6n\n\ninfrastructure metrics, 213n\n\nInGraphs, 208\n\nintegrated development environment, 128n\n\nintegration, 120n Intuit, 241–248\n\niteration length, 68\n\nITIL, 116n, 231n, 253, 333, 334n\n\nITIL CMDB, 212, 212n\n\nJ Jacob, Adam, 6n\n\nJacques, Olivier, 297\n\nJava\n\nautomation, 127 Bazaarvoice, 149\n\ndependency scanning, 319\n\neBay, 179n\n\nGoogle, 296n\n\ninformation security, 324 LinkedIn, 71\n\nlogging infrastructure, 201n\n\nORM, 79n\n\nproduction metrics, 204\n\nthreading libraries, 292\n\nJavaScript\n\nFacebook, 175 Google, 296n\n\nproduction telemetry, 209\n\nJones, Daniel T., 19\n\njust culture, 38\n\nK Kalantzis, Christos, 281–282\n\nkanban boards\n\nand the Lean Movement, 4\n\nsharing between Ops and Dev, 104 workflow management, 16–17, 16f\n\nKanies, Luke, 6n\n\nKastner, Erik, 163–164\n\nKim, Gene, 195, 203n, 233, 252, 313\n\nKissler, Courtney, 51–54, 61–62\n\nKnight Capital, 251–252 knowledge sharing, 42–43\n\nKohavi, Ronny, 244\n\nKrishnan, Kripa, 284\n\nL LAMP stack\n\nDevOps myths, xvi at Etsy, 196\n\nLauderbach, John, 88n\n\nLaunch Readiness Review, 238–239\n\nlead time\n\nchange review, 258f\n\ndeployment, 8–11, 9f, 165 and the Lean Movement, 353\n\nreduction, 156\n\nLean Manufacturing\n\ndeployment lead time, 8–11\n\nfunctional-oriented organizations, 84 manufacturing value stream, 7–8\n\ntechnology value stream, 8–11\n\nThe Three Ways, 11–12\n\nLean Movement, 4, 353\n\nLean principles, xxii\n\nLean Product Development, 9 Lean Startup Movement, 355\n\nLean UX Movement, 355\n\nlearning culture\n\nAmazon AWS case study, 271–273\n\namplifying weak failure signals, 279–280 bad apple theory, 273\n\nblameless post-mortems, 274–276\n\nChaos Monkey, 272–273, 281–282\n\nGame Days, 282–284\n\ninjecting faults into production environment, 281–282, 282n\n\njust culture, 273–274 leaders and, 44–46\n\nMorgue, 277–278\n\nNetflix case study, 271–273\n\npublicizing post-mortems, 277–278\n\nredefining failure, 280–281\n\nrehearsing failures, 282–284\n\nresilience, 281–282\n\nresilience engineering, 282–283\n\nThe Third Way, 44–46\n\nand Toyota Kata, 45\n\nand Toyota Production System, 45\n\nLesiecki, Nick, 306–307\n\nLetuchy, Eugene, 174–175\n\nLightbody, Patrick, 231\n\nLimoncelli, Tom, 218, 238–239, 292, 296–297\n\nLinkedIn\n\ncase study, 71–73 Java, 71\n\nOperation Inversion, 72\n\nOracle, 71\n\nself-service metrics, 207–208\n\nLittle, Christopher, xxvii, 73 logging, 201–203, 201n\n\nlogging infrastructure\n\nJava, 201n\n\nRuby on Rails, 201n\n\nLoura, Ralph, 297\n\nLove, Paul, 313 low-risk releases\n\napplication-based release patterns, 165–166, 171–175\n\nautomated self-service deployments, 159–160\n\nautomating manual steps, 155–156\n\nblue-green deployment pattern, 166–169, 166f\n\ncanary release pattern, 169–171, 170f canary tests, 153\n\ncluster immune system release pattern, 169, 170–171\n\ncode deployment as part of deployment pipeline, 160–162\n\ncode deployment process changes, 154\n\ncode promotion processes, 160 continuous delivery, 175–177\n\ncontinuous deployment, 175–177\n\nCSG International case study, 157–159\n\ndark launches, 173–175\n\ndatabase changes, 167–168, 168n\n\ndecoupling deployment from releases, 164–175 deployment consistency, 156\n\ndeployment defined, 164\n\ndeployment flow, 156\n\ndeployment lead time, 165\n\ndeployment on demand, 165\n\ndeployment pace, 154 deployment pipeline requirements, 156–157\n\ndeployment process automation, 155–164\n\ndeployment tool, 163–164, 163f\n\nDixons Retail case study, 168–169\n\nenvironment consistency, 157, 158 environment-based release patterns, 165, 166–171\n\nEtsy case study, 162–164\n\nevolutionary architecture, 179–189\n\nFacebook case study, 174–175\n\nfast deployments, 161, 161f\n\nfeature toggles, 171–173, 175 lead time reduction, 156\n\nMTTR, 158, 159f, 161f\n\nperformance degradation, 172\n\npoint-of-sale systems, 168–169\n\nprocess documentation, 155 production incident decrease, 158, 159f\n\nrelease frequency, 153, 154f\n\nrelease risk, 165\n\nreleases defined, 164–165\n\nresilience, 172\n\nroll back, 172 self-service developer deployment, 162–164\n\nShared Operations team, 157\n\nsmoke testing, 156, 163\n\nLRR. See Launch Readiness Review\n\nM Macri, Bethany, 363\n\nMacys.com, 136\n\nmaking work visible, 15–17, 73, 104\n\nMalpass, Ian, 197, 276 Mangot, Dave, 337–338\n\nmanufacturing lead time, 4\n\nmanufacturing value stream\n\ndefined, 7\n\ndescription of, 7–8 feedback issues, 29\n\nintegrating learning, 37\n\nlow-trust environment, 37\n\nworkflow, 7–8\n\nMarsh, Dianne, 98\n\nMartin, Karen, 7 Massie, Bill, 339, 341\n\nMathew, Reena, 337–338\n\nMaximilien, E. Michael, 135n\n\nMcDonnell, Patrick, 196\n\nMcKinley, Dan, 298\n\nMediratta, Bharat, 124, 306–307\n\nMesseri, Eran, 125, 291 metadata, 115\n\nMetasploit, 325\n\nmetrics\n\nactionable business, 211f\n\napplications and business, 210–212\n\ninfrastructure, 212–213 libraries, 204–205\n\nproduction, 204–206\n\nself-service, 207–208\n\nsources, 209\n\nfor telemetry improvement, 65–66\n\nMetz, Cade, 302\n\nMickman, Heather, 91–93, 263, 305\n\nMicrosoft, 118\n\nMicrosoft Operations Framework, 195\n\nMilstein, Dan, 276\n\nminimum viable product, 355 MOF. See Microsoft Operations Framework\n\nMoore, Geoffrey A., 57\n\nMorgue, 277–278\n\nMTTR\n\ndeployment process automation, 158, 159f\n\nrecording, 277 telemetry, 197, 197f\n\nMueller, Ernst, 97n, 149–151, 207\n\nMulkey, Jody, 84–85, 212\n\nMySQL\n\nDevOps myths, xvi at Etsy, 196, 297–298",
      "page_number": 584
    },
    {
      "number": 70,
      "title": "Segment 70 (pages 592-599)",
      "start_page": 592,
      "end_page": 599,
      "detection_method": "topic_boundary",
      "content": "N Nagappan, Nachi, 135n\n\nNagios, 198 NASA, 279–280\n\nNational Instruments, 54–55\n\nNationwide Insurance, 304–306\n\nNetflix\n\nauto-scaling capacity, 221–222\n\nmarket-oriented organization, 81 Netflix AWS, 118n\n\norganizational values, 90n\n\nredefining failure, 280–281\n\nresilience, 271–273, 281–282\n\nself-service platforms, 98 telemetry analysis, 215–216\n\nNewland, Jesse, 287–289\n\nNike, 159\n\nNmap, 325\n\nnon-functional requirements, 294\n\nNordstrom, 51–55, 61–62 North, Dan, 168–169, 202\n\nNR Program, 42–43\n\nNygard, Michael, 281\n\nO Obidos, 184\n\nObject Relational Mapping layer, 79, 79n\n\nO’Donnell, Glenn, 304\n\nOhno, Taiichi, 18n\n\nO’Neill, Paul, 41, 279 Open Web Application Security Project, 319, 319n, 324n\n\nOperation Desert Shield, 165n\n\nOperation Inversion, 72 Operations. See entries under Ops\n\nOps and market-oriented outcomes\n\ncreating shared services, 97–99\n\nengineers embedded into service teams, 99–100\n\nfeedback, 103\n\nintegration into Dev rituals, 101–104 internal shared services teams, 97–99\n\nliaisons assigned to service teams, 100–101\n\nmaking work visible, 104\n\nOps liaisons, 96\n\nparticipation in Dev retrospectives, 102–103 participation in Dev standups, 102\n\nself-service platforms, 97–98\n\nsilos, 102\n\ntool standardization, 98–99\n\noptimizing for downstream work centers\n\ncustomer types, 34 Designing for Manufacturing principles, 34\n\nOracle\n\napplication configuration settings, 368\n\nCOTS software, 361\n\neBay, 179n ERP code migration, 117n\n\nLinkedIn, 71\n\norganizational cultures, 39, 39f\n\norganizational knowledge\n\nautomated tests as documentation, 293\n\nautomating standardized processes, 289–290 ChatOps, 287–289\n\ncode re-use, 289–290\n\ncodified non-functional requirements, 294\n\ncommunities of practice, 293\n\nEtsy case study, 297–298\n\nHubot, 287–289 integrating automation into chat rooms, 287–289\n\nresuable Ops user stories, 295\n\nsource code repository, 290–292\n\ntechnology choices to achieve organizational goals, 295–298\n\ntechnology stack standardization, 297–298 test-driven development, 293\n\nusing chat rooms and chat bots, 287–289\n\norganizational learning and improvement\n\nattending external conferences, 304–306\n\nblitz goals, 301\n\nCapitol One case study, 304–306 enable learning and teaching, 303–304\n\nimprovement blitz, 299\n\ninternal consulting and coaching, 306–307\n\nkaizen blitz, 299\n\nNationwide Insurance case study, 304–306 paying down technical debt, 300–303\n\nTarget case study, 299–300, 304–306\n\nThe Third Way, 38–40\n\norganizations\n\nfunctional-oriented, 80, 84\n\nmarket-oriented, 80–81 matrix-oriented, 80\n\nORM. See Object Relational Mapping layer\n\nOsterling, Mike, 7\n\nÖzil, Giray, 256\n\nP pair programming\n\nand code reviews, 256\n\ndescription of, 259–263\n\npair programmed rollback, 139n\n\npairing hours, 260n Pivotal Labs case study, 260–261\n\nPal, Tapabrata, 263, 305, 313\n\nPandey, Ravi, 300\n\nParoski, Drew, 302\n\npathological organizations, 39 PayPal, 58n\n\nPerl\n\nBlackboard Learn, 187\n\neBay, 179n\n\nproduction metrics, 204\n\nPerrow, Charles, 28 PHP\n\nConway’s Law, 78\n\nDevOps myths, xvi\n\nEtsy, 196, 297, 297n\n\nFacebook, 153n, 154, 175, 302\n\nORM, 79 production telemetry, 205, 230, 230f\n\nPivotal Labs, 260–261\n\npoint-of-sale systems, 168–169\n\nPoppendieck, Mary, 24\n\nPoppendieck, Tom, 24 post-mortems. See blameless post-mortems\n\nproblem visibility, 29–30\n\nproblems\n\nfact-based problem solving, 203–204\n\nintegration, 143\n\nleaders and problem solving, 44\n\nprevention of, 30 problem visibility, 29–30\n\nswarming of smaller, 32\n\nproduction metrics\n\nJava, 204\n\nPerl, 204\n\nPython, 204 Ruby on Rails, 204\n\nproduction monitoring. See telemetry\n\nproduction telemetry. See also telemetry; telemetry analysis\n\ncontextual inquiry, 232–233\n\nfeature toggles, 229–230\n\nfeedback mechanisms, 229 fix forward, 230, 230f\n\nfunction-oriented teams, 231–232\n\nGoogle case study, 237–239\n\nHand-Off Readiness Review, 238–239\n\nimproving flow, 232n JavaScript, 209\n\nlaunch and hand-off readiness reviews, 237–239, 239f\n\nlaunch guidance, 234–235\n\nLaunch Readiness Review, 238–239\n\nmaking deployments safer, 229–230\n\nmarket-oriented teams, 231 pager rotation duties, 230–232\n\nPHP, 205, 230, 230f\n\nproduction service, 234–239\n\nregulatory compliance objectives, 235–236\n\nroll back, 230 service handback mechanism, 236f, 237\n\nsite reliability engineers, 237–239\n\nUX observation, 233, 233n\n\nPrugh, Scott\n\nand bimodal IT, 57\n\ncross-training, 86–87 daily deployments, 157–158\n\ntelemetry, 201\n\nPuppet Labs, 159–160\n\nPython\n\nEtsy, 297, 297n\n\nGoogle, 296 ORM, 79n\n\nproduction metrics, 204\n\nQ quality controls, 32–34\n\nqueue time, 358–359, 358f\n\nqueues\n\nlong, 81\n\nqueue time, 358–359, 358f shared work, 73–74\n\nsize, 18, 18n\n\nR Rachitsky, Lenny, 365\n\nRajan, Karthik, 337\n\nRapoport, Roy, 215–216, 280–281\n\nRational Unified Process, 5, 354 Raymond, Eric A., 77\n\nRed Hat, 114\n\nReddy, Tarun, 222–223\n\nreinforcing desired behavior, 73–74\n\nrelease patterns\n\napplication-based, 165–166, 171–175\n\ncanary, 169–171, 170f\n\ncluster immune system, 169, 170–171\n\nenvironment-based, 165, 166–171\n\nreleases, 164–165\n\nRembetsy, Michael, 51, 196, 298 repositories\n\ncode, 187f, 188f, 290–292, 315–317\n\nversion control, 115–118\n\nresilience, 43–44, 172, 271–273, 281–282\n\nrework, 11 Richardson, Vernon, xxviiin\n\nRies, Eric, 20, 171n, 355\n\nRight Media, 227–229\n\nRobbins, Jesse, 283, 354\n\nRoberto, Michael, 279\n\nroll back, 139n, 230 Rossi, Chuck, 153–155, 174n\n\nRother, Mike\n\ncoaching kata, 45\n\nfunctional-oriented organizations, 84\n\nimprovement kata, 40\n\nToyota Kata movement, 6\n\nRuby on Rails\n\nautomation, 127\n\nblue-green deployment, 167n\n\ndependency scanning, 319\n\ninformation security, 322 logging infrastructure, 201n\n\nORM, 79\n\nproduction metrics, 204\n\nRugged Computing Movement, 355–356\n\nS Sachs, Marcus, 326\n\nsafety culture, 28, 38–40\n\nsafety in complex systems, 27–29\n\nsafety in the workplace, 41–42 Salesforce.com, 337–338\n\nSchafer, Andrew, 5, 354\n\nSchwaber, Ken, 102n\n\nScott, Kevin, 72–73\n\nScrum methodology, 102n, 119–120\n\nScryer, 221–222 second stories, 360f\n\nself-service platforms, 97–98, 206–208\n\nSenge, Peter\n\nlearning organizations, 40\n\nproblem visibility, 29\n\nservice handback mechanism, 236f, 237, 237n\n\nservices, brownfield\n\nCSG International, 56\n\ndefined, 55\n\nDevOps transformation, 54–56\n\nEtsy, 56 improving speed and quality, 57\n\nwith largest potential business benefit, 55n\n\ntransformations of, 55–56",
      "page_number": 592
    },
    {
      "number": 71,
      "title": "Segment 71 (pages 600-607)",
      "start_page": 600,
      "end_page": 607,
      "detection_method": "topic_boundary",
      "content": "value streams, 54–56\n\nservices, greenfield\n\nconsequences of new features, 57\n\ndefined, 54 types of projects, 54–56\n\nservices, immutable, 185\n\nservices, shared, 97–99\n\nservices, versioned, 185\n\nShingo, Shigeo, 23\n\nShinn, Bill, 342–344 Shoup, Randy\n\ndeployment pipeline breakdowns, 139\n\nevolutionary architecture, 179, 182\n\nloosely-coupled architecture, 90\n\npeer reviews of code changes, 255–256 publicizing post-mortems, 277\n\nsource code repository, 291–292\n\nsilos\n\nOps and market-oriented outcomes, 102\n\nteam organization, 85\n\nSimian Army, 364–365 single-piece flow, 19, 20\n\nsmoke testing, 156, 163\n\nsmoothing, 223, 223f, 223n\n\nSOAs. See architecture, service-oriented\n\nSouders, Steve, 354 Spafford, George, 195, 203n, 313\n\nSpear, Steven\n\nconditions for safety, 28\n\nimprovement blitz, 299\n\nIT failures, xxvii\n\norganizational learning, 40 paying down technical debt, 302\n\nresilience, 271\n\nworkplace safety, 41–42\n\nsprint planning boards, 16–17\n\nsprints, 119–120\n\nSprouter, 78–80, 88 stabilization phase, 53n\n\nstack engineers, 86\n\nStatsD, 204–205\n\nStillman, Jessica, 302\n\nStoneham, Him, 246–248 strangler application pattern, 180, 185–189, 186n\n\nSussman, Noah, 162–163\n\nSussna, Jeff, 233n\n\nswarming\n\nAndon cord, 31, 32\n\nand common management practice, 31 goal of, 30\n\nreasons for, 31\n\nof smaller problems, 32\n\nsystems of engagement\n\ndefined, 56–57\n\nand related brownfield systems of record, 57\n\nsystems of record, 56\n\nT Tableau, 223 Target\n\nAPI enablement, 91–93\n\ncase study, 91–93\n\ncutting bureaucratic processes, 263–264\n\nDevOps Dojo, 299–300\n\nfirst DevOpsDays, 305n\n\ninternal technology conferences, 304–306\n\nTDD. See development, test-driven\n\nteam organization\n\nAPI enablement at Target, 91–93\n\nbounded contexts, 89\n\nbusiness logic changes, 78, 79 business relationship manager, 96\n\ncollaboration, 88\n\nConway’s Law, 77–78, 88\n\ncross-functional and independent teams, 82\n\ncross-training, 85–87\n\ndatabase stored procedures changes, 78 decreasing handoffs, 79\n\ndedicated release engineer, 96\n\ndeployment issues, 78\n\ndeployment speed and success, 79\n\nembedding needed skills, 82–83\n\nfixed mindset, 87 functional-oriented organizations, 80\n\nfunding services and products, 87–88\n\ngrowth mindset, 87\n\nintegrating Ops into Dev teams, 95–105\n\ninternal shared services teams, 97–99 long queues, 81\n\nloosely-coupled architecture, 89–93\n\nmaking functional orientation work, 83–84, 83f\n\nmarket orientations, 80–81, 82–83\n\nmatrix-orientations, 80\n\noptimizing for cost, 81–82 optimizing for speed, 82–83\n\nquality as shared goal, 84–85\n\nservice interactions via APIs, 89\n\nservice-oriented architecture, 89\n\nshared pain, 85 silos, 85\n\nspecialists vs. generalists, 85–87, 86f\n\nstack engineers, 86\n\nsynchronization, 78\n\nteam boundaries, 88\n\nteam size, 90–91 testing and operations, 89\n\ntwo-pizza team, 90–91\n\nteam size, 90–91\n\nteams\n\n18F team, 325–326\n\ndevelopment, 8 function-oriented, 231–232\n\nmarket-oriented, 231\n\nservice, 83n, 97–101\n\nshared operations, 157, 158n\n\nteam formation, 67 team size, 90–91\n\ntechnology value stream, 8\n\ntesting, 124–126\n\ntransformation, 66–74\n\ntwo-pizza team, 90–91\n\ntechnical debt\n\ndescription of, 148\n\nmanaging, 69–71\n\npaying down, 144, 300–303\n\nreducing, 69–71, 70f\n\ntechnology adoption curve, 58f\n\ntechnology value stream\n\nabsence of fast feedback, 29–30\n\ncreating a high-trust culture, 37–38\n\ndefined, 8\n\ndeployment lead time, 8–11\n\ninputs, 8 integrating learning, 37–38\n\nresponses to incidents and accidents, 38–39\n\ntelemetry. See also production telemetry; telemetry analysis\n\nactionable business metrics, 211f\n\napplication logging, 201–203\n\napplications and business metrics, 210–212 centralized infrastructure, 198–200\n\nculture of causality, 195\n\ncustomer acquisition funnel, 210\n\ndata collection, 199\n\nDEBUG level, 201 defined, 196\n\nERROR level, 202\n\nevent router, 199\n\nfact-based problem solving, 203–204\n\nFATAL level, 202\n\ngraphs and dashboards, 204–205 identify gaps, 209–213\n\nincident resolution time, 197f\n\nINFO level, 202\n\ninformation radiator, 206–208\n\ninformation security in product telemetry, 326–327\n\ninfrastructure metrics, 212–213 ITIL CMDB, 212\n\nLinkedIn case study, 207–208\n\nlog centralization, 199\n\nlogging entry generation, 202–203\n\nlogging levels, 201–202 making deployments safer, 229–230\n\nmetrics for improvement, 65–66\n\nmetrics libraries, 204–205\n\nmetrics library, 204–206\n\nmetrics sources, 209\n\nmonitoring architecture, 198–199 monitoring framework, 200f\n\nMTTR, 197, 197f\n\noverlay of production deployment activities, 213\n\nproduction metrics, 204–206\n\nand the Second Way, 30 security telemetry, 327–330\n\nself-service metrics, 207–208\n\nself-service platforms, 206–208\n\nStatsD, 204–205\n\ntools, 205n\n\nWARN level, 202\n\ntelemetry analysis. See also production telemetry; telemetry\n\n3 standard deviation rule, 219, 219f, 225, 225f\n\nalerts for undesired outcomes, 218\n\nanalysis tools, 224n\n\nanomaly detection techniques, 222–226\n\nautomated, 222f\n\nauto-scaling capacity, 221–222, 221f case study, 215–216\n\nfiltering techniques, 224\n\nGaussian distribution, 217, 217f Kolmogorov-Smirnov test, 224, 225, 226f\n\nmeans, 216–217 Netflix case study, 221–222\n\nnon-Gaussian distribution, 219–222, 220f non-parametric techniques, 225 outlier detection, 215–216\n\nprecursors to production incidents, 218 Server Outlier Detection, 216\n\nsmoothing, 223, 223f, 223n standard deviations, 216–217, 217f statistical techniques, 216–217, 223\n\ntest environments, 113n testing\n\nautomated, 123–127, 125n, 130, 134–135, 136, 293 automated validation test suite, 132, 133–134, 133f, 134f, 136– 138\n\ndestructive testing, 338 fast testing, 132, 133–134\n\nideal vs non-ideal testing, 133f manual testing, 258–259\n\nnon-functional requirements testing, 137–138 performance testing, 136–137 smoke testing, 156, 163\n\ntesting environments, 113 testing in parallel, 133–134, 134f\n\ntesting, A/B\n\ncase study, 246–248 Feature API, 245\n\nhistory of, 243\n\nintegrating into feature planning, 245–248\n\nintegrating into feature testing, 244–245 integrating into release, 245 outcomes of, 244\n\nuser research, 243n, 244–245 Yahoo! Answers, 246–248\n\ntesting, automated\n\nacceptance test-driven development, 134–135\n\nacceptance tests, 131, 132, 139 analysis tools, 138 automated build and test processes, 127\n\nautomated test suites, 126 automated validation test suite, 129–138\n\nautomating manual tests, 135–136 change deployment, 124 code configuration management tools, 138\n\ncode packaging, 128 deployment pipeline, 127–129\n\nenvironment validation, 137–138 error detection, 132–133 failure indicators, 139\n\nfast testing, 132, 133–134 feedback, 126, 130\n\ngreen builds, 129–130 handling input from external integration points, 131n\n\nideal vs non-ideal testing, 133f integration tests, 131, 132 non-functional requirements testing, 137–138\n\nperformance testing, 136–137 production increases, 124\n\ntest types, 130–131",
      "page_number": 600
    },
    {
      "number": 72,
      "title": "Segment 72 (pages 608-615)",
      "start_page": 608,
      "end_page": 615,
      "detection_method": "topic_boundary",
      "content": "test-driven development, 134–135 testing in parallel, 133–134, 134f\n\ntesting teams, 124–126 and trunk-based development, 145–146\n\nunit tests, 130–131, 132–133, 139 unreliable test, 135 version control, 128\n\nThe First Way\n\nbatch size comparison, 20f\n\nbottlenecks, 22 constraint identification, 21–23\n\ncontinuous, 20 controlling queue size, 18 description of, 11\n\nerror management, 19 handoff reduction, 21\n\nincreasing workflow, 15 kanban boards, 16–17 large batch sizes, 19–20\n\nlimiting work in process, 17–18 loss of knowledge, 21\n\nmaking work visible, 15–17 multitasking, 17–18 reducing batch size, 18–20\n\nsingle piece flow, 19, 20 small batch sizes, 18–20\n\nsmall batch strategy, 19 sprint planning boards, 16–17 transferring work, 15–16\n\nwaste elimination, 23–25 work interruptions, 17\n\nworkflow management, 16, 21 workflow visualizations, 16–17\n\nThe Second Way\n\nAndon cord, 31, 32 conditions for safety, 28–29\n\ndescription of, 12 error detection, 28\n\nfailure, 28 feedback loops, 29, 30 feed-forward loops, 29, 30\n\nincreasing information flow, 29 ineffective quality controls, 32–34\n\noptimizing for downstream work centers, 34–35 peer reviews, 33–34 problem visibility, 29–30\n\nQA automation, 33–34 safety in complex systems, 27–29\n\nswarming, 30–32 telemetry, 30\n\nThe Third Way\n\nblameless post-mortems, 40 collective knowledge, 42–43\n\ncreating a high-trust culture, 37–38 description of, 12–13\n\nimprovement of daily work, 40–42 integrating learning, 37–38 knowledge sharing, 42–43\n\nleadership, 44 learning culture, 44–46\n\norganizational cultures, 39, 39f organizational learning, 38–40\n\nresilience, 43–44\n\nsafety culture, 38–40 workarounds, 40\n\nworkplace safety, 41–42\n\nThe Three Ways\n\nThe First Way, 11 illustrated, 12f increasing workflow, 12\n\nThe Phoenix Project, 11 The Second Way, 12\n\nThe Third Way, 12–13\n\nTheory of Constraints, 356–357 Three Mile Island, 28\n\nthreshold-based alerting tools, 199n Ticketmaster, 84–85\n\nTimberland, 67 Tischler, Tim, 159 Tomayko, Ryan, 261–262\n\nTotal Productive Maintenance, 4, 353 Toyota Kata\n\ndescription of, 6 functional-oriented organizations, 84\n\nand the improvement of daily work, 40 and learning culture, 45 movement, 355\n\nToyota Production System\n\nAndon cord, 138\n\nchange approval processes, 253 improvement blitz, 299 information radiator, 206\n\nand the Lean Movement, 4, 353\n\nand learning culture, 45\n\nand safe systems, 28 Toyota Kata movement, 6\n\ntransparent uptime, 365\n\nTreynor, Ben, 237–238 Trimble, Chris, 66\n\nTurnbull, James, 198 Twitter, 320–323 two-pizza team, 90–91\n\nU US Navy, 42 user research, 244–245 user stories, 8, 295, 335, 336\n\nV value stream\n\nDevelopment, 63\n\nInfosec, 63 manufacturing, 7–8 Operations, 63\n\nproduct owner, 63 release managers, 63\n\nsupporting members, 63 technology, 8–11 technology executives, 63\n\ntest, 63 value stream manager, 63\n\nvalue stream mapping, 4, 61–62, 353\n\nvalue stream mapping\n\n%C/A, 65\n\nareas of focus, 64 creating, 63–66\n\nexample of, 65f first pass, 65\n\nfuture value stream map, 66 and the Lean Movement, 4, 353 metrics for improvement, 65–66\n\nvalue stream improvements, 61–62\n\nvalue streams\n\nexpanding DevOps, 58–59 greenfield vs brownfield services, 54–56 leveraging innovators, 57–58\n\nselecting a stream for DevOps transformation, 51–60 systems of engagement, 56–57\n\nsystems of record, 56 technology adoption curve, 58f\n\nVan Leeuwen, Evelijn, 305n Vance, Ashlee, 72 Velocity Movement, 354\n\nversion control\n\nassets to check into version control repository, 116\n\nautomated testing, 128 branching, 143n and continuous integration, 148–149\n\ncritical role of version control, 117 environments stored in version control, 115–116\n\nmetadata, 115 shared version control repository, 115–118 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nVincent, John, 216\n\nVogels, Werner, 91, 184\n\nW Wall Street Journal, 66 waste and hardship\n\ndefects, 25 extra features, 24 extra processes, 24\n\nheroics, 25, 25n motion, 24–25\n\nnonstandard or manual work, 25 partially done work, 24 task switching, 24\n\nwaiting, 24 waste elimination, 23–25\n\nwater-Scrum-fall anti-pattern, 140n Westrum, Ron, 39–40\n\nWickett, James, 313 Williams, Laurie, 135n, 260 Willis, John\n\nAgile infrastructure, 5 convergence of DevOps, 3\n\nWIP. See work in process Wolaberg, Kirsten, 58n Womack, James P.\n\nbatch sizes, 19 leaders and problem solving, 44\n\nWong, Eric, 208 work in process\n\ncontrolling queue size, 18\n\ninterruptions, 17\n\nmultitasking, 17–18\n\nwork visibility, 15–17, 73, 104 workflow\n\nincreasing, 12, 15 management, 16 visualizations, 16–17\n\nY Yahoo! Answers, 246–248\n\nZ Zenoss, 198, 208 Zhao, Haiping, 302\n\nZuckerberg, Mark, 302\n\nAcknowledgments\n\nJez Humble\n\nCreating this book has been a labor of love for Gene in particular.\n\nIt’s an immense privilege and pleasure to have worked with Gene\n\nand my other co-authors, John and Pat, along with Todd, Anna,\n\nRobyn and the editorial and production team at IT Revolution\n\npreparing this work—thank you. I also want to thank Nicole\n\nForsgren whose work with Gene, Alanna Brown, Nigel Kersten and I on the PuppetLabs/DORA State of DevOps Report over the\n\nlast three years has been instrumental in developing, testing and refining many of the ideas in this book. My wife, Rani, and my two\n\ndaughters, Amrita and Reshmi, have given me boundless love and\n\nsupport during my work on this book, as in every part of my life. Thank you. I love you. Finally, I feel incredibly lucky to be part of\n\nthe DevOps community, which almost without exception walks the talk of practicing empathy and growing a culture of respect and\n\nlearning. Thanks to each and every one of you.\n\nJohn Willis\n\nFirst and foremost, I need to acknowledge my saint of a wife for putting up with my crazy career. It would take another book to\n\nexpress how much I learned from my co-authors Patrick, Gene and Jez. Other very important influencers and advisers in my journey are Mark Hinkle, Mark Burgess, Andrew Clay Shafer, and",
      "page_number": 608
    },
    {
      "number": 73,
      "title": "Segment 73 (pages 616-623)",
      "start_page": 616,
      "end_page": 623,
      "detection_method": "topic_boundary",
      "content": "Michael Cote. I also want to give a shout out to Adam Jacob for\n\nhiring me at Chef and giving me the freedom to explore, in the\n\nearly days, this thing we call Devops. Last but definitely not least\n\nis my partner in crime, my Devops Cafe cohost, Damon Edwards.\n\nPatrick Debois\n\nI would like to thank those who were on this ride, much gratitude\n\nto you all.\n\nGene Kim\n\nI cannot thank Margueritte, my loving wife of nearly eleven\n\namazing years, enough for putting up with me being in deadline mode for over five years, as well as my sons, Reid, Parker, and\n\nGrant. And of course, my parents, Ben and Gail Kim, for helping me become a nerd early in life. I also want to thank my fellow co- authors for everything that I learned from them, as well as Anna Noak, Aly Hoffman, Robyn Crummer-Olsen, Todd Sattersten, and the rest of the IT Revolution team for shepherding this book to its\n\ncompletion.\n\nI am so grateful for all the people who taught me so many things, which form the foundation of this book: John Allspaw (Etsy),\n\nAlanna Brown (Puppet), Adrian Cockcroft (Battery Ventures), Justin Collins (Brakeman Pro), Josh Corman (Atlantic Council), Jason Cox (The Walt Disney Company), Dominica DeGrandis (LeanKit), Damon Edwards (DTO Solutions), Dr. Nicole Forsgren (Chef), Gary Gruver, Sam Guckenheimer (Microsoft), Elisabeth\n\nHendrickson (Pivotal Software), Nick Galbreath (Signal Sciences), Tom Limoncelli (Stack Exchange), Chris Little, Ryan Martens, Ernest Mueller (AlienVault), Mike Orzen, Scott Prugh (CSG\n\nInternational), Roy Rapoport (Netflix), Tarun Reddy (CA/Rally),\n\nJesse Robbins (Orion Labs), Ben Rockwood (Chef), Andrew Shafer\n\n(Pivotal), Randy Shoup (Stitch Fix), James Turnbull (Kickstarter),\n\nand James Wickett (Signal Sciences).\n\nI also want to thank the many people whose incredible DevOps\n\njourneys we studied, including Justin Arbuckle, David Ashman,\n\nCharlie Betz, Mike Bland, Dr. Toufic Boubez, Em Campbell-Pretty,\n\nJason Chan, Pete Cheslock, Ross Clanton, Jonathan Claudius,\n\nShawn Davenport, James DeLuccia, Rob England, John Esser,\n\nJames Fryman, Paul Farrall, Nathen Harvey, Mirco Hering, Adam\n\nJacob, Luke Kanies, Kaimar Karu, Nigel Kersten, Courtney Kissler, Bethany Macri, Simon Morris, Ian Malpass, Dianne Marsh, Norman Marks, Bill Massie, Neil Matatall, Michael\n\nNygard, Patrick McDonnell, Eran Messeri, Heather Mickman, Jody Mulkey, Paul Muller, Jesse Newland, Dan North, Dr. Tapabrata Pal, Michael Rembetsy, Mike Rother, Paul Stack, Gareth Rushgrove, Mark Schwartz, Nathan Shimek, Bill Shinn, JP Schneider, Dr. Steven Spear, Laurence Sweeney, Jim Stoneham, and Ryan Tomayko.\n\nAnd I am so profoundly grateful for the many reviewers who gave us fantastic feedback that shaped this book: Will Albenzi, JT\n\nArmstrong, Paul Auclair, Ed Bellis, Daniel Blander, Matt Brender, Alanna Brown, Branden Burton, Ross Clanton, Adrian Cockcroft, Jennifer Davis, Jessica DeVita, Stephen Feldman, Martin Fisher, Stephen Fishman, Jeff Gallimore, Becky Hartman, Matt Hatch, William Hertling, Rob Hirschfeld, Tim Hunter, Stein Inge Morisbak, Mark Klein, Alan Kraft, Bridget Kromhaut, Chris Leavory, Chris Leavoy, Jenny Madorsky, Dave Mangot, Chris\n\nMcDevitt, Chris McEniry, Mike McGarr, Thomas McGonagle, Sam\n\nMcLeod, Byron Miller, David Mortman, Chivas Nambiar, Charles Nelles, John Osborne, Matt O’Keefe, Manuel Pais, Gary Pedretti,\n\nDan Piessens, Brian Prince, Dennis Ravenelle, Pete Reid, Markos Rendell, Trevor Roberts, Jr., Frederick Scholl, Matthew\n\nSelheimer, David Severski, Samir Shah, Paul Stack, Scott\n\nStockton, Dave Tempero, Todd Varland, Jeremy Voorhis, and Branden Williams.\n\nAnd several people gave me an amazing glimpse of what the future\n\nof authoring with modern toolchains looks like, including Andrew Odewahn (O’Reilly Media) who let us use the fantastic Chimera\n\nreviewing platform, James Turnbull (Kickstarter) for his help creating my first publishing rendering toolchain, and Scott Chacon\n\n(GitHub) for his work on GitHub Flow for authors.\n\nGENE KIM\n\nJEZ HUMBLE\n\nPATRICK DEBOIS\n\nJOHN WILLIS\n\nAuthor Biographies\n\nGene Kim is a multiple award-winning CTO, researcher, and author of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win and The Visible Ops Handbook. He is founder of IT Revolution and hosts the DevOps Enteprise Summit conferences.\n\nJez Humble is co-author of Lean Enterprise and the Jolt Award-winning Continuous Delivery. He works at 18F, teaches at UC Berkeley, and is CTO and co- founder of DevOps Research and Assessment, LLC.\n\nPatrick Debois is an independent IT consultant who is bridging the gap between projects and operations by using Agile techniques, in development, project management, and system administration.\n\nJohn Willis has worked in the IT management industry for more than thirty-five years. He has authored six IBM Redbooks and was the founder and chief architect at Chain Bridge Systems. Currently he is an Evangelist at Docker, Inc.",
      "page_number": 616
    },
    {
      "number": 74,
      "title": "Segment 74 (pages 624-629)",
      "start_page": 624,
      "end_page": 629,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 624
    }
  ],
  "pages": [
    {
      "page_number": 3,
      "content": "IT Revolution Press, LLC 25 NW 23rd Pl, Suite 6314 Portland, OR 97210 Copyright © 2016 by Gene Kim, Jez Humble, Patrick Debois, and John Willis All rights reserved, for information about permission to reproduce selections from this book, write to Permissions, IT Revolution Press, LLC, 25 NW 23rd Pl, Suite 6314, Portland, OR 97210 First Edition Printed in the United States of America 10 9 8 7 6 5 4 3 2 1 Cover design by Strauber Design Studio Cover illustration by eboy Book design by Mammoth Collective Ebook design by Digital Bindery Print ISBN: 978-1942788003 Ebook–EPUB ISBN: 978-1-942788-07-2 Ebook–Kindle ISBN: 978-1-942788-08-9 Library of Congress Control Number: 2016951904 Publisher’s note to readers: Many of the ideas, quotations, and paraphrases attributed to different thinkers and industry leaders herein are excerpted from informal conversations, correspondence, interviews, conference roundtables, and other forms of oral communication that took place over the last six years during the development and writing of this book. Although the authors and publisher have made every effort to ensure that the information in this book was correct at press time, the authors and publisher do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause. The author of the 18F case study on page 325 has dedicated the work to the public domain by waiving all of his or her rights to the work worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law. You can copy, modify, distribute, and perform case study 18F, even for commercial purposes, all without asking permission. For information about special discounts for bulk purchases or for information on booking authors for an event, please visit ITRevolution.com. THE DEVOPS HANDBOOK",
      "content_length": 1942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Preface\n\nAha!\n\nThe journey to complete The DevOps Handbook has been a long\n\none—it started with weekly working Skype calls between the co-\n\nauthors in February of 2011, with the vision of creating a\n\nprescriptive guide that would serve as a companion to the as-yet unfinished book The Phoenix Project: A Novel About IT, DevOps,\n\nand Helping Your Business Win.\n\nMore than five years later, with over two thousand hours of work, The DevOps Handbook is finally here. Completing this book has\n\nbeen an extremely long process, although one that has been highly\n\nrewarding and full of incredible learning, with a scope that is much broader than we originally envisioned. Throughout the\n\nproject, all the co-authors shared a belief that DevOps is genuinely important, formed in a personal “aha” moment much earlier in\n\neach of our professional careers, which I suspect many of our readers will resonate with.\n\nGene Kim\n\nI’ve had the privilege of studying high-performing technology organizations since 1999, and one of the earliest findings was that boundary-spanning between the different functional groups of IT Operations, Information Security, and",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Development was critical to success. But I still remember the\n\nfirst time I saw the magnitude of the downward spiral that\n\nwould result when these functions worked toward opposing\n\ngoals.\n\nIt was 2006, and I had the opportunity to spend a week with\n\nthe group who managed the outsourced IT Operations of a\n\nlarge airline reservation service. They described the\n\ndownstream consequences of their large, annual software\n\nreleases: each release would cause immense chaos and\n\ndisruption for the outsourcer, as well as customers; there\n\nwould be SLA (service level agreement) penalties, because of the customer-impacting outages; there would be layoffs of the most talented and experienced staff, because of the resulting\n\nprofit shortfalls; there would be much unplanned work and firefighting so that the remaining staff couldn’t work on the ever-growing service request backlogs coming from customers; the contract would be held together by the heroics of middle management; and everyone felt that the contract would be\n\ndoomed to be put out for re-bid in three years.\n\nThe sense of hopelessness and futility that resulted created for me the beginnings of a moral crusade. Development seemed to\n\nalways be viewed as strategic, but IT Operations was viewed as tactical, often delegated away or outsourced entirely, only to return in five years in worse shape than it was first handed over.\n\nFor many years, many of us knew that there must be a better way. I remember seeing the talks coming out of the 2009\n\nVelocity Conference, describing amazing outcomes enabled by architecture, technical practices, and cultural norms that we",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "now know as DevOps. I was so excited, because it clearly\n\npointed to the better way that we had all been searching for.\n\nAnd helping spread that word was one of my personal\n\nmotivations to co-author The Phoenix Project. You can\n\nimagine how incredibly rewarding it was to see the broader\n\ncommunity react to that book, describing how it helped them\n\nachieve their own “aha” moments.\n\nJez Humble\n\nMy DevOps “aha” moment was at a start-up in 2000—my first\n\njob after graduating. For some time, I was one of two technical\n\nstaff. I did everything: networking, programming, support, systems administration. We deployed software to production by FTP directly from our workstations.\n\nThen in 2004 I got a job at ThoughtWorks, a consultancy where my first gig was working on a project involving about seventy people. I was on a team of eight engineers whose full- time job was to deploy our software into a production-like environment. In the beginning, it was really stressful. But over\n\na few months we went from manual deployments that took two weeks to an automated deployment that took one hour, where we could roll forward and back in milliseconds using the blue- green deployment pattern during normal business hours.\n\nThat project inspired a lot of the ideas in both the Continuous Delivery (Addison-Wesley, 2000) book and this one. A lot of what drives me and others working in this space is the knowledge that, whatever your constraints, we can always do\n\nbetter, and the desire to help people on their journey.",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Patrick Debois\n\nFor me, it was a collection of moments. In 2007 I was working\n\non a data center migration project with some Agile teams. I was jealous that they had such high productivity—able to get so\n\nmuch done in so little time.\n\nFor my next assignment, I started experimenting with Kanban in Operations and saw how the dynamic of the team changed.\n\nLater, at the Agile Toronto 2008 conference I presented my\n\nIEEE paper on this, but I felt it didn’t resonate widely in the Agile community. We started an Agile system administration\n\ngroup, but I overlooked the human side of things.\n\nAfter seeing the 2009 Velocity Conference presentation “10 Deploys per Day” by John Allspaw and Paul Hammond, I was\n\nconvinced others were thinking in a similar way. So I decided to organize the first DevOpsDays, accidently coining the term\n\nDevOps.\n\nThe energy at the event was unique and contagious. When\n\npeople started to thank me because it changed their life for the better, I understood the impact. I haven’t stopped promoting\n\nDevOps since.\n\nJohn Willis\n\nIn 2008, I had just sold a consulting business that focused on\n\nlarge-scale, legacy IT operations practices around configuration management and monitoring (Tivoli) when I\n\nfirst met Luke Kanies (the founder of Puppet Labs). Luke was giving a presentation on Puppet at an O’Reilly open source\n\nconference on configuration management (CM).",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "At first I was just hanging out at the back of the room killing time and thinking, “What could this twenty-year-old tell me\n\nabout configuration management?” After all, I had literally been working my entire life at some of the largest enterprises\n\nin the world, helping them architect CM and other operations management solutions. However, about five minutes into his\n\nsession, I moved up to the first row and realized everything I\n\nhad been doing for the last twenty years was wrong. Luke was describing what I now call second generation CM.\n\nAfter his session I had an opportunity to sit down and have\n\ncoffee with him. I was totally sold on what we now call infrastructure as code. However, while we met for coffee, Luke\n\nstarted going even further, explaining his ideas. He started telling me he believed that operations was going to have to\n\nstart behaving like software developers. They were going to\n\nhave to keep their configurations in source control and adopt CI/CD delivery patterns for their workflow. Being the old IT\n\nOperations person at the time, I think I replied to him with something like, “That idea is going to sink like Led Zeppelin\n\nwith Ops folk.” (I was clearly wrong.)\n\nThen about a year later in 2009 at another O’Reilly conference, Velocity, I saw Andrew Clay Shafer give a presentation on Agile\n\nInfrastructure. In his presentation, Andrew showed this iconic\n\npicture of a wall between developers and operations with a metaphorical depiction of work being thrown over the wall. He\n\ncoined this “the wall of confusion.” The ideas he expressed in that presentation codified what Luke was trying to tell me a\n\nyear earlier. That was the light bulb for me. Later that year, I was the only American invited to the original DevOpsDays in",
      "content_length": 1761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Ghent. By the time that event was over, this thing we call\n\nDevOps was clearly in my blood.\n\nClearly, the co-authors of this book all came to a similar epiphany,\n\neven if they came there from very different directions. But there is now an overwhelming weight of evidence that the problems\n\ndescribed above happen almost everywhere, and that the solutions associated with DevOps are nearly universally applicable.\n\nThe goal of writing this book is to describe how to replicate the\n\nDevOps transformations we’ve been a part of or have observed, as well as dispel many of the myths of why DevOps won’t work in\n\ncertain situations. Below are some of the most common myths we\n\nhear about DevOps.\n\nMyth—DevOps is Only for Startups: While DevOps practices have been pioneered by the web-scale, Internet “unicorn”\n\ncompanies such as Google, Amazon, Netflix, and Etsy, each of these organizations has, at some point in their history, risked\n\ngoing out of business because of the problems associated with more traditional “horse” organizations: highly dangerous code\n\nreleases that were prone to catastrophic failure, inability to release\n\nfeatures fast enough to beat the competition, compliance concerns, an inability to scale, high levels of distrust between\n\nDevelopment and Operations, and so forth.\n\nHowever, each of these organizations was able to transform their architecture, technical practices, and culture to create the amazing\n\noutcomes that we associate with DevOps. As Dr. Branden Williams, an information security executive, quipped, “Let there be no more talk of DevOps unicorns or horses but only\n\nthoroughbreds and horses heading to the glue factory.”",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Myth—DevOps Replaces Agile: DevOps principles and practices are compatible with Agile, with many observing that DevOps is a\n\nlogical continuation of the Agile journey that started in 2001. Agile often serves as an effective enabler of DevOps, because of its focus on small teams continually delivering high quality code to\n\ncustomers.\n\nMany DevOps practices emerge if we continue to manage our work beyond the goal of “potentially shippable code” at the end of\n\neach iteration, extending it to having our code always in a deployable state, with developers checking into trunk daily, and that we demonstrate our features in production-like\n\nenvironments.\n\nMyth—DevOps is incompatible with ITIL: Many view DevOps as a backlash to ITIL or ITSM (IT Service Management), which was\n\noriginally published in 1989. ITIL has broadly influenced multiple generations of Ops practitioners, including one of the co-authors, and is an ever-evolving library of practices intended to codify the processes and practices that underpin world-class IT Operations,\n\nspanning service strategy, design, and support.\n\nDevOps practices can be made compatible with ITIL process. However, to support the shorter lead times and higher\n\ndeployment frequencies associated with DevOps, many areas of the ITIL processes become fully automated, solving many problems associated with the configuration and release\n\nmanagement processes (e.g., keeping the configuration management database and definitive software libraries up to date). And because DevOps requires fast detection and recovery when service incidents occur, the ITIL disciplines of service",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "design, incident, and problem management remain as relevant as ever.\n\nMyth—DevOps is Incompatible with Information Security and Compliance: The absence of traditional controls (e.g., segregation of duty, change approval processes, manual security reviews at the\n\nend of the project) may dismay information security and compliance professionals.\n\nHowever, that doesn’t mean that DevOps organizations don’t have\n\neffective controls. Instead of security and compliance activities only being performed at the end of the project, controls are integrated into every stage of daily work in the software\n\ndevelopment life cycle, resulting in better quality, security, and compliance outcomes.\n\nMyth—DevOps Means Eliminating IT Operations, or “NoOps”:\n\nMany misinterpret DevOps as the complete elimination of the IT Operations function. However, this is rarely the case. While the nature of IT Operations work may change, it remains as important\n\nas ever. IT Operations collaborates far earlier in the software life cycle with Development, who continues to work with IT Operations long after the code has been deployed into production.\n\nInstead of IT Operations doing manual work that comes from work tickets, it enables developer productivity through APIs and self-serviced platforms that create environments, test and deploy code, monitor and display production telemetry, and so forth. By\n\ndoing this, IT Operations become more like Development (as do QA and Infosec), engaged in product development, where the product is the platform that developers use to safely, quickly, and\n\nsecurely test, deploy, and run their IT services in production.",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Myth—DevOps is Just “Infrastructure as Code” or Automation: While many of the DevOps patterns shown in this book require\n\nautomation, DevOps also requires cultural norms and an architecture that allows for the shared goals to be achieved throughout the IT value stream. This goes far beyond just automation. As Christopher Little, a technology executive and one\n\nof the earliest chroniclers of DevOps, wrote, “DevOps isn’t about automation, just as astronomy isn’t about telescopes.”\n\nMyth—DevOps is Only for Open Source Software: Although\n\nmany DevOps success stories take place in organizations using software such as the LAMP stack (Linux, Apache, MySQL, PHP), achieving DevOps outcomes is independent of the technology\n\nbeing used. Successes have been achieved with applications written in Microsoft.NET, COBOL, and mainframe assembly code, as well as with SAP and even embedded systems (e.g., HP LaserJet\n\nfirmware).\n\nSPREADING THE AHA! MOMENT\n\nEach of the authors has been inspired by the amazing innovations happening in the DevOps community and the outcomes they are creating: they are creating safe systems of work, and enabling small teams to quickly and independently develop and validate\n\ncode that can be safely deployed to customers. Given our belief that DevOps is a manifestation of creating dynamic, learning organizations that continually reinforce high-trust cultural norms,\n\nit is inevitable that these organizations will continue to innovate and win in the marketplace.",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "It is our sincere hope that The DevOps Handbook will serve as a valuable resource for many people in different ways: a guide for planning and executing DevOps transformations, a set of case\n\nstudies to research and learn from, a chronicle of the history of DevOps, a means to create a coalition that spans Product Owners, Architecture, Development, QA, IT Operations, and Information\n\nSecurity to achieve common goals, a way to get the highest levels of leadership support for DevOps initiatives, as well as a moral imperative to change the way we manage technology organizations\n\nto enable better effectiveness and efficiency, as well as enabling a happier and more humane work environment, helping everyone become lifelong learners—this not only helps everyone achieve their highest goals as human beings, but also helps their\n\norganizations win.",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Foreword\n\nIn the past, many fields of engineering have experienced a sort of\n\nnotable evolution, continually “leveling-up” its understanding of\n\nits own work. While there are university curriculums and\n\nprofessional support organizations situated within specific\n\ndisciplines of engineering (civil, mechanical, electrical, nuclear,\n\netc.), the fact is, modern society needs all forms of engineering to\n\nrecognize the benefits of and work in a multidisciplinary way.\n\nThink about the design of a high-performance vehicle. Where does the work of a mechanical engineer end and the work of an\n\nelectrical engineer begin? Where (and how, and when) should\n\nsomeone with domain knowledge of aerodynamics (who certainly would have well-formed opinions on the shape, size, and\n\nplacement of windows) collaborate with an expert in passenger ergonomics? What about the chemical influences of fuel mixture\n\nand oil on the materials of the engine and transmission over the\n\nlifetime of the vehicle? There are other questions we can ask about the design of an automobile, but the end result is the same: success in modern technical endeavors absolutely requires\n\nmultiple perspectives and expertise to collaborate.\n\nIn order for a field or discipline to progress and mature, it needs to reach a point where it can thoughtfully reflect on its origins, seek out a diverse set of perspectives on those reflections, and place that synthesis into a context that is useful for how the community\n\npictures the future.",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "This book represents such a synthesis and should be seen as a\n\nseminal collection of perspectives on the (I will argue, still\n\nemerging and quickly evolving) field of software engineering and\n\noperations.\n\nNo matter what industry you are in, or what product or service\n\nyour organization provides, this way of thinking is paramount and\n\nnecessary for survival for every business and technology leader.\n\n—John Allspaw, CTO, Etsy\n\nBrooklyn, NY, August 2016",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Imagine a World Where Dev and\n\nOps Become DevOps",
      "content_length": 48,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "An Introduction to The DevOps\n\nHandbook\n\nImagine a world where product owners, Development, QA, IT\n\nOperations, and Infosec work together, not only to help each\n\nother, but also to ensure that the overall organization succeeds. By\n\nworking toward a common goal, they enable the fast flow of\n\nplanned work into production (e.g., performing tens, hundreds, or\n\neven thousands of code deploys per day), while achieving world- class stability, reliability, availability, and security.\n\nIn this world, cross-functional teams rigorously test their hypotheses of which features will most delight users and advance the organizational goals. They care not just about implementing\n\nuser features, but also actively ensure their work flows smoothly and frequently through the entire value stream without causing chaos and disruption to IT Operations or any other internal or external customer.\n\nSimultaneously, QA, IT Operations, and Infosec are always\n\nworking on ways to reduce friction for the team, creating the work systems that enable developers to be more productive and get better outcomes. By adding the expertise of QA, IT Operations,\n\nand Infosec into delivery teams and automated self-service tools and platforms, teams are able to use that expertise in their daily work without being dependent on other teams.\n\nThis enables organizations to create a safe system of work, where small teams are able to quickly and independently develop, test,",
      "content_length": 1443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "and deploy code and value quickly, safely, securely, and reliably to\n\ncustomers. This allows organizations to maximize developer\n\nproductivity, enable organizational learning, create high employee\n\nsatisfaction, and win in the marketplace.\n\nThese are the outcomes that result from DevOps. For most of us,\n\nthis is not the world we live in. More often than not, the system we\n\nwork in is broken, resulting in extremely poor outcomes that fall\n\nwell short of our true potential. In our world, Development and IT\n\nOperations are adversaries; testing and Infosec activities happen\n\nonly at the end of a project, too late to correct any problems found;\n\nand almost any critical activity requires too much manual effort and too many handoffs, leaving us to always be waiting. Not only does this contribute to extremely long lead times to get anything\n\ndone, but the quality of our work, especially production deployments, is also problematic and chaotic, resulting in negative impacts to our customers and our business.\n\nAs a result, we fall far short of our goals, and the whole organization is dissatisfied with the performance of IT, resulting in budget reductions and frustrated, unhappy employees who feel powerless to change the process and its outcomes.† The solution? We need to change how we work; DevOps shows us the best way\n\nforward.\n\nTo better understand the potential of the DevOps revolution, let us\n\nlook at the Manufacturing Revolution of the 1980s. By adopting Lean principles and practices, manufacturing organizations dramatically improved plant productivity, customer lead times, product quality, and customer satisfaction, enabling them to win\n\nin the marketplace.",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Before the revolution, average manufacturing plant order lead times were six weeks, with fewer than 70% of orders being shipped\n\non time. By 2005, with the widespread implementation of Lean practices, average product lead times had dropped to less than\n\nthree weeks, and more than 95% of orders were being shipped on\n\ntime. Organizations that did not implement Lean practices lost market share, and many went out of business entirely.\n\nSimilarly, the bar has been raised for delivering technology\n\nproducts and services—what was good enough in previous decades is not good enough now. For each of the last four decades, the cost\n\nand time required to develop and deploy strategic business capabilities and features has dropped by orders of magnitude.\n\nDuring the 1970s and 1980s, most new features required one to\n\nfive years to develop and deploy, often costing tens of millions of dollars.\n\nBy the 2000’s, because of advances in technology and the\n\nadoption of Agile principles and practices, the time required to develop new functionality had dropped to weeks or months, but\n\ndeploying into production would still require weeks or months, often with catastrophic outcomes.\n\nAnd by 2010, with the introduction of DevOps and the\n\nneverending commoditization of hardware, software, and now the\n\ncloud, features (and even entire startup companies) could be created in weeks, quickly being deployed into production in just\n\nhours or minutes—for these organizations, deployment finally became routine and low risk. These organizations are able to\n\nperform experiments to test business ideas, discovering which ideas create the most value for customers and the organization as",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "a whole, which are then further developed into features that can be rapidly and safely deployed into production.\n\nTable 1. The ever accelerating trend toward faster, cheaper, low-risk delivery of software\n\n(Source: Adrian Cockcroft, “Velocity and Volume (or Speed Wins),” presentation at FlowCon, San Francisco, CA, November 2013.)\n\nToday, organizations adopting DevOps principles and practices often deploy changes hundreds or even thousands of times per\n\nday. In an age where competitive advantage requires fast time to market and relentless experimentation, organizations that are\n\nunable to replicate these outcomes are destined to lose in the\n\nmarketplace to more nimble competitors and could potentially go out of business entirely, much like the manufacturing\n\norganizations that did not adopt Lean principles.\n\nThese days, regardless of what industry we are competing in, the way we acquire customers and deliver value to them is dependent\n\non the technology value stream. Put even more succinctly, as",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Jeffrey Immelt, CEO of General Electric, stated, “Every industry\n\nand company that is not bringing software to the core of their\n\nbusiness will be disrupted.” Or as Jeffrey Snover, Technical Fellow at Microsoft, said, “In previous economic eras, businesses created\n\nvalue by moving atoms. Now they create value by moving bits.”\n\nIt’s difficult to overstate the enormity of this problem—it affects every organization, independent of the industry we operate in, the\n\nsize of our organization, whether we are profit or non-profit. Now more than ever, how technology work is managed and performed\n\npredicts whether our organizations will win in the marketplace, or\n\neven survive. In many cases, we will need to adopt principles and practices that look very different from those that have successfully\n\nguided us over the past decades. See Appendix 1.\n\nNow that we have established the urgency of the problem that DevOps solves, let us take some time to explore in more detail the\n\nsymptomatology of the problem, why it occurs, and why, without dramatic intervention, the problem worsens over time.\n\nTHE PROBLEM: SOMETHING IN YOUR ORGANIZATION MUST NEED IMPROVEMENT (OR YOU WOULDN’T BE READING THIS BOOK)\n\nMost organizations are not able to deploy production changes in\n\nminutes or hours, instead requiring weeks or months. Nor are they able to deploy hundreds or thousands of changes into production per day; instead, they struggle to deploy monthly or",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "even quarterly. Nor are production deployments routine, instead involving outages and chronic firefighting and heroics.\n\nIn an age where competitive advantage requires fast time to market, high service levels, and relentless experimentation, these organizations are at a significant competitive disadvantage. This is\n\nin large part due to their inability to resolve a core, chronic conflict within their technology organization.\n\nTHE CORE, CHRONIC CONFLICT\n\nIn almost every IT organization, there is an inherent conflict between Development and IT Operations which creates a\n\ndownward spiral, resulting in ever-slower time to market for new products and features, reduced quality, increased outages, and, worst of all, an ever-increasing amount of technical debt.\n\nThe term “technical debt” was first coined by Ward Cunningham. Analogous to financial debt, technical debt describes how decisions we make lead to problems that get increasingly more\n\ndifficult to fix over time, continually reducing our available options in the future—even when taken on judiciously, we still incur interest.\n\nOne factor that contributes to this is the often competing goals of Development and IT Operations. IT organizations are responsible for many things. Among them are the two following goals, which\n\nmust be pursued simultaneously:\n\nRespond to the rapidly changing competitive landscape\n\nProvide stable, reliable, and secure service to the customer",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Frequently, Development will take responsibility for responding to changes in the market, deploying features and changes into\n\nproduction as quickly as possible. IT Operations will take responsibility for providing customers with IT service that is stable, reliable, and secure, making it difficult or even impossible\n\nfor anyone to introduce production changes that could jeopardize production. Configured this way, Development and IT Operations have diametrically opposed goals and incentives.\n\nDr. Eliyahu M. Goldratt, one of the founders of the manufacturing management movement, called these types of configuration “the core, chronic conflict”—when organizational measurements and incentives across different silos prevent the achievement of global, organizational goals.‡\n\nThis conflict creates a downward spiral so powerful it prevents the achievement of desired business outcomes, both inside and\n\noutside the IT organization. These chronic conflicts often put technology workers into situations that lead to poor software and service quality, and bad customer outcomes, as well as a daily\n\nneed for workarounds, firefighting, and heroics, whether in Product Management, Development, QA, IT Operations, or Information Security. See Appendix 2.\n\nDOWNWARD SPIRAL IN THREE ACTS\n\nThe downward spiral in IT has three acts that are likely familiar to\n\nmost IT practitioners.\n\nThe first act begins in IT Operations, where our goal is to keep applications and infrastructure running so that our organization\n\ncan deliver value to customers. In our daily work, many of our",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "problems are due to applications and infrastructure that are complex, poorly documented, and incredibly fragile. This is the\n\ntechnical debt and daily workarounds that we live with constantly, always promising that we’ll fix the mess when we have a little more time. But that time never comes.\n\nAlarmingly, our most fragile artifacts support either our most important revenue-generating systems or our most critical projects. In other words, the systems most prone to failure are also our most important and are at the epicenter of our most urgent\n\nchanges. When these changes fail, they jeopardize our most important organizational promises, such as availability to customers, revenue goals, security of customer data, accurate\n\nfinancial reporting, and so forth.\n\nThe second act begins when somebody has to compensate for the latest broken promise—it could be a product manager promising a\n\nbigger, bolder feature to dazzle customers with or a business executive setting an even larger revenue target. Then, oblivious to what technology can or can’t do, or what factors led to missing our\n\nearlier commitment, they commit the technology organization to deliver upon this new promise.\n\nAs a result, Development is tasked with another urgent project\n\nthat inevitably requires solving new technical challenges and cutting corners to meet the promised release date, further adding to our technical debt—made, of course, with the promise that we’ll fix any resulting problems when we have a little more time.\n\nThis sets the stage for the third and final act, where everything becomes just a little more difficult, bit by bit—everybody gets a\n\nlittle busier, work takes a little more time, communications",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "become a little slower, and work queues get a little longer. Our work becomes more tightly-coupled, smaller actions cause bigger failures, and we become more fearful and less tolerant of making\n\nchanges. Work requires more communication, coordination, and approvals; teams must wait just a little longer for their dependent work to get done; and our quality keeps getting worse. The wheels\n\nbegin grinding slower and require more effort to keep turning. See Appendix 3.\n\nAlthough it’s difficult to see in the moment, the downward spiral is\n\nobvious when one takes a step back. We notice that production code deployments are taking ever-longer to complete, moving from minutes to hours to days to weeks. And worse, the\n\ndeployment outcomes have become even more problematic, that resulting in an ever-increasing number of customer-impacting outages that require more heroics and firefighting in Operations, further depriving them of their ability to pay down technical debt.\n\nAs a result, our product delivery cycles continue to move slower and slower, fewer projects are undertaken, and those that are, are less ambitious. Furthermore, the feedback on everyone’s work\n\nbecomes slower and weaker, especially the feedback signals from our customers. And, regardless of what we try, things seem to get worse—we are no longer able to respond quickly to our changing\n\ncompetitive landscape, nor are we able to provide stable, reliable service to our customers. As a result, we ultimately lose in the marketplace.\n\nTime and time again, we learn that when IT fails, the entire organization fails. As Steven J. Spear noted in his book The High- Velocity Edge, whether the damages “unfold slowly like a wasting",
      "content_length": 1701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "disease” or rapidly “like a fiery crash...the destruction can be just as complete.”\n\nWHY DOES THIS DOWNWARD SPIRAL HAPPEN EVERYWHERE?\n\nFor over a decade, the authors of this book have observed this destructive spiral occur in countless organizations of all types and\n\nsizes. We understand better than ever why this downward spiral occurs and why it requires DevOps principles to mitigate. First, as described earlier, every IT organization has two opposing goals, and second, every company is a technology company, whether\n\nthey know it or not.\n\nAs Christopher Little, a software executive and one of the earliest chroniclers of DevOps, said, “Every company is a technology\n\ncompany, regardless of what business they think they’re in. A bank is just an IT company with a banking license.”§\n\nTo convince ourselves that this is the case, consider that the vast majority of capital projects have some reliance upon IT. As the saying goes, “It is virtually impossible to make any business decision that doesn’t result in at least one IT change.”\n\nIn the business and finance context, projects are critical because they serve as the primary mechanism for change inside organizations. Projects are typically what management needs to\n\napprove, budget for, and be held accountable for; therefore, they are the mechanism that achieve the goals and aspirations of the organization, whether it is to grow or even shrink.¶\n\nProjects are typically funded through capital spending (i.e., factories, equipment, and major projects, and expenditures are",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "capitalized when payback is expected to take years), of which 50% is now technology related. This is even true in “low tech” industry verticals with the lowest historical spending on technology, such\n\nas energy, metal, resource extraction, automotive, and construction. In other words, business leaders are far more reliant upon the effective management of IT in order to achieve their goals than they think.**\n\nTHE COSTS: HUMAN AND ECONOMIC\n\nWhen people are trapped in this downward spiral for years, especially those who are downstream of Development, they often feel stuck in a system that pre-ordains failure and leaves them powerless to change the outcomes. This powerlessness is often\n\nfollowed by burnout, with the associated feelings of fatigue, cynicism, and even hopelessness and despair.\n\nMany psychologists assert that creating systems that cause feelings of powerlessness is one of the most damaging things we can do to fellow human beings—we deprive other people of their ability to control their own outcomes and even create a culture\n\nwhere people are afraid to do the right thing because of fear of punishment, failure, or jeopardizing their livelihood. This can create the conditions of learned helplessness, where people\n\nbecome unwilling or unable to act in a way that avoids the same problem in the future.\n\nFor our employees, it means long hours, working on weekends,\n\nand a decreased quality of life, not just for the employee, but for everyone who depends on them, including family and friends. It is\n\nnot surprising that when this occurs, we lose our best people",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "(except for those that feel like they can’t leave, because of a sense\n\nof duty or obligation).\n\nIn addition to the human suffering that comes with the current way of working, the opportunity cost of the value that we could be\n\ncreating is staggering—the authors believe that we are missing out\n\non approximately $2.6 trillion of value creation per year, which is, at the time of this writing, equivalent to the annual economic\n\noutput of France, the sixth-largest economy in the world.\n\nConsider the following calculation—both IDC and Gartner estimated that in 2011, approximately 5% of the worldwide gross\n\ndomestic product($3.1 trillion) was spent on IT (hardware,\n\nservices, and telecom). If we estimate that 50% of that $3.1 trillion was spent on operating costs and maintaining existing systems,\n\nand that one-third of that 50% was spent on urgent and\n\nunplanned work or rework, approximately $520 billion was wasted.\n\nIf adopting DevOps could enable us, through better management\n\nand increased operational excellence, to halve that waste and redeploy that human potential into something that’s five times the\n\nvalue (a modest proposal), we could create $2.6 trillion of value\n\nper year.\n\nTHE ETHICS OF DEVOPS: THERE IS A BETTER WAY\n\nIn the previous sections, we described the problems and the\n\nnegative consequences of the status quo due to the core, chronic\n\nconflict, from the inability to achieve organizational goals, to the",
      "content_length": 1435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "damage we inflict on fellow human beings. By solving these\n\nproblems, DevOps astonishingly enables us to simultaneously\n\nimprove organizational performance, achieve the goals of all the various functional technology roles (e.g., Development, QA, IT\n\nOperations, Infosec), and improve the human condition.\n\nThis exciting and rare combination may explain why DevOps has generated so much excitement and enthusiasm in so many in such\n\na short time, including technology leaders, engineers, and much of\n\nthe software ecosystem we reside in.\n\nBREAKING THE DOWNWARD SPIRAL WITH DEVOPS\n\nIdeally, small teams of developers independently implement their features, validate their correctness in production-like\n\nenvironments, and have their code deployed into production\n\nquickly, safely and securely. Code deployments are routine and predictable. Instead of starting deployments at midnight on Friday\n\nand spending all weekend working to complete them, deployments occur throughout the business day when everyone is\n\nalready in the office and without our customers even noticing—\n\nexcept when they see new features and bug fixes that delight them. And, by deploying code in the middle of the workday, for the first\n\ntime in decades IT Operations is working during normal business\n\nhours like everyone else.\n\nBy creating fast feedback loops at every step of the process,\n\neveryone can immediately see the effects of their actions.\n\nWhenever changes are committed into version control, fast automated tests are run in production-like environments, giving\n\ncontinual assurance that the code and environments operate as\n\ndesigned and are always in a secure and deployable state.",
      "content_length": 1665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Automated testing helps developers discover their mistakes\n\nquickly (usually within minutes), which enables faster fixes as well\n\nas genuine learning—learning that is impossible when mistakes are discovered six months later during integration testing, when\n\nmemories and the link between cause and effect have long faded. Instead of accruing technical debt, problems are fixed as they are\n\nfound, mobilizing the entire organization if needed, because global\n\ngoals outweigh local goals.\n\nPervasive production telemetry in both our code and production environments ensure that problems are detected and corrected\n\nquickly, confirming that everything is working as intended and customers are getting value from the software we create.\n\nIn this scenario, everyone feels productive—the architecture\n\nallows small teams to work safely and architecturally decoupled from the work of other teams who use self-service platforms that\n\nleverage the collective experience of Operations and Information\n\nSecurity. Instead of everyone waiting all the time, with large amounts of late, urgent rework, teams work independently and\n\nproductively in small batches, quickly and frequently delivering\n\nnew value to customers.\n\nEven high-profile product and feature releases become routine by\n\nusing dark launch techniques. Long before the launch date, we put\n\nall the required code for the feature into production, invisible to everyone except internal employees and small cohorts of real\n\nusers, allowing us to test and evolve the feature until it achieves\n\nthe desired business goal.\n\nAnd, instead of firefighting for days or weeks to make the new\n\nfunctionality work, we merely change a feature toggle or",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "configuration setting. This small change makes the new feature\n\nvisible to ever-larger segments of customers, automatically rolling\n\nback if something goes wrong. As a result, our releases are controlled, predictable, reversible, and low stress.\n\nIt’s not just feature releases that are calmer—all sorts of problems\n\nare being found and fixed early, when they are smaller, cheaper, and easier to correct. With every fix, we also generate\n\norganizational learnings, enabling us to prevent the problem from recurring and enabling us to detect and correct similar problems\n\nfaster in the future.\n\nFurthermore, everyone is constantly learning, fostering a\n\nhypothesis-driven culture where the scientific method is used to ensure nothing is taken for granted—we do nothing without\n\nmeasuring and treating product development and process improvement as experiments.\n\nBecause we value everyone’s time, we don’t spend years building\n\nfeatures that our customers don’t want, deploying code that doesn’t work, or fixing something that isn’t actually the cause of\n\nour problem.\n\nBecause we care about achieving goals, we create long-term teams that are responsible for meeting them. Instead of project teams\n\nwhere developers are reassigned and shuffled around after each\n\nrelease, never receiving feedback on their work, we keep teams intact so they can keep iterating and improving, using those\n\nlearnings to better achieve their goals. This is equally true for the\n\nproduct teams who are solving problems for our external customers, as well as our internal platform teams who are helping\n\nother teams be more productive, safe, and secure.",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Instead of a culture of fear, we have a high-trust, collaborative\n\nculture, where people are rewarded for taking risks. They are able to fearlessly talk about problems as opposed to hiding them or\n\nputting them on the backburner—after all, we must see problems\n\nin order to solve them.\n\nAnd, because everyone fully owns the quality of their work, everyone builds automated testing into their daily work and uses\n\npeer reviews to gain confidence that problems are addressed long before they can impact a customer. These processes mitigate risk,\n\nas opposed to approvals from distant authorities, allowing us to\n\ndeliver value quickly, reliably, and securely—even proving to skeptical auditors that we have an effective system of internal\n\ncontrols.\n\nAnd when something does go wrong, we conduct blameless post- mortems, not to punish anyone, but to better understand what\n\ncaused the accident and how to prevent it. This ritual reinforces\n\nour culture of learning. We also hold internal technology conferences to elevate our skills and ensure that everyone is\n\nalways teaching and learning.\n\nBecause we care about quality, we even inject faults into our production environment so we can learn how our system fails in a\n\nplanned manner. We conduct planned exercises to practice large-\n\nscale failures, randomly kill processes and compute servers in production, and inject network latencies and other nefarious acts\n\nto ensure we grow ever more resilient. By doing this, we enable\n\nbetter resilience, as well as organizational learning and improvement.",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "In this world, everyone has ownership in their work, regardless of\n\ntheir role in the technology organization They have confidence that their work matters and is meaningfully contributing to\n\norganizational goals, proven by their low-stress work environment\n\nand their organization’s success in the marketplace. Their proof is that the organization is indeed winning in the marketplace.\n\nTHE BUSINESS VALUE OF DEVOPS\n\nWe have decisive evidence of the business value of DevOps. From\n\n2013 through 2016, as part of Puppet Labs’ State Of DevOps\n\nReport, to which authors Jez Humble and Gene Kim contributed, we collected data from over twenty-five thousand technology\n\nprofessionals, with the goal of better understanding the health and\n\nhabits of organizations at all stages of DevOps adoption.\n\nThe first surprise this data revealed was how much high-\n\nperforming organizations using DevOps practices were\n\noutperforming their non–high performing peers in the following areas:\n\nThroughput metrics\n\nCode and change deployments (thirty times more frequent)\n\nCode and change deployment lead time (two hundred times faster)\n\nReliability metrics\n\nProduction deployments (sixty times higher change success rate)\n\nMean time to restore service (168 times faster)",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Organizational performance metrics\n\nProductivity, market share, and profitability goals (two times more likely to exceed)\n\nMarket capitalization growth (50% higher over three years)\n\nIn other words, high performers were both more agile and more\n\nreliable, providing empirical evidence that DevOps enables us to break the core, chronic conflict. High performers deployed code\n\nthirty times more frequently, and the time required to go from\n\n“code committed” to “successfully running in production” was two hundred times faster—high performers had lead times measured\n\nin minutes or hours, while low performers had lead times measured in weeks, months, or even quarters.\n\nFurthermore, high performers were twice as likely to exceed\n\nprofitability, market share, and productivity goals. And, for those\n\norganizations that provided a stock ticker symbol, we found that high performers had 50% higher market capitalization growth\n\nover three years. They also had higher employee job satisfaction, lower rates of employee burnout, and their employees were 2.2\n\ntimes more likely to recommend their organization to friends as a great place to work.†† High performers also had better information security outcomes. By integrating security objectives into all stages\n\nof the development and operations processes, they spent 50% less\n\ntime remediating security issues.\n\nDEVOPS HELPS SCALE DEVELOPER PRODUCTIVITY\n\nWhen we increase the number of developers, individual developer productivity often significantly decreases due to communication,\n\nintegration, and testing overhead. This is highlighted in the",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "famous book by Frederick Brook, The Mythical Man-Month,\n\nwhere he explains that when projects are late, adding more developers not only decreases individual developer productivity\n\nbut also decreases overall productivity.\n\nOn the other hand, DevOps shows us that when we have the right architecture, the right technical practices, and the right cultural\n\nnorms, small teams of developers are able to quickly, safely, and\n\nindependently develop, integrate, test, and deploy changes into production. As Randy Shoup, formerly a director of engineering at\n\nGoogle, observed, large organizations using DevOps “have\n\nthousands of developers, but their architecture and practices enable small teams to still be incredibly productive, as if they were\n\na startup.”\n\nThe 2015 State of DevOps Report examined not only “deploys per day” but also “deploys per day per developer.” We hypothesized\n\nthat high performers would be able to scale their number of\n\ndeployments as team sizes grew.\n\nFigure 1. Deployments/day vs. number of developers (Source: Puppet Labs, 2015 State Of DevOps Report.)‡‡",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Indeed, this is what we found. Figure 1 shows that in low\n\nperformers, deploys per day per developer go down as team size increases, stays constant for medium performers, and increases\n\nlinearly for high performers.\n\nIn other words, organizations adopting DevOps are able to linearly increase the number of deploys per day as they increase their\n\nnumber of developers, just as Google, Amazon, and Netflix have done.§§\n\nTHE UNIVERSALITY OF THE SOLUTION\n\nOne of the most influential books in the Lean manufacturing movement is The Goal: A Process of Ongoing Improvement\n\nwritten by Dr. Eliyahu M. Goldratt in 1984. It influenced an entire\n\ngeneration of professional plant managers around the world. It was a novel about a plant manager who had to fix his cost and\n\nproduct due date issues in ninety days, otherwise his plant would\n\nbe shut down.\n\nLater in his career, Dr. Goldratt described the letters he received\n\nin response to The Goal. These letters would typically read, “You\n\nhave obviously been hiding in our factory, because you’ve described my life [as a plant manager] exactly…” Most\n\nimportantly, these letters showed people were able to replicate the\n\nbreakthroughs in performance that were described in the book in their own work environments.\n\nThe Phoenix Project: A Novel About IT, DevOps, and Helping\n\nYour Business Win, written by Gene Kim, Kevin Behr, and George Spafford in 2013, was closely modeled after The Goal. It is a novel\n\nthat follows an IT leader who faces all the typical problems that",
      "content_length": 1515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "are endemic in IT organizations: an over-budget, behind-schedule\n\nproject that must get to market in order for the company to survive. He experiences catastrophic deployments; problems with\n\navailability, security, and compliance; and so forth. Ultimately, he\n\nand his team use DevOps principles and practices to overcome those challenges, helping their organization win in the\n\nmarketplace. In addition, the novel shows how DevOps practices\n\nimproved the workplace environment for the team, creating lower stress and higher satisfaction because of greater practitioner\n\ninvolvement throughout the process.\n\nAs with The Goal, there is tremendous evidence of the universality of the problems and solutions described in The Phoenix Project.\n\nConsider some of the statements found in the Amazon reviews: “I find myself relating to the characters in The Phoenix Project...I’ve\n\nprobably met most of them over the course of my career,” “If you\n\nhave ever worked in any aspect of IT, DevOps, or Infosec you will definitely be able to relate to this book,” or “There’s not a\n\ncharacter in The Phoenix Project that I don’t identify with myself\n\nor someone I know in real life… not to mention the problems faced and overcome by those characters.”\n\nIn the remainder of this book, we will describe how to replicate the\n\ntransformation described in The Phoenix Project, as well provide many case studies of how other organizations have used DevOps\n\nprinciples and practices to replicate those outcomes.\n\nTHE DEVOPS HANDBOOK: AN ESSENTIAL GUIDE",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "The purpose of the DevOps Handbook is to give you the theory,\n\nprinciples, and practices you need to successfully start your DevOps initiative and achieve your desired outcomes. This\n\nguidance is based on decades of sound management theory, study\n\nof high-performing technology organizations, work we have done helping organizations transform, and research that validates the\n\neffectiveness of the prescribed DevOps practices. As well as interviews with relevant subject matter experts and analyses of\n\nnearly one hundred case studies presented at the DevOps\n\nEnterprise Summit.\n\nBroken into six parts, this book covers DevOps theories and\n\nprinciples using the Three Ways, a specific view of the\n\nunderpinning theory originally introduced in The Phoenix Project. The DevOps Handbook is for everyone who performs or influences\n\nwork in the technology value stream (which typically includes\n\nProduct Management, Development, QA, IT Operations, and Information Security), as well as for business and marketing\n\nleadership, where most technology initiatives originate.\n\nThe reader is not expected to have extensive knowledge of any of these domains, or of DevOps, Agile, ITIL, Lean, or process\n\nimprovement. Each of these topics is introduced and explained in\n\nthe book as it becomes necessary.\n\nOur intent is to create a working knowledge of the critical concepts\n\nin each of these domains, both to serve as a primer and to\n\nintroduce the language necessary to help practitioners work with all their peers across the entire IT value stream, and to frame\n\nshared goals.",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "This book will be of value to business leaders and stakeholders\n\nwho are increasingly reliant upon the technology organization for the achievement of their goals.\n\nFurthermore, this book is intended for readers whose\n\norganizations might not be experiencing all the problems described in the book (e.g., long deployment lead times or painful\n\ndeployments). Even readers in this fortunate position will benefit\n\nfrom understanding DevOps principles, especially those relating to shared goals, feedback, and continual learning.\n\nIn Part I, we present a brief history of DevOps and introduce the\n\nunderpinning theory and key themes from relevant bodies of knowledge that span over decades. We then present the high level\n\nprinciples of the Three Ways: Flow, Feedback, and Continual Learning and Experimentaion.\n\nPart II describes how and where to start, and presents concepts such as value streams, organizational design principles and\n\npatterns, organizational adoption patterns, and case studies.\n\nPart III describes how to accelerate Flow by building the foundations of our deployment pipeline: enabling fast and\n\neffective automated testing, continuous integration, continuous\n\ndelivery, and architecting for low-risk releases.\n\nPart IV discusses how to accelerate and amplify Feedback by\n\ncreating effective production telemetry to see and solve problems,\n\nbetter anticipate problems and achieve goals, enable feedback so\n\nthat Dev and Ops can safely deploy changes, integrate A/B testing\n\ninto our daily work, and create review and coordination processes\n\nto increase the quality of our work.",
      "content_length": 1595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Part V describes how we accelerate Continual Learning by\n\nestablishing a just culture, converting local discoveries into global\n\nimprovements, and properly reserving time to create organizational learning and improvements.\n\nFinally, in Part VI we describe how to properly integrate security\n\nand compliance into our daily work, by integrating preventative security controls into shared source code repositories and services,\n\nintegrating security into our deployment pipeline, enhancing\n\ntelemetry to better enable detection and recovery, protecting the\n\ndeployment pipeline, and achieving change management\n\nobjectives.\n\nBy codifying these practices, we hope to accelerate the adoption of\n\nDevOps practices, increase the success of DevOps initiatives, and\n\nlower the activation energy required for DevOps transformations.\n\n† This is just a small sample of the problems found in typical IT organizations.\n\n‡ In the manufacturing realm, a similar core, chronic conflict existed: the need to simultaneously ensure on-time shipments to customers and control costs. How this core, chronic conflict was broken is described in Appendix 2.\n\n§ In 2013, the European bank HSBC employed more software developers than Google.\n\n¶ For now, let us suspend the discussion of whether software should be funded as a “project” or a “product.” This\n\nis discussed later in the book.\n\n** For instance, Dr. Vernon Richardson and his colleagues published this astonishing finding. They studied the\n\n10-K SEC filings of 184 public corporations and divided them into three groups: A) firms with material weaknesses with IT-related deficiencies, B) firms with material weaknesses with no IT-related deficiencies, and C) “clean firms” with no material weaknesses. Firms in Group A saw eight times higher CEO turnover than Group C, and there was four times higher CFO turnover in Group A than in Group C. Clearly, IT may matter far more than we typically think.\n\n†† As measured by employee Net Promoter Score (eNPS). This is a significant finding, as research has shown that\n\n“companies with highly engaged workers grew revenues two and a half times as much as those with low engagement levels. And [publicly traded] stocks of companies with a high-trust work environment outperformed market indexes by a factor of three from 1997 through 2011.”\n\n‡‡ Only organizations that are deploying at least once per day are shown.\n\n§§ Another more extreme example is Amazon. In 2011, Amazon was performing approximately seven thousand\n\ndeploys per day. By 2015, they were performing 130,000 deploys per day.",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Part\n\nIntroduction\n\nIn Part I of The DevOps Handbook, we will explore how the\n\nconvergence of several important movements in management and\n\ntechnology set the stage for the DevOps movement. We describe\n\nvalue streams, how DevOps is the result of applying Lean principles to the technology value stream, and the Three Ways:\n\nFlow, Feedback, and Continual Learning and Experimentation.\n\nPrimary focuses within these chapters include:\n\nThe principles of Flow, which accelerate the delivery of work\n\nfrom Development to Operations to our customers\n\nThe principles of Feedback, which enable us to create ever safer systems of work\n\nThe principles of Continual Learning and Experimentation foster a high-trust culture and a scientific approach to\n\norganizational improvement risk-taking as part of our daily work\n\nA BRIEF HISTORY",
      "content_length": 824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "DevOps and its resulting technical, architectural, and cultural\n\npractices represent a convergence of many philosophical and\n\nmanagement movements. While many organizations have\n\ndeveloped these principles independently, understanding that\n\nDevOps resulted from a broad stroke of movements, a\n\nphenomenon described by John Willis (one of the co-authors of\n\nthis book) as the “convergence of DevOps,” shows an amazing\n\nprogression of thinking and improbable connections. There are\n\ndecades of lessons learned from manufacturing, high reliability\n\norganization, high-trust management models, and others that\n\nhave brought us to the DevOps practices we know today.\n\nDevOps is the outcome of applying the most trusted principles from the domain of physical manufacturing and leadership to the\n\nIT value stream. DevOps relies on bodies of knowledge from Lean, Theory of Constraints, the Toyota Production System, resilience engineering, learning organizations, safety culture, human factors, and many others. Other valuable contexts that DevOps draws from include high-trust management cultures, servant leadership,\n\nand organizational change management. The result is world-class quality, reliability, stability, and security at ever lower cost and effort; and accelerated flow and reliability throughout the technology value stream, including Product Management, Development, QA, IT Operations, and Infosec.\n\nWhile the foundation of DevOps can be seen as being derived from Lean, the Theory of Constraints, and the Toyota Kata movement, many also view DevOps as the logical continuation of the Agile\n\nsoftware journey that began in 2001.\n\nTHE LEAN MOVEMENT",
      "content_length": 1653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Techniques such as Value Stream Mapping, Kanban Boards, and\n\nTotal Productive Maintenance were codified for the Toyota\n\nProduction System in the 1980s. In 1997, the Lean Enterprise\n\nInstitute started researching applications of Lean to other value\n\nstreams, such as the service industry and healthcare.\n\nTwo of Lean’s major tenets include the deeply held belief that\n\nmanufacturing lead time required to convert raw materials into\n\nfinished goods was the best predictor of quality, customer\n\nsatisfaction, and employee happiness, and that one of the best\n\npredictors of short lead times was small batch sizes of work.\n\nLean principles focus on how to create value for the customer through systems thinking by creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual.\n\nTHE AGILE MANIFESTO\n\nThe Agile Manifesto was created in 2001 by seventeen of the leading thinkers in software development. They wanted to create a lightweight set of values and principles against heavyweight software development processes such as waterfall development,\n\nand methodologies such as the Rational Unified Process.\n\nOne key principle was to “deliver working software frequently,\n\nfrom a couple of weeks to a couple of months, with a preference to the shorter timescale,” emphasizing the desire for small batch sizes, incremental releases instead of large, waterfall releases. Other principles emphasized the need for small, self-motivated teams, working in a high-trust management model.",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Agile is credited for dramatically increasing the productivity of many development organizations. And interestingly, many of the\n\nkey moments in DevOps history also occurred within the Agile community or at Agile conferences, as described below.\n\nAGILE INFRASTRUCTURE AND VELOCITY MOVEMENT\n\nAt the 2008 Agile conference in Toronto, Canada, Patrick Debois and Andrew Schafer held a “birds of a feather” session on applying\n\nAgile principles to infrastructure as opposed to application code. Although they were the only people who showed up, they rapidly\n\ngained a following of like-minded thinkers, including co-author\n\nJohn Willis.\n\nLater, at the 2009 Velocity conference, John Allspaw and Paul Hammond gave the seminal “10 Deploys per Day: Dev and Ops\n\nCooperation at Flickr” presentation, where they described how they created shared goals between Dev and Ops and used\n\ncontinuous integration practices to make deployment part of everyone’s daily work. According to first hand accounts, everyone\n\nattending the presentation immediately knew they were in the\n\npresence of something profound and of historic significance.\n\nPatrick Debois was not there, but was so excited by Allspaw and Hammond’s idea that he created the first DevOpsDays in Ghent,\n\nBelgium, (where he lived) in 2009. There the term “DevOps” was coined.\n\nTHE CONTINUOUS DELIVERY MOVEMENT\n\nBuilding upon the development discipline of continuous build,\n\ntest, and integration, Jez Humble and David Farley extended the concept to continuous delivery, which defined the role of a",
      "content_length": 1542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "“deployment pipeline” to ensure that code and infrastructure are always in a deployable state, and that all code checked in to trunk\n\ncan be safely deployed into production. This idea was first presented at the 2006 Agile conference, and was also\n\nindependently developed in 2009 by Tim Fitz in a blog post on his website titled “Continuous Deployment.”¶¶\n\nTOYOTA KATA\n\nIn 2009, Mike Rother wrote Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results, which framed\n\nhis twenty-year journey to understand and codify the Toyota Production System. He had been one of the graduate students who\n\nflew with GM executives to visit Toyota plants and helped develop\n\nthe Lean toolkit, but he was puzzled when none of the companies adopting these practices replicated the level of performance\n\nobserved at the Toyota plants.\n\nHe concluded that the Lean community missed the most important practice of all, which he called the improvement kata.\n\nHe explains that every organization has work routines, and the improvement kata requires creating structure for the daily,\n\nhabitual practice of improvement work, because daily practice is\n\nwhat improves outcomes. The constant cycle of establishing desired future states, setting weekly target outcomes, and the\n\ncontinual improvement of daily work is what guided improvement at Toyota.\n\nThe above describes the history of DevOps and relevant\n\nmovements that it draws upon. Throughout the rest of Part I, we look at value streams, how Lean principles can be applied to the",
      "content_length": 1533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "technology value stream, and the Three Ways of Flow, Feedback,\n\nand Continual Learning and Experimentation.\n\n¶¶ DevOps also extends and builds upon the practices of infrastructure as code, which was pioneered by Dr. Mark Burgess, Luke Kanies, and Adam Jacob. In infrastructure as code, the work of Operations is automated and treated like application code, so that modern development practices can be applied to the entire development stream. This further enabled fast deployment flow, including continuous integration (pioneered by Grady Booch and integrated as one of the key 12 practices of Extreme Programming), continuous delivery (pioneered by Jez Humble and David Farley), and continuous deployment (pioneered by Etsy, Wealthfront, and Eric Ries’s work at IMVU).",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "1 Agile, Continuous\n\nDelivery, and the Three Ways\n\nIn this chapter, an introduction to the underpinning theory of\n\nLean Manufacturing is presented, as well as the Three Ways, the\n\nprinciples from which all of the observed DevOps behaviors can be\n\nderived.\n\nOur focus here is primarily on theory and principles, describing\n\nmany decades of lessons learned from manufacturing, high- reliability organizations, high-trust management models, and\n\nothers, from which DevOps practices have been derived. The\n\nresulting concrete principles and patterns, and their practical application to the technology value stream, are presented in the\n\nremaining chapters of the book.\n\nTHE MANUFACTURING VALUE STREAM\n\nOne of the fundamental concepts in Lean is the value stream. We will define it first in the context of manufacturing and then extrapolate how it applies to DevOps and the technology value\n\nstream.",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Karen Martin and Mike Osterling define value stream in their\n\nbook Value Stream Mapping: How to Visualize Work and Align\n\nLeadership for Organizational Transformation as “the sequence\n\nof activities an organization undertakes to deliver upon a customer\n\nrequest,” or “the sequence of activities required to design,\n\nproduce, and deliver a good or service to a customer, including the\n\ndual flows of information and material.”\n\nIn manufacturing operations, the value stream is often easy to see\n\nand observe: it starts when a customer order is received and the\n\nraw materials are released onto the plant floor. To enable fast and\n\npredictable lead times in any value stream, there is usually a relentless focus on creating a smooth and even flow of work, using techniques such as small batch sizes, reducing work in process\n\n(WIP), preventing rework to ensure we don’t pass defects to downstream work centers, and constantly optimizing our system toward our global goals.\n\nTHE TECHNOLOGY VALUE STREAM\n\nThe same principles and patterns that enable the fast flow of work in physical processes are equally applicable to technology work\n\n(and, for that matter, for all knowledge work). In DevOps, we typically define our technology value stream as the process required to convert a business hypothesis into a technology- enabled service that delivers value to the customer.\n\nThe input to our process is the formulation of a business objective, concept, idea, or hypothesis, and starts when we accept the work in Development, adding it to our committed backlog of work.",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "From there, Development teams that follow a typical Agile or\n\niterative process will likely transform that idea into user stories\n\nand some sort of feature specification, which is then implemented\n\nin code into the application or service being built. The code is then\n\nchecked in to the version control repository, where each change is\n\nintegrated and tested with the rest of the software system.\n\nBecause value is created only when our services are running in\n\nproduction, we must ensure that we are not only delivering fast\n\nflow, but that our deployments can also be performed without\n\ncausing chaos and disruptions such as service outages, service\n\nimpairments, or security or compliance failures.\n\nFOCUS ON DEPLOYMENT LEAD TIME\n\nFor the remainder of this book, our attention will be on deployment lead time, a subset of the value stream described above. This value stream begins when any engineer*** in our value stream (which includes Development, QA, IT Operations, and Infosec) checks a change into version control and ends when that change is successfully running in production, providing value to\n\nthe customer and generating useful feedback and telemetry.\n\nThe first phase of work that includes Design and Development is\n\nakin to Lean Product Development and is highly variable and highly uncertain, often requiring high degrees of creativity and work that may never be performed again, resulting in high variability of process times. In contrast, the second phase of work, which includes Testing and Operations, is akin to Lean Manufacturing. It requires creativity and expertise, and strives to\n\nbe predictable and mechanistic, with the goal of achieving work",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "outputs with minimized variability (e.g., short and predictable lead times, near zero defects).\n\nInstead of large batches of work being processed sequentially\n\nthrough the design/development value stream and then through the test/operations value stream (such as when we have a large\n\nbatch waterfall process or long-lived feature branches), our goal is\n\nto have testing and operations happening simultaneously with design/development, enabling fast flow and high quality. This\n\nmethod succeeds when we work in small batches and build quality into every part of our value stream.†††\n\nDefining Lead Time vs. Processing Time\n\nIn the Lean community, lead time is one of two measures\n\ncommonly used to measure performance in value streams, with the other being processing time (sometimes known as touch time or task time).‡‡‡\n\nWhereas the lead time clock starts when the request is made and ends when it is fulfilled, the process time clock starts only when\n\nwe begin work on the customer request—specifically, it omits the time that the work is in queue, waiting to be processed (figure 2).\n\nFigure 2. Lead time vs. process time of a deployment operation",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Because lead time is what the customer experiences, we typically focus our process improvement attention there instead of on\n\nprocess time. However, the proportion of process time to lead time serves as an important measure of efficiency—achieving fast flow\n\nand short lead times almost always requires reducing the time our work is waiting in queues.\n\nThe Common Scenario: Deployment Lead Times Requiring Months\n\nIn business as usual, we often find ourselves in situations where\n\nour deployment lead times require months. This is especially\n\ncommon in large, complex organizations that are working with tightly-coupled, monolithic applications, often with scarce\n\nintegration test environments, long test and production environment lead times, high reliance on manual testing, and\n\nmultiple required approval processes.When this occurs, our value stream may look like figure 3:\n\nFigure 3: A technology value stream with a deployment lead time of three months (Source: Damon Edwards, “DevOps Kaizen,” 2015.)\n\nWhen we have long deployment lead times, heroics are required at\n\nalmost every stage of the value stream. We may discover that nothing works at the end of the project when we merge all the\n\ndevelopment team’s changes together, resulting in code that no\n\nlonger builds correctly or passes any of our tests. Fixing each problem requires days or weeks of investigation to determine who",
      "content_length": 1391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "broke the code and how it can be fixed, and still results in poor\n\ncustomer outcomes.\n\nOur DevOps Ideal: Deployment Lead Times of Minutes\n\nIn the DevOps ideal, developers receive fast, constant feedback on their work, which enables them to quickly and independently\n\nimplement, integrate, and validate their code, and have the code deployed into the production environment (either by deploying\n\nthe code themselves or by others).\n\nWe achieve this by continually checking small code changes into\n\nour version control repository, performing automated and exploratory testing against it, and deploying it into production.\n\nThis enables us to have a high degree of confidence that our changes will operate as designed in production and that any\n\nproblems can be quickly detected and corrected.\n\nThis is most easily achieved when we have architecture that is modular, well encapsulated, and loosely-coupled so that small\n\nteams are able to work with high degrees of autonomy, with\n\nfailures being small and contained, and without causing global disruptions.\n\nIn this scenario, our deployment lead time is measured in\n\nminutes, or, in the worst case, hours. Our resulting value stream map should look something like figure 4:",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Figure 4: A technology value stream with a lead time of minutes\n\nOBSERVING “%C/A” AS A MEASURE OF REWORK\n\nIn addition to lead times and process times, the third key metric in the technology value stream is percent complete and accurate\n\n(%C/A). This metric reflects the quality of the output of each step in our value stream. Karen Martin and Mike Osterling state that “the %C/A can be obtained by asking downstream customers what percentage of the time they receive work that is ‘usable as is,’\n\nmeaning that they can do their work without having to correct the information that was provided, add missing information that should have been supplied, or clarify information that should have\n\nand could have been clearer.”\n\nTHE THREE WAYS: THE PRINCIPLES UNDERPINNING DEVOPS\n\nThe Phoenix Project presents the Three Ways as the set of underpinning principles from which all the observed DevOps behaviors and patterns are derived (figure 5).\n\nThe First Way enables fast left-to-right flow of work from Development to Operations to the customer. In order to maximize flow, we need to make work visible, reduce our batch sizes and\n\nintervals of work, build in quality by preventing defects from being",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "passed to downstream work centers, and constantly optimize for the global goals.\n\nFigure 5: The Three Ways (Source: Gene Kim, “The Three Ways: The Principles Underpinning DevOps,” IT Revolution Press blog, accessed August 9, 2016, http://itrevolution.com/the-three-ways-principles- underpinning-devops/.)\n\nBy speeding up flow through the technology value stream, we\n\nreduce the lead time required to fulfill internal or customer requests, especially the time required to deploy code into the production environment. By doing this, we increase the quality of\n\nwork as well as our throughput, and boost our ability to out- experiment the competition.\n\nThe resulting practices include continuous build, integration, test,\n\nand deployment processes; creating environments on demand;",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "limiting work in process (WIP); and building systems and organizations that are safe to change.\n\nThe Second Way enables the fast and constant flow of feedback from right to left at all stages of our value stream. It requires that we amplify feedback to prevent problems from happening again,\n\nor enable faster detection and recovery. By doing this, we create quality at the source and generate or embed knowledge where it is needed—this allows us to create ever-safer systems of work where problems are found and fixed long before a catastrophic failure\n\noccurs.\n\nBy seeing problems as they occur and swarming them until\n\neffective countermeasures are in place, we continually shorten and amplify our feedback loops, a core tenet of virtually all modern process improvement methodologies. This maximizes the opportunities for our organization to learn and improve.\n\nThe Third Way enables the creation of a generative, high-trust culture that supports a dynamic, disciplined, and scientific approach to experimentation and risk-taking, facilitating the\n\ncreation of organizational learning, both from our successes and failures. Furthermore, by continually shortening and amplifying our feedback loops, we create ever-safer systems of work and are\n\nbetter able to take risks and perform experiments that help us learn faster than our competition and win in the marketplace.\n\nAs part of the Third Way, we also design our system of work so\n\nthat we can multiply the effects of new knowledge, transforming local discoveries into global improvements. Regardless of where someone performs work, they do so with the cumulative and\n\ncollective experience of everyone in the organization.",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "CONCLUSION\n\nIn this chapter, we described the concepts of value streams, lead time as one of the key measures of the effectiveness for both manufacturing and technology value streams, and the high-level\n\nconcepts behind each of the Three Ways, the principles that underpin DevOps.\n\nIn the following chapters, the principles for each of the Three Ways are described in greater detail. The first of these principles is Flow, which is focused on how we create the fast flow of work in any value stream, whether it’s in manufacturing or technology\n\nwork. The practices that enable fast flow are described in Part III.\n\n*** Going forward, engineer refers to anyone working in our value stream, not just developers.\n\n††† In fact, with techniques such as test-driven development, testing occurs even before the first line of code is\n\nwritten.\n\n‡‡‡ In this book, the term process time will be favored for the same reason Karen Martin and Mike Osterling cite: “To minimize confusion, we avoid using the term cycle time as it has several definitions synonymous with processing time and pace or frequency of output, to name a few.”",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "2 The First Way:\n\nThe Principles of Flow\n\nIn the technology value stream, work typically flows from\n\nDevelopment to Operations, the functional areas between our business and our customers. The First Way requires the fast and\n\nsmooth flow of work from Development to Operations, to deliver value to customers quickly. We optimize for this global goal\n\ninstead of local goals, such as Development feature completion\n\nrates, test find/fix ratios, or Ops availability measures.\n\nWe increase flow by making work visible, by reducing batch sizes\n\nand intervals of work, and by building quality in, preventing\n\ndefects from being passed to downstream work centers. By speeding up the flow through the technology value stream, we\n\nreduce the lead time required to fulfill internal and external customer requests, further increasing the quality of our work\n\nwhile making us more agile and able to out-experiment the competition.\n\nOur goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services. Clues on how we do this in the",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "technology value stream can be gleaned from how the Lean\n\nprinciples were applied to the manufacturing value stream.\n\nMAKE OUR WORK VISIBLE\n\nA significant difference between technology and manufacturing\n\nvalue streams is that our work is invisible. Unlike physical\n\nprocesses, in the technology value stream we cannot easily see\n\nwhere flow is being impeded or when work is piling up in front of\n\nconstrained work centers. Transferring work between work\n\ncenters is usually highly visible and slow because inventory must\n\nbe physically moved.\n\nHowever, in technology work the move can be done with a click of\n\na button, such as by re-assigning a work ticket to another team. Because it is so easy, work can bounce between teams endlessly due to incomplete information, or work can be passed onto downstream work centers with problems that remain completely invisible until we are late delivering what we promised to the customer or our application fails in the production environment.\n\nTo help us see where work is flowing well and where work is queued or stalled, we need to make our work as visible as possible.\n\nOne of the best methods of doing this is using visual work boards, such as kanban boards or sprint planning boards, where we can represent work on physical or electronic cards. Work originates on the left (often being pulled from a backlog), is pulled from work center to work center (represented in columns), and finishes when it reaches the right side of the board, usually in a column labeled “done” or “in production.”",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Figure 6: An example kanban board, spanning Requirements, Dev, Test, Staging, and In Production (Source: David J. Andersen and Dominica DeGrandis, Kanban for ITOps, training materials for workshop, 2012.)\n\nNot only does our work become visible, we can also manage our work so that it flows from left to right as quickly as possible. Furthermore, we can measure lead time from when a card is placed on the board to when it is moved into the “Done” column.\n\nIdeally, our kanban board will span the entire value stream, defining work as completed only when it reaches the right side of the board (figure 6). Work is not done when Development\n\ncompletes the implementation of a feature—rather, it is only done when our application is running successfully in production, delivering value to the customer.\n\nBy putting all work for each work center in queues and making it visible, all stakeholders can more easily prioritize work in the context of global goals. Doing this enables each work center to\n\nsingle-task on the highest priority work until it is completed, increasing throughput.",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "LIMIT WORK IN PROCESS (WIP)\n\nIn manufacturing, daily work is typically dictated by a production schedule that is generated regularly (e.g., daily, weekly),\n\nestablishing which jobs must be run based on customer orders, order due dates, parts available, and so forth.\n\nIn technology, our work is usually far more dynamic—this is\n\nespecially the case in shared services, where teams must satisfy\n\nthe demands of many different stakeholders. As a result, daily work becomes dominated by the priority du jour, often with\n\nrequests for urgent work coming in through every communication mechanism possible, including ticketing systems, outage calls,\n\nemails, phone calls, chat rooms, and management escalations.\n\nDisruptions in manufacturing are also highly visible and costly, often requiring breaking the current job and scrapping any\n\nincomplete work in process to start the new job. This high level of\n\neffort discourages frequent disruptions.\n\nHowever, interrupting technology workers is easy, because the consequences are invisible to almost everyone, even though the\n\nnegative impact to productivity may be far greater than in manufacturing. For instance, an engineer assigned to multiple\n\nprojects must switch between tasks, incurring all the costs of having to re-establish context, as well as cognitive rules and goals.\n\nStudies have shown that the time to complete even simple tasks,\n\nsuch as sorting geometric shapes, significantly degrades when\n\nmultitasking. Of course, because our work in the technology value stream is far more cognitively complex than sorting geometric\n\nshapes, the effects of multitasking on process time is much worse.",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "We can limit multitasking when we use a kanban board to manage our work, such as by codifying and enforcing WIP (work in\n\nprogress) limits for each column or work center that puts an upper limit on the number of cards that can be in a column.\n\nFor example, we may set a WIP limit of three cards for testing.\n\nWhen there are already three cards in the test lane, no new cards can be added to the lane unless a card is completed or removed\n\nfrom the “in work” column and put back into queue (i.e., putting\n\nthe card back to the column to the left). Nothing can can be worked on until it is represented first in a work card, reinforcing\n\nthat all work must be made visible.\n\nDominica DeGrandis, one of the leading experts on using kanbans in DevOps value streams, notes that “controlling queue size [WIP]\n\nis an extremely powerful management tool, as it is one of the few leading indicators of lead time—with most work items, we don’t\n\nknow how long it will take until it’s actually completed.”\n\nLimiting WIP also makes it easier to see problems that prevent the completion of work.† For instance, when we limit WIP, we find that we may have nothing to do because we are waiting on\n\nsomeone else. Although it may be tempting to start new work (i.e., “It’s better to be doing something than nothing”), a far better\n\naction would be to find out what is causing the delay and help fix that problem. Bad multitasking often occurs when people are\n\nassigned to multiple projects, resulting in many prioritization\n\nproblems.\n\nIn other words, as David J. Andersen, author of Kanban: Successful Evolutionary Change for Your Technology Business,\n\nquipped, “Stop starting. Start finishing.”",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "REDUCE BATCH SIZES\n\nAnother key component to creating smooth and fast flow is\n\nperforming work in small batch sizes. Prior to the Lean\n\nmanufacturing revolution, it was common practice to manufacture in large batch sizes (or lot sizes), especially for operations where\n\njob setup or switching between jobs was time-consuming or costly. For example, producing large car body panels requires setting\n\nlarge and heavy dies onto metal stamping machines, a process that could take days. When changeover cost is so expensive, we would\n\noften stamp as many panels at a time as possible, creating large\n\nbatches in order to reduce the number of changeovers.\n\nHowever, large batch sizes result in skyrocketing levels of WIP and high levels of variability in flow that cascade through the entire\n\nmanufacturing plant. The result is long lead times and poor quality—if a problem is found in one body panel, the entire batch\n\nhas to be scrapped.\n\nOne of the key lessons in Lean is that in order to shrink lead times and increase quality, we must strive to continually shrink batch\n\nsizes. The theoretical lower limit for batch size is single-piece flow, where each operation is performed one unit at a time.‡\n\nThe dramatic differences between large and small batch sizes can be seen in the simple newsletter mailing simulation described in\n\nLean Thinking: Banish Waste and Create Wealth in Your Corporation by James P. Womack and Daniel T. Jones.\n\nSuppose in our own example we have ten brochures to send and mailing each brochure requires four steps: fold the paper, insert",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "the paper into the envelope, seal the envelope, and stamp the envelope.\n\nThe large batch strategy (i.e., “mass production”) would be to sequentially perform one operation on each of the ten brochures. In other words, we would first fold all ten sheets of paper, then\n\ninsert each of them into envelopes, then seal all ten envelopes, and then stamp them.\n\nOn the other hand, in the small batch strategy (i.e., “single-piece flow”), all the steps required to complete each brochure are performed sequentially before starting on the next brochure. In other words, we fold one sheet of paper, insert it into the envelope,\n\nseal it, and stamp it—only then do we start the process over with the next sheet of paper.\n\nThe difference between using large and small batch sizes is\n\ndramatic (see figure 7). Suppose each of the four operations takes ten seconds for each of the ten envelopes. With the large batch size strategy, the first completed and stamped envelope is produced\n\nonly after 310 seconds.\n\nWorse, suppose we discover during the envelope sealing operation that we made an error in the first step of folding—in this case, the\n\nearliest we would discover the error is at two hundred seconds, and we have to refold and reinsert all ten brochures in our batch again.",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Figure 7: Simulation of “envelope game” (fold, insert, seal, and stamp the envelope)\n\n(Source: Stefan Luyten, “Single Piece Flow: Why mass production isn’t the most efficient way of doing ‘stuff’,” Medium.com, August 8, 2014, https://medium.com/@stefanluyten/single-piece-flow- 5d2c2bec845b#.9o7sn74ns.)\n\nIn contrast, in the small batch strategy the first completed stamped envelope is produced in only forty seconds, eight times faster than the large batch strategy. And, if we made an error in the first step, we only have to redo the one brochure in our batch.\n\nSmall batch sizes result in less WIP, faster lead times, faster detection of errors, and less rework.\n\nThe negative outcomes associated with large batch sizes are just as relevant to the technology value stream as in manufacturing. Consider when we have an annual schedule for software releases, where an entire year’s worth of code that Development has worked\n\non is released to production deployment.\n\nLike in manufacturing, this large batch release creates sudden, high levels of WIP and massive disruptions to all downstream\n\nwork centers, resulting in poor flow and poor quality outcomes. This validates our common experience that the larger the change going into production, the more difficult the production errors are\n\nto diagnose and fix, and the longer they take to remediate.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "In a post on Startup Lessons Learned, Eric Ries states, “The batch size is the unit at which work-products move between stages in a\n\ndevelopment [or DevOps] process. For software, the easiest batch to see is code. Every time an engineer checks in code, they are batching up a certain amount of work. There are many techniques for controlling these batches, ranging from the tiny batches\n\nneeded for continuous deployment to more traditional branch- based development, where all of the code from multiple developers working for weeks or months is batched up and\n\nintegrated together.”\n\nThe equivalent to single piece flow in the technology value stream is realized with continuous deployment, where each change\n\ncommitted to version control is integrated, tested, and deployed into production. The practices that enable this are described in Part IV.\n\nREDUCE THE NUMBER OF HANDOFFS\n\nIn the technology value stream, whenever we have long\n\ndeployment lead times measured in months, it is often because there are hundreds (or even thousands) of operations required to move our code from version control into the production environment. To transmit code through the value stream requires\n\nmultiple departments to work on a variety of tasks, including functional testing, integration testing, environment creation, server administration, storage administration, networking, load\n\nbalancing, and information security.\n\nEach time the work passes from team to team, we require all sorts of communication: requesting, specifying, signaling, coordinating,",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "and often prioritizing, scheduling, deconflicting, testing, and verifying. This may require using different ticketing or project management systems; writing technical specification documents;\n\ncommunicating via meetings, emails, or phone calls; and using file system shares, FTP servers, and Wiki pages.\n\nEach of these steps is a potential queue where work will wait when we rely on resources that are shared between different value streams (e.g., centralized operations). The lead times for these requests are often so long that there is constant escalation to have\n\nwork performed within the needed timelines.\n\nEven under the best circumstances, some knowledge is inevitably lost with each handoff. With enough handoffs, the work can\n\ncompletely lose the context of the problem being solved or the organizational goal being supported. For instance, a server administrator may see a newly created ticket requesting that user\n\naccounts be created, without knowing what application or service it’s for, why it needs to be created, what all the dependencies are, or whether it’s actually recurring work.\n\nTo mitigate these types of problems, we strive to reduce the number of handoffs, either by automating significant portions of the work or by reorg-anizing teams so they can deliver value to the\n\ncustomer themselves, instead of having to be constantly dependent on others. As a result, we increase flow by reducing the amount of time that our work spends waiting in queue, as well as the amount of non–value-added time. See Appendix 4.\n\nCONTINUALLY IDENTIFY AND ELEVATE OUR CONSTRAINTS",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "To reduce lead times and increase throughput, we need to continually identify our system’s constraints and improve its work capacity. In Beyond the Goal, Dr. Goldratt states, “In any value\n\nstream, there is always a direction of flow, and there is always one and only constraint; any improvement not made at that constraint is an illusion.” If we improve a work center that is positioned\n\nbefore the constraint, work will merely pile up at the bottleneck even faster, waiting for work to be performed by the bottlenecked work center.\n\nOn the other hand, if we improve a work center positioned after the bottleneck, it remains starved, waiting for work to clear the bottleneck. As a solution, Dr. Goldratt defined the “five focusing steps”:\n\nIdentify the system’s constraint.\n\nDecide how to exploit the system’s constraint.\n\nSubordinate everything else to the above decisions.\n\nElevate the system’s constraint.\n\nIf in the previous steps a constraint has been broken, go back to step one, but do not allow inertia to cause a system constraint.\n\nIn typical DevOps transformations, as we progress from deployment lead times measured in months or quarters to lead\n\ntimes measured in minutes, the constraint usually follows this progression:",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Environment creation: We cannot achieve deployments on- demand if we always have to wait weeks or months for production or test environments. The countermeasure is to\n\ncreate environments that are on demand and completely self- serviced, so that they are always available when we need them.\n\nCode deployment: We cannot achieve deployments on\n\ndemand if each of our production code deployments take weeks or months to perform (i.e., each deployment requires 1,300 manual, error-prone steps, involving up to three hundred\n\nengineers). The countermeasure is to automate our deployments as much as possible, with the goal of being completely automated so they can be done self-service by any developer.\n\nTest setup and run: We cannot achieve deployments on demand if every code deployment requires two weeks to set up our test environments and data sets, and another four weeks to\n\nmanually execute all our regression tests. The countermeasure is to automate our tests so we can execute deployments safely and to parallelize them so the test rate can keep up with our\n\ncode development rate.\n\nOverly tight architecture: We cannot achieve deployments on demand if overly tight architecture means that every time\n\nwe want to make a code change we have to send our engineers to scores of committee meetings in order to get permission to\n\nmake our changes. Our countermeasure is to create more\n\nloosely-coupled architecture so that changes can be made safely and with more autonomy, increasing developer\n\nproductivity.",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "After all these constraints have been broken, our constraint will\n\nlikely be Development or the product owners. Because our goal is\n\nto enable small teams of developers to independently develop, test, and deploy value to customers quickly and reliably, this is\n\nwhere we want our constraint to be. High performers, regardless\n\nof whether an engineer is in Development, QA, Ops, or Infosec, state that their goal is to help maximize developer productivity.\n\nWhen the constraint is here, we are limited only by the number of\n\ngood business hypotheses we create and our ability to develop the code necessary to test these hypotheses with real customers.\n\nThe progression of constraints listed above are generalizations of\n\ntypical transformations—techniques to identify the constraint in actual value streams, such as through value stream mapping and\n\nmeasurements, are described later in this book.\n\nELIMINATE HARDSHIPS AND WASTE IN THE VALUE STREAM\n\nShigeo Shingo, one of the pioneers of the Toyota Production System, believed that waste constituted the largest threat to\n\nbusiness viability—the commonly used definition in Lean is “the\n\nuse of any material or resource beyond what the customer requires and is willing to pay for.” He defined seven major types of\n\nmanufacturing waste: inventory, overproduction, extra\n\nprocessing, transportation, waiting, motion, and defects.\n\nMore modern interpretations of Lean have noted that “eliminating\n\nwaste” can have a demeaning and dehumanizing context; instead,\n\nthe goal is reframed to reduce hardship and drudgery in our daily",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "work through continual learning in order to achieve the\n\norganization’s goals. For the remainder of this book, the term\n\nwaste will imply this more modern definition, as it more closely matches the DevOps ideals and desired outcomes.\n\nIn the book Implementing Lean Software Development: From\n\nConcept to Cash, Mary and Tom Poppendieck describe waste and hardship in the software development stream as anything that\n\ncauses delay for the customer, such as activities that can be\n\nbypassed without affecting the result.\n\nThe following categories of waste and hardship come from\n\nImplementing Lean Software Development unless otherwise\n\nnoted:\n\nPartially done work: This includes any work in the value\n\nstream that has not been completed (e.g., requirement\n\ndocuments or change orders not yet reviewed) and work that is sitting in queue (e.g., waiting for QA review or server admin\n\nticket). Partially done work becomes obsolete and loses value\n\nas time progresses.\n\nExtra processes: Any additional work that is being\n\nperformed in a process that does not add value to the\n\ncustomer. This may include documentation not used in a downstream work center, or reviews or approvals that do not\n\nadd value to the output. Extra processes add effort and increase lead times.\n\nExtra features: Features built into the service that are not\n\nneeded by the organization or the customer (e.g., “gold",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "plating”). Extra features add complexity and effort to testing\n\nand managing functionality.\n\nTask switching: When people are assigned to multiple projects and value streams, requiring them to context switch\n\nand manage dependencies between work, adding additional\n\neffort and time into the value stream.\n\nWaiting: Any delays between work requiring resources to wait\n\nuntil they can complete the current work. Delays increase cycle\n\ntime and prevent the customer from getting value.\n\nMotion: The amount of effort to move information or\n\nmaterials from one work center to another. Motion waste can\n\nbe created when people who need to communicate frequently are not colocated. Handoffs also create motion waste and often\n\nrequire additional communication to resolve ambiguities.\n\nDefects: Incorrect, missing, or unclear information, materials, or products create waste, as effort is needed to resolve these\n\nissues. The longer the time between defect creation and defect\n\ndetection, the more difficult it is to resolve the defect.\n\nNonstandard or manual work: Reliance on nonstandard\n\nor manual work from others, such as using non-rebuilding\n\nservers, test environments, and configurations. Ideally, any dependencies on Operations should be automated, self-\n\nserviced, and available on demand.\n\nHeroics: In order for an organization to achieve goals, individuals and teams are put in a position where they must\n\nperform unreasonable acts, which may even become a part of\n\ntheir daily work (e.g., nightly 2:00 a.m. problems in",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "production, creating hundreds of work tickets as part of every software release).§\n\nOur goal is to make these wastes and hardships—anywhere heroics become necessary—visible, and to systematically do what\n\nis needed to alleviate or eliminate these burdens and hardships to\n\nachieve our goal of fast flow.\n\nCONCLUSION\n\nImproving flow through the technology value stream is essential to\n\nachieving DevOps outcomes. We do this by making work visible, limiting WIP, reducing batch sizes and the number of handoffs,\n\ncontinually identifying and evaluating our constraints, and\n\neliminating hardships in our daily work.\n\nThe specific practices that enable fast flow in the DevOps value\n\nstream are presented in Part IV. In the next chapter, we present\n\nThe Second Way: The Principles of Feedback.\n\n† Taiichi Ohno compared enforcing WIP limits to draining water from the river of inventory in order to reveal all\n\nthe problems that obstruct fast flow.\n\n‡ Also known as “batch size of one” or “1x1 flow,” terms that refer to batch size and a WIP limit of one.\n\n§ Although heroics is not included in the Poppendieck categories of waste, it is included here because of how often\n\nit occurs, especially in Operation shared services.",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "3 The Second Way:\n\nThe Principles of Feedback\n\nWhile the First Way describes the principles that enable the fast\n\nflow of work from left to right, the Second Way describes the principles that enable the reciprocal fast and constant feedback\n\nfrom right to left at all stages of the value stream. Our goal is to create an ever safer and more resilient system of work.\n\nThis is especially important when working in complex systems,\n\nwhen the earliest opportunity to detect and correct errors is typically when a catastrophic event is underway, such as a\n\nmanufacturing worker being hurt on the job or a nuclear reactor\n\nmeltdown in progress.\n\nIn technology, our work happens almost entirely within complex systems with a high risk of catastrophic consequences. As in\n\nmanufacturing, we often discover problems only when large failures are underway, such as a massive production outage or a security breach resulting in the theft of customer data.\n\nWe make our system of work safer by creating fast, frequent, high quality information flow throughout our value stream and our\n\norganization, which includes feedback and feedforward loops. This",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "allows us to detect and remediate problems while they are smaller,\n\ncheaper, and easier to fix; avert problems before they cause\n\ncatastrophe; and create organizational learning that we integrate\n\ninto future work. When failures and accidents occur, we treat\n\nthem as opportunities for learning, as opposed to a cause for\n\npunishment and blame. To achieve all of the above, let us first\n\nexplore the nature of complex systems and how they can be made\n\nsafer.\n\nWORKING SAFELY WITHIN COMPLEX SYSTEMS\n\nOne of the defining characteristics of a complex system is that it\n\ndefies any single person’s ability to see the system as a whole and\n\nunderstand how all the pieces fit together. Complex systems typically have a high degree of interconnectedness of tightly- coupled components, and system-level behavior cannot be explained merely in terms of the behavior of the system components.\n\nDr. Charles Perrow studied the Three Mile Island crisis and observed that it was impossible for anyone to understand how the\n\nreactor would behave in all circumstances and how it might fail. When a problem was underway in one component, it was difficult\n\nto isolate from the other components, quickly flowing through the paths of least resistance in unpredictable ways.\n\nDr. Sidney Dekker, who also codified some of the key elements of safety culture, observed another characteristic of complex systems: doing the same thing twice will not predictably or necessarily lead to the same result. It is this characteristic that",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "makes static checklists and best practices, while valuable,\n\ninsufficient to prevent catastrophes from occurring. See Appendix\n\n5.\n\nTherefore, because failure is inherent and inevitable in complex\n\nsystems, we must design a safe system of work, whether in\n\nmanufacturing or technology, where we can perform work without\n\nfear, confident that any errors will be detected quickly, long before\n\nthey cause catastrophic outcomes, such as worker injury, product\n\ndefects, or negative customer impact.\n\nAfter he decoded the causal mechanism behind the Toyota\n\nProduct System as part of his doctoral thesis at Harvard Business School, Dr. Steven Spear stated that designing perfectly safe systems is likely beyond our abilities, but we can make it safer to work in complex systems when the four following conditions are met:†\n\nComplex work is managed so that problems in design and\n\noperations are revealed\n\nProblems are swarmed and solved, resulting in quick\n\nconstruction of new knowledge\n\nNew local knowledge is exploited globally throughout the organization\n\nLeaders create other leaders who continually grow these types of capabilities\n\nEach of these capabilities are required to work safely in a complex system. In the next sections, the first two capabilities and their\n\nimportance are described, as well as how they have been created",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "in other domains and what practices enable them in the technology value stream. (The third and fourth capabilities are\n\ndescribed in chapter 4.)\n\nSEE PROBLEMS AS THEY OCCUR\n\nIn a safe system of work, we must constantly test our design and\n\noperating assumptions. Our goal is to increase information flow in our system from as many areas as possible, sooner, faster,\n\ncheaper, and with as much clarity between cause and effect as possible. The more assumptions we can invalidate, the faster we\n\ncan find and fix problems, increasing our resilience, agility, and\n\nability to learn and innovate.\n\nWe do this by creating feedback and feedforward loops into our\n\nsystem of work. Dr. Peter Senge in his book The Fifth Discipline:\n\nThe Art & Practice of the Learning Organization described feedback loops as a critical part of learning organizations and\n\nsystems thinking. Feedback and feedforward loops cause components within a system to reinforce or counteract each other.\n\nIn manufacturing, the absence of effective feedback often\n\ncontribute to major quality and safety problems. In one well- documented case at the General Motors Fremont manufacturing\n\nplant, there were no effective procedures in place to detect\n\nproblems during the assembly process, nor were there explicit procedures on what to do when problems were found. As a result,\n\nthere were instances of engines being put in backward, cars missing steering wheels or tires, and cars even having to be towed\n\noff the assembly line because they wouldn’t start.",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "In contrast, in high-performing manufacturing operations there is fast, frequent, and high quality information flow throughout the\n\nentire value stream—every work operation is measured and monitored, and any defects or significant deviations are quickly\n\nfound and acted upon. These are the foundation of what enables quality, safety, and continual learning and improvement.\n\nIn the technology value stream, we often get poor outcomes\n\nbecause of the absence of fast feedback. For instance, in a waterfall\n\nsoftware project, we may develop code for an entire year and get no feedback on quality until we begin the testing phase—or worse,\n\nwhen we release our software to customers. When feedback is this delayed and infrequent, it is too slow to enable us to prevent\n\nundesirable outcomes.\n\nIn contrast, our goal is to create fast feedback and fastforward loops wherever work is performed, at all stages of the technology\n\nvalue stream, encompassing Product Management, Development,\n\nQA, Infosec, and Operations. This includes the creation of automated build, integration, and test processes, so that we can\n\nimmediately detect when a change has been introduced that takes us out of a correctly functioning and deployable state.\n\nWe also create pervasive telemetry so we can see how all our\n\nsystem components are operating in the production environment, so that we can quickly detect when they are not operating as\n\nexpected. Telemetry also allows us to measure whether we are\n\nachieving our intended goals and, ideally, is radiated to the entire value stream so we can see how our actions affect other portions of\n\nthe system as a whole.",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Feedback loops not only enable quick detection and recovery of\n\nproblems, but they also inform us on how to prevent these\n\nproblems from occurring again in the future. Doing this increases the quality and safety of our system of work, and creates\n\norganizational learning.\n\nAs Elisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of Explore It!: Reduce Risk and Increase\n\nConfidence with Exploratory Testing, said, “When I headed up quality engineering, I described my job as ‘creating feedback\n\ncycles.’ Feedback is critical because it is what allows us to steer.\n\nWe must constantly validate between customer needs, our intentions and our implementations. Testing is merely one sort of\n\nfeedback.”\n\nSWARM AND SOLVE PROBLEMS TO BUILD NEW KNOWLEDGE\n\nObviously, it is not sufficient to merely detect when the unexpected occurs. When problems occur, we must swarm them,\n\nmobilizing whoever is required to solve the problem.\n\nAccording to Dr. Spear, the goal of swarming is to contain problems before they have a chance to spread, and to diagnose\n\nand treat the problem so that it cannot recur. “In doing so,” he\n\nsays, “they build ever-deeper knowledge about how to manage the systems for doing our work, converting inevitable up-front\n\nignorance into knowledge.”\n\nThe paragon of this principle is the Toyota Andon cord. In da\n\nToyota manufacturing plant, above every work center is a cord",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "that every worker and manager is trained to pull when something goes wrong; for example, when a part is defective, when a required\n\npart is not available, or even when work takes longer than documented.‡\n\nWhen the Andon cord is pulled, the team leader is alerted and\n\nimmediately works to resolve the problem. If the problem cannot be resolved within a specified time (e.g., fifty-five seconds), the production line is halted so that the entire organization can be\n\nmobilized to assist with problem resolution until a successful countermeasure has been developed.\n\nInstead of working around the problem or scheduling a fix “when\n\nwe have more time,” we swarm to fix it immediately—this is nearly the opposite of the behavior at the GM Fremont plant described earlier. Swarming is necessary for the following reasons:\n\nIt prevents the problem from progressing downstream, where the cost and effort to repair it increases exponentially and technical debt is allowed to accumulate.\n\nIt prevents the work center from starting new work, which will likely introduce new errors into the system.\n\nIf the problem is not addressed, the work center could potentially have the same problem in the next operation (e.g., fifty-five seconds later), requiring more fixes and work. See Appendix 6.\n\nThis practice of swarming seems contrary to common management practice, as we are deliberately allowing a local problem to disrupt operations globally. However, swarming\n\nenables learning. It prevents the loss of critical information due to",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "fading memories or changing circumstances. This is especially critical in complex systems, where many problems occur because\n\nof some unexpected, idiosyncratic interaction of people, processes, products, places, and circumstances—as time passes, it becomes impossible to reconstruct exactly what was going on when the\n\nproblem occurred.\n\nAs Dr. Spear notes, swarming is part of the “disciplined cycle of real-time problem recognition, diagnosis,...and treatment\n\n(countermeasures or corrective measures in manufacturing vernacular). It [is] the discipline of the Shewhart cycle—plan, do, check, act—popularized by W. Edwards Deming, but accelerated to warp speed.”\n\nIt is only through the swarming of ever smaller problems discovered ever earlier in the life cycle that we can deflect problems before a catastrophe occurs. In other words, when the\n\nnuclear reactor melts down, it is already too late to avert worst outcomes.\n\nTo enable fast feedback in the technology value stream, we must create the equivalent of an Andon cord and the related swarming response. This requires that we also create the culture that makes it safe, and even encouraged, to pull the Andon cord when\n\nsomething goes wrong, whether it is when a production incident occurs or when errors occur earlier in the value stream, such as when someone introduces a change that breaks our continuous\n\nbuild or test processes.\n\nWhen conditions trigger an Andon cord pull, we swarm to solve the problem and prevent the introduction of new work until the issue has been resolved.§ This provides fast feedback for everyone",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "in the value stream (especially the person who caused the system to fail), enables us to quickly isolate and diagnose the problem,\n\nand prevents further complicating factors that can obscure cause and effect.\n\nPreventing the introduction of new work enables continuous\n\nintegration and deployment, which is single-piece flow in the technology value stream. All changes that pass our continuous build and integration tests are deployed into production, and any changes that cause any tests to fail trigger our Andon cord and are\n\nswarmed until resolved.\n\nKEEP PUSHING QUALITY CLOSER TO THE SOURCE\n\nWe may inadvertently perpetuate unsafe systems of work due to\n\nthe way we respond to accidents and incidents. In complex systems, adding more inspection steps and approval processes actually increases the likelihood of future failures. The effectiveness of approval processes decreases as we push decision-\n\nmaking further away from where the work is performed. Doing so not only lowers the quality of decisions but also increases our cycle time, thus decreasing the strength of the feedback between cause\n\nand effect, and reducing our ability to learn from successes and failures.¶\n\nThis can be seen even in smaller and less complex systems. When\n\ntop-down, bureaucratic command and control systems become ineffective, it is usually because the variance between “who should do something” and “who is actually doing something” is too large,\n\ndue to insufficient clarity and timeliness.",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Examples of ineffective quality controls include:\n\nRequiring another team to complete tedious, error-prone, and\n\nmanual tasks that could be easily automated and run as needed by the team who needs the work performed\n\nRequiring approvals from busy people who are distant from the\n\nwork, forcing them to make decisions without an adequate knowledge of the work or the potential implications, or to merely rubber stamp their approvals\n\nCreating large volumes of documentation of questionable detail which become obsolete shortly after they are written\n\nPushing large batches of work to teams and special committees for approval and processing and then waiting for responses\n\nInstead, we need everyone in our value stream to find and fix\n\nproblems in their area of control as part of our daily work. By doing this, we push quality and safety responsibilities and decision-making to where the work is performed, instead of relying on approvals from distant executives.\n\nWe use peer reviews of our proposed changes to gain whatever assurance is needed that our changes will operate as designed. We\n\nautomate as much of the quality checking typically performed by a QA or Information Security department as possible. Instead of developers needing to request or schedule a test to be run, these tests can be performed on demand, enabling developers to quickly\n\ntest their own code and even deploy those changes into production themselves.",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "By doing this, we truly make quality everyone’s responsibility as opposed to it being the sole responsibility of a separate department. Information security is not just Information\n\nSecurity’s job, just as availability isn’t merely the job of Operations.\n\nHaving developers share responsibility for the quality of the\n\nsystems they build not only improves outcomes but also accelerates learning. This is especially important for developers as they are typically the team that is furthest removed from the\n\ncustomer. Gary Gruver observes, “It’s impossible for a developer to learn anything when someone yells at them for something they broke six months ago—that’s why we need to provide feedback to everyone as quickly as possible, in minutes, not months.”\n\nENABLE OPTIMIZING FOR DOWNSTREAM WORK CENTERS\n\nIn the 1980s, Designing for Manufacturability principles sought to design parts and processes so that finished goods could be created\n\nwith the lowest cost, highest quality, and fastest flow. Examples include designing parts that are wildly asymmetrical to prevent them from being put on backwards, and designing screw fasteners so that they are impossible to over-tighten.\n\nThis was a departure from how design was typically done, which focused on the external customers but overlooked internal stakeholders, such as the people performing the manufacturing.\n\nLean defines two types of customers that we must design for: the external customer (who most likely pays for the service we are",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "delivering) and the internal customer (who receives and processes the work immediately after us). According to Lean, our most important customer is our next step downstream. Optimizing our\n\nwork for them requires that we have empathy for their problems in order to better identify the design problems that prevent fast and smooth flow.\n\nIn the technology value stream, we optimize for downstream work centers by designing for operations, where operational non- functional requirements (e.g., architecture, performance, stability,\n\ntestability, configurability, and security) are prioritized as highly as user features.\n\nBy doing this, we create quality at the source, likely resulting in a\n\nset of codified non-functional requirements that we can proactively integrate into every service we build.\n\nCONCLUSION\n\nCreating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream. We do this by seeing\n\nproblems as they occur, swarming and solving problems to build new knowledge, pushing quality closer to the source, and continually optimizing for downstream work centers.\n\nThe specific practices that enable fast flow in the DevOps value stream are presented in Part IV. In the next chapter, we present\n\nthe Third Way: The Principles of Feedback\n\n† Dr. Spear extended his work to explain the long-lasting successes of other organizations, such as the Toyota\n\nsupplier network, Alcoa, and the US Navy’s Nuclear Power Propulsion Program.\n\n‡ In some of its plants, Toyota has moved to using an Andon button.",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "§ Astonishingly, when the number of Andon cord pulls drop, plant managers will actually decrease the tolerances\n\nto get an increase in the number of Andon cord pulls in order to continue to enable more learnings and improvements and to detect ever-weaker failure signals.\n\n¶ In the 1700s, the British government engaged in a spectacular example of top-down, bureaucratic command and control, which proved remarkably ineffective. At the time, Georgia was still a colony, and despite the fact that the British government was three thousand miles away and lacked firsthand knowledge of local land chemistry, rockiness, topography, accessibility to water, and other conditions, it tried to plan Georgia’s entire agricultural economy. The results of the attempt were dismal and left Georgia with the lowest levels of prosperity and population in the thirteen colonies.",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "4 The Third Way:\n\nThe Principles of Continual Learning\n\nand Experimentation\n\nWhile the First Way addresses work flow from left to right and the\n\nSecond Way addresses the reciprocal fast and constant feedback\n\nfrom right to left, the Third Way focuses on creating a culture of continual learning and experimentation. These are the principles\n\nthat enable constant creation of individual knowledge, which is then turned into team and organizational knowledge.\n\nIn manufacturing operations with systemic quality and safety\n\nproblems, work is typically rigidly defined and enforced. For instance, in the GM Fremont plant described in the previous\n\nchapter, workers had little ability to integrate improvements and\n\nlearnings into their daily work, with suggestions for improvement\n\n“apt to meet a brick wall of indifference.”\n\nIn these environments, there is also often a culture of fear and low\n\ntrust, where workers who make mistakes are punished, and those who make suggestions or point out problems are viewed as whistle-blowers and troublemakers. When this occurs, leadership\n\nis actively suppressing, even punishing, learning and improvement, perpetuating quality and safety problems.",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "In contrast, high-performing manufacturing operations require\n\nand actively promote learning—instead of work being rigidly\n\ndefined, the system of work is dynamic, with line workers\n\nperforming experiments in their daily work to generate new\n\nimprovements, enabled by rigorous standardization of work\n\nprocedures and documentation of the results.\n\nIn the technology value stream, our goal is to create a high-trust\n\nculture, reinforcing that we are all lifelong learners who must take\n\nrisks in our daily work. By applying a scientific approach to both\n\nprocess improvement and product development, we learn from\n\nour successes and failures, identifying which ideas don’t work and reinforcing those that do. Moreover, any local learnings are rapidly turned into global improvements, so that new techniques\n\nand practices can be used by the entire organization.\n\nWe reserve time for the improvement of daily work and to further accelerate and ensure learning. We consistently introduce stress into our systems to force continual improvement. We even simulate and inject failures in our production services under controlled conditions to increase our resilience.\n\nBy creating this continual and dynamic system of learning, we\n\nenable teams to rapidly and automatically adapt to an ever-\n\nchanging environment, which ultimately helps us win in the marketplace.\n\nENABLING ORGANIZATIONAL LEARNING AND A SAFETY CULTURE",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "When we work within a complex system, by definition it is\n\nimpossible for us to perfectly predict all the outcomes for any\n\naction we take. This is what contributes to unexpected, or even\n\ncatastrophic, outcomes and accidents in our daily work, even\n\nwhen we take precautions and work carefully.\n\nWhen these accidents affect our customers, we seek to understand\n\nwhy it happened. The root cause is often deemed to be human\n\nerror, and the all too common management response is to “name, blame, and shame” the person who caused the problem.† And, either subtly or explicitly, management hints that the person\n\nguilty of committing the error will be punished. They then create more processes and approvals to prevent the error from happening again.\n\nDr. Sidney Dekker, who codified some of the key elements of safety culture and coined the term just culture, wrote, “Responses\n\nto incidents and accidents that are seen as unjust can impede safety investigations, promote fear rather than mindfulness in people who do safety-critical work, make organizations more bureaucratic rather than more careful, and cultivate professional secrecy, evasion, and self-protection.”\n\nThese issues are especially problematic in the technology value stream—our work is almost always performed within a complex system, and how management chooses to react to failures and\n\naccidents leads to a culture of fear, which then makes it unlikely that problems and failure signals are ever reported. The result is that problems remain hidden until a catastrophe occurs.\n\nDr. Ron Westrum was one of the first to observe the importance of\n\norganizational culture on safety and performance. He observed",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "that in healthcare organizations, the presence of “generative” cultures was one of the top predictors of patient safety. Dr.\n\nWestrum defined three types of culture:\n\nPathological organizations are characterized by large amounts of fear and threat. People often hoard information, withhold it\n\nfor political reasons, or distort it to make themselves look\n\nbetter. Failure is often hidden.\n\nBureaucratic organizations are characterized by rules and processes, often to help individual departments maintain their\n\n“turf.” Failure is processed through a system of judgment, resulting in either punishment or justice and mercy.\n\nGenerative organizations are characterized by actively seeking\n\nand sharing information to better enable the organization to achieve its mission. Responsibilities are shared throughout the\n\nvalue stream, and failure results in reflection and genuine\n\ninquiry.\n\nFigure 8: The Westrum organizational typology model: how organizations process information (Source: Ron Westrum, “A typology of organisation culture,” BMJ Quality & Safety 13, no. 2 (2004), doi:10.1136/qshc.2003.009522.)",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Just as Dr. Westrum found in healthcare organizations, a high- trust, generative culture also predicted IT and organizational\n\nperformance in technology value streams.\n\nIn the technology value stream, we establish the foundations of a generative culture by striving to create a safe system of work.\n\nWhen accidents and failures occur, instead of looking for human error, we look for how we can redesign the system to prevent the\n\naccident from happening again.\n\nFor instance, we may conduct a blameless post-mortem after\n\nevery incident to gain the best understanding of how the accident occurred and agree upon what the best countermeasures are to\n\nimprove the system, ideally preventing the problem from occurring again and enabling faster detection and recovery.\n\nBy doing this, we create organizational learning. As Bethany\n\nMacri, an engineer at Etsy who led the creation of the Morgue tool to help with recording of post-mortems, stated, “By removing\n\nblame, you remove fear; by removing fear, you enable honesty;\n\nand honesty enables prevention.”\n\nDr. Spear observes that the result of removing blame and putting organizational learning in its place is that “organizations become\n\never more self-diagnosing and self-improving, skilled at detecting problems [and] solving them.”\n\nMany of these attributes were also described by Dr. Senge as\n\nattributes of learning organizations. In The Fifth Discipline, he wrote that these characteristics help customers, ensure quality,\n\ncreate competitive advantage and an energized and committed workforce, and uncover the truth.",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "INSTITUTIONALIZE THE IMPROVEMENT OF DAILY WORK\n\nTeams are often not able or not willing to improve the processes\n\nthey operate within. The result is not only that they continue to suffer from their current problems, but their suffering also grows\n\nworse over time. Mike Rother observed in Toyota Kata that in the\n\nabsence of improvements, processes don’t stay the same—due to chaos and entropy, processes actually degrade over time.\n\nIn the technology value stream, when we avoid fixing our\n\nproblems, relying on daily workarounds, our problems and technical debt accumulates until all we are doing is performing\n\nworkarounds, trying to avoid disaster, with no cycles leftover for doing productive work. This is why Mike Orzen, author of Lean IT,\n\nobserved, “Even more important than daily work is the\n\nimprovement of daily work.”\n\nWe improve daily work by explicitly reserving time to pay down technical debt, fix defects, and refactor and improve problematic\n\nareas of our code and environments—we do this by reserving cycles in each development interval, or by scheduling kaizen\n\nblitzes, which are periods when engineers self-organize into teams to work on fixing any problem they want.\n\nThe result of these practices is that everyone finds and fixes\n\nproblems in their area of control, all the time, as part of their daily\n\nwork. When we finally fix the daily problems that we’ve worked\n\naround for months (or years), we can eradicate from our system the less obvious problems. By detecting and responding to these",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "ever-weaker failure signals, we fix problems when it is not only easier and cheaper but also when the consequences are smaller.\n\nConsider the following example that improved workplace safety at Alcoa, an aluminum manufacturer with $7.8 billion in revenue in 1987. Aluminum manufacturing requires extremely high heat,\n\nhigh pressures, and corrosive chemicals. In 1987, Alcoa had a frightening safety record, with 2% of the ninety thousand employee workforce being injured each year—that’s seven injuries\n\nper day. When Paul O’Neill started as CEO, his first goal was to have zero injuries to employees, contractors, and visitors.\n\nO’Neill wanted to be notified within twenty-four hours of anyone\n\nbeing injured on the job—not to punish, but to ensure and promote that learnings were being generated and incorporated to create a safer workplace. Over the course of ten years, Alcoa\n\nreduced their injury rate by 95%.\n\nThe reduction in injury rates allowed Alcoa to focus on smaller problems and weaker failure signals—instead of notifying O’Neill\n\nonly when injuries occurred, they started reporting any close calls as well.‡ By doing this, they improved workplace safety over the subsequent twenty years and have one of the most enviable safety records in the industry.\n\nAs Dr. Spear writes, “Alcoans gradually stopped working around the difficulties, inconveniences, and impediments they\n\nexperienced. Coping, fire fighting, and making do were gradually replaced throughout the organization by a dynamic of identifying opportunities for process and product improvement. As those opportunities were identified and the problems were investigated,\n\nthe pockets of ignorance that they reflected were converted into",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "nuggets of knowledge.” This helped give the company a greater competitive advantage in the market.\n\nSimilarly, in the technology value stream, as we make our system of work safer, we find and fix problems from ever weaker failure signals. For example, we may initially perform blameless post-\n\nmortems only for customer-impacting incidents. Over time, we may perform them for lesser team-impacting incidents and near misses as well.\n\nTRANSFORM LOCAL DISCOVERIES INTO GLOBAL IMPROVEMENTS\n\nWhen new learnings are discovered locally, there must also be some mechanism to enable the rest of the organization to use and benefit from that knowledge. In other words, when teams or\n\nindividuals have experiences that create expertise, our goal is to convert that tacit knowledge (i.e., knowledge that is difficult to transfer to another person by means of writing it down or\n\nverbalizing) into explicit, codified knowledge, which becomes someone else’s expertise through practice.\n\nThis ensures that when anyone else does similar work, they do so\n\nwith the cumulative and collective experience of everyone in the organization who has ever done the same work. A remarkable example of turning local knowledge into global knowledge is the US Navy’s Nuclear Power Propulsion Program (also known as\n\n“NR” for “Naval Reactors”), which has over 5,700 reactor-years of operation without a single reactor-related casualty or escape of radiation.",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "The NR is known for their intense commitment to scripted procedures and standardized work, and the need for incident\n\nreports for any departure from procedure or normal operations to accumulate learnings, no matter how minor the failure signal— they constantly update procedures and system designs based on these learnings.\n\nThe result is that when a new crew sets out to sea on their first deployment, they and their officers benefit from the collective knowledge of 5,700 accident-free reactor-years. Equally\n\nimpressive is that their own experiences at sea will be added to this collective knowledge, helping future crews safely achieve their own missions.\n\nIn the technology value stream, we must create similar mechanisms to create global knowledge, such as making all our blameless post-mortem reports searchable by teams trying to\n\nsolve similar problems, and by creating shared source code repositories that span the entire organization, where shared code, libraries, and configurations that embody the best collective\n\nknowledge of the entire organization can be easily utilized. All these mechanisms help convert individual expertise into artifacts that the rest of the organization can use.\n\nINJECT RESILIENCE PATTERNS INTO OUR DAILY WORK\n\nLower-performing manufacturing organizations buffer themselves from disruptions in many ways—in other words, they bulk up or add flab. For instance, to reduce the risk of a work center being\n\nidle (due to inventory arriving late, inventory that had to be",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "scrapped, etc.), managers may choose to stockpile more inventory at each work center. However, that inventory buffer also increases WIP, which has all sorts of undesired outcomes, as previously\n\ndiscussed.\n\nSimilarly, to reduce the risk of a work center going down due to\n\nmachinery failure, managers may increase capacity by buying more capital equipment, hiring more people, or even increasing floor space. All these options increase costs.\n\nIn contrast, high performers achieve the same results (or better) by improving daily operations, continually introducing tension to elevate performance, as well as engineering more resilience into their system.\n\nConsider a typical experiment at one of Aisin Seiki Global’s mattress factories, one of Toyota’s top suppliers. Suppose they had\n\ntwo production lines, each capable of producing one hundred units per day. On slow days, they would send all production onto one line, experimenting with ways to increase capacity and identify vulnerabilities in their process, knowing that if\n\noverloading the line caused it to fail, they could send all production to the second line.\n\nBy relentless and constant experimentation in their daily work,\n\nthey were able to continually increase capacity, often without adding any new equipment or hiring more people. The emergent pattern that results from these types of improvement rituals not\n\nonly improves performance but also improves resilience, because the organization is always in a state of tension and change. This process of applying stress to increase resilience was named antifragility by author and risk analyst Nassim Nicholas Taleb.",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "In the technology value stream, we can introduce the same type of tension into our systems by seeking to always reduce deployment lead times, increase test coverage, decrease test execution times,\n\nand even by re-architecting if necessary to increase developer productivity or increase reliability.\n\nWe may also perform Game Day exercises, where we rehearse\n\nlarge scale failures, such as turning off entire data centers. Or we may inject ever-larger scale faults into the production environment (such as the famous Netflix “Chaos Monkey,” which\n\nrandomly kills processes and compute servers in production) to ensure that we’re as resilient as we want to be.\n\nLEADERS REINFORCE A LEARNING CULTURE\n\nTraditionally, leaders were expected to be responsible for setting\n\nobjectives, allocating resources for achieving those objectives, and establishing the right combination of incentives. Leaders also establish the emotional tone for the organizations they lead. In\n\nother words, leaders lead by “making all the right decisions.”\n\nHowever, there is significant evidence that shows greatness is not achieved by leaders making all the right decisions—instead, the\n\nleader’s role is to create the conditions so their team can discover greatness in their daily work. In other words, creating greatness requires both leaders and workers, each of whom are mutually dependent upon each other.\n\nJim Womack, author of Gemba Walks, described the complementary working relationship and mutual respect that",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "must occur between leaders and frontline workers. According to Womack, this relationship is necessary because neither can solve problems alone—leaders are not close enough to the work, which\n\nis required to solve any problem, and frontline workers do not have the broader organizational context or the authority to make changes outside of their area of work.§\n\nLeaders must elevate the value of learning and disciplined problem solving. Mike Rother formalized these methods in what he calls the coaching kata. The result is one that mirrors the\n\nscientific method, where we explicitly state our True North goals, such as “sustain zero accidents” in the case of Alcoa, or “double throughput within a year” in the case of Aisin.\n\nThese strategic goals then inform the creation of iterative, shorter term goals, which are cascaded and then executed by establishing target conditions at the value stream or work center level (e.g., “reduce lead time by 10% within the next two weeks”).\n\nThese target conditions frame the scientific experiment: we explicitly state the problem we are seeking to solve, our hypothesis\n\nof how our proposed countermeasure will solve it, our methods for testing that hypothesis, our interpretation of the results, and our use of learnings to inform the next iteration.\n\nThe leader helps coach the person conducting the experiment with\n\nquestions that may include:\n\nWhat was your last step and what happened?\n\nWhat did you learn?\n\nWhat is your condition now?",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "What is your next target condition?\n\nWhat obstacle are you working on now?\n\nWhat is your next step?\n\nWhat is your expected outcome?\n\nWhen can we check?\n\nThis problem-solving approach in which leaders help workers see\n\nand solve problems in their daily work is at the core of the Toyota Production System, of learning organizations, the Improvement\n\nKata, and high-reliability organizations. Mike Rother observes\n\nthat he sees Toyota “as an organization defined primarily by the unique behavior routines it continually teaches to all its\n\nmembers.”\n\nIn the technology value stream, this scientific approach and iterative method guides all of our internal improvement processes,\n\nbut also how we perform experiments to ensure that the products\n\nwe build actually help our internal and external customers achieve their goals.\n\nCONCLUSION\n\nThe principles of the Third Way address the need for valuing organizational learning, enabling high trust and boundary-\n\nspanning between functions, accepting that failures will always\n\noccur in complex systems, and making it acceptable to talk about problems so we can create a safe system of work. It also requires\n\ninstitutionalizing the improvement of daily work, converting local",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "learnings into global learnings that can be used by the entire\n\norganization, as well as continually injecting tension into our daily\n\nwork.\n\nAlthough fostering a culture of continual learning and\n\nexperimentation is the principle of the Third Way, it is also\n\ninterwoven into the First and Second Ways. In other words, improving flow and feedback requires an iterative and scientific\n\napproach that includes framing of a target condition, stating a\n\nhypothesis of what will help us get there, designing and conducting experiments, and evaluating the results.\n\nThe results are not only better performance but also increased\n\nresilience, higher job satisfaction, and improved organization adaptability.",
      "content_length": 701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "PART I CONCLUSION\n\nIn Part I of The DevOps Handbook we looked back at several\n\nmovements in history that helped lead to the development of DevOps. We also looked at the three main principles that form the\n\nfoundation for successful DevOps organizations: the principles of\n\nFlow, Feedback, and Continual Learning and Experimentation. In Part II, we will begin to look at how to start a DevOps movement\n\nin your organization.\n\n† The “name, blame, shame” pattern is part of the Bad Apple Theory criticized by Dr. Sydney Dekker and\n\nextensively discussed in his book The Field Guide to Understanding Human Error.\n\n‡ It is astonishing, instructional, and truly moving to see the level of conviction and passion that Paul O’Neill has\n\nabout the moral responsibility leaders have to create workplace safety.\n\n§ Leaders are responsible for the design and operation of processes at a higher level of aggregation where others\n\nhave less perspective and authority.",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Part\n\nIntroduction\n\nHow do we decide where to start a DevOps transformation in our\n\norganization? Who needs to be involved? How should we organize\n\nour teams, protect their work capacity, and maximize their\n\nchances of succeess? These are the questions we aim to answer in Part II of The DevOps Handbook.\n\nIn the following chapters we will walk through the process of\n\ninitiating a DevOps transformation. We begin by evaluating the value streams in our organization, locating a good place to start,\n\nand forming a strategy to create a dedicated transformation team\n\nwith specific improvement goals and eventual expansion. For each value stream being transformed, we identify the work being\n\nperformed and then look at organizational design strategies and organizational archetypes that best support the transformation\n\ngoals.\n\nPrimary focuses in these chapters include:\n\nSelecting which value streams to start with\n\nUnderstanding the work being done in our candidate value\n\nstreams",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Designing our organization and architecture with Conway’s\n\nLaw in mind\n\nEnabling market-oriented outcomes through more effective\n\ncollaboration between functions throughout the value stream\n\nProtecting and enabling our teams\n\nBeginning any transformation is full of uncertainty—we are\n\ncharting a journey to an ideal end state, but where virtually all the\n\nintermediate steps are unknown. These next chapters are intended\n\nto provide a thought process to guide our decisions, provide\n\nactionable steps we can take, and illustrate case studies as examples.",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "'\n\n5\n\nSelecting Which Value Stream to Start With\n\nChoosing a value stream for DevOps transformation deserves\n\ncareful consideration. Not only does the value stream we choose\n\ndictate the difficulty of our transformation, but it also dictates who will be involved in the transformation. It will affect how we need\n\nto organize into teams and how we can best enable the teams and individuals in them.\n\nAnother challenge was noted by Michael Rembetsy, who helped\n\nlead the DevOps transformation as the Director of Operations at Etsy in 2009. He observed, “We must pick our transformation\n\nprojects carefully—when we’re in trouble, we don’t get very many shots. Therefore, we must carefully pick and then protect those\n\nimprovement projects that will most improve the state of our\n\norganization.”\n\nLet us examine how the Nordstrom team started their DevOps transformation initiative in 2013, which Courtney Kissler, their VP of E-Commerce and Store Technologies, described at the DevOps\n\nEnterprise Summit in 2014 and 2015.",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Founded in 1901, Nordstrom is a leading fashion retailer that is\n\nfocused on delivering the best possible shopping experience to\n\ntheir customers. In 2015, Nordstrom had annual revenue of $13.5\n\nbillion.\n\nThe stage for Nordstrom’s DevOps journey was likely set in 2011\n\nduring one of their annual board of directors meetings. That year,\n\none of the strategic topics discussed was the need for online\n\nrevenue growth. They studied the plight of Blockbusters, Borders,\n\nand Barnes & Nobles, which demonstrated the dire consequences\n\nwhen traditional retailers were late creating competitive e-\n\ncommerce capabilities—these organizations were clearly at risk of losing their position in the marketplace or even going out of business entirely.†\n\nAt that time, Courtney Kissler was the senior director of Systems Delivery and Selling Technology, responsible for a significant portion of the technology organization, including their in-store systems and online e-commerce site. As Kissler described, “In 2011, the Nordstrom technology organization was very much optimized for cost—we had outsourced many of our technology\n\nfunctions, we had an annual planning cycle with large batch, ‘waterfall’ software releases. Even though we had a 97% success\n\nrate of hitting our schedule, budget, and scope goals, we were ill- equipped to achieve what the five-year business strategy required from us, as Nordstrom started optimizing for speed instead of merely optimizing for cost.”\n\nKissler and the Nordstrom technology management team had to decide where to start their initial transformation efforts. They\n\ndidn’t want to cause upheaval in the whole system. Instead, they wanted to focus on very specific areas of the business so that they",
      "content_length": 1727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "could experiment and learn. Their goal was to demonstrate early\n\nwins, which would give everyone confidence that these\n\nimprovements could be replicated in other areas of the\n\norganization. How exactly that would be achieved was still\n\nunknown.\n\nThey focused on three areas: the customer mobile application,\n\ntheir in-store restaurant systems, and their digital properties.\n\nEach of these areas had business goals that weren’t being met;\n\nthus, they were more receptive to considering a different way of\n\nworking. The stories of the first two are described below.\n\nThe Nordstrom mobile application had experienced an inauspicious start. As Kissler said, “Our customers were extremely frustrated with the product, and we had uniformly negative reviews when we launched it in the App Store. Worse, the existing structure and processes (aka “the system”) had designed their\n\nprocesses so that they could only release updates twice per year.” In other words, any fixes to the application would have to wait months to reach the customer.\n\nTheir first goal was to enable faster or on-demand releases, providing faster iteration and the ability to respond to customer\n\nfeedback. They created a dedicated product team that was solely dedicated to supporting the mobile application, with the goal of enabling that team to be able to independently implement, test,\n\nand deliver value to the customer. By doing this, they would no longer have to depend on and coordinate with scores of other teams inside Nordstrom. Furthermore, they moved from planning once per year to a continuous planning process. The result was a\n\nsingle prioritized backlog of work for the mobile app based on",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "customer need—gone were all the conflicting priorities when the team had to support multiple products.\n\nOver the following year, they eliminated testing as a separate phase of work, instead integrating it into everyone’s daily work.‡ They doubled the features being delivered per month and halved\n\nthe number of defects—creating a successful outcome.\n\nTheir second area of focus was the systems supporting their in-\n\nstore Café Bistro restaurants. Unlike the mobile app value stream where the business need was to reduce time to market and\n\nincrease feature throughput, the business need here was to decrease cost and increase quality. In 2013, Nordstrom had\n\ncompleted eleven “restaurant re-concepts” which required changes to the in-store applications, causing a number of\n\ncustomer-impacting incidents. Disturbingly, they had planned\n\nforty-four more of these re-concepts for 2014—four times as many as in the previous year.\n\nAs Kissler stated, “One of our business leaders suggested that we\n\ntriple our team size to handle these new demands, but I proposed that we had to stop throwing more bodies at the problem and\n\ninstead improve the way we worked.”\n\nThey were able to identify problematic areas, such as in their work intake and deployment processes, which is where they focused\n\ntheir improvement efforts. They were able to reduce code\n\ndeployment lead times by 60% and reduce the number of production incidents 60% to 90%.\n\nThese successes gave the teams confidence that DevOps principles\n\nand practices were applicable to a wide variety of value streams.",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Kissler was promoted to VP of E-Commerce and Store Technologies in 2014.\n\nIn 2015, Kissler said that in order for the selling or customer-\n\nfacing technology organization to enable the business to meet their goals, “…we needed to increase productivity in all our\n\ntechnology value streams, not just in a few. At the management level, we created an across-the-board mandate to reduce cycle\n\ntimes by 20% for all customer-facing services.”\n\nShe continued, “This is an audacious challenge. We have many\n\nproblems in our current state—process and cycle times are not consistently measured across teams, nor are they visible. Our first\n\ntarget condition requires us to help all our teams measure, make it visible, and perform experiments to start reducing their process\n\ntimes, iteration by iteration.”\n\nKissler concluded, “From a high level perspective, we believe that techniques such as value stream mapping, reducing our batch\n\nsizes toward single-piece flow, as well as using continuous delivery\n\nand microservices will get us to our desired state. However, while we are still learning, we are confident that we are heading in the\n\nright direction, and everyone knows that this effort has support from the highest levels of management.”\n\nIn this chapter, various models are presented that will enable us to\n\nreplicate the thought processes that the Nordstrom team used to decide which value streams to start with. We will evaluate our\n\ncandidate value streams in many ways, including whether they are\n\na greenfield or brownfield service, a system of engagement or a system of record. We will also estimate the risk/reward balance of",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "transforming and assess the likely level of resistance we may get\n\nfrom the teams we would work with.\n\nGREENFIELD VS. BROWNFIELD SERVICES\n\nWe often categorize our software services or products as either\n\ngreenfield or brownfield. These terms were originally used for urban planning and building projects. Greenfield development is\n\nwhen we build on undeveloped land. Brownfield development is\n\nwhen we build on land that was previously used for industrial purposes, potentially contaminated with hazardous waste or\n\npollution. In urban development, many factors can make greenfield projects simpler than brownfield projects—there are no\n\nexisting structures that need to be demolished nor are there toxic materials that need to be removed.\n\nIn technology, a greenfield project is a new software project or\n\ninitiative, likely in the early stages of planning or implementation, where we build our applications and infrastructure anew, with few\n\nconstraints. Starting with a greenfield software project can be\n\neasier, especially if the project is already funded and a team is either being created or is already in place. Furthermore, because\n\nwe are starting from scratch, we can worry less about existing code bases, processes, and teams.\n\nGreenfield DevOps projects are often pilots to demonstrate\n\nfeasibility of public or private clouds, piloting deployment\n\nautomation, and similar tools. An example of a greenfield DevOps project is the Hosted LabVIEW product in 2009 at National Instruments, a thirty-year-old organization with five thousand\n\nemployees and $1 billion in annual revenue. To bring this product",
      "content_length": 1614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "to market quickly, a new team was created and allowed to operate outside of the existing IT processes and explore the use of public\n\nclouds. The initial team included an applications architect, a systems architect, two developers, a system automation developer, an operations lead, and two offshore operations staff. By using\n\nDevOps practices, they were able to deliver Hosted LabVIEW to market in half the time of their normal product introductions.\n\nOn the other end of the spectrum are brownfield DevOps projects,\n\nthese are existing products or services that are already serving customers and have potentially been in operation for years or even decades. Brownfield projects often come with significant amounts\n\nof technical debt, such as having no test automation or running on unsupported platforms. In the Nordstrom example presented earlier in this chapter, both the in-store restaurant systems and e- commerce systems were brownfield projects.\n\nAlthough many believe that DevOps is primarily for greenfield projects, DevOps has been used to successfully transform brownfield projects of all sorts. In fact, over 60% of the\n\ntransformation stories shared at the DevOps Enterprise Summit in 2014 were for brownfield projects. In these cases, there was a large performance gap between what the customer needed and\n\nwhat the organization was currently delivering, and the DevOps transformations created tremendous business benefit.\n\nIndeed, one of the findings in the 2015 State of DevOps Report\n\nvalidated that the age of the application was not a significant predictor of performance; instead, what predicted performance was whether the application was architected (or could be re- architected) for testability and deployability.",
      "content_length": 1737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Teams supporting brownfield projects may be very receptive to experimenting with DevOps, particularly when there is a\n\nwidespread belief that traditional methods are insufficient to achieve their goals—and especially if there is a strong sense of urgency around the need for improvement.§\n\nWhen transforming brownfield projects, we may face significant impediments and problems, especially when no automated testing exists or when there is a tightly-coupled architecture that prevents\n\nsmall teams from developing, testing, and deploying code independently. How we overcome these issues are discussed throughout this book.\n\nExamples of successful brownfield transformations include:\n\nCSG (2013): In 2013, CSG International had $747 million in revenue and over 3,500 employees, enabling over ninety\n\nthousand customer service agents to provide billing operations and customer care to over fifty million video, voice, and data customers, executing over six billion transactions, and printing\n\nand mailing over seventy million paper bill statements every month. Their initial scope of improvement was bill printing, one of their primary businesses, and involved a COBOL mainframe application and the twenty surrounding technology\n\nplatforms. As part of their transformation, they started performing daily deployments into a production-like environment, and doubled the frequency of customer releases\n\nfrom twice annually to four times annually. As a result, they significantly increased the reliability of the application and reduced code deployment lead times from two weeks to less than one day.",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Etsy (2009): In 2009, Etsy had thirty-five employees and was generating $87 million in revenue, but after they “barely\n\nsurvived the holiday retail season,” they started transforming virtually every aspect of how the organization worked, eventually turning the company into one of the most admired DevOps organizations and set the stage for a successful 2015\n\nIPO.\n\nCONSIDER BOTH SYSTEMS OF RECORD AND SYSTEMS OF ENGAGEMENT\n\nThe Gartner research firm has recently popularized the notion of\n\nbimodal IT, referring to the wide spectrum of services that typical enterprises support. Within bimodal IT there are systems of record, the ERP-like systems that run our business (e.g., MRP, HR, financial reporting systems), where the correctness of the\n\ntransactions and data are paramount; and systems of engagement, which are customer-facing or employee-facing systems, such as e-commerce systems and productivity\n\napplications.\n\nSystems of record typically have a slower pace of change and often have regulatory and compliance requirements (e.g., SOX). Gartner\n\ncalls these types of systems “Type 1,” where the organization focuses on “doing it right.”\n\nSystems of engagement typically have a much higher pace of\n\nchange to support rapid feedback loops that enable them to conduct experimentation to discover how to best meet customer needs. Gartner calls these types of systems “Type 2,” where the\n\norganization focuses on “doing it fast.”",
      "content_length": 1435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "It may be convenient to divide up our systems into these categories; however, we know that the core, chronic conflict between “doing it right” and “doing it fast” can be broken with\n\nDevOps. The data from Puppet Labs’ State of DevOps Reports— following the lessons of Lean manufacturing—shows that high- performing organizations are able to simultaneously deliver\n\nhigher levels of throughput and reliability.\n\nFurthermore, because of how interdependent our systems are, our ability to make changes to any of these systems is limited by the\n\nsystem that is most difficult to safely change, which is almost always a system of record.\n\nScott Prugh, VP of Product Development at CSG, observed, “We’ve\n\nadopted a philosophy that rejects bi-modal IT, because every one of our customers deserve speed and quality. This means that we need technical excellence, whether the team is supporting a 30\n\nyear old mainframe application, a Java application, or a mobile application.”\n\nConsequently, when we improve brownfield systems, we should\n\nnot only strive to reduce their complexity and improve their reliability and stability, we should also make them faster, safer, and easier to change. Even when new functionality is added just to\n\ngreenfield systems of engagement, they often cause reliability problems in the brownfield systems of record they rely on. By making these downstream systems safer to change, we help the entire organization more quickly and safely achieve its goals.\n\nSTART WITH THE MOST SYMPATHETIC AND INNOVATIVE GROUPS",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Within every organization, there will be teams and individuals with a wide range of attitudes toward the adoption of new ideas. Geoffrey A. Moore first depicted this spectrum in the form of the\n\ntechnology adoption life cycle in Crossing The Chasm, where the chasm represents the classic difficulty of reaching groups beyond the innovators and early adopters (see figure 9).\n\nIn other words, new ideas are often quickly embraced by innovators and early adopters, while others with more conservative attitudes resist them (the early majority, late\n\nmajority, and laggards). Our goal is to find those teams that already believe in the need for DevOps principles and practices, and who possess a desire and demonstrated ability to innovate and improve their own processes. Ideally, these groups will be\n\nenthusiastic supporters of the DevOps journey.\n\nFigure 9: The Technology Adoption Curve (Source: Moore and McKenna, Crossing The Chasm, 15.)\n\nEspecially in the early stages, we will not spend much time trying to convert the more conservative groups. Instead, we will focus our energy on creating successes with less risk-averse groups and",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "build out our base from there (a process that is discussed further in the next section). Even if we have the highest levels of executive sponsorship, we will avoid the big bang approach (i.e., starting\n\neverywhere all at once), choosing instead to focus our efforts in a few areas of the organization, ensuring that those initiatives are successful, and expanding from there.¶\n\nEXPANDING DEVOPS ACROSS OUR ORGANIZATION\n\nRegardless of how we scope our initial effort, we must demonstrate early wins and broadcast our successes. We do this by breaking up our larger improvement goals into small,\n\nincremental steps. This not only creates our improvements faster, it also enables us to discover when we have made the wrong choice of value stream—by detecting our errors early, we can quickly back up and try again, making different decisions armed with our new\n\nlearnings.\n\nAs we generate successes, we earn the right to expand the scope of our DevOps initiative. We want to follow a safe sequence that\n\nmethodically grows our levels of credibility, influence, and\n\nsupport. The following list, adapted from a course taught by Dr. Roberto Fernandez, a William F. Pounds Professor in\n\nManagement at MIT, describes the ideal phases used by change agents to build and expand their coalition and base of support:\n\n1. Find Innovators and Early Adopters: In the beginning,\n\nwe focus our efforts on teams who actually want to help—these are our kindred spirits and fellow travelers who are the first to\n\nvolunteer to start the DevOps journey. In the ideal, these are",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "also people who are respected and have a high degree of\n\ninfluence over the rest of the organization, giving our initiative\n\nmore credibility.\n\n2. Build Critical Mass and Silent Majority: In the next\n\nphase, we seek to expand DevOps practices to more teams and\n\nvalue streams with the goal of creating a stable base of support. By working with teams who are receptive to our ideas, even if\n\nthey are not the most visible or influential groups, we expand\n\nour coalition who are generating more successes, creating a “bandwagon effect” that further increases our influence. We\n\nspecifically bypass dangerous political battles that could\n\njeopardize our initiative.\n\n3. Identify the Holdouts: The “holdouts” are the high profile,\n\ninfluential detractors who are most likely to resist (and maybe\n\neven sabotage) our efforts. In general, we tackle this group only after we have achieved a silent majority, when we have\n\nestablished enough successes to successfully protect our initiative.\n\nExpanding DevOps across an organization is no small task. It can\n\ncreate risk to individuals, departments, and the organization as a\n\nwhole. But as Ron van Kemenade, CIO of ING, who helped transform the organization into one of the most admired\n\ntechnology organizations, said, “Leading change requires courage, especially in corporate environments where people are scared and\n\nfight you. But if you start small, you really have nothing to fear.\n\nAny leader needs to be brave enough to allocate teams to do some calculated risk-taking.”",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "CONCLUSION\n\nPeter Drucker, a leader in the development of management\n\neducation, observed that “little fish learn to be big fish in little\n\nponds.” By choosing carefully where and how to start, we are able to experiment and learn in areas of our organization that create\n\nvalue without jeopardizing the rest of the organization. By doing\n\nthis, we build our base of support, earn the right to expand the use of DevOps in our organization, and gain the recognition and\n\ngratitude of an ever-larger constituency.\n\n† These organizations were sometimes known as the “Killer B’s that are Dying.”\n\n‡ The practice of relying on a stabilization phase or hardening phase at the end of a project often has very poor\n\noutcomes, because it means problems are not being found and fixed as part of daily work and are left unaddressed, potentially snowballing into larger issues.\n\n§ That the services that have the largest potential business benefit are brownfield systems shouldn’t be surprising. After all, these are the systems that are most relied upon and have the largest number of existing customers or highest amount of revenue depending upon them.\n\n¶ Big bang, top-down transformations are possible, such as the Agile transformation at PayPal in 2012 that was led by their vice president of technology, Kirsten Wolberg. However, as with any sustainable and successful transformation, this required the highest level of management support and a relentless, sustained focus on driving the necessary outcomes.",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "6 Understanding the\n\nWork in Our Value Stream, Making it Visible, and Expanding it Across\n\nthe Organization\n\nOnce we have identified a value stream to which we want to apply DevOps principles and patterns, our next step is to gain a\n\nsufficient understanding of how value is delivered to the customer:\n\nwhat work is performed and by whom, and what steps can we take to improve flow.\n\nIn the previous chapter, we learned about the DevOps\n\ntransformation led by Courtney Kissler and the team at Nordstrom. Over the years, they have learned that one of the most\n\nefficient ways to start improving any value stream is to conduct a\n\nworkshop with all the major stakeholders and perform a value stream mapping exercise—a process (described later in this chapter) designed to help capture all the steps required to create\n\nvalue.\n\nKissler’s favorite example of the valuable and unexpected insights\n\nthat can come from value stream mapping is when they tried to improve the long lead times associated with requests going",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "through the Cosmetics Business Office application, a COBOL\n\nmainframe application that supported all the floor and\n\ndepartment managers of their in-store beauty and cosmetic\n\ndepartments.\n\nThis application allowed department managers to register new\n\nsalespeople for various product lines carried in their stores, so that\n\nthey could track sales commissions, enable vendor rebates, and so\n\nforth.\n\nKissler explained:\n\nI knew this particular mainframe application well—earlier in my career, I supported this technology team, so I know firsthand that for nearly a decade, during each annual planning cycle, we would debate about how we needed to get this application off the mainframe. Of course, like in most\n\norganizations, even when there was full management support, we never seemed to get around to migrating it.\n\nMy team wanted to conduct a value stream mapping exercise to determine whether the COBOL application really was the problem, or maybe there was a larger problem that we needed to address. They conducted a workshop that assembled everyone with any accountability for delivering value to our internal customers, including our business partners, the\n\nmainframe team, the shared service teams, and so forth.\n\nWhat they discovered was that when department managers\n\nwere submitting the ‘product line assignment’ request form, we were asking them for an employee number, which they didn’t have—so they would either leave it blank or put in something",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "like ‘I don’t know.’ Worse, in order to fill out the form,\n\ndepartment managers would have to inconveniently leave the\n\nstore floor in order to use a PC in the back office. The end\n\nresult was all this wasted time, with work bouncing back and\n\nforth in the process.\n\nDuring the workshop, the participants conducted several\n\nexperiments, including deleting the employee number field in the\n\nform and letting another department get that information in a\n\ndownstream step. These experiments, conducted with the help of\n\ndepartment managers, showed a four-day reduction in processing\n\ntime. The team later replaced the PC application with an iPad application, which allowed managers to submit the necessary information without leaving the store floor, and the processing\n\ntime was further reduced to seconds.\n\nShe said proudly, “With those amazing improvements, all the\n\ndemands to get this application off the mainframe disappeared. Furthermore, other business leaders took notice and started coming to us with a whole list of further experiments they wanted to conduct with us in their own organizations. Everyone in the business and technology teams were excited by the outcome because they solved a real business problem, and, most\n\nimportantly, they learned something in the process.”\n\nIn the remainder of this chapter, we will go through the following\n\nsteps: identifying all the teams required to create customer value, creating a value stream map to make visible all the required work, and using it to guide the teams in how to better and more quickly create value. By doing this, we can replicate the amazing outcomes\n\ndescribed in this Nordstrom example.",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "IDENTIFYING THE TEAMS SUPPORTING OUR VALUE STREAM\n\nAs this Nordstrom example demonstrates, in value streams of any complexity, no one person knows all the work that must be\n\nperformed in order to create value for the customer—especially since the required work must be performed by many different\n\nteams, often far removed from each other on the organization charts, geographically, or by incentives.\n\nAs a result, after we select a candidate application or service for\n\nour DevOps initiative, we must identify all the members of the\n\nvalue stream who are responsible for working together to create value for the customers being served. In general, this includes:\n\nProduct owner: the internal voice of the business that\n\ndefines the next set of functionality in the service\n\nDevelopment: the team responsible for developing application functionality in the service\n\nQA: the team responsible for ensuring that feedback loops\n\nexist to ensure the service functions as desired\n\nOperations: the team often responsible for maintaining the production environment and helping ensure that required\n\nservice levels are met\n\nInfosec: the team responsible for securing systems and data\n\nRelease managers: the people responsible for managing and coordinating the production deployment and release processes",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Technology executives or value stream manager: in Lean literature, someone who is responsible for “ensuring that\n\nthe value stream meets or exceeds the customer [and organizational] requirements for the overall value stream, from\n\nstart to finish”\n\nCREATE A VALUE STREAM MAP TO SEE THE WORK\n\nAfter we identify our value stream members, our next step is to gain a concrete understanding of how work is performed,\n\ndocumented in the form of a value stream map. In our value\n\nstream, work likely begins with the product owner, in the form of a customer request or the formulation of a business hypothesis.\n\nSome time later, this work is accepted by Development, where features are implemented in code and checked in to our version\n\ncontrol repository. Builds are then integrated, tested in a\n\nproduction-like environment, and finally deployed into production, where they (ideally) create value for our customer.\n\nIn many traditional organizations, this value stream will consist of\n\nhundreds, if not thousands, of steps, requiring work from hundreds of people. Because documenting any value stream map\n\nthis complex likely requires multiple days, we may conduct a multi-day workshop, where we assemble all the key constituents\n\nand remove them from the distractions of their daily work.\n\nOur goal is not to document every step and associated minutiae, but to sufficiently understand the areas in our value stream that\n\nare jeopardizing our goals of fast flow, short lead times, and\n\nreliable customer outcomes. Ideally, we have assembled those",
      "content_length": 1540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "people with the authority to change their portion of the value stream.†\n\nDamon Edwards, co-host of DevOps Café podcast, observed, “In\n\nmy experience, these types of value stream mapping exercises are always an eye-opener. Often, it is the first time when people see\n\nhow much work and heroics are required to deliver value to the customer. For Operations, it may be the first time that they see the\n\nconsequences that result when developers don’t have access to correctly configured environments, which contributes to even\n\nmore crazy work during code deployments. For Development, it\n\nmay be the first time they see all the heroics that are required by Test and Operations in order to deploy their code into production,\n\nlong after they flag a feature as ‘completed.’”\n\nUsing the full breadth of knowledge brought by the teams engaged in the value stream, we should focus our investigation and\n\nscrutiny on the following areas:\n\nPlaces where work must wait weeks or even months, such as getting production-like environments, change approval\n\nprocesses, or security review processes\n\nPlaces where significant rework is generated or received\n\nOur first pass of documenting our value stream should only consist of high-level process blocks. Typically, even for complex\n\nvalue streams, groups can create a diagram with five to fifteen\n\nprocess blocks within a few hours. Each process block should include the lead time and process time for a work item to be processed, as well as the %C/A as measured by the downstream consumers of the output.‡",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Figure 10: An example of a value stream map (Source: Humble, Molesky, and O’Reilly, Lean Enterprise, 139.)\n\nWe use the metrics from our value stream map to guide our improvement efforts. In the Nordstrom example, they focused on the low %C/A rates on the request form submitted by department\n\nmanagers due to the absence of employee numbers. In other cases, it may be long lead times or low %C/A rates when delivering correctly configured test environments to Development teams, or\n\nit might be the long lead times required to execute and pass regression testing before each software release.\n\nOnce we identify the metric we want to improve, we should\n\nperform the next level of observations and measurements to better understand the problem and then construct an idealized, future value stream map, which serves as a target condition to achieve by\n\nsome date (e.g., usually three to twelve months).\n\nLeadership helps define this future state and then guides and enables the team to brainstorm hypotheses and countermeasures\n\nto achieve the desired improvement to that state, perform",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "experiments to test those hypotheses, and interpret the results to determine whether the hypotheses were correct. The teams keep\n\nrepeating and iterating, using any new learnings to inform the next experiments.\n\nCREATING A DEDICATED TRANSFORMATION TEAM\n\nOne of the inherent challenges with initiatives such as DevOps\n\ntransformations is that they are inevitably in conflict with ongoing business operations. Part of this is a natural outcome of how successful businesses evolve. An organization that has been\n\nsuccessful for any extended period of time (years, decades, or even centuries) has created mechanisms to perpetuate the practices that made them successful, such as product development, order administration, and supply chain operations.\n\nMany techniques are used to perpetuate and protect how current processes operate, such as specialization, focus on efficiency and\n\nrepeatability, bureaucracies that enforce approval processes, and controls to protect against variance. In particular, bureaucracies are incredibly resilient and are designed to survive adverse conditions—one can remove half the bureaucrats, and the process\n\nwill still survive.\n\nWhile this is good for preserving status quo, we often need to change how we work to adapt to changing conditions in the\n\nmarketplace. Doing this requires disruption and innovation, which puts us at odds with groups who are currently responsible for daily operations and the internal bureaucracies, and who will\n\nalmost always win.",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "In their book The Other Side of Innovation: Solving the Execution Challenge, Dr. Vijay Govindarajan and Dr. Chris Trimble, both\n\nfaculty members of Dartmouth College’s Tuck School of Business, described their studies of how disruptive innovation is achieved despite these powerful forces of daily operations. They documented how customer-driven auto insurance products were\n\nsuccessfully developed and marketed at Allstate, how the profitable digital publishing business was created at the Wall Street Journal, the development of the breakthrough trail-running\n\nshoe at Timberland, and the development of the first electric car at BMW.\n\nBased on their research, Dr. Govindarajan and Dr. Trimble assert\n\nthat organizations need to create a dedicated transformation team that is able to operate outside of the rest of the organization that is responsible for daily operations (which they call the “dedicated\n\nteam” and “performance engine” respectively).\n\nFirst and foremost, we will hold this dedicated team accountable for achieving a clearly defined, measurable, system-level result\n\n(e.g., reduce the deployment lead time from “code committed into version control to successfully running in production” by 50%). In order to execute such an initiative, we do the following:\n\nAssign members of the dedicated team to be solely allocated to the DevOps transformation efforts (as opposed to “maintain all your current responsibilities, but spend 20% of your time on this new DevOps thing.”).\n\nSelect team members who are generalists, who have skills across a wide variety of domains.",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Select team members who have longstanding and mutually respectful relationships with the rest of the organization.\n\nCreate a separate physical space for the dedicated team, if possible, to maximize communication flow within the team, and creating some isolation from the rest of the organization.\n\nIf possible, we will free the transformation team from many of the rules and policies that restrict the rest of the organization, as National Instruments did, described in the previous chapter. After\n\nall, established processes are a form of institutional memory—we need the dedicated team to create the new processes and learnings required to generate our desired outcomes, creating new institutional memory.\n\nCreating a dedicated team is not only good for the team, but also good for the performance engine. By creating a separate team, we\n\ncreate the space for them to experiment with new practices, protecting the rest of the organization from the potential disruptions and distractions associated with it.\n\nAGREE ON A SHARED GOAL\n\nOne of the most important parts of any improvement initiative is\n\nto define a measurable goal with a clearly defined deadline, between six months and two years in the future. It should require considerable effort but still be achievable. And achievement of the goal should create obvious value for the organization as a whole\n\nand to our customers.\n\nThese goals and the time frame should be agreed upon by the executives and known to everyone in the organization. We also\n\nwant to limit the number of these types of initiatives going on",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "simultaneously to prevent us from overly taxing the organizational change management capacity of leaders and the organization. Examples of improvement goals might include:\n\nReduce the percentage of the budget spent on product support and unplanned work by 50%.\n\nEnsure lead time from code check-in to production release is one week or less for 95% of changes.\n\nEnsure releases can always be performed during normal\n\nbusiness hours with zero downtime.\n\nIntegrate all the required information security controls into the deployment pipeline to pass all required compliance\n\nrequirements.\n\nOnce the high-level goal is made clear, teams should decide on a\n\nregular cadence to drive the improvement work. Like product development work, we want transformation work to be done in an iterative, incremental manner. A typical iteration will be in the range of two to four weeks. For each iteration, the teams should\n\nagree on a small set of goals that generate value and makes some progress toward the long-term goal. At the end of each iteration, teams should review their progress and set new goals for the next\n\niteration.\n\nKEEP OUR IMPROVEMENT PLANNING HORIZONS SHORT\n\nIn any DevOps transformation project, we need to keep our\n\nplanning horizons short, just as if we were in a startup doing product or customer development. Our initiative should strive to",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "generate measurable improvements or actionable data within weeks (or, in the worst case, months).\n\nBy keeping our planning horizons and iteration intervals short, we achieve the following:\n\nFlexibility and the ability to reprioritize and replan quickly\n\nDecrease the delay between work expended and improvement realized, which strengthens our feedback loop, making it more likely to reinforce desired behaviors—when improvement\n\ninitiatives are successful, it encourages more investment\n\nFaster learning generated from the first iteration, meaning faster integration of our learnings into the next iteration\n\nReduction in activation energy to get improvements\n\nQuicker realization of improvements that make meaningful\n\ndifferences in our daily work\n\nLess risk that our project is killed before we can generate any demonstrable outcomes\n\nRESERVE 20% OF CYCLES FOR NON-FUNCTIONAL REQUIREMENTS AND REDUCING TECHNICAL DEBT\n\nA problem common to any process improvement effort is how to\n\nproperly prioritize it—after all, organizations that need it most are those that have the least amount of time to spend on\n\nimprovement. This is especially true in technology organizations\n\nbecause of technical debt.",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Organizations that struggle with financial debt only make interest\n\npayments and never reduce the loan principal, and may eventually\n\nfind themselves in situations where they can no longer service the interest payments. Similarly, organizations that don’t pay down\n\ntechnical debt can find themselves so burdened with daily\n\nworkarounds for problems left unfixed that they can no longer complete any new work. In other words, they are now only making\n\nthe interest payment on their technical debt.\n\nWe will actively manage this technical debt by ensuring that we invest at least 20% of all Development and Operations cycles on\n\nrefactoring, investing in automation work and architecture and\n\nnon-functional requirements (NFRs, sometimes referred to as the “ilities”), such as maintainability, manageability, scalability,\n\nreliability, testability, deployability, and security.\n\nFigure 11: Invest 20% of cycles on those that create positive, user- invisible value (Source: “Machine Learning and Technical Debt with D. Sculley,” Software Engineering Daily podcast, November 17, 2015, http://softwareengineeringdaily.com/2015/11/17/machine-learning- and-technical-debt-with-d-sculley/.)\n\nAfter the near-death experience of eBay in the late 1990s, Marty\n\nCagan, author of Inspired: How To Create Products Customers",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Love, the seminal book on product design and management,\n\ncodified the following lesson:\n\nThe deal [between product owners and] engineering goes like this: Product management takes 20% of the team’s capacity\n\nright off the top and gives this to engineering to spend as they\n\nsee fit. They might use it to rewrite, re-architect, or re-factor problematic parts of the code base...whatever they believe is\n\nnecessary to avoid ever having to come to the team and say, ‘we\n\nneed to stop and rewrite [all our code].’ If you’re in really bad shape today, you might need to make this 30% or even more of\n\nthe resources. However, I get nervous when I find teams that think they can get away with much less than 20%.\n\nCagan notes that when organizations do not pay their “20% tax,”\n\ntechnical debt will increase to the point where an organization\n\ninevitably spends all of its cycles paying down technical debt. At some point, the services become so fragile that feature delivery\n\ngrinds to a halt because all the engineers are working on reliability issues or working around problems.\n\nBy dedicating 20% of our cycles so that Dev and Ops can create\n\nlasting countermeasures to the problems we encounter in our daily work, we ensure that technical debt doesn’t impede our\n\nability to quickly and safely develop and operate our services in\n\nproduction. Elevating added pressure of technical debt from workers can also reduce levels of burnout.\n\nCase Study Operation InVersion at LinkedIn (2011)",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "LinkedIn’s Operation InVersion presents an interesting case\n\nstudy that illustrates the need to pay down technical debt as\n\na part of daily work. Six months after their successful IPO in 2011, LinkedIn continued to struggle with problematic\n\ndeployments that became so painful that they launched Operation InVersion, where they stopped all feature\n\ndevelopment for two months in order to overhaul their\n\ncomputing environments, deployments, and architecture.\n\nLinkedIn was created in 2003 to help users “connect to your network for better job opportunities.” By the end of their first\n\nweek of operation, they had 2,700 members. One year later, they had over one million members, and have grown\n\nexponentially since then. By November 2015, LinkedIn had\n\nover 350 million members, who generate tens of thousands of requests per second, resulting in millions of queries per\n\nsecond on the LinkedIn back-end systems.\n\nFrom the beginning, LinkedIn primarily ran on their homegrown Leo application, a monolithic Java application\n\nthat served every page through servlets and managed JDBC\n\nconnections to various back-end Oracle databases. However, to keep up with growing traffic in their early years,\n\ntwo critical services were decoupled from Leo: the first\n\nhandled queries around the member connection graph entirely in-memory, and the second was member search,\n\nwhich layered over the first.\n\nBy 2010, most new development was occurring in new services, with nearly one hundred services running outside\n\nof Leo. The problem was that Leo was only being deployed\n\nonce every two weeks.",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Josh Clemm, a senior engineering manager at LinkedIn,\n\nexplained that by 2010, the company was having significant\n\nproblems with Leo. Despite vertically scaling Leo by adding memory and CPUs, “Leo was often going down in\n\nproduction, it was difficult to troubleshoot and recover, and difficult to release new code….It was clear we needed to ‘Kill\n\nLeo’ and break it up into many small functional and stateless\n\nservices.”\n\nIn 2013, journalist Ashlee Vance of Bloomberg described\n\nhow “when LinkedIn would try to add a bunch of new things\n\nat once, the site would crumble into a broken mess, requiring engineers to work long into the night and fix the\n\nproblems.” By Fall 2011, late nights were no longer a rite of\n\npassage or a bonding activity, because the problems had become intolerable. Some of LinkedIn’s top engineers,\n\nincluding Kevin Scott, who had joined as the LinkedIn VP of\n\nEngineering three months before their initial public offering, decided to completely stop engineering work on new\n\nfeatures and dedicate the whole department to fixing the\n\nsite’s core infrastructure. They called the effort Operation InVersion.\n\nScott launched Operation InVersion as a way to “inject the\n\nbeginnings of a cultural manifesto into his team’s engineering culture. There would be no new feature\n\ndevelopment until LinkedIn’s computing architecture was\n\nrevamped—it’s what the business and his team needed.”\n\nScott described one downside: “You go public, have all the\n\nworld looking at you, and then we tell management that\n\nwe’re not going to deliver anything new while all of",
      "content_length": 1576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "engineering works on this [InVersion] project for the next two\n\nmonths. It was a scary thing.”\n\nHowever, Vance described the massively positive results of Operation InVersion. “LinkedIn created a whole suite of\n\nsoftware and tools to help it develop code for the site.\n\nInstead of waiting weeks for their new features to make their way onto LinkedIn’s main site, engineers could develop a\n\nnew service, have a series of automated systems examine the code for any bugs and issues the service might have\n\ninteracting with existing features, and launch it right to the\n\nlive LinkedIn site...LinkedIn’s engineering corps [now] performs major upgrades to the site three times a day.” By\n\ncreating a safer system of work, the value they created\n\nincluded fewer late night cram sessions, with more time to develop new, innovative features.\n\nAs Josh Clemm described in his article on scaling at\n\nLinkedIn, “Scaling can be measured across many dimensions, including organizational…. [Operation\n\nInVersion] allowed the entire engineering organization to\n\nfocus on improving tooling and deployment, infrastructure, and developer productivity. It was successful in enabling the\n\nengineering agility we need to build the scalable new\n\nproducts we have today….[In] 2010, we already had over 150 separate services. Today, we have over 750 services.”\n\nKevin Scott stated, “Your job as an engineer and your\n\npurpose as a technology team is to help your company win. If you lead a team of engineers, it’s better to take a CEO’s\n\nperspective. Your job is to figure out what it is that your\n\ncompany, your business, your marketplace, your competitive",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "environment needs. Apply that to your engineering team in\n\norder for your company to win.”\n\nBy allowing LinkedIn to pay down nearly a decade of technical debt, Project InVersion enabled stability and safety,\n\nwhile setting the next stage of growth for the company. However, it required two months of total focus on non-\n\nfunctional requirements, at the expense of all the promised\n\nfeatures made to the public markets during an IPO. By finding and fixing problems as part of our daily work, we\n\nmanage our technical debt so that we avoid these “near\n\ndeath” experiences.\n\nINCREASE THE VISIBILITY OF WORK\n\nIn order to be able to know if we are making progress toward our goal, it’s essential that everyone in the organization knows the\n\ncurrent state of work. There are many ways to make the current\n\nstate visible, but what’s most important is that the information we display is up to date, and that we constantly revise what we\n\nmeasure to make sure it’s helping us understand progress toward\n\nour current target conditions.\n\nThe following section discusses patterns that can help create\n\nvisibility and alignment across teams and functions.\n\nUSE TOOLS TO REINFORCE DESIRED BEHAVIOR\n\nAs Christopher Little, a software executive and one of the earliest chroniclers of DevOps, observed, “Anthropologists describe tools",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "as a cultural artifact. Any discussion of culture after the invention\n\nof fire must also be about tools.” Similarly, in the DevOps value stream, we use tools to reinforce our culture and accelerate\n\ndesired behavior changes.\n\nOne goal is that our tooling reinforces that Development and Operations not only have shared goals, but have a common\n\nbacklog of work, ideally stored in a common work system and\n\nusing a shared vocabulary, so that work can be prioritized globally.\n\nBy doing this, Development and Operations may end up creating a\n\nshared work queue, instead of each silo using a different one (e.g.,\n\nDevelopment uses JIRA while Operations uses ServiceNow). A significant benefit of this is that when production incidents are\n\nshown in the same work systems as development work, it will be\n\nobvious when ongoing incidents should halt other work, especially when we have a kanban board.\n\nAnother benefit of having Development and Operations using a\n\nshared tool is a unified backlog, where everyone prioritizes improvement projects from a global perspective, selecting work\n\nthat has the highest value to the organization or most reduces\n\ntechnical debt. As we identify technical debt, we add it to our prioritized backlog if we can’t address it immediately. For issues\n\nthat remain unaddressed, we can use our “20% time for non-\n\nfunctional requirements” to fix the top items from our backlog.\n\nOther technologies that reinforce shared goals are chat rooms,\n\nsuch as IRC channels, HipChat, Campfire, Slack, Flowdock, and\n\nOpenFire. Chat rooms allow the fast sharing of information (as opposed to filling out forms that are processed through predefined\n\nworkflows), the ability to invite other people as needed, and",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "history logs that are automatically recorded for posterity and can\n\nbe analyzed during post-mortem sessions.\n\nAn amazing dynamic is created when we have a mechanism that\n\nallows any team member to quickly help other team members, or\n\neven people outside their team—the time required to get information or needed work can go from days to minutes. In\n\naddition, because everything is being recorded, we may not need\n\nto ask someone else for help in the future—we simply search for it.\n\nHowever, the rapid communication environment facilitated by\n\nchat rooms can also be a drawback. As Ryan Martens, the founder\n\nand CTO of Rally Software, observes, “In a chat room, if someone doesn’t get an answer in a couple of minutes, it’s totally accepted\n\nand expected that you can bug them again until they get what they\n\nneed.”\n\nThe expectations of immediate response can, of course, lead to\n\nundesired outcomes. A constant barrage of interruptions and\n\nquestions can prevent people from getting necessary work done. As a result, teams may decide that certain types of requests should\n\ngo through more structured and asynchronous tools.\n\nCONCLUSION\n\nIn this chapter, we identified all the teams supporting our value\n\nstream and captured in a value stream map what work is required\n\nin order to deliver value to the customer. The value stream map provides the basis for understanding our current state, including\n\nour lead time and %C/A metrics for problematic areas, and\n\ninforms how we set a future state.",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "This enables dedicated transformation teams to rapidly iterate and\n\nexperiment to improve performance. We also make sure that we allocate a sufficient amount of time for improvement, fixing\n\nknown problems and architectural issues, including our non-\n\nfunctional requirements. The case studies from Nordstrom and LinkedIn demonstrate how dramatic improvements can be made\n\nin lead times and quality when we find problems in our value\n\nstream and pay down technical debt.\n\n† Which makes it all the more important that we limit the level of detail being collected—everyone’s time is\n\nvaluable and scarce.\n\n‡ Conversely, there are many examples of using tools in a way that guarantees no behavior changes occur. For\n\ninstance, an organization commits to an agile planning tool but then configures it for a waterfall process, which merely maintains status quo.",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "7 How to Design Our\n\nOrganization and Architecture with Conway’s Law in\n\nMind\n\nIn the previous chapters, we identified a value stream to start our\n\nDevOps transformation and established shared goals and\n\npractices to enable a dedicated transformation team to improve how we deliver value to the customer.\n\nIn this chapter, we will start thinking about how to organize\n\nourselves to best achieve our value stream goals. After all, how we organize our teams affects how we perform our work. Dr. Melvin\n\nConway performed a famous experiment in 1968 with a contract research organization that had eight people who were\n\ncommissioned to produce a COBOL and an ALGOL compiler. He\n\nobserved, “After some initial estimates of difficulty and time, five\n\npeople were assigned to the COBOL job and three to the ALGOL job. The resulting COBOL compiler ran in five phases, the ALGOL compiler ran in three.”\n\nThese observations led to what is now known as Conway’s Law, which states that “organizations which design systems...are\n\nconstrained to produce designs which are copies of the communication structures of these organizations….The larger an",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "organization is, the less flexibility it has and the more pronounced\n\nthe phenomenon.” Eric S. Raymond, author of the book The\n\nCathedral and the Bazaar: Musings on Linux and Open Source\n\nby an Accidental Revolutionary, crafted a simplified (and now,\n\nmore famous) version of Conway’s Law in his Jargon File: “The\n\norganization of the software and the organization of the software\n\nteam will be congruent; commonly stated as ‘if you have four\n\ngroups working on a compiler, you’ll get a 4-pass compiler.’”\n\nIn other words, how we organize our teams has a powerful effect\n\non the software we produce, as well as our resulting architectural\n\nand production outcomes. In order to get fast flow of work from Development into Operations, with high quality and great customer outcomes, we must organize our teams and our work so\n\nthat Conway’s Law works to our advantage. Done poorly, Conway’s Law will prevent teams from working safely and independently; instead, they will be tightly-coupled together, all waiting on each other for work to be done, with even small changes creating potentially global, catastrophic consequences.\n\nAn example of how Conway’s Law can either impede or reinforce\n\nour goals can be seen in a technology that was developed at Etsy called Sprouter. Etsy’s DevOps journey began in 2009, and is one\n\nof the most admired DevOps organizations, with 2014 revenue of nearly $200 million and a successful IPO in 2015.\n\nOriginally developed in 2007, Sprouter connected people, processes, and technology in ways that created many undesired outcomes. Sprouter, shorthand for “stored procedure router,” was originally designed to help make life easier for the developers and\n\ndatabase teams. As Ross Snyder, a senior engineer at Etsy, said during his presentation at Surge 2011, “Sprouter was designed to",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "allow the Dev teams to write PHP code in the application, the\n\nDBAs to write SQL inside Postgres, with Sprouter helping them\n\nmeet in the middle.”\n\nSprouter resided between their front-end PHP application and the\n\nPostgres database, centralizing access to the database and hiding\n\nthe database implementation from the application layer. The\n\nproblem was that adding any changes to business logic resulted in\n\nsignificant friction between developers and the database teams. As\n\nSnyder observed, “For nearly any new site functionality, Sprouter\n\nrequired that the DBAs write a new stored procedure. As a result,\n\nevery time developers wanted to add new functionality, they would need something from the DBAs, which often required them to wade through a ton of bureaucracy.” In other words, developers\n\ncreating new functionality had a dependency on the DBA team, which needed to be prioritized, communicated, and coordinated, resulting in work sitting in queues, meetings, longer lead times, and so forth. This is because Sprouter created a tight coupling between the development and database teams, preventing developers from being able to independently develop, test, and deploy their code into production.\n\nAlso, the database stored procedures were tightly-coupled to\n\nSprouter—any time a stored procedure was changed, it required changes to Sprouter too. The result was that Sprouter became an ever-larger single point of failure. Snyder explained that everything was so tightly-coupled and required such a high level of synchronization as a result, that almost every deployment caused a mini-outage.\n\nBoth the problems associated with Sprouter and their eventual solution can be explained by Conway’s Law. Etsy initially had two",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "teams, the developers and the DBAs, who were each responsible for two layers of the service, the application logic layer and stored\n\nprocedure layer. Two teams working on two layers, as Conway’s Law predicts. Sprouter was intended to make life easier for both\n\nteams, but it didn’t work as expected—when business rules\n\nchanged, instead of changing only two layers, they now needed to make changes to three layers (in the application, in the stored\n\nprocedures, and now in Sprouter). The resulting challenges of coordinating and prioritizing work across three teams significantly\n\nincreased lead times and caused reliability problems.\n\nIn the spring of 2009, as part of what Snyder called “the great Etsy cultural transformation,” Chad Dickerson joined as their new\n\nCTO. Dickerson put into motion many things, including a massive\n\ninvestment into site stability, having developers perform their own deployments into production, as well as beginning a two-year\n\njourney to eliminate Sprouter.\n\nTo do this, the team decided to move all the business logic from the database layer into the application layer, removing the need\n\nfor Sprouter. They created a small team that wrote a PHP Object Relational Mapping (ORM) layer,† enabling the front-end developers to make calls directly to the database and reducing the\n\nnumber of teams required to change business logic from three teams down to one team.\n\nAs Snyder described, “We started using the ORM for any new\n\nareas of the site and migrated small parts of our site from Sprouter to the ORM over time. It took us two years to migrate the entire\n\nsite off of Sprouter. And even though we all grumbled about Sprouter the entire time, it remained in production throughout.”",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "By eliminating Sprouter, they also eliminated the problems associated with multiple teams needing to coordinate for business\n\nlogic changes, decreased the number of handoffs, and significantly increased the speed and success of production deployments,\n\nimproving site stability. Furthermore, because small teams could independently develop and deploy their code without requiring\n\nanother team to make changes in other areas of the system,\n\ndeveloper productivity increased.\n\nSprouter was finally removed from production and Etsy’s version control repositories in early 2001. As Snyder said, “Wow, it felt good.”‡\n\nAs Snyder and Etsy experienced, how we design our organization dictates how work is performed, and, therefore, the outcomes we\n\nachieve. Throughout the rest of this chapter we will explore how Conway’s Law can negatively impact the performance of our value\n\nstream, and, more importantly, how we organize our teams to use\n\nConway’s Law to our advantage.\n\nORGANIZATIONAL ARCHETYPES\n\nIn the field of decision sciences, there are three primary types of organizational structures that inform how we design our DevOps\n\nvalue streams with Conway’s Law in mind: functional, matrix, and market. They are defined by Dr. Roberto Fernandez as follows:\n\nFunctional-oriented organizations optimize for expertise,\n\ndivision of labor, or reducing cost. These organizations\n\ncentralize expertise, which helps enable career growth and skill development, and often have tall hierarchical organizational",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "structures. This has been the prevailing method of organization\n\nfor Operations (i.e., server admins, network admins, database\n\nadmins, and so forth are all organized into separate groups).\n\nMatrix-oriented organizations attempt to combine functional and market orientation. However, as many who work in or\n\nmanage matrix organizations observe, matrix organizations often result in complicated organizational structures, such as\n\nindividual contributors reporting to two managers or more, and sometimes achieving neither of the goals of functional or\n\nmarket orientation.\n\nMarket-oriented organizations optimize for responding quickly\n\nto customer needs. These organizations tend to be flat, composed of multiple, cross-functional disciplines (e.g.,\n\nmarketing, engineering, etc.), which often lead to potential redundancies across the organization. This is how many\n\nprominent organizations adopting DevOps operate—in extreme examples, such as at Amazon or Netflix, each service team is\n\nsimultaneously responsible for feature delivery and service support.§\n\nWith these three categories of organizations in mind, let’s explore further how an overly functional orientation, especially in\n\nOperations, can cause undesired outcomes in the technology value stream, as Conway’s Law would predict.\n\nPROBLEMS OFTEN CAUSED BY OVERLY FUNCTIONAL ORIENTATION (“OPTIMIZING FOR COST”)",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "In traditional IT Operations organizations, we often use functional orientation to organize our teams by their specialties. We put the\n\ndatabase administrators in one group, the network administrators in another, the server administrators in a third, and so forth. One of the most visible consequences of this is long lead times,\n\nespecially for complex activities like large deployments where we must open up tickets with multiple groups and coordinate work handoffs, resulting in our work waiting in long queues at every step.\n\nCompounding the issue, the person performing the work often has little visibility or understanding of how their work relates to any\n\nvalue stream goals (e.g., “I’m just configuring servers because someone told me to.”). This places workers in a creativity and motivation vacuum.\n\nThe problem is exacerbated when each Operations functional area has to serve multiple value streams (i.e., multiple Development teams) who all compete for their scarce cycles. In order for Development teams to get their work done in a timely manner, we\n\noften have to escalate issues to a manager or director, and eventually to someone (usually an executive) who can finally prioritize the work against the global organizational goals instead\n\nof the functional silo goals. This decision must then get cascaded down into each of the functional areas to change the local priorities, and this, in turn, slows down other teams. When every team expedites their work, the net result is that every project ends\n\nup moving at the same slow crawl.\n\nIn addition to long queues and long lead times, this situation results in poor handoffs, large amounts of re-work, quality issues,\n\nbottlenecks, and delays. This gridlock impedes the achievement of",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "important organizational goals, which often far outweigh the desire to reduce costs.¶\n\nSimilarly, functional orientation can also be found with centralized QA and Infosec functions, which may have worked fine (or at least, well enough) when performing less frequent software\n\nreleases. However, as we increase the number of Development teams and their deployment and release frequencies, most functionally-oriented organizations will have difficulty keeping up\n\nand delivering satisfactory outcomes, especially when their work is being performed manually. Now we’ll study how market oriented organizations work.\n\nENABLE MARKET-ORIENTED TEAMS (“OPTIMIZING FOR SPEED”)\n\nBroadly speaking, to achieve DevOps outcomes, we need to reduce the effects of functional orientation (“optimizing for cost”) and enable market orientation (“optimizing for speed”) so we can have\n\nmany small teams working safely and independently, quickly delivering value to the customer.\n\nTaken to the extreme, market-oriented teams are responsible not\n\nonly for feature development, but also for testing, securing, deploying, and supporting their service in production, from idea conception to retirement. These teams are designed to be cross- functional and independent—able to design and run user\n\nexperiments, build and deliver new features, deploy and run their service in production, and fix any defects without manual dependencies on other teams, thus enabling them to move faster.\n\nThis model has been adopted by Amazon and Netflix and is touted",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "by Amazon as one of the primary reasons behind their ability to move fast even as they grow.\n\nTo achieve market orientation, we won’t do a large, top-down reorganization, which often creates large amounts of disruption, fear, and paralysis. Instead, we will embed the functional\n\nengineers and skills (e.g., Ops, QA, Infosec) into each service team, or provide their capabilities to teams through automated self-service platforms that provide production-like environments, initiate automated tests, or perform deployments.\n\nThis enables each service team to independently deliver value to the customer without having to open tickets with other groups, such as IT Operations, QA, or Infosec.**\n\nMAKING FUNCTIONAL ORIENTATION WORK\n\nHaving just recommended market-orientated teams, it is worth pointing out that it is possible to create effective, high-velocity\n\norganizations with functional orientation. Cross-functional and market-oriented teams are one way to achieve fast flow and reliability, but they are not the only path. We can also achieve our desired DevOps outcomes through functional orientation, as long\n\nas everyone in the value stream views customer and organizational outcomes as a shared goal, regardless of where they reside in the organization.",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Figure 12: Functional vs. market orientation\n\nLeft: Functional orientation: all work flows through centralized IT Operations; Right: Market orientation: all product teams can deploy their loosely-coupled components self-service into production. (Source: Humble, Molesky, and O’Reilly, Lean Enterprise, Kindle edition, 4523 & 4592.)\n\nFor example, high performance with a functional-oriented and centralized Operations group is possible, as long as service teams\n\nget what they need from Operations reliably and quickly (ideally on demand) and vice-versa. Many of the most admired DevOps organizations retain functional orientation of Operations,\n\nincluding Etsy, Google, and GitHub.\n\nWhat these organizations have in common is a high-trust culture that enables all departments to work together effectively, where all\n\nwork is transparently prioritized and there is sufficient slack in the system to allow high-priority work to be completed quickly. This is, in part, enabled by automated self-service platforms that build\n\nquality into the products everyone is building.\n\nIn the Lean manufacturing movement of the 1980s, many researchers were puzzled by Toyota’s functional orientation, which\n\nwas at odds with the best practice of having cross-functional,",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "market-oriented teams. They were so puzzled it was called “the second Toyota paradox.”\n\nAs Mike Rother wrote in Toyota Kata, “As tempting as it seems, one cannot reorganize your way to continuous improvement and adaptiveness. What is decisive is not the form of the organization, but how people act and react. The roots of Toyota’s success lie not\n\nin its organizational structures, but in developing capability and habits in its people. It surprises many people, in fact, to find that Toyota is largely organized in a traditional, functional-department\n\nstyle.” It is this development of habits and capabilities in people and the workforce that are the focus of our next sections.\n\nTESTING, OPERATIONS, AND SECURITY AS EVERYONE’S JOB, EVERY DAY\n\nIn high-performing organizations, everyone within the team\n\nshares a common goal—quality, availability, and security aren’t the responsibility of individual departments, but are a part of everyone’s job, every day.\n\nThis means that the most urgent problem of the day may be working on or deploying a customer feature or fixing a Severity 1 production incident. Alternatively, the day may require reviewing\n\na fellow engineer’s change, applying emergency security patches to production servers, or making improvements so that fellow engineers are more productive.\n\nReflecting on shared goals between Development and Operations, Jody Mulkey, CTO at Ticketmaster, said, “For almost 25 years, I used an American football metaphor to describe Dev and Ops. You",
      "content_length": 1501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "know, Ops is defense, who keeps the other team from scoring, and Dev is offense, trying to score goals. And one day, I realized how flawed this metaphor was, because they never all play on the field\n\nat the same time. They’re not actually on the same team!”\n\nHe continued, “The analogy I use now is that Ops are the offensive linemen, and Dev are the ‘skill’ positions (like the quarterback and\n\nwide receivers) whose job it is to move the ball down the field—the job of Ops is to help make sure Dev has enough time to properly execute the plays.”\n\nA striking example of how shared pain can reinforce shared goals is when Facebook was undergoing enormous growth in 2009. They were experiencing significant problems related to code\n\ndeployments—while not all issues caused customer-impacting issues, there was chronic firefighting and long hours. Pedro Canahuati, their director of production engineering, described a meeting full of Ops engineers where someone asked that all people\n\nnot working on an incident close their laptops, and no one could.\n\nOne of the most significant things they did to help change the\n\noutcomes of deployments was to have all Facebook engineers, engineering managers, and architects rotate through on-call duty for the services they built. By doing this, everyone who worked on the service experienced visceral feedback on the upstream\n\narchitectural and coding decisions they made, which made an enormous positive impact on the downstream outcomes.\n\nENABLE EVERY TEAM MEMBER TO BE A GENERALIST",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "In extreme cases of a functionally-oriented Operations\n\norganization, we have departments of specialists, such as network\n\nadministrators, storage administrators, and so forth. When departments over-specialize, it causes siloization, which Dr. Spear\n\ndescribes as when departments “operate more like sovereign\n\nstates.” Any complex operational activity then requires multiple handoffs and queues between the different areas of the\n\ninfrastructure, leading to longer lead times (e.g., because every network change must be made by someone in the networking\n\ndepartment).\n\nBecause we rely upon an ever increasing number of technologies,\n\nwe must have engineers who have specialized and achieved mastery in the technology areas we need. However, we don’t want\n\nto create specialists who are “frozen in time,” only understanding and able to contribute to that one area of the value stream.\n\nOne countermeasure is to enable and encourage every team\n\nmember to be a generalist. We do this by providing opportunities for engineers to learn all the skills necessary to build and run the\n\nsystems they are responsible for, and regularly rotating people\n\nthrough different roles. The term full stack engineer is now commonly used (sometimes as a rich source of parody) to describe\n\ngeneralists who are familiar—at least have a general level of\n\nunderstanding—with the entire application stack (e.g., application code, databases, operating systems, networking, cloud).",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Table 2: Specialists vs. Generalists vs. “E-shaped” Staff (experience, expertise, exploration, and execution)\n\n(Source: Scott Prugh, “Continuous Delivery,” ScaledAgileFramework.com, February 14, 2013, http://scaledagileframework.com/continuous-delivery/.)\n\nScott Prugh writes that CSG International has undergone a\n\ntransformation that brings most resources required to build and\n\nrun the product onto one team, including analysis, architecture, development, test, and operations. “By cross-training and growing\n\nengineering skills, generalists can do orders of magnitude more work than their specialist counterparts, and it also improves our\n\noverall flow of work by removing queues and wait time.” This\n\napproach is at odds with traditional hiring practices, but, as Prugh explains, it is well worth it. “Traditional managers will often object\n\nto hiring engineers with generalist skill sets, arguing that they are\n\nmore expensive and that ‘I can hire two server administrators for every multi-skilled operations engineer.’” However, the business\n\nbenefits of enabling faster flow are overwhelming. Furthermore,\n\nas Prugh notes, “[I]nvesting in cross training is the right thing for",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "[employees’] career growth, and makes everyone’s work more\n\nfun.”\n\nWhen we value people merely for their existing skills or performance in their current role rather than for their ability to\n\nacquire and deploy new skills, we (often inadvertently) reinforce\n\nwhat Dr. Carol Dweck describes as the fixed mindset, where people view their intelligence and abilities as static “givens” that\n\ncan’t be changed in meaningful ways.\n\nInstead, we want to encourage learning, help people overcome learning anxiety, help ensure that people have relevant skills and a\n\ndefined career road map, and so forth. By doing this, we help\n\nfoster a growth mindset in our engineers—after all, a learning organization requires people who are willing to learn. By\n\nencouraging everyone to learn, as well as providing training and support, we create the most sustainable and least expensive way to\n\ncreate greatness in our teams—by investing in the development of\n\nthe people we already have.\n\nAs Jason Cox, Director of Systems Engineering at Disney, described, “Inside of Operations, we had to change our hiring\n\npractices. We looked for people who had ‘curiosity, courage, and candor,’ who were not only capable of being generalists but also\n\nrenegades...We want to promote positive disruption so our\n\nbusiness doesn’t get stuck and can move into the future.” As we’ll see in the next section, how we fund our teams also affects our\n\noutcomes.\n\nFUND NOT PROJECTS, BUT SERVICES AND PRODUCTS",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Another way to enable high-performing outcomes is to create\n\nstable service teams with ongoing funding to execute their own\n\nstrategy and road map of initiatives. These teams have the dedicated engineers needed to deliver on concrete commitments\n\nmade to internal and external customers, such as features, stories, and tasks.\n\nContrast this to the more traditional model where Development\n\nand Test teams are assigned to a “project” and then reassigned to another project as soon as the project is completed and funding\n\nruns out. This leads to all sorts of undesired outcomes, including\n\ndevelopers being unable to see the long-term consequences of decisions they make (a form of feedback) and a funding model\n\nthat only values and pays for the earliest stages of the software life\n\ncycle—which, tragically, is also the least expensive part for successful products or services.††\n\nOur goal with a product-based funding model is to value the\n\nachievement of organizational and customer outcomes, such as revenue, customer lifetime value, or customer adoption rate,\n\nideally with the minimum of output (e.g., amount of effort or time,\n\nlines of code). Contrast this to how projects are typically measured, such as whether it was completed within the promised\n\nbudget, time, and scope.\n\nDESIGN TEAM BOUNDARIES IN ACCORDANCE WITH CONWAY’S LAW\n\nAs organizations grow, one of the largest challenges is maintaining effective communication and coordination between people and\n\nteams. All too often, when people and teams reside on a different",
      "content_length": 1535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "floor, in a different building, or in a different time zone, creating\n\nand maintaining a shared understanding and mutual trust becomes more difficult, impeding effective collaboration.\n\nCollaboration is also impeded when the primary communication\n\nmechanisms are work tickets and change requests, or worse, when teams are separated by contractual boundaries, such as when\n\nwork is performed by an outsourced team.\n\nAs we saw in the Etsy Sprouter example at the beginning of this chapter, the way we organize teams can create poor outcomes, a\n\nside effect of Conway’s Law. These include splitting teams by\n\nfunction (e.g., by putting developers and testers in different locations or by outsourcing testers entirely) or by architectural\n\nlayer (e.g., application, database).\n\nThese configurations require significant communication and coordination between teams, but still results in a high amount of\n\nrework, disagreements over specifications, poor handoffs, and\n\npeople sitting idle waiting for somebody else.\n\nIdeally, our software architecture should enable small teams to be\n\nindependently productive, sufficiently decoupled from each other\n\nso that work can be done without excessive or unnecessary communication and coordination.\n\nCREATE LOOSELY-COUPLED ARCHITECTURES TO ENABLE DEVELOPER PRODUCTIVITY AND SAFETY\n\nWhen we have a tightly-coupled architecture, small changes can\n\nresult in large scale failures. As a result, anyone working in one",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "part of the system must constantly coordinate with anyone else\n\nworking in another part of the system they may affect, including navigating complex and bureaucratic change management\n\nprocesses.\n\nFurthermore, to test that the entire system works together requires integrating changes with the changes from hundreds, or\n\neven thousands, of other developers, which may, in turn, have\n\ndependencies on tens, hundreds, or thousands of interconnected systems. Testing is done in scarce integration test environments,\n\nwhich often require weeks to obtain and configure. The result is\n\nnot only long lead times for changes (typically measured in weeks or months) but also low developer productivity and poor\n\ndeployment outcomes.\n\nIn contrast, when we have an architecture that enables small teams of developers to independently implement, test, and deploy\n\ncode into production safely and quickly, we can increase and\n\nmaintain developer productivity and improve deployment outcomes. These characteristics can be found in service-oriented\n\narchitectures (SOAs) first described in the 1990s, in which\n\nservices are independently testable and deployable. A key feature of SOAs is that they’re composed of loosely-coupled services with bounded contexts.‡‡\n\nHaving architecture that is loosely-coupled means that services can update in production independently, without having to update\n\nother services. Services must be decoupled from other services\n\nand, just as important, from shared databases (although they can share a database service, provided they don’t have any common\n\nschemas).",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Bounded contexts are described in the book Domain Driven\n\nDesign by Eric J. Evans. The idea is that developers should be able to understand and update the code of a service without knowing\n\nanything about the internals of its peer services. Services interact\n\nwith their peers strictly through APIs and thus don’t share data structures, database schemata, or other internal representations of\n\nobjects. Bounded contexts ensure that services are\n\ncompartmentalized and have well-defined interfaces, which also enables easier testing.\n\nRandy Shoup, former Engineering Director for Google App\n\nEngine, observed that “organizations with these types of service- oriented architectures, such as Google and Amazon, have\n\nincredible flexibility and scalability. These organizations have tens\n\nof thousands of developers where small teams can still be incredibly productive.”\n\nKEEP TEAM SIZES SMALL (THE “TWO-PIZZA TEAM” RULE)\n\nConway’s Law helps us design our team boundaries in the context of desired communication patterns, but it also encourages us to\n\nkeep our team sizes small, reducing the amount of inter-team\n\ncommunication and encouraging us to keep the scope of each team’s domain small and bounded.\n\nAs part of its transformation initiative away from a monolithic\n\ncode base in 2002, Amazon used the two-pizza rule to keep team sizes small—a team only as large as can be fed with two pizzas—\n\nusually about five to ten people.\n\nThis limit on size has four important effects:",
      "content_length": 1477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "1. It ensures the team has a clear, shared understanding of the\n\nsystem they are working on. As teams get larger, the amount of communication required for everybody to know what’s going\n\non scales in a combinatorial fashion.\n\n2. It limits the growth rate of the product or service being worked on. By limiting the size of the team, we limit the rate at which their system can evolve. This also helps to ensure the team\n\nmaintains a shared understanding of the system.\n\n3. It decentralizes power and enables autonomy. Each two-pizza\n\nteam (2PT) is as autonomous as possible. The team’s lead,\n\nworking with the executive team, decides on the key business metric that the team is responsible for, known as the fitness\n\nfunction, which becomes the overall evaluation criteria for the\n\nteam’s experiments. The team is then able to act autonomously to maximize that metric.§§\n\n4. Leading a 2PT is a way for employees to gain some leadership\n\nexperience in an environment where failure does not have catastrophic consequences. An essential element of Amazon’s\n\nstrategy was the link between the organizational structure of a\n\n2PT and the architectural approach of a service-oriented architecture.\n\nAmazon CTO Werner Vogels explained the advantages of this\n\nstructure to Larry Dignan of Baseline in 2005. Dignan writes:\n\n“Small teams are fast...and don’t get bogged down in so-called\n\nadministrivia….Each group assigned to a particular business is\n\ncompletely responsible for it….The team scopes the fix, designs it, builds it, implements it and monitors its ongoing use. This",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "way, technology programmers and architects get direct\n\nfeedback from the business people who use their code or applications—in regular meetings and informal conversations.”\n\nAnother example of how architecture can profoundly improve\n\nproductivity is the API Enablement program at Target, Inc.\n\nCase Study API Enablement at Target (2015)\n\nTarget is the sixth-largest retailer in the US and spends over $1 billion on technology annually. Heather Mickman, a\n\ndirector of development for Target, described the beginnings\n\nof their DevOps journey: “In the bad old days, it used to take ten different teams to provision a server at Target, and when\n\nthings broke, we tended to stop making changes to prevent\n\nfurther issues, which of course makes everything worse.”\n\nThe hardships associated with getting environments and\n\nperforming deployments created significant difficulties for\n\ndevelopment teams, as did getting access to data they needed. As Mickman described:\n\nThe problem was that much of our core data, such as\n\ninformation on inventory, pricing, and stores, was locked up in legacy systems and mainframes. We\n\noften had multiple sources of truths of data, especially between e-commerce and our physical stores, which\n\nwere owned by different teams, with different data\n\nstructures and different priorities....The result was that if a new development team wanted to build something",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "for our guests, it would take three to six months to\n\nbuild the integrations to get the data they needed. Worse, it would take another three to six months to do\n\nthe manual testing to make sure they didn’t break\n\nanything critical, because of how many custom point- to-point integrations we had in a very tightly-coupled\n\nsystem. Having to manage the interactions with the\n\ntwenty to thirty different teams, along with all their dependencies, required lots of project managers,\n\nbecause of all the coordination and handoffs. It meant that development was spending all their time waiting in\n\nqueues, instead of delivering results and getting stuff\n\ndone.\n\nThis long lead time for retrieving and creating data in their\n\nsystems of record was jeopardizing important business\n\ngoals, such as integrating the supply chain operations of Target’s physical stores and their e-commerce site, which\n\nnow required getting inventory to stores and customer\n\nhomes. This pushed the Target supply chain well beyond what it was designed for, which was merely to facilitate the\n\nmovement of goods from vendors to distribution centers and\n\nstores.\n\nIn an attempt to solve the data problem, in 2012 Mickman\n\nled the API Enablement team to enable development teams\n\nto “deliver new capabilities in days instead of months.” They wanted any engineering team inside of Target to be able to\n\nget and store the data they needed, such as information on\n\ntheir products or their stores, including operating hours,",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "location, whether there was as Starbucks on-site, and so\n\nforth.\n\nTime constraints played a large role in team selection.\n\nMickman explained that:\n\nBecause our team also needed to deliver capabilities in days, not months, I needed a team who could do the\n\nwork, not give it to contractors—we wanted people\n\nwith kickass engineering skills, not people who knew how to manage contracts. And to make sure our work\n\nwasn’t sitting in queue, we needed to own the entire\n\nstack, which meant that we took over the Ops requirements as well....We brought in many new tools\n\nto support continuous integration and continuous\n\ndelivery. And because we knew that if we succeeded, we would have to scale with extremely high growth, we\n\nbrought in new tools such as the Cassandra database and Kafka message broker. When we asked for\n\npermission, we were told no, but we did it anyway,\n\nbecause we knew we needed it.\n\nIn the following two years, the API Enablement team\n\nenabled fifty-three new business capabilities, including Ship\n\nto Store and Gift Registry, as well as their integrations with Instacart and Pinterest. As Mickman described, “Working\n\nwith Pinterest suddenly became very easy, because we just\n\nprovided them our APIs.”\n\nIn 2014, the API Enablement team served over 1.5 billion\n\nAPI calls per month. By 2015, this had grown to seventeen\n\nbillion calls per month spanning ninety different APIs. To",
      "content_length": 1398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "support this capability, they routinely performed eighty\n\ndeployments per week.\n\nThese changes have created major business benefits for\n\nTarget—digital sales increased 42% during the 2014 holiday\n\nseason and increased another 32% in Q2. During the Black Friday weekend of 2015, over 280k in-store pickup orders\n\nwere created. By 2015, their goal is to enable 450 of their\n\n1,800 stores to be able to fulfill e-commerce orders, up from one hundred.\n\n“The API Enablement team shows what a team of\n\npassionate change agents can do,” Mickman says. “And it help set us up for the next stage, which is to expand\n\nDevOps across the entire technology organization.”\n\nCONCLUSION\n\nThrough the Etsy and Target case studies, we can see how\n\narchitecture and organizational design can dramatically improve our outcomes. Done incorrectly, Conway’s Law will ensure that the\n\norganization creates poor outcomes, preventing safety and agility.\n\nDone well, the organization enables developers to safely and\n\nindependently develop, test, and deploy value to the customer.\n\n† Among many things, an ORM abstracts a database, enabling developers to do queries and data manipulation as if they were merely another object in the programming language. Popular ORMs include Hibernate for Java, SQLAlchemy for Python, and ActiveRecord for Ruby on Rails.\n\n‡ Sprouter was one of many technologies used in development and production that Etsy eliminated as part of their\n\ntransformation.\n\n§ However, as will be explained later, equally prominent organizations such as Etsy and GitHub have functional\n\norientation.\n\n¶ Adrian Cockcroft remarked, “For companies who are now coming off of five-year IT outsourcing contracts, it’s like they’ve been frozen in time, during one of the most disruptive times in technology.” In other words, IT",
      "content_length": 1804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "outsourcing is a tactic used to control costs through contractually-enforced stasis, with firm fixed prices that schedule annual cost reductions. However, it often results in organizations being unable to respond to changing business and technology needs.\n\n** For the remainder of this books, we will use service teams interchangeably with feature teams, product teams, development teams, and delivery teams. The intent is to specify the team primarily developing, testing, and securing the code so that value is delivered to the customer.\n\n†† As John Lauderbach, currently VP of Information Technology at Roche Bros. Supermarkets, quipped, “Every\n\nnew application is like a free puppy. It’s not the upfront capital cost that kills you…It’s the ongoing maintenance and support.”\n\n‡‡ These properties are also found in “microservices,” which build upon the principles of SOA. One popular set of\n\npatterns for modern web architecture based on these principles is the “12-factor app.”\n\n§§ In the Netflix culture, one of the seven key values is “highly aligned, loosely-coupled.”",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "8 How to Get Great\n\nOutcomes by Integrating Operations into the Daily Work of\n\nDevelopment\n\nOur goal is to enable market-oriented outcomes where many small teams can quickly and independently deliver value to the\n\ncustomer. This can be a challenge to achieve when Operations is\n\ncentralized and functionally-oriented, having to serve the needs of many different development teams with potentially wildly\n\ndifferent needs. The result can often be long lead times for needed Ops work, constant reprioritization and escalation, and poor\n\ndeployment outcomes.\n\nWe can create more market-oriented outcomes by better\n\nintegrating Ops capabilities into Dev teams, making both more efficient and productive. In this chapter, we’ll explore many ways to achieve this, both at the organizational level and through daily\n\nrituals. By doing this, Ops can significantly improve the productivity of Dev teams throughout the entire organization, as well as enable better collaboration and organizational outcomes.",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "At Big Fish Games, which develops and supports hundreds of\n\nmobile and thousands of PC games and had more than $266\n\nmillion in revenue in 2013, VP of IT Operations Paul Farrall was in\n\ncharge of the centralized Operations organization. He was\n\nresponsible for supporting many different business units that had\n\na great deal of autonomy.\n\nEach of these business units had dedicated development teams\n\nwho often chose wildly different technologies. When these groups\n\nwanted to deploy new functionality, they would have to compete\n\nfor a common pool of scarce Ops resources. Furthermore,\n\neveryone was struggling with unreliable Test and Integration environments, as well as extremely cumbersome release processes.\n\nFarrall thought the best way to solve this problem was by embedding Ops expertise into Development teams. He observed, “When Dev teams had problems with testing or deployment, they needed more than just technology or environments. What they also needed was help and coaching. At first, we embedded Ops engineers and architects into each of the Dev teams, but there\n\nsimply weren’t enough Ops engineers to cover that many teams. We were able to help more teams with what we called an Ops\n\nliaison model and with fewer people.”\n\nFarrall defined two types of Ops liaisons: the business relationship manager and the dedicated release engineer. The business relationship managers worked with product management, line-of- business owners, project management, Dev management, and developers. They became intimately familiar with product group\n\nbusiness drivers and product road maps, acted as advocates for product owners inside of Operations, and helped their product",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "teams navigate the Operations landscape to prioritize and\n\nstreamline work requests.\n\nSimilarly, the dedicated release engineer became intimately\n\nfamiliar with the product’s Development and QA issues, and\n\nhelped them get what they needed from the Ops organization to\n\nachieve their goals. They were familiar with the typical Dev and\n\nQA requests for Ops, and would often execute the needed work\n\nthemselves. As needed, they would also pull in dedicated technical\n\nOps engineers (e.g., DBAs, Infosec, storage engineers, network\n\nengineers), and help determine which self-service tools the entire\n\nOperations group should prioritize building.\n\nBy doing this, Farrall was able to help Dev teams across the organization become more productive and achieve their team goals. Furthermore, he helped the teams prioritize around his global Ops constraints, reducing the number of surprises\n\ndiscovered mid-project and ultimately increasing the overall project throughput.\n\nFarrall notes that both working relationships with Operations and code release velocity were noticeably improved as a result of the changes. He concludes, “The Ops liaison model allowed us to\n\nembed IT Operations expertise into the Dev and Product teams without adding new headcount.”\n\nThe DevOps transformation at Big Fish Games shows how a centralized Operations team was able to achieve the outcomes\n\ntypically associated with market-oriented teams. We can employ the three following broad strategies:",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Create self-service capabilities to enable developers in the service teams to be productive.\n\nEmbed Ops engineers into the service teams.\n\nAssign Ops liaisons to the service teams when embedding Ops\n\nis not possible.\n\nLastly, we describe how Ops engineers can integrate into the Dev team rituals used in their daily work, including daily standups,\n\nplanning, and retrospectives.\n\nCREATE SHARED SERVICES TO INCREASE DEVELOPER PRODUCTIVITY\n\nOne way to enable market-oriented outcomes is for Operations to\n\ncreate a set of centralized platforms and tooling services that any Dev team can use to become more productive, such as getting\n\nproduction-like environments, deployment pipelines, automated testing tools, production telemetry dashboards, and so forth.† By doing this, we enable Dev teams to spend more time building functionality for their customer, as opposed to obtaining all the\n\ninfrastructure required to deliver and support that feature in\n\nproduction.\n\nAll the platforms and services we provide should (ideally) be automated and available on demand, without requiring a\n\ndeveloper to open up a ticket and wait for someone to manually perform work. This ensures that Operations doesn’t become a\n\nbottleneck for their customers (e.g., “We received your work",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "request, and it will take six weeks to manually configure those test environments.”).‡\n\nBy doing this, we enable the product teams to get what they need,\n\nwhen they need it, as well as reduce the need for communications and coordination. As Damon Edwards observed, “Without these\n\nself-service Operations platforms, the cloud is just Expensive Hosting 2.0.”\n\nIn almost all cases, we will not mandate that internal teams use\n\nthese platforms and services—these platform teams will have to\n\nwin over and satisfy their internal customers, sometimes even competing with external vendors. By creating this effective\n\ninternal marketplace of capabilities, we help ensure that the platforms and services we create are the easiest and most\n\nappealing choice available (the path of least resistance).\n\nFor instance, we may create a platform that provides a shared version control repository with pre-blessed security libraries, a\n\ndeployment pipeline that automatically runs code quality and\n\nsecurity scanning tools, which deploys our applications into known, good environments that already have production\n\nmonitoring tools installed on them. Ideally, we make life so much easier for Dev teams that they will overwhelmingly decide that\n\nusing our platform is the easiest, safest, and most secure means to get their applications into production.\n\nWe build into these platforms the cumulative and collective\n\nexperience of everyone in the organization, including QA,\n\nOperations, and Infosec, which helps to create an ever safer system of work. This increases developer productivity and makes\n\nit easy for product teams to leverage common processes, such as",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "performing automated testing and satisfying security and\n\ncompliance requirements.\n\nCreating and maintaining these platforms and tools is real product\n\ndevelopment—the customers of our platform aren’t our external customer but our internal Dev teams. Like creating any great\n\nproduct, creating great platforms that everyone loves doesn’t happen by accident. An internal platform team with poor\n\ncustomer focus will likely create tools that everyone will hate and quickly abandon for other alternatives, whether for another\n\ninternal platform team or an external vendor.\n\nDianne Marsh, Director of Engineering Tools at Netflix, states that\n\nher team’s charter is to “support our engineering teams’ innovation and velocity. We don’t build, bake, or deploy anything\n\nfor these teams, nor do we manage their configurations. Instead, we build tools to enable self-service. It’s okay for people to be\n\ndependent on our tools, but it’s important that they don’t become dependent on us.”\n\nOften, these platform teams provide other services to help their\n\ncustomers learn their technology, migrate off of other\n\ntechnologies, and even provide coaching and consulting to help elevate the state of the practice inside the organization. These\n\nshared services also facilitate standardization, which enable engineers to quickly become productive, even if they switch\n\nbetween teams. For instance, if every product team chooses a different toolchain, engineers may have to learn an entirely new\n\nset of technologies to do their work, putting the team goals ahead of the global goals.",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "In organizations where teams can only use approved tools, we can start by removing this requirement for a few teams, such as the\n\ntransformation team, so that we can experiment and discover what capabilities make those teams more productive.\n\nInternal shared services teams should continually look for internal\n\ntoolchains that are widely being adopted in the organization, deciding which ones make sense to be supported centrally and made available to everyone. In general, taking something that’s\n\nalready working somewhere and expanding its usage is far more likely to succeed than building these capabilities from scratch.§\n\nEMBED OPS ENGINEERS INTO OUR SERVICE TEAMS\n\nAnother way we can enable more market-oriented outcomes is by\n\nenabling product teams to become more self-sufficient by embedding Operations engineers within them, thus reducing their reliance on centralized Operations. These product teams may also\n\nbe completely responsible for service delivery and service support.\n\nBy embedding Operations engineers into the Dev teams, their priorities are driven almost entirely by the goals of the product\n\nteams they are embedded in—as opposed to Ops focusing inwardly on solving their own problems. As a result, Ops engineers become more closely connected to their internal and external customers.\n\nFurthermore, the product teams often have the budget to fund the hiring of these Ops engineers, although interviewing and hiring decisions will likely still be done from the centralized Operations group, to ensure consistency and quality of staff.",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Jason Cox said, “In many parts of Disney we have embedded Ops (system engineers) inside the product teams in our business units,\n\nalong with inside Development, Test, and even Information Security. It has totally changed the dynamics of how we work. As Operations Engineers, we create the tools and capabilities that\n\ntransform the way people work, and even the way they think. In traditional Ops, we merely drove the train that someone else built. But in modern Operations Engineering, we not only help build the train, but also the bridges that the trains roll on.”\n\nFor new large Development projects, we may initially embed Ops engineers into those teams. Their work may include helping decide what to build and how to build it, influencing the product\n\narchitecture, helping influence internal and external technology choices, helping create new capabilities in our internal platforms, and maybe even generating new operational capabilities. After the\n\nproduct is released to production, embedded Ops engineers may help with the production responsibilities of the Dev team.\n\nThey will take part in all of the Dev team rituals, such as planning\n\nmeetings, daily standups, and demonstrations where the team shows off new features and decides which ones to ship. As the need for Ops knowledge and capabilities decreases, Ops engineers\n\nmay transition to different projects or engagements, following the general pattern that the composition within product teams changes throughout its life cycle.\n\nThis paradigm has another important advantage: pairing Dev and Ops engineers together is an extremely efficient way to cross-train operations knowledge and expertise into a service team. It can also have the powerful benefit of transforming operations",
      "content_length": 1750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "knowledge into automated code that can be far more reliable and widely reused.\n\nASSIGN AN OPS LIAISON TO EACH SERVICE TEAM\n\nFor a variety of reasons, such as cost and scarcity, we may be unable to embed Ops engineers into every product team. However, we can get many of the same benefits by assigning a designated\n\nliaison for each product team.\n\nAt Etsy, this model is called “designated Ops.” Their centralized Operations group continues to manage all the environments—not\n\njust production environments but also pre-production environments—to help ensure they remain consistent. The designated Ops engineer is responsible for understanding:\n\nWhat the new product functionality is and why we’re building it\n\nHow it works as it pertains to operability, scalability, and observability (diagramming is strongly encouraged)\n\nHow to monitor and collect metrics to ensure the progress,\n\nsuccess, or failure of the functionality\n\nAny departures from previous architectures and patterns, and the justifications for them\n\nAny extra needs for infrastructure and how usage will affect infrastructure capacity",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Feature launch plans\n\nFurthermore, just like in the embedded Ops model, this liaison\n\nattends the team standups, integrating their needs into the Operations road map and performing any needed tasks. We rely on these liaisons to escalate any resource contention or\n\nprioritization issue. By doing this, we identify any resource or time conflicts that should be evaluated and prioritized in the context of wider organizational goals.\n\nAssigning Ops liaisons allows us to support more product teams than the embedded Ops model. Our goal is to ensure that Ops is not a constraint for the product teams. If we find that Ops liaisons are stretched too thin, preventing the product teams from\n\nachieving their goals, then we will likely need to either reduce the number of teams each liaison supports or temporarily embed an Ops engineer into specific teams.\n\nINTEGRATE OPS INTO DEV RITUALS\n\nWhen Ops engineers are embedded or assigned as liaisons into\n\nour product teams, we can integrate them into our Dev team rituals. In this section, our goal is to help Ops engineers and other non-developers better understand the existing Development\n\nculture and proactively integrate them into all aspects of planning and daily work. As a result, Operations is better able to plan and radiate any needed knowledge into the product teams, influencing work long before it gets into production. The following sections\n\ndescribe some of the standard rituals used by Development teams using agile methods and how we would integrate Ops engineers into them. By no means are agile practices a prerequisite for this",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "step—as Ops engineers, our goal is to discover what rituals the product teams follow, integrate into them, and add value to them.¶\n\nAs Ernest Mueller observed, “I believe DevOps works a lot better if Operations teams adopt the same agile rituals that Dev teams have used—we’ve had fantastic successes solving many problems associated with Ops pain points, as well as integrating better with\n\nDev teams.”\n\nINVITE OPS TO OUR DEV STANDUPS\n\nOne of the Dev rituals popularized by Scrum is the daily standup, a quick meeting where everyone on the team gets together and presents to each other three things: what was done yesterday,\n\nwhat is going to be done today, and what is preventing you from getting your work done.**\n\nThe purpose of this ceremony is to radiate information throughout the team and to understand the work that is being done and is going to be done. By having team members present this information to each other, we learn about any tasks that are\n\nexperiencing roadblocks and discover ways to help each other move our work toward completion. Furthermore, by having managers present, we can quickly resolve prioritization and\n\nresource conflicts.\n\nA common problem is that this information is compartmentalized within the Development team. By having Ops engineers attend,\n\nOperations can gain an awareness of the Development team’s activities, enabling better planning and preparation—for instance, if we discover that the product team is planning a big feature rollout in two weeks, we can ensure that the right people and",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "resources are available to support the rollout. Alternatively, we may highlight areas where closer interaction or more preparation is needed (e.g., creating more monitoring checks or automation\n\nscripts). By doing this, we create the conditions where Operations can help solve our current team problems (e.g., improving performance by tuning the database, instead of optimizing code) or future problems before they turn into a crisis (e.g., creating\n\nmore integration test environments to enable performance testing).\n\nINVITE OPS TO OUR DEV RETROSPECTIVES\n\nAnother widespread agile ritual is the retrospective. At the end of each development interval, the team discusses what was\n\nsuccessful, what could be improved, and how to incorporate the successes and improvements in future iterations or projects. The team comes up with ideas to make things better and reviews\n\nexperiments from the previous iteration. This is one of the primary mechanisms where organizational learning and the development of countermeasures occurs, with resulting work implemented immediately or added to the team’s backlog.\n\nHaving Ops engineers attend our project team retrospectives means they can also benefit from any new learnings. Furthermore,\n\nwhen there is a deployment or release in that interval, Operations should present the outcomes and any resulting learnings, creating\n\nfeedback into the product team. By doing this, we can improve\n\nhow future work is planned and performed, improving our outcomes. Examples of feedback that Operations can bring to a\n\nretrospective include:",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "“Two weeks ago, we found a monitoring blind-spot and agreed\n\non how to fix it. It worked. We had an incident last Tuesday,\n\nand we were able to quickly detect and correct it before any customers were impacted.”\n\n“Last week’s deployment was one of the most difficult and\n\nlengthy we’ve had in over a year. Here are some ideas on how it can be improved.”\n\n“The promotion campaign we did last week was far more\n\ndifficult than we thought it would be, and we should probably not make an offer like that again. Here are some ideas on other\n\noffers we can make to achieve our goals.”\n\n“During the last deployment, the biggest problem we had was our firewall rules are now thousands of lines long, making it\n\nextremely difficult and risky to change. We need to re-architect\n\nhow we prevent unauthorized network traffic.”\n\nFeedback from Operations helps our product teams better see and\n\nunderstand the downstream impact of decisions they make. When\n\nthere are negative outcomes, we can make the changes necessary to prevent them in the future. Operations feedback will also likely\n\nidentify more problems and defects that should be fixed—it may\n\neven uncover larger architectural issues that need to be addressed.\n\nThe additional work identified during project team retrospectives\n\nfalls into the broad category of improvement work, such as fixing\n\ndefects, refactoring, and automating manual work. Product managers and project managers may want to defer or deprioritize\n\nimprovement work in favor of customer features.",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "However, we must remind everyone that improvement of daily\n\nwork is more important than daily work itself, and that all teams\n\nmust have dedicated capacity for this (e.g., reserving 20% of all cycles for improvement work, scheduling one day per week or one\n\nweek per month, etc.). Without doing this, the productivity of the\n\nteam will almost certainly grind to a halt under the weight of its own technical and process debt.\n\nMAKE RELEVANT OPS WORK VISIBLE ON SHARED KANBAN BOARDS\n\nOften, Development teams will make their work visible on a project board or kanban board. It’s far less common, however, for\n\nwork boards to show the relevant Operations work that must be\n\nperformed in order for the application to run successfully in production, where customer value is actually created. As a result,\n\nwe are not aware of necessary Operations work until it becomes an urgent crisis, jeopardizing deadlines or creating a production\n\noutage.\n\nBecause Operations is part of the product value stream, we should put the Operations work that is relevant to product delivery on the\n\nshared kanban board. This enables us to more clearly see all the\n\nwork required to move our code into production, as well as keep track of all Operations work required to support the product.\n\nFurthermore, it enables us to see where Ops work is blocked and\n\nwhere work needs escalation, highlighting areas where we may need improvement.\n\nKanban boards are an ideal tool to create visibility, and visibility is\n\na key component in properly recognizing and integrating Ops work into all the relevant value streams. When we do this well, we",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "achieve market-oriented outcomes, regardless of how we’ve drawn\n\nour organization charts.\n\nCONCLUSION\n\nThroughout this chapter, we explored ways to integrate\n\nOperations into the daily work of Development, and looked at how\n\nto make our work more visible to Operations. To accomplish this, we explored three broad strategies, including creating self-service\n\ncapabilities to enable developers in service teams to be productive,\n\nembedding Ops engineers into the service teams, and assigning Ops liaisons to the service teams when embedding Ops engineers\n\nwas not possible. Lastly, we described how Ops engineers can\n\nintegrate with the Dev team through inclusion in their daily work, including daily standups, planning, and retrospectives.",
      "content_length": 739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "PART II CONCLUSION\n\nIn Part II: Where to Start, we explored a variety of ways to think\n\nabout DevOps transformations, including how to choose where to start, relevant aspects of architecture and organizational design,\n\nand how to organize our teams. We also explored how to integrate\n\nOps into all aspects of Dev planning and daily work.\n\nIn Part III: The First Way, The Technical Practices of Flow, we\n\nwill now start to explore how to implement the specific technical\n\npractices to realize the principles of flow, which enable the fast flow of work from Development to Operations without causing\n\nchaos and disruption downstream.\n\n† The terms platform, shared service, and toolchain will be used interchangeably in this book.\n\n‡ Ernest Mueller observed, “At Bazaarvoice, the agreement was that these platform teams that make tools accept\n\nrequirements, but not work from other teams.”\n\n§ After all, designing a system upfront for re-use is a common and expensive failure mode of many enterprise\n\narchitectures.\n\n¶ However, if we discover that the entire Development organization merely sits at their desks all day without ever\n\ntalking to each other, we may have to find a different way to engage them, such as buying them lunch, starting a book club, taking turns doing “lunch and learn” presentations, or having conversations to discover what everyone’s biggest problems are, so that we can figure out how we can make their lives better.\n\n** Scrum is an agile development methodology, described as “a flexible, holistic product development strategy where a development team works as a unit to reach a common goal.” It was first fully described by Ken Schwaber and Mike Beedle in the book Agile Software Development with Scrum. In this book, we use the term “agile development” or “iterative development” to encompass the various techniques used by special methodologies such as Agile and Scrum.",
      "content_length": 1898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Part\n\nIntroduction\n\nIn Part III, our goal is to create the technical practices and\n\narchitecture required to enable and sustain the fast flow of work\n\nfrom Development into Operations without causing chaos and\n\ndisruption to the production environment or our customers. This means we need to reduce the risk associated with deploying and\n\nreleasing changes into production. We will do this by\n\nimplementing a set of technical practices known as continuous delivery.\n\nContinuous delivery includes creating the foundations of our\n\nautomated deployment pipeline, ensuring that we have automated tests that constantly validate that we are in a deployable state,\n\nhaving developers integrate their code into trunk daily, and architecting our environments and code to enable low-risk\n\nreleases. Primary focuses within these chapters include:\n\nCreating the foundation of our deployment pipeline\n\nEnabling fast and reliable automated testing\n\nEnabling and practicing continuous integration and testing\n\nAutomating, enabling, and architecting for low-risk releases",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Implementing these practices reduces the lead time to get\n\nproduction-like environments, enables continuous testing that\n\ngives everyone fast feedback on their work, enables small teams to\n\nsafely and independently develop, test, and deploy their code into\n\nproduction, and makes production deployments and releases a\n\nroutine part of daily work.\n\nFurthermore, integrating the objectives of QA and Operations into\n\neveryone’s daily work reduces firefighting, hardship, and toil,\n\nwhile making people more productive and increasing joy in the\n\nwork we do. We not only improve outcomes, but our organization\n\nis better able to win in the marketplace.",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "9 Create the\n\nFoundations of Our Deployment Pipeline\n\nIn order to create fast and reliable flow from Dev to Ops, we must\n\nensure that we always use production-like environments at every stage of the value stream. Furthermore, these environments must\n\nbe created in an automated manner, ideally on demand from scripts and configuration information stored in version control,\n\nand entirely self-serviced, without any manual work required from\n\nOperations. Our goal is to ensure that we can re-create the entire production environment based on what’s in version control.\n\nAll too often, the only time we discover how our applications\n\nperform in anything resembling a production-like environment is during production deployment—far too late to correct problems\n\nwithout the customer being adversely impacted. An illustrative example of the spectrum of problems that can be caused by\n\ninconsistently built applications and environments is the Enterprise Data Warehouse program led by Em Campbell-Pretty at a large Australian telecommunications company in 2009. Campbell-Pretty became the general manager and business\n\nsponsor for this $200 million program, inheriting responsibility for all the strategic objectives that relied upon this platform.",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "In her presentation at the 2014 DevOps Enterprise Summit,\n\nCampbell-Pretty explained, “At the time, there were ten streams of\n\nwork in progress, all using waterfall processes, and all ten streams\n\nwere significantly behind schedule. Only one of the ten streams\n\nhad successfully reached User Acceptance Testing [UAT] on\n\nschedule, and it took another six months for that stream to\n\ncomplete UAT, with the resulting capability falling well short of\n\nbusiness expectations. This under-performance was the main\n\ncatalyst for the department’s Agile transformation.”\n\nHowever, after using Agile for nearly a year, they experienced only\n\nsmall improvements, still falling short of their needed business outcomes. Campbell-Pretty held a program-wide retrospective and asked, “After reflecting on all our experiences over the last release,\n\nwhat are things we could do that would double our productivity?”\n\nThroughout the project, there was grumbling about the “lack of business engagement.” However, during the retrospective, “improve availability of environments” was at the top of the list. In hindsight, it was obvious—Development teams needed provisioned environments in order to begin work, and were often\n\nwaiting up to eight weeks.\n\nThey created a new integration and build team that was\n\nresponsible for “building quality into our processes, instead of trying to inspect quality after the fact.” It was initially comprised of database administrators (DBAs) and automation specialists tasked with automating their environment creation process. The team quickly made a surprising discovery: only 50% of the source code in their development and test environments matched what\n\nwas running in production.",
      "content_length": 1701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Campbell-Pretty observed, “Suddenly, we understood why we\n\nencountered so many defects each time we deployed our code into\n\nnew environments. In each environment, we kept fixing forward,\n\nbut the changes we made were not being put back into version\n\ncontrol.”\n\nThe team carefully reverse-engineered all the changes that had\n\nbeen made to the different environments and put them all into\n\nversion control. They also automated their environment creation\n\nprocess so they could repeatedly and correctly spin up\n\nenvironments.\n\nCampbell-Pretty described the results, noting that “the time it took to get a correct environment went from eight weeks to one day. This was one of the key adjustments that allowed us to hit our objectives concerning our lead time, the cost to deliver, and the number of escaped defects that made it into production.”\n\nCampbell-Pretty’s story shows the variety of problems that can be\n\ntraced back to inconsistently constructed environments and changes not being systematically put back into version control.\n\nThroughout the remainder of this chapter, we will discuss how to build the mechanisms that will enable us to create environments on demand, expand the use of version control to everyone in the value stream, make infrastructure easier to rebuild than to repair, and ensure that developers run their code in production-like environments along every stage of the software development life\n\ncycle.",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "ENABLE ON DEMAND CREATION OF DEV, TEST, AND PRODUCTION ENVIRONMENTS\n\nAs seen in the enterprise data warehouse example above, one of the major contributing causes of chaotic, disruptive, and\n\nsometimes even catastrophic software releases, is the first time we ever get to see how our application behaves in a production-like\n\nenvironment with realistic load and production data sets is during the release.† In many cases, development teams may have requested test environments in the early stages of the project.\n\nHowever, when there are long lead times required for Operations\n\nto deliver test environments, teams may not receive them soon enough to perform adequate testing. Worse, test environments are\n\noften mis-configured or are so different from our production environments that we still end up with large production problems\n\ndespite having performed pre-deployment testing.\n\nIn this step, we want developers to run production-like environments on their own workstations, created on demand and\n\nself-serviced. By doing this, developers can run and test their code\n\nin production-like environments as part of their daily work, providing early and constant feedback on the quality their work.\n\nInstead of merely documenting the specifications of the\n\nproduction environment in a document or on a wiki page, we create a common build mechanism that creates all of our\n\nenvironments, such as for development, test, and production. By doing this, anyone can get production-like environments in\n\nminutes, without opening up a ticket, let alone having to wait weeks.‡",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "To do this requires defining and automating the creation of our known, good environments, which are stable, secure, and in a risk-\n\nreduced state, embodying the collective knowledge of the organization. All our requirements are embedded, not in\n\ndocuments or as knowledge in someone’s head, but codified in our automated environment build process.\n\nInstead of Operations manually building and configuring the\n\nenvironment, we can use automation for any or all of the\n\nfollowing:\n\nCopying a virtualized environment (e.g., a VMware image, running a Vagrant script, booting an Amazon Machine Image\n\nfile in EC2)\n\nBuilding an automated environment creation process that starts from “bare metal” (e.g., PXE install from a baseline\n\nimage)\n\nUsing “infrastructure as code” configuration management tools (e.g., Puppet, Chef, Ansible, Salt, CFEngine, etc.)\n\nUsing automated operating system configuration tools (e.g.,\n\nSolaris Jumpstart, Red Hat Kickstart, Debian preseed)\n\nAssembling an environment from a set of virtual images or containers (e.g., Vagrant, Docker)\n\nSpinning up a new environment in a public cloud (e.g., Amazon\n\nWeb Services, Google App Engine, Microsoft Azure), private\n\ncloud, or other PaaS (platform as a service, such as OpenStack or Cloud Foundry, etc.).",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Because we’ve carefully defined all aspects of the environment\n\nahead of time, we are not only able to create new environments\n\nquickly, but also ensure that these environments will be stable, reliable, consistent, and secure. This benefits everyone.\n\nOperations benefits from this capability, to create new\n\nenvironments quickly, because automation of the environment creation process enforces consistency and reduces tedious, error-\n\nprone manual work. Furthermore, Development benefits by being able to reproduce all the necessary parts of the production\n\nenvironment to build, run, and test their code on their\n\nworkstations. By doing this, we enable developers to find and fix many problems, even at the earliest stages of the project, as\n\nopposed to during integration testing or worse, in production.\n\nBy providing developers an environment they fully control, we enable them to quickly reproduce, diagnose, and fix defects, safely\n\nisolated from production services and other shared resources. They can also experiment with changes to the environments, as\n\nwell as to the infrastructure code that creates it (e.g., configuration\n\nmanagement scripts), further creating shared knowledge between Development and Operations.§\n\nCREATE OUR SINGLE REPOSITORY OF TRUTH FOR THE ENTIRE SYSTEM\n\nIn the previous step, we enabled the on demand creation of the\n\ndevelopment, test, and production environments. Now we must ensure that all parts of our software system.",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "For decades, comprehensive use of version control has increasingly become a mandatory practice of individual developers and development teams.¶ A version control system records changes to files or sets of files stored within the system. This can be source code, assets, or other documents that may be\n\npart of a software development project. We make changes in groups called commits or revisions. Each revision, along with metadata such as who made the change and when, is stored within the system in one way or another, allowing us to commit,\n\ncompare, merge, and restore past revisions to objects to the repository. It also minimizes risks by establishing a way to revert objects in production to previous versions. (In this book, the\n\nfollowing terms will be used interchangeably: checked in to version control, committed into version control, code commit, change commit, commit.)\n\nWhen developers put all their application source files and configurations in version control, it becomes the single repository of truth that contains the precise intended state of the system.\n\nHowever, because delivering value to the customer requires both our code and the environments they run in, we need our environments in version control as well. In other words, version control is for everyone in our value stream, including QA,\n\nOperations, Infosec, as well as developers. By putting all production artifacts into version control, our version control repository enables us to repeatedly and reliably reproduce all\n\ncomponents of our working software system—this includes our applications and production environment, as well as all of our pre- production environments.",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "To ensure that we can restore production service repeatedly and predictably (and, ideally, quickly) even when catastrophic events\n\noccur, we must check in the following assets to our shared version control repository:\n\nAll application code and dependencies (e.g., libraries, static\n\ncontent, etc.)\n\nAny script used to create database schemas, application reference data, etc.\n\nAll the environment creation tools and artifacts described in the previous step (e.g., VMware or AMI images, Puppet or Chef\n\nrecipes, etc.)\n\nAny file used to create containers (e.g., Docker or Rocket definition or composition files)\n\nAll supporting automated tests and any manual test scripts\n\nAny script that supports code packaging, deployment, database\n\nmigration, and environment provisioning\n\nAll project artifacts (e.g., requirements documentation, deployment procedures, release notes, etc.)\n\nAll cloud configuration files (e.g., AWS Cloudformation templates, Microsoft Azure Stack DSC files, OpenStack HEAT)\n\nAny other script or configuration information required to create infrastructure that supports multiple services (e.g., enterprise service buses, database management systems, DNS zone files, configuration rules for firewalls, and other networking devices).**",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "We may have multiple repositories for different types of objects and services, where they are labelled and tagged alongside our\n\nsource code. For instance, we may store large virtual machine images, ISO files, compiled binaries, and so forth in artifact repositories (e.g., Nexus, Artifactory). Alternatively, we may put them in blob stores (e.g., Amazon S3 buckets) or put Docker\n\nimages into Docker registries, and so forth.\n\nIt is not sufficient to merely be able to re-create any previous state of the production environment; we must also be able to re-create\n\nthe entire pre-production and build processes as well. Consequently, we need to put into version control everything relied upon by our build processes, including our tools (e.g.,\n\ncompilers, testing tools) and the environments they depend upon.††\n\nIn Puppet Labs’ 2014 State of DevOps Report, the use of version\n\ncontrol by Ops was the highest predictor of both IT performance and organizational performance. In fact, whether Ops used version control was a higher predictor for both IT performance\n\nand organizational performance than whether Dev used version control.\n\nThe findings from Puppet Labs' 2014 State of DevOps Report\n\nunderscores the critical role version control plays in the software development process. We now know when all application and environment changes are recorded in version control, it enables us to not only quickly see all changes that might have contributed to\n\na problem, but also provides the means to roll back to a previous known, running state, allowing us to more quickly recover from failures.",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "But why does using version control for our environments predict IT and organizational performance better than using version control for our code?\n\nBecause in almost all cases, there are orders of magnitude more configurable settings in our environment than in our code.\n\nConsequently, it is the environment that needs to be in version control the most.‡‡\n\nVersion control also provides a means of communication for\n\neveryone working in the value stream—having Development, QA, Infosec, and Operations able to see each other’s changes helps reduce surprises, creates visibility into each other’s work, and helps build and reinforce trust. See Appendix 7.\n\nMAKE INFRASTRUCTURE EASIER TO REBUILD THAN TO REPAIR\n\nWhen we can quickly rebuild and re-create our applications and environments on demand, we can also quickly rebuild them\n\ninstead of repairing them when things go wrong. Although this is something that almost all large-scale web operations do (i.e., more than one thousand servers), we should also adopt this practice even if we have only one server in production.\n\nBill Baker, a distinguished engineer at Microsoft, quipped that we used to treat servers like pets: “You name them and when they get sick, you nurse them back to health. [Now] servers are [treated]\n\nlike cattle. You number them and when they get sick, you shoot them.”",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "By having repeatable environment creation systems, we are able to easily increase capacity by adding more servers into rotation (i.e., horizontal scaling). We also avoid the disaster that inevitably\n\nresults when we must restore service after a catastrophic failure of irreproducible infrastructure, created through years of undocumented and manual production changes.\n\nTo ensure consistency of our environments, whenever we make production changes (configuration changes, patching, upgrading, etc.), those changes need to be replicated everywhere in our\n\nproduction and pre-production environments, as well as in any newly created environments.\n\nInstead of manually logging into servers and making changes, we\n\nmust make changes in a way that ensures all changes are replicated everywhere automatically and that all our changes are put into version control.\n\nWe can rely on our automated configuration systems to ensure consistency (e.g., Puppet, Chef, Ansible, Salt, Bosh, etc.), or we can create new virtual machines or containers from our automated\n\nbuild mechanism and deploy them into production, destroying the old ones or taking them out of rotation.§§\n\nThe latter pattern is what has become known as immutable\n\ninfrastructure, where manual changes to the production environment are no longer allowed—the only way production changes can be made is to put the changes into version control\n\nand re-create the code and environments from scratch. By doing this, no variance is able to creep into production.",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "To prevent uncontrolled configuration variances, we may disable remote logins to production servers¶¶ or routinely kill and replace production instances, ensuring that manually-applied production\n\nchanges are removed. This action motivates everyone to put their changes in the correct way through version control. By applying such measures, we are systematically reducing the ways our infrastructure can drift from our known, good states (e.g.,\n\nconfiguration drift, fragile artifacts, works of art, snowflakes, and so forth).\n\nAlso, we must keep our pre-production environments up to date— specifically, we need developers to stay running on our most current environment. Developers will often want to keep running on older environments because they fear environment updates\n\nmay break existing functionality. However, we want to update them frequently so we can find problems at the earliest part of the life cycle.***\n\nMODIFY OUR DEFINITION OF DEVELOPMENT “DONE” TO INCLUDE RUNNING IN PRODUCTION-LIKE ENVIRONMENTS\n\nNow that our environments can be created on demand and\n\neverything is checked in to version control, our goal is to ensure\n\nthat these environments are being used in the daily work of Development. We need to verify that our application runs as\n\nexpected in a production-like environment long before the end of\n\nthe project or before our first production deployment.",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Most modern software development methodologies prescribe\n\nshort and iterative development intervals, as opposed to the big\n\nbang approach (e.g., the waterfall `model). In general, the longer the interval between deployment, the worse the outcomes. For\n\nexample, in the Scrum methodology a sprint is a time-boxed\n\ndevelopment interval (typically one month or less) within which we are required to be done, widely defined as when we have\n\n“working and potentially shippable code.”\n\nOur goal is to ensure that Development and QA are routinely integrating the code with production-like environments at increasingly frequent intervals throughout the project.††† We do this by expanding the definition of “done” beyond just correct code functionality (addition in bold text): at the end of each\n\ndevelopment interval, we have integrated, tested, working and potentially shippable code, demonstrated in a production-\n\nlike environment.\n\nIn other words, we will only accept development work as done when it can be successfully built, deployed, and confirmed that it\n\nruns as expected in a production-like environment, instead of\n\nmerely when a developer believes it to be done—ideally, it runs under a production-like load with a production-like dataset, long\n\nbefore the end of a sprint. This prevents situations where a feature\n\nis called done merely because a developer can run it successfully on their laptop but nowhere else.\n\nBy having developers write, test, and run their own code in a\n\nproduction-like environment, the majority of the work to successfully integrate our code and environments happens during\n\nour daily work, instead of at the end of the release. By the end of\n\nour first interval, our application can be demonstrated to run",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "correctly in a production-like environment, with the code and\n\nenvironment having been integrated together many times over,\n\nideally with all the steps automated (no manual tinkering required).\n\nBetter yet, by the end of the project, we will have successfully\n\ndeployed and run our code in production-like environments hundreds or even thousands of times, giving us confidence that\n\nmost of our production deployment problems have been found\n\nand fixed.\n\nIdeally, we use the same tools, such as monitoring, logging, and\n\ndeployment, in our pre-production environments as we do in\n\nproduction. By doing this, we have familiarity and experience that will help us smoothly deploy and run, as well as diagnose and fix,\n\nour service when it is in production.\n\nBy enabling Development and Operations to gain a shared mastery of how the code and environment interact, and practicing\n\ndeployments early and often, we significantly reduce the\n\ndeployment risks that are associated with production code releases. This also allows us to eliminate an entire class of\n\noperational and security defects and architectural problems that are usually caught too late in the project to fix.\n\nCONCLUSION\n\nThe fast flow of work from Development to Operations requires\n\nthat anyone can get production-like environments on demand. By allowing developers to use production-like environments even at\n\nthe earliest stages of a software project, we significantly reduce the",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "risk of production problems later. This is one of many practices\n\nthat demonstrate how Operations can make developers far more\n\nproductive. We enforce the practice of developers running their code in production-like environments by incorporating it into the\n\ndefinition of “done.”\n\nFurthermore, by putting all production artifacts into version control, we have a “single source of truth” that allows us to re-\n\ncreate the entire production environment in a quick, repeatable,\n\nand documented way, using the same development practices for Operations work as we do for Development work. And by making\n\nproduction infrastructure easier to rebuild than to repair, we make resolving problems easier and faster, as well as making it easier to\n\nexpand capacity.\n\nHaving these practices in place sets the stage for enabling comprehensive test automation, which is explored in the next\n\nchapter.\n\n† In this context, environment is defined as everything in the application stack except for the application,\n\nincluding the databases, operating systems, networking, virtualization, and all associated configurations.\n\n‡ Most developers want to test their code, and they have often gone to extreme lengths to obtain test environments to do so. Developers have been known to reuse old test environments from previous projects (often years old) or ask someone who has a reputation of being able to find one—they just won’t ask where it came from, because, invariably, someone somewhere is now missing a server.\n\n§ Ideally, we should be finding errors before integration testing when is too late in the testing cycle to create fast\n\nfeedback for developers. If we are unable to do so, we likely have an architectural issue that needs to be addressed. Designing our systems for testability, to include the ability to discover most defects using a non- integrated virtual environment on a development workstation, is a key part of creating an architecture that supports fast flow and feedback.\n\n¶ The first version control system was likely UPDATE on the CDC6600 (1969). Later came SCCS (1972), CMS on\n\nVMS (1978), RCS (1982), and so forth.\n\n** One may observe that version control fulfills some of the ITIL constructs of the Definitive Media Library (DML)\n\nand Configuration Management Database (CMDB), inventorying everything required to re-create the production environment.\n\n†† In future steps, we will also check in to version control all the supporting infrastructure we build, such as the\n\nautomated test suites and our continuous integration and deployment pipeline infrastructure.\n\n‡‡ Anyone who has done a code migration for an ERP system (e.g., SAP, Oracle Financials, etc.) may recognize the following situation: When a code migration fails, it is rarely due to a coding error. Instead, it’s far more likely",
      "content_length": 2801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "that the migration failed due to some difference in the environments, such as between Development and QA or QA and Production.\n\n§§ At Netflix, the average age of Netflix AWS instance is twenty-four days, with 60% being less than one week old.\n\n¶¶ Or allow it only in emergencies, ensuring that a copy of the console log is automatically emailed to the\n\noperations team.\n\n*** The entire application stack and environment can be bundled into containers, which can enable\n\nunprecedented simplicity and speed across the entire deployment pipeline.\n\n††† The term integration has many slightly different usages in Development and Operations. In Development,\n\nintegration typically refers to code integration, which is the integration of multiple code branches into trunk in version control. In continuous delivery and DevOps, integration testing refers to the testing of the application in a production-like environment or integrated test environment.",
      "content_length": 945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "10Enable Fast and Reliable Automated Testing\n\nAt this point, Development and QA are using production-like\n\nenvironments in their daily work, and we are successfully\n\nintegrating and running our code into a production-like\n\nenvironment for every feature that is accepted, with all changes\n\nchecked in to version control. However, we are likely to get undesired outcomes if we find and fix errors in a separate test\n\nphase, executed by a separate QA department only after all\n\ndevelopment has been completed. And, if testing is only performed a few times a year, developers learn about their\n\nmistakes months after they introduced the change that caused the error. By then, the link between cause and effect has likely faded,\n\nsolving the problem requires firefighting and archaeology, and,\n\nworst of all, our ability to learn from the mistake and integrate it into our future work is significantly diminished.\n\nAutomated testing addresses another significant and unsettling\n\nproblem. Gary Gruver observes that “without automated testing, the more code we write, the more time and money is required to test our code—in most cases, this is a totally unscalable business\n\nmodel for any technology organization.”\n\nAlthough Google now undoubtedly exemplifies a culture that\n\nvalues automated testing at scale, this wasn’t always the case. In",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "2005, when Mike Bland joined the organization, deploying to\n\nGoogle.com was often extremely problematic, especially for the\n\nGoogle Web Server (GWS) team.\n\nAs Bland explains, “The GWS team had gotten into a position in\n\nthe mid 2000s where it was extremely difficult to make changes to\n\nthe web server, a C++ application that handled all requests to\n\nGoogle’s home page and many other Google web pages. As\n\nimportant and prominent as Google.com was, being on the GWS\n\nteam was not a glamorous assignment—it was often the dumping\n\nground for all the different teams who were creating various\n\nsearch functionality, all of whom were developing code independently of each other. They had problems such as builds and tests taking too long, code being put into production without\n\nbeing tested, and teams checking in large, infrequent changes that conflicted with those from other teams.”\n\nThe consequences of this were large—search results could have errors or become unacceptably slow, affecting thousands of search queries on Google.com. The potential result was not only loss of revenue, but customer trust.\n\nBland describes how it affected developers deploying changes,\n\n“Fear became the mind-killer. Fear stopped new team members\n\nfrom changing things because they didn’t understand the system. But fear also stopped experienced people from changing things because they understood it all too well.”† Bland was part of the group that was determined to solve this problem.\n\nGWS team lead Bharat Mediratta believed automated testing would help. As Bland describes, “They created a hard line: no changes would be accepted into GWS without accompanying",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "automated tests. They set up a continuous build and religiously\n\nkept it passing. They set up test coverage monitoring and ensured\n\nthat their level of test coverage went up over time. They wrote up\n\npolicy and testing guides, and insisted that contributors both\n\ninside and outside the team follow them.”\n\nThe results were startling. As Bland notes, “GWS quickly became\n\none of the most productive teams in the company, integrating\n\nlarge numbers of changes from different teams every week while\n\nmaintaining a rapid release schedule. New team members were\n\nable to make productive contributions to this complex system\n\nquickly, thanks to good test coverage and code health. Ultimately, their radical policy enabled the Google.com home page to quickly expand its capabilities and thrive in an amazingly fast-moving and\n\ncompetitive technology landscape.”\n\nBut GWS was still a relatively small team in a large and growing\n\ncompany. The team wanted to expand these practices across the entire organization. Thus, the Testing Grouplet was born, an informal group of engineers who wanted to elevate automated testing practices across the entire organization. Over the next five years, they helped replicate this culture of automated testing across all of Google.‡\n\nNow when any Google developer commits code, it is automatically run against a suite of hundreds of thousands of automated tests. If\n\nthe code passes, it is automatically merged into trunk, ready to be deployed into production. Many Google properties build hourly or daily, then pick which builds to release; others adopt a continuous “Push on Green” delivery philosophy.",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "The stakes are higher than ever—a single code deployment error at Google can take down every property, all at the same time (such as\n\na global infrastructure change or when a defect is introduced into a core library that every property depends upon).\n\nEran Messeri, an engineer in the Google Developer Infrastructure\n\ngroup, notes, “Large failures happen occasionally. You’ll get a ton\n\nof instant messages and engineers knocking on your door. [When the deployment pipeline is broken,] we need to fix it right away,\n\nbecause developers can no longer commit code. Consequently, we want to make it very easy to roll back.”\n\nWhat enables this system to work at Google is engineering\n\nprofessionalism and a high-trust culture that assumes everyone wants to do a good job, as well as the ability to detect and correct\n\nissues quickly. Messeri explains, “There are no hard policies at\n\nGoogle, such as, ‘If you break production for more than ten projects, you have an SLA to fix the issue within ten minutes.’\n\nInstead, there is mutual respect between teams and an implicit agreement that everyone does whatever it takes to keep the\n\ndeployment pipeline running. We all know that one day, I’ll break your project by accident; the next day, you may break mine.”\n\nWhat Mike Bland and the Testing Grouplet team achieved has\n\nmade Google one of the most productive technology organizations\n\nin the world. By 2013, automated testing and continuous integration at Google enabled over four thousand small teams to\n\nwork together and stay productive, all simultaneously developing, integrating, testing, and deploying their code into production. All\n\ntheir code is in a single, shared repository, made up of billions of files, all being continuously built and integrated, with 50% of their",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "code being changed each month. Some other impressive statistics on their performance include:\n\n40,000 code commits/day\n\n50,000 builds/day (on weekdays, this may exceed 90,000)\n\n120,000 automated test suites\n\n75 million test cases run daily\n\n100+ engineers working on the test engineering, continuous\n\nintegration, and release engineering tooling to increase developer productivity (making up 0.5% of the R&D workforce)\n\nIn the remainder of this chapter, we will go through the\n\ncontinuous integration practices required to replicate these outcomes.\n\nCONTINUOUSLY BUILD, TEST, AND INTEGRATE OUR CODE AND ENVIRONMENTS\n\nOur goal is to build quality into our product, even at the earliest\n\nstages, by having developers build automated tests as part of their daily work. This creates a fast feedback loop that helps developers\n\nfind problems early and fix them quickly, when there are the fewest constraints (e.g., time, resources).\n\nIn this step, we create automated test suites that increase the\n\nfrequency of integration and testing of our code and our environments from periodic to continuous. We do this by building",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "our deployment pipeline, which will perform integration of our\n\ncode and environments and trigger a series of tests every time a new change is put into version control.§ (See figure 13.)\n\nThe deployment pipeline, first defined by Jez Humble and David Farley in their book Continuous Delivery: Reliable Software\n\nReleases Through Build, Test, and Deployment Automation, ensures that all code checked in to version control is automatically\n\nbuilt and tested in a production-like environment. By doing this, we find any build, test, or integration errors as soon as a change is\n\nintroduced, enabling us to fix them immediately. Done correctly,\n\nthis allows us to always be assured that we are in a deployable and shippable state.\n\nTo achieve this, we must create automated build and test\n\nprocesses that run in dedicated environments. This is critical for the following reasons:\n\nOur build and test process can run all the time, independent of\n\nthe work habits of individual engineers.\n\nA segregated build and test process ensures that we understand all the dependencies required to build, package, run, and test\n\nour code (i.e., removing the “it worked on the developer’s\n\nlaptop, but it broke in production” problem).\n\nWe can package our application to enable the repeatable\n\ninstallation of code and configurations into an environment\n\n(e.g., on Linux RPM, yum, npm; on Windows, OneGet; alternatively framework-specific packaging systems can be used, such as EAR and WAR files for Java, gems for Ruby,\n\netc.).",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Instead of putting our code in packages, we may choose to package our applications into deployable containers (e.g.,\n\nDocker, Rkt, LXD, AMIs).\n\nEnvironments can be made more production-like in a way that is consistent and repeatable (e.g., compilers are removed from\n\nthe environment, debugging flags are turned off, etc.).\n\nOur deployment pipeline validates after every change that our\n\ncode successfully integrates into a production-like environment. It becomes the platform through which testers request and certify builds during acceptance testing and usability testing, and it will run automated performance and security validations.\n\n>\n\nFigure 13: The deployment pipeline (Source: Humble and Farley, Continuous Delivery, 3.)\n\nFurthermore, it will be used to self-service builds to UAT (user acceptance testing), integration testing, and security testing environments. In future steps, as we evolve the deployment\n\npipeline, it will also be used to manage all activities required to take our changes from version control to deployment.\n\nA variety of tools have been designed to provide deployment\n\npipeline functionality, many of them open source (e.g., Jenkins, ThoughtWorks Go, Concourse, Bamboo, Microsoft Team Foundation Server, TeamCity, Gitlab CI, as well as cloud-based solutions such as Travis CI and Snap).¶",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "We begin the deployment pipeline by running the commit stage, which builds and packages the software, runs automated unity\n\ntests, and performs additional validation such as static code analysis, duplication and test coverage analysis, and checking style.** If successful, this triggers the acceptance stage, which automatically deploys the packages created in the commit stage into a production-like environment and runs the automated acceptance tests.\n\nOnce changes are accepted into version control, we want to package our code only once, so that the same packages are used to deploy code throughout our entire deployment pipeline. By doing this, code will be deployed into our integrated test and staging\n\nenvironments in the same way that it is deployed into production. This reduces variances that can avoid downstream errors that are difficult to diagnose (e.g., using different compilers, compiler flags, library versions, or configurations).††\n\nThe goal of the deployment pipeline is to provide everyone in the value stream, especially developers, the fastest possible feedback\n\nthat a change has taken us out of a deployable state. This could be a change to our code, to any of our environments, to our automated tests, or even to the deployment pipeline infrastructure\n\n(e.g., a Jenkins configuration setting).\n\nAs a result, our deployment pipeline infrastructure becomes as foundational for our development processes as our version control\n\ninfrastructure. Our deployment pipeline also stores the history of each code build, including information about which tests were performed on which build, which builds have been deployed to which environment, and what the test results were. In\n\ncombination with the information in our version control history,",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "we can quickly determine what caused our deployment pipeline to break and, likely, how to fix the error.\n\nThis information also helps us fulfill evidence requirements for audit and compliance purposes, with evidence being automatically generated as part of daily work.\n\nNow that we have a working deployment pipeline infrastructure, we must create our continuous integration practices, which require three capabilities:\n\nA comprehensive and reliable set of automated tests that validate we are in a deployable state.\n\nA culture that “stops the entire production line” when our validation tests fail.\n\nDevelopers working in small batches on trunk rather than\n\nlong-lived feature branches.\n\nIn the next section, we describe why fast and reliable automated testing is needed and how to build it.\n\nBUILD A FAST AND RELIABLE AUTOMATED VALIDATION TEST SUITE\n\nIn the previous step, we started to create the automated testing infrastructure that validates that we have a green build (i.e.,\n\nwhatever is in version control is in a buildable and deployable state). To underscore why we need to perform this integration and testing step continuously, consider what happens when we only",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "perform this operation periodically, such as during a nightly build process.\n\nSuppose we have a team of ten developers, with everyone checking their code into version control daily, and a developer introduces a change that breaks our nightly build and test job. In this scenario,\n\nwhen we discover the next day that we no longer have a green build, it will take minutes, or more likely hours, for our development team to figure out which change caused the problem, who introduced it, and how to fix it.\n\nWorse, suppose the problem wasn’t caused by a code change, but was due to a test environment issue (e.g., an incorrect configuration setting somewhere). The development team may\n\nbelieve that they fixed the problem because all the unit tests pass, only to discover that the tests will still fail later that night.\n\nFurther complicating the issue, ten more changes will have been checked in to version control by the team that day. Each of these changes has the potential to introduce more errors that could break our automated tests, further increasing the difficulty of\n\nsuccessfully diagnosing and fixing the problem.\n\nIn short, slow and periodic feedback kills. Especially for larger development teams. The problem becomes even more daunting\n\nwhen we have tens, hundreds, or even thousands of other developers checking their changes into version control each day. The result is that our builds and automated tests are frequently\n\nbroken, and developers even stop checking their changes into version control (“Why bother, since the builds and tests are always broken?”). Instead they wait to integrate their code at the end of",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "the project, resulting in all the undesired outcomes of large batch size, big bang integrations, and production deployments.‡‡\n\nTo prevent this scenario, we need fast automated tests that run within our build and test environments whenever a new change is introduced into version control. In this way we can find and fix any problems immediately, as the Google Web Server example\n\ndemonstrated. By doing this, we ensure our batches remains small, and, at any given point in time, we remain in a deployable state.\n\nIn general, automated tests fall into one of the following categories, from fastest to slowest:\n\nUnit tests: These typically test a single method, class, or function in isolation, providing assurance to the developer that their code operates as designed. For many reasons, including the need to keep our tests fast and stateless, unit tests often\n\n“stub out” databases and other external dependencies (e.g., functions are modified to return static, predefined values, instead of calling the real database).§§\n\nAcceptance tests: These typically test the application as a whole to provide assurance that a higher level of functionality operates as designed (e.g., the business acceptance criteria for\n\na user story, the correctness of an API), and that regression errors have not been introduced (i.e., we broke functionality that was previously operating correctly). Humble and Farley\n\ndefine the difference between unit and acceptance testing as, “The aim of a unit test is to show that a single part of the application does what the programmer intends it to....The objective of acceptance tests is to prove that our application",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "does what the customer meant it to, not that it works the way its programmers think it should.” After a build passes our unit tests, our deployment pipeline runs it against our acceptance\n\ntests. Any build that passes our acceptance tests is then typically made available for manual testing (e.g., exploratory testing, UI testing, etc.), as well as for integration testing.\n\nIntegration tests: Integration tests are where we ensure that our application correctly interacts with other production applications and services, as opposed to calling stubbed out\n\ninterfaces. As Humble and Farley observe, “much of the work in the SIT environment involves deploying new versions of each of the applications until they all cooperate. In this situation the smoke test is usually a fully fledged set of\n\nacceptance tests that run against the whole application.” Integration tests are performed on builds that have passed our unit and acceptance tests. Because integration tests are often\n\nbrittle, we want to minimize the number of integration tests and find as many of our defects as possible during unit and acceptance testing. The ability to use virtual or simulated versions of remote services when running acceptance tests\n\nbecomes an essential architectural requirement.\n\nWhen facing deadline pressures, developers may stop creating\n\nunit tests as part of their daily work, regardless of how we’ve defined ‘done.’ To detect this, we may choose to measure and\n\nmake visible our test coverage (as a function of number of classes,\n\nlines of code, permutations, etc.), maybe even failing our validation test suite when it drops below a certain level (e.g., when less than 80% of our classes have unit tests).¶¶",
      "content_length": 1702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Martin Fowler observes that, in general, “a ten-minute build [and\n\ntest process] is perfectly within reason…[We first] do the\n\ncompilation and run tests that are more localized unit tests with the database completely stubbed out. Such tests can run very fast,\n\nkeeping within the ten minute guideline. However any bugs that\n\ninvolve larger scale interactions, particularly those involving the real database, won’t be found. The second stage build runs a\n\ndifferent suite of tests [acceptance tests] that do hit the real database and involve more end-to-end behavior. This suite may\n\ntake a couple of hours to run.”\n\nCATCH ERRORS AS EARLY IN OUR AUTOMATED TESTING AS POSSIBLE\n\nA specific design goal of our automated test suite is to find errors as early in the testing as possible. This is why we run faster-\n\nrunning automated tests (e.g., unit tests) before slower-running\n\nautomated tests (e.g., acceptance and integration tests), which are both run before any manual testing.\n\nAnother corollary of this principle is that any errors should be\n\nfound with the fastest category of testing possible. If most of our errors are found in our acceptance and integration tests, the\n\nfeedback we provide to developers is orders of magnitude slower than with unit tests—and integration testing requires using scarce\n\nand complex integration test environments, which can only be\n\nused by one team at a time, further delaying feedback.\n\nFurthermore, not only are errors detected during integration\n\ntesting difficult and time-consuming for developers to reproduce,\n\neven validating that it has been fixed is difficult (i.e., a developer",
      "content_length": 1627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "creates a fix but then needs to wait four hours to learn whether the\n\nintegration tests now pass).\n\nTherefore, whenever we find an error with an acceptance or integration test, we should create a unit test that could find the\n\nerror faster, earlier, and cheaper. Martin Fowler described the\n\nnotion of the “ideal testing pyramid,” where we are able to catch most of our errors using our unit tests. (See figure 14.) In contrast,\n\nin many testing programs the inverse is true, where most of the\n\ninvestment is in manual and integration testing.\n\nFigure 14: The ideal and non-ideal automated testing pyramids (Source: Martin Fowler, “TestPyramid.”)\n\nIf we find that unit or acceptance tests are too difficult and\n\nexpensive to write and maintain, it’s likely that we have an architecture that is too tightly-coupled, where strong separation\n\nbetween our module boundaries no longer exist (or maybe never existed). In this case, we will need to create a more loosely-\n\ncoupled system so modules can be independently tested without",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "integration environments. Acceptance test suites for even the most\n\ncomplex applications that run in minutes are possible.\n\nENSURE TESTS RUN QUICKLY (IN PARALLEL, IF NECESSARY)\n\nBecause we want our tests to run quickly, we need to design our\n\ntests to run in parallel, potentially across many different servers.\n\nWe may also want to run different categories of tests in parallel. For example, when a build passes our acceptance tests, we may\n\nrun our performance testing in parallel with our security testing,\n\nas shown in figure 15. We may or may not allow manual exploratory testing until the build has passed all our automated\n\ntests—which enables faster feedback, but may also allow manual testing on builds that will eventually fail.\n\nWe make any build that passes all our automated tests available to\n\nuse for exploratory testing, as well as for other forms of manual or\n\nresource-intensive testing (such as performance testing). We want to do all such testing as frequently as possible and practical, either\n\ncontinually or on a schedule.",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Figure 15: Running automated and manual tests in parallel (Source: Humble and Farley, Continuous Delivery, Kindle edition, location 3868.)\n\nAny tester (which includes all our developers) should use the\n\nlatest build that has passed all the automated tests, as opposed to\n\nwaiting for developers to flag a specific build as ready to test. By doing this, we ensure that testing happens as early in the process\n\nas possible.\n\nWRITE OUR AUTOMATED TESTS BEFORE WE WRITE THE CODE (“TEST-DRIVEN DEVELOPMENT”)\n\nOne of the most effective ways to ensure we have reliable\n\nautomated testing, is to write those tests as part of our daily work,\n\nusing techniques such as test-driven development (TDD) and acceptance test-driven development (ATDD). This is when we\n\nbegin every change to the system by first writing an automated test that validates the expected behavior fails, and then we write\n\nthe code to make the tests pass.\n\nThis technique was developed by Kent Beck in the late 1990s as part of Extreme Programming, and has the following three steps:",
      "content_length": 1043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "1. Ensure the tests fail. “Write a test for the next bit of\n\nfunctionality you want to add.” Check in.\n\n2. Ensure the tests pass. “Write the functional code until the test\n\npasses.”Check in.\n\n3. “Refactor both new and old code to make it well\n\nstructured.”Ensure the tests pass. Check in again.\n\nThese automated test suites are checked in to version control\n\nalongside our code, which provides a living, up-to-date\n\nspecification of the system. Developers wishing to understand how to use the system can look at this test suite to find working examples of how to use the system’s API.***\n\nAUTOMATE AS MANY OF OUR MANUAL TESTS AS POSSIBLE\n\nOur goal is to find as many code errors through our automated\n\ntest suites, reducing our reliance on manual testing. In her 2013\n\npresentation at Flowcon titled “On the Care and Feeding of Feedback Cycles,” Elisabeth Hendrickson observed, “Although\n\ntesting can be automated, creating quality cannot. To have\n\nhumans executing tests that should be automated is a waste of human potential.”\n\nBy doing this, we enable all our testers (which, of course, includes\n\ndevelopers) work on high-value activities that cannot be automated, such as exploratory testing or improving the test\n\nprocess itself.\n\nHowever, merely automating all our manual tests may create undesired outcomes—we do not want automated tests that are",
      "content_length": 1353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "unreliable or generate false positives (i.e., tests that should have\n\npassed because the code is functionally correct but failed due to problems such as slow performance, causing timeouts,\n\nuncontrolled starting state, or unintended state due to using\n\ndatabase stubs or shared test environments).\n\nUnreliable tests that generate false positives create significant\n\nproblems—they waste valuable time (e.g., forcing developers to re-\n\nrun the test to determine whether there is actually a problem), increase the overall effort of running and interpreting our test\n\nresults, and often result in stressed developers ignoring test\n\nresults entirely or turning off the automated tests in favor of focusing on creating code.\n\nThe result is always the same: we detect the problems later, the\n\nproblems are more difficult to fix, and our customers have worse outcomes, which in turn creates stress across the value stream.\n\nTo mitigate of this, a small number of reliable, automated tests are\n\nalmost always preferable over a large number of manual or unreliable automated tests. Therefore, we focus on automating\n\nonly the tests that genuinely validate the business goals we are\n\ntrying to achieve. If abandoning a test results in production defects, we should add it back to our manual test suite, with the\n\nideal of eventually automating it.\n\nAs Gary Gruver, formerly VP of Quality Engineering, Release Engineering, and Operations for Macys.com, described observes,\n\n“For a large retailer e-commerce site, we went from running 1,300\n\nmanual tests that we ran every ten days to running only ten automated tests upon every code commit—it’s far better to run a\n\nfew tests that we trust than to run tests that aren’t reliable. Over",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "time, we grew this test suite to having hundreds of thousands of\n\nautomated tests.”\n\nIn other words, we start with a small number of reliable\n\nautomated tests and add to them over time, creating an ever-\n\nincreasing level of assurance that we will quickly detect any changes to the system that take us out of a deployable state.\n\nINTEGRATE PERFORMANCE TESTING INTO OUR TEST SUITE\n\nAll too often, we discover that our application performs poorly\n\nduring integration testing or after it has been deployed to production. Performance problems are often difficult to detect,\n\nsuch as when things slow down over time, going unnoticed until it is too late (e.g., database queries without an index). And many\n\nproblems are difficult to solve, especially when they are caused by\n\narchitectural decisions we made or unforeseen limitations of our networking, database, storage, or other systems.\n\nOur goal is to write and run automated performance tests that\n\nvalidate our performance across the entire application stack (code, database, storage, network, virtualization, etc.) as part of the\n\ndeployment pipeline, so we detect problems early, when the fixes\n\nare cheapest and fastest.\n\nBy understanding how our application and environments behave\n\nunder a production-like load, we can do a far better job at capacity\n\nplanning, as well as detecting conditions such as:\n\nWhen our database query times grow non-linearly (e.g., we\n\nforget to turn on database indexing, and page load goes from\n\none hundred minutes to thirty seconds).",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "When a code change causes the number of database calls,\n\nstorage use, or network traffic to increase ten-fold.\n\nWhen we have acceptance tests that are able to be run in parallel,\n\nwe can use them as the basis of our performance tests. For\n\ninstance, suppose we run an e-commerce site and have identified “search” and “checkout” as two high-value operations that must\n\nperform well under load. To test this, we may run thousands of\n\nparallel search acceptance tests simultaneously with thousands of parallel checkout tests.\n\nDue to the large amount of compute and I/O that is required to\n\nrun performance tests, creating a performance testing environment can easily be more complex than creating the\n\nproduction environment for the application itself. Because of this,\n\nwe may build our performance testing environment at the start of any project and ensure that we dedicate whatever resources are\n\nrequired to build it early and correctly.\n\nTo find performance problems early, we should log performance results and evaluate each performance run against previous\n\nresults. For instance, we might fail the performance tests if\n\nperformance deviates more than 2% from the previous run.\n\nINTEGRATE NON-FUNCTIONAL REQUIREMENTS TESTING INTO OUR TEST SUITE\n\nIn addition to testing that our code functions as designed and it\n\nperforms under production-like loads, we also want to validate every other attribute of the system we care about. These are often\n\ncalled non-functional requirements, which include availability,\n\nscalability, capacity, security, and so forth.",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Many of these requirements are fulfilled by the correct\n\nconfiguration of our environments, so we must also build automated tests to validate that our environments have been built\n\nand configured properly. For example, we want to enforce the\n\nconsistency and correctness of the following, which many non- functional requirements rely upon (e.g., security, performance,\n\navailability):\n\nSupporting applications, databases, libraries, etc.\n\nLanguage interpreters, compilers, etc.\n\nOperating systems (e.g., audit logging enabled, etc.)\n\nAll dependencies\n\nWhen we use infrastructure as code configuration management tools (e.g., Puppet, Chef, Ansible, Salt, Bosh), we can use the same\n\ntesting frameworks that we use to test our code to also test that\n\nour environments are configured and operating correctly (e.g., encoding environment tests into cucumber or gherkin tests).\n\nFurthermore, similar to how we run analysis tools on our\n\napplication in our deployment pipeline (e.g., static code analysis, test coverage analysis), we should run tools that analyze the code\n\nthat constructs our environments (e.g., Foodcritic for Chef, puppet-lint for Puppet). We should also run any security\n\nhardening checks as part of our automated tests to ensure that\n\neverything is configured securely and correctly (e.g., server-spec).\n\nAt any point in time, our automated tests can validate that we have a green build and that we are in a deployable state. Now, we\n\nmust create an Andon cord so that when someone breaks the",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "deployment pipeline, we take all necessary steps to get back into a\n\ngreen build state.\n\nPULL OUR ANDON CORD WHEN THE DEPLOYMENT PIPELINE BREAKS\n\nWhen we have a green build in our deployment pipeline, we have a\n\nhigh degree of confidence that our code and environment will\n\noperate as designed when we deploy our changes into production.\n\nIn order to keep our deployment pipeline in a green state, we will\n\ncreate a virtual Andon Cord, similar to the physical one in the\n\nToyota Production System. Whenever someone introduces a change that causes our build or automated tests to fail, no new\n\nwork is allowed to enter the system until the problem is fixed. And\n\nif someone needs help to resolve the problem, they can bring in whatever help they need, as in the Google example at the\n\nbeginning of this chapter.\n\nWhen our deployment pipeline is broken, at a minimum, we notify the entire team of the failure, so anyone can either fix the problem\n\nor roll-back the commit. We may even configure the version\n\ncontrol system to prevent further code commits until the first stage (i.e., builds and unit tests) of the deployment pipeline is back\n\nin a green state. If the problem was due to an automated test\n\ngenerating a false positive error, the offending test should either be rewritten or removed.††† Every member of the team should be empowered to roll back the commit to get back into a green state.\n\nRandy Shoup, former engineering director for Google App Engine, wrote about the importance of bringing the deployment back into",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "a green state. “We prioritize the team goals over individual goals—\n\nwhenever we help someone move their work forward, we help the entire team. This applies whether we’re helping someone fix the\n\nbuild or an automated test, or even performing a code review for\n\nthem. And of course, we know that they’ll do the same for us, when we need help. This system worked without a lot of formality\n\nor policy—everyone knew that our job was not just ‘write code,’ but it was to ‘run a service.’ This is why we prioritized all quality\n\nissues, especially those related to reliability and scaling, at the\n\nhighest level, treating them as a Priority 0 ‘show-stopper’ problems. From a systems perspective, these practices keep us\n\nfrom slipping backwards.”\n\nWhen later stages of the deployment pipeline fail, such as acceptance tests or performance tests, instead of stopping all new\n\nwork, we will have developers and testers on-call who are\n\nresponsible for fixing these problems immediately. They should also create new tests that run at an earlier stage in the deployment\n\npipeline to catch any future regressions. For example, if we\n\ndiscover a defect in our acceptance tests, we should write a unit test to catch the problem. Similarly, if we discover a defect in\n\nexploratory testing, we should write a unit or acceptance test.\n\nTo increase the visibility of automated test failures, we should create highly visible indicators so that the entire team can see\n\nwhen our build or automated tests are failing. Many teams have\n\ncreated highly visible build lights that get mounted on a wall, indicating the current build status, or other fun ways of telling the\n\nteam the build is broken, including lava lamps, playing a voice\n\nsample or song, klaxons, traffic lights, and so forth.",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "In many ways, this step is more challenging than creating our\n\nbuilds and test servers—those were purely technical activities, whereas this step requires changing human behavior and\n\nincentives. However, continuous integration and continuous delivery require these changes, as we explore in the next section.\n\nWHY WE NEED TO PULL THE ANDON CORD\n\nThe consequence of not pulling the Andon cord and immediately fixing any deployment pipeline issues results in the all too familiar\n\nproblem where it becomes ever more difficult to bring our applications and environment back into a deployable state.\n\nConsider the following situation:\n\nSomeone checks in code that breaks the build or our\n\nautomated tests, but no one fixes it.\n\nSomeone else checks in another change onto the broken build, which also doesn’t pass our automated tests—but no one sees\n\nthe failing test results which would have enabled us to see the\n\nnew defect, let alone fix it.\n\nOur existing tests don’t run reliably, so we are very unlikely to\n\nbuild new tests. (Why bother? We can’t even get the current\n\ntests to run.)\n\nWhen this happens, our deployments to any environment become\n\nas unreliable as when we had no automated tests or were using a\n\nwaterfall method, where the majority of our problems are being\n\ndiscovered in production. The inevitable outcome of this vicious\n\ncycle is that we end up where we started, with an unpredictable\n\n“stabilization phase” that takes weeks or months where our whole team is plunged into crisis, trying to get all our tests to pass,",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "taking shortcuts because of deadline pressures, and adding to our technical debt.‡‡‡\n\nCONCLUSION\n\nIn this chapter, we have created a comprehensive set of automated\n\ntests to confirm that we have a green build that is still in a passing\n\nand deployable state. We have organized our test suites and\n\ntesting activities into a deployment pipeline. We have also created the cultural norm of doing whatever it takes to get back into a\n\ngreen build state if someone introduces a change that breaks any\n\nof our automated tests.\n\nBy doing this, we set the stage for implementing continuous\n\nintegration, which allows many small teams to independently and\n\nsafely develop, test, and deploy code into production, delivering\n\nvalue to customers.\n\n† Bland described that at Google, one of the consequences of having so many talented developers was that it\n\ncreated “imposter syndrome,” a term coined by psychologists to informally describe people who are unable to internalize their accomplishments. Wikipedia states that “despite external evidence of their competence, those exhibiting the syndrome remain convinced that they are frauds and do not deserve the success they have achieved. Proof of success is dismissed as luck, timing, or as a result of deceiving others into thinking they are more intelligent and competent than they believe themselves to be.”\n\n‡ They created training programs, pushed the famous Testing on the Toilet newsletter (which they posted in the\n\nbathrooms), the Test Certified roadmap and certification program, and led multiple “fix-it” days (i.e., improvement blitzes), which helped teams improve their automated testing processes so they could replicate the amazing outcomes that the GWS team was able to achieve.\n\n§ In Development, continuous integration often refers to the continuous integration of multiple code branches into trunk and ensuring that it passes unit tests. However, in the context of continuous delivery and DevOps, continuous integration also mandates running on production-like environments and passing acceptance and integration tests. Jez Humble and David Farley disambiguate these by calling the latter CI+. In this book, continuous integration will always refer to CI+ practices.\n\n¶ If we create containers in our deployment pipeline and have an architecture such as microservices, we can enable each developer to build immutable artifacts where developers assemble and run all the service components in an environment identical to production on their workstation. This enables developers to build and run more tests on their workstation instead of on testing servers, giving us even faster feedback on their work.\n\n** We may even require that these tools are run before changes are accepted into version control (e.g., get pre-\n\ncommit hooks). We may also run these tools within the developer integrated development environment (IDE;",
      "content_length": 2879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "where the developer edits, compiles, and runs code), which creates an even faster feedback loop.\n\n†† We can also use containers, such as Docker, as the packaging mechanism. Containers enable the capability to\n\nwrite once, run anywhere. These containers are created as part of our build process and can be quickly deployed and run in any environment. Because the same container is run in every environment, we help enforce the consistency of all our build artifacts.\n\n‡‡ It is exactly this problem that led to the development of continuous integration practices.\n\n§§ There is a broad category of architectural and testing techniques used to handle the problems of tests requiring input from external integration points, including “stubs,” “mocks,” “service virtualization,” and so forth. This becomes even more important for acceptance and integration testing, which place far more reliance on external states.\n\n¶¶ We should do this only when our teams already value automated testing—this type of metric is easily gamed by\n\ndevelopers and managers.\n\n*** Nachi Nagappan, E. Michael Maximilien, and Laurie Williams (from Microsoft Research, IBM Almaden Labs, and North Carolina State University, respectively) conducted a study that showed teams using TDD produced code 60%–90% better in terms of defect density than non-TDD teams, while taking only 15%–35% longer.\n\n††† If the process for rolling back the code is not well-known, a potential countermeasure is to schedule a pair\n\nprogrammed rollback, so that it can be better documented.\n\n‡‡‡ This is sometimes called the water-Scrum-fall anti-pattern, which refers to when an organization claims to be using Agile-like practices, but, in reality, all testing and defect fixing are performed at the end of the project.",
      "content_length": 1767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "11Enable and Practice Continuous Integration\n\nIn the previous chapter, we created the automated testing\n\npractices to ensure that developers get fast feedback on the quality\n\nof their work. This becomes even more important as we increase\n\nthe number of developers and the number of branches they work\n\non in version control.\n\nThe ability to “branch” in version control systems was created\n\nprimarily to enable developers to work on different parts of the\n\nsoftware system in parallel, without the risk of individual developers checking in changes that could destabilize or introduce errors into trunk (sometimes also called master or mainline).†\n\nHowever, the longer developers are allowed to work in their branches in isolation, the more difficult it becomes to integrate\n\nand merge everyone’s changes back into trunk. In fact, integrating\n\nthose changes becomes exponentially more difficult as we increase the number of branches and the number of changes in each code branch.\n\nIntegration problems result in a significant amount of rework to get back into a deployable state, including conflicting changes that\n\nmust be manually merged or merges that break our automated or manual tests, usually requiring multiple developers to successfully",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "resolve. And because integration has traditionally been done at the\n\nend of the project, when it takes far longer then planned, we are\n\noften forced to cut corners to make the release date.\n\nThis causes another downward spiral: when merging code is\n\npainful, we tend to do it less often, making future merges even\n\nworse. Continuous integration was designed to solve this problem\n\nby making merging into trunk a part of everyone’s daily work.\n\nThe surprising breadth of problems that continuous integration\n\nsolves, as well as the solutions themselves, are exemplified in Gary\n\nGruver’s experience as the director of engineering for HP’s\n\nLaserJet Firmware division, which builds the firmware that runs\n\nall their scanners, printers, and multifunction devices.\n\nThe team consisted of four hundred developers distributed across the US, Brazil, and India. Despite the size of their team, they were\n\nmoving far too slowly. For years, they were unable to deliver new features as quickly as the business needed.\n\nGruver described the problem thusly: “Marketing would come to us with a million ideas to dazzle our customer, and we’d just tell them, ‘Out of your list, pick the two things you’d like to get in the next six to twelve months.’”\n\nThey were only completing two firmware releases per year, with the majority of their time spent porting code to support new products. Gruver estimated that only 5% of their time was spent creating new features—the rest of the time was spent on non-\n\nproductive work associated with their technical debt, such as managing multiple code branches and manual testing, as shown below:",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "20% on detailed planning (Their poor throughput and high\n\nlead times were misattributed to faulty estimation, and so,\n\nhoping to get a better answer, they were asked to estimate the\n\nwork in greater detail.)\n\n25% spent porting code, all maintained on separate code\n\nbranches\n\n10% spent integrating their code between developer branches\n\n15% spent completing manual testing\n\nGruver and his team created a goal of increasing the time spent on innovation and new functionality by a factor of ten. The team hoped this goal could be achieved through:\n\nContinuous integration and trunk-based development\n\nSignificant investment in test automation\n\nCreation of a hardware simulator so tests could be run on a virtual platform\n\nThe reproduction of test failures on developer workstations\n\nA new architecture to support running all printers off a common build and release\n\nBefore this, each product line would require a new code branch, with each model having a unique firmware build with capabilities defined at compile time.‡ The new architecture would have all developers working in a common code base, with a single firmware release supporting all LaserJet models built off of trunk,",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "with printer capabilities being established at runtime in an XML configuration file.\n\nFour years later, they had one codebase supporting all twenty-four\n\nHP LaserJet product lines being developed on trunk. Gruver admits trunk-based development requires a big mindset shift.\n\nEngineers thought trunk-based development would never work,\n\nbut once they started, they couldn’t imagine ever going back. Over the years we’ve had several engineers leave HP, and they would\n\ncall me to tell me about how backward development was in their new companies, pointing out how difficult it is to be effective and\n\nrelease good code when there is no feedback that continuous integration gives them.\n\nHowever, trunk-based development required them to build more\n\neffective automated testing. Gruver observed, “Without automated\n\ntesting, continuous integration is the fastest way to get a big pile of junk that never compiles or runs correctly.” In the beginning, a\n\nfull manual testing cycle required six weeks.\n\nIn order to have all firmware builds automatically tested, they invested heavily in their printer simulators and created a testing\n\nfarm in six weeks—within a few years two thousand printer simulators ran on six racks of servers that would load the\n\nfirmware builds from their deployment pipeline. Their continuous\n\nintegration (CI) system ran their entire set of automated unit, acceptance, and integration tests on builds from trunk, just as\n\ndescribed in the previous chapter. Furthermore, they created a culture that halted all work anytime a developer broke the\n\ndeployment pipeline, ensuring that developers quickly brought the system back into a green state.",
      "content_length": 1662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Automated testing created fast feedback that enabled developers to quickly confirm that their committed code actually worked.\n\nUnit tests would run on their workstations in minutes, three levels of automated testing would run on every commit as well as every\n\ntwo and four hours. The final full regression testing would run every twenty-four hours. During this process, they:\n\nReduced the build to one build per day, eventually doing ten to\n\nfifteen builds per day\n\nWent from around twenty commits per day performed by a\n\n“build boss” to over one hundred commits per day performed by individual developers\n\nEnabled developers to change or add 75k–100k lines of code\n\neach day\n\nReduced regression test times from six weeks to one day\n\nThis level of productivity could never have been supported prior to adopting continuous integration, when merely creating a green\n\nbuild required days of heroics. The resulting business benefits were astonishing:\n\nTime spent on driving innovation and writing new features\n\nincreased from 5% of developer time to 40%.\n\nOverall development costs were reduced by approximately 40%.\n\nPrograms under development were increased by about 140%.\n\nDevelopment costs per program were decreased by 78%.",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "What Gruver’s experience shows is that, after comprehensive use\n\nof version control, continuous integration is one of the most\n\ncritical practices that enable the fast flow of work in our value stream, enabling many development teams to independently\n\ndevelop, test, and deliver value. Nevertheless, continuous integration remains a controversial practice. The remainder of this\n\nchapter describes the practices required to implement continuous integration, as well as how to overcome common objections.\n\nSMALL BATCH DEVELOPMENT AND WHAT HAPPENS WHEN WE COMMIT CODE TO TRUNK INFREQUENTLY\n\nAs described in the previous chapters, whenever changes are\n\nintroduced into version control that cause our deployment pipeline to fail, we quickly swarm the problem to fix it, bringing\n\nour deployment pipeline back into a green state. However,\n\nsignificant problems result when developers work in long-lived private branches (also known as “feature branches”), only merging\n\nback into trunk sporadically, resulting in a large batch size of changes. As described in the HP LaserJet example, what results is\n\nsignificant chaos and rework in order to get their code into a\n\nreleasable state.\n\nJeff Atwood, founder of the Stack Overflow site and author of the\n\nCoding Horror blog, observes that while there are many\n\nbranching strategies, they can all be put on the following spectrum:",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Optimize for individual productivity: Every single person on the project works in their own private branch. Everyone\n\nworks independently, and nobody can disrupt anyone else’s work; however, merging becomes a nightmare. Collaboration becomes almost comically difficult—every person’s work has to\n\nbe painstakingly merged with everyone else’s work to see even the smallest part of the complete system.\n\nOptimize for team productivity: Everyone works in the\n\nsame common area. There are no branches, just a long, unbroken straight line of development. There’s nothing to understand, so commits are simple, but each commit can break\n\nthe entire project and bring all progress to a screeching halt.\n\nAtwood’s observation is absolutely correct—stated more precisely, the required effort to successfully merge branches back together\n\nincreases exponentially as the number of branches increase. The problem lies not only in the rework this “merge hell” creates, but also in the delayed feedback we receive from our deployment pipeline. For instance, instead of performance testing against a\n\nfully integrated system happening continuously, it will likely happen only at the end of our process.\n\nFurthermore, as we increase the rate of code production as we add\n\nmore developers, we increase the probability that any given change will impact someone else and increase the number of developers who will be impacted when someone breaks the\n\ndeployment pipeline.\n\nHere is one last troubling side effect of large batch size merges: when merging is difficult, we become less able and motivated to\n\nimprove and refactor our code, because refactorings are more",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "likely to cause rework for everyone else. When this happens, we are more reluctant to modify code that has dependencies\n\nthroughout the codebase, which is (tragically) where we may have the highest payoffs.\n\nThis is how Ward Cunningham, developer of the first wiki, first\n\ndescribed technical debt: when we do not aggressively refactor our codebase, it becomes more difficult to make changes and to maintain over time, slowing down the rate at which we can add\n\nnew features. Solving this problem was one of the primary reasons behind the creation of continuous integration and trunk-based development practices, to optimize for team productivity over individual productivity.\n\nADOPT TRUNK-BASED DEVELOPMENT PRACTICES\n\nOur countermeasure to large batch size merges is to institute continuous integration and trunk-based development practices,\n\nwhere all developers check in their code to trunk at least once per day. Checking code in this frequently reduces our batch size to the work performed by our entire developer team in a single day. The more frequently developers check in their code to trunk, the\n\nsmaller the batch size and the closer we are to the theoretical ideal of single-piece flow.\n\nFrequent code commits to trunk means we can run all automated\n\ntests on our software system as a whole and receive alerts when a change breaks some other part of the application or interferes with the work of another developer. And because we can detect merge\n\nproblems when they are small, we can correct them faster.",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "We may even configure our deployment pipeline to reject any commits (e.g., code or environment changes) that take us out of a\n\ndeployable state. This method is called gated commits, where the deployment pipeline first confirms that the submitted change will successfully merge, build as expected, and pass all the automated tests before actually being merged into trunk. If not, the developer\n\nwill be notified, allowing corrections to be made without impacting anyone else in the value stream.\n\nThe discipline of daily code commits also forces us to break our\n\nwork down into smaller chunks while still keeping trunk in a working, releasable state. And version control becomes an integral mechanism of how the team communicates with each other—\n\neveryone has a better shared understanding of the system, is aware of the state of the deployment pipeline, and can help each other when it breaks. As a result, we achieve higher quality and\n\nfaster deployment lead times.\n\nHaving these practices in place, we can now again modify our definition of “done” (addition in bold text): “At the end of each\n\ndevelopment interval, we must have integrated, tested, working, and potentially shippable code, demonstrated in a production-like environment, created from trunk using a one-click process, and validated with automated tests.”\n\nAdhering to this revised definition of done helps us further ensure the ongoing testability and deployability of the code we’re producing. By keeping our code in a deployable state, we are able\n\nto eliminate the common practice of having a separate test and stabilization phase at the end of the project.",
      "content_length": 1629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Case Study Continuous Integration at Bazaarvoice (2012)\n\nErnest Mueller, who helped engineer the DevOps\n\ntransformation at National Instruments, later helped transform the development and release processes at Bazaarvoice in 2012. Bazaarvoice supplies customer generated content (e.g., reviews, ratings) for thousands of\n\nretailers, such as Best Buy, Nike, and Walmart.\n\nAt that time, Bazaarvoice had $120 million in revenue and was preparing for an IPO.§ The business was primarily driven by the Bazaarvoice Conversations application, a monolithic Java application comprised of nearly five million lines of code dating back to 2006, spanning fifteen thousand\n\nfiles. The service ran on 1,200 servers across four data centers and multiple cloud service providers.\n\nPartially as a result of switching to an Agile development\n\nprocess and to two-week development intervals, there was a tremendous desire to increase release frequency from their current ten-week production release schedule. They had\n\nalso started to decouple parts of their monolithic application, breaking it down into microservices.\n\nTheir first attempt at a two-week release schedule was in\n\nJanuary of 2012. Mueller observed, “It didn’t go well. It caused massive chaos, with forty-four production incidents filed by our customers. The major reaction from management was basically ‘Let’s not ever do that again.’”",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Mueller took over the release processes shortly afterward, with the goal of doing bi-weekly releases without causing customer downtime. The business objectives for releasing\n\nmore frequently included enabling faster A/B testing (described in upcoming chapters) and increasing the flow of features into production. Mueller identified three core\n\nproblems:\n\nLack of test automation made any level of testing during the two-week intervals inadequate to prevent large-scale\n\nfailures.\n\nThe version control branching strategy allowed developers to check in new code right up to the production release.\n\nThe teams running microservices were also performing independent releases, which were often causing issues during the monolith release or vice versa.\n\nMueller concluded that the monolithic Conversations application deployment process needed to be stabilized,\n\nwhich required continuous integration. In the six weeks that followed, developers stopped doing feature work to focus instead on writing automated testing suites, including unit tests in JUnit, regression tests in Selenium, and getting a\n\ndeployment pipeline running in TeamCity. “By running these tests all the time, we felt like we could make changes with some level of safety. And most importantly, we could\n\nimmediately find when someone broke something, as opposed to discovering it only after it’s in production.”",
      "content_length": 1377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "They also changed to a trunk/branch release model, where every two weeks they created a new dedicated release branch, with no new commits allowed to that branch unless\n\nthere was an emergency—all changes would be worked through a sign-off process, either per-ticket or per-team through their internal wiki. That branch would go through a QA process, which would then be promoted into production.\n\nThe improvements to predictability and quality of the releases were startling:\n\nJanuary 2012 release: forty-four customer incidents (continuous integration effort begins)\n\nMarch 6, 2012 release: five days late, five customer incidents\n\nMarch 22, 2012 release: on time, one customer incident\n\nApril 5, 2012 release: on time, zero customer incidents\n\nMueller further described how successful this effort was:\n\nWe had such success with releases every two weeks, we went to weekly releases, which required almost no\n\nchanges from the engineering teams. Because releases became so routine, it was as simple as\n\ndoubling the number of releases on the calendar and\n\nreleasing when the calendar told us to. Seriously, it was almost a non-event. The majority of changes\n\nrequired were in our customer service and marketing\n\nteams, who had to change their processes, such as changing the schedule of their weekly customer emails",
      "content_length": 1315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "to make sure customers knew that feature changes\n\nwere coming. After that, we started working toward our\n\nnext goals, which eventually led to speeding up our testing times from three plus hours to less than an\n\nhour, reducing the number of environments from four\n\nto three (Dev, Test, Production, eliminating Staging), and moving to a full continuous delivery model where\n\nwe enable fast, one-click deployments.\n\nCONCLUSION\n\nTrunk-based development is likely the most controversial practice\n\ndiscussed in this book. Many engineers will not believe that it’s\n\npossible, even those that prefer working uninterrupted on a private branch without having to deal with other developers.\n\nHowever, the data from Puppet Labs’ 2015 State of DevOps\n\nReport is clear: trunk-based development predicts higher throughput and better stability, and even higher job satisfaction\n\nand lower rates of burnout.\n\nWhile convincing developers may be difficult at first, once they see the extraordinary benefits, they will likely become lifetime\n\nconverts, as the HP LaserJet and Bazaarvoice examples illustrate.\n\nContinuous integration practices set the stage for the next step, which is automating the deployment process and enabling low-\n\nrisk releases.\n\n† Branching in version control has been used in many ways, but is typically used to divide work between team\n\nmembers by release, promotion, task, component, technology platforms, and so forth.\n\n‡ Compile flags (#define and #ifdef) were used to enable/disable code execution for presence of copiers, paper size\n\nsupported, and so on.\n\n§ The production release was delayed due to their (successful) IPO.",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "12Automate and Enable Low-Risk Releases\n\nChuck Rossi is the director of release engineering at Facebook.\n\nOne of his responsibilities is overseeing the daily code push. In\n\n2012, Rossi described their process as follows: “Starting around 1\n\np.m., I switch over to ‘operations mode’ and work with my team to\n\nget ready to launch the changes that are going out to Facebook.com that day. This is the more stressful part of the job\n\nand really relies heavily on my team’s judgment and past\n\nexperience. We work to make sure that everyone who has changes going out is accounted for and is actively testing and supporting\n\ntheir changes.”\n\nJust prior to the production push, all developers with changes\n\ngoing out must be present and check in on their IRC chat channel\n\n—any developers not present have their changes automatically removed from the deployment package. Rossi continued, “If everything looks good and our test dashboards and canary tests† are green, we push the big red button and the entire Facebook.com server fleet gets the new code delivered. Within twenty minutes, thousands and thousands of machines are up on new code with no visible impact to the people using the site.”‡\n\nLater that year, Rossi doubled their software release frequency to\n\ntwice daily. He explained that the second code push gave",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "engineers not on the US West Coast the ability to “move and ship\n\nas quickly as any other engineer in the company,” and also gave\n\neveryone a second opportunity each day to ship code and launch\n\nfeatures.\n\nFigure 16: Number of developers deploying per week at Facebook (Source: Chuck Rossi, “Ship early and ship twice as often.”)\n\nKent Beck, the creator of the Extreme Programming methodology, one of the leading proponents of Test Driven Development, and technical coach at Facebook, further comments on the their code release strategy in an article posted on his Facebook page: “Chuck\n\nRossi made the observation that there seem to be a fixed number of changes Facebook can handle in one deployment. If we want\n\nmore changes, we need more deployments. This has led to a steady increase in deployment pace over the past five years, from weekly to daily to thrice daily deployments of our PHP code and from six to four to two week cycles for deploying our mobile apps. This improvement has been driven primarily by the release engineering team.”",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "By using continuous integration and making code deployment a\n\nlow-risk process, Facebook has enabled code deployment to be a\n\npart of everyone’s daily work and sustain developer productivity.\n\nThis requires that code deployment be automated, repeatable, and\n\npredictable. In the practices described in the book so far, even\n\nthough our code and environments have been tested together,\n\nmost likely we are not deploying to production very often because\n\ndeployments are manual, time-consuming, painful, tedious, and\n\nerror-prone, and they often involve an inconvenient and\n\nunreliable handoff between Development and Operations.\n\nAnd because it is painful, we tend to do it less and less frequently, resulting in another self-reinforcing downward spiral. By deferring production deployments, we accumulate ever-larger\n\ndifferences between the code to be deployed and what’s running in production, increasing the deployment batch size. As deployment batch size grows, so does the risk of unexpected outcomes associated with the change, as well as the difficulty fixing them.\n\nIn this chapter, we reduce the friction associated with production deployments, ensuring that they can be performed frequently and easily, either by Operations or Development. We do this by extending our deployment pipeline.\n\nInstead, of merely continually integrating our code in a production-like environment, we will enable the promotion into\n\nproduction of any build that passes our automated test and validation process, either on demand (i.e., at the push of a button) or automatically (i.e., any build that passes all the tests is automatically deployed).",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Because of the number of practices presented, extensive footnotes are provided with numerous examples and additional information,\n\nwithout interrupting the presentation of concepts in the chapter.\n\nAUTOMATE OUR DEPLOYMENT PROCESS\n\nAchieving outcomes like those at Facebook requires that we have\n\nan automated mechanism that deploys our code into production. Especially if we have a deployment process that has existed for\n\nyears, we need to fully document the steps in the deployment process, such as in a value stream mapping exercise, which we can\n\nassemble in a workshop or document incrementally (e.g., in a\n\nwiki).\n\nOnce we have the process documented, our goal is to simplify and\n\nautomate as many of the manual steps as possible, such as:\n\nPackaging code in ways suitable for deployment\n\nCreating pre-configured virtual machine images or containers\n\nAutomating the deployment and configuration of middleware\n\nCopying packages or files onto production servers\n\nRestarting servers, applications, or services\n\nGenerating configuration files from templates\n\nRunning automated smoke tests to make sure the system is working and correctly configured\n\nRunning testing procedures",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Scripting and automating database migrations\n\nWhere possible, we will re-architect to remove steps, particularly those that take a long time to complete. We also want to not only\n\nreduce our lead times but also the number of handoffs as much as possible in order to reduce errors and loss of knowledge.\n\nHaving developers focus on automating and optimizing the\n\ndeployment process can lead to significant improvements in deployment flow, such as ensuring that small application\n\nconfiguration changes no longer need new deployments or new\n\nenvironments.\n\nHowever, this requires that Development works closely with Operations to ensure that all the tools and processes we co-create\n\ncan be used downstream, as opposed to alienating Operations or reinventing the wheel.\n\nMany tools that provide continuous integration and testing also\n\nsupport the ability to extend the deployment pipeline so that validated builds can be promoted into production, typically after\n\nthe production acceptance tests are performed (e.g., the Jenkins\n\nBuild Pipeline plugin, ThoughtWorks Go.cd and Snap CI, Microsoft Visual Studio Team Services, and Pivotal Concourse).\n\nThe requirements for our deployment pipeline include:\n\nDeploying the same way to every environment: By\n\nusing the same deployment mechanism for every environment (e.g., development, test, and production), our production\n\ndeployments are likely to be far more successful, since we know that it has been successfully performed many times already\n\nearlier in the pipeline.",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Smoke testing our deployments: During the deployment\n\nprocess, we should test that we can connect to any supporting\n\nsystems (e.g., databases, message buses, external services) and run a single test transaction through the system to ensure that\n\nour system is performing as designed. If any of these tests fail, we should fail the deployment.\n\nEnsure we maintain consistent environments: In\n\nprevious steps, we created a single-step environment build process so that the development, test, and production\n\nenvironments had a common build mechanism. We must\n\ncontinually ensure that these environments remain synchronized.\n\nOf course, when any problems occur during deployment, we pull\n\nthe Andon cord and swarm the problem until the problem is resolved, just as we do when our deployment pipeline fails in any\n\nof the earlier steps.\n\nCase Study Daily Deployments at CSG International (2013)\n\nCSG International runs one of the largest bill printing\n\noperations in the US. Scott Prugh, their chief architect and VP of Development, in an effort to improve the predictability\n\nand reliability of their software releases, doubled their\n\nrelease frequency from two per year to four per year (halving their deployment interval from twenty-eight weeks to fourteen weeks).",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Although the Development teams were using continuous integration to deploy their code into test environments daily,\n\nthe production releases were being performed by the Operations team. Prugh observed, “It was as if we had a ‘practice team’ that practiced daily (or even more frequently)\n\nin low-risk test environments, perfecting their processes and tools. But our production ‘game team’ got very few attempts to practice, only twice per year. Worse, they were practicing in the high-risk production environments, which were often\n\nvery different than the pre-production environments with different constraints—the development environments were missing many production assets such as security, firewalls,\n\nload balancers, and a SAN.”\n\nTo solve this problem, they created a Shared Operations Team (SOT) that was responsible for managing all the\n\nenvironments (development, test, production) performing daily deployments into those development and test environments, as well as doing production deployments and\n\nreleases every fourteen weeks. Because the SOT was doing deployments every day, any problems they encountered that were left unfixed would simply occur again the next day. This created tremendous motivation to automate tedious or error-\n\nprone manual steps and to fix any issues that could potentially happen again. Because the deployments were performed nearly one hundred times before the production\n\nrelease, most problems were found and fixed long before then.\n\nDoing this revealed problems that were previously only\n\nexperienced by the Ops team, which were then problems for",
      "content_length": 1590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "the entire value stream to solve. The daily deployments enabled daily feedback on which practices worked and\n\nwhich didn’t.\n\nThey also focused on making all their environments look as similar as possible, including the restricted security access\n\nrights and load balancers. Prugh writes, “We made non- production environments as similar to production as possible, and we sought to emulate production constraints in\n\nas many ways as possible. Early exposure to production- class environments altered the designs of the architecture to make them friendlier in these constrained or different environments. Everyone gets smarter from this approach.”\n\nPrugh also observes:\n\n“We have experienced many cases where changes to\n\ndatabase schemas are either 1) handed off to a DBA team for them to ‘go and figure it out’ or 2) automated tests that run on unrealistically small data sets (i.e., “100’s of MB vs.\n\n100’s of GBs”), which led to production failures. In our old way of working, this would become a late-night blame game between teams trying to unwind the mess. We created a development and deployment process that removed the\n\nneed for handoffs to DBAs by cross-training developers, automating schema changes, and executing them daily. We created realistic load testing against sanitized customer\n\ndata, ideally running migrations every day. By doing this, we run our service hundreds of times with realistic scenarios before seeing actual production traffic.”§",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Their results were astonishing. By doing daily deployments and doubling the frequency of production releases, the\n\nnumber of production incidents went down by 91%, MTTR went down by 80%, and the deployment lead time required for the service to run in production in a “fully hands-off state” went from fourteen days to one day.\n\nPrugh reported that deployments became so routine that the Ops team was playing video games by the end of the first day. In addition to deployments going more smoothly for Dev\n\nand Ops, in 50% of the cases the customer received the value in half the time, underscoring how more frequent deployments can be good for Development, QA, Operations,\n\nand the customer.\n\nFigure 17: Daily deployments and increasing release frequency resulted in decrease in # of production incidents and MTTR (Source: “DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),” YouTube video, 29:39, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=tKdIHCL0DUg.)\n\nENABLE AUTOMATED SELF-SERVICE DEPLOYMENTS",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Consider the following quote from Tim Tischler, Director of Operations Automation at Nike, Inc., that describes the common experience of a generation of developers: “As a developer, there\n\nhas never been a more satisfying point in my career than when I wrote the code, when I pushed the button to deploy it, when I could see the production metrics confirm that it actually worked in\n\nproduction, and when I could fix it myself if it didn’t.”\n\nDevelopers’ ability to self-deploy code into production, to quickly see happy customers when their feature works, and to quickly fix\n\nany issues without having to open up a ticket with Operations has diminished over the last decade—in part as a result of a need for control and oversight, perhaps driven by security and compliance\n\nrequirements.\n\nThe resulting common practice is for Operations to perform code deployments, because separation of duties is a widely accepted\n\npractice to reduce the risk of production outages and fraud. However, to achieve DevOps outcomes, our goal is to shift our reliance to other control mechanisms that can mitigate these risks equally or even more effectively, such as through automated\n\ntesting, automated deployment, and peer review of changes.\n\nThe Puppet Labs’ 2013 State of DevOps Report, which surveyed\n\nover four thousand technology professionals, found that there was no statistically significant difference in the change success rates between organizations where Development deployed code and those where Operations deployed code.\n\nIn other words, when there are shared goals that span Development and Operations, and there is transparency, responsibility, and accountability for deployment outcomes, it",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "doesn’t matter who performs the deployment. In fact, we may even have other roles, such as testers or project managers, able to deploy to certain environments so they can get their own work\n\ndone quickly, such as setting up demonstrations of specific features in test or UAT environments.\n\nTo better enable fast flow, we want a code promotion process that\n\ncan be performed by either Development or Operations, ideally without any manual steps or handoffs. This affects the following steps:\n\nBuild: Our deployment pipeline must create packages from version control that can be deployed to any environment, including production.\n\nTest: Anyone should be able to run any or all of our automated test suite on their workstation or on our test systems.\n\nDeploy: Anybody should be able to deploy these packages to any environment where they have access, executed by running scripts that are also checked in to version control.\n\nThese are the practices that enable deployments to be performed successfully, regardless of who is performing the deployment.\n\nINTEGRATE CODE DEPLOYMENT INTO THE DEPLOYMENT PIPELINE\n\nOnce the code deployment process is automated, we can make it part of the deployment pipeline. Consequently, our deployment automation must provide the following capabilities:",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Ensure that packages created during the continuous integration process are suitable for deployment into production\n\nShow the readiness of production environments at a glance\n\nProvide a push-button, self-service method for any suitable version of the packaged code to be deployed into production\n\nRecord automatically, for auditing and compliance purposes, which commands were run on which machines when, who authorized it, and what the output was\n\nRun a smoke test to ensure the system is operating correctly and the configuration settings, including items such as database connection strings, are correct\n\nProvide fast feedback for the deployer so they can quickly determine whether their deployment was successful (e.g., did the deployment succeed, is the application performing as\n\nexpected in production, etc.)\n\nOur goal is ensure that deployments are fast—we don’t want to\n\nhave to wait hours to determine whether our code deployment succeeded or failed and then need hours to deploy any needed code fixes. Now that we have technologies such as containers, it is possible to complete even the most complex deployments in\n\nseconds or minutes. In Puppet Labs’ 2014 State of DevOps Report, the data showed that high performers had deployment\n\nlead times measured in minutes or hours, while the lowest\n\nperformers had deployment lead times measured in months.",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Figure 18: High performers had much faster deployment lead times and much faster time to restore production service after incidents (Source: Puppet Labs, 2014 State of DevOps Report.)\n\nBy building this capability, we now have a “deploy code” button\n\nthat allows us to safely and quickly promote changes to our code\n\nand our environments into production through our deployment pipeline.\n\nCase Study Etsy—Self-Service Developer Deployment, an Example of Continuous Deployment (2014)\n\nUnlike at Facebook where deployments are managed by release engineers, at Etsy deployments are performed by\n\nanyone who wants to perform a deployment, such as\n\nDevelopment, Operations, or Infosec. The deployment process at Etsy has become so safe and routine that new",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "engineers will perform a production deployment on their first\n\nday at work—as have Etsy board members and even dogs!\n\nAs Noah Sussman, a test architect at Etsy, wrote, “By the time 8am rolls around on a normal business day, 15 or so\n\npeople and dogs are starting to queue up, all of them\n\nexpecting to collectively deploy up to 25 changesets before the day is done.”\n\nEngineers who want to deploy their code first go to a chat\n\nroom, where engineers add themselves to the deploy queue, see the deployment activity in progress, see who else is in\n\nthe queue, broadcast their activities, and get help from other\n\nengineers when they need it. When it’s an engineer’s turn to deploy, they are notified in the chat room.\n\nThe goal at Etsy has been to make it easy and safe to\n\ndeploy into production with the fewest number of steps and the least amount of ceremony. Likely before the developer\n\neven checks in code, they will run on their workstation all\n\n4,500 unit tests, which takes less than one minute. All calls to external systems, such as databases, have been stubbed\n\nout.\n\nAfter they check their changes in to trunk in version control, over seven thousand automated trunk tests are instantly run\n\non their continuous integration (CI) servers. Sussman writes, “Through trial-and-error, we’ve settled on about 11 minutes\n\nas the longest that the automated tests can run during a\n\npush. That leaves time to re-run the tests once during a deployment [if someone breaks something and needs to fix\n\nit], without going too far past the 20 minute time limit.”",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "If all the tests were run sequentially, Sussman states that\n\n“the 7,000 trunk tests would take about half an hour to\n\nexecute. So we split these tests up into subsets, and distribute those onto the 10 machines in our Jenkins [CI]\n\ncluster....Splitting up our test suite and running many tests in parallel, gives us the desired 11 minute runtime.”\n\nFigure 19: The Deployinator console at Etsy (Source: Erik Kastner, “Quantum of Deployment,” CodeasCraft.com, May 20, 2010, https://codeascraft.com/2010/05/20/quantum-of-deployment/.)\n\nThe next tests to run are the smoke tests, which are system\n\nlevel tests that run cURL to execute PHPUnit test cases. Following these tests, the functional tests are run, which\n\nexecute end-to-end GUI-driven tests on a live server—this\n\nserver is either their QA environment or staging environment (nicknamed “Princess”), which is actually a production\n\nserver that has been taken out of rotation, ensuring that it\n\nexactly matches the production environment.",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Once it is an engineer’s turn to deploy, Erik Kastner writes,\n\n“you go to Deployinator [an internally developed tool, see\n\nfigure 19] and push the button to get it on QA. From there it visits Princess....Then, when it’s ready to go live, you hit the\n\n“Prod” button and soon your code is live, and everyone in IRC [chat channel] knows who pushed what code, complete\n\nwith a link to the diff. For anyone not on IRC, there’s the\n\nemail that everyone gets with the same information.”\n\nIn 2009, the deployment process at Etsy was a cause of\n\nstress and fear. By 2011, it had become a routine operation,\n\nhappening twenty-five to fifty times per day, helping engineers get their code quickly into production, delivering\n\nvalue to their customers.\n\nDECOUPLE DEPLOYMENTS FROM RELEASES\n\nIn the traditional launch of a software project, releases are driven by our marketing launch date. On the prior evening, we deploy our\n\ncompleted software (or as close to complete as we could get) into\n\nproduction. The next morning, we announce our new capabilities to the world, start taking orders, deliver the new functionality to\n\ncustomer, etc.\n\nHowever, all too often things don’t go according to plan. We may experience production loads that we never tested or designed for,\n\ncausing our service to fail spectacularly, both for our customers\n\nand our organization. Worse, restoring service may require a painful rollback process or an equally risky fix forward operation,\n\nwhere we make changes directly in production, this can all be a",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "truly miserable experience for workers. When everything is finally\n\nworking, everyone breathes a sigh of relief, grateful that production deployments and releases don’t happen more often.\n\nOf course, we know that we need to be deploying more frequently\n\nto achieve our desired outcome of smooth and fast flow, not less\n\nfrequently. To enable this, we need to decouple our production deployments from our feature releases. In practice, the terms\n\ndeployment and release are often used interchangeably. However, they are two distinct actions that serve two very different\n\npurposes:\n\nDeployment is the installation of a specified version of software to a given environment (e.g., deploying code into an integration\n\ntest environment or deploying code into production).\n\nSpecifically, a deployment may or may not be associated with a release of a feature to customers.\n\nRelease is when we make a feature (or set of features) available\n\nto all our customers or a segment of customers (e.g., we enable the feature to be used by 5% of our customer base). Our code\n\nand environments should be architected in such a way that the\n\nrelease of functionality does not require changing our application code.¶\n\nIn other words, when we conflate deployment and release, it\n\nmakes it difficult to create accountability for successful outcomes —decoupling these two activities allows us to empower\n\nDevelopment and Operations to be responsible for the success of\n\nfast and frequent deployments, while enabling product owners to be responsible for the successful business outcomes of the release\n\n(i.e., was building and launching the feature worth our time).",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "The practices described so far in this book ensure that we are\n\ndoing fast and frequent production deployments throughout feature development, with the goal of reducing the risk and impact\n\nof deployment errors. The remaining risk is release risk, which is\n\nwhether the features we put into production achieve the desired customer and business outcomes.\n\nIf we have extremely long deployment lead times, this dictates\n\nhow frequently we can release new features to the marketplace. However, as we become able to deploy on demand, how quickly\n\nwe expose new functionality to customers becomes a business and\n\nmarketing decision, not a technical decision. There are two broad categories of release patterns we can use:\n\nEnvironment-based release patterns: This is where we\n\nhave two or more environments that we deploy into, but only one environment is receiving live customer traffic (e.g., by\n\nconfiguring our load balancers). New code is deployed into a\n\nnon-live environment, and the release is performed moving traffic to this environment. These are extremely powerful\n\npatterns, because they typically require little or no change to\n\nour applications. These patterns include blue-green deployments, canary releases, and cluster immune systems,\n\nall of which will be discussed shortly.\n\nApplication-based release patterns: This is where we modify our application so that we can selectively release and\n\nexpose specific application functionality by small configuration\n\nchanges. For instance, we can implement feature flags that progressively expose new functionality in production to the\n\ndevelopment team, all internal employees, 1% of our customers, or, when we are confident that the release will",
      "content_length": 1702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "operate as designed, our entire customer base. As discussed\n\nearlier, this enables a technique called dark launching, where we stage all the functionality to be launched in production and\n\ntest it with production traffic before our release. For instance,\n\nwe may invisibly test our new functionality with production traffic for weeks before our launch in order to expose problems\n\nso that they can be fixed before our actual launch.\n\nENVIRONMENT-BASED RELEASE PATTERNS\n\nDecoupling deployments from our releases dramatically changes\n\nhow we work. We no longer have to perform deployments in the middle of the night or on weekends to lower the risk of negatively\n\nimpacting customers. Instead, we can do deployments during\n\ntypical business hours, enabling Ops to finally have normal working hours, just like everyone else.\n\nThis section focuses on environment-based release patterns,\n\nwhich require no changes to application code. We do this by having multiple environments to deploy into, but only one of them\n\nreceives live customer traffic. By doing this, we can significantly\n\ndecrease the risk associated with production releases and reduce the deployment lead time.\n\nThe Blue-Green Deployment Pattern\n\nThe simplest of the three patterns is called blue-green\n\ndeployment. In this pattern, we have two production environments: blue and green. At any time, only one of these is\n\nserving customer traffic—in figure 20, the green environment is\n\nlive.",
      "content_length": 1451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Figure 20: Blue-green deployment patterns (Source: Humble and North, Continuous Delivery, 261.)\n\nTo release a new version of our service, we deploy to the inactive\n\nenvironment where we can perform our testing without\n\ninterrupting the user experience. When we are confident that everything is functioning as designed, we execute our release by\n\ndirecting traffic to the blue environment. Thus, blue becomes live and green becomes staging. Roll back is performed by sending customer traffic back to the green environment.**\n\nThe blue-green deployment pattern is simple, and it is extremely easy to retrofit onto existing systems. It also has incredible\n\nbenefits, such as enabling the team to perform deployments\n\nduring normal business hours and conduct simple changeovers (e.g., changing a router setting, changing a symlink) during off-\n\npeak times. This alone can dramatically improve the work\n\nconditions for the team performing the deployment.\n\nDealing with Database Changes\n\nHaving two versions of our application in production creates problems when they depend upon a common database—when the\n\ndeployment requires database schema changes or adding,\n\nmodifying, or deleting tables or columns, the database cannot support both versions of our application. There are two general\n\napproaches to solving this problem:",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Create two databases (i.e., a blue and green database):\n\nEach version—blue (old) and green (new)—of the application has its own database. During the release, we put the blue\n\ndatabase into read-only mode, perform a backup of it, restore\n\nonto the green database, and finally switch traffic to the green environment. The problem with this pattern is that if we need\n\nto roll back to the blue version, we can potentially lose\n\ntransactions if we don’t manually migrate them from the green version first.\n\nDecouple database changes from application changes:\n\nInstead of supporting two databases, we decouple the release of database changes from the release of application changes by\n\ndoing two things: First, we make only additive changes to our\n\ndatabase, we never mutate existing database objects, and second, we make no assumptions in our application about\n\nwhich database version will be in production. This is very different than how we’ve been traditionally trained to think\n\nabout databases, where we avoid duplicating data. The process\n\nof Decoupling database changes from application changes was used by IMVU (among others) around 2009, enabling them to\n\ndo fifty deployments per day, some of which required database changes.††\n\nCase Study Dixons Retail—Blue-Green Deployment for Point-Of-Sale System (2008)\n\nDan North and Dave Farley, co-authors of Continuous\n\nDelivery, were working on a project for Dixons Retail, a large British retailer involving thousands of point-of-sale (POS)",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "systems that resided in hundreds of retail stores and\n\noperating under a number of different customer brands.\n\nAlthough blue-green deployments are mostly associated\n\nwith online web services, North and Farley used this pattern\n\nto significantly reduce the risk and changeover times for POS upgrades.\n\nTraditionally, upgrading POS systems are a big bang,\n\nwaterfall project: the POS clients and the centralized server are upgraded at the same time, which requires extensive\n\ndowntime (often an entire weekend), as well as significant\n\nnetwork bandwidth to push out the new client software to all the retail stores. When things don’t go entirely according to\n\nplan, it can be incredibly disruptive to store operations.\n\nFor this upgrade, there was not enough network bandwidth to upgrade all the POS systems simultaneously, which made\n\nthe traditional strategy impossible. To solve this problem,\n\nthey used the blue-green strategy and created two production versions of the centralized server software,\n\nenabling them to simultaneously support the old and new\n\nversions of the POS clients.\n\nAfter they did this, weeks before the planned POS upgrade,\n\nthey started sending out new versions of client POS\n\nsoftware installers to the retail stores over the slow network links, deploying the new software onto the POS systems in\n\nan inactive state. Meanwhile, the old version kept running as normal.",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "When all the POS clients had everything staged for the\n\nupgrade (the upgraded client and server had tested together successfully, and new client software had been deployed to\n\nall clients), the store managers were empowered to decide\n\nwhen to release the new version.\n\nDepending on their business needs, some managers\n\nwanted to use the new features immediately and released\n\nright away, while others wanted to wait. In either case, whether releasing features immediately or waiting, itwas\n\nsignificantly better for the managers than having the centralized IT department choose for them when the release\n\nwould occur.\n\nThe result was a significantly smoother and faster release,\n\nhigher satisfaction from the store managers, and far less disruption to store operations. Furthermore, this application\n\nof blue-green deployments to thick-client PC applications demonstrates how DevOps patterns can be universally\n\napplied to different technologies, often in very surprising\n\nways but with the same fantastic outcomes.\n\nThe Canary and Cluster Immune System Release Patterns\n\nThe blue-green release pattern is easy to implement and can dramatically increase the safety of software releases. There are\n\nvariants of this pattern that can further improve safety and\n\ndeployment lead times using automation, but with the potential trade-off of additional complexity.\n\nThe canary release pattern automates the release process of\n\npromoting to successively larger and more critical environments",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "as we confirm that the code is operating as designed.\n\nThe term canary release comes from the tradition of coal miners bringing caged canaries into mines to provide early detection of\n\ntoxic levels of carbon monoxide. If there wastoo much gas in the\n\ncave, it would kill the canaries before it killed the miners, alerting them to evacuate.\n\nIn this pattern, when we perform a release, we monitor how the\n\nsoftware in each environment is performing. When something appears to be going wrong, we roll back; otherwise, we deploy to the next environment.‡‡\n\nFigure 21 shows the groups of environments Facebook created to support this release pattern:\n\nA1 group: Production servers that only serve internal employees\n\nA2 group: Production servers that only serve a small\n\npercentage of customers and are deployed when certain acceptance criteria have been met (either automated or\n\nmanual)\n\nA3 group: The rest of the production servers, which are deployed after the software running in the A2 cluster meets\n\ncertain acceptance criteria",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Figure 21: The canary release pattern (Source: Humble and Farley, Continuous Delivery, 263.)\n\nThe cluster immune system expands upon the canary release\n\npattern by linking our production monitoring system with our\n\nrelease process and by automating the roll back of code when the\n\nuser-facing performance of the production system deviates outside of a predefined expected range, such as when the conversion rates\n\nfor new users drops below our historical norms of 15%–20%.\n\nThere are two significant benefits to this type of safeguard. First, we protect against defects that are hard to find through automated\n\ntests, such as a web page change that renders some critical page\n\nelement invisible (e.g., CSS change). Second, we reduce the time\n\nrequired to detect and respond to the degraded performance created by our change.§§\n\nAPPLICATION-BASED PATTERNS TO ENABLE SAFER RELEASES\n\nIn the previous section, we created environment-based patterns\n\nthat allowed us to decouple our deployments from our releases by\n\nusing multiple environments and by switching between which",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "environment was live, which can be entirely implemented at the\n\ninfrastructure level.\n\nIn this section, we describe application-based release patterns that we can implement in our code, allowing even greater flexibility in\n\nhow we safely release new features to our customer, often on a\n\nper-feature basis. Because application-based release patterns are\n\nimplemented in the application, these require involvement from\n\nDevelopment.\n\nImplement Feature Toggles\n\nThe primary way we enable application-based release patterns is\n\nby implementing feature toggles, which provide us with the\n\nmechanism to selectively enable and disable features without\n\nrequiring a production code deployment. Feature toggles can also control which features are visible and available to specific user\n\nsegments (e.g., internal employees, segments of customers).\n\nFeature toggles are usually implemented by wrapping application logic or UI elements with a conditional statement, where the\n\nfeature is enabled or disabled based on a configuration setting\n\nstored somewhere. This can be as simple as an application\n\nconfiguration file (e.g., configuration files in JSON, XML), or it\n\nmight be through a directory service or even a web service specifically designed to manage feature toggling.¶¶\n\nFeature toggles also enable us to do the following:\n\nRoll back easily: Features that create problems or\n\ninterruptions in production can be quickly and safely disabled\n\nby merely changing the feature toggle setting. This is especially\n\nvaluable when deployments are infrequent—switching off one",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "particular stakeholder’s features is usually much easier than\n\nrolling back an entire release.\n\nGracefully degrade performance: When our service\n\nexperiences extremely high loads that would normally require\n\nus to increase capacity or, worse, risk having our service fail in\n\nproduction, we can use feature toggles to reduce the quality of service. In other words, we can increase the number of users\n\nwe serve by reducing the level of functionality delivered (e.g.,\n\nreduce the number of customers who can access a certain\n\nfeature, disable CPU-intensive features such as\n\nrecommendations, etc.).\n\nIncrease our resilience through a service-oriented\n\narchitecture: If we have a feature that relies on another\n\nservice that isn’t complete yet, we can still deploy our feature\n\ninto production but hide it behind a feature toggle. When that service finally becomes available, we can toggle the feature on.\n\nSimilarly, when a service we rely upon fails, we can turn off the\n\nfeature to prevent calls to the downstream service while\n\nkeeping the rest of the application running.\n\nTo ensure that we find errors in features wrapped in feature\n\ntoggles, our automated acceptance tests should run with all\n\nfeature toggles on. (We should also test that our feature toggling\n\nfunctionality works correctly too!)\n\nFeature toggles enable the decoupling of code deployments and\n\nfeature releases, later in the book we use feature toggles to enable\n\nhypothesis-driven development and A/B testing, furthering our ability to achieve our desired business outcomes.",
      "content_length": 1548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Perform Dark Launches\n\nFeature toggles allow us to deploy features into production\n\nwithout making them accessible to users, enabling a technique\n\nknown as dark launching. This is where we deploy all the\n\nfunctionality into production and then perform testing of that\n\nfunctionality while it is still invisible to customers. For large or\n\nrisky changes, we often do this for weeks before the production launch, enabling us to safely test with the anticipated production-\n\nlike loads.\n\nFor instance, suppose we dark launch a new feature that poses significant release risk, such as new search features, account\n\ncreation processes, or new database queries. After all the code is in\n\nproduction, keeping the new feature disabled, we may modify user\n\nsession code to make calls to new functions—instead of displaying\n\nthe results to the user, we simply log or discard the results.\n\nFor example, we may have 1% of our online users make invisible\n\ncalls to a new feature scheduled to be launched to see how our new\n\nfeature behaves under load. After we find and fix any problems,\n\nwe progressively increase the simulated load by increasing the frequency and number of users exercising the new functionality.\n\nBy doing this, we are able to safely simulate production-like loads,\n\ngiving us confidence that our service will perform as it needs to.\n\nFurthermore, when we launch a feature, we can progressively roll\n\nout the feature to small segments of customers, halting the release\n\nif any problems are found. That way, we minimize the number of\n\ncustomers who are given a feature only to have it taken away\n\nbecause we find a defect or are unable to maintain the required\n\nperformance.",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "In 2009, when John Allspaw was VP of Operations at Flickr, he\n\nwrote to the Yahoo! executive management team about their dark\n\nlaunch process, saying that it “increases everyone’s confidence almost to the point of apathy, as far as fear of load-related issues\n\nare concerned. I have no idea how many code deploys there were\n\nmade to production on any given day in the past 5 years...because\n\nfor the most part I don’t care, because those changes made in\n\nproduction have such a low chance of causing issues. When they have caused issues, everyone on the Flickr staff can find on a\n\nwebpage when the change was made, who made the change, and exactly (line-by-line) what the change was.”***\n\nLater, when we have built adequate production telemetry in our\n\napplication and environments, we can also enable faster feedback\n\ncycles to validate our business assumptions and outcomes\n\nimmediately after we deploy the feature into production.\n\nBy doing this, we no longer wait until a big bang release to test\n\nwhether customers want to use the functionality we build. Instead,\n\nby the time we announce and release our big feature, we have\n\nalready tested our business hypotheses and run countless\n\nexperiments to continually refine our product with real customers, which helps us validate that the features will achieve the desired\n\ncustomer outcomes.\n\nCase Study Dark Launch of Facebook Chat (2008)\n\nFor nearly a decade, Facebook has been one of the most\n\nwidely visited Internet sites, as measured by pages viewed\n\nand unique site users. In 2008, it had over seventy million",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "daily active users, which created a challenge for the team that was developing the new Facebook Chat functionality.†††\n\nEugene Letuchy, an engineer on the Chat team, wrote about how the number of concurrent users presented a huge\n\nsoftware engineering challenge: “The most resource-\n\nintensive operation performed in a chat system is not\n\nsending messages. It is rather keeping each online user\n\naware of the online-idle-offline states of their friends, so that conversations can begin.”\n\nImplementing this computationally-intensive feature was one\n\nof the largest technical undertakings ever at Facebook and took almost a year to complete.‡‡‡ Part of the complexity of the project was due to the wide variety of technologies\n\nneeded to achieve the desired performance, including C++,\n\nJavaScript, and PHP, as well as their first use of Erlang in\n\ntheir back-end infrastructure.\n\nThroughout the course of the year-long endeavor, the Chat\n\nteam checked their code in to version control, where it would\n\nbe deployed into production at least once per day. At first,\n\nthe Chat functionality was visible only to the Chat team. Later, it was made visible to all internal employees, but it\n\nwas completely hidden from external Facebook users\n\nthrough Gatekeeper, the Facebook feature toggling service.\n\nAs part of their dark launch process, every Facebook user\n\nsession, which runs JavaScript in the user browser, had a\n\ntest harness loaded into it—the chat UI elements were\n\nhidden, but the browser client would send invisible test chat\n\nmessages to the back-end chat service that was already in",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "production, enabling them to simulate production-like loads\n\nthroughout the entire project, allowing them to find and fix performance problems long before the customer release.\n\nBy doing this, every Facebook user was part of a massive\n\nload testing program, which enabled the team to gain confidence that their systems could handle realistic\n\nproduction-like loads. The Chat release and launch required\n\nonly two steps: modifying the Gatekeeper configuration\n\nsetting to make the Chat feature visible to some portion of\n\nexternal users, and having Facebook users load new\n\nJavaScript code that rendered the Chat UI and disabled the invisible test harness. If something went wrong, the two\n\nsteps would be reversed. When the launch day of Facebook\n\nChat arrived, it was surprisingly successful and uneventful,\n\nseeming to scale effortlessly from zero to seventy million\n\nusers overnight. During the release, they incrementally\n\nenabled the chat functionality to ever-larger segments of the customer population—first to all internal Facebook\n\nemployees, then to 1% of the customer population, then to\n\n5%, and so forth. As Letuchy wrote, “The secret for going\n\nfrom zero to seventy million users overnight is to avoid doing\n\nit all in one fell swoop.”\n\nSURVEY OF CONTINUOUS DELIVERY AND CONTINUOUS DEPLOYMENT IN PRACTICE\n\nIn Continuous Delivery, Jez Humble and David Farley define the\n\nterm continuous delivery. The term continuous deployment was\n\nfirst mentioned by Tim Fitz in his blog post “Continuous",
      "content_length": 1502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Deployment at IMVU: Doing the impossible fifty times a day.”\n\nHowever, in 2015, during the construction of The DevOps\n\nHandbook, Jez Humble commented, “In the last five years, there\n\nhas been confusion around the terms continuous delivery versus\n\ncontinuous deployment—and, indeed, my own thinking and definitions have changed since we wrote the book. Every\n\norganization should create their variations, based on what they\n\nneed. The key thing we should care about is not the form, but the\n\noutcomes: deployments should be low-risk, push-button events we\n\ncan perform on demand.”\n\nHis updated definitions of continuous delivery and continuous\n\ndeployment are as follows:\n\nWhen all developers are working in small batches on trunk, or\n\neveryone is working off trunk in short-lived feature branches\n\nthat get merged to trunk regularly, and when trunk is always\n\nkept in a releasable state, and when we can release on demand\n\nat the push of a button during normal business hours, we are doing continuous delivery. Developers get fast feedback when\n\nthey introduce any regression errors, which include defects,\n\nperformance issues, security issues, usability issues, etc. When\n\nthese issues are found, they are fixed immediately so that trunk\n\nis always deployable.\n\nIn addition to the above, when we are deploying good builds\n\ninto production on a regular basis through self-service (being\n\ndeployed by Dev or by Ops)—which typically means that we are deploying to production at least once per day per developer, or\n\nperhaps even automatically deploying every change a\n\ndeveloper commits—this is when we are engaging in\n\ncontinuous deployment.",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Defined this way, continuous delivery is the prerequisite for\n\ncontinuous deployment—just as continuous integration is a prerequisite for continuous delivery. Continuous deployment is\n\nlikely applicable in the context of web services that are delivered\n\nonline. However, continuous delivery is applicable in almost every\n\ncontext where we desire deployments and releases that have high\n\nquality, fast lead times and have highly predictable, low-risk\n\noutcomes, including for embedded systems, COTS products, and mobile apps.\n\nAt Amazon and Google, most teams practice continuous delivery,\n\nalthough some perform continuous deployment— thus, there is considerable variation between teams in how frequently they\n\ndeploy code and how deployments are performed. Teams are\n\nempowered to choose how to deploy based on the risks they are\n\nmanaging. For example, the Google App Engine team often\n\ndeploys once per day, while the Google Search property deploys several times per week.\n\nSimilarly, most of the cases studies presented in this book are also\n\ncontinuous delivery, such as the embedded software running on\n\nHP LaserJet printers, the CSG bill printing operations running on twenty technology platforms including a COBOL mainframe\n\napplication, Facebook, and Etsy. These same patterns can be used\n\nfor software that runs on mobile phones, ground control stations\n\nthat control satellites, and so forth.\n\nCONCLUSION\n\nAs the Facebook, Etsy, and CSG examples have shown, releases\n\nand deployments do not have to be high-risk, high-drama affairs",
      "content_length": 1542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "that require tens or hundreds of engineers to work around the\n\nclock to complete. Instead, they can be made entirely routine and\n\na part of everyone’s daily work.\n\nBy doing this, we can reduce our deployment lead times from\n\nmonths to minutes, allowing our organizations to quickly deliver\n\nvalue to our customer without causing chaos and disruption.\n\nFurthermore, by having Dev and Ops work together, we can finally\n\nmake Operations work humane.\n\n† A canary release test is when software is deployed to a small group of production servers to make sure nothing\n\nterrible happens to them with live customer traffic.\n\n‡ The Facebook front-end codebase is primarily written in PHP. In 2010, to increase site performance, the PHP code was converted into C++ by their internally developed HipHop compiler, which was then compiled into a 1.5 GB executable. This file was then copied onto production servers using BitTorrent, enabling the copy operation to be completed in fifteen minutes.\n\n§ In their experiments, they found that SOT teams were successful, regardless of whether they were managed by\n\nDevelopment or Operations, as long as the teams were staffed with the right people and were dedicated to SOT success.\n\n¶ Operation Desert Shield may serve as an effective metaphor. Starting on August 7, 1990, thousands of men and materials were safely deployed over four months into the production theater, culminating in a single, multi- disciplinary, highly coordinated release.\n\n** Other ways that we can implement the blue-green pattern include setting up multiple Apache/NGINX web\n\nservers to listen on different physical or virtual interfaces; employing multiple virtual roots on Windows IIS servers bound to different ports; using different directories for every version of the system, with a symbolic link determining which one is live (e.g., as Capistrano does for Ruby on Rails); running multiple versions of services or middleware concurrently, with each listening on different ports; using two different data centers and switching traffic between the data centers, instead of using them merely as hot- or warm-spares for disaster recovery purposes (incidentally, by routinely using both environments, we are continually ensuring that our disaster recovery process works as designed); or using different availability zones in the cloud.\n\n†† This pattern is also commonly referred to as the expand/contract pattern, which Timothy Fitz described when he said, “We do not change (mutate) database objects, such as columns or tables. Instead, we first expand, by adding new objects, then, later, contract by removing the old ones.” Furthermore, increasingly, there are technologies that enable virtualization, versioning, labeling, and roll back of databases, such as Redgate, Delphix, DBMaestro, and Datical, as well as open source tools, such as DBDeploy, that make database changes dramatically safer and faster.\n\n‡‡ Note that canary releases require having multiple versions of our software running in production\n\nsimultaneously. However, because each additional version we have in production creates additional complexity to manage, we should keep the number of versions to a minimum. This may require the use of the expand/contract database pattern described earlier.\n\n§§ The cluster immune system was first documented by Eric Ries while working at IMVU. This functionality is\n\nalso supported by Etsy in their Feature API library, as well as by Netflix.\n\n¶¶ One sophisticated example of such a service is Facebook’s Gatekeeper, an internally developed service that dynamically selects which features are visible to specific users based on demographic information such as location, browser type, and user profile data (age, gender, etc.). For instance, a particular feature could be configured so that it is only accessible by internal employees, 10% of their user base, or only users between the",
      "content_length": 3903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "ages of twenty-five and thirty-five. Other examples include the Etsy Feature API and the Netflix Archaius library.\n\n*** Similarly, as Chuck Rossi, Director of Release Engineering at Facebook, described, “All the code supporting\n\nevery feature we’re planning to launch over the next six months has already been deployed onto our production servers. All we need to do is turn it on.”\n\n††† By 2015, Facebook had over one billion active users, growing 17% over the previous year.\n\n‡‡‡ This problem has a worst-case computational characteristic of O(n3). In other words, the compute time increases exponentially as the function of the number of online users, the size of their friend lists, and the frequency of online/offline state change.",
      "content_length": 735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "13Architect for Low-Risk Releases\n\nAlmost every well-known DevOps exemplar has had near-death\n\nexperiences due to architectural problems, such as in the stories\n\npresented about LinkedIn, Google, eBay, Amazon, and Etsy. In\n\neach case, they were able to successfully migrate to a more\n\nsuitable architecture that addressed their current problems and organizational needs.\n\nThis is the principle of evolutionary architecture—Jez Humble\n\nobserves that architecture of “any successful product or organization will necessarily evolve over its life cycle.” Before his\n\ntenure at Google, Randy Shoup served as chief engineer and distinguished architect at eBay from 2004 to 2011. He observes\n\nthat “both eBay and Google are each on their fifth entire rewrite of\n\ntheir architecture from top to bottom.”\n\nHe reflects, “Looking back with 20/20 hindsight, some technology [and architectural choices] look prescient and others look shortsighted. Each decision most likely best served the\n\norganizational goals at the time. If we had tried to implement the 1995 equivalent of micro-services out of the gate, we would have likely failed, collapsing under our own weight and probably taking the entire company with us.”†",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "The challenge is how to keep migrating from the architecture we\n\nhave to the architecture we need. In the case of eBay, when they\n\nneeded to re-architect, they would first do a small pilot project to\n\nprove to themselves that they understood the problem well\n\nenough to even undertake the effort. For instance, when Shoup’s\n\nteam was planning on moving certain portions of the site to full-\n\nstack Java in 2006, they looked for the area that would get them\n\nthe biggest bang for their buck by sorting the site pages by revenue\n\nproduced. They chose the highest revenue areas, stopping when\n\nthere was not enough of a business return to justify the effort.\n\nWhat Shoup’s team did at eBay is a textbook example of evolutionary design, using a technique called the strangler application pattern—instead of “ripping out and replacing” old\n\nservices with architectures that no longer support our organizational goals, we put the existing functionality behind an API and avoid making further changes to it. All new functionality is then implemented in the new services that use the new desired architecture, making calls to the old system when necessary.\n\nThe strangler application pattern is especially useful for helping\n\nmigrate portions of a monolithic application or tightly-coupled services to one that is more loosely-coupled. All too often, we find\n\nourselves working within an architecture that has become too tightly-coupled and too interconnected, often having been created years (or decades) ago.\n\nThe consequences of overly tight architectures are easy to spot: every time we attempt to commit code in to trunk or release code in to production, we risk creating global failures (e.g., we break\n\neveryone else’s tests and functionality, or the entire site goes down). To avoid this, every small change requires enormous",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "amounts of communication and coordination over days or weeks,\n\nas well as approvals from any group that could potentially be\n\naffected. Deployments become problematic as well—the number\n\nof changes that are batched together for each deployment grows,\n\nfurther complicating the integration and test effort, and increasing\n\nthe already high likelihood of something going wrong.\n\nEven deploying small changes may require coordinating with\n\nhundreds (or even thousands) of other developers, with any one of\n\nthem able to create a catastrophic failure, potentially requiring\n\nweeks to find and fix the problem. (This results in another\n\nsymptom: “My developers spend only 15% of their time coding— the rest of their time is spent in meetings.”)\n\nThese all contribute to an extremely unsafe system of work, where small changes have seemingly unknowable and catastrophic consequences. It also often contributes to a fear of integrating and\n\ndeploying our code, and the self-reinforcing downward spiral of deploying less frequently.\n\nFrom an enterprise architecture perspective, this downward spiral is the consequence of the Second Law of Architectural Thermodynamics, especially in large, complex organizations.\n\nCharles Betz, author of Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler’s Children, observes, “[IT project owners]\n\nare not held accountable for their contributions to overall system entropy.” In other words, reducing our overall complexity and increasing the productivity of all our development teams is rarely the goal of an individual project.",
      "content_length": 1622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "In this chapter, we will describe steps we can take to reverse the downward spiral, review the major architectural archetypes,\n\nexamine the attributes of architectures that enable developer productivity, testability, deployability, and safety, as well as\n\nevaluate strategies that allow us to safely migrate from whatever\n\ncurrent architecture we have to one that better enables the achievement of our organizational goals.\n\nAN ARCHITECTURE THAT ENABLES PRODUCTIVITY, TESTABILITY, AND SAFETY\n\nIn contrast to a tightly-coupled architecture that can impede\n\neveryone’s productivity and ability to safely make changes, a loosely-coupled architecture with well-defined interfaces that\n\nenforce how modules connect with each other promotes productivity and safety. It enables small, productive, two-pizza\n\nteams that are able to make small changes that can be safely and\n\nindependently deployed. And because each service also has a well- defined API, it enables easier testing of services and the creation\n\nof contracts and SLAs between teams.",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Figure 22: Google cloud datastore (Source: Shoup, “From the Monolith to Micro-services.”)\n\nAs Randy Shoup describes, “This type of architecture has served Google extremely well—for a service like Gmail, there’s five or six\n\nother layers of services underneath it, each very focused on a very specific function. Each service is supported by a small team, who\n\nbuilds it and runs their functionality, with each group potentially\n\nmaking different technology choices. Another example is the Google Cloud Datastore service, which is one of the largest NoSQL\n\nservices in the world—and yet it is supported by a team of only about eight people, largely because it is based on layers upon\n\nlayers of dependable services built upon each other.”\n\nThis kind of service-oriented architecture allows small teams to work on smaller and simpler units of development that each team\n\ncan deploy independently, quickly, and safely. Shoup notes,\n\n“Organizations with these types of architectures, such as Google and Amazon, show how it can impact organizational structures,",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "[creating] flexibility and scalability. These are both organizations\n\nwith tens of thousands of developers, where small teams can still\n\nbe incredibly productive.”\n\nARCHITECTURAL ARCHETYPES: MONOLITHS VS. MICROSERVICES\n\nAt some point in their history, most DevOps organizations were hobbled by tightly-coupled, monolithic architectures that—while\n\nextremely successful at helping them achieve product/market fit— put them at risk of organizational failure once they had to operate\n\nat scale (e.g., eBay’s monolithic C++ application in 2001, Amazon’s monolithic OBIDOS application in 2001, Twitter’s\n\nmonolithic Rails front-end in 2009, and LinkedIn’s monolithic\n\nLeo application in 2011). In each of these cases, they were able to re-architect their systems and set the stage not only to survive, but\n\nalso to thrive and win in the marketplace.\n\nMonolithic architectures are not inherently bad—in fact, they are often the best choice for an organization early in a product life\n\ncycle. As Randy Shoup observes, “There is no one perfect architecture for all products and all scales. Any architecture meets\n\na particular set of goals or range of requirements and constraints,\n\nsuch as time to market, ease of developing functionality, scaling, etc. The functionality of any product or service will almost\n\ncertainly evolve over time—it should not be surprising that our\n\narchitectural needs will change as well. What works at scale 1x rarely works at scale 10x or 100x.”",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Table 3: Architectural archetypes\n\n(Source: Shoup, “From the Monolith to Micro-services.”)\n\nThe major architectural archetypes are shown in table 3, each row\n\nindicates a different evolutionary need for an organization, with each column giving the pros and cons of each of the different archetypes. As the table shows, a monolithic architecture that\n\nsupports a startup (e.g., rapid prototyping of new features, and potential pivots or large changes in strategies) is very different",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "from an architecture that needs hundreds of teams of developers, each of whom must be able to independently deliver value to the\n\ncustomer. By supporting evolutionary architectures, we can ensure that our architecture always serves the current needs of the organization.\n\nCase Study Evolutionary Architecture at Amazon (2002)\n\nOne of the most studied architecture transformations occurred at Amazon. In an interview with ACM Turing\n\nAward-winner and Microsoft Technical Fellow Jim Gray, Amazon CTO Werner Vogels explains that Amazon.com started in 1996 as a “monolithic application, running on a\n\nweb server, talking to a database on the back end. This application, dubbed Obidos, evolved to hold all the business logic, all the display logic, and all the functionality that Amazon eventually became famous for: similarities,\n\nrecommendations, Listmania, reviews, etc.”\n\nAs time went by, Obidos grew too tangled, with complex sharing relationships meaning individual pieces could not be\n\nscaled as needed. Vogels tells Gray that this meant “many things that you would like to see happening in a good software environment couldn’t be done anymore; there were\n\nmany complex pieces of software combined into a single system. It couldn’t evolve anymore.”\n\nDescribing the thought process behind the new desired\n\narchitecture, he tells Gray, “We went through a period of serious introspection and concluded that a service-oriented",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "architecture would give us the level of isolation that would allow us to build many software components rapidly and\n\nindependently.”\n\nVogels notes, “The big architectural change that Amazon went through in the past five years [from 2001–2005] was to\n\nmove from a two-tier monolith to a fully-distributed, decentralized, services platform serving many different applications. A lot of innovation was necessary to make this happen, as we were one of the first to take this approach.”\n\nThe lessons from Vogel’s experience at Amazon that are important to our understanding of architecture shifts include the following:\n\nLesson 1: When applied rigorously, strict service orientation is an excellent technique to achieve isolation; you achieve a level of ownership and control that was not\n\nseen before.\n\nLesson 2: Prohibiting direct database access by clients makes performing scaling and reliability improvements to\n\nyour service state possible without involving your clients.\n\nLesson 3: Development and operational process greatly\n\nbenefits from switching to service-orientation. The services model has been a key enabler in creating teams that can innovate quickly with a strong customer focus. Each service has a team associated with it, and that team is completely\n\nresponsible for the service—from scoping out the functionality to architecting, building, and operating it.",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "The extent to which applying these lessons enhances developer productivity and reliability is breathtaking. In 2011, Amazon was performing approximately fifteen thousands\n\ndeployments per day. By 2015, they were performing nearly 136,000 deployments per day.\n\nUSE THE STRANGLER APPLICATION PATTERN TO SAFELY EVOLVE OUR ENTERPRISE ARCHITECTURE\n\nThe term strangler application was coined by Martin Fowler in 2004 after he was inspired by seeing massive strangler vines\n\nduring a trip to Australia, writing, “They seed in the upper branches of a fig tree and gradually work their way down the tree until they root in the soil. Over many years they grow into fantastic and beautiful shapes, meanwhile strangling and killing\n\nthe tree that was their host.”\n\nIf we have determined that our current architecture is too tightly- coupled, we can start safely decoupling parts of the functionality\n\nfrom our existing architecture. By doing this, we enable teams supporting the decoupled functionality to independently develop, test, and deploy their code into production with autonomy and\n\nsafety, and reduce architectural entropy.\n\nAs described earlier, the strangler application pattern involves placing existing functionality behind an API, where it remains\n\nunchanged, and implementing new functionality using our desired architecture, making calls to the old system when necessary. When we implement strangler applications, we seek to access all",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "services through versioned APIs, also called versioned services or immutable services.\n\nVersioned APIs enable us to modify the service without impacting the callers, which allows the system to be more loosely-coupled—if we need to modify the arguments, we create a new API version and migrate teams who depend on our service to the new version.\n\nAfter all, we are not achieving our re-architecting goals if we allow our new strangler application to get tightly-coupled into other services (e.g., connecting directly to another service’s database).\n\nIf the services we call do not have cleanly-defined APIs, we should build them or at least hide the complexity of communicating with such systems within a client library that has a cleanly defined API.\n\nBy repeatedly decoupling functionality from our existing tightly- coupled system, we move our work into a safe and vibrant ecosystem where developers can be far more productive resulting\n\nin the legacy application shrinking in functionality. It might even disappear entirely as all the needed functionality migrates to our new architecture.\n\nBy creating strangler applications, we avoid merely reproducing existing functionality in some new architecture or technology— often, our business processes are far more complex than necessary\n\ndue to the idiosyncrasies of the existing systems, which we will end up replicating. (By researching the user, we can often re- engineer the process so that we can design a far simpler and more streamlined means to achieving the business goal.)‡\n\nAn observation from Martin Fowler underscores this risk: “Much of my career has involved rewrites of critical systems. You would",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "think such a thing is easy—just make the new one do what the old one did. Yet they are always much more complex than they seem, and overflowing with risk. The big cut-over date looms, and the\n\npressure is on. While new features (there are always new features) are liked, old stuff has to remain. Even old bugs often need to be added to the rewritten system.”\n\nAs with any transformation, we seek to create quick wins and deliver early incremental value before continuing to iterate. Up- front analysis helps us identify the smallest possible piece of work\n\nthat will usefully achieve a business outcome using the new architecture.\n\nCase Study Strangler Pattern at Blackboard Learn (2011)\n\nBlackboard Inc. is one of the pioneers of providing technology for educational institutions, with annual revenue of approximately $650 million in 2011. At that time, the\n\ndevelopment team for their flagship Learn product, packaged software that was installed and run on-premise at their customer sites, was living with the daily consequences of a legacy J2EE codebase that went back to 1997. As David Ashman, their chief architect, observes, “we still have\n\nfragments of Perl code still embedded throughout our\n\ncodebase.”\n\nIn 2010, Ashman was focused on the complexity and\n\ngrowing lead times associated with the old system,\n\nobserving that “our build, integration, and testing processes kept getting more and more complex and error prone. And",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "the larger the product got, the longer our lead times and the\n\nworse the outcomes for our customers. To even get\n\nfeedback from our integration process would require twenty- four to thirty-six hours.”\n\nFigure 23: Blackboard Learn code repository: before Building Blocks (Source: “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4.)\n\nHow this started to impact developer productivity was made\n\nvisible to Ashman in graphs generated from their source code repository going all the way back to 2005.\n\nIn figure 24, the top graph represents the number of lines of\n\ncode in the monolithic Blackboard Learn code repository; the bottom graph represents the number of code commits. The\n\nproblem that became evident to Ashman was that the\n\nnumber of code commits started to decrease, objectively showing the increasing difficulty of introducing code\n\nchanges, while the number of lines of code continued to",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "increase. Ashman noted, “To me, it said we needed to do\n\nsomething, otherwise the problems would keep getting\n\nworse, with no end in sight.”\n\nAs a result, in 2012 Ashman focused on implementing a\n\ncode re-architecturing project that used the strangler pattern.\n\nThe team accomplished this by creating what they internally called Building Blocks, which allowed developers to work in\n\nseparate modules that were decoupled from the monolithic\n\ncodebase and accessed through fixed APIs. This enabled them to work with far more autonomy, without having to\n\nconstantly communicate and coordinate with other development teams.\n\nFigure 24: Blackboard Learn code repository: after Building Blocks (Source: “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds.” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4.)\n\nWhen Building Blocks were made available to developers, the size of the monolith source code repository began to\n\ndecrease (as measured by number of lines of code).\n\nAshman explained that this was because developers were",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "moving their code into the Building Block modules source\n\ncode repository. “In fact,” Ashman reported, “every\n\ndeveloper given a choice would work in the Building Block codebase, where they could work with more autonomy and\n\nfreedom and safety.”\n\nThe graph above shows the connection between the exponential growth in the number of lines of code and the\n\nexponential growth of the number of code commits for the\n\nBuilding Blocks code repositories. The new Building Blocks codebase allowed developers to be more productive, and\n\nthey made the work safer because mistakes resulted in small, local failures instead of major catastrophes that\n\nimpacted the global system.\n\nAshman concluded, “Having developers work in the Building Blocks architecture made for impressive improvements in\n\ncode modularity, allowing them to work with more\n\nindependence and freedom. In combination with the updates to our build process, they also got faster, better feedback on\n\ntheir work, which meant better quality. ”\n\nCONCLUSION\n\nTo a large extent, the architecture that our services operate within dictates how we test and deploy our code. This was validated in\n\nPuppet Labs’ 2015 State of DevOps Report, showing that\n\narchitecture is one of the top predictors of the productivity of the engineers that work within it and of how changes can be quickly\n\nand safely made.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Because we are often stuck with architectures that were optimized\n\nfor a different set of organizational goals, or for an era long-\n\npassed, we must be able to safely migrate from one architecture to another. The case studies presented in this chapter, as well as the\n\nAmazon case study previously presented, describe techniques like the strangler pattern that can help us migrate between\n\narchitectures incrementally, enabling us to adapt to the needs of\n\nthe organization.",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "PART III CONCLUSION\n\nWithin the previous chapters of Part III, we have implemented the\n\narchitecture and technical practices that enable the fast flow of work from Dev to Ops, so that value can be quickly and safely\n\ndelivered to customers.\n\nIn Part IV: The Second Way, The Technical Practices of Feedback, we will create the architecture and mechanisms to enable the\n\nreciprocal fast flow of feedback from right to left, to find and fix\n\nproblems faster, radiate feedback, and ensure better outcomes from our work. This enables our organization to further increase\n\nthe rate at which it can adapt.\n\n† eBay’s architecture went through the following phases: Perl and files (v1, 1995), C++ and Oracle (v2, 1997), XSL\n\nand Java (v3, 2002), full-stack Java (v4, 2007), Polyglot microservices (2013+).\n\n‡ The strangler application pattern involves incrementally replacing a whole system, usually a legacy system, with a completely new one. Conversely, branching by abstraction, a term coined by Paul Hammant, is a technique where we create an abstraction layer between the areas that we are changing. This enables evolutionary design of the application architecture while allowing everybody to work off trunk/master and practice continuous integration.",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Part\n\nIntroduction\n\nIn Part III, we described the architecture and technical practices\n\nrequired to create fast flow from Development into Operations.\n\nNow in Part IV, we describe how to implement the technical\n\npractices of the Second Way, which are required to create fast and continuous feedback from Operations to Development.\n\nBy doing this, we shorten and amplify feedback loops so that we\n\ncan see problems as they occur and radiate this information to everyone in the value stream. This allows us to quickly find and fix\n\nproblems earlier in the software development life cycle, ideally\n\nlong before they cause a catastrophic failure.\n\nFurthermore, we will create a system of work where knowledge\n\nacquired downstream in Operations is integrated into the upstream work of Development and Product Management. This allows us to quickly create improvements and learnings, whether\n\nit’s from a production issue, a deployment issue, early indicators of problems, or our customer usage patterns.\n\nAdditionally, we will create a process that allows everyone to get feedback on their work, makes information visible to enable\n\nlearning, and enables us to rapidly test product hypotheses,",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "helping us determine if the features we are building are helping us\n\nachieve our organizational goals.\n\nWe will also demonstrate how to create telemetry from our build,\n\ntest, and deploy processes, as well as from user behavior,\n\nproduction issues and outages, audit issues, and security breaches.\n\nBy amplifying signals as part of our daily work, we make it\n\npossible to see and solve problems as they occur, and we grow safe\n\nsystems of work that allow us to confidently make changes and\n\nrun product experiments, knowing we can quickly detect and\n\nremediate failures. We will do all of this by exploring the\n\nfollowing:\n\nCreating telemetry to enable seeing and solving problems\n\nUsing our telemetry to better anticipate problems and achieve goals\n\nIntegrating user research and feedback into the work of product teams\n\nEnabling feedback so Dev and Ops can safely perform\n\ndeployments\n\nEnabling feedback to increase the quality of our work through\n\npeer reviews and pair programming\n\nThe patterns in this chapter help reinforce the common goals of Product Management, Development, QA, Operations, and Infosec, and encourage them to share in the responsibility of ensuring that services run smoothly in production and collaborate on the improvement of the system as a whole. Where possible, we want to link cause to effect. The more assumptions we can invalidate,",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "the faster we can discover and fix problems, but also the greater\n\nour ability to learn and innovate.\n\nThroughout the following chapters, we will implement feedback\n\nloops, enabling everyone to work together toward shared goals, to\n\nsee problems as they occur, enable quick detection and recovery,\n\nand ensure that features not only operate as designed in\n\nproduction, but also achieve organizational goals and support\n\norganizational learning.",
      "content_length": 444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "14Create Telemetry to Enable Seeing and Solving Problems\n\nA fact of life in Operations is that things go wrong—small changes\n\nmay result in many unexpected outcomes, including outages and\n\nglobal failures that impact all our customers. This is the reality of\n\noperating complex systems; no single person can see the whole system and understand how all the pieces fit together.\n\nWhen production outages and other problems occur in our daily work, we don’t often have the information we need to solve the\n\nproblem. For example, during an outage we may not be able to\n\ndetermine whether the issue is due to a failure in our application (e.g., defect in the code), in our environment (e.g., a networking\n\nproblem, server configuration problem), or something entirely external to us (e.g., a massive denial of service attack).\n\nIn Operations, we may deal with this problem with the following rule of thumb: When something goes wrong in production, we just\n\nreboot the server. If that doesn’t work, reboot the server next to it. If that doesn’t work, reboot all the servers. If that doesn’t work, blame the developers, they’re always causing outages.\n\nIn contrast, the Microsoft Operations Framework (MOF) study in 2001 found that organizations with the highest service levels",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "rebooted their servers twenty times less frequently than average\n\nand had five times fewer “blue screens of death.” In other words,\n\nthey found that the best-performing organizations were much\n\nbetter at diagnosing and fixing service incidents, in what Kevin\n\nBehr, Gene Kim, and George Spafford called a “culture of\n\ncausality” in The Visible Ops Handbook. High performers used a\n\ndisciplined approach to solving problems, using production\n\ntelemetry to understand possible contributing factors to focus\n\ntheir problem solving, as opposed to lower performers who would\n\nblindly reboot servers.\n\nTo enable this disciplined problem-solving behavior, we need to design our systems so that they are continually creating telemetry, widely defined as “an automated communications process by\n\nwhich measurements and other data are collected at remote points and are subsequently transmitted to receiving equipment for monitoring.” Our goal is to create telemetry within our applications and environments, both in our production and pre- production environments as well as in our deployment pipeline.\n\nMichael Rembetsy and Patrick McDonnell described how\n\nproduction monitoring was a critical part of Etsy’s DevOps transformation that started in 2009. This was because they were\n\nstandardizing and transitioning their entire technology stack to the LAMP stack (Linux, Apache, MySQL, and PHP), abandoning a myriad of different technologies being used in production that were increasingly difficult to support.\n\nAt the 2012 Velocity Conference, McDonnell described how much risk this created, “We were changing some of our most critical\n\ninfrastructure, which, ideally, customers would never notice. However, they’d definitely notice if we screwed something up. We",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "needed more metrics to give us confidence that we weren’t\n\nactually breaking things while we were doing these big changes,\n\nboth for our engineering teams and for team members in the non-\n\ntechnical areas, such as marketing.”\n\nMcDonnell explained further, “We started collecting all our server\n\ninformation in a tool called Ganglia, displaying all the information\n\ninto Graphite, an open source tool we invested heavily into. We\n\nstarted aggregating metrics together, everything from business\n\nmetrics to deployments. This is when we modified Graphite with\n\nwhat we called ‘our unparalleled and unmatched vertical line\n\ntechnology’ that overlaid onto every metric graph when deployments happened. By doing this, we could more quickly see any unintended deployment side effects. We even started putting\n\nTV screens all around the office so that everyone could see how our services were performing.”\n\nBy enabling developers to add telemetry to their features as part of their daily work, they created enough telemetry to help make deployments safe. By 2011, Etsy was tracking over two hundred thousand production metrics at every layer of the application stack (e.g., application features, application health, database, operating system, storage, networking, security, etc.) with the top\n\nthirty most important business metrics prominently displayed on their “deploy dashboard.” By 2014, they were tracking over eight hundred thousand metrics, showing their relentless goal of instrumenting everything and making it easy for engineers to do so.\n\nAs Ian Malpass, an engineer at Etsy, quipped, “If Engineering at\n\nEtsy has a religion, it’s the Church of Graphs. If it moves, we track it. Sometimes we’ll draw a graph of something that isn’t moving",
      "content_length": 1743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "yet, just in case it decides to make a run for it….Tracking everything is key to moving fast, but the only way to do it is to\n\nmake tracking anything easy....We enable engineers to track what they need to track, at the drop of a hat, without requiring time-\n\nsucking configuration changes or complicated processes.”\n\nOne of the findings of the 2015 State of DevOps Report was that\n\nhigh performers could resolve production incidents 168 times faster than their peers, with the median high performer having a\n\nMTTR measured in minutes, while the median low performer had an MTTR measured in days. The top two technical practices that\n\nenabled fast MTTR were the use of version control by Operations and having telemetry and proactive monitoring in the production\n\nenvironment.",
      "content_length": 775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Figure 25: Incident resolution time for high, medium, and low performers (Source: Puppet Labs, 2014 State of DevOps Report.)\n\nAs was created at Etsy, our goal in this chapter is to ensure that we always have enough telemetry so that we can confirm that our\n\nservices are correctly operating in production. And when problems do occur, make it possible to quickly determine what is going\n\nwrong and make informed decisions on how best to fix it, ideally\n\nlong before customers are impacted. Furthermore, telemetry is what enables us to assemble our best understanding of reality and\n\ndetect when our understanding of reality is incorrect.",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "CREATE OUR CENTRALIZED TELEMETRY INFRASTRUCTURE\n\nOperational monitoring and logging is by no means new—multiple\n\ngenerations of Operations engineers have used and customized monitoring frameworks (e.g., HP OpenView, IBM Tivoli, and BMC\n\nPatrol/BladeLogic) to ensure the health of production systems.\n\nData was typically collected through agents that ran on servers or through agent-less monitoring (e.g., SNMP traps or polling based\n\nmonitors). There was often a graphical user interface (GUI) front end, and back-end reporting was often augmented through tools\n\nsuch as Crystal Reports.\n\nSimilarly, the practices of developing applications with effective logging and managing the resulting telemetry are not new—a\n\nvariety of mature logging libraries exist for almost all\n\nprogramming languages.\n\nHowever, for decades we have ended up with silos of information, where Development only creates logging events that are\n\ninteresting to developers, and Operations only monitors whether the environments are up or down. As a result, when inopportune\n\nevents occur, no one can determine why the entire system is not operating as designed or which specific component is failing,\n\nimpeding our ability to bring our system back to a working state.\n\nIn order for us to see all problems as they occur, we must design\n\nand develop our applications and environments so that they\n\ngenerate sufficient telemetry, allowing us to understand how our system is behaving as a whole. When all levels of our application stack have monitoring and logging, we enable other important",
      "content_length": 1559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "capabilities, such as graphing and visualizing our metrics, anomaly detection, proactive alerting and escalation, etc.\n\nIn The Art of Monitoring, James Turnbull describes a modern monitoring architecture, which has been developed and used by Operations engineers at web-scale companies (e.g., Google,\n\nAmazon, Facebook). The architecture often consisted of open source tools, such as Nagios and Zenoss, that were customized and deployed at a scale that was difficult to accomplish with licensed\n\ncommercial software at the time. This architecture has the following components:\n\nData collection at the business logic, application, and\n\nenvironments layer: In each of these layers, we are creating telemetry in the form of events, logs, and metrics. Logs may be stored in application-specific files on each server (e.g.,\n\n/var/log/httpd-error.log), but preferably we want all our logs sent to a common service that enables easy centralization, rotation, and deletion. This is provided by most operating systems, such as syslog for Linux, the Event Log for Windows,\n\netc. Furthermore, we gather metrics at all layers of the application stack to better understand how our system is behaving. At the operating system level, we can collect metrics\n\nsuch as CPU, memory, disk, or network usage over time using tools like collectd, Ganglia, etc. Other tools that collect performance information include AppDynamics, New Relic, and Pingdom.\n\nAn event router responsible for storing our events and metrics: This capability potentially enables visualization, trending, alerting, anomaly detection, and so forth. By\n\ncollecting, storing, and aggregating all our telemetry, we better",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "enable further analysis and health checks. This is also where we store configurations related to our services (and their\n\nsupporting applications and environments) and is likely where we do threshold-based alerting and health checks.†\n\nOnce we have centralized our logs, we can transform them into\n\nmetrics by counting them in the event router—for example, a log event such as “child pid 14024 exit signal Segmentation fault” can be counted and summarized as a single segfault metric across our\n\nentire production infrastructure.\n\nBy transforming logs into metrics, we can now perform statistical operations on them, such as using anomaly detection to find\n\noutliers and variances even earlier in the problem cycle. For instance, we might configure our alerting to notify us if we went from “ten segfaults last week” to “thousands of segfaults in the last hour,” prompting us to investigate further.\n\nIn addition to collecting telemetry from our production services and environments, we must also collect telemetry from our\n\ndeployment pipeline when important events occur, such as when our automated tests pass or fail and when we perform deployments to any environment. We should also collect telemetry on how long it takes us to execute our builds and tests. By doing\n\nthis, we can detect conditions that could indicate problems, such as if the performance test or our build takes twice as long as normal, allowing us to find and fix errors before they go into\n\nproduction.",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Figure 26: Monitoring framework (Source: Turnbull, The Art of Monitoring, Kindle edition, chap. 2.)\n\nFurthermore, we should ensure that it is easy to enter and retrieve information from our telemetry infrastructure. Preferably, everything should be done through self-service APIs, as opposed\n\nto requiring people to open up tickets and wait to get reports.\n\nIdeally, we will create telemetry that tells us exactly when anything of interest happens, as well as where and how. Our\n\ntelemetry should also be suitable for manual and automated analysis and should be able to be analyzed without having the application that produced the logs on hand. As Adrian Cockcroft",
      "content_length": 664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "pointed out, “Monitoring is so important that our monitoring systems need to be more available and scalable than the systems being monitored.”\n\nFrom here on, the term telemetry will be used interchangeably with metrics, which includes all event logging and metrics created\n\nby our services at all levels of our application stack and generated from all our production and pre-production environments, as well as from our deployment pipeline.\n\nCREATE APPLICATION LOGGING TELEMETRY THAT HELPS PRODUCTION\n\nNow that we have a centralized telemetry infrastructure, we must ensure that the applications we build and operate are creating sufficient telemetry. We do this by having Dev and Ops engineers create production telemetry as part of their daily work, both for\n\nnew and existing services.\n\nScott Prugh, Chief Architect and Vice President of Development at\n\nCSG, said, “Every time NASA launches a rocket, it has millions of automated sensors reporting the status of every component of this valuable asset. And yet, we often don’t take the same care with software—we found that creating application and infrastructure\n\ntelemetry to be one of the highest return investments we’ve made. In 2014, we created over one billion telemetry events per day, with over one hundred thousand code locations instrumented.”\n\nIn the applications we create and operate, every feature should be instrumented—if it was important enough for an engineer to implement, it is certainly important enough to generate enough",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "production telemetry so that we can confirm that it is operating as designed and that the desired outcomes are being achieved.‡\n\nEvery member of our value stream will use telemetry in a variety of ways. For example, developers may temporarily create more telemetry in their application to better diagnose problems on their workstation, while Ops engineers may use telemetry to diagnose a\n\nproduction problem. In addition, Infosec and auditors may review the telemetry to confirm the effectiveness of a required control, and a product manager may use them to track business outcomes,\n\nfeature usage, or conversion rates.\n\nTo support these various usage models, we have different logging levels, some of which may also trigger alerts, such as the following:\n\nDEBUG level: Information at this level is about anything that happens in the program, most often used during debugging. Often, debug logs are disabled in production but temporarily\n\nenabled during troubleshooting.\n\nINFO level: Information at this level consists of actions that\n\nare user-driven or system specific (e.g., “beginning credit card transaction”).\n\nWARN level: Information at this level tells us of conditions\n\nthat could potentially become an error (e.g., a database call taking longer than some predefined time). These will likely initiate an alert and troubleshooting, while other logging messages may help us better understand what led to this\n\ncondition.\n\nERROR level: Information at this level focuses on error conditions (e.g., API call failures, internal error conditions).",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "FATAL level: Information at this level tells us when we must terminate (e.g., a network daemon can’t bind a network socket).\n\nChoosing the right logging level is important. Dan North, a former ThoughtWorks consultant who was involved in several projects in which the core continuous delivery concepts took shape, observes,\n\n“When deciding whether a message should be ERROR or WARN, imagine being woken up at 4 a.m. Low printer toner is not an ERROR.”\n\nTo help ensure that we have information relevant to the reliable and secure operations of our service, we should ensure that all potentially significant application events generate logging entries,\n\nincluding those provided on this list assembled by Anton A. Chuvakin, a research VP at Gartner’s GTP Security and Risk Management group:\n\nAuthentication/authorization decisions (including logoff)\n\nSystem and data access\n\nSystem and application changes (especially privileged changes)\n\nData changes, such as adding, editing, or deleting data\n\nInvalid input (possible malicious injection, threats, etc.)\n\nResources (RAM, disk, CPU, bandwidth, or any other resource\n\nthat has hard or soft limits)\n\nHealth and availability\n\nStartups and shutdowns",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Faults and errors\n\nCircuit breaker trips\n\nDelays\n\nBackup success/failure\n\nTo make it easier to interpret and give meaning to all these log\n\nentries, we should (ideally) create logging hierarchical categories,\n\nsuch as for non-functional attributes (e.g., performance, security) and for attributes related to features (e.g., search, ranking).\n\nUSE TELEMETRY TO GUIDE PROBLEM SOLVING\n\nAs described in the beginning of this chapter, high performers use\n\na disciplined approach to solving problems. This is in contrast to the more common practice of using rumor and hearsay, which can\n\nlead to the unfortunate metric of mean time until declared\n\ninnocent—how quickly can we convince everyone else that we didn’t cause the outage.\n\nWhen there is a culture of blame around outages and problems,\n\ngroups may avoid documenting changes and displaying telemetry where everyone can see them to avoid being blamed for outages.\n\nOther negative outcomes due to lack of public telemetry include a\n\nhighly charged political atmosphere, the need to deflect accusations, and, worse, the inability to create institutional\n\nknowledge around how the incidents occurred and the learnings",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "needed to prevent these errors from happening again in the future.§\n\nIn contrast, telemetry enables us to use the scientific method to formulate hypotheses about what is causing a particular problem\n\nand what is required to solve it. Examples of questions we can\n\nanswer during problem resolution include:\n\nWhat evidence do we have from our monitoring that a problem\n\nis actually occurring?\n\nWhat are the relevant events and changes in our applications and environments that could have contributed to the problem?\n\nWhat hypotheses can we formulate to confirm the link between\n\nthe proposed causes and effects?\n\nHow can we prove which of these hypotheses are correct and successfully effect a fix?\n\nThe value of fact-based problem solving lies not only in\n\nsignificantly faster MTTR (and better customer outcomes), but also in its reinforcement of the perception of a win/win\n\nrelationship between Development and Operations.",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "ENABLE CREATION OF PRODUCTION METRICS AS PART OF DAILY WORK\n\nTo enable everyone to be able to find and fix problems in their\n\ndaily work, we need to enable everyone to create metrics in their\n\ndaily work that can be easily created, displayed, and analyzed. To do this, we must create the infrastructure and libraries necessary\n\nto make it as easy as possible for anyone in Development or\n\nOperations to create telemetry for any functionality they build. In the ideal, it should be as easy as writing one line of code to create a\n\nnew metric that shows up in a common dashboard where everyone\n\nin the value stream can see it.\n\nThis was the philosophy that guided the development of one of the\n\nmost widely used metrics libraries, called StatsD, which was\n\ncreated and open-sourced at Etsy. As John Allspaw described, “We designed StatsD to prevent any developer from saying, ‘It’s too\n\nmuch of a hassle to instrument my code.’ Now they can do it with one line of code. It was important to us that for a developer,\n\nadding production telemetry didn’t feel as difficult as doing a\n\ndatabase schema change.”\n\nStatsD can generate timers and counters with one line of code (in Ruby, Perl, Python, Java, and other languages) and is often used\n\nin conjunction with Graphite or Grafana, which renders metric events into graphs and dashboards.",
      "content_length": 1333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Figure 27: One line of code to generate telemetry using StatsD and Graphite at Etsy (Source: Ian Malpass, “Measure Anything, Measure Everything.”)\n\nFigure 27 above shows an example of how a single line of code\n\ncreates a user login event (in this case, one line of PHP code: “StatsD::increment(“login.successes”)). The resulting graph shows\n\nthe number of successful and failed logins per minute, and\n\noverlaid on the graph are vertical lines that represent a production deployment.\n\nWhen we generate graphs of our telemetry, we will also overlay\n\nonto them when production changes occur, because we know that the significant majority of production issues are caused by\n\nproduction changes, which include code deployments. This is part\n\nof what allows us to have a high rate of change, while still preserving a safe system of work.\n\nAlternative libraries to StatsD that allow developers to generate\n\nproduction telemetry can be easily aggregated and analyzed include JMX and codahale metrics. Other tools that create metrics\n\ninvaluable for problem solving include New Relic, AppDynamics,",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "and Dynatrace. Tools such as munin and collectd can be used to create similar functionality.¶\n\nBy generating production telemetry as part of our daily work, we create an ever-improving capability to not only see problems as\n\nthey occur, but also to design our work so that problems in design\n\nand operations can be revealed, allowing an increasing number of metrics to be tracked, as we saw in the Etsy case study.\n\nCREATE SELF-SERVICE ACCESS TO TELEMETRY AND INFORMATION RADIATORS\n\nIn the previous steps, we enabled Development and Operations to\n\ncreate and improve production telemetry as part of their daily work. In this step, our goal is to radiate this information to the rest\n\nof the organization, ensuring that anyone who wants information\n\nabout any of the services we are running can get it without needing production system access or privileged accounts, or\n\nhaving to open up a ticket and wait for days for someone to\n\nconfigure the graph for them.\n\nBy making telemetry fast, easy to get, and sufficiently centralized,\n\neveryone in the value stream can share a common view of reality.\n\nTypically, this means that production metrics will be radiated on web pages generated by a centralized server, such as Graphite or\n\nany of the other technologies described in the previous section.\n\nWe want our production telemetry to be highly visible, which means putting it in central areas where Development and\n\nOperations work, thus allowing everyone who is interested to see",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "how our services are performing. At a minimum, this includes\n\neveryone in our value stream, such as Development, Operations, Product Management, and Infosec.\n\nThis is often referred to as an information radiator, defined by\n\nthe Agile Alliance as “the generic term for any of a number of handwritten, drawn, printed, or electronic displays which a team\n\nplaces in a highly visible location, so that all team members as well\n\nas passers-by can see the latest information at a glance: count of automated tests, velocity, incident reports, continuous integration\n\nstatus, and so on. This idea originated as part of the Toyota\n\nProduction System.”\n\nBy putting information radiators in highly visible places, we\n\npromote responsibility among team members, actively\n\ndemonstrating the following values:\n\nThe team has nothing to hide from its visitors (customers,\n\nstakeholders, etc.)\n\nThe team has nothing to hide from itself: it acknowledges and confronts problems\n\nNow that we possess the infrastructure to create and radiate\n\nproduction telemetry to the entire organization, we may also\n\nchoose to broadcast this information to our internal customers and even to our external customers. For example, we might do this\n\nby creating publicly-viewable service status pages so that customers can learn how the services they depend upon are\n\nperforming.\n\nAlthough there may be some resistance to providing this amount of transparency, Ernest Mueller describes the value of doing so:",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "One of the first actions I take when starting in an organization\n\nis to use information radiators to communicate issues and detail the changes we are making—this is usually extremely\n\nwell-received by our business units, who were often left in the\n\ndark before. And for Development and Operations groups who must work together to deliver a service to others, we need that\n\nconstant communication, information, and feedback.\n\nWe may even extend this transparency further—instead of trying to keep customer-impacting problems a secret, we can broadcast\n\nthis information to our external customers. This demonstrates\n\nthat we value transparency, thereby helping to build and earn customers’ trust.** See Appendix 10.\n\nCase Study Creating Self-Service Metrics at LinkedIn (2011)\n\nAs described in Part III, LinkedIn was created in 2003 to help users connect “to your network for better job\n\nopportunities.” By November 2015, LinkedIn had over 350\n\nmillion members generating tens of thousands of requests per second, resulting in millions of queries per second on\n\nthe LinkedIn back-end systems.\n\nPrachi Gupta, Director of Engineering at LinkedIn, wrote in 2011 about the importance of production telemetry: “At\n\nLinkedIn, we emphasize making sure the site is up and our members have access to complete site functionality at all\n\ntimes. Fulfilling this commitment requires that we detect and\n\nrespond to failures and bottlenecks as they start happening.",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "That’s why we use these time-series graphs for site\n\nmonitoring to detect and react to incidents within minutes...This monitoring technique has proven to be a\n\ngreat tool for engineers. It lets us move fast and buys us\n\ntime to detect, triage, and fix problems.”\n\nHowever, in 2010, even though there was an incredibly large\n\nvolume of telemetry being generated, it was extremely\n\ndifficult for engineers to get access to the data, let alone analyze it. Thus began Eric Wong’s summer intern project at\n\nLinkedIn, which turned into the production telemetry initiative\n\nthat created InGraphs.\n\nWong wrote, “To get something as simple as CPU usage of\n\nall the hosts running a particular service, you would need to\n\nfile a ticket and someone would spend 30 minutes putting it [a report] together.”\n\nAt the time, LinkedIn was using Zenoss to collect metrics,\n\nbut as Wong explains, “Getting data from Zenoss required digging through a slow web interface, so I wrote some\n\npython scripts to help streamline the process. While there\n\nwas still manual intervention in setting up metric collection, I was able to cut down the time spent navigating Zenoss’\n\ninterface.”\n\nOver the course of the summer, he continued to add functionality to InGraphs so that engineers could see exactly\n\nwhat they wanted to see, adding the ability to make\n\ncalculations across multiple datasets, view week-over-week trending to compare historical performance, and even define",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "custom dashboards to pick exactly which metrics would be\n\ndisplayed on a single page.\n\nIn writing about the outcomes of adding functionality to\n\nInGraphs and the value of this capability, Gupta notes, “The\n\neffectiveness of our monitoring system was highlighted in an instant where our InGraphs monitoring functionality tied to a\n\nmajor web-mail provider started trending downwards and the\n\nprovider realized they had a problem in their system only after we reached out to them!”\n\nWhat started off as a summer internship project is now one\n\nof the most visible parts of LinkedIn operations. InGraphs has been so successful that the real-time graphs are\n\nfeatured prominently in the company’s engineering offices\n\nwhere visitors can’t fail to see them.\n\nFIND AND FILL ANY TELEMETRY GAPS\n\nWe have now created the infrastructure necessary to quickly\n\ncreate production telemetry throughout our entire application stack and radiate it throughout our organization.\n\nIn this step, we will identify any gaps in our telemetry that impede\n\nour ability to quickly detect and resolve incidents—this is especially relevant if Dev and Ops currently have little (or no)\n\ntelemetry. We will use this data later to better anticipate problems,\n\nas well as to enable everyone to gather the information they need to make better decisions to achieve organizational goals.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Achieving this requires that we create enough telemetry at all\n\nlevels of the application stack for all our environments, as well as for the deployment pipelines that support them. We need metrics\n\nfrom the following levels:\n\nBusiness level: Examples include the number of sales transactions, revenue of sales transactions, user signups, churn\n\nrate, A/B testing results, etc.\n\nApplication level: Examples include transaction times, user response times, application faults, etc.\n\nInfrastructure level (e.g., database, operating system,\n\nnetworking, storage): Examples include web server traffic, CPU load, disk usage, etc.\n\nClient software level (e.g., JavaScript on the client\n\nbrowser, mobile application): Examples include application errors and crashes, user measured transaction\n\ntimes, etc.\n\nDeployment pipeline level: Examples include build pipeline status (e.g., red or green for our various automated\n\ntest suites), change deployment lead times, deployment\n\nfrequencies, test environment promotions, and environment status.\n\nBy having telemetry coverage in all of these areas, we will be able\n\nto see the health of everything that our service relies upon, using data and facts instead of rumors, finger-pointing, blame, and so\n\nforth.",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Further, we better enable detection of security-relevant events by\n\nmonitoring any application and infrastructure faults (e.g., abnormal program terminations, application errors and\n\nexceptions, and server and storage errors). Not only does this\n\ntelemetry better inform Development and Operations when our services are crashing, but these errors are often indicators that a\n\nsecurity vulnerability is being actively exploited.\n\nBy detecting and correcting problems earlier, we can fix them while they are small and easy to fix, with fewer customers\n\nimpacted. Furthermore, after every production incident, we should identify any missing telemetry that could have enabled\n\nfaster detection and recovery; or, better yet, we can identify these\n\ngaps during feature development in our peer review process.\n\nAPPLICATION AND BUSINESS METRICS\n\nAt the application level, our goal is to ensure that we are generating telemetry not only around application health (e.g.,\n\nmemory usage, transaction counts, etc.), but also to measure to\n\nwhat extent we are achieving our organizational goals (e.g., number of new users, user login events, user session lengths,\n\npercent of users active, how often certain features are being used,\n\nand so forth).\n\nFor example, if we have a service that is supporting e-commerce,\n\nwe want to ensure that we have telemetry around all of the user\n\nevents that lead up to a successful transaction that generates revenue. We can then instrument all the user actions that are\n\nrequired for our desired customer outcomes.",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "These metrics will vary according to different domains and\n\norganizational goals. For instance, for e-commerce sites, we may want to maximize the time spent on the site; however, for search\n\nengines, we may want to reduce the time spent on the site, since long sessions may indicate that users are having difficulty finding\n\nwhat they’re looking for.\n\nIn general, business metrics will be part of a customer acquisition\n\nfunnel, which is the theoretical steps a potential customer will take to make a purchase. For instance, in an e-commerce site, the\n\nmeasurable journey events include total time on site, product link clicks, shopping cart adds, and completed orders.\n\nEd Blankenship, Senior Product Manager for Microsoft Visual\n\nStudio Team Services, describes, “Often, feature teams will define their goals in an acquisition funnel, with the goal of their feature\n\nbeing used in every customer’s daily work. Sometimes they’re\n\ninformally described as ‘tire kickers,’ ‘active users,’ ‘engaged\n\nusers,’ and ‘deeply engaged users,’ with telemetry supporting each\n\nstage.”\n\nOur goal is to have every business metric be actionable—these top\n\nmetrics should help inform how to change our product and be\n\namenable to experimentation and A/B testing. When metrics aren’t actionable, they are likely vanity metrics that provide little\n\nuseful information—these we want to store, but likely not display,\n\nlet alone alert on.\n\nIdeally, anyone viewing our information radiators will be able to\n\nmake sense of the information we are showing in the context of\n\ndesired organizational outcomes, such as goals around revenue,\n\nuser attainment, conversion rates, etc. We should define and link",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "each metric to a business outcome metric at the earliest stages of\n\nfeature definition and development, and measure the outcomes\n\nafter we deploy them in production. Furthermore, doing this helps product owners describe the business context of each feature for\n\neveryone in the value stream.\n\nFigure 28: Amount of user excitement of new features in user forum posts after deployments (Source: Mike Brittain, “Tracking Every Release,” CodeasCraft.com, December 8, 2010, https://codeascraft.com/2010/12/08/track-every-release/.)\n\nFurther business context can be created by being aware of and\n\nvisually displaying time periods relevant to high-level business\n\nplanning and operations, such as high transaction periods associated with peak holiday selling seasons, end-of-quarter\n\nfinancial close periods, or scheduled compliance audits. This\n\ninformation may be used as a reminder to avoid scheduling risky\n\nchanges when availability is critical or avoid certain activities\n\nwhen audits are in progress.\n\nBy radiating how customers interact with what we build in the\n\ncontext of our goals, we enable fast feedback to feature teams so",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "they can see whether the capabilities we are building are actually\n\nbeing used and to what extent they are achieving business goals.\n\nAs a result, we reinforce the cultural expectations that\n\ninstrumenting and analyzing customer usage is also a part of our\n\ndaily work, so we better understand how our work contributes to\n\nour organizational goals.\n\nINFRASTRUCTURE METRICS\n\nJust as we did for application metrics, our goal for production and non-production infrastructure is to ensure that we are generating\n\nenough telemetry so that if a problem occurs in any environment,\n\nwe can quickly determine whether infrastructure is a contributing\n\ncause of the problem. Furthermore, we must be able to pinpoint\n\nexactly what in the infrastructure is contributing to the problem (e.g., database, operating system, storage, networking, etc.).\n\nWe want to make as much infrastructure telemetry visible as\n\npossible, across all the technology stakeholders, ideally organized\n\nby service or application. In other words, when something goes wrong with something in our environment, we need to know\n\nexactly what applications and services could be or are being affected.††\n\nIn decades past, creating links between a service and the\n\nproduction infrastructure it depended on was often a manual\n\neffort (such as ITIL CMDBs or creating configuration definitions\n\ninside alerting tools in tools such as Nagios). However,\n\nincreasingly these links are now registered automatically within our services, which are then dynamically discovered and used in\n\nproduction through tools such as Zookeeper, Etcd, Consul, etc.",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "These tools enable services to register themselves, storing\n\ninformation that other services need to interact with it (e.g., IP\n\naddress, port numbers, URIs). This solves the manual nature of the ITIL CMDB and is absolutely necessary when services are\n\nmade up of hundreds (or thousands or even millions) of nodes, each with dynamically assigned IP addresses.‡‡\n\nRegardless of how simple or complex our services are, graphing\n\nour business metrics alongside our application and infrastructure\n\nmetrics allow us to detect when things go wrong. For instance, we\n\nmay see that new customer signups drop to 20% of daily norms,\n\nand then immediately also see that all our database queries are taking five times longer than normal, enabling us to focus our\n\nproblem solving.\n\nFurthermore, business metrics create context for our\n\ninfrastructure metrics, enabling Development and Operations to better work together toward common goals. As Jody Mulkey, CTO\n\nof Ticketmaster/LiveNation, observes, “Instead of measuring\n\nOperations against the amount of downtime, I find it’s much\n\nbetter to measure both Dev and Ops against the real business\n\nconsequences of downtime: how much revenue should we have attained, but didn’t.”§§\n\nNote that in addition to monitoring our production services, we\n\nalso need telemetry for those services in our pre-production environments (e.g., development, test, staging, etc.). Doing this\n\nenables us to find and fix issues before they go into production,\n\nsuch as detecting when we have ever-increasing database insert\n\ntimes due to a missing table index.",
      "content_length": 1577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "OVERLAYING OTHER RELEVANT INFORMATION ONTO OUR METRICS\n\nEven after we have created our deployment pipeline that allows us\n\nto make small and frequent production changes, changes still inherently create risk. Operational side effects are not just\n\noutages, but also significant disruptions and deviations from\n\nstandard operations.\n\nTo make changes visible, we make work visible by overlaying all\n\nproduction deployment activities on our graphs. For instance, for\n\na service that handles a large number of inbound transactions,\n\nproduction changes can result in a significant settling period,\n\nwhere performance degrades substantially as all cache lookups\n\nmiss.\n\nTo better understand and preserve quality of service, we want to\n\nunderstand how quickly performance returns to normal, and if\n\nnecessary, take steps to improve performance.\n\nSimilarly, we want to overlay other useful operational activities,\n\nsuch as when the service is under maintenance or being backed\n\nup, in places where we may want to display or suppress alerts.\n\nCONCLUSION\n\nThe improvements enabled by production telemetry from Etsy\n\nand LinkedIn show us how critical it is to see problems as they occur, so we can search out the cause and quickly remedy the\n\nsituation. By having all elements of our service emitting telemetry\n\nthat can be analyzed, whether it is in our application, database, or\n\nin our environment, and making that telemetry widely available,",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "we can find and fix problems long before they cause something\n\ncatastrophic, ideally long before a customer even notices that\n\nsomething is wrong. The result is not only happier customers, but, by reducing the amount of firefighting and crises when things go\n\nwrong, we have a happier and more productive workplace with\n\nless stress and lower levels of burnouts.\n\n† Example tools include Sensu, Nagios, Zappix, LogsStash, Splunk, Sumo Logic, Datadog, and Riemann.\n\n‡ A variety of application logging libraries exist that make it easy for developers to create useful telemetry, and we should choose one that allows us to send all our application logs to the centralized logging infrastructure that we created in the previous section. Popular examples include rrd4j and log4j for Java, and log4r and ruby-cabin for Ruby.\n\n§ In 2004, Gene Kim, Kevin Behr and George Spafford described this as a symptom of lacking a “culture of\n\ncausality,” noting that high-performing organizations recognize that 80% of all outages are caused by change and 80% of MTTR is spent trying to determine what changed.\n\n¶ A whole other set of tools to aid in monitoring, aggregation, and collection include Splunk, Zabbix, Sumo Logic, DataDog, as well as Nagios, Cacti, Sensu, RRDTool, Netflix Atlas, Riemann, and others. Analysts often call this broad category of tools “application performance monitors.”\n\n** Creating a simple dashboard should be part of creating any new product or service—automated tests should\n\nconfirm that both the service and dashboard are working correctly, helping both our customers and our ability to safely deploy code.\n\n†† Exactly as an ITIL Configuration Management Database (CMDB) would prescribe.\n\n‡‡ Consul may be of specific interest, as it creates an abstraction layer that easily enables service mapping, monitoring, locks, and key-value configuration stores, as well as host clustering and failure detection.\n\n§§ This could be the cost of production downtime or the costs associated with a late feature. In product\n\ndevelopment terms, the second metric is known as cost of delay, and is key to making effective prioritization decisions.",
      "content_length": 2150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "15Analyze Telemetry to Better Anticipate Problems and Achieve Goals\n\nAs we saw in the previous chapter, we need sufficient production\n\ntelemetry in our applications and infrastructure to see and solve problems as they occur. In this chapter, we will create tools that\n\nallow us to discover variances and ever-weaker failure signals hidden in our production telemetry so we can avert catastrophic\n\nfailures. Numerous statistical techniques will be presented, along\n\nwith case studies demonstrating their use.\n\nA great example of analyzing telemetry to proactively find and fix\n\nproblems before customers are impacted can be seen at Netflix, a\n\nglobal provider of streaming films and television series. Netflix had revenue of $6.2 billion from seventy-five million subscribers\n\nin 2015. One of their goals is to provide the best experience to those watching videos online around the world, which requires a\n\nrobust, scalable, and resilient delivery infrastructure. Roy Rapoport describes one of the challenges of managing the Netflix cloud-based video delivery service: “Given a herd of cattle that should all look and act the same, which cattle look different from\n\nthe rest? Or more concretely, if we have a thousand-node stateless compute cluster, all running the same software and subject to the",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "same approximate traffic load, our challenge is to find any nodes\n\nthat don’t look like the rest of the nodes.”\n\nOne of the statistical techniques that the team used at Netflix in\n\n2012 was outlier detection, defined by Victoria J. Hodge and Jim\n\nAustin of the University of York as detecting “abnormal running\n\nconditions from which significant performance degradation may\n\nwell result, such as an aircraft engine rotation defect or a flow\n\nproblem in a pipeline.”\n\nRapoport explains that Netflix “used outlier detection in a very\n\nsimple way, which was to first compute what was the ‘current\n\nnormal’ right now, given population of nodes in a compute cluster.\n\nAnd then we identified which nodes didn’t fit that pattern, and removed those nodes from production.”\n\nRapoport continues, “We can automatically flag misbehaving\n\nnodes without having to actually define what the ‘proper’ behavior is in any way. And since we’re engineered to run resiliently in the cloud, we don’t tell anyone in Operations to do something— instead, we just kill the sick or misbehaving compute node, and then log it or notify the engineers in whatever form they want.”\n\nBy implementing the Server Outlier Detection process, Rapoport states, Netflix has “massively reduced the effort of finding sick servers, and, more importantly, massively reduced the time\n\nrequire Rapoport states d to fix them, resulting in improved service quality. The benefit of using these techniques to preserve employee sanity, work/life balance, and service quality cannot be overstated.” The work done at Netflix highlights one very specific way we can use telemetry to mitigate problems before they impact our customer.",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Throughout this chapter we will explore many statistical and\n\nvisualization techniques (including outlier detection) that we can\n\nuse to analyze our telemetry to better anticipate problems. This\n\nenables us to solve problems faster, cheaper, and earlier than ever,\n\nbefore our customer or anyone in our organization is impacted;\n\nfurthermore, we will also create more context for our data to help\n\nus make better decisions and achieve our organizational goals.\n\nUSE MEANS AND STANDARD DEVIATIONS TO DETECT POTENTIAL PROBLEMS\n\nOne of the simplest statistical techniques that we can use to analyze a production metric is computing its mean (or average) and standard deviations. By doing this, we can create a filter that detects when this metric is significantly different from its norm, and even configure our alerting so that we can take corrective\n\naction (e.g., notify on-call production staff at 2 a.m. to investigate when database queries are significantly slower than average).\n\nWhen critical production services have problems, waking people at 2 a.m. may be the right thing to do. However, when we create alerts that are not actionable or are false-positives, we’ve\n\nunnecessarily woken up people in the middle of the night. As John Vincent, an early leader in the DevOps movement, observed, “Alert fatigue is the single biggest problem we have right now…We need to be more intelligent about our alerts or we’ll all go insane.”\n\nWe create better alerts by increasing the signal-to-noise ratio,\n\nfocusing on the variances or outliers that matter. Suppose we are analyzing the number of unauthorized login attempts per day. Our\n\ncollected data has a Gaussian distribution (i.e., normal or bell",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "curve distribution) that matches the graph in the figure 29. The vertical line in the middle of the bell curve is the mean, and the\n\nfirst, second, and third standard deviations indicated by the other vertical lines contain 68%, 95%, and 99.7% of the data,\n\nrespectively.\n\nFigure 29: Standard deviations (σ) & mean (µ) with Gaussian distribution (Source: Wikipedia’s “Normal Distribution” entry, https://en.wikipedia.org/wiki/Normal_distribution.)\n\nA common use of standard deviations is to periodically inspect the\n\ndata set for a metric and alert if it has significantly varied from the mean. For instance, we may set an alert for when the number of\n\nunauthorized login attempts per day is three standard deviations greater than the mean. Provided that this data set has Gaussian\n\ndistribution, we would expect that only 0.3% of the data points\n\nwould trigger the alert.\n\nEven this simple type of statistical analysis is valuable, because no one had to define a static threshold value, something which is\n\ninfeasible if we are tracking thousands or hundreds of thousands of production metrics.",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "For the remainder of this book, we will use the terms telemetry, metric, and data sets interchangeably—in other words, a metric\n\n(e.g., “page load times”) will map to a data set (e.g., 2 ms, 8 ms, 11 ms, etc.), the term used by statisticians to describe a matrix of\n\ndata points where each column represents a variable of which statistical operations are performed.\n\nINSTRUMENT AND ALERT ON UNDESIRED OUTCOMES\n\nTom Limoncelli, co-author of The Practice of Cloud System\n\nAdministration: Designing and Operating Large Distributed\n\nSystems and a former Site Reliability Engineer at Google, relates the following story on monitoring: “When people ask me for\n\nrecommendations on what to monitor, I joke that in an ideal world, we would delete all the alerts we currently have in our\n\nmonitoring system. Then, after each user-visible outage, we’d ask\n\nwhat indicators would have predicted that outage and then add those to our monitoring system, alerting as needed. Repeat. Now\n\nwe only have alerts that prevent outages, as opposed to being bombarded by alerts after an outage already occurred.”\n\nIn this step, we will replicate the outcomes of such an exercise.\n\nOne of the easiest ways to do this is to analyze our most severe incidents in the recent past (e.g., 30 days) and create a list of\n\ntelemetry that could have enabled earlier and faster detection and\n\ndiagnosis of the problem, as well as easier and faster confirmation that an effective fix had been implemented.\n\nFor instance, if we had an issue where our NGINX web server\n\nstopped responding to requests, we would look at the leading",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "indicators that could have warned us earlier that we were starting\n\nto deviate from standard operations, such as:\n\nApplication level: increasing web page load times, etc.\n\nOS level: server free memory running low, disk space running\n\nlow, etc.\n\nDatabase level: database transaction times taking longer\n\nthan normal, etc.\n\nNetwork level: number of functioning servers behind the\n\nload balancer dropping, etc.\n\nEach of these metrics is a potential precursor to a production incident. For each, we would configure our alerting systems to\n\nnotify them when they deviate sufficiently from the mean, so that we can take corrective action.\n\nBy repeating this process on ever-weaker failure signals, we find\n\nproblems ever earlier in the life cycle, resulting in fewer customer impacting incidents and near misses. In other words, we are\n\npreventing problems as well as enabling quicker detection and\n\ncorrection.\n\nPROBLEMS THAT ARISE WHEN OUR TELEMETRY DATA HAS NON-GAUSSIAN DISTRIBUTION\n\nUsing means and standard deviations to detect variance can be extremely useful. However, using these techniques on many of the\n\ntelemetry data sets that we use in Operations will not generate the",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "desired results. As Dr. Toufic Boubez observes, “Not only will we get wakeup calls at 2 a.m., we’ll get them at 2:37 a.m., 4:13 a.m.,\n\n5:17 a.m. This happens when the underlying data that we’re monitoring doesn’t have a Gaussian distribution.”\n\nIn other words, when the distribution of the data set does not have\n\nthe Gaussian bell curve described earlier, the properties associated with standard deviations do not apply. For example, consider the scenario in which we are monitoring the number of file downloads\n\nper minute from our website. We want to detect periods when we have unusually high numbers of downloads, such as when our download rate is greater than three standard deviations from our\n\naverage, so that we can proactively add more capacity.\n\nFigure 30 shows our number of simultaneous downloads per minute over time, with a bar overlaid on top. When the bar is\n\nblack, the number of downloads within a given period (sometimes called a “sliding window”) is at least three standard deviations from the average. Otherwise, it is gray.",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Figure 30: Downloads per minute: over-alerting when using “3 standard deviation” rule (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nThe obvious problem that the graph shows is that we are alerting almost all of the time. This is because in almost any given period of time, we have instances when the download count exceeds our\n\nthree standard deviation threshold.\n\nTo confirm this, when we create a histogram (see figure 31) that shows the frequency of downloads per minute, we can see that it\n\ndoes not have the classic, symmetrical bell curve shape. Instead, it is obvious that the distribution is skewed toward the lower end, showing that the majority of the time we have very few downloads\n\nper minute but that download counts frequently spike three standard deviations higher.",
      "content_length": 802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Figure 31: Downloads per minute: histogram of data showing non- Gaussian distribution (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nMany production data sets are non-Gaussian distribution. Dr. Nicole Forsgren explains, “In Operations, many of our data sets have what we call ‘chi squared’ distribution. Using standard\n\ndeviations for this data not only results in over- or under-alerting, but it also results in nonsensical results.” She continues, “When you compute the number of simultaneous downloads that are three standard deviations below the mean, you end up with a\n\nnegative number, which obviously doesn’t make sense.”\n\nOver-alerting causes Operations engineers to be woken up in the\n\nmiddle of the night for protracted periods of time, even when there are few actions that they can appropriately take. The problem associated with under-alerting is just as significant. For instance, suppose we are monitoring the number of completed\n\ntransactions, and the completed transaction count drops by 50% in the middle of the day due to a software component failure. If this is still within three standard deviations of the mean, no alert\n\nwill be generated, meaning that our customers will discover the",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "problem before we do, at which point the problem may be much more difficult to solve.\n\nFortunately, there are techniques we can use to detect anomalies in even non-Gaussian data sets, which are described next.\n\nCase Study Auto-Scaling Capacity at Netflix (2012)\n\nAnother tool developed at Netflix to increase service quality, Scryer, addresses some of the shortcomings of Amazon Auto Scaling (AAS), which dynamically increases and\n\ndecreases AWS compute server counts based on workload data. Scryer works by predicting what customer demands will be based on historical usage patterns and provisions the\n\nnecessary capacity.\n\nScryer addressed three problems with AAS. The first was dealing with rapid spikes in demand. Because AWS instance\n\nstartup times can be ten to forty-five minutes, additional compute capacity was often delivered too late to deal with spikes in demand. The second problem was that after outages, the rapid decrease in customer demand led to AAS\n\nremoving too much compute capacity to handle future incoming demand. The third problem was that AAS didn’t factor in known usage traffic patterns when scheduling\n\ncompute capacity.",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Figure 32: Netflix customer viewing demand for five days (Source: Daniel Jacobson, Danny Yuan, and Neeraj Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine,” The Netflix Tech Blog, November 5, 2013, http://techblog.netflix.com/2013/11/scryer-netflixs-predictive- auto-scaling.html.)\n\nNetflix took advantage of the fact that their consumer viewing patterns were surprisingly consistent and predictable, despite not having Gaussian distributions.\n\nBelow is a chart reflecting customer requests per second throughout the work week, showing regular and consistent customer viewing patterns Monday through Friday.\n\nScryer uses a combination of outlier detections to throw out spurious data points and then uses techniques such as Fast Fourier Transform (FFT) and linear regression to smooth the data while preserving legitimate traffic spikes that recur in\n\ntheir data. The result is that Netflix can forecast traffic demand with surprising accuracy.",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Figure 33: Netflix Scryer forecasting customer traffic and the resulting AWS schedule of compute resources (Source: Jacobson, Yuan, Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine.”)\n\nOnly months after first using Scryer in production, Netflix significantly improved their customer viewing experience, improved service availability, and reduced Amazon EC2\n\ncosts.\n\nUSING ANOMALY DETECTION TECHNIQUES\n\nWhen our data does not have Gaussian distribution, we can still find noteworthy variances using a variety of methods. These\n\ntechniques are broadly categorized as anomaly detection, often defined as “the search for items or events which do not conform to an expected pattern.” Some of these capabilities can be found\n\ninside our monitoring tools, while others may require help from people with statistical skills.\n\nTarun Reddy, VP of Development and Operations at Rally\n\nSoftware, actively advocates this active collaboration between Operations and statistics, observing, “To better enable service\n\nquality, we put all our production metrics into Tableau, a\n\nstatistical analysis software package. We even have an Ops",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "engineer trained in statistics who writes R code (another statistical\n\npackage)—this engineer has her own backlog, filled with requests\n\nfrom other teams inside the company who want to find variance ever earlier, before it causes an even larger variance that could\n\naffect our customers.”\n\nOne of the statistical techniques we can use is called smoothing, which is especially suitable if our data is a time series, meaning\n\neach data point has a time stamp (e.g., download events,\n\ncompleted transaction events, etc.). Smoothing often involves using moving averages (or rolling averages), which transform our\n\ndata by averaging each point with all the other data within our\n\nsliding window. This has the effect of smoothing out short-term fluctuations and highlighting longer-term trends or cycles.†\n\nAn example of this smoothing effect is shown in the figure 34. The\n\nblack line represents the raw data, while the blue line indicates the thirty day moving average (i.e., the average of the trailing thirty days).‡\n\nFigure 34: Autodesk share price and thirty day moving average filter (Source: Jacobson, Yuan, Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine.”)",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "More exotic filtering techniques exist, such as Fast Fourier\n\nTransforms, which has been widely used in image processing, and\n\nthe Kolmogorov-Smirnov test (found in Graphite and Grafana), which is often used to find similarities or differences in\n\nperiodic/seasonal metric data.\n\nWe can expect that a large percentage of telemetry concerning user data will have periodic/seasonal similarities—web traffic,\n\nretail transactions, movie watching, and many other user\n\nbehaviors have very regular and surprisingly predictable daily, weekly, and yearly patterns. This enables us to be able to detect\n\nsituations that vary from historical norms, such as when our order transaction rate on a Tuesday afternoon drops to 50% of our\n\nweekly norms.\n\nBecause of the usefulness of these techniques in forecasting, we\n\nmay be able to find people in the Marketing or Business Intelligence departments with the knowledge and skills necessary\n\nto analyze this data. We may want to seek these people out and explore working together to identify shared problems and use\n\nimproved anomaly detection and incident prediction to solve them.§\n\nCase Study Advanced Anomaly Detection (2014)\n\nAt Monitorama in 2014, Dr. Toufic Boubez described the power of using anomaly detection techniques, specifically\n\nhighlighting the effectiveness of the Komogorov-Smirnov\n\ntest, a technique that is often used in statistics to determine whether two data sets differ significantly and is found in the",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "popular Graphite and Grafana tool. The purpose of\n\npresenting this case study here is not as a tutorial, but to\n\ndemonstrate how a class of statistical techniques can be used in our work, as well as how it’s likely being used in our\n\norganizations in completely different applications.\n\nFigure 35 shows the number of transactions per minute at an e-commerce site. Note the weekly periodicity of the\n\ngraph, with transaction volume dropping on the weekends.\n\nBy visual inspection, we can see that something peculiar seems to happen on the fourth week when normal\n\ntransaction volume doesn’t return to normal levels on Monday. This suggests an event we should investigate.\n\nFigure 35: Transaction volume: under-alerting using “3 standard deviation” rule (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nUsing the three standard deviation rule would only alert us twice, missing the critical Monday dropoff in transaction",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "volume. Ideally, we would also want to be alerted that the\n\ndata has drifted from our expected Monday pattern.\n\n“Even saying ‘Kolmogorov-Smirnov’ is a great way to impress everyone,” Dr. Boubez jokes. “But what Ops\n\nengineers should tell statisticians is that these types of non-\n\nparametric techniques are great for Operations data, because it makes no assumptions about normality or any\n\nother probability distribution, which is crucial for us to understand what’s going on in our very complex systems.\n\nThese techniques compare two probability distributions,\n\nallowing us to compare periodic or seasonal data, which helps us find variances in data that varies from day to day or\n\nweek to week.”\n\nFigure 36, on the following page, shows is the same data set with the K-S filter applied, with the third area highlighting the\n\nanomalous Monday where transaction volume didn’t return\n\nto normal levels. This would have alerted us of a problem in our system that would have been virtually impossible to\n\ndetect using visual inspection or using standard deviations.\n\nIn this scenario, this early detection could prevent a customer impacting event, as well as better enable us to\n\nachieve our organizational goals.",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Figure 36: Transaction volume: using Kolmogorov-Smirnov test to alert on anomalies (Source: Dr. Toufic Boubez, “Simple math for anomaly detection.”)\n\nCONCLUSION\n\nIn this chapter, we explored several different statistical techniques\n\nthat can be used to analyze our production telemetry so we can\n\nfind and fix problems earlier than ever, often when they are still small and long before they cause catastrophic outcomes. This\n\nenables us to find ever-weaker failure signals that we can then act\n\nupon, creating an ever safer system of work, as well as increasing our ability to achieve our goals.\n\nSpecific case studies were presented, including how Netflix used\n\nthese techniques to proactively remove compute servers from production and auto-scale their compute infrastructure. We also\n\ndiscussed how to use a moving average and the Kolmogorov-",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Smirnov filter, both of which can be found in popular telemetry\n\ngraphing tools.\n\nIn the next chapter, we will describe how to integrate production telemetry into the daily work of Development in order to make\n\ndeployments safer and improve the system as a whole.\n\n† Smoothing and other statistical techniques are also used to manipulate graphic and audio files. For instance,\n\nimage smoothing (or blurring) as each pixel is replaced by the average of all its neighbors.\n\n‡ Other examples of smoothing filters include weighted moving averages or exponential smoothing (which linearly\n\nor exponentially weight more recent data points over older data points, respectively), and so forth.\n\n§ Tools we can using to solve these types of problems include Microsoft Excel (which remains one of the easiest\n\nand fastest ways to manipulate data for one-time purposes), as well as statistical packages such as SPSS, SAS, and the open source R project, now one of the most widely used statistical packages. Many other tools have been created, including several that Etsy has open-sourced, such as Oculus, which finds graphs with similar shapes that may indicate correlation; Opsweekly, which tracks alert volumes and frequencies; and Skyline, which attempts to identify anomalous behavior in system and application graphs.",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "16Enable Feedback So Development and\n\nOperations Can Safely Deploy Code\n\nIn 2006, Nick Galbreath was VP of Engineering at Right Media,\n\nresponsible for both the Development and Operations departments for an online advertising platform that displayed and\n\nserved over ten billion impressions daily.\n\nGalbreath described the competitive landscape they operated in:\n\nIn our business, ad inventory levels were extremely dynamic,\n\nso we needed to respond to market conditions within minutes. This meant that Development had to be able to quickly make\n\ncode changes and get them into production as soon as possible,\n\notherwise we would lose to faster competitors. We found that having a separate group for testing, and even deployment, was simply too slow. We had to integrate all these functions into\n\none group, with shared responsibilities and goals. Believe it or not, our biggest challenge was getting developers to overcome their fear of deploying their own code!\n\nThere is an interesting irony here: Dev often complains about Ops being afraid to deploy code. But in this case, when given the power",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "to deploy their own code, developers became just as afraid to\n\nperform code deployments.\n\nThe fear of deploying code that was shared by both Dev and Ops at\n\nRight Media is not unusual. However, Galbreath observed that\n\nproviding faster and more frequent feedback to engineers\n\nperforming deployments (whether Dev or Ops), as well as\n\nreducing the batch size of their work, created safety and then\n\nconfidence.\n\nAfter observing many teams go through this transformation,\n\nGalbreath describes their progression as follows:\n\nWe start with no one in Dev or Ops being willing to push the “deploy code” button that we’ve built that automates the entire code deployment process, because of the paralyzing fear of being the first person to potentially bring all of the production\n\nsystems down. Eventually, when someone is brave enough to volunteer to push their code into production, inevitably, due to incorrect assumptions or production subtleties that weren’t fully appreciated, the first production deployment doesn’t go smoothly—and because we don’t have enough production telemetry, we only find out about the problems when\n\ncustomers tell us.\n\nTo fix the problem, our team urgently fixes the code and pushes it\n\ninto production, but this time with more production telemetry added to our applications and environment. This way, we can actually confirm that our fix restored service correctly, and we’ll be able to detect this type of problem before a customer tells us next time.",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Later, more developers start to push their own code into\n\nproduction. And because we’re working in a complex system, we’ll\n\nstill probably break something in production, but this time we’ll be\n\nable to quickly see what functionality broke, and quickly decide\n\nwhether to roll back or fix-forward, resolving the problem. This is\n\na huge victory for the entire team and everyone celebrates—we’re\n\nnow on a roll.\n\nHowever, the team wants to improve the outcomes of their\n\ndeployments, so developers proactively get more peer reviews of\n\ntheir code changes (described in chapter 18), and everyone helps\n\neach other write better automated tests so we can find errors before deployment. And because everyone now knows that the smaller our production changes, the fewer problems we will have,\n\ndevelopers start checking ever-smaller increments of code more frequently into the deployment pipeline, ensuring that their change is working successfully in production before moving to their next change.\n\nWe are now deploying code more frequently than ever, and service stability is better than ever too. We have re-discovered that the secret to smooth and continuous flow is making small, frequent changes that anyone can inspect and easily understand.\n\nGalbreath observes that the above progression benefits everyone, including Development, Operations, and Infosec. “As the person\n\nwho is also responsible for security, it’s reassuring to know that we can deploy fixes into production quickly, because changes are going into production throughout the entire day. Furthermore, it always amazes me how interested every engineer becomes in\n\nsecurity when you find problems in their code that they are responsible for and that they can quickly fix themselves.”",
      "content_length": 1746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "The Right Media story shows that it is not enough to merely automate the deployment process—we must also integrate the\n\nmonitoring of production telemetry into our deployment work, as well as establish the cultural norms that everyone is equally\n\nresponsible for the health of the entire value stream.\n\nIn this chapter, we create the feedback mechanisms that enable us\n\nto improve the health of the value stream at every stage of the service life cycle, from product design through development and\n\ndeployment and into operation and eventually retirement. By doing this, we ensure that our services are “production ready,”\n\neven at the earliest stages of the project, as well as integrating the learnings from each release and production problem into our\n\nfuture work, resulting in better safety and productivity for\n\neveryone.\n\nUSE TELEMETRY TO MAKE DEPLOYMENTS SAFER\n\nIn this step, we ensure that we are actively monitoring our production telemetry when anyone performs a production\n\ndeployment, as was illustrated in the Right Media story. This allows whoever is doing the deployment, be it Dev or Ops, to\n\nquickly determine whether features are operating as designed\n\nafter the new release is running in production. After all, we should never consider our code deployment or production change to be\n\ndone until it is operating as designed in the production environment.\n\nWe do this by actively monitoring the metrics associated with our\n\nfeature during our deployment to ensure we haven’t inadvertently",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "broken our service—or worse, that we broke another service. If our change breaks or impairs any functionality, we quickly work to\n\nrestore service, bringing in whoever else is required to diagnose and fix the issue.†\n\nAs described in Part III, our goal is to catch errors in our\n\ndeployment pipeline before they get into production. However, there will still be errors that we don’t detect, and we rely on\n\nproduction telemetry to quickly restore service. We may choose to\n\nturn off broken features with feature toggles (which is often the easiest and least risky option since it involves no deployments to\n\nproduction), or fix forward (i.e., make code changes to fix the defect, which are then pushed into production through the\n\ndeployment pipeline), or roll back (e.g., switch back to the previous release by using feature toggles or by taking broken\n\nservers out of rotation using the blue-green or canary release\n\npatterns, etc.)\n\nAlthough fixing forward can often be dangerous, it can be extremely safe when we have automated testing and fast\n\ndeployment processes, and sufficient telemetry that allows us to quickly confirm whether everything is functioning correctly in\n\nproduction.\n\nFigure 37 shows a deployment of PHP code change at Etsy that generated a spike in PHP runtime warnings—in this case, the\n\ndeveloper quickly noticed the problem within minutes, and\n\ngenerated a fix and deployed it into production, resolving the issue in less than ten minutes.\n\nBecause production deployments are one of the top causes of\n\nproduction issues, each deployment and change event is overlaid",
      "content_length": 1593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "onto our metric graphs to ensure that everyone in the value stream\n\nis aware of relevant activity, enabling better communication and\n\ncoordination, as well as faster detection and recovery.\n\nFigure 37: Deployment to Etsy.com causes PHP run-time warnings and is quickly fixed (Source: Mike Brittain, “Tracking Every Release.”)\n\nDEV SHARES PAGER ROTATION DUTIES WITH OPS\n\nEven when our production deployments and releases go\n\nflawlessly, in any complex service we will still have unexpected problems, such as incidents and outages that happen at\n\ninopportune times (every night at 2 a.m.). Left unfixed, these can cause recurring problems and suffering for Ops engineers downstream, especially when these problems are not made visible\n\nto the upstream engineers responsible for creating the problem.",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Even if the problem results in a defect being assigned to the feature team, it may be prioritized below the delivery of new\n\nfeatures. The problem may keep recurring for weeks, months, or even years, causing continual chaos and disruption in Operations. This is an example of how upstream work centers can locally\n\noptimize for themselves but actually degrade performance for the entire value stream.\n\nTo prevent this from happening, we will have everyone in the\n\nvalue stream share the downstream responsibilities of handling operational incidents. We can do this by putting developers, development managers, and architects on pager rotation, just as\n\nPedro Canahuati, Facebook Director of Production Engineering, did in 2009. This ensures everyone in the value stream gets visceral feedback on any upstream architectural and coding decisions they make.\n\nBy doing this, Operations doesn’t struggle, isolated and alone with code-related production issues; instead, everyone is helping find the proper balance between fixing production defects and\n\ndeveloping new functionality, regardless of where we reside in the value stream. As Patrick Lightbody, SVP of Product Management at New Relic, observed in 2011, “We found that when we woke up\n\ndevelopers at 2 a.m., defects were fixed faster than ever.”\n\nOne side effect of this practice is that it helps Development management see that business goals are not achieved simply\n\nbecause features have been marked as “done.” Instead, the feature is only done when it is performing as designed in production, without causing excessive escalations or unplanned work for either Development or Operations.‡",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "This practice is equally applicable for market-oriented teams, responsible for both developing the feature and running it in\n\nproduction, and for functionally-oriented teams. As Arup Chakrabarti, Operations Engineering Manager at PagerDuty, observed during a 2014 presentation, “It’s becoming less and less\n\ncommon for companies to have dedicated on-call teams; instead, everyone who touches production code and environments is expected to be reachable in the event of downtime.”\n\nRegardless of how we’ve organized our teams, the underlying principles remain the same: when developers get feedback on how their applications perform in production, which includes fixing it when it breaks, they become closer to the customer, this creates a\n\nbuy-in that everyone in the value stream benefits from.\n\nHAVE DEVELOPERS FOLLOW WORK DOWNSTREAM\n\nOne of the most powerful techniques in interaction and user\n\nexperience design (UX) is contextual inquiry. This is when the product team watches a customer use the application in their natural environment, often working at their desk. Doing so often uncovers startling ways that customers struggle with the\n\napplication, such as requiring scores of clicks to perform simple tasks in their daily work, cutting and pasting text from multiple screens, or writing down notes on paper. All of these are examples\n\nof compensatory behaviors and workarounds for usability issues.\n\nThe most common reaction for developers after participating in a customer observation is dismay, often stating “how awful it was\n\nseeing the many ways we have been inflicting pain on our",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "customers.” These customer observations almost always result in significant learning and a fervent desire to improve the situation\n\nfor the customer.\n\nOur goal is to use this same technique to observe how our work affects our internal customers. Developers should follow their\n\nwork downstream, so they can see how downstream work centers must interact with their product to get it running into production.§\n\nDevelopers want to follow their work downstream—by seeing\n\ncustomer difficulties firsthand, they make better and more informed decisions in their daily work.\n\nBy doing this, we create feedback on the non-functional aspects of\n\nour code—all the elements that are not related to the customer- facing feature—and identify ways that we can improve deployability, manageability, operability, and so on.\n\nUX observation often has a powerful impact on the observers. When describing his first customer observation, Gene Kim, the founder and CTO at Tripwire for thirteen years and co-author of\n\nthis book, said:\n\nOne of the worst moments of my professional career was in 2006 when I spent an entire morning watching one of our\n\ncustomers use our product. I was watching him perform an operation that we expected customers to do weekly, and, to our extreme horror, we discovered that it required sixty-three\n\nclicks. This person kept apologizing, saying things like, “Sorry, there’s probably a better way to do this.”\n\nUnfortunately, there wasn’t a better way to do that operation.\n\nAnother customer described how initial product setup took",
      "content_length": 1540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "1,300 steps. Suddenly, I understood why the job of managing our product was always assigned to the newest engineer on the team—no one wanted the job of running our product. That was\n\none of the reasons I helped create the UX practice at my company, to help atone for the pain we were inflicting on our customers.\n\nUX observation enables the creation of quality at the source and results in far greater empathy for fellow team members in the value stream. Ideally, UX observation helps us as we create\n\ncodified non-functional requirements to add to our shared backlog of work, eventually allowing us to proactively integrate them into every service we build, which is an important part of creating a DevOps work culture.¶\n\nHAVE DEVELOPERS INITIALLY SELF- MANAGE THEIR PRODUCTION SERVICE\n\nEven when Developers are writing and running their code in production-like environments in their daily work, Operations may\n\nstill experience disastrous production releases because it is the first time we actually see how our code behaves during a release and under true production conditions. This result occurs because\n\noperational learnings often occur too late in the software life cycle.\n\nLeft unaddressed, the result is often production software that is difficult to operate. As an anonymous Ops engineer once said, “In\n\nour group, most system administrators lasted only six months. Things were always breaking in production, the hours were insane, and application deployments were painful beyond belief— the worst part was pairing the application server clusters, which",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "would take us six hours. During each moment, we all felt like the developers personally hated us.”\n\nThis can be an outcome of not having enough Ops engineers to support all the product teams and the services we already have in production, which can happen in both functionally- and market- oriented teams.\n\nOne potential countermeasure is to do what Google does, which is have Development groups self-manage their services in\n\nproduction before they become eligible for a centralized Ops group to manage. By having developers be responsible for deployment and production support, we are far more likely to have a smooth transition to Operations.**\n\nTo prevent the possibility of problematic, self-managed services going into production and creating organizational risk, we may define launch requirements that must be met in order for services\n\nto interact with real customers and be exposed to real production traffic. Furthermore, to help the product teams, Ops engineers should act as consultants to help them make their services\n\nproduction-ready.\n\nBy creating launch guidance, we help ensure that every product team benefits from the cumulative and collective experience of the\n\nentire organization, especially Operations. Launch guidance and requirements will likely include the following:\n\nDefect counts and severity: Does the application actually\n\nperform as designed?\n\nType/frequency of pager alerts: Is the application generating an unsupportable number of alerts in production?",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Monitoring coverage: Is the coverage of monitoring sufficient to restore service when things go wrong?\n\nSystem architecture: Is the service loosely-coupled enough to support a high rate of changes and deployments in production?\n\nDeployment process: Is there a predictable, deterministic, and sufficiently automated process to deploy code into production?\n\nProduction hygiene: Is there evidence of enough good production habits that would allow production support to be managed by anyone else?\n\nSuperficially, these requirements may appear similar to traditional production checklists we have used in the past. However, the key differences are we require effective monitoring to be in place,\n\ndeployments to be reliable and deterministic, and an architecture that supports fast and frequent deployments.\n\nIf any deficiencies are found during the review, the assigned Ops\n\nengineer should help the feature team resolve the issues or even help re-engineer the service if necessary, so that it can be easily deployed and managed in production.\n\nAt this time, we may also want to learn whether this service is subject to any regulatory compliance objectives or if it is likely to\n\nbe in the future:\n\nDoes the service generate a significant amount of revenue?\n\n(For example, if it is more than 5% of total revenue of a publicly-held US corporation, it is a “significant account” and",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "in-scope for compliance with Section 404 of the Sarbanes-\n\nOxley Act of 2002 [SOX].)\n\nDoes the service have high user traffic or have high outage/impairment costs? (i.e., do operational issues risk\n\ncreating availability or reputational risk?)\n\nDoes the service store payment cardholder information, such as credit card numbers, or personally identifiable information,\n\nsuch as Social Security numbers or patient care records? Are\n\nthere other security issues that could create regulatory, contractual obligation, privacy, or reputation risk?\n\nDoes the service have any other regulatory or contractual\n\ncompliance requirements associated with it, such as US export regulations, PCI-DSS, HIPAA, and so forth?\n\nThis information helps ensure that we effectively manage not only\n\nthe technical risks associated with this service, but also any potential security and compliance risks. It also provides essential\n\ninput into the design of the production control environment.",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Figure 38: The “Service Handback” at Google (Source: “SRE@Google: Thousands of DevOps Since 2004,” YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n\nBy integrating operability requirements into the earliest stages of\n\nthe development process and having Development initially self- manage their own applications and services, the process of\n\ntransitioning new services into production becomes smoother,\n\nbecoming far easier and more predictable to complete. However, for services already in production, we need a different mechanism\n\nto ensure that Operations is never stuck with an unsupportable service in production. This is especially relevant for functionally-\n\noriented Operations organizations.\n\nIn this step, we may create a service handback mechanism—in\n\nother words, when a production service becomes sufficiently fragile, Operations has the ability to return production support\n\nresponsibility back to Development.",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "When a service goes back into a developer-managed state, the role\n\nof Operations shifts from production support to consultation,\n\nhelping the team make the service production-ready.\n\nThis mechanism serves as our pressure escape valve, ensuring that\n\nwe never put Operations in a situation where they are trapped into\n\nmanaging a fragile service while an ever-increasing amount of technical debt buries them and amplifies a local problem into a\n\nglobal problem. This mechanism also helps ensure that\n\nOperations has enough capacity to work on improvement work and preventive projects.\n\nThe hand-back remains a long-standing practice at Google and is\n\nperhaps one of the best demonstrations of the mutual respect between Dev and Ops engineers. By doing this, Development is\n\nable to quickly generate new services, with Ops engineers joining the team when the services become strategically important to the\n\ncompany and, in rare cases, handing them back when they become too troublesome to manage in production.†† The following case study of Site Reliability Engineering at Google describes how the\n\nHand-off Readiness Review and Launch Readiness Review\n\nprocesses evolved, and the benefits that resulted.\n\nCase Study The Launch and Hand-off Readiness Review at Google (2010)\n\nOne of the many surprising facts about Google is that they\n\nhave a functional orientation for their Ops engineers, who are referred to as “Site Reliability Engineers” (SRE), a term coined by Ben Treynor Sloss in 2004.‡‡ That year, Treynor",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Sloss started off with a staff of seven SREs that grew to over\n\n1,200 SREs by 2014. As Treynor Sloss said, “If Google ever\n\ngoes down, it’s my fault.” Treynor Sloss has resisted creating a single sentence definition of what SREs are, but,\n\nhe once described SREs as “what happens when a software engineer is tasked with what used to be called operations.”\n\nEvery SRE reports to Treynor Sloss’s organization to help\n\nensure consistency of quality of staffing and hiring, and they are embedded into product teams across Google (which\n\nalso provide their funding). However, SREs are still so\n\nscarce they are assigned only to the product teams that have the highest importance to the company or those that\n\nmust comply with regulatory requirements. Furthermore,\n\nthose services must have low operational burden. Products that don’t meet the necessary criteria remain in a developer-\n\nmanaged state.\n\nEven when new products become important enough to the company to warrant being assigned an SRE, developers still\n\nmust have self-managed their service in production for at\n\nleast six months before it becomes eligible to have an SRE assigned to the team.\n\nTo help ensure that these self-managed product teams can\n\nstill benefit from the collective experience of the SRE organization, Google created two sets of safety checks for\n\ntwo critical stages of releasing new services called the\n\nLaunch Readiness Review and the Hand-Off Readiness Review (LRR and HRR, respectively).",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "The LRR must be performed and signed off on before any\n\nnew Google service is made publicly available to customers and receives live production traffic, while the HRR is\n\nperformed when the service is transitioned to an Ops-\n\nmanaged state, usually months after the LRR. The LRR and HRR checklists are similar, but the HRR is far more stringent\n\nand has higher acceptance standards, while the LRR is self-\n\nreported by the product teams.\n\nAny product team going through an LRR or HRR has an\n\nSRE assigned to them to help them understand the\n\nrequirements and to help them achieve those requirements. The LRR and HRR launch checklists have evolved over time\n\nso every team can benefit from the collective experiences of\n\nall previous launches, whether successful or unsuccessful. Tom Limoncelli noted during his “SRE@Google: Thousands\n\nof DevOps Since 2004” presentation in 2012, “Every time we\n\ndo a launch, we learn something. There will always be some people who are less experienced than others doing releases\n\nand launches. The LRR and HRR checklists are a way to\n\ncreate that organizational memory.”\n\nRequiring product teams to self-manage their own services\n\nin production forces Development to walk in the shoes of\n\nOps, but guided by the LRR and HRR, which not only makes service transition easier and more predictable, but also\n\nhelps create empathy between upstream and downstream work centers.",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Figure 39: The “Launch readiness review and hand-offs readiness review” at Google (Source: “SRE@Google: Thousands of DevOps Since 2004,” YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n\nLimoncelli noted, “In the best case, product teams have been using the LRR checklist as a guideline, working on\n\nfulfilling it in parallel with developing their service, and\n\nreaching out to SREs to get help when they need it.”\n\nFurthermore, Limoncelli observed, “The teams that have the\n\nfastest HRR production approval are the ones that worked\n\nwith SREs earliest, from the early design stages up until launch. And the great thing is, it’s always easy to get an\n\nSRE to volunteer to help with your project. Every SRE sees\n\nvalue in giving advice to project teams early, and will likely volunteer a few hours or days to do just that.”\n\nThe practice of SREs helping product teams early is an\n\nimportant cultural norm that is continually reinforced at Google. Limoncelli explained, “Helping product teams is a",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "long-term investment that will pay off many months later\n\nwhen it comes time to launch. It is a form of ‘good citizenship’ and ‘community service’ that is valued, it is\n\nroutinely considered when evaluating engineers for SRE\n\npromotions.”\n\nCONCLUSION\n\nIn this chapter, we discussed the feedback mechanisms that\n\nenable us to improve our service at every stage of our daily work,\n\nwhether it is deploying changes into production, fixing code when things go wrong and engineers are paged, having developers\n\nfollow their work downstream, creating non-functional requirements that help development teams write more\n\nproduction-ready code, or even handing problematic services back\n\nto be self-managed by Development.\n\nBy creating these feedback loops, we make production\n\ndeployments safer, increase the production readiness of code\n\ncreated by Development, and help create a better working relationship between Development and Operations by reinforcing\n\nshared goals, responsibilities, and empathy.\n\nIn the next chapter, we explore how telemetry can enable hypothesis-driven development and A/B testing to perform\n\nexperiments that help us achieve our organizational goals and win\n\nin the marketplace.\n\n† By doing this, along with the required architecture, we “optimize for MTTR, instead of MTBF,” a popular DevOps maxim to describe our desire to optimize for recovering from failures quickly, as opposed to attempting to prevent failures.\n\n‡ ITIL defines warranty as when a service can run in production reliably without intervention for a predefined period of time (e.g., two weeks). This definition of warranty should ideally be integrated into our collective",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "definition of “done.”\n\n§ By following work downstream, we may uncover ways to help improve flow, such as automating complex, manual steps (e.g., pairing application server clusters that require six hours to successfully complete); performing packaging of code once instead of creating it multiple times at different stages of QA and Production deployment; working with testers to automate manual test suites, thus removing a common bottleneck for more frequent deployment; and creating more useful documentation instead of having someone decipher developer application notes to build packaged installers.\n\n¶ More recently, Jeff Sussna attempted to further codify how to better achieve UX goals in what he calls “digital conversations,” which are intended to help organizations understand the customer journey as a complex system, broadening the context of quality. The key concepts include designing for service, not software; minimizing latency and maximizing strength of feedback; designing for failure and operating to learn; using Operations as an input to design; and seeking empathy.\n\n** We further increase the likelihood of production problems being fixed by ensuring that the Development teams\n\nremain intact, and not disbanded after the project is complete.\n\n†† In organizations with project-based funding, there may be no developers to hand the service back to, as the\n\nteam has already been disbanded or may not have the budget or time to take on service responsibility. Potential countermeasures include holding an improvement blitz to improve the service, temporarily funding or staffing improvement efforts, or retiring the service.\n\n‡‡ In this book, we use the term “Ops engineer,” but the two terms, “Ops Engineer” and “Site Reliability\n\nEngineer,” are intended to be interchangeable.",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "17Integrate Hypothesis- Driven Development and A/B Testing into Our Daily Work\n\nAll too often in software projects, developers work on features for\n\nmonths or years, spanning multiple releases, without ever confirming whether the desired business outcomes are being met,\n\nsuch as whether a particular feature is achieving the desired results or even being used at all.\n\nWorse, even when we discover that a given feature isn’t achieving\n\nthe desired results, making corrections to the feature may be out- prioritized by other new features, ensuring that the under-\n\nperforming feature will never achieve its intended business goal.\n\nIn general, Jez Humble observes, “the most inefficient way to test a business model or product idea is to build the complete product\n\nto see whether the predicted demand actually exists.”\n\nBefore we build a feature, we should rigorously ask ourselves, “Should we build it, and why?” We should then perform the cheapest and fastest experiments possible to validate through user\n\nresearch whether the intended feature will actually achieve the desired outcomes. We can use techniques such as hypothesis- driven development, customer acquisition funnels, and A/B\n\ntesting, concepts we explore throughout this chapter. Intuit, Inc.",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "provides a dramatic example of how organizations use these\n\ntechniques to create products that customers love, to promote\n\norganizational learning, and to win in the marketplace.\n\nIntuit is focused on creating business and financial management\n\nsolutions to simplify life for small businesses, consumers, and\n\naccounting professionals. In 2012, they had $4.5 billion in revenue\n\nand 8,500 employees, with flagship products that include QuickBooks, TurboTax, Mint, and, until recently, Quicken.†\n\nScott Cook, the founder of Intuit, has long advocated building a\n\nculture of innovation, encouraging teams to take an experimental\n\napproach to product development and exhorting leadership to\n\nsupport them. As he said, “Instead of focusing on the boss’s vote… the emphasis is on getting real people to really behave in real\n\nexperiments, and basing your decisions on that.” This is the epitome of a scientific approach to product development.\n\nCook explained that what is needed is “a system where every employee can do rapid, high-velocity experiments….Dan Maurer runs our consumer division....[which] runs the TurboTax website. When he took over, we did about seven experiments a year.”\n\nHe continued, “By installing a rampant innovation culture [in 2010], they now do 165 experiments in the three months of the [US] tax season. Business result? [The] conversion rate of the\n\nwebsite is up 50 percent…. The folks [team members] just love it, because now their ideas can make it to market.”\n\nAside from the effect on the website conversion rate, one of the most surprising elements of this story is that TurboTax performed production experiments during their peak traffic seasons. For",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "decades, especially in retailing, the risk of revenue-impacting\n\noutages during the holiday season were so high that we would\n\noften put into place a change freeze from mid-October to mid-\n\nJanuary.\n\nHowever, by making software deployments and releases fast and\n\nsafe, the TurboTax team made online user experimentation and\n\nany required production changes a low-risk activity that could be\n\nperformed during the highest traffic and revenue generating\n\nperiods.\n\nThis highlights the notion that the period when experimentation\n\nhas the highest value is during peak traffic seasons. Had the TurboTax team waited until April 16th, the day after the US tax filing deadline, to implement these changes, the company could have lost many of its prospective customers, and even some of its existing customers, to the competition.\n\nThe faster we can experiment, iterate, and integrate feedback into\n\nour product or service, the faster we can learn and out-experiment the competition. And how quickly we can integrate our feedback depends on our ability to deploy and release software.\n\nThe Intuit example shows that the Intuit TurboTax team was able to make this situation work for them and won in the marketplace as a result.\n\nA BRIEF HISTORY OF A/B TESTING\n\nAs the Intuit TurboTax story highlights, an extremely powerful user research technique is defining the customer acquisition",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "funnel and performing A/B testing. A/B testing techniques were pioneered in direct response marketing, which is one of the two\n\nmajor categories of marketing strategies. The other is called mass marketing or brand marketing and often relies on placing as\n\nmany ad impressions in front of people as possible to influence\n\nbuying decisions.\n\nIn previous eras, before email and social media, direct response marketing meant sending thousands of postcards or flyers via\n\npostal mail, and asking prospects to accept an offer by calling a telephone number, returning a postcard, or placing an order.\n\nIn these campaigns, experiments were performed to determine\n\nwhich offer had the highest conversion rates. They experimented with modifying and adapting the offer, re-wording the offer,\n\nvarying the copywriting styles, design and typography, packaging,\n\nand so forth, to determine which was most effective in generating the desired action (e.g., calling a phone number, ordering a\n\nproduct).\n\nEach experiment often required doing another design and print run, mailing out thousands of offers, and waiting weeks for\n\nresponses to come back. Each experiment typically cost tens of thousands of dollars per trial and required weeks or months to\n\ncomplete. However, despite the expense, iterative testing easily\n\npaid off if it significantly increased conversion rates (e.g., the percentage of respondents ordering a product going from 3%–\n\n12%).\n\nWell-documented cases of A/B testing include campaign fundraising, Internet marketing, and the Lean Startup\n\nmethodology. Interestingly, it has also been used by the British",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "government to determine which letters were most effective in collecting overdue tax revenue from delinquent citizens.‡\n\nINTEGRATING A/B TESTING INTO OUR FEATURE TESTING\n\nThe most commonly used A/B technique in modern UX practice\n\ninvolves a website where visitors are randomly selected to be shown one of two versions of a page, either a control (the “A”) or a\n\ntreatment (the “B”). Based on statistical analysis of the subsequent behavior of these two cohorts of users, we demonstrate whether\n\nthere is a significant difference in the outcomes of the two,\n\nestablishing a causal link between the treatment (e.g., a change in a feature, design element, background color) and the outcome\n\n(e.g., conversion rate, average order size).\n\nFor example, we may conduct an experiment to see whether modifying the text or color on a “buy” button increases revenue or\n\nwhether slowing down the response time of a website (by introducing an artificial delay as the treatment) reduces revenue.\n\nThis type of A/B testing allows us to establish a dollar value on\n\nperformance improvements.\n\nSometimes, A/B tests are also known as online controlled experiments and split tests. It’s also possible to run experiments\n\nwith more than one variable. This allows us to see how the variables interact, a technique known as multivariate testing.\n\nThe outcomes of A/B tests are often startling. Ronny Kohavi,\n\nDistinguished Engineer and General Manager of the Analysis and Experimentation group at Microsoft, observed that after",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "“evaluating well-designed and executed experiments that were\n\ndesigned to improve a key metric, only about one-third were\n\nsuccessful at improving the key metric!” In other words, two- thirds of features either have a negligible impact or actually make\n\nthings worse. Kohavi goes on to note that all these features were originally thought to be reasonable, good ideas, further elevating\n\nthe need for user testing over intuition and expert opinions.\n\nThe implications of the Kohavi data are staggering. If we are not performing user research, the odds are that two-thirds of the\n\nfeatures we are building deliver zero or negative value to our\n\norganization, even as they make our codebase ever more complex, thus increasing our maintenance costs over time and making our\n\nsoftware more difficult to change. Furthermore, the effort to build these features is often made at the expense of delivering features\n\nthat would deliver value (i.e., opportunity cost). Jez Humble joked, “Taken to an extreme, the organization and customers\n\nwould have been better off giving the entire team a vacation,\n\ninstead of building one of these non–value-adding features.”\n\nOur countermeasure is to integrate A/B testing into the way we design, implement, test, and deploy our features. Performing\n\nmeaningful user research and experiments ensures that our efforts help achieve our customer and organizational goals, and help us\n\nwin in the marketplace.\n\nINTEGRATE A/B TESTING INTO OUR RELEASE\n\nFast and iterative A/B testing is made possible by being able to\n\nquickly and easily do production deployments on demand, using",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "feature toggles and potentially delivering multiple versions of our code simultaneously to customer segments. Doing this requires\n\nuseful production telemetry at all levels of the application stack.\n\nBy hooking into our feature toggles, we can control which percentage of users see the treatment version of an experiment.\n\nFor example, we may have one-half of our customers be our treatment group and one-half get shown the following: “Similar items link on unavailable items in the cart.” As part of our\n\nexperiment, we compare the behavior of the control group (no offer made) against the treatment group (offer made), perhaps measuring number of purchases made in that session.\n\nEtsy open-sourced their experimentation framework Feature API (formerly known as the Etsy A/B API), which not only supports A/B testing but also online ramp-ups, enabling throttling\n\nexposure to experiments. Other A/B testing products include Optimizely, Google Analytics, etc.\n\nIn a 2014 interview with Kendrick Wang of Apptimize, Lacy\n\nRhoades at Etsy described their journey: “Experimentation at Etsy comes from a desire to make informed decisions, and ensure that when we launch features for our millions of members, they work. Too often, we had features that took a lot of time and had to be\n\nmaintained without any proof of their success or any popularity among users. A/B testing allows us to...say a feature is worth working on as soon as it’s underway.”\n\nINTEGRATING A/B TESTING INTO OUR FEATURE PLANNING",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Once we have the infrastructure to support A/B feature release and testing, we must ensure that product owners think about each\n\nfeature as a hypothesis and use our production releases as experiments with real users to prove or disprove that hypothesis. Constructing experiments should be designed in the context of the\n\noverall customer acquisition funnel. Barry O’Reilly, co-author of Lean Enterprise: How High Performance Organizations Innovate at Scale, described how we can frame hypotheses in feature development in the following form:\n\nWe Believe that increasing the size of hotel images on the booking page\n\nWill Result in improved customer engagement and conversion\n\nWe Will Have Confidence To Proceed When we see a 5%\n\nincrease in customers who review hotel images who then proceed to book in forty-eight hours.\n\nAdopting an experimental approach to product development\n\nrequires us to not only break down work into small units (stories or requirements), but also validate whether each unit of work is delivering the expected outcomes. If it does not, we modify our\n\nroad map of work with alternative paths that will actually achieve those outcomes.\n\nCase Study\n\nDoubling Revenue Growth through Fast Release Cycle Experimentation at Yahoo! Answers (2010)",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "The faster we can iterate and integrate feedback into the product or service we are delivering to customers, the faster\n\nwe can learn and the bigger the impact we can create. How dramatically outcomes can be affected by faster cycle times was evident at Yahoo! Answers as they went from one release every six weeks to multiple releases every week.\n\nIn 2009, Jim Stoneham was General Manager of the Yahoo! Communities group that included Flickr and Answers. Previously, he had been primarily responsible for Yahoo!\n\nAnswers, competing against other Q&A companies such as Quora, Aardvark, and Stack Exchange.\n\nAt that time, Answers had approximately 140 million monthly visitors, with over twenty million active users answering questions in over twenty different languages. However, user growth and revenue had flattened, and user engagement\n\nscores were declining.\n\nStoneham observes that “Yahoo Answers was and continues to be one of the biggest social games on the\n\nInternet; tens of millions of people are actively trying to ‘level up’ by providing quality answers to questions faster than the next member of the community. There were many\n\nopportunities to tweak the game mechanic, viral loops, and other community interactions. When you’re dealing with these human behaviors, you’ve got to be able to do quick iterations and testing to see what clicks with people.”\n\nHe continues, “These [experiments] are the things that Twitter, Facebook, and Zynga did so well. Those\n\norganizations were doing experiments at least twice per",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "week—they were even reviewing the changes they made before their deployments, to make sure they were still on track. So here I am, running [the] largest Q&A site in the\n\nmarket, wanting to do rapid iterative feature testing, but we can’t release any faster than once every 4 weeks. In contrast, the other people in the market had a feedback loop\n\n10x faster than us.”\n\nStoneham observed that as much as product owners and developers talk about being metrics-driven, if experiments\n\nare not performed frequently (daily or weekly), the focus of daily work is merely on the feature they’re working on, as opposed to customer outcomes.\n\nAs the Yahoo! Answers team was able to move to weekly deployments, and later multiple deployments per week, their ability to experiment with new features increased\n\ndramatically. Their astounding achievements and learnings over the next twelve months of experimentation included increased monthly visits of 72%, increased user engagement of threefold, and the team doubled their\n\nrevenue. To continue their success, the team focused on optimizing the following top metrics:\n\nTime to first answer: How quickly was an answer posted to a user question?\n\nTime to best answer: How quickly did the user community\n\naward a best answer?\n\nUpvotes per answer: How many times was an answer upvoted by the user community?",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Answers/week/person: How many answers were users creating?\n\nSecond search rate: How often did visitors have to search again to get an answer? (Lower is better.)\n\nStoneham concluded, “This was exactly the learning that we\n\nneeded to win in the marketplace—and it changed more than our feature velocity. We transformed from a team of employees to a team of owners. When you move at that\n\nspeed, and are looking at the numbers and the results daily, your investment level radically changes.”\n\nCONCLUSION\n\nSuccess requires us to not only deploy and release software quickly, but also to out-experiment our competition. Techniques such as hypothesis-driven development, defining and measuring\n\nout customer acquisition funnel, and A/B testing allow us to perform user-experiments safely and easily, enabling us to unleash creativity and innovation, and create organizational\n\nlearning. And, while succeeding is important, the organizational learning that comes from experimentation also gives employees ownership of business objectives and customer satisfaction. In the next chapter, we examine and create review and coordination\n\nprocesses as a way to increase the quality of our current work.\n\n† In 2016, Intuit sold the Quicken business to the private equity firm H.I.G. Capital.\n\n‡ There are many other ways to conduct user research before embarking on development. Among the most\n\ninexpensive methods include performing surveys, creating prototypes (either mock-ups using tools such as Balsamiq or interactive versions written in code), and performing usability testing. Alberto Savoia, Director of Engineering at Google, coined the term pretotyping for the practice of using prototypes to validate whether we are building the right thing. User research is so inexpensive and easy relative to the effort and cost of building a",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "useless feature in code that, in almost every case, we shouldn’t prioritize a feature without some form of validation.",
      "content_length": 118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "18Create Review and\n\nCoordination Processes\n\nto Increase Quality of Our Current Work\n\nIn the previous chapters, we created the telemetry necessary to see\n\nand solve problems in production and at all stages of our\n\ndeployment pipeline, and created fast feedback loops from customers to help enhance organizational learning—learning that\n\nencourages ownership and responsibility for customer satisfaction and feature performance, which helps us succeed.\n\nOur goal in this chapter is to enable Development and Operations to\n\nreduce the risk of production changes before they are made. Traditionally, when we review changes for deployment, we tend to\n\nrely heavily on reviews, inspections, and approvals just prior to deployment. Frequently those approvals are given by external teams\n\nwho are often too far removed from the work to make informed decisions on whether a change is risky or not, and the time required\n\nto get all the necessary approvals also lengthens our change lead\n\ntimes.\n\nThe peer review process at GitHub is a striking example of how\n\ninspection can increase quality, make deployments safe, and be\n\nintegrated into the flow of everyone’s daily work. They pioneered the process called pull request, one of the most popular forms of peer\n\nreview that span Dev and Ops.",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Scott Chacon, CIO and co-founder of GitHub, wrote on his website\n\nthat pull requests are the mechanism that lets engineers tell others\n\nabout changes they have pushed to a repository on GitHub. Once a\n\npull request is sent, interested parties can review the set of changes,\n\ndiscuss potential modifications, and even push follow-up commits if\n\nnecessary. Engineers submitting a pull request will often request a\n\n“+1,” “+2,” or so forth, depending on how many reviews they need,\n\nor “@mention” engineers that they’d like to get reviews from.\n\nAt GitHub, pull requests are also the mechanism used to deploy code\n\ninto production through a collective set of practices they call\n\n“GitHub Flow”—it’s how engineers request code reviews, gather and\n\nintegrate feedback, and announce that code will be deployed to production (i.e., “master” branch).\n\nFigure 40: Comments and suggestions on a GitHub pull request (Source: Scott Chacon, “GitHub Flow,” ScottChacon.com, August 31, 2011, http://scottchacon.com/2011/08/31/github-flow.html.)\n\nGitHub Flow is composed of five steps:",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "1. To work on something new, the engineer creates a descriptively\n\nnamed branch off of master (e.g., “new-oauth2-scopes”).\n\n2. The engineer commits to that branch locally, regularly pushing\n\ntheir work to the same named branch on the server.\n\n3. When they need feedback or help, or when they think the branch\n\nis ready for merging, they open a pull request.\n\n4. When they get their desired reviews and get any necessary\n\napprovals of the feature, the engineer can then merge it into\n\nmaster.\n\n5. Once the code changes are merged and pushed to master, the\n\nengineer deploys them into production.\n\nThese practices, which integrate review and coordination into daliy work, have allowed GitHub to quickly and reliably deliver features to\n\nmarket with high quality and security. For example, in 2012 they performed an amazing 12,602 deployments. In particular, on August 23rd, after a company-wide summit where many exciting ideas were brainstormed and discussed, the company had their busiest\n\ndeployment day of the year, with 563 builds and 175 successful deployments into production, all made possible through the pull request process.\n\nThroughout this chapter we will integrate practices, such as those used at GitHub, to shift our reliance away from periodic inspections and approvals, and moving to integrated peer review performed continually as a part of our daily work. Our goal is to ensure that\n\nDevelopment, Operations, and Infosec are continuously collaborating so that changes we make to our systems will operate\n\nreliably, securely, safely, and as designed.",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "THE DANGERS OF CHANGE APPROVAL PROCESSES\n\nThe Knight Capital failure is one of the most prominent software\n\ndeployment errors in recent memory. A fifteen minute deployment\n\nerror resulted in a $440 million trading loss, during which the\n\nengineering teams were unable to disable the production services.\n\nThe financial losses jeopardized the firm’s operations and forced the\n\ncompany to be sold over the weekend so they could continue\n\noperating without jeopardizing the entire financial system.\n\nJohn Allspaw observed that when high-profile incidents occur, such\n\nas the Knight Capital deployment accident, there are typically two counterfactual narratives for why the accident occurred.†\n\nThe first narrative is that the accident was due to a change control failure, which seems valid because we can imagine a situation where better change control practices could have detected the risk earlier and prevented the change from going into production. And if we\n\ncouldn’t prevent it, we might have taken steps to enable faster detection and recovery.\n\nThe second narrative is that the accident was due to a testing failure. This also seems valid, with better testing practices we could have identified the risk earlier and canceled the risky deployment, or we could have at least taken steps to enable faster detection and recovery.\n\nThe surprising reality is that in environments with low-trust, command-and-control cultures, the outcomes of these types of change control and testing countermeasures often result in an increased likelihood that problems will occur again, potentially with even worse outcomes.",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Gene Kim (co-author of this book) describes his realization that\n\nchange and testing controls can potentially have the opposite effect\n\nthan intended as “one of the most important moments of my\n\nprofessional career. This ‘aha’ moment was the result of a\n\nconversation in 2013 with John Allspaw and Jez Humble about the\n\nKnight Capital accident, making me question some of my core\n\nbeliefs that I’ve formed over the last ten years, especially having\n\nbeen trained as an auditor.”\n\nHe continues, “However upsetting it was, it was also a very\n\nformative moment for me. Not only did they convince me that they\n\nwere correct, we tested these beliefs in the 2014 State of DevOps\n\nReport, which led to some astonishing findings that reinforce that building high-trust cultures is likely the largest management challenge of this decade.”\n\nPOTENTIAL DANGERS OF “OVERLY CONTROLLING CHANGES”\n\nTraditional change controls can lead to unintended outcomes, such as contributing to long lead times, and reducing the strength and immediacy of feedback from the deployment process. In order to understand how this happens, let us examine the controls we often put in place when change control failures occur:\n\nAdding more questions that need to be answered to the change\n\nrequest form\n\nRequiring more authorizations, such as one more level of management approval (e.g., instead of merely the VP of Operations approving, we now require that the CIO also approve)",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "or more stakeholders (e.g., network engineering, architecture\n\nreview boards, etc.)\n\nRequiring more lead time for change approvals so that change\n\nrequests can be properly evaluated\n\nThese controls often add more friction to the deployment process by\n\nmultiplying the number of steps and approvals, and increasing batch\n\nsizes and deployment lead times, which we know reduces the\n\nlikelihood of successful production outcomes for both Dev and Ops.\n\nThese controls also reduce how quickly we get feedback from our\n\nwork.\n\nOne of the core beliefs in the Toyota Production System is that “people closest to a problem typically know the most about it.” This becomes more pronounced as the work being performed and the system the work occurs in become more complex and dynamic, as is typical in DevOps value streams. In these cases, creating approval steps from people who are located further and further away from the work may actually reduce the likelihood of success. As has been\n\nproven time and again, the further the distance between the person doing the work (i.e., the change implementer) and the person deciding to do the work (i.e., the change authorizer), the worse the outcome.\n\nIn Puppet Labs’ 2014 State of DevOps Report, one of the key findings was that high-performing organizations relied more on peer review and less on external approval of changes. Figure 41 shows\n\nthat the more organizations rely on change approvals, the worse their IT performance in terms of both stability (mean time to restore service and change fail rate) and throughput (deployment lead times, deployment frequency).",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "In many organizations, change advisory boards serve an important\n\nrole in coordinating and governing the delivery process, but their job\n\nshould not be to manually evaluate every change, nor does ITIL\n\nmandate such a practice.\n\nTo understand why this is the case, consider the predicament of\n\nbeing on a change advisory board, reviewing a complex change\n\ncomposed of hundreds of thousands of lines of code changes, and\n\ncreated by hundreds of engineers.\n\nAt one extreme, we cannot reliably predict whether a change will be\n\nsuccessful either by reading a hundred-word description of the\n\nchange or by merely validating that a checklist has been completed. At the other extreme, painfully scrutinizing thousands of lines of code changes is unlikely to reveal any new insights. Part of this is the nature of making changes inside of a complex system. Even the engineers who work inside the codebase as part of their daily work are often surprised by the side effects of what should be low-risk changes.",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Figure 41: Organizations that rely on peer review outperform those with change approvals (Source: Puppet Labs, DevOps Survey Of Practice 2014)\n\nFor all these reasons, we need to create effective control practices that more closely resemble peer review, reducing our reliance on external bodies to authorize our changes. We also need to coordinate and schedule changes effectively. We explore both of these in the\n\nnext two sections.\n\nENABLE COORDINATION AND SCHEDULING OF CHANGES\n\nWhenever we have multiple groups working on systems that share dependencies, our changes will likely need to be coordinated to ensure that they don’t interfere with each other (e.g., marshaling, batching, and sequencing the changes). In general, the more loosely- coupled our architecture, the less we need to communicate and",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "coordinate with other component teams—when the architecture is\n\ntruly service-oriented, teams can make changes with a high degree of\n\nautonomy, where local changes are unlikely to create global\n\ndisruptions.\n\nHowever, even in a loosely-coupled architecture, when many teams\n\nare doing hundreds of independent deployments per day, there may\n\nbe a risk of changes interfering with each other (e.g., simultaneous\n\nA/B tests). To mitigate these risks, we may use chat rooms to\n\nannounce changes and proactively find collisions that may exist.\n\nFor more complex organizations and organizations with more\n\ntightly-coupled architectures, we may need to deliberately schedule\n\nour changes, where representatives from the teams get together, not to authorize changes, but to schedule and sequence their changes in order to minimize accidents.\n\nHowever, certain areas, such as global infrastructure changes (e.g., core network switch changes) will always have a higher risk associated with them. These changes will always require technical countermeasures, such as redundancy, failover, comprehensive testing, and (ideally) simulation.\n\nENABLE PEER REVIEW OF CHANGES\n\nInstead of requiring approval from an external body prior to deployment, we may require engineers to get peer reviews of their changes. In Development, this practice has been called code review, but it is equally applicable to any change we make to our applications or environments, including servers, networking, and databases.‡ The goal is to find errors by having fellow engineers close to the work scrutinize our changes. This review improves the",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "quality of our changes, which also creates the benefits of cross-\n\ntraining, peer learning, and skill improvement.\n\nA logical place to require reviews is prior to committing code to\n\ntrunk in source control, where changes could potentially have a\n\nteam-wide or global impact. At a minimum, fellow engineers should\n\nreview our change, but for higher risk areas, such as database\n\nchanges or business-critical components with poor automated test\n\ncoverage, we may require further review from a subject matter\n\nexpert (e.g., information security engineer, database engineer) or\n\nmultiple reviews (e.g., “+2” instead of merely “+1”).\n\nThe principle of small batch sizes also applies to code reviews. The larger the size of the change that needs to be reviewed, the longer it takes to understand and the larger the burden on the reviewing engineer. As Randy Shoup observed, “There is a non-linear relationship between the size of the change and the potential risk of\n\nintegrating that change—when you go from a ten line code change to a one hundred line code, the risk of something going wrong is more than ten times higher, and so forth.” This is why it’s so essential for developers to work in small, incremental steps rather than on long- lived feature branches.\n\nFurthermore, our ability to meaningfully critique code changes goes down as the change size goes up. As Giray Özil tweeted, “Ask a programmer to review ten lines of code, he’ll find ten issues. Ask him to do five hundred lines, and he’ll say it looks good.”\n\nGuidelines for code reviews include:\n\nEveryone must have someone to review their changes (e.g., to the code, environment, etc.) before committing to trunk.",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Everyone should monitor the commit stream of their fellow team\n\nmembers so that potential conflicts can be identified and\n\nreviewed.\n\nDefine which changes qualify as high risk and may require review\n\nfrom a designated subject matter expert (e.g., database changes, security-sensitive modules such as authentication, etc.).§\n\nIf someone submits a change that is too large to reason about\n\neasily—in other words, you can’t understand its impact after\n\nreading through it a couple of times, or you need to ask the\n\nsubmitter for clarification—it should be split up into multiple,\n\nsmaller changes that can be understood at a glance.\n\nTo ensure that we are not merely rubber stamping reviews, we may also want to inspect the code review statistics to determine the number of proposed changes approved versus not approved, and perhaps sample and inspect specific code reviews.\n\nCode reviews come in various forms:\n\nPair programming: programmers work in pairs (see section\n\nbelow)\n\n“Over-the-shoulder”: One developer looks over the author’s shoulder as the latter walks through the code.\n\nEmail pass-around: A source code management system emails code to reviewers automatically after the code is checked in.\n\nTool-assisted code review: Authors and reviewers use specialized tools designed for peer code review (e.g., Gerrit, GitHub pull requests, etc.) or facilities provided by the source code repositories (e.g., GitHub, Mercurial, Subversion, as well as",
      "content_length": 1451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "other platforms such as Gerrit, Atlassian Stash, and Atlassian\n\nCrucible).\n\nClose scrutiny of changes in many forms is effective in locating\n\nerrors previously overlooked. Code reviews can facilitate increased\n\ncode commits and production deployments, and support trunk-\n\nbased deployment and continuous delivery at scale, as we will see in\n\nthe following case study.\n\nCase Study\n\nCode Reviews at Google (2010)\n\nGoogle is an excellent example of a company that employees trunk-based development and continuous delivery at scale. As noted earlier in this book, Eran Messeri described that in 2013\n\nthe processes at Google enabled over thirteen thousand developers to work off of trunk on a single source code tree, performing over 5,500 code commits per week, resulting in hundreds of production deployments per week. In 2010, there were 20+ changes being checked in to trunk every minute, resulting in 50% of the codebase being changed every month.\n\nThis requires considerable discipline from Google team\n\nmembers and mandatory code reviews, which cover the following areas:\n\nCode readability for languages (enforces style guide)\n\nOwnership assignments for code sub-trees to maintain consistency and correctness\n\nCode transparency and code contributions across teams",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "Figure 42 shows how code review lead times are affected by\n\nthe change size. On the x-axis is the size of the change, and\n\non the y-axis is the lead time required for code review process.\n\nIn general, the larger the change submitted for code reviews,\n\nthe longer the lead time required to get the necessary sign\n\noffs. And the data points in the upper-left corner represent the\n\nmore complex and potentially risky changes that required\n\nmore deliberation and discussion.\n\nFigure 42: Size of change vs. lead time for reviews at Google (Source: Ashish Kumar, “Development at the Speed and Scale of Google,” presentation at QCon, San Francisco, CA, 2010, https://qconsf.com/sf2010/dl/qcon-sanfran-\n\n2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf.)\n\nWhile he was working as a Google engineering director, Randy Shoup started a personal project to solve a technical problem that the organization was facing. He said, “I worked on that project for weeks and finally got around to asking a\n\nsubject matter expert to review my code. It was nearly three thousand lines of code, which took the reviewer days of work to go through. He told me, ‘Please don’t do that to me again.’ I was grateful that this engineer took the time to do that. That",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "was also when I learned how to make code reviews a part of\n\nmy daily work.”\n\nPOTENTIAL DANGERS OF DOING MORE MANUAL TESTING AND CHANGE FREEZES\n\nNow that we have created peer reviews that reduce our risk, shorten\n\nlead times associated with change approval processes, and enable\n\ncontinuous delivery at scale, such as we saw in the Google case\n\nstudy, let us examine the effects of how testing countermeasure can\n\nsometimes backfire. When testing failures occur, our typical reaction\n\nis to do more testing. However, if we are merely performing more testing at the end of the project, we may worsen our outcomes.\n\nThis is especially true if we are doing manual testing, because manual testing is naturally slower and more tedious than automated testing and performing “additional testing” often has the consequence of taking significantly longer to test, which means we are deploying less frequently, thus increasing our deployment batch size. And we know from both theory and practice that when we increase our deployment batch size, our change success rates go down and our incident counts and MTTR go up—the opposite of the outcome we want.\n\nInstead of performing testing on large batches of changes that are scheduled around change freeze periods, we want to fully integrate testing our daily work as part of the smooth and continual flow into production, and increase our deployment frequency. By doing this, we build in quality, which allows us to test, deploy, and release in ever smaller batch sizes.",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "ENABLE PAIR PROGRAMMING TO IMPROVE ALL OUR CHANGES\n\nPair programming is when two engineers work together at the same\n\nworkstation, a method popularized by Extreme Programming and\n\nAgile in the early 2000s. As with code reviews, this practice started\n\nin Development but is equally applicable to the work that any\n\nengineer does in our value stream. In this book, we will use the term\n\npairing and pair programming interchangeably, to indicate that the\n\npractice is not just for developers.\n\nIn one common pattern of pairing, one engineer fills the role of the\n\ndriver, the person who actually writes the code, while the other engineer acts as the navigator, observer, or pointer, the person who reviews the work as it is being performed. While reviewing, the observer may also consider the strategic direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the tactical aspects of completing the task, using the observer as a safety net and guide. When the two have differing specialties, skills are transferred as an automatic side effect, whether it’s through ad- hoc training or by sharing techniques and workarounds.\n\nAnother pair programming pattern reinforces test-driven development (TDD) by having one engineer write the automated test and the other engineer implement the code. Jeff Atwood, one of the founders of Stack Exchange, wrote, “I can’t help wondering if pair programming is nothing more than code review on steroids….The advantage of pair programming is its gripping immediacy: it is impossible to ignore the reviewer when he or she is sitting right next\n\nto you.”",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "He continued, “Most people will passively opt out [of reviewing\n\ncode] if given the choice. With pair programming, that’s not\n\npossible. Each half of the pair has to understand the code, right then\n\nand there, as it’s being written. Pairing may be invasive, but it can\n\nalso force a level of communication that you’d otherwise never\n\nachieve.”\n\nDr. Laurie Williams performed a study in 2001 that showed “paired\n\nprogrammers are 15% slower than two independent individual\n\nprogrammers, while ‘error-free’ code increased from 70% to 85%.\n\nSince testing and debugging are often many times more costly than\n\ninitial programming, this is an impressive result. Pairs typically\n\nconsider more design alternatives than programmers working alone and arrive at simpler, more maintainable designs; they also catch design defects early.” Dr. Williams also reported that 96% of her respondents stated that they enjoyed their work more when they programmed in pairs than when they programmed alone.¶\n\nPair programming has the additional benefit of spreading knowledge throughout the organization and increasing information flow within the team. Having more experienced engineers review while the less experienced engineer codes is also an effective way to teach and be taught.\n\nCase Study Pair Programming Replacing Broken Code Review Processes at Pivotal Labs (2011)\n\nElisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of Explore It!: Reduce Risk and Increase Confidence with Exploratory Testing, has spoken extensively about making every team responsible for their own quality, as",
      "content_length": 1595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "opposed to making separate departments responsible. She\n\nargues that doing so not only increase quality, but significantly\n\nincreases the flow of work into production.\n\nIn her 2015 DevOps Enterprise Summit presentation, she\n\ndescribed how in 2011, there were two accepted methods of\n\ncode review at Pivotal: pair programming (which ensured that\n\nevery line of code was inspected by two people) or a code\n\nreview process that was managed by Gerrit (which ensured\n\nthat every code commit had two designated people “+1” the\n\nchange before it was allowed into trunk).\n\nThe problem Hendrickson observed with the Gerrit code review process was that it would often take an entire week for developers to receive their required reviews. Worse, skilled developers were experiencing the “frustrating and soul crushing experience of not being able to get simple changes into the codebase, because we had inadvertently created intolerable bottlenecks.”\n\nHendrickson lamented that “the only people who had the ability to ‘+1’ the changes were senior engineers, who had many other responsibilities and often didn’t care as much about the fixes the more junior developers were working on or their productivity. It created a terrible situation—while you were waiting for your changes to get reviewed, other developers were checking in their changes. So for a week, you would have to merge all their code changes onto your laptop, re-run all the tests to ensure that everything still worked, and (sometimes) you’d have to resubmit your changes for review again!”",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "To fix the problem and eliminate all of these delays, they\n\nended up dismantling the entire Gerrit code review process,\n\ninstead requiring pair programming to implement code\n\nchanges into the system. By doing this, they reduced the\n\namount of time required to get code reviewed from weeks to\n\nhours.\n\nHendrickson is quick to note that code reviews work fine in\n\nmany organizations, but it requires a culture that values\n\nreviewing code as highly as it values writing the code in the\n\nfirst place. When that culture is not yet in place, pair\n\nprogramming can serve as a valuable interim practice.\n\nEVALUATING THE EFFECTIVENESS OF PULL REQUEST PROCESSES\n\nBecause the peer review process is an important part of our control environment, we need to be able to determine whether it is working effectively or not. One method is to look at production outages and examine the peer review process for any relevant changes.\n\nAnother method comes from Ryan Tomayko, CIO and co-founder of GitHub and one of the inventors of the pull request process. When asked to describe the difference between a bad pull request and a good pull request, he said it has little to do with the production outcome. Instead, a bad pull request is one that doesn’t have enough context for the reader, having little or no documentation of what the change is intended to do. For example, a pull request that merely has the following text: “Fixing issue #3616 and #3841.”**\n\nThat was an actual internal GitHub pull request, which Tomayko critiqued, “This was probably written by a new engineer here. First off, no specific engineers were specifically @mentioned—at a",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "minimum, the engineer should have mentioned their mentor or a\n\nsubject matter expert in the area that they’re modifying to ensure\n\nthat someone appropriate reviews their change. Worse, there isn’t\n\nany explanation of what the changes actually are, why it’s important,\n\nor exposing any of the implementer’s thinking.”\n\nOn the other hand, when asked to describe a great pull request that\n\nindicates an effective review process, Tomayko quickly listed off the\n\nessential elements: there must be sufficient detail on why the change\n\nis being made, how the change was made, as well as any identified\n\nrisks and resulting countermeasures.\n\nTomayko also looks for good discussion of the change, enabled by all the context that the pull request provided—often, there will be additional risks pointed out, ideas on better ways to implement the desired change, ideas on how to better mitigate the risk, and so\n\nforth. And if something bad or unexpected happens upon deployment, it is added to the pull request, with a link to the corresponding issue. All discussion happens without placing blame; instead, there is a candid conversation on how to prevent the problem from recurring in the future.\n\nAs an example, Tomayko produced another internal GitHub pull request for a database migration. It was many pages long, with lengthy discussions about the potential risks, leading up to the following statement by the pull request author: “I am pushing this now. Builds are now failing for the branch, because of a missing column in the CI servers. (Link to Post-Mortem: MySQL outage)”\n\nThe change submitter then apologized for the outage, describing what conditions and mistaken assumptions led to the accident, as well as a list of proposed countermeasures to prevent it from happening again. This was followed by pages and pages of",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "discussion. Reading through the pull request, Tomayko smiled,\n\n“Now that is a great pull request.”\n\nAs described above, we can evaluate the effectiveness of our peer\n\nreview process by sampling and examining pull requests, either from\n\nthe entire population of pull requests or those that are relevant to\n\nproduction incidents.\n\nFEARLESSLY CUT BUREAUCRATIC PROCESSES\n\nSo far, we have discussed peer review and pair programming processes that enable us to increase the quality of our work without relying on external approvals for changes. However, many companies still have long-standing processes for approval that require months to navigate. These approval processes can significantly increase lead times, not only preventing us from delivering value quickly to customers, but potentially increasing the risk to our organizational objectives. When this happens, we must re-engineer our processes so that we can achieve our goals more quickly and safely.\n\nAs Adrian Cockcroft observed, “A great metric to publish widely is how many meetings and work tickets are mandatory to perform a release—the goal is to relentlessly reduce the effort required for engineers to perform work and deliver it to the customer.”\n\nSimilarly, Dr. Tapabrata Pal, technical fellow at Capital One, described a program at Capital One called Got Goo?, which involves a dedicated team removing obstacles—including tools, processes, and approvals—that impede work completion. Jason Cox, Senior Director of Systems Engineering at Disney, described in his",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "presentation at the DevOps Enterprise Summit in 2015 a program\n\ncalled Join The Rebellion that aimed to remove toil and obstacles\n\nfrom daily work.\n\nAt Target in 2012, a combination of the Technology Enterprise\n\nAdoption Process and Lead Architecture Review Board (TEAP-LARB\n\nprocess) resulted in complicated, long approval times for anyone\n\nattempting to bring in new technology. The TEAP form needed to be\n\nfilled out by anyone wanting to propose new technologies to be\n\nadopted, such as a new database or monitoring technologies. These\n\nproposals were evaluated, and those deemed appropriate were put\n\nonto the monthly LARB meeting agenda.\n\nHeather Mickman and Ross Clanton, Director of Development and Director of Operations at Target, Inc., respectively, were helping to lead the DevOps movement at Target. During their DevOps initiative, Mickman had identified a technology needed to enable an initiative from the lines of business (in this case, Tomcat and Cassandra). The decision from the LARB was that Operations could not support it at the time. However, because Mickman was so convinced that this technology was essential, she proposed that her Development team be responsible for service support as well as integration, availability, and security, instead of relying on the Operations team.\n\n“As we went through the process, I wanted to better understand why the TEAP-LARB process took so long to get through, and I used the technique of ‘the five why’s’....Which eventually led to the question of why TEAP-LARB existed in the first place. The surprising thing was that no one knew, outside of a vague notion that we needed some sort of governance process. Many knew that there had been some sort of disaster that could never happen again years ago, but",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "no one could remember exactly what that disaster was, either,”\n\nMickman observed.\n\nMickman concluded that this process was not necessary for her\n\ngroup if they were responsible for the operational responsibilities of\n\nthe technology she was introducing. She added, “I let everyone know\n\nthat any future technologies that we would support wouldn’t have to\n\ngo through the TEAP-LARB process, either.”\n\nThe outcome was that Cassandra was successfully introduced inside\n\nTarget and eventually widely adopted. Furthermore, the TEAP-\n\nLARB process was eventually dismantled. Out of appreciation, her\n\nteam awarded Mickman the Lifetime Achievement Award for removing barriers to get technology work done within Target.\n\nCONCLUSION\n\nIn this chapter, we discussed how to integrate practices into our daily work that increase the quality of our changes and reduce the risk of poor deployment outcomes, reducing our reliance on approval processes. Case studies from GitHub and Target show that these practices not only improve our outcomes, but also significantly reduce lead times and increase developer productivity. To do this kind of work requires a high-trust culture.\n\nConsider a story that John Allspaw told about a newly hired junior engineer: The engineer asked if it was okay to deploy a small HTML change, and Allspaw responded, “I don’t know, is it?” He then asked “Did you have someone review your change? Do you know who the best person to ask is for changes of this type? Did you do everything you absolutely could to assure yourself that this change operates in",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "production as designed? If you did, then don’t ask me—just make\n\nthe change!”\n\nBy responding this way, Allspaw reminded the engineer that she was\n\nsolely responsibility for the quality of her change—if she did\n\neverything she felt she could to give herself confidence that the\n\nchange would work, then she didn’t need to ask anyone for approval,\n\nshe should make the change.\n\nCreating the conditions that enable change implementers to fully\n\nown the quality of their changes is an essential part of the high-trust,\n\ngenerative culture we are striving to build. Furthermore, these\n\nconditions enable us to create an ever-safer system of work, where we are all helping each other achieve our goals, spanning whatever boundaries necessary to get there.",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "PART IV CONCLUSION\n\nPart IV has shown us that by implementing feedback loops we can\n\nenable everyone to work together toward shared goals, see problems\n\nas they occur, and, with quick detection and recovery, ensure that\n\nfeatures not only operate as designed in production, but also achieve\n\norganizational goals and organizational learning. We have also\n\nexamined how to enable shared goals spanning Dev and Ops so that\n\nthey can improve the health of the entire value stream.\n\nWe are now ready to enter Part V: The Third Way, The Technical\n\nPractices of Learning, so we can create opportunities for learning that happen earlier and ever more quickly and cheaply, and so that we can unleash a culture of innovation and experimentation that enables everyone to do meaningful work that helps our organization succeed.\n\n† Counterfactual thinking is a term used in psychology that involves the human tendency to create possible\n\nalternatives to life events that have already occurred. In reliability engineering, it often involves narratives of the “system as imagined” as opposed to the “system in reality.”\n\n‡ In this book, the terms code review and change review will be used interchangeably.\n\n§ Incidentally, a list of high-risk areas of code and environments has likely already been created by the change\n\nadvisory board.\n\n¶ Some organizations may require pair programming, while in others, engineers find someone to pair program with\n\nwhen working in areas where they want more scrutiny (such as before checking in) or for challenging tasks. Another common practice is to set pairing hours for a subset of the working day, perhaps four hours from mid- morning to mid-afternoon.\n\n** Gene Kim is grateful to Shawn Davenport, James Fryman, Will Farr, and Ryan Tomayko at GitHub for discussing\n\nthe differences between good and bad pull requests.",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Part\n\nIntroduction\n\nIn Part III, The First Way: The Technical Practices of Flow, we\n\ndiscussed implementing the practices required to create fast flow\n\nin our value stream. In Part IV, The Second Way: The Technical\n\nPractices of Feedback, our goal was to create as much feedback as possible, from as many areas in our system as possible—sooner,\n\nfaster, and cheaper.\n\nIn Part V, The Third Way: The Technical Practices of Learning, we present the practices that create opportunities for learning, as\n\nquickly, frequently, cheaply, and as soon as possible. This includes\n\ncreating learnings from accidents and failures, which are inevitable when we work within complex systems, as well as\n\norganizing and designing our systems of work so that we are constantly experimenting and learning, continually making our\n\nsystems safer. The results include higher resilience and an ever- growing collective knowledge of how our system actually works, so\n\nthat we are better able to achieve our goals.\n\nIn the following chapters, we will institutionalize rituals that\n\nincrease safety, continuous improvement, and learning by doing the following:\n\nEstablish a just culture to make safety possible",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Inject production failures to create resilience\n\nConvert local discoveries into global improvements\n\nReserve time to create organizational improvements and\n\nlearning\n\nWe will also create mechanisms so that any new learnings\n\ngenerated in one area of the organization can be rapidly used\n\nacross the entire organization, turning local improvements into\n\nglobal advancements. In this way, we not only learn faster than\n\nour competition, helping us win in the marketplace, but also\n\ncreate a safer, more resilient work culture that people are excited to be a part of and that helps them achieve their highest potential.",
      "content_length": 616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "19Enable and Inject Learning into Daily Work\n\nWhen we work within a complex system, it is impossible for us to\n\npredict all the outcomes for the actions we take. This contributes\n\nto unexpected and sometimes catastrophic accidents, even when\n\nwe use static precautionary tools, such as checklists and runbooks,\n\nwhich codify our current understanding of the system.\n\nTo enable us to safely work within complex systems, our\n\norganizations must become ever better at self-diagnostics and self-\n\nimprovement and must be skilled at detecting problems, solving them, and multiplying the effects by making the solutions\n\navailable throughout the organization. This creates a dynamic system of learning that allows us to understand our mistakes and\n\ntranslate that understanding into actions that prevent those\n\nmistakes from recurring in the future.\n\nThe result is what Dr. Steven Spear describes as resilient organizations, who are “skilled at detecting problems, solving them, and multiplying the effect by making the solutions available\n\nthroughout the organization.” These organizations can heal themselves. “For such an organization, responding to crises is not idiosyncratic work. It is something that is done all the time. It is this responsiveness that is their source of reliability.”",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "A striking example of the incredible resilience that can result from\n\nthese principles and practices was seen on April 21, 2011, when the\n\nentire Amazon AWS US-EAST availability zone went down, taking\n\ndown virtually all of their customers who depended on it, including Reddit and Quora.† However, Netflix was a surprising exception, seemingly unaffected by this massive AWS outage.\n\nFollowing the event, there was considerable speculation about\n\nhow Netflix kept their services running. A popular theory was\n\nsince Netflix was one of the largest customers of Amazon Web\n\nServices, it was given some special treatment that allowed them to\n\nkeep running. However, a Netflix Engineering blog post explained that it was their architectural design decisions in 2009 enabled their exceptional resilience.\n\nBack in 2008, Netflix’s online video delivery service ran on a monolithic J2EE application hosted in one of their data centers. However, starting in 2009, they began re-architecting this system to be what they called cloud native—it was designed to run entirely in the Amazon public cloud and to be resilient enough to survive significant failures.\n\nOne of their specific design objectives was to ensure Netflix\n\nservices kept running, even if an entire AWS availability zone went\n\ndown, such as happened with US-EAST. To do this required that their system be loosely-coupled, with each component having aggressive timeouts to ensure that failing components didn’t bring the entire system down.Instead, each feature and component was designed to gracefully degrade. For example, during traffic surges that created CPU-usage spikes, instead of showing a list of movies\n\npersonalized to the user, they would show static content, such as",
      "content_length": 1735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "cached or un-personalized results, which required less\n\ncomputation.\n\nFurthermore, the blog post explained that, in addition to\n\nimplementing these architectural patterns, they also built and had\n\nbeen running a surprising and audacious service called Chaos\n\nMonkey, which simulated AWS failures by constantly and\n\nrandomly killing production servers. They did so because they\n\nwanted all “engineering teams to be used to a constant level of\n\nfailure in the cloud” so that services could “automatically recover\n\nwithout any manual intervention.”\n\nIn other words, the Netflix team ran Chaos Monkey to gain assurance that they had achieved their operational resilience objectives, constantly injecting failures into their pre-production and production environments.\n\nAs one might expect, when they first ran Chaos Monkey in their production environments, services failed in ways they never could\n\nhave predicted or imagined—by constantly finding and fixing these issues during normal working hours, Netflix engineers quickly and iteratively created a more resilient service, while simultaneously creating organizational learnings (during normal\n\nworking hours!) that enabled them to evolve their systems far beyond their competition.\n\nChaos Monkey is just one example of how learning can be integrated into daily work. The story also shows how learning\n\norganizations think about failures, accidents, and mistakes—as an opportunity for learning and not something to be punished. This chapter explores how to create a system of learning and how to",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "establish a just culture, as well as how to routinely rehearse and deliberately create failures to accelerate learning.\n\nESTABLISH A JUST, LEARNING CULTURE\n\nOne of the prerequisites for a learning culture is that when accidents occur (which they undoubtedly will), the response to\n\nthose accidents is seen as “just.” Dr. Sidney Dekker, who helped codify some of the key elements of safety culture and coined the\n\nterm just culture, writes, “When responses to incidents and accidents are seen as unjust, it can impede safety investigations,\n\npromoting fear rather than mindfulness in people who do safety-\n\ncritical work, making organizations more bureaucratic rather than more careful, and cultivating professional secrecy, evasion, and\n\nself-protection.”\n\nThis notion of punishment is present, either subtly or prominently, in the way many managers have operated during the\n\nlast century. The thinking goes, in order to achieve the goals of the organization, leaders must command, control, establish\n\nprocedures to eliminate errors, and enforce compliance of those\n\nprocedures.\n\nDr. Dekker calls this notion of eliminating error by eliminating the\n\npeople who caused the errors the Bad Apple Theory. He asserts\n\nthat this is invalid, because “human error is not our cause of troubles; instead, human error is a consequence of the design of\n\nthe tools that we gave them.”\n\nIf accidents are not caused by “bad apples,” but rather are due to inevitable design problems in the complex system that we created,",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "then instead of “naming, blaming, and shaming” the person who caused the failure, our goal should always be to maximize\n\nopportunities for organizational learning, continually reinforcing that we value actions that expose and share more widely the\n\nproblems in our daily work. This is what enables us to improve the quality and safety of the system we operate within and reinforce\n\nthe relationships between everyone who operates within that\n\nsystem.\n\nBy turning information into knowledge and building the results of the learning into our systems, we start to achieve the goals of a\n\njust culture, balancing the needs for safety and accountability. As John Allspaw, CTO of Etsy, states, “Our goal at Etsy is to view\n\nmistakes, errors, slips, lapses, and so forth with a perspective of learning.”\n\nWhen engineers make mistakes and feel safe when giving details\n\nabout it, they are not only willing to be held accountable, but they\n\nare also enthusiastic in helping the rest of the company avoid the same error in the future. This is what creates organizational\n\nlearning. On the other hand, if we punish that engineer, everyone is dis-incentivized to provide the necessary details to get an\n\nunderstanding of the mechanism, pathology, and operation of the failure, which guarantees that the failure will occur again.\n\nTwo effective practices that help create a just, learning-based\n\nculture are blameless post-mortems and the controlled\n\nintroduction of failures into production to create opportunities to practice for the inevitable problems that arise within complex\n\nsystems. We will first look at blameless post-mortems and follow that with an exploration of why failure can be a good thing.",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "SCHEDULE BLAMELESS POST-MORTEM MEETINGS AFTER ACCIDENTS OCCUR\n\nTo help enable a just culture, when accidents and significant\n\nincidents occur (e.g., failed deployment, production issue that affected customers), we should conduct a blameless post-mortem after the incident has been resolved.‡ Blameless post-mortems, a term coined by John Allspaw, help us examine “mistakes in a way that focuses on the situational aspects of a failure’s mechanism\n\nand the decision-making process of individuals proximate to the failure.”\n\nTo do this, we schedule the post-mortem as soon as possible after\n\nthe accident occurs and before memories and the links between cause and effect fade or circumstances change. (Of course, we wait\n\nuntil after the problem has been resolved so as not to distract the\n\npeople who are still actively working on the issue.)\n\nIn the blameless post-mortem meeting, we will do the following:\n\nConstruct a timeline and gather details from multiple\n\nperspectives on failures, ensuring we don’t punish people for\n\nmaking mistakes\n\nEmpower all engineers to improve safety by allowing them to give detailed accounts of their contributions to failures\n\nEnable and encourage people who do make mistakes to be the experts who educate the rest of the organization on how not to\n\nmake them in the future",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Accept that there is always a discretionary space where humans can decide to take action or not, and that the judgment of those\n\ndecisions lies in hindsight\n\nPropose countermeasures to prevent a similar accident from happening in the future and ensure these countermeasures are\n\nrecorded with a target date and an owner for follow-up\n\nTo enable us to gain this understanding, the following\n\nstakeholders need to be present at the meeting:\n\nThe people involved in decisions that may have contributed to the problem\n\nThe people who identified the problem\n\nThe people who responded to the problem\n\nThe people who diagnosed the problem\n\nThe people who were affected by the problem\n\nAnd anyone else who is interested in attending the meeting.\n\nOur first task in the blameless post-mortem meeting is to record our best understanding of the timeline of relevant events as they\n\noccurred. This includes all actions we took and what time (ideally supported by chat logs, such as IRC or Slack), what effects we observed (ideally in the form of the specific metrics from our\n\nproduction telemetry, as opposed to merely subjective narratives), all investigation paths we followed, and what resolutions were considered.",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "To enable these outcomes, we must be rigorous about recording details and reinforcing a culture that information can be shared\n\nwithout fear of punishment or retribution. Because of this, especially for our first few post-mortems, it may be helpful to have the meeting led by a trained facilitator who wasn’t involved in the\n\naccident.\n\nDuring the meeting and the subsequent resolution, we should explicitly disallow the phrases “would have” or “could have,” as\n\nthey are counterfactual statements that result from our human tendency to create possible alternatives to events that have already occurred.\n\nCounterfactual statements, such as “I could have...” or “If I had known about that, I should have…,” frame the problem in terms of the system as imagined instead of in terms of the system that actually exists, which is the context we need to restrict ourselves\n\nto. See Appendix 8.\n\nOne of the potentially surprising outcomes of these meetings is\n\nthat people will often blame themselves for things outside of their control or question their own abilities. Ian Malpass, an engineer at Etsy observes, “In that moment when we do something that causes the entire site to go down, we get this ‘ice water down the spine’\n\nfeeling, and likely the first thought through our head is, ‘I suck and I have no idea what I’m doing.’ We need to stop ourselves from doing that, as it is route to madness, despair, and feelings of being\n\nan imposter, which is something that we can’t let happen to good engineers. The better question to focus on is, ‘Why did it make sense to me when I took that action?’”",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "In the meeting, we must reserve enough time for brainstorming and deciding which countermeasures to implement. Once the\n\ncountermeasures have been identified, they must be prioritized and given an owner and timeline for implementation. Doing this further demonstrates that we value improvement of our daily work more than daily work itself.\n\nDan Milstein, one of the principal engineers at Hubspot, writes that he begins all blameless post-mortem meetings “by saying, ‘We’re trying to prepare for a future where we’re as stupid as we\n\nare today.’” In other words, it is not acceptable to have a countermeasure to merely “be more careful” or “be less stupid”— instead, we must design real countermeasures to prevent these\n\nerrors from happening again.\n\nExamples of such countermeasures include new automated tests to detect dangerous conditions in our deployment pipeline, adding\n\nfurther production telemetry, identifying categories of changes that require additional peer review, and conducting rehearsals of this category of failure as part of regularly scheduled Game Day\n\nexercises.\n\nPUBLISH OUR POST-MORTEMS AS WIDELY AS POSSIBLE\n\nAfter we conduct a blameless post-mortem meeting, we should widely announce the availability of the meeting notes and any\n\nassociated artifacts (e.g., timelines, IRC chat logs, external communications). This information should (ideally) be placed in a centralized location where our entire organization can access it\n\nand learn from the incident. Conducting post-mortems is so",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "important that we may even prohibit production incidents from being closed until the post-mortem meeting has been completed.\n\nDoing this helps us translate local learnings and improvements into global learnings and improvements. Randy Shoup, former engineering director for Google App Engine, describes how\n\ndocumentation of post-mortem meetings can have tremendous value to others in the organization, “As you can imagine at Google, everything is searchable. All the post-mortem documents are in places where other Googlers can see them. And trust me, when\n\nany group has an incident that sounds similar to something that happened before, these post-mortem documents are among the first documents being read and studied.”§\n\nWidely publishing post-mortems and encouraging others in the organization to read them increases organizational learning, and it also becoming increasingly commonplace for online service\n\ncompanies to publish post-mortems for customer-impacting outages. This often significantly increases the transparency we have with our internal and external customers, which in turn increases their trust in us.\n\nThis desire to conduct as many blameless post-mortem meetings as necessary at Etsy led to some problems—over the course of four\n\nyears, Etsy accumulated a large number of post-mortem meeting notes in wiki pages, which became increasingly difficult to search, save, and collaborate from.\n\nTo help with this issue, they developed a tool called Morgue to easily record aspects of each accident, such as the incident MTTR and severity, better address time zones (which became relevant as more Etsy employees were working remotely), and include other",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "data, such as rich text in Markdown format, embedded images, tags, and history.\n\nMorgue was designed to make it easy for the team to record:\n\nWhether the problem was due to a scheduled or an unscheduled incident\n\nThe post-mortem owner\n\nRelevant IRC chat logs (especially important for 3 a.m. issues\n\nwhen accurate note-taking may not happen)\n\nRelevant JIRA tickets for corrective actions and their due dates (information particularly important to management)\n\nLinks to customer forum posts (where customers complain about issues)\n\nAfter developing and using Morgue, the number of recorded post- mortems at Etsy increased significantly compared to when they used wiki pages, especially for P2, P3, and P4 incidents (i.e., lower\n\nseverity problems). This result reinforced the hypothesis that if they made it easier to document post-mortems through tools such as Morgue, more people would record and detail the outcomes of their post-mortem meetings, enabling more organizational\n\nlearning.\n\nDr. Amy C. Edmondson, Novartis Professor of Leadership and Management at Harvard Business School and co-author of\n\nBuilding the Future: Big Teaming for Audacious Innovation, writes:",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "Again, the remedy—which does not necessarily involve much time and expense—is to reduce the stigma of failure. Eli Lilly has done this since the early 1990s by holding ‘failure parties’\n\nto honor intelligent, high-quality scientific experiments that fail to achieve the desired results. The parties don’t cost much, and redeploying valuable resources—particularly scientists—to new projects earlier rather than later can save hundreds of\n\nthousands of dollars, not to mention kickstart potential new discoveries.\n\nDECREASE INCIDENT TOLERANCES TO FIND EVER-WEAKER FAILURE SIGNALS\n\nInevitably, as organizations learn how to see and solve problems efficiently, they need to decrease the threshold of what constitutes a problem in order to keep learning. To do this, we seek to amplify weak failure signals. For example, as described in chapter 4, when\n\nAlcoa was able to reduce the rate of workplace accidents so that they were no longer commonplace, Paul O’Neill, CEO of Alcoa, started to be notified of accident near-misses in addition to actual\n\nworkplace accidents.\n\nDr. Spear summarizes O’Neill’s accomplishments at Alcoa when he writes, “Though it started by focusing on problems related to\n\nworkplace safety, it soon found that safety problems reflected process ignorance and that this ignorance would also manifest\n\nitself in other problems such as quality, timeliness, and yield\n\nversus scrap.”\n\nWhen we work within complex systems, this need to amplify weak\n\nfailure signals is critical to averting catastrophic failures. The way",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "NASA handled failure signals during the space shuttle era serves\n\nas an illustrative example: In 2003, sixteen days into the\n\nColumbia space shuttle mission, it exploded as it re-entered the earth’s atmosphere. We now know that a piece of insulating foam\n\nhad broken off the external fuel tank during takeoff.\n\nHowever, prior to Columbia’s re-entry, a handful of mid-level NASA engineers had reported this incident, but their voices had\n\ngone unheard. They observed the foam strike on video monitors\n\nduring a post-launch review session and immediately notified NASA’s managers, but they were told that the foam issue was\n\nnothing new. Foam dislodgement had damaged shuttles in\n\nprevious launches, but had never resulted in an accident. It was considered a maintenance problem and not acted upon until it was\n\ntoo late.\n\nMichael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson wrote in a 2006 article for Harvard Business Review how NASA\n\nculture contributed to this problem. They describe how organizations are typically structured in one of two models: a\n\nstandardized model, where routine and systems govern\n\neverything, including strict compliance with timelines and budgets, or an experimental model, where every day every\n\nexercise and every piece of new information is evaluated and\n\ndebated in a culture that resembles a research and design (R&D) laboratory.\n\nThey observe, “Firms get into trouble when they apply the wrong\n\nmind-set to an organization [which dictates how they respond to ambiguous threats or, in the terminology of this book, weak\n\nfailure signals]....By the 1970s, NASA had created a culture of\n\nrigid standardization, promoting to Congress the space shuttle as",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "a cheap and reusable spacecraft.” NASA favored strict process\n\ncompliance instead of an experimental model where every piece of\n\ninformation needed to be evaluated as it occured without bias. The absence of continuous learning and experimentation had dire\n\nconsequences. The authors conclude that it is culture and mind-\n\nset that matters, not just “being careful”—as they write, “vigilance alone will not prevent ambiguous threats [weak failure signals]\n\nfrom turning into costly (and sometimes tragic) failures.”\n\nOur work in the technology value stream, like space travel, should be approached as a fundamentally experimental endeavor and\n\nmanaged that way. All work we do is a potentially important hypothesis and a source of data, rather than a routine application\n\nand validation of past practice. Instead of treating technology\n\nwork as entirely standardized, where we strive for process compliance, we must continually seek to find ever-weaker failure\n\nsignals so that we can better understand and manage the system\n\nwe operate in.\n\nREDEFINE FAILURE AND ENCOURAGE CALCULATED RISK-TAKING\n\nLeaders of an organization, whether deliberately or inadvertently,\n\nreinforce the organizational culture and values through their\n\nactions. Audit, accounting, and ethics experts have long observed that the “tone at the top” predicts the likelihood of fraud and other\n\nunethical practices. To reinforce our culture of learning and\n\ncalculated risk-taking, we need leaders to continually reinforce that everyone should feel both comfortable with and responsible\n\nfor surfacing and learning from failures.",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "On failures, Roy Rapoport from Netflix observes, “What the 2014\n\nState of DevOps Report proved to me is that high-performing\n\nDevOps organizations will fail and make mistakes more often. Not only is this okay, it’s what organizations need! You can even see it\n\nin the data: if high performers are performing thirty times more frequently but with only half the change failure rate, they’re\n\nobviously having more failures.”\n\nHe continues, “I was talking with a co-worker about a massive\n\noutage we just had at Netflix—it was caused by, frankly, a dumb mistake. In fact, it was caused by an engineer who had taken down\n\nNetflix twice in the last eighteen months. But, of course, this is a person we’d never fire. In that same eighteen months, this\n\nengineer moved the state of our operations and automation\n\nforward not by miles but by light-years. That work has enabled us to do deployments safely on a daily basis, and has personally\n\nperformed huge numbers of production deployments.”\n\nHe concludes, “DevOps must allow this sort of innovation and the resulting risks of people making mistakes. Yes, you’ll have more\n\nfailures in production. But that’s a good thing, and should not be\n\npunished.”\n\nINJECT PRODUCTION FAILURES TO ENABLE RESILIENCE AND LEARNING\n\nAs we saw in the chapter introduction, injecting faults into the\n\nproduction environment (such as Chaos Monkey) is one way we\n\ncan increase our resilience. In this section, we describe the processes involved in rehearsing and injecting failures into our\n\nsystem to confirm that we have designed and architected our",
      "content_length": 1574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "systems properly, so that failures happen in specific and\n\ncontrolled ways. We do this by regularly (or even continuously)\n\nperforming tests to make certain that our systems fail gracefully.\n\nAs Michael Nygard, author of Release It! Design and Deploy\n\nProduction-Ready Software, comments, “Like building crumple\n\nzones into cars to absorb impacts and keep passengers safe, you can decide what features of the system are indispensable and build\n\nin failure modes that keep cracks away from those features. If you do not design your failure modes, then you will get whatever\n\nunpredictable—and usually dangerous—ones happen to emerge.”\n\nResilience requires that we first define our failure modes and then\n\nperform testing to ensure that these failure modes operate as designed. One way we do this is by injecting faults into our\n\nproduction environment and rehearsing large-scale failures so we are confident we can recover from accidents when they occur,\n\nideally without even impacting our customers.\n\nThe 2012 story about Netflix and the Amazon AWS-EAST outage presented in the introduction is just one example. An even more\n\ninteresting example of resilience at Netflix was during the “Great\n\nAmazon Reboot of 2014,” when nearly 10% of the entire Amazon EC2 server fleet had to be rebooted to apply an emergency Xen\n\nsecurity patch. As Christos Kalantzis of Netflix Cloud Database\n\nEngineering recalled, “When we got the news about the emergency EC2 reboots, our jaws dropped. When we got the list of how many\n\nCassandra nodes would be affected, I felt ill.”But, Kalantzis\n\ncontinues, “Then I remembered all the Chaos Monkey exercises we’ve gone through. My reaction was, ‘Bring it on!’”",
      "content_length": 1689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Once again, the outcomes were astonishing. Of the 2,700+\n\nCassandra nodes used in production, 218 were rebooted, and twenty-two didn’t reboot successfully. As Kalantzis and Bruce\n\nWong from Netflix Chaos Engineering wrote, “Netflix experienced\n\n0 downtime that weekend. Repeatedly and regularly exercising failure, even in the persistence [database] layer, should be part of\n\nevery company’s resilience planning. If it wasn’t for Cassandra’s\n\nparticipation in Chaos Monkey, this story would have ended much differently.”\n\nEven more surprising, not only was no one at Netflix working\n\nactive incidents due to failed Cassandra nodes, no one was even in the office—they were in Hollywood at a party celebrating an\n\nacquisition milestone. This is another example demonstrating that\n\nproactively focusing on resilience often means that a firm can handle events that may cause crises for most organizations in a manner that is routine and mundane.¶ See Appendix 9.\n\nINSTITUTE GAME DAYS TO REHEARSE FAILURES\n\nIn this section, we describe specific disaster recovery rehearsals called Game Days, a term popularized by Jesse Robbins, one of\n\nthe founders of the Velocity Conference community and co-\n\nfounder of Chef, for the work he did at Amazon, where he was responsible for programs to ensure site availability and was widely\n\nknown internally as the “Master of Disaster.”The concept of Game\n\nDays comes from the discipline of resilience engineering. Robbins defines resilience engineering as “an exercise designed to increase",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "resilience through large-scale fault injection across critical\n\nsystems.”\n\nRobbins observes that “whenever you set out to engineer a system at scale, the best you can hope for is to build a reliable software\n\nplatform on top of components that are completely unreliable. That puts you in an environment where complex failures are both\n\ninevitable and unpredictable.”\n\nConsequently, we must ensure that services continue to operate when failures occur, potentially throughout our system, ideally\n\nwithout crisis or even manual intervention. As Robbins quips, “a\n\nservice is not really tested until we break it in production.”\n\nOur goal for Game Day is to help teams simulate and rehearse accidents to give them the ability to practice. First, we schedule a\n\ncatastrophic event, such as the simulated destruction of an entire data center, to happen at some point in the future. We then give\n\nteams time to prepare, to eliminate all the single points of failure,\n\nand to create the necessary monitoring procedures, failover procedures, etc.\n\nOur Game Day team defines and executes drills, such as\n\nconducting database failovers (i.e., simulating a database failure and ensuring that the secondary database works) or turning off an\n\nimportant network connection to expose problems in the defined\n\nprocesses. Any problems or difficulties that are encountered are identified, addressed, and tested again.\n\nAt the scheduled time, we then execute the outage. As Robbins\n\ndescribes, at Amazon they “would literally power off a facility—",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "without notice—and then let the systems fail naturally and [allow]\n\nthe people to follow their processes wherever they led.”\n\nBy doing this, we start to expose the latent defects in our system,\n\nwhich are the problems that appear only because of having\n\ninjected faults into the system. Robbins explains, “You might discover that certain monitoring or management systems crucial\n\nto the recovery process end up getting turned off as part of the\n\nfailure you’ve orchestrated. [Or] you would find some single points of failure you didn’t know about that way.” These exercises\n\nare then conducted in an increasingly intense and complex way\n\nwith the goal of making them feel like just another part of an average day.\n\nBy executing Game Days, we progressively create a more resilient\n\nservice and a higher degree of assurance that we can resume operations when inopportune events occur, as well create more\n\nlearnings and a more resilient organization.\n\nAn excellent example of simulating disaster is Google’s Disaster Recovery Program (DiRT). Kripa Krishnan is a technical program\n\ndirector at Google, and, at the time of this writing, has led the\n\nprogram for over seven years. During that time, they’ve simulated an earthquake in Silicon Valley, which resulted in the entire\n\nMountain View campus being disconnected from Google; major\n\ndata centers having complete loss of power; and even aliens attacking cities where engineers resided.\n\nAs Krishnan wrote, “An often-overlooked area of testing is\n\nbusiness process and communications. Systems and processes are highly intertwined, and separating testing of systems from testing\n\nof business processes isn’t realistic: a failure of a business system",
      "content_length": 1698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "will affect the business process, and conversely a working system\n\nis not very useful without the right personnel.”\n\nSome of the learnings gained during these disasters included:\n\nWhen connectivity was lost, the failover to the engineer\n\nworkstations didn’t work\n\nEngineers didn’t know how to access a conference call bridge or the bridge only had capacity for fifty people or they needed a\n\nnew conference call provider who would allow them to kick off\n\nengineers who had subjected the entire conference to hold music\n\nWhen the data centers ran out of diesel for the backup\n\ngenerators, no one knew the procedures for making emergency purchases through the supplier, resulting in someone using a\n\npersonal credit card to purchase $50,000 worth of diesel.\n\nBy creating failure in a controlled situation, we can practice and create the playbooks we need. One of the other outputs of Game\n\nDays is that people actually know who to call and know who to talk\n\nto—by doing this, they develop relationships with people in other departments so they can work together during an incident,\n\nturning conscious actions into unconscious actions that are able to become routine.\n\nCONCLUSION\n\nTo create a just culture that enables organizational learning, we\n\nhave to re-contextualize so-called failures. When treated properly, errors that are inherent in complex systems can create a dynamic",
      "content_length": 1377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "learning environment where all of the shareholders feel safe\n\nenough to come forward with ideas and observations, and where groups rebound more readily from projects that don’t perform as\n\nexpected.\n\nBoth blameless post-mortems and injecting production failures reinforce a culture that everyone should feel both comfortable\n\nwith and responsible for surfacing and learning from failures. In\n\nfact, when we sufficiently reduce the number of accidents, we decrease our tolerance so that we can keep learning. As Peter\n\nSenge is known to say, “The only sustainable competitive\n\nadvantage is an organization’s ability to learn faster than the competition.”\n\n† In January 2013 at re:Invent, James Hamilton, VP and Distinguished Engineer for Amazon Web Services said that the US East region had more than ten data centers all by itself, and added that a typical data center has between fifty thousand and eighty thousand servers. By this math, the 2011 EC2 outage affected customers on more than half a million servers.\n\n‡ This practice has also been called blameless post-incident reviews as well as post-event retrospectives. There is\n\nalso a noteworthy similarity to the routine retrospectives that are a part of many iterative and agile development practices.\n\n§ We may also choose to extend the philosophies of Transparent Uptime to our post-mortem reports and, in\n\naddition to making a service dashboard available to the public, we may choose to publish (maybe sanitized) post-mortem meetings to the public. Some of the most widely admired public post-mortems include those posted by the Google App Engine team after a significant 2010 outage, as well as the post-mortem of the 2015 Amazon DynamoDB outage. Interestingly, Chef publishes their post-mortem meeting notes on their blog, as well as recorded videos of the actual post-mortem meetings.\n\n¶ Specific architectural patterns that they implemented included fail fasts (setting aggressive timeouts such that failing components don’t make the entire system crawl to a halt), fallbacks (designing each feature to degrade or fall back to a lower quality representation), and feature removal (removing non-critical features when they run slowly from any given page to prevent them from impacting the member experience). Another astonishing example of the resilience that the Netflix team created beyond preserving business continuity during the AWS outage, was that Netflix went over six hours into the AWS outage before declaring a Sev 1 incident, assuming that AWS service would eventually be restored (i.e., “AWS will come back… it usually does, right?”). Only after six hours into the outage did they activate any business continuity procedures.",
      "content_length": 2701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "20Convert Local Discoveries into Global Improvements\n\nIn the previous chapter, we discussed developing a safe learning\n\nculture by encouraging everyone to talk about mistakes and\n\naccidents through blameless post-mortems. We also explored\n\nfinding and fixing ever-weaker failure signals, as well as reinforcing and rewarding experimentation and risk-taking.\n\nFurthermore, we helped make our system of work more resilient by proactively scheduling and testing failure scenarios, making\n\nour systems safer by finding latent defects and fixing them.\n\nIn this chapter, we will create mechanisms that make it possible for new learnings and improvements discovered locally to be\n\ncaptured and shared globally throughout the entire organization, multiplying the effect of global knowledge and improvement. By\n\ndoing this, we elevate the state of the practice of the entire\n\norganization so that everyone doing work benefits from the cumulative experience of the organization.\n\nUSE CHAT ROOMS AND CHAT BOTS TO AUTOMATE AND CAPTURE ORGANIZATIONAL KNOWLEDGE",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "Many organizations have created chat rooms to facilitate fast\n\ncommunication within teams. However, chat rooms are also used\n\nto trigger automation.\n\nThis technique was pioneered in the ChatOps journey at GitHub.\n\nThe goal was to put automation tools into the middle of the\n\nconversation in their chat rooms, helping create transparency and\n\ndocumentation of their work. As Jesse Newland, a systems\n\nengineer at GitHub, describes, “Even when you’re new to the\n\nteam, you can look in the chat logs and see how everything is\n\ndone. It’s as if you were pair-programming with them all the\n\ntime.”\n\nThey created Hubot, a software application that interacted with the Ops team in their chat rooms, where it could be instructed to\n\nperform actions merely by sending it a command (e.g., “@hubot deploy owl to production”). The results would also be sent back into the chat room.\n\nHaving this work performed by automation in the chat room (as opposed to running automated scripts via command line) had numerous benefits, including:\n\nEveryone saw everything that was happening.\n\nEngineers on their first day of work could see what daily work looked like and how it was performed.\n\nPeople were more apt to ask for help when they saw others helping each other.\n\nRapid organizational learning was enabled and accumulated.",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Furthermore, beyond the above tested benefits, chat rooms\n\ninherently record and make all communications public; in\n\ncontrast, emails are private by default, and the information in\n\nthem cannot easily be discovered or propagated within an\n\norganization.\n\nIntegrating our automation into chat rooms helps document and\n\nshare our observations and problem solving as an inherent part of\n\nperforming our work. This reinforces a culture of transparency\n\nand collaboration in everything we do.\n\nThis is also an extremely effective way of converting local learning\n\nto global knowledge. At Github, all the Operations staff worked remotely—in fact, no two engineers worked in the same city. As Mark Imbriaco, former VP of Operations at GitHub, recalls, “There was no physical water cooler at GitHub. The chat room was the water cooler.”\n\nGithub enabled Hubot to trigger their automation technologies,\n\nincluding Puppet, Capistrano, Jenkins, resque (a Redis-backed library for creating background jobs), and graphme (which generates graphs from Graphite).\n\nActions performed through Hubot included checking the health of services, doing puppet pushes or code deployments into production, and muting alerts as services went into maintenance mode. Actions that were performed multiple times, such as pulling up the smoke test logs when a deployment failed, taking\n\nproduction servers out of rotation, reverting to master for production front-end services, or even apologizing to the engineers who were on call, also became Hubot actions.†",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Similarly, commits to the source code repository and the commands that trigger production deployments both emit\n\nmessages to the chat room. Additionally, as changes move through the deployment pipeline, their status is posted in the chat room.\n\nA typical quick chat room exchange might look like:\n\n“@sr: @jnewland, how do you get that list of big repos?\n\ndisk_hogs or something?”\n\n“@jnewland: /disk-hogs”\n\nNewland observes that certain questions that were previously asked during the course of a project are rarely asked now. For\n\nexample, engineers may ask each other, “How is that deploy\n\ngoing?” or “Are you deploying that, or should I?” or “How does the load look?”\n\nAmong all the benefits that Newland describes, which include\n\nfaster onboarding of newer engineers and making all engineers more productive, the result that he felt was most important was\n\nthat Ops work became more humane as Ops engineers were enabled to discover problems and help each other quickly and\n\neasily.\n\nGitHub created an environment for collaborative local learning\n\nthat could be transformed into learnings across the organization. Throughout the rest of this chapter we will explore ways to create\n\nand accelerate the spread of new organizational learnings.\n\nAUTOMATE STANDARDIZED PROCESSES IN SOFTWARE FOR RE-USE",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "All too often, we codify our standards and processes for architecture, testing, deployment, and infrastructure management\n\nin prose, storing them in Word documents that are uploaded somewhere. The problem is that engineers who are building new\n\napplications or environments often don’t know that these documents exist, or they don’t have the time to implement the\n\ndocumented standards. The result is they create their own tools\n\nand processes, with all the disappointing outcomes we’d expect: fragile, insecure, and unmaintainable applications and\n\nenvironments that are expensive to run, maintain, and evolve.\n\nInstead of putting our expertise into Word documents, we need to transform these documented standards and processes, which\n\nencompass the sum of our organizational learnings and knowledge, into an executable form that makes them easier to\n\nreuse. One of the best ways we can make this knowledge re-usable\n\nis by putting it into a centralized source code repository, making the tool available for everyone to search and use.\n\nJustin Arbuckle was chief architect at GE Capital in 2013 when he\n\nsaid, “We needed to create a mechanism that would allow teams to easily comply with policy—national, regional, and industry\n\nregulations across dozens of regulatory frameworks, spanning thousands of applications running on tens of thousands of servers\n\nin tens of data centers.”\n\nThe mechanism they created was called ArchOps, which “enabled\n\nour engineers to be builders, not bricklayers. By putting our design standards into automated blueprints that were able to be\n\nused easily by anyone, we achieved consistency as a byproduct.”",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "By encoding our manual processes into code that is automated\n\nand executed, we enable the process to be widely adopted,\n\nproviding value to anyone who uses them. Arbuckle concluded that “the actual compliance of an organization is in direct\n\nproportion to the degree to which its policies are expressed as code.”\n\nBy making this automated process the easiest means to achieve\n\nthe goal, we allow practices to be widely adopted—we may even consider turning them into shared services supported by the\n\norganization.\n\nCREATE A SINGLE, SHARED SOURCE CODE REPOSITORY FOR OUR ENTIRE ORGANIZATION\n\nA firm-wide, shared source code repository is one of the most powerful mechanisms used to integrate local discoveries across\n\nthe entire organization. When we update anything in the source\n\ncode repository (e.g., a shared library), it rapidly and automatically propagates to every other service that uses that\n\nlibrary, and it is integrated through each team’s deployment pipeline.\n\nGoogle is one of the largest examples of using an organization-\n\nwide shared source code repository. By 2015, Google had a single shared source code repository with over one billion files and over\n\ntwo billion lines of code. This repository is used by every one of their twenty-five thousand engineers and spans every Google",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "property, including Google Search, Google Maps, Google Docs, Google+, Google Calendar, Gmail, and YouTube.‡\n\nOne of the valuable results of this is that engineers can leverage the diverse expertise of everyone in the organization. Rachel Potvin, a Google engineering manager overseeing the Developer\n\nInfrastructure group, told Wired that every Google engineer can access “a wealth of libraries” because “almost everything has already been done.”\n\nFurthermore, as Eran Messeri, an engineer in the Google Developer Infrastructure group, explains, one of the advantages of using a single repository is that it allows users to easily access all\n\nof the code in its most up-to-date form, without the need for coordination.\n\nWe put into our shared source code repository not only source\n\ncode, but also other artifacts that encode knowledge and learning, including:\n\nConfiguration standards for our libraries, infrastructure, and environments (Chef recipes, Puppet manifests, etc.)\n\nDeployment tools\n\nTesting standards and tools, including security\n\nDeployment pipeline tools\n\nMonitoring and analysis tools\n\nTutorials and standards",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "Encoding knowledge and sharing it through this repository is one of the most powerful mechanisms we have for propagating\n\nknowledge. As Randy Shoup describes, “The most powerful mechanism for preventing failures at Google is the single code repository. Whenever someone checks in anything into the repo, it\n\nresults in a new build, which always uses the latest version of everything. Everything is built from source rather than dynamically linked at runtime—there is always a single version of a library that is the current one in use, which is what gets statically\n\nlinked during the build process.”\n\nTom Limoncelli is the co-author of The Practice of Cloud System Administration: Designing and Operating Large Distributed\n\nSystems and a former Site Reliability Engineer at Google. In his book, he states that the value of having a single repository for an entire organization is so powerful it is difficult to even explain.\n\nYou can write a tool exactly once and have it be usable for all projects. You have 100% accurate knowledge of who depends on a library; therefore, you can refactor it and be 100% sure of\n\nwho will be affected and who needs to test for breakage. I could probably list one hundred more examples. I can’t express in words how much of a competitive advantage this is for Google.\n\nAt Google, every library (e.g., libc, OpenSSL, as well internally developed libraries such as Java threading libraries) has an owner who is responsible for ensuring that the library not only compiles,\n\nbut also successfully passes the tests for all projects that depend upon it, much like a real-world librarian. That owner is also responsible for migrating each project from one version to the next.",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Consider the real-life example of an organization that runs eighty- one different versions of the Java Struts framework library in\n\nproduction—all but one of those versions have critical security vulnerabilities, and maintaining all those versions, each with its own quirks and idiosyncrasies, creates significant operational burden and stress. Furthermore, all this variance makes\n\nupgrading versions risky and unsafe, which in turn discourages developers from upgrading. And the cycle continues.\n\nThe single source repository solves much of this problem, as well\n\nas having automated tests that allow teams to migrate to new versions safely and confidently.\n\nIf we are not able to build everything off a single source tree, we must find another means to maintain known good versions of the libraries and their dependencies. For instance, we may have an organization-wide repository such as Nexus, Artifactory, or a\n\nDebian or RPM repository, which we must then update where there are known vulnerabilities, both in these repositories and in production systems.\n\nSPREAD KNOWLEDGE BY USING AUTOMATED TESTS AS DOCUMENTATION AND COMMUNITIES OF PRACTICE\n\nWhen we have shared libraries being used across the organization, we should enable rapid propagation of expertise and\n\nimprovements. Ensuring that each of these libraries has significant amounts of automated testing included means these",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "libraries become self-documenting and show other engineers how to use them.\n\nThis benefit will be nearly automatic if we have test-driven development (TDD) practices in place, where automated tests are written before we write the code. This discipline turns our test\n\nsuites into a living, up-to-date specification of the system. Any engineer wishing to understand how to use the system can look at the test suite to find working examples of how to use the system’s API.\n\nIdeally, each library will have a single owner or a single team supporting it, representing where knowledge and expertise for the library resides. Furthermore, we should (ideally) only allow one\n\nversion to be used in production, ensuring that whatever is in production leverages the best collective knowledge of the organization.\n\nIn this model, the library owner is also responsible for safely migrating each group using the repository from one version to the next. This in turn requires quick detection of regression errors\n\nthrough comprehensive automated testing and continuous integration for all systems that use the library.\n\nIn order to more rapidly propagate knowledge, we can also create\n\ndiscussion groups or chat rooms for each library or service, so anyone who has questions can get responses from other users, who are often faster to respond than the developers.\n\nBy using this type of communication tool instead of having isolated pockets of expertise spread throughout the organization, we facilitate an exchange of knowledge and experience, ensuring",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "that workers are able to help each other with problems and new patterns.\n\nDESIGN FOR OPERATIONS THROUGH CODIFIED NON-FUNCTIONAL REQUIREMENTS\n\nWhen Development follows their work downstream and participates in production incident resolution activities, the\n\napplication becomes increasingly better designed for Operations. Furthermore, as we start to deliberately design our code and application so that it can accommodate fast flow and deployability, we will likely identify a set of non-functional requirements that we\n\nwill want to integrate into all of our production services.\n\nImplementing these non-functional requirements will enable our\n\nservices to be easy to deploy and keep running in production, where we can quickly detect and correct problems, and ensure it degrades gracefully when components fail. Examples of non- functional requirements include ensuring that we have:\n\nSufficient production telemetry in our applications and environments\n\nThe ability to accurately track dependencies\n\nServices that are resilient and degrade gracefully\n\nForward and backward compatibility between versions\n\nThe ability to archive data to manage the size of the production data set",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "The ability to easily search and understand log messages across services\n\nThe ability to trace requests from users through multiple services\n\nSimple, centralized runtime configuration using feature flags\n\nand so forth\n\nBy codifying these types of non-functional requirements, we make it easier for all our new and existing services to leverage the\n\ncollective knowledge and experience of the organization. These are all responsibilities of the team building the service.\n\nBUILD REUSABLE OPERATIONS USER STORIES INTO DEVELOPMENT\n\nWhen there is Operations work that cannot be fully automated or\n\nmade self-service, our goal is to make this recurring work as repeatable and deterministic as possible. We do this by standardizing the needed work, automating as much as possible,\n\nand documenting our work so that we can best enable product teams to better plan and resource this activity.\n\nInstead of manually building servers and then putting them into\n\nproduction according to manual checklists, we should automate as much of this work as possible. Where certain steps cannot be\n\nautomated (e.g., manually racking a server and having another\n\nteam cable it), we should collectively define the handoffs as clearly as possible to reduce lead times and errors. This will also enable\n\nus to better plan and schedule these steps in the future. For",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "instance, we can use tools such as Rundeck to automate and\n\nexecute workflows, or work ticket systems such as JIRA or\n\nServiceNow.\n\nIdeally, for all our recurring Ops work we will know the following:\n\nwhat work is required, who is needed to perform it, what the steps\n\nto complete it are, and so forth. For instance, “We know a high- availability rollout takes fourteen steps, requiring work from four\n\ndifferent teams, and the last five times we performed this, it took\n\nan average of three days.”\n\nJust as we create user stories in Development that we put into the\n\nbacklog and then pull into work, we can create well-defined “Ops\n\nuser stories” that represent work activities that can be reused across all our projects (e.g., deployment, capacity, security, etc.).\n\nBy creating these well defined Ops user stories, we expose\n\nrepeatable IT Operations work in a manner where it shows up alongside Development work, enabling better planning and more\n\nrepeatable outcomes.\n\nENSURE TECHNOLOGY CHOICES HELP ACHIEVE ORGANIZATIONAL GOALS\n\nWhen one of our goals is to maximize developer productivity and we have service-oriented architectures, small service teams can\n\npotentially build and run their service in whatever language or\n\nframework that best serves their specific needs. In some cases, this is what best enables us to achieve our organizational goals.\n\nHowever, there are scenarios when the opposite occurs, such as\n\nwhen expertise for a critical service resides only in one team, and",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "only that team can make changes or fix problems, creating a\n\nbottleneck. In other words, we may have optimized for team\n\nproductivity but inadvertently impeded the achievement of organizational goals.\n\nThis often happens when we have a functionally-oriented\n\nOperations group that is responsible for any aspect of service support. In these scenarios, to ensure that we enable the deep skill\n\nsets in specific technologies, we want to make sure that Operations\n\ncan influence which components are used in production, or give them the ability to not be responsible for unsupported platforms.\n\nIf we do not have a list of technologies that Operations will\n\nsupport, collectively generated by Development and Operations, we should systematically go through the production infrastructure\n\nand services, as well as all their dependencies that are currently\n\nsupported, to find which ones are creating a disproportionate amount of failure demand and unplanned work. Our goal is to\n\nidentify the technologies that:\n\nImpede or slow down the flow of work\n\nDisproportionately create high levels of unplanned work\n\nDisproportionately create large numbers of support requests\n\nAre most inconsistent with our desired architectural outcomes (e.g. throughput, stability, security, reliability, business\n\ncontinuity)\n\nBy removing these problematic infrastructures and platforms from the technologies supported by Ops, we enable them to focus",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "on infrastructure that best helps achieve the global goals of the\n\norganization.\n\nAs Tom Limoncelli describes, “When I was at Google, we had one official compiled language, one official scripting language, and one\n\nofficial UI language. Yes, other languages were supported in some\n\nway or another, but sticking with ‘the big three’ meant support libraries, tools, and an easier way to find collaborators.”§ These standards were also reinforced by the code review process, as well\n\nas what languages were supported by their internal platforms.\n\nIn a presentation that he gave with Olivier Jacques and Rafael\n\nGarcia at the 2015 DevOps Enterprise Summit, Ralph Loura, CIO\n\nof HP, stated:\n\nInternally, we described our goal as creating “buoys, not\n\nboundaries.” Instead of drawing hard boundaries that\n\neveryone has to stay within, we put buoys that indicate deep areas of the channel where you’re safe and supported. You can\n\ngo past the buoys as long as you follow the organizational\n\nprinciples. After all, how are we ever going to see the next innovation that helps us win if we’re not exploring and testing\n\nat the edges? As leaders, we need to navigate the channel, mark the channel, and allow people to explore past it.\n\nCase Study Standardizing a New Technology Stack at Etsy (2010)\n\nIn many organizations adopting DevOps, a common story developers tell is, “Ops wouldn’t provide us what we needed,\n\nso we just built and supported it ourselves.” However, in the",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "early stages of the Etsy transformation, technology\n\nleadership took the opposite approach, significantly reducing\n\nthe number of supported technologies in production.\n\nIn 2010, after a nearly disastrous peak holiday season, the\n\nEtsy team decided to massively reduce the number of\n\ntechnologies used in production, choosing a few that the entire organization could fully support and eradicating the rest.¶\n\nTheir goal was to standardize and very deliberately reduce the supported infrastructure and configurations. One of the\n\nearly decisions was to migrate Etsy’s entire platform to PHP\n\nand MySQL. This was primarily a philosophical decision rather than a technological decision—they wanted both Dev\n\nand Ops to be able to understand the full technology stack so that everyone could contribute to a single platform, as\n\nwell as enable everyone to be able to read, rewrite, and fix\n\neach other’s code. Over the next several years, as Michael Rembetsy, who was Etsy’s Director of Operations at the\n\ntime, recalls, “We retired some great technologies, taking\n\nthem entirely out of production,” including lighttpd, Postgres, MongoDB, Scala, CoffeeScript, Python, and many others.\n\nSimilarly, Dan McKinley, a developer on the feature team\n\nthat introduced MongoDB into Etsy in 2010, writes on his blog that all the benefits of having a schema-less database\n\nwere negated by all the operational problems the team had\n\nto solve. These included problems concerning logging, graphing, monitoring, production telemetry, and backups and\n\nrestoration, as well as numerous other issues that\n\ndevelopers typically do not need to concern themselves",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "with. The result was to abandon MongoDB, porting the new\n\nservice to use the already supported MySQL database infrastructure.\n\nCONCLUSION\n\nThe techniques described in this chapter enable every new\n\nlearning to be incorporated into the collective knowledge of the organization, multiplying its effect. We do this by actively and\n\nwidely communicating new knowledge, such as through chat\n\nrooms and through technology such as architecture as code, shared source code repositories, technology standardization, and\n\nso forth. By doing this, we elevate the state of the practice of not\n\njust Dev and Ops, but also the entire organization, so everyone who performs work does so with the cumulative experience of the\n\nentire organization.\n\n† Hubot often performed tasks by calling shell scripts, which could then be executed from the chat room\n\nanywhere, including from an engineer’s phone.\n\n‡ The Chrome and Android projects reside in a separate source code repository, and certain algorithms that are\n\nkept secret, such as PageRank, are available only to certain teams.\n\n§ Google used C++ as their official compiled language, Python (and later Go) as their official scripting language,\n\nand Java and JavaScript via Google Web Toolkit as their official UI languages.\n\n¶ At that time, Etsy used PHP, lighttp, Postgres, MongoDB, Scala, CoffeeScript, Python, as well as many other\n\nplatforms and languages.",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "21Reserve Time to Create Organizational Learning and Improvement\n\nOne of the practices that forms part of the Toyota Production\n\nSystem is called the improvement blitz (or sometimes a kaizen\n\nblitz), defined as a dedicated and concentrated period of time to\n\naddress a particular issue, often over the course of a several days. Dr. Spear explains, “...blitzes often take this form: A group is\n\ngathered to focus intently on a process with problems…The blitz lasts a few days, the objective is process improvement, and the\n\nmeans are the concentrated use of people from outside the process\n\nto advise those normally inside the process.”\n\nSpear observes that the output of the improvement blitz team will\n\noften be a new approach to solving a problem, such as new layouts of equipment, new means of conveying material and information,\n\na more organized workspace, or standardized work. They may also\n\nleave behind a to-do list of changes to be made down the road.\n\nAn example of a DevOps improvement blitz is the Monthly Challenge program at the Target DevOps Dojo. Ross Clanton, Director of Operations at Target, is responsible for accelerating\n\nthe adoption of DevOps. One of his primary mechanisms for this is the Technology Innovation Center, more popularly known as the DevOps Dojo.",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "The DevOps Dojo occupies about eighteen thousand square feet of\n\nopen office space, where DevOps coaches help teams from across\n\nthe Target technology organization elevate the state of their\n\npractice. The most intensive format is what they call “30-Day\n\nChallenges,” where internal development teams come in for a\n\nmonth and work together with dedicated Dojo coaches and\n\nengineers. The team brings their work with them, with the goal of\n\nsolving an internal problem they have been struggling with and to\n\ncreate a breakthrough in thirty days.\n\nThroughout the thirty days, they work intensively with the Dojo\n\ncoaches on the problem—planning, working, and doing demos in two-day sprints. When the 30-Day Challenge is complete, the internal teams return to their lines of business, not only having\n\nsolved a significant problem, but bringing their new learnings back to their teams.\n\nClanton describes, “We currently have capacity to have eight teams doing 30-Day Challenges concurrently, so we are focused on the most strategic projects of the organization. So far, we’ve had some of our most critical capabilities come through the Dojo,\n\nincluding teams from Point Of Sale (POS), Inventory, Pricing, and Promotion.”\n\nBy having full-time assigned Dojo staff and being focused on only one objective, teams going through a 30-Day Challenge make incredible improvements.\n\nRavi Pandey, a Target development manager who went through this program, explains, “In the old days, we would have to wait six weeks to get a test environment. Now, we get it in minutes, and we’re working side by side with Ops engineers who are helping us",
      "content_length": 1625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "increase our productivity and building tooling for us to help us\n\nachieve our goals.” Clanton expands on this idea, “It is not\n\nuncommon for teams to achieve in days what would usually take\n\nthem three to six months. So far, two hundred learners have come\n\nthrough the Dojo, having completed fourteen challenges.”\n\nThe Dojo also supports less intensive engagement models,\n\nincluding Flash Builds, where teams come together for one- to\n\nthree-day events, with the goal of shipping a minimal viable\n\nproduct (MVP) or a capability by the end of the event. They also\n\nhost Open Labs every two weeks, where anyone can visit the Dojo\n\nto talk to the Dojo coaches, attend demos, or receive training.\n\nIn this chapter, we will describe this and other ways of reserving time for organizational learning and improvement, further institutionalizing the practice of dedicating time for improving daily work.\n\nINSTITUTIONALIZE RITUALS TO PAY DOWN TECHNICAL DEBT\n\nIn this section, we schedule rituals that help enforce the practice\n\nof reserving Dev and Ops time for improvement work, such as non-functional requirements, automation, etc. One of the easiest ways to do this is to schedule and conduct day- or week-long improvement blitzes, where everyone on a team (or in the entire organization) self-organizes to fix problems they care about—no feature work is allowed. It could be a problematic area of the code,\n\nenvironment, architecture, tooling, and so forth. These teams span the entire value stream, often combining Development,\n\nOperations, and Infosec engineers. Teams that typically don’t",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "work together combine their skills and effort to improve a chosen area and then demonstrate their improvement to the rest of the\n\ncompany.\n\nIn addition to the Lean-oriented terms kaizen blitz and improvement blitz, the technique of dedicated rituals for\n\nimprovement work has also been called spring or fall cleanings\n\nand ticket queue inversion weeks. Other terms have also been used, such as hack days, hackathons, and 20% innovation time.\n\nUnfortunately, these specific rituals sometimes focus on product innovation and prototyping new market ideas, rather than on\n\nimprovement work, and worse, they are often restricted to developers—which is considerably different than the goals of an\n\nimprovement blitz.†\n\nOur goal during these blitzes is not to simply experiment and\n\ninnovate for the sake of testing out new technologies, but to improve our daily work, such as solving our daily workarounds.\n\nWhile experiments can also lead to improvements, improvement blitzes are very focused on solving specific problems we encounter\n\nin our daily work.\n\nWe may schedule week-long improvement blitzes that prioritize Dev and Ops working together toward improvement goals. These\n\nimprovement blitzes are simple to administer: One week is\n\nselected where everyone in the technology organization works on an improvement activity at the same time. At the end of the\n\nperiod, each team makes a presentation to their peers that discusses the problem they were tackling and what they built. This\n\npractice reinforces a culture in which engineers work across the entire value stream to solve problems. Furthermore, it reinforces",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "fixing problems as part of our daily work and demonstrates that we value paying down technical debt.\n\nWhat makes improvement blitzes so powerful is that we are\n\nempowering those closest to the work to continually identify and solve their own problems. Consider for a moment that our\n\ncomplex system is like a spider web, with intertwining strands that are constantly weakening and breaking. If the right combination\n\nof strands breaks, the entire web collapses. There is no amount of\n\ncommand-and-control management that can direct workers to fix each strand one by one. Instead, we must create the organizational\n\nculture and norms that lead to everyone continually finding and fixing broken strands as part of our daily work. As Dr. Spear\n\nobserves, “No wonder then that spiders repair rips and tears in the web as they occur, not waiting for the failures to accumulate.”\n\nA great example of the success of the improvement blitz concept is\n\ndescribed by Mark Zuckerberg, Facebook CEO. In an interview\n\nwith Jessica Stillman of Inc.com, he says, “Every few months we have a hackathon, where everyone builds prototypes for new ideas\n\nthey have. At the end, the whole team gets together and looks at everything that has been built. Many of our most successful\n\nproducts came out of hackathons, including Timeline, chat, video, our mobile development framework and some of our most\n\nimportant infrastructure like the HipHop compiler.”\n\nOf particular interest is the HipHop PHP compiler. In 2008,\n\nFacebook was facing significant capacity problems, with over one- hundred million active users and rapidly growing, creating\n\ntremendous problems for the entire engineering team. During a hack day, Haiping Zhao, Senior Server Engineer at Facebook,\n\nstarted experimenting with converting PHP code to compilable",
      "content_length": 1804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "C++ code, with the hope of significantly increasing the capacity of\n\ntheir existing infrastructure. Over the next two years, a small team\n\nwas assembled to build what became known as the HipHop compiler, converting all Facebook production services from\n\ninterpreted PHP to compiled C++ binaries. HipHop enabled Facebook’s platform to handle six times higher production loads\n\nthan the native PHP.\n\nIn an interview with Cade Metz of Wired, Drew Paroski, one of the engineers who worked on the project, noted, “There was a\n\nmoment where, if HipHop hadn’t been there, we would have been\n\nin hot water. We would probably have needed more machines to serve the site than we could have gotten in time. It was a Hail\n\nMary pass that worked out.”\n\nLater, Paroski and fellow engineers Keith Adams and Jason Evans decided that they could beat the performance of the HipHop\n\ncompiler effort and reduce some of its limitations that reduced developer productivity. The resulting project was the HipHop\n\nvirtual machine project (“HHVM”), taking a just-in-time\n\ncompilation approach. By 2012, HHVM had completely replaced the HipHop compiler in production, with nearly twenty engineers\n\ncontributing to the project.\n\nBy performing regularly scheduled improvement blitzes and hack weeks, we enable everyone in the value stream to take pride and\n\nownership in the innovations they create, and we continually integrate improvements into our system, further enabling safety,\n\nreliability, and learning.",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "ENABLE EVERYONE TO TEACH AND LEARN\n\nA dynamic culture of learning creates conditions so that everyone can not only learn, but also teach, whether through traditional didactic methods (e.g., people taking classes, attending training) or more experiential or open methods (e.g., conferences,\n\nworkshops, mentoring). One way that we can foster this teaching and learning is to dedicate organizational time to it.\n\nSteve Farley, VP of Information Technology at Nationwide\n\nInsurance, said, “We have five thousand technology professionals, who we call ‘associates.’ Since 2011, we have been committed to create a culture of learning—part of that is something we call\n\nTeaching Thursday, where each week we create time for our associates to learn. For two hours, each associate is expected to teach or learn. The topics are whatever our associates want to learn about—some of them are on technology, on new software\n\ndevelopment or process improvement techniques, or even on how to better manage their career. The most valuable thing any associate can do is mentor or learn from other associates.”\n\nAs has been made evident throughout this book, certain skills are becoming increasingly needed by all engineers, not just by developers. For instance, it is becoming more important for all\n\nOperations and Test engineers to be familiar with Development techniques, rituals, and skills, such as version control, automated testing, deployment pipelines, configuration management, and\n\ncreating automation. Familiarity with Development techniques helps Operations engineers remain relevant as more technology value streams adopt DevOps principles and patterns.",
      "content_length": 1649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "Although the prospect of learning something new may be intimidating or cause a sense of embarrassment or shame, it\n\nshouldn’t. After all, we are all lifelong learners, and one of the best ways to learn is from our peers. Karthik Gaekwad, who was part of the National Instruments DevOps transformation, said, “For\n\nOperations people who are trying to learn automation, it shouldn’t be scary—just ask a friendly developer, because they would love to help.”\n\nWe can help further help teach skills through our daily work by jointly performing code reviews that include both parties so that we learn by doing, as well as by having Development and Operations work together to solve small problems. For instance,\n\nwe might have Development show Operations how to authenticate an application, and login and run automated tests against various parts of the application to ensure that critical components are\n\nworking correctly (e.g., key application functionality, database transactions, message queues). We would then integrate this new automated test into our deployment pipeline and run it periodically, sending the results to our monitoring and alerting\n\nsystems so that we get earlier detection when critical components fail.\n\nAs Glenn O’Donnell from Forrester Research quipped in his 2014 DevOps Enterprise Summit presentation, “For all technology professionals who love innovating, love change, there is a wonderful and vibrant future ahead of us.”\n\nSHARE YOUR EXPERIENCES FROM DEVOPS CONFERENCES",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "In many cost-focused organizations, engineers are often discouraged from attending conferences and learning from their\n\npeers. To help build a learning organization, we should encourage our engineers (both from Development and Operations) to attend conferences, give talks at them, and, when necessary, create and organize internal or external conferences themselves.\n\nDevOpsDays remains one of the most vibrant self-organized conference series today. Many DevOps practices have been shared and promulgated at these events. It has remained free or nearly\n\nfree, supported by a vibrant community of practitioner communities and vendors.\n\nThe DevOps Enterprise Summit was created in 2014 for technology leaders to share their experiences adopting DevOps principles and practices in large, complex organizations. The program is organized primarily around experience reports from\n\ntechnology leaders on the DevOps journey, as well as subject matter experts on topics selected by the community.\n\nCase Study Internal Technology Conferences at Nationwide Insurance, Capital One, and Target (2014)\n\nAlong with attending external conferences, many\n\ncompanies, including those described in this section, have internal conferences for their technology employees.\n\nNationwide Insurance is a leading provider of insurance and\n\nfinancial services, and operates in heavily regulated industries. Their many offerings include auto and",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "homeowners insurance, and they are the top provider of public-sector retirement plans and pet insurance. As of 2014, $195 billion in assets, with $24 billion in revenue.\n\nSince 2005, Nationwide has been adopting Agile and Lean principles to elevate the state of practice for their five thousand technology professionals, enabling grassroots\n\ninnovation.\n\nSteve Farley, VP of Information Technology, remembers, “Exciting technology conferences were starting to appear\n\naround that time, such as the Agile national conference. In 2011, the technology leadership at Nationwide agreed that we should create a technology conference, called TechCon.\n\nBy holding this event, we wanted to create a better way to teach ourselves, as well as ensure that everything had a Nationwide context, as opposed to sending everyone to an external conference.”\n\nCapital One, one of the largest banks in the US with over $298 billion in assets and $24 billion in revenue in 2015, held their first internal software engineering conference in\n\n2015 as part of their goal to create a world-class technology organization. The mission was to promote a culture of sharing and collaboration, and to build relationships between\n\nthe technology professionals and enable learning. The conference had thirteen learning tracks and fifty-two sessions, and over 1,200 internal employees attended.\n\nDr. Tapabrata Pal, a technical fellow at Capital One and one of the organizers of the event, describes, “We even had an expo hall, where we had twenty-eight booths, where internal\n\nCapital One teams were showing off all the amazing",
      "content_length": 1593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "capabilities they were working on. We even decided very deliberately that there would be no vendors there, because we wanted to keep the focus on Capital One goals.”\n\nTarget is the sixth-largest retailer in the US, with $72 billion in revenue in 2014 and 1,799 retail stores and 347,000 employees worldwide. Heather Mickman, a director of\n\nDevelopment, and Ross Clanton have held six internal DevOpsDays events since 2014 and have over 975 followers inside their internal technology community,\n\nmodeled after the DevOpsDays held at ING in Amsterdam in 2013.‡\n\nAfter Mickman and Clanton attended the DevOps Enterprise\n\nSummit in 2014, they held their own internal conference, inviting many of the speakers from outside firms so that they could re-create their experience for their senior leadership.\n\nClanton describes, “2015 was the year when we got executive attention and when we built up momentum. After that event, tons of people came up to us, asking how they could get involved and how they could help.”\n\nCREATE INTERNAL CONSULTING AND COACHES TO SPREAD PRACTICES\n\nCreating an internal coaching and consulting organization is a method commonly used to spread expertise across an organization. This can come in many different forms. At Capital\n\nOne, designated subject matter experts hold office hours where anyone can consult with them, ask questions, etc.",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Earlier in the book, we began the story of how the Testing Grouplet built a world-class automated testing culture at Google starting in 2005. Their story continues here, as they try to improve\n\nthe state of automated testing across all of Google by using dedicated improvement blitzes, internal coaches, and even an internal certification program.\n\nBland said, at that time, there was a 20% innovation time policy at Google, enabling developers to spend roughly one day per week on a Google-related project outside of their primary area of\n\nresponsibility. Some engineers chose to form grouplets, ad hoc teams of like-minded engineers who wanted to pool their 20% time, allowing them to do focused improvement blitzes.\n\nA testing grouplet was formed by Bharat Mediratta and Nick Lesiecki, with the mission of driving the adoption of automated testing across Google. Even though they had no budget or formal authority, as Mike Bland described, “There were no explicit\n\nconstraints put upon us, either. And we took advantage of that.”\n\nThey used several mechanisms to drive adoption, but one of the\n\nmost famous was Testing on the Toilet (or TotT), their weekly testing periodical. Each week, they published a newsletter in nearly every bathroom in nearly every Google office worldwide. Bland said, “The goal was to raise the degree of testing knowledge\n\nand sophistication throughout the company. It’s doubtful an online-only publication would’ve involved people to the same\n\ndegree.”\n\nBland continues, “One of the most significant TotT episodes was\n\nthe one titled, ‘Test Certified: Lousy Name, Great Results,’ because",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "it outlined two initiatives that had significant success in advancing\n\nthe use of automated testing.”\n\nTest Certified (TC) provided a road map to improve the state of automated testing. As Bland describes, “It was intended to hack\n\nthe measurement-focused priorities of Google culture...and to\n\novercome the first, scary obstacle of not knowing where or how to start. Level 1 was to quickly establish a baseline metric, Level 2\n\nwas setting a policy and reaching an automated test coverage goal,\n\nand Level 3 was striving towards a long-term coverage goal.”\n\nThe second capability was providing Test Certified mentors to any\n\nteam who wanted advice or help, and Test Mercenaries (i.e., a full-\n\ntime team of internal coaches and consultants) to work hands-on with teams to improve their testing practices and code quality. The\n\nMercenaries did so by applying the Testing Grouplet’s knowledge,\n\ntools, and techniques to a team’s own code, using TC as both a guide and a goal. Bland was eventually a leader of the Testing\n\nGrouplet from 2006 to 2007, and a member of the Test Mercenaries from 2007 to 2009.\n\nBland continues, “It was our goal to get every team to TC Level 3,\n\nwhether they were enrolled in our program our not. We also\n\ncollaborated closely with the internal testing tools teams, providing feedback as we tackled testing challenges with the\n\nproduct teams. We were boots on the ground, applying the tools we built, and eventually, we were able to remove ‘I don’t have time\n\nto test’ as a legitimate excuse.”\n\nHe continues, “The TC levels exploited the Google metrics-driven culture—the three levels of testing were something that people\n\ncould discuss and brag about at performance review time. The",
      "content_length": 1712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Testing Grouplet eventually got funding for the Test Mercenaries,\n\na staffed team of full-time internal consultants. This was an\n\nimportant step, because now management was fully onboard, not with edicts, but by actual funding.”\n\nAnother important construct was leveraging company-wide “Fixit”\n\nimprovement blitzes. Bland describes Fixits as “when ordinary engineers with an idea and a sense of mission recruit all of Google\n\nengineering for one-day, intensive sprints of code reform and tool\n\nadoption.” He organized four company-wide Fixits, two pure Testing Fixits and two that were more tools-related, the last\n\ninvolving more than one hundred volunteers in over twenty offices in thirteen countries. He also led the Fixit Grouplet from 2007 to\n\n2008.\n\nThese Fixits, as Bland describes means that we should provide\n\nfocused missions at critical points in time to generate excitement and energy, which helps advance the state-of-the-art. This will\n\nhelp the long-term culture change mission reach a new plateau with every big, visible effort.\n\nThe results of the testing culture are self-evident in the amazing\n\nresults Google has achieved, presented throughout the book.\n\nCONCLUSION\n\nThis chapter described how we can institute rituals that help\n\nreinforce the culture that we are all lifelong learners and that we\n\nvalue the improvement of daily work over daily work itself. We do this by reserving time to pay down technical debt, create forums\n\nthat allow everyone to learn from and teach each other, both",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "inside our organization and outside it. And we make experts\n\navailable to help internal teams, either by coaching or consulting\n\nor even just holding office hours to answer questions.\n\nBy having everyone help each other learn in our daily work, we\n\nout-learn the competition, helping us win in the marketplace. But\n\nalso we help each other achieve our full potential as human beings.",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "CONCLUSION TO PART V\n\nThroughout Part V, we explored the practices that help create a\n\nculture of learning and experimentation in your organization. Learning from incidents, creating shared repositories, and sharing\n\nlearnings is essential when we work in complex systems, helping\n\nto make our work culture more just and our systems safer and more resilient.\n\nIn Part VI, we’ll explore how to extend flow, feedback, and\n\nlearning and experimentation by using them to simultaneously help us achieve our Information Security goals.\n\n† From here on, the terms “hack week” and “hackathon” are used interchangeably with “improvement blitz,” and\n\nnot in the context of “you can work on whatever you want.”\n\n‡ Incidentally, the first Target internal DevOpsDays event was modeled after the first ING DevOpsDays that was\n\norganized by Ingrid Algra, Jan-Joost Bouwman, Evelijn Van Leeuwen, and Kris Buytaert in 2013, after some of the ING team attended the 2013 Paris DevOpsDays.",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "22Information Security as Everyone’s Job, Every Day\n\nOne of the top objections to implementing DevOps principles and\n\npatterns has been, “Information security and compliance won’t let\n\nus.” And yet, DevOps may be one of the best ways to better\n\nintegrate information security into the daily work of everyone in\n\nthe technology value stream.\n\nWhen Infosec is organized as a silo outside of Development and\n\nOperations, many problems arise. James Wickett, one of the\n\ncreators of the Gauntlt security tool and organizer of DevOpsDays Austin and the Lonestar Application Security conference,\n\nobserved:\n\nOne interpretation of DevOps is that it came from the need to enable developers productivity, because as the number of\n\ndevelopers grew, there weren’t enough Ops people to handle all\n\nthe resulting deployment work. This shortage is even worse in Infosec—the ratio of engineers in Development, Operations, and Infosec in a typical technology organization is 100:10:1.\n\nWhen Infosec is that outnumbered, without automation and integrating information security into the daily work of Dev and Ops, Infosec can only do compliance checking, which is the opposite of security engineering—and besides, it also makes\n\neveryone hate us.",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "James Wickett and Josh Corman, former CTO of Sonatype and\n\nrespected information security researcher, have written about\n\nincorporating information security objectives into DevOps, a set of\n\npractices and principles termed Rugged DevOps. Similar ideas\n\nwere created by Dr. Tapabrata Pal, Director and Platform\n\nEngineering Technical Fellow at Capital One, and the Capital One\n\nteam, who describe their processes as DevOpsSec, where Infosec is\n\nintegrated into all stages of the SDLC. Rugged DevOps traces\n\nsome of its history to Visible Ops Security, written by Gene Kim,\n\nPaul Love, and George Spafford.\n\nThroughout The DevOps Handbook, we have explored how to fully integrate the QA and Operations objectives throughout our entire technology value stream. In this chapter, we describe how\n\nto similarly integrate Infosec objectives into our daily work, where we can increase developer and operational productivity, increase safety, and increase our security.\n\nINTEGRATE SECURITY INTO DEVELOPMENT ITERATION DEMONSTRATIONS\n\nOne of our goals is to have feature teams engaged with Infosec as early as possible, as opposed to primarily engaging at the end of the project. One way we can do this is by inviting Infosec to the product demonstrations at the end of each development interval so that they can better understand the team goals in the context of\n\norganizational goals, observe their implementations as they are being built, and provide guidance and feedback at the earliest",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "stages of the project, when there is the most amount of time and\n\nfreedom to make corrections.\n\nJustin Arbuckle, former chief architect at GE Capital, observes,\n\n“When it came to information security and compliance, we found\n\nthat blockages at the end of the project were much more expensive\n\nthan at the beginning—and Infosec blockages were among the\n\nworst. ‘Compliance by demonstration’ became one of the rituals\n\nwe used to shift all this complexity earlier in the process.”\n\nHe continues, “By having Infosec involved throughout the creation\n\nof any new capability, we were able to reduce our use of static\n\nchecklists dramatically and rely more on using their expertise throughout the entire software development process.”\n\nThis helped the organization achieve its goals. Snehal Antani, former CIO of Enterprise Architecture at GE Capital Americas, described their top three key business measurements were “development velocity (i.e., speed of delivering features to\n\nmarket), failed customer interactions (i.e., outages, errors), and compliance response time (i.e., lead time from audit request to delivery of all quantitative and qualitative information required to fulfill the request).”\n\nWhen Infosec is an assigned part of the team, even if they are only being kept informed and observing the process, they gain the business context they need to make better risk-based decisions. Furthermore, Infosec is able to help feature teams learn what is\n\nrequired to meet security and compliance objectives.",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "INTEGRATE SECURITY INTO DEFECT TRACKING AND POST-MORTEMS\n\nWhen possible, we want to track all open security issues in the same work tracking system that Development and Operations are\n\nusing, ensuring the work is visible and can be prioritized against all other work. This is very different from how Infosec has\n\ntraditionally worked, where all security vulnerabilities are stored in a GRC (governance, risk, and compliance) tool that only Infosec\n\nhas access to. Instead, we will put any needed work in the systems\n\nthat Dev and Ops use.\n\nIn a presentation at the 2012 Austin DevOpsDays, Nick Galbreath, who headed up Information Security at Etsy for many years,\n\ndescribes how they treated security issues, “We put all security issues into JIRA, which all engineers use in their daily work, and\n\nthey were either ‘P1’ or ‘P2,’ meaning that they had to be fixed immediately or by the end of the week, even if the issue is only an\n\ninternally-facing application.”\n\nFurthermore, he states, “Any time we had a security issue, we\n\nwould conduct a post-mortem, because it would result in better educating our engineers on how to prevent it from happening\n\nagain in the future, as well as a fantastic mechanism for transferring security knowledge to our engineering teams.”\n\nINTEGRATE PREVENTIVE SECURITY CONTROLS INTO SHARED SOURCE CODE REPOSITORIES AND SHARED SERVICES",
      "content_length": 1365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "In chapter 20, we created a shared source code repository that makes it easy for anyone to discover and reuse the collective\n\nknowledge of our organization—not only for our code, but also for our toolchains, deployment pipeline, standards, etc. By doing this,\n\nanyone can benefit from the cumulative experience of everyone in the organization.\n\nNow we will add to our shared source code repository any\n\nmechanisms or tools that help enable us to ensure our\n\napplications and environments are secure. We will add libraries that are pre-blessed by security to fulfill specific Infosec\n\nobjectives, such as authentication and encryption libraries and services. Because everyone in the DevOps value stream uses\n\nversion control for anything they build or support, putting our information security artifacts there makes it much easier to\n\ninfluence the daily work of Dev and Ops, because anything we\n\ncreate is available, searchable, and reusable. Version control also serves as a omni-directional communication mechanism to keep\n\nall parties aware of changes being made.\n\nIf we have a centralized shared services organization, we may also collaborate with them to create and operate shared security-\n\nrelevant platforms, such as authentication, authorization, logging, and other security and auditing services that Dev and Ops require.\n\nWhen engineers use one of these predefined libraries or services,\n\nthey won’t need to schedule a separate security design review for that module; they’ll be using the guidance we’ve created\n\nconcerning configuration hardening, database security settings, key lengths, and so forth.\n\nTo further increase the likelihood that the services and libraries\n\nwe provide will be used correctly, we can provide security training",
      "content_length": 1751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "to Dev and Ops, as well as review what they’ve created to help\n\nensure that security objectives are being implemented correctly,\n\nespecially for teams using these tools for the first time.\n\nUltimately, our goal is to provide the security libraries or services that every modern application or environment requires, such as\n\nenabling user authentication, authorization, password management, data encryption, and so forth. Furthermore, we can\n\nprovide Dev and Ops with effective security-specific configuration settings for the components they use in their application stacks,\n\nsuch as for logging, authentication, and encryption. We may\n\ninclude items such as:\n\nCode libraries and their recommended configurations (e.g., 2FA [two-factor authentication library], bcrypt password\n\nhashing, logging)\n\nSecret management (e.g., connection settings, encryption keys) using tools such as Vault, sneaker, Keywhiz, credstash,\n\nTrousseau, Red October, etc.\n\nOS packages and builds (e.g., NTP for time syncing, secure versions of OpenSSL with correct configurations, OSSEC or\n\nTripwire for file integrity monitoring, syslog configuration to\n\nensure logging of critical security into our centralized ELK stack)\n\nBy putting all these into our shared source code repository, we\n\nmake it easy for any engineer to correctly create and use logging and encryption standards in their applications and environments, with no further work from us.",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "We should also collaborate with Ops teams to create a base cookbook or build image of our OS, databases, and other\n\ninfrastructure (e.g., NGINX, Apache, Tomcat), showing they are in a known, secure, and risk-reduced state. Our shared repository not only becomes the place where we can get the latest versions,\n\nbut also becomes a place where we can collaborate with other engineers and monitor and alert on changes made to security- sensitive modules.\n\nINTEGRATE SECURITY INTO OUR DEPLOYMENT PIPELINE\n\nIn previous eras, in order to harden and secure our application, we would start our security review after development was completed. Often, the output of this review would be hundreds of pages of\n\nvulnerabilities in a PDF, which we’d give to Development and Operations, which would be completely un-addressed due to project due date pressure or problems being found too late in the software life cycle to be easily corrected.\n\nIn this step, we will automate as many of our information security tests as possible, so that they run alongside all our other automated tests in our deployment pipeline, being performed\n\n(ideally) upon every code commit by Dev or Ops, and even in the earliest stages of a software project.\n\nOur goal is to provide both Dev and Ops with fast feedback on their work so that they are notified whenever they commit changes that are potentially insecure. By doing this, we enable them to quickly detect and correct security problems as part of their daily\n\nwork, which enables learning and prevents future errors.",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "Ideally, these automated security tests will be run in our deployment pipeline alongside the other static code analysis tools.\n\nTools such as Gauntlt have been designed to integrate into the deployment pipelines, which run automated security tests on our applications, our application dependencies, our environment, etc.\n\nRemarkably, Gauntlt even puts all its security tests in Gherkin syntax test scripts, which is widely used by developers for unit and functional testing. Doing this puts security testing in a framework\n\nthey are likely already familiar with. This also allows security tests to easily run in a deployment pipeline on every committed change, such as static code analysis, checking for vulnerable dependencies, or dynamic testing.\n\nFigure 43: Jenkins running automated security testing (Source: James Wicket and Gareth Rushgrove, “Battle-tested code without the battle,” Velocity 2014 conference presentation, posted to Speakerdeck.com, June 24, 2014, https://speakerdeck.com/garethr/battle-tested-code-without-the- battle.)\n\nBy doing this, we provide everyone in the value stream with the fastest possible feedback about the security of what they are\n\ncreating, enabling Dev and Ops engineers to find and fix issues quickly.",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "ENSURE SECURITY OF THE APPLICATION\n\nOften, Development testing focuses on the correctness of functionality, looking at positive logic flows. This type of testing is\n\noften referred to as the happy path, which validates user journeys (and sometimes alternative paths) where everything goes as expected, with no exceptions or error conditions.\n\nOn the other hand, effective QA, Infosec, and Fraud practitioners will often focus on the sad paths, which happen when things go wrong, especially in relation to security-related error conditions. (These types of security-specific conditions are often jokingly\n\nreferred to as the bad paths.)\n\nFor instance, suppose we have an e-commerce site with a\n\ncustomer input form that accepts credit card numbers as part of generating a customer order. We want to define all the sad and bath paths required to ensure that invalid credit cards are properly rejected to prevent fraud and security exploits, such as\n\nSQL injections, buffer overruns, and other undesirable outcomes.\n\nInstead of performing these tests manually, we would ideally generate them as part of our automated unit or functional tests so\n\nthat they can be run continuously in our deployment pipeline. As part of our testing, we will want to include the following:\n\nStatic analysis: This is testing that we perform in a non- runtime environment, ideally in the deployment pipeline. Typically, a static analysis tool will inspect program code for all possible run-time behaviors and seek out coding flaws, back\n\ndoors, and potentially malicious code (this is sometimes known as “testing from the inside-out”). Examples of tools include",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "Brakeman, Code Climate, and searching for banned code functions (e.g., “exec()”).\n\nDynamic analysis: As opposed to static testing, dynamic analysis consists of tests executed while a program is in operation. Dynamic tests monitor items such as system\n\nmemory, functional behavior, response time, and overall performance of the system. This method (sometimes known as “testing from the outside-in”) is similar to the manner in which a malicious third party might interact with an application.\n\nExamples include Arachni and OWASP ZAP (Zed Attack Proxy).† Some types of penetration testing can also be performed in an automated fashion and should be included as\n\npart of dynamic analysis using tools such as Nmap and Metasploit. Ideally, we should perform automated dynamic testing during the automated functional testing phase of our deployment pipeline, or even against our services while they\n\nare in production. To ensure correct security handling, tools like OWASP ZAP can be configured to attack our services through a web browser proxy and inspect the network traffic\n\nwithin our test harness.\n\nDependency scanning: Another type of static testing we would normally perform at build time inside of our deployment\n\npipeline involves inventorying all our dependencies for binaries and executables, and ensuring that these dependencies, which we often don’t have control over, are free of vulnerabilities or malicious binaries. Examples include Gemnasium and bundler\n\naudit for Ruby, Maven for Java, and the OWASP Dependency- Check.",
      "content_length": 1532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "Source code integrity and code signing: All developers should have their own PGP key, perhaps created and managed in a system such as keybase.io. All commits to version control\n\nshould be signed —that is straightforward to configure using the open source tools gpg and git. Furthermore, all packages created by the CI process should be signed, and their hash\n\nrecorded in the centralized logging service for audit purposes.\n\nFurthermore, we should define design patterns to help developers write code to prevent abuse, such as putting in rate limits for our\n\nservices and graying out submit buttons after they have being pressed. OWASP publishes a great deal of useful guidance such as the Cheat Sheet series, which includes:\n\nHow to store passwords\n\nHow to handle forgotten passwords\n\nHow to handle logging\n\nHow to prevent cross-site scripting (XSS) vulnerabilities\n\nCase Study Static Security Testing at Twitter (2009)\n\nThe “10 Deploys per Day: Dev and Ops Cooperation at Flickr” presentation by John Allspaw and Paul Hammond is\n\nfamous for catalyzing the Dev and Ops community in 2009. The equivalent for the information security community is likely the presentation that Justin Collins, Alex Smolen, and Neil Matatall gave on their information security",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "transformation work at Twitter at the AppSecUSA conference in 2012.\n\nTwitter had many challenges due to hyper-growth. For years, the famous Fail Whale error page would be displayed when Twitter did not have sufficient capacity to keep up with user demand, showing a graphic of a whale being lifted by\n\neight birds. The scale of user growth was breathtaking— between January and March 2009, the number of active Twitter users went from 2.5 million to 10 million.\n\nTwitter also had security problems during this period. In early 2009, two serious security breaches occurred. First, in January the @BarackObama Twitter account was hacked.\n\nThen in April, the Twitter administrative accounts were compromised through a brute-force dictionary attack. These events led the Federal Trade Commission to judge that Twitter was misleading its users into believing that their\n\naccounts were secure and issued an FTC consent order.\n\nThe consent order required that Twitter comply within sixty\n\ndays by instituting a set of processes that were to be enforced for the following twenty years and would do the following:\n\nDesignate an employee or employees to be responsible for\n\nTwitter’s information security plan\n\nIdentify reasonably foreseeable risks, both internal and external, that could lead to an intrusion incident and create and implement a plan to address these risks‡",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "Maintain the privacy of user information, not just from\n\noutside sources but also internally, with an outline of\n\npossible sources of verification and testing of the security and correctness of these implementations\n\nThe group of engineers assigned to solve this problem had\n\nto integrate security into the daily work of Dev and Ops and close the security holes that allowed the breaches to happen\n\nin the first place.\n\nIn their previously mentioned presentation, Collins, Smolen, and Matatall identified several problems they needed to\n\naddress:\n\nPrevent security mistakes from being repeated: They found that they were fixing the same defects and\n\nvulnerabilities over and over again. They needed to modify\n\nthe system of work and automation tools to prevent the issues from happening again.\n\nIntegrate security objectives into existing developer\n\ntools: They identified early on that the major source of vulnerabilities were code issues. They couldn’t run a tool\n\nthat generated a huge PDF report and then email it to\n\nsomeone in Development or Operations. Instead, they needed to provide the developer who had created the\n\nvulnerability with the exact information needed to fix it.\n\nPreserve trust of Development: They needed to earn and maintain the trust of Development. That meant they needed\n\nto know when they sent Development false positives, so",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "they could fix the error that prompted the false positive and\n\navoid wasting Development’s time.\n\nMaintain fast flow through Infosec through automation: Even when code vulnerability scanning was automated,\n\nInfosec still had to do lots of manual work and waiting. They\n\nhad to wait for the scan to complete, get back the big stack of reports, interpret the reports, and then find the person\n\nresponsible for fixing it. And when the code changed, it had\n\nto be done all over again. By automating the manual work, they did fewer dumb “button-pushing” tasks, enabling them\n\nto use more creativity and judgment.\n\nMake everything security related self-service, if possible: They trusted that most people wanted to do the\n\nright thing, so it was necessary to provide them with all the\n\ncontext and information they needed to fix any issues.\n\nTake a holistic approach to achieving Infosec\n\nobjectives: Their goal was to do analysis from all the\n\nangles: source code, the production environment, and even what their customers were seeing.\n\nThe first big breakthrough for the Infosec team occured\n\nduring a company-wide hack week when they integrated static code analysis into the Twitter build process. The team\n\nused Brakeman, which scans Ruby on Rails applications for vulnerabilities. The goal was to integrate security scanning\n\ninto the earliest stages of the Development process, not just\n\nwhen the code was committed into the source code repo.",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "Figure 44: Number of Brakeman security vulnerabilities detected\n\nThe results of integrating security testing into the development process were breathtaking. Over the years, by\n\ncreating fast feedback for developers when they write\n\ninsecure code and showing them how to fix the vulnerabilities, Brakeman has reduced the rate of\n\nvulnerabilities found by 60%, as shown in figure 44. (The\n\nspikes are usually associated with new releases of Brakeman.)\n\nThis cases study illustrates just how necessary it is to\n\nintegrate security into the daily work and tools of DevOps and how effectively it can work. Doing so mitigates security\n\nrisk, reduces the probability of vulnerabilities in the system, and helps teach developers to write more secure code.\n\nENSURE SECURITY OF OUR SOFTWARE SUPPLY CHAIN",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "Josh Corman observed that as developers “we are no longer\n\nwriting customized software—instead, we assemble what we need\n\nfrom open source parts, which has become the software supply chain that we are very much reliant upon.” In other words, when\n\nwe use components or libraries —either commercial or open source —in our software, we not only inherit their functionality, but also\n\nany security vulnerabilities they contain.\n\nWhen selecting software, we detect when our software projects are relying on components or libraries that have known\n\nvulnerabilities, and help developers choose the components they\n\nuse deliberately and with due care, selecting those components (e.g., open source projects) that have a demonstrated history of\n\nquickly fixing software vulnerabilities. We also look for multiple\n\nversions of the same library being used across our production landscape, particularly the presence of older versions of libraries\n\nwhich contain known vulnerabilities.\n\nExamining cardholder data breaches shows how important the security of open source components we choose can be. Since\n\n2008, the annual Verizon PCI Data Breach Investigation Report\n\n(DBIR) has been the most authoritative voice on data breaches where cardholder data was lost or stolen. In the 2014 report, they\n\nstudied over eighty-five thousand breaches to better understand\n\nwhere attacks were coming from, how cardholder data was stolen, and factors leading to the breach.\n\nThe DBIR found that ten vulnerabilities (i.e., CVEs) accounted for\n\nalmost 97% of the exploits used in studied cardholder data breaches in 2014. Of these ten vulnerabilities, eight of them were\n\nover ten years old.",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "The 2015 Sonatype State of the Software Supply Chain Report\n\nfurther analyzed the vulnerability data from the Nexus Central Repository. In 2015, this repository provided the build artifacts for\n\nover 605,000 open source projects, servicing over seventeen\n\nbillion download requests of artifacts and dependencies primarily for the Java platform, originating from 106,000 organizations.\n\nThe report included these startling findings:\n\nThe typical organization relied upon 7,601 build artifacts (i.e.,\n\nsoftware suppliers or components) and used 18,614 different versions (i.e., software parts).\n\nOf those components being used, 7.5% had known\n\nvulnerabilities, with over 66% of those vulnerabilities being over two years old without having been resolved.\n\nThe last statistic confirms another information security study by\n\nDr. Dan Geer and Josh Corman, which showed that of the open source projects with known vulnerabilities registered in the\n\nNational Vulnerability Database, only 41% were ever fixed and\n\nrequired, on average, 390 days to publish a fix. For those vulnerabilities that were labeled at the highest severity (i.e., those scored as CVSS level 10), fixes required 224 days.§\n\nENSURE SECURITY OF THE ENVIRONMENT\n\nIn this step, we should do whatever is required to help ensure that the environments are in a hardened, risk-reduced state. Although\n\nwe may have created known, good configurations already, we",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "must put in monitoring controls to ensure that all production\n\ninstances match these known good states.\n\nWe do this by generating automated tests to ensure that all appropriate settings have been correctly applied for configuration\n\nhardening, database security settings, key lengths, and so forth. Furthermore, we will use tests to scan our environments for known vulnerabilities.¶\n\nAnother category of security verification is understanding actual environments (i.e., “as they actually are”). Examples of tools for\n\nthis include Nmap to ensure that only expected ports are open and\n\nMetasploit to ensure that we’ve adequately hardened our environments against known vulnerabilities, such as scanning\n\nwith SQL injection attacks. The output of these tools should be put\n\ninto our artifact repository and compared with the previous version as part of our functional testing process. Doing this will\n\nhelp us detect any undesirable changes as soon as they occur.\n\nCase Study 18F Automating Compliance for the Federal Government with Compliance Masonry\n\nUS Federal Government agencies were projected to spend\n\nnearly $80 billion on IT in 2016, supporting the mission of all\n\nthe executive branch agencies. Regardless of agency, to take any system from “dev complete” to “live in production”\n\nrequires obtaining an Authority to Operate (ATO) from a\n\nDesignated Approving Authority (DAA). The laws and policies that govern complience in government are\n\ncomprised of tens of documents that together number over",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "four thousand pages, littered with acronyms such as FISMA,\n\nFedRAMP, and FITARA. Even for systems that only require low levels of confidentiality, integrity, and availability, over\n\none hundred controls must be implemented, documented,\n\nand tested. It typically takes between eight and fourteen months for an ATO to be granted following “dev complete.”\n\nThe 18F team in the federal government’s General Services\n\nAdministration has taken a multi-pronged approach to solving this problem. Mike Bland explains, “18F was created\n\nwithin the General Services Administration to capitalize on\n\nthe momentum generated by the Healthcare.gov recovery to reform how the government builds and buys software.”\n\nOne 18F effort is a platform as a service called Cloud.gov,\n\ncreated from open source components. Cloud.gov runs on AWS GovCloud at present. Not only does the platform\n\nhandle many of the operational concerns delivery teams\n\nmight otherwise have to take care of, such as logging, monitoring, alerting, and service lifecycle management, it\n\nalso handles the bulk of compliance concerns. By running\n\non this platform, a large majority of the controls that government systems must implement can be taken care of\n\nat the infrastructure and platform level. Then, only the remaining controls that are in scope at the application layer\n\nhave to be documented and tested, significantly reducing\n\nthe compliance burden and the time it takes to receive an ATO.\n\nAWS GovCloud has already been approved for use for\n\nfederal government systems of all types, including those which require high levels of confidentiality, integrity, and",
      "content_length": 1620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "availability. By the time you read this book, it is expected that\n\nCloud.gov will be approved for all systems that require moderate levels of confidentiality, integrity, and availability.**\n\nFurthermore, the Cloud.gov team is building a framework to\n\nautomate the creation of system security plans (SSPs), which are “comprehensive descriptions of the system’s\n\narchitecture, implemented controls, and general security\n\nposture…[which are] often incredibly complex, running several hundred pages in length.” They developed a\n\nprototype tool called compliance masonry so that SSP data\n\nis stored in machine-readable YAML and then turned into GitBooks and PDFs automatically.\n\n18F is dedicated to working in the open and publishes its\n\nwork open source in the public domain. You can find compliance masonry and the components that make up\n\nCloud.gov in 18F’s GitHub repositories—you can even stand\n\nup your own instance of Cloud.gov. The work on open documentation for SSPs is being done in close partnership\n\nwith the OpenControl community.\n\nINTEGRATE INFORMATION SECURITY INTO PRODUCTION TELEMETRY\n\nMarcus Sachs, one of the Verizon Data Breach researchers, observed in 2010, “Year after year, in the vast majority of\n\ncardholder data breaches, the organization detected the security\n\nbreach months or quarters after the breach occurred. Worse, the way the breach was detected was not an internal monitoring\n\ncontrol, but was far more likely someone outside of the",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "organization, usually a business partner or the customer who\n\nnotices fraudulent transactions. One of the primary reasons for this is that no one in the organization was regularly reviewing the\n\nlog files.”\n\nIn other words, internal security controls are often ineffective in successfully detecting breaches in a timely manner, either because\n\nof blind spots in our monitoring or because no one in our\n\norganization is examining the relevant telemetry in their daily work.\n\nIn chapter 14, we discussed creating a culture in Dev and Ops\n\nwhere everyone in the value stream is creating production telemetry and metrics, making them visible in prominent public\n\nplaces so that everyone can see how our services are performing in\n\nproduction. Furthermore, we explored the necessity of relentlessly seeking ever-weaker failure signals so that we can find and fix\n\nproblems before they result in a catastrophic failure.\n\nHere, we deploy the monitoring, logging, and alerting required to fulfill our information security objectives throughout our\n\napplications and environments, as well as ensure that it is\n\nadequately centralized to facilitate easy and meaningful analysis and response.\n\nWe do this by integrating our security telemetry into the same\n\ntools that Development, QA, and Operations are using, giving everyone in the value stream visibility into how their application\n\nand environments are performing in a hostile threat environment\n\nwhere attackers are constantly attempting to exploit vulnerabilities, gain unauthorized access, plant backdoors,\n\ncommit fraud, perform denials-of-service, and so forth.",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "By radiating how our services are being attacked in the production\n\nenvironment, we reinforce that everyone needs to be thinking about security risks and designing countermeasures in their daily\n\nwork.\n\nCREATING SECURITY TELEMETRY IN OUR APPLICATIONS\n\nIn order to detect problematic user behavior that could be an indicator or enabler of fraud and unauthorized access, we must\n\ncreate the relevant telemetry in our applications.\n\nExamples may include:\n\nSuccessful and unsuccessful user logins\n\nUser password resets\n\nUser email address resets\n\nUser credit card changes\n\nFor instance, as an early indicator of brute-force login attempts to\n\ngain unauthorized access, we might display the ratio of\n\nunsuccessful login attempts to successful logins. And, of course, we should create alerting around important events to ensure we\n\ncan detect and correct issues quickly.\n\nCREATING SECURITY TELEMETRY IN OUR ENVIRONMENT",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "In addition to instrumenting our application, we also need to\n\ncreate sufficient telemetry in our environments so that we can detect early indicators of unauthorized access, especially in the\n\ncomponents that are running on infrastructure that we do not\n\ncontrol (e.g., hosting environments, in the cloud).\n\nWe need to monitor and potentially alert on items, including the\n\nfollowing:\n\nOS changes (e.g., in production, in our build infrastructure)\n\nSecurity group changes\n\nChanges to configurations (e.g., OSSEC, Puppet, Chef,\n\nTripwire)\n\nCloud infrastructure changes (e.g., VPC, security groups, users and privileges)\n\nXSS attempts (i.e., “cross-site scripting attacks”)\n\nSQLi attempts (i.e., “SQL injection attacks”)\n\nWeb server errors (e.g., 4XX and 5XX errors)\n\nWe also want to confirm that we’ve correctly configured our\n\nlogging so that all telemetry is being sent to the right place. When we detect attacks, in addition to logging that it happened, we may\n\nalso choose to block access and store information about the source\n\nto aid us in choosing the best mitigation actions.\n\nCase Study Instrumenting the Environment at Etsy (2010)",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "In 2010, Nick Galbreath was director of engineering at Etsy\n\nand responsible for information security, fraud control, and privacy. Galbreath defined fraud as when “the system works\n\nincorrectly, allowing invalid or un-inspected input into the\n\nsystem, causing financial loss, data loss/theft, system downtime, vandalism, or an attack on another system.”\n\nTo achieve these goals, Galbreath did not create a separate\n\nfraud control or information security department; instead, he embedded those responsibilities throughout the DevOps\n\nvalue stream.\n\nGalbreath created security-related telemetry that were displayed alongside all the other more Dev and Ops oriented\n\nmetrics, which every Etsy engineer routinely saw:\n\nAbnormal production program terminations (e.g.,\n\nsegmentation faults, core dumps, etc.): “Of particular\n\nconcern was why certain processes kept dumping core\n\nacross our entire production environment, triggered from\n\ntraffic coming from the one IP address, over and over again. Of equal concern were those HTTP ‘500 Internal Server\n\nErrors.’ These are indicators that a vulnerability was being\n\nexploited to gain unauthorized access to our systems, and\n\nthat a patch needs to be urgently applied.”\n\nDatabase syntax error: “We were always looking for\n\ndatabase syntax errors inside our code—these either\n\nenabled SQL Injection attacks or were actual attacks in\n\nprogress. For this reason, we had zero-tolerance for",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "database syntax errors in our code, because it remains one\n\nof the leading attack vectors used to compromise systems.”\n\nIndications of SQL injection attacks: “This was a\n\nridiculously simple test—we’d merely alert whenever\n\n‘UNION ALL’ showed up in user-input fields, since it almost\n\nalways indicates a SQL injection attack. We also added unit tests to make sure that this type of uncontrolled user input\n\ncould never be allowed into our database queries.”\n\nFigure 45: Developers would see SQL injection attempts in Graphite at Etsy (Source: “DevOpsSec: Appling DevOps Priciples to Security, DevOpsDays Austin 2012,” SlideShare.net, posted by Nick Galbreath, April 12, 2012, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops- principles-to-security.)\n\nFigure 45 is an example of a graph that every developer\n\nwould see, which shows the number of potential SQL\n\ninjection attacks that were attempted in the production\n\nenvironment. As Galbreath observed, “Nothing helps developers understand how hostile the operating\n\nenvironment is than seeing their code being attacked in real-\n\ntime.”\n\nGalbreath observed, “One of the results of showing this\n\ngraph was that developers realized that they were being",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "attacked all the time! And that was awesome, because it\n\nchanged how developers thought about the security of their\n\ncode as they were writing the code.”\n\nPROTECT OUR DEPLOYMENT PIPELINE\n\nThe infrastructure that supports our continuous integration and\n\ncontinuous deployment processes also presents a new surface area\n\nvulnerable to attack. For instance, if someone compromises the\n\nservers running deployment pipeline that has the credentials for\n\nour version control system, it could enable someone to steal source code. Worse, if the deployment pipeline has write access,\n\nan attacker could also inject malicious changes into our version\n\ncontrol repository, and, therefore, inject malicious changes into\n\nour application and services.\n\nAs Jonathan Claudius, former Senior Security Tester at TrustWave\n\nSpiderLabs, observed, “Continuous build and test servers are\n\nawesome, and I use them myself. But I started thinking about\n\nways to use CI/CD as a way to inject malicious code. Which led to\n\nthe question of where would be a good place to hide malicious code? The answer was obvious: in the unit tests. No one actually\n\nlooks at the unit tests, and they’re run every time someone\n\ncommits code to the repo.”\n\nThis demonstrates that in order to adequately protect the integrity\n\nof our applications and environments, we must also mitigate the\n\nattack vectors on our deployment pipeline. Risks include\n\ndevelopers introducing code that enables unauthorized access\n\n(which we’ve mitigated through controls such as code testing, code\n\nreviews, and penetration testing) and unauthorized users gaining",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "access to our code or environment (which we’ve mitigated through\n\ncontrols such as ensuring configurations match known, good\n\nstates, and effective patching).\n\nHowever, in order to protect our continuous build, integration, or\n\ndeployment pipeline, our mitigation strategies may include:\n\nHardening continuous build and integration servers and\n\nensuring we can reproduce them in an automated manner, just\n\nas we would for infrastructure that supports customer-facing\n\nproduction services, to prevent our continuous build and integration servers from being compromised\n\nReviewing all changes introduced into version control, either\n\nthrough pair programming at commit time or by a code review process between commit and merge into trunk, to prevent\n\ncontinuous integration servers from running uncontrolled code\n\n(e.g., unit tests may contain malicious code that allows or\n\nenables unauthorized access)\n\nInstrumenting our repository to detect when test code contains\n\nsuspicious API calls (e.g., unit tests accessing the filesystem or\n\nnetwork) is checked in to the repository, perhaps quarantining\n\nit and triggering an immediate code review\n\nEnsuring every CI process runs on its own isolated container or\n\nVM\n\nEnsuring the version control credentials used by the CI system\n\nare read-only\n\nCONCLUSION",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Throughout this chapter we have described ways to integrate\n\ninformation security objectives into all stages of our daily work.\n\nWe do this by integrating security controls into the mechanisms\n\nwe’ve already created, ensuring that all on-demand environments\n\nare also in a hardened, risk-reduced state—by integrating security testing into the deployment pipeline and ensuring the creation of\n\nsecurity telemetry in pre-production and production\n\nenvironments. By doing so, we enable developer and operational\n\nproductivity to increase while simultaneously increasing our\n\noverall safety. Our next step is to protect the deployment pipeline.\n\n† The Open Web Application Security Project (OWASP) is a non-profit organization focused on improving the\n\nsecurity of software.\n\n‡ Strategies for managing these risks include providing employee training and management; rethinking the design of information systems, including network and software; and instituting processes designed to prevent, detect, and respond to attacks.\n\n§ Tools that can help ensure the integrity of our software dependencies include OWASP Dependency Check and\n\nSonatype Nexus Lifecycle.\n\n¶ Examples of tools that can help with security correctness testing (i.e., “as it should be”) include automated\n\nconfiguration management systems (e.g., Puppet, Chef, Ansible, Salt), as well as tools such as ServerSpec and the Netflix Simian Army (e.g., Conformity Monkey, Security Monkey, etc.).\n\n** These approvals are known as FedRAMP JAB P-ATOs.",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "23Protecting the Deployment Pipeline\n\nThroughout this chapter we will look at how to protect our\n\ndeployment pipeline, as well as how to acheive security and\n\ncompliance objectives in our control environment, including\n\nchange management and separation of duty.\n\nINTEGRATE SECURITY AND COMPLIANCE INTO CHANGE APPROVAL PROCESSES\n\nAlmost any IT organization of any significant size will have\n\nexisting change management processes, which are the primary controls to reduce operations and security risks. Compliance\n\nmanager and security managers place reliance on change\n\nmanagement processes for compliance requirements, and they typically require evidence that all changes have been appropriately authorized.\n\nIf we have constructed our deployment pipeline correctly so that\n\ndeployments are low-risk, the majority of our changes won’t need to go through a manual change approval process, because we will have placed our reliance on controls such as automated testing and proactive production monitoring.",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "In this step, we will do what is required to ensure that we can\n\nsuccessfully integrate security and compliance into any existing\n\nchange management process. Effective change management\n\npolicies will recognize that there are different risks associated with\n\ndifferent types of changes and that those changes are all handled\n\ndifferently. These processes are defined in ITIL, which breaks\n\nchanges down into three categories:\n\nStandard changes: These are lower-risk changes that follow\n\nan established and approved process, but can also be pre-\n\napproved. They include monthly updates of application tax\n\ntables or country codes, website content and styling changes, and certain types of application or operating system patches that have a well-understood impact. The change proposer does\n\nnot require approval before deploying the change, and change deployments can be completely automated and should be logged so there is traceability.\n\nNormal changes: These are higher-risk changes that require review or approval from the agreed upon change authority. In many organizations, this responsibility is inappropriately\n\nplaced on the change advisory board (CAB) or emergency change advisory board (ECAB), which may lack the required\n\nexpertise to understand the full impact of the change, often leading to unacceptably long lead times. This problem is especially relevant for large code deployments, which may contain hundreds of thousands (or even millions) of lines of new code, submitted by hundreds of developers over the course\n\nof several months. In order for normal changes to be authorized, the CAB will almost certainly have a well-defined request for change (RFC) form that defines what information is",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "required for the go/no-go decision. The RFC form usually\n\nincludes the desired business outcomes, planned utility and warranty,† a business case with risks and alternatives, and a proposed schedule.‡\n\nUrgent changes: These are emergency, and, consequently,\n\npotentially high risk, changes that must be put into production\n\nimmediately (e.g., urgent security patch, restore service). These\n\nchanges often require senior management approval, but allow\n\ndocumentation to be performed after the fact. A key goal of\n\nDevOps practices is to streamline our normal change process\n\nsuch that it is also suitable for emergency changes.\n\nRE-CATEGORIZE THE MAJORITY OF OUR LOWER RISK CHANGES AS STANDARD CHANGES\n\nIdeally, by having a reliable deployment pipeline in place, we will have already earned a reputation for fast, reliable, and non- dramatic deployments. At this point, we should seek to gain agreement from Operations and the relevant change authorities that our changes have been demonstrated to be low risk enough to\n\nbe defined as standard changes, pre-approved by the CAB. This enables us to deploy into production without need for further approval, although the changes should still be properly recorded.\n\nOne way to support an assertion that our changes are low risk is to show a history of changes over a significant time period (e.g., months or quarters) and provide a complete list of production\n\nissues during that same period. If we can show high change",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "success rates and low MTTR, we can assert that we have a control environment that is effectively preventing deployment errors, as\n\nwell as prove that we can effectively and quickly detect and correct any resulting problems.\n\nEven when our changes are categorized as standard changes, they\n\nstill need to be visual and recorded in our change management\n\nsystems (e.g., Remedy or ServiceNow). Ideally, deployments will be performed automatically by our configuration management\n\nand deployment pipeline tools (e.g., Puppet, Chef, Jenkins) and the results will be automatically recorded. By doing this, everyone\n\nin our organization (DevOps or not) will have visibility into our changes in addition to all the other changes happening in the\n\norganization.\n\nWe may automatically link these change request records to\n\nspecific items in our work planning tools (e.g., JIRA, Rally, LeanKit, ThoughtWorks Mingle), allowing us to create more\n\ncontext for our changes, such as linking to feature defects, production incidents, or user stories. This can be accomplished in\n\na lightweight way by including ticket numbers from planning tools in the comments associated with version control check ins.§ By doing this, we can trace a production deployment to the changes\n\nin version control and, from there, trace them further back to the planning tool tickets.\n\nCreating this traceability and context should be easy and should\n\nnot create an overly onerous or time-consuming burden for engineers. Linking to user stories, requirements, or defects is\n\nalmost certainly sufficient—any further detail, such as opening a ticket for each commit to version control, is likely not useful, and",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "thus unnecessary and undesired, as it will impose a significant level of friction on their daily work.\n\nWHAT TO DO WHEN CHANGES ARE CATEGORIZED AS NORMAL CHANGES\n\nFor those changes that we cannot get classified as standard\n\nchanges, they will be considered normal changes and will require approval from at least a subset of the CAB before deployment. In\n\nthis case, our goal is still to ensure that we can deploy quickly, even if it is not fully automated.\n\nIn this case, we must ensure that any submitted change requests\n\nare as complete and accurate as possible, giving the CAB\n\neverything they need to properly evaluate our change—after all, if our change request is malformed or incomplete, it will be bounced\n\nback to us, increasing the time required for us to get into production and casting doubt on whether we actually understand\n\nthe goals of the change management process.\n\nWe can almost certainly automate the creation of complete and accurate RFCs, populating the ticket with details of exactly what is\n\nto be changed. For instance, we could automatically create a\n\nServiceNow change ticket with a link to the JIRA user story, along with the build manifests and test output from our deployment\n\npipeline tool and links to the Puppet/Chef scripts that will be run.\n\nBecause our submitted changes will be manually evaluated by people, it is even more important that we describe the context of\n\nthe change. This includes identifying why we are making the",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "change (e.g., providing a link to the features, defects, or incidents),\n\nwho is affected by the change, and what is going to be changed.\n\nOur goal is to share the evidence and artifacts that give us\n\nconfidence that the change will operate in production as designed. Although RFCs typically have free-form text fields, we should\n\nprovide links to machine-readable data to enable others to integrate and process our data (e.g., links to JSON files).\n\nIn many toolchains, this can be done in a compliant and fully\n\nautomated way. For example, ThoughtWorks’ Mingle and Go can automatically link this information together, such as a list of\n\ndefects fixed and new features completed that are associated with\n\nthe change, and put it into an RFC.\n\nUpon submission of our RFC, the relevant members of the CAB will review, process, and approve these changes as they would any\n\nother submitted change request. If all goes well, the change authorities will appreciate the thoroughness and detail of our\n\nsubmitted changes, because we have allowed them to quickly validate the correctness of the information we’ve provided (e.g.,\n\nviewing the links to artifacts from our deployment pipeline tools).\n\nHowever, our goal should be to continually show an exemplary track record of successful changes, so we can eventually gain their\n\nagreement that our automated changes can be safely classified as standard changes.\n\nCase Study Automated Infrastructure Changes as Standard Changes at Salesforce.com (2012)",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "Salesforce was founded in 2000 with the aim of making customer relationship management easily available and\n\ndeliverable as a service. Salesforce’s offerings were widely adopted by the marketplace, leading to a successful IPO in 2004. By 2007, the company had over fifty-nine thousand\n\nenterprise customers, processing hundreds of millions of transactions per day, with annual revenue of $497 million.\n\nHowever, around that same time, their ability to develop and\n\nrelease new functionality to their customers seemed to grind to a halt. In 2006, they had four major customer releases, but in 2007 they were only able to do one customer release\n\ndespite having hired more engineers. The result was that the number of features delivered per team kept decreasing and the days between major releases kept increasing.\n\nAnd because the batch size of each release kept getting larger, the deployment outcomes also kept getting worse. Karthik Rajan, then VP of Infrastructure Engineering, reports in a 2013 presentation that 2007 marked “the last year when\n\nsoftware was created and shipped using a waterfall process and when we made our shift to a more incremental delivery process.”\n\nAt the 2014 DevOps Enterprise Summit, Dave Mangot and Reena Mathew described the resulting multi-year DevOps transformation that started in 2009. According to Mangot\n\nand Mathew, by implementing DevOps principles and practices, the company reduced their deployment lead times from six days to five minutes by 2013. As a result, they were able to scale capacity more easily, allowing them to process\n\nover one billion transactions per day.",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "One of the main themes of the Salesforce transformation was to make quality engineering everyone’s job, regardless\n\nof whether they were part of Development, Operations, or Infosec. To do this, they integrated automated testing into all stages of the application and environment creation, as well\n\nas into the continuous integration and deployment process, and created the open source tool Rouster to conduct functional testing of their Puppet modules.\n\nThey also started to routinely perform destructive testing, a term used in manufacturing to refer to performing prolonged endurance testing under the most severe operating conditions until the component being tested is destroyed.\n\nThe Salesforce team started routinely testing their services under increasingly higher loads until the service broke, which helped them understand their failure modes and make\n\nappropriate corrections. Unsurprisingly, the result was significantly higher service quality with normal production loads.\n\nInformation Security also worked with Quality Engineering at the earliest stages of their project, continually collaborating in critical phases such as architecture and test design, as\n\nwell as properly integrating security tools into the automated testing process.\n\nFor Mangot and Mathew, one of the key successes from all\n\nthe repeatability and rigor they designed into the process was being told by their change management group that “infrastructure changes made through Puppet would now be treated as ‘standard changes,’ requiring far less or even no\n\nfurther approvals from the CAB.” Furthermore, they noted",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "that “manual changes to infrastructure would still require approvals.”\n\nBy doing this, they had not only integrated their DevOps processes with the change management process, but also created further motivation to automate the change process\n\nfor more of their infrastructure.\n\nREDUCE RELIANCE ON SEPARATION OF DUTY\n\nFor decades, we have used separation of duty as one of our primary controls to reduce the risk of fraud or mistakes in the\n\nsoftware development process. It has been the accepted practice in most SDLCs to require developer changes to be submitted to a code librarian, who would review and approve the change before\n\nIT Operations promoted the change into production.\n\nThere are plenty of other less contentious examples of separation of duty in Ops work, such as server administrators ideally being\n\nable to view logs but not delete or modify them, in order to prevent someone with privileged access from deleting evidence of fraud or other issues.\n\nWhen we did production deployments less frequently (e.g., annually) and when our work was less complex, compartmentalizing our work and doing hand-offs were tenable\n\nways of conducting business. However, as complexity and deployment frequency increase, performing production deployments successfully increasingly requires everyone in the value stream to quickly see the outcomes of their actions.",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "Separation of duty often can impede this by slowing down and reducing the feedback engineers receive on their work. This prevents engineers from taking full responsibility for the quality of\n\ntheir work and reduces a firm’s ability to create organizational learning.\n\nConsequently, wherever possible, we should avoid using separation of duties as a control. Instead, we should choose controls such as pair programming, continuous inspection of code check-ins, and code review. These controls can give us the\n\nnecessary reassurance about the quality of our work. Furthermore, by putting these controls in place, if separation of duties is required, we can show that we achieve equivalent outcomes with\n\nthe controls we have created.\n\nCase Study PCI Compliance and a Cautionary Tale of Separating Duties at Etsy (2014)\n\nBill Massie is a development manager at Etsy and is responsible for the payment application called ICHT (an\n\nabbreviation for “I Can Haz Tokens”). ICHT takes customer credit orders through a set of internally-developed payment processing applications that handle online order entry by\n\ntaking customer-entered cardholder data, tokenizing it, communicating with the payment processor, and completing the order transaction.¶\n\nBecause the scope of the Payment Card Industry Data Security Standards (PCI DSS) cardholder data environment (CDE) is “the people, processes and technology that store,",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "process or transmit cardholder data or sensitive authentication data,” including any connected system components, the ICHT application has in scope for the PCI\n\nDSS.\n\nTo contain the PCI DSS scope, the ICHT application is physically and logically separated from the rest of the Etsy\n\norganization and is managed by a completely separate application team of developers, database engineers, networking engineers, and ops engineers. Each team\n\nmember is issued two laptops: one for ICHT (which are configured differently to meet the DSS requirements, as well as being locked in a safe when not in use) and one for the rest of Etsy.\n\nBy doing this, they were able to decouple the CDE environment from the rest of the Etsy organization, limiting\n\nthe scope of the PCI DSS regulations to one segregated area. The systems that form the CDE are separated (and managed differently) from the rest of Etsy’s environments at the physical, network, source code, and logical infrastructure\n\nlevels. Furthermore, the CDE is built and operated by a cross-functional team that is solely responsible for the CDE.\n\nThe ICHT team had to modify their continuous delivery\n\npractices in order to accommodate the need for code approvals. According to Section 6.3.2 of the PCI DSS v3.1, teams should review:\n\nAll custom code prior to release to production or customers in order to identify any potential coding vulnerability (using either manual or automated processes) as follows:",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Are code changes reviewed by individuals other than the originating code author, and by individuals knowledgeable about code-review techniques and secure coding practices?\n\nDo code reviews ensure code is developed according to secure coding guidelines?\n\nAre appropriate corrections implemented prior to release?\n\nAre code review results reviewed and approved by management prior to release?\n\nTo fulfill this requirement, the team initially decided to designate Massie as the change approver responsible for deploying any changes into production. Desired\n\ndeployments would be flagged in JIRA, and Massie would mark them as reviewed and approved, and manually deploy them into the ICHT production.\n\nThis has enabled Etsy to meet their PCI DSS requirements and get their signed Report of Compliance from their assessors. However, with regard to the team, significant\n\nproblems have resulted.\n\nMassie observes that one troubling side effect “is a level of ‘compartmentalization’ that is happening in the ICHT team\n\nthat no other group is having at Etsy. Ever since we implemented separation of duty and other controls required\n\nby the PCI DSS compliance, no one can be a full-stack\n\nengineer in this environment.”\n\nAs a result, while the rest of the Development and Operations teams at Etsy work together closely and deploy",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "changes smoothly and with confidence, Massie notes that\n\n“within our PCI environment, there is fear and reluctance\n\naround deployment and maintenance because no one has visibility outside their portion of the software stack. The\n\nseemingly minor changes we made to the way we work\n\nseem to have created an impenetrable wall between developers and ops, and creates an undeniable tension that\n\nno one at Etsy has had since 2008. Even if you have confidence in your portion, it’s impossible to get confidence\n\nthat someone else’s change isn’t going to break your part of\n\nthe stack.”\n\nThis case study shows that compliance is possible in organizations using DevOps. However, the potentially\n\ncautionary tale here is that all the virtues that we associate with high-performing DevOps teams are fragile—even a\n\nteam that has shared experiences with high trust and shared\n\ngoals can begin to struggle when low trust control mechanisms are put into place.\n\nENSURE DOCUMENTATION AND PROOF FOR AUDITORS AND COMPLIANCE OFFICERS\n\nAs technology organizations increasingly adopt DevOps patterns,\n\nthere is more tension than ever between IT and audit. These new\n\nDevOps patterns challenge traditional thinking about auditing, controls, and risk mitigation.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "As Bill Shinn, a principal security solutions architect at Amazon\n\nWeb Services, observes, “DevOps is all about bridging the gap\n\nbetween Dev and Ops. In some ways, the challenge of bridging the gap between DevOps and auditors and compliance officers is even\n\nlarger. For instance, how many auditors can read code and how\n\nmany developers have read NIST 800-37 or the Gramm-Leach- Bliley Act? That creates a gap of knowledge, and the DevOps\n\ncommunity needs to help bridge that gap.”\n\nCase Study Proving Compliance in Regulated Environments (2015)\n\nHelping large enterprise customers show that they can still\n\ncomply with all relevant laws and regulations is among Bill Shinn’s responsibilities as a principal security solutions\n\narchitect at Amazon Web Services. Over the years, he has\n\nspent time with over one thousand enterprise customers, including Hearst Media, GE, Phillips, and Pacific Life, who\n\nhave publicly referenced their use of public clouds in highly\n\nregulated environments.\n\nShinn notes, “One of the problems is that auditors have\n\nbeen trained in methods that aren’t very suitable for DevOps\n\nwork patterns. For example, if an auditor saw an environment with ten thousand productions servers, they\n\nhave been traditionally trained to ask for a sample of one\n\nthousand servers, along with screenshot evidence of asset management, access control settings, agent installations,\n\nserver logs, and so forth.”",
      "content_length": 1422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "“That was fine with physical environments,” Shinn continues.\n\n“But when infrastructure is code, and when auto-scaling\n\nmakes servers appear and disappear all the time, how do you sample that? You run into the same problems when you\n\nhave a deployment pipeline, which is very different than the traditional software development process, where one group\n\nwrites the code and another group deploys that code into\n\nproduction.”\n\nHe explains, “In audit fieldwork, the most commonplace methods of gathering evidence are still screenshots and\n\nCSV files filled with configuration settings and logs. Our goal is to create alternative methods of presenting the data that\n\nclearly show auditors that our controls are operating and\n\neffective.”\n\nTo help bridge that gap, he has teams work with auditors in\n\nthe control design process. They use an iterative approach,\n\nassigning a single control for each sprint to determine what is needed in terms of audit evidence. This has helped\n\nensure that auditors get the information they need when the\n\nservice is in production, entirely on demand.\n\nShinn states that the best way to accomplish this is to “send\n\nall data into our telemetry systems, such as Splunk or\n\nKibana. This way auditors can get what they need, completely self-serviced. They don’t need to request a data\n\nsample—instead, they log into Kibana, and then search for\n\naudit evidence they need for a given time range. Ideally, they’ll see very quickly that there’s evidence to support that\n\nour controls are working.”",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "Shinn continues, “With modern audit logging, chat rooms,\n\nand deployment pipelines, there’s unprecedented visibility\n\nand transparency into what’s happening in production, especially compared to how Operations used to be done,\n\nwith far lower probability of errors and security flaws being introduced. So, the challenge is to turn all that evidence into\n\nsomething an auditor recognizes.”\n\nThat requires deriving the engineering requirements from the actual regulations. Shinn explains, “To discover what\n\nHIPAA requires from an information security perspective,\n\nyou have to look into the forty-five CFR Part 160 legislation, go into Subparts A and C of Part 164. Even then, you need\n\nto keep reading until you get into ‘technical safeguards and\n\naudit controls.’ Only there will you see that what is required is that we need to determine activities that will be tracked\n\nand audited relevant to Patient Healthcare Information,\n\ndocument and implement those controls, select tools, and then finally review and capture the appropriate information.”\n\nShinn continues, “How to fulfill that requirement is the\n\ndiscussion that needs to be happening between compliance and regulatory officers, and the security and DevOps teams,\n\nspecifically around how to prevent, detect, and correct\n\nproblems. Sometimes they can be fulfilled in a configuration setting in version control. Other times, it’s a monitoring\n\ncontrol.”\n\nShinn gives an example: “We may choose to implement one of those controls using AWS CloudWatch, and we can test\n\nthat the control is operating with one command line.\n\nFurthermore, we need to show where the logs are going—in",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "the ideal, we push all this into our logging framework, where\n\nwe can link the audit evidence with the actual control requirement.”\n\nTo help solve this problem, the DevOps Audit Defense\n\nToolkit describes the end-to-end narrative of the compliance\n\nand audit process for a fictitious organization (Parts Unlimited from The Phoenix Project). It starts by describing\n\nthe entity’s organizational goals, business processes, top risks, and resulting control environment, as well as how\n\nmanagement could successfully prove that controls exist\n\nand are effective. A set of audit objections is also presented, as well as how to overcome them.\n\nThe document describes how controls could be designed in\n\na deployment pipeline to mitigate the stated risks, and provides examples of control attestations and control\n\nartifacts to demonstrate control effectiveness. It was\n\nintended to be general to all control objectives, including in support of accurate financial reporting, regulatory\n\ncompliance (e.g., SEC SOX-404, HIPAA, FedRAMP, EU\n\nModel Contracts, and the proposed SEC Reg-SCI regulations), contractual obligations (e.g., PCI DSS, DOD\n\nDISA), and effective and efficient operations.\n\nCase Study Relying on Production Telemetry for ATM Systems\n\nMary Smith (a pseudonym) heads up the DevOps initiative\n\nfor the consumer banking property of a large US financial",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "services organization. She made the observation that\n\ninformation security, auditors, and regulators often put too much reliance on code reviews to detect fraud. Instead, they\n\nshould be relying on production monitoring controls in\n\naddition to using automated testing, code reviews, and approvals, to effectively mitigate the risks associated with\n\nerrors and fraud.\n\nShe observed:\n\nMany years ago, we had a developer who planted a\n\nbackdoor in the code that we deploy to our ATM cash\n\nmachines. They were able to put the ATMs into maintenance mode at certain times, allowing them to\n\ntake cash out of the machines. We were able to detect\n\nthe fraud very quickly, and it wasn’t through a code review. These types of backdoors are difficult, or even\n\nimpossible, to detect when the perpetrators have\n\nsufficient means, motive, and opportunity.\n\nHowever, we quickly detected the fraud during our\n\nregularly operations review meeting when someone\n\nnoticed that ATMs in a city were being put into maintenance mode at unscheduled times. We found\n\nthe fraud even before the scheduled cash audit\n\nprocess, when they reconcile the amount of cash in the ATMs with authorized transactions.\n\nIn this case study, the fraud occurred despite separation of\n\nduties between Development and Operations and a change approval process, but was quickly detected and corrected\n\nthrough effective production telemetry.",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "CONCLUSION\n\nThroughout this chapter, we have discussed practices that make\n\ninformation security everyone’s job, where all of our information security objectives are integrated into the daily work of everyone\n\nin the value stream. By doing this, we significantly improve the\n\neffectiveness of our controls, so that we can better prevent security breaches, as well as detect and recover from them faster. And we\n\nsignificantly reduce the work associated with preparing and\n\npassing compliance audits.",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "PART VI CONCLUSION\n\nThroughout the previous chapters, we explored how to take\n\nDevOps principles and apply them to Information Security, helping us achieve our goals, and making sure security is a part of\n\neveryone’s job, every day. Better security ensures that we are\n\ndefensible and sensible with our data, that we can recover from security problems before they become catastrophic, and, most\n\nimportantly, that we can make the security of our systems and data better than ever.\n\n† ITIL defines utility as “what the service does,” while warranty is defined as “how the service is delivered and can\n\nbe used to determine whether a service is ‘fit for use.’”\n\n‡ To further manage risk changes, we may also have defined rules, such as certain changes can only be\n\nimplemented by a certain group or individual (e.g., only DBAs can deploy database schema changes). Traditionally, the CAB meetings have been held weekly, where the change requests are approved and scheduled. From ITIL version 3 onward, it is acceptable for changes to be approved electronically in a just-in- time fashion through a change management tool. It also specifically recommends that “standard changes should be identified early on when building the Change Management process to promote efficiency. Otherwise, a Change Management implementation can create unnecessarily high levels of administration and resistance to the Change Management process.”\n\n§ The term ticket is used generically to indicate any uniquely identifiable work item.\n\n¶ The authors thank Bill Massie and John Allspaw for spending an entire day with Gene Kim sharing their\n\ncompliance experience.",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "A Call to Action",
      "content_length": 16,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "Conclusion to the DevOps\n\nHandbook\n\nWe have come to the end of a detailed exploration of both the\n\nprinciples and technical practices of DevOps. At a time when every\n\ntechnology leader is challenged with enabling security, reliability,\n\nand agility, and at a time when security breaches, time to market,\n\nand massive technology transformation is taking place, DevOps\n\noffers a solution. Hopefully, this book has provided an in-depth understanding of the problem and a road map to creating relevant\n\nsolutions.\n\nAs we have explored throughout The DevOps Handbook, we know that, left unmanaged, an inherent conflict can exist between\n\nDevelopment and Operations that creates ever-worsening problems,which results in slower time to market for new products and features, poor quality, increased outages and technical debt, reduced engineering productivity, as well as increased employee dissatisfaction and burnout.\n\nDevOps principles and patterns enable us to break this core, chronic conflict. After reading this book, we hope you see how a DevOps transformation can enable the creation of dynamic\n\nlearning organizations, achieving the amazing outcomes of fast flow and world-class reliability and security, as well as increased competitiveness and employee satisfaction.\n\nDevOps requires potentially new cultural and management norms, and changes in our technical practices and architecture. This",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "requires a coalition that spans business leadership, Product\n\nManagement, Development, QA, IT Operations, Information\n\nSecurity, and even Marketing, where many technology initiatives\n\noriginate. When all these teams work together, we can create a\n\nsafe system of work, enabling small teams to quickly and\n\nindependently develop and validate code that can be safely\n\ndeployed to customers. This results in maximizing developer\n\nproductivity, organizational learning, high employee satisfaction,\n\nand the ability to win in the marketplace.\n\nOur goal in writing this book was to sufficiently codify DevOps\n\nprinciples and practices so that the amazing outcomes achieved within the DevOps community could be replicated by others. We hope to accelerate the adoption of DevOps initiatives and support\n\ntheir successful implementations while lowering the activation energy required for them to be completed.\n\nWe know the dangers of postponing improvements and settling for daily work-arounds, as well as the difficulties of changing how we prioritize and perform our daily work. Furthermore, we understand the risks and effort required to get organizations to embrace a different way of working, as well as the perception that DevOps is another passing fad, soon to replaced by the next\n\nbuzzword.\n\nWe assert that DevOps is transformational to how we perform\n\ntechnology work, just as Lean forever transformed how manufacturing work was performed in the 1980s. Those that adopt DevOps will win in the marketplace, at the expense of those that do not. They will create energized and continually learning\n\norganizations that out-perform and out-innovate their competitors.",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Because of this, DevOps is not just a technology imperative, but also an organizational imperative. The bottom line is, DevOps is\n\napplicable and relevant to any and all organizations that must increase flow of planned work through the technology\n\norganization, while maintaining quality, reliability, and security\n\nfor our customers.\n\nOur call to action is this: no matter what role you play in your organization, start finding people around you who want to change\n\nhow work is performed. Show this book to others and create a coalition of like-minded thinkers to break out of the downward\n\nspiral. Ask organizational leaders to support these efforts, or, better yet, sponsor and lead these efforts yourself.\n\nFinally, since you’ve made it this far, we have a dirty secret to\n\nreveal. In many of our case studies, following the achievement of\n\nthe breakthrough results presented, many of the change agents were promoted—but, in some cases, there was later a change of\n\nleadership which resulted in many of the people involved leaving, accompanied by a rolling back of the organizational changes they\n\nhad created.\n\nWe believe it’s important not to be cynical about this possibility. The people involved in these transformations knew up front that\n\nwhat they were doing had a high chance of failure, and they did it\n\nanyway. In doing so, perhaps most importantly, they inspired the rest of us by showing us what can be done. Innovation is\n\nimpossible without risk taking, and if you haven’t managed to upset at least some people in management, you’re probably not\n\ntrying hard enough. Don’t let your organization’s immune system deter or distract you from your vision. As Jesse Robbins,",
      "content_length": 1686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "previously “master of disaster” at Amazon, likes to say, “Don’t fight stupid, make more awesome.”\n\nDevOps benefits all of us in the technology value stream, whether\n\nwe are Dev, Ops, QA, Infosec, Product Owners, or customers. It brings joy back to developing great products, with fewer death\n\nmarches. It enables humane work conditions with fewer weekends and missed holidays with our loved ones. It enables teams to work\n\ntogether to survive, learn, thrive, delight our customers, and help\n\nour organization succeed.\n\nWe sincerely hope The DevOps Handbook helps you achieve these goals.",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "Appendices\n\nAPPENDIX 1 THE CONVERGENCE OF DEVOPS\n\nWe believe that DevOps is benefiting from an incredible\n\nconvergence of management movements, which are all mutually reinforcing and can help create a powerful coalition to transform\n\nhow organizations develop and deliver IT products and services.\n\nJohn Willis named this “the Convergence of DevOps.” The various elements of this convergence are described below in approximate\n\nchronological order. (Note that these descriptions are not intended to be an exhaustive description, but merely enough to\n\nshow the progression of thinking and the rather improbable\n\nconnections that led to DevOps.)\n\nTHE LEAN MOVEMENT\n\nThe Lean Movement started in the 1980s as an attempt to codify the Toyota Production System with the popularization of techniques such as Value Stream Mapping, kanban boards, and\n\nTotal Productive Maintenance.\n\nTwo major tenets of Lean were the deeply held belief that lead time (i.e., the time required to convert raw materials into finished goods) was the best predictor of quality, customer satisfaction,",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "and employee happiness; and that one of the best predictors of\n\nshort lead times was small batch sizes, with the theoretical ideal\n\nbeing “single piece flow” (i.e., “1x1” flow: inventory of 1, batch size\n\nof 1).\n\nLean principles focus on creating value for the customer—thinking\n\nsystematically, creating constancy of purpose, embracing scientific\n\nthinking, creating flow and pull (versus push), assuring quality at\n\nthe source, leading with humility, and respecting every individual.\n\nTHE AGILE MOVEMENT\n\nStarted in 2001, the Agile Manifesto was created by seventeen of the leading thinkers in software development, with the goal of\n\nturning lightweight methods such as DP and DSDM into a wider movement that could take on heavyweight software development processes such as waterfall development and methodologies such as the Rational Unified Process.\n\nA key principle was to “deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.” Two other principles focus on the need for\n\nsmall, self-motivated teams, working in a high-trust management model and an emphasis on small batch sizes. Agile is also\n\nassociated with a set of tools and practices such as Scrum, Standups, and so on.\n\nTHE VELOCITY CONFERENCE MOVEMENT\n\nStarted in 2007, the Velocity Conference was created by Steve Souders, John Allspaw, and Jesse Robbins to provide a home for the IT Operations and Web Performance tribe. At the Velocity",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "2009 conference, John Allspaw and Paul Hammond gave the\n\nseminal “10 Deploys per Day: Dev and Ops Cooperation at Flickr.”\n\nTHE AGILE INFRASTRUCTURE MOVEMENT\n\nAt the 2008 Agile Toronto conference, Patrick Dubois and Andrew\n\nSchafer held a “birds of a feather” session on applying Agile\n\nprinciples to infrastructure as opposed to application code. They\n\nrapidly gained a following of like-minded thinkers, including John\n\nWillis. Later, Dubois was so excited by Allspaw and Hammond’s\n\n“10 Deploys per Day: Dev and Ops Cooperation at Flickr”\n\npresentation that he created the first DevOpsDays in Ghent,\n\nBelgium, in 2009, coining the word “DevOps.”\n\nTHE CONTINUOUS DELIVERY MOVEMENT\n\nBuilding upon the Development discipline of continuous build, test, and integration, Jez Humble and David Farley extended the concept of continuous delivery, which included a “deployment\n\npipeline” to ensure that code and infrastructure are always in a deployable state and that all code checked in to truck is deployed into production.\n\nThis idea was first presented at Agile 2006 and was also\n\nindependently developed by Tim Fitz in a blog post titled “Continuous Deployment.”\n\nTHE TOYOTA KATA MOVEMENT\n\nIn 2009, Mike Rother wrote Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results, which described learnings over his twenty-year journey to understand\n\nand codify the causal mechanisms of the Toyota Production",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "System. Toyota Kata describes the “unseen managerial routines and thinking that lie behind Toyota’s success with continuous\n\nimprovement and adaptation… and how other companies develop similar routines and thinking in their organizations.”\n\nHis conclusion was that the Lean community missed the most\n\nimportant practice of all, which he described as the Improvement\n\nKata. He explains that every organization has work routines, and the critical factor in Toyota was making improvement work\n\nhabitual, and building it into the daily work of everyone in the organization. The Toyota Kata institutes an iterative, incremental,\n\nscientific approach to problem solving in the pursuit of a shared organizational true north.\n\nTHE LEAN STARTUP MOVEMENT\n\nIn 2011, Eric Ries wrote The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically\n\nSuccessful Businesses, codifying his lessons learned at IMVU, a Silicon Valley startup, which built upon the work of Steve Blank in\n\nThe Four Steps to the Epiphany as well as continuous deployment\n\ntechniques. Eric Ries also codified related practices and terms including Minimum Viable Product, the build-measure-learn\n\ncycle, and many continuous deployment technical patterns.\n\nTHE LEAN UX MOVEMENT\n\nIn 2013, Jeff Gothelf wrote Lean UX: Applying Lean Principles to Improve User Experience, which codified how to improve the\n\n“fuzzy front end” and explained how product owners can frame business hypotheses, experiment, and gain confidence in those\n\nbusiness hypotheses before investing time and resources in the",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "resulting features. By adding Lean UX, we now have the tools to fully optimize the flow between business hypotheses, feature\n\ndevelopment, testing, deployment, and service delivery to the customer.\n\nTHE RUGGED COMPUTING MOVEMENT\n\nIn 2011, Joshua Corman, David Rice, and Jeff Williams examined the apparent futility of securing applications and environments\n\nlate in the life cycle. In response, they created a philosophy called “Rugged Computing,” which attempts to frame the non-functional\n\nrequirements of stability, scalability, availability, survivability, sustainability, security, supportability, manageability, and\n\ndefensibility.\n\nBecause of the potential for high release rates, DevOps can put\n\nincredible pressure on QA and Infosec, because when deploy rates go from monthly or quarterly to hundreds or thousands daily, no\n\nlonger are two week turnaround times from Infosec or QA tenable. The Rugged Computing movement posited that the current\n\napproach to fighting the vulnerable industrial complex being employed by most information security programs is hopeless.\n\nAPPENDIX 2 THEORY OF CONSTRAINTS AND CORE, CHRONIC CONFLICTS\n\nThe Theory of Constraints body of knowledge extensively\n\ndiscusses the use of creating core conflict clouds (often referred to\n\nas “C3”). Here is the conflict cloud for IT:",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "Figure 46: The core, chronic conflict facing every IT organization\n\nDuring the 1980s, there was a very well-known core, chronic\n\nconflict in manufacturing. Every plant manager had two valid business goals: protect sales and reduce costs. The problem was\n\nthat in order to protect sales, sales management was incentivized\n\nto increase inventory to ensure that it was always possible to fulfill customer demand.\n\nOn the other hand, in order to reduce cost, production\n\nmanagement was incentivized to decrease inventory to ensure that money was not tied up in work in progress that wasn’t\n\nimmediately shippable to the customer in the form of fulfilled sales.\n\nThey were able to break the conflict by adopting Lean principles,\n\nsuch as reducing batch sizes, reducing work in process, and\n\nshortening and amplifying feedback loops. This resulted in dramatic increases in plant productivity, product quality, and\n\ncustomer satisfaction.\n\nThe principles behind DevOps work patterns are the same as those\n\nthat transformed manufacturing, allowing us to optimize the IT",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "value stream, converting business needs into capabilities and services that provide value for our customers.\n\nAPPENDIX 3 TABULAR FORM OF DOWNWARD SPIRAL\n\nThe columnar form of the downward spiral depicted in The Phoenix Project is shown below:\n\nTable 4: The Downward Spiral",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "APPENDIX 4 THE DANGERS OF HANDOFFS AND QUEUES\n\nThe problem with high amounts of queue time is exacerbated\n\nwhen there are many handoffs, because that is where queues are created. Figure 47 shows wait time as a function of how busy a resource at a work center is. The asymptotic curve shows why a\n\n“simple thirty-minute change” often takes weeks to complete— specific engineers and work centers often become problematic bottlenecks when they operate at high utilization. As a work center approaches 100% utilization, any work required from it will\n\nlanguish in queues and won’t be worked on without someone expediting/escalating.\n\nFigure 47: Queue size and wait times as function of percent utilization (Source: Kim, Behr, and Spafford, The Phoenix Project, ePub edition, 557.)\n\nIn figure 47, the x-axis is the percent busy for a given resource at a work center, and the y-axis is the approximate wait time (or, more precisely stated, the queue length). What the shape of the line",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "shows is that as resource utilization goes past 80%, wait time goes through the roof.\n\nIn The Phoenix Project, here’s how Bill and his team realized the devastating consequences of this property on lead times for the commitments they were making to the project management office:\n\nI tell them about what Erik told me at MRP-8, about how wait times depend upon resource utilization. “The wait time is the ‘percentage of time busy’ divided by the ‘percentage of time\n\nidle.’ In other words, if a resource is fifty percent busy, then it’s fifty percent idle. The wait time is fifty percent divided by fifty percent, so one unit of time. Let’s call it one hour.\n\nSo, on average, our task would wait in the queue for one hour before it gets worked.\n\n“On the other hand, if a resource is ninety percent busy, the wait time is ‘ninety percent divided by ten percent,’ or nine hours. In other words, our task would wait in queue nine times longer than if the resource were fifty percent idle.”\n\nI conclude, “So…For the Phoenix task, assuming we have seven handoffs, and that each of those resources is busy ninety percent of the time, the tasks would spend in queue a total of\n\nnine hours times the seven steps…”\n\n“What? Sixty-three hours, just in queue time?” Wes says, incredulously. “That’s impossible!”\n\nPatty says with a smirk, “Oh, of course. Because it’s only thirty seconds of typing, right?”",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "Bill and team realize that their “simple thirty-minute task” actually requires seven handoffs (e.g., server team, networking team, database team, virtualization team, and, of course, Brent,\n\nthe “rockstar” engineer).\n\nAssuming that all work centers were 90% busy, the figure shows\n\nus that the average wait time at each work center is nine hours— and because the work had to go through seven work centers, the total wait time is seven times that: sixty-three hours.\n\nIn other words, the total % of value added time (sometimes known as process time) was only 0.16% of the total lead time (thirty minutes divided by sixty-three hours). That means that for 99.8% of our total lead time, the work was simply sitting in queue,\n\nwaiting to be worked on.\n\nAPPENDIX 5 MYTHS OF INDUSTRIAL SAFETY\n\nDecades of research into complex systems shows that\n\ncountermeasures are based on several myths. In “Some Myths about Industrial Safety,” by Denis Besnard and Erik Hollnagel, they are summarized as such:\n\nMyth 1: “Human error is the largest single cause of accidents and incidents.”\n\nMyth 2: “Systems will be safe if people comply with the\n\nprocedures they have been given.”\n\nMyth 3: “Safety can be improved by barriers and protection; more layers of protection results in higher safety.”",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "Myth 4: “Accident analysis can identify the root cause (the ‘truth’) of why the accident happened.”\n\nMyth 5: “Accident investigation is the logical and rational identification of causes based on facts.”\n\nMyth 6: “Safety always has the highest priority and will never\n\nbe compromised.”\n\nThe differences between what is myth and what is true are shown below:\n\nTable 5: Two Stories\n\nAPPENDIX 6 THE TOYOTA ANDON CORD\n\nMany ask how can any work be completed if the Andon cord is being pulled over five thousand times per day? To be precise, not\n\nevery Andon cord pull results in stopping the entire assembly line. Rather, when the Andon cord is pulled, the team leader overseeing the specified work center has fifty seconds to resolve the problem.\n\nIf the problem has not been resolved by the time the fifty seconds",
      "content_length": 810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "is up, the partially assembled vehicle will cross a physically drawn line on the floor, and the assembly line will be stopped.\n\nFigure 48: The Toyota Andon cord\n\nAPPENDIX 7 COTS SOFTWARE\n\nCurrently, in order to get complex COTS (commercial off-the- shelf) software (e.g., SAP, IBM WebSphere, Oracle WebLogic) into\n\nversion control, we may have to eliminate the use of graphical\n\npoint-and-click vendor installer tools. To do that, we need to discover what the vendor installer is doing, and we may need to do\n\nan install on a clean server image, diff the file system, and put\n\nthose added files into version control. Files that don’t vary by environment are put into one place (“base install”), while\n\nenvironment-specific files are put into their own directory (“test”",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "or “production”). By doing this, software install operations\n\nbecome merely a version control operation, enabling better\n\nvisibility, repeatability, and speed.\n\nWe may also have to transform any application configuration\n\nsettings so that they are in version control. For instance, we may\n\ntransform application configurations that are stored in a database into XML files and vice versa.\n\nAPPENDIX 8 POST-MORTEM MEETINGS\n\nA sample agenda of the post-mortem meeting is shown below:\n\nAn initial statement will be made by the meeting leader or facilitator to reinforce that this meeting is a blameless post-\n\nmortem and that we will not focus on past events or speculate\n\non “would haves” or “could haves.” Facilitators might read the “Retrospective Prime Directive” from the website\n\nRetrospective.com.\n\nFurthermore, the facilitator will remind everyone that any countermeasures must be assigned to someone, and if the\n\ncorrective action does not warrant being a top priority when\n\nthe meeting is over, then it is not a corrective action. (This is to prevent the meeting from generating a list of good ideas that\n\nare never implemented.)\n\nThose at the meeting will reach an agreement on the complete timeline of the incident, including when and who detected the\n\nissue, how it was discovered (e.g., automated monitoring,",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "manual detection, customer notified us), when service was\n\nsatisfactorily restored, and so forth. We will also integrate into\n\nthe timeline all external communications during the incident.\n\nWhen we use the word “timeline,” it may evoke the image of a\n\nlinear set of steps of how we gained an understanding of the problem and eventually fixed it. In reality, especially in\n\ncomplex systems, there will likely be many events that contributed to the accident, and many troubleshooting paths\n\nand actions will have been taken in an effort to fix it. In this\n\nactivity, we seek to chronicle all of these events and the perspectives of the actors and establish hypotheses concerning\n\ncause and effect where possible.\n\nThe team will create a list of all the factors which contributed to the incident, both human and technical. They may then sort\n\nthem into categories, such as ‘design decision,’ ‘remediation,’\n\n‘discovering there was a problem,’ and so forth. The team will use techniques such as brainstorming and the ‘infinite hows’ to\n\ndrill down on contributing factors they deem particularly\n\nimportant to discover deeper levels of contributing factors. All perspectives should be included and respected—nobody should\n\nbe permitted to argue with or deny the reality of a contributing\n\nfactor somebody else has identified. It’s important for the post- mortem facilitator to ensure that sufficient time is spent on this\n\nactivity, and that the team doesn’t try and engage in convergent behavior such as trying to identify one or more ‘root causes.’\n\nThose at the meeting will reach an agreement on the list of\n\ncorrective actions that will be made top priorities after the\n\nmeeting. Assembling this list will require brainstorming and",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "choosing the best potential actions to either prevent the issue\n\nfrom occurring or enable faster detection or recovery. Other\n\nways to improve the systems may also be included.\n\nOur goal is to identify the smallest number of incremental steps to achieve the desired outcomes, as opposed to “big bang”\n\nchanges, which not only take longer to implement, but delay\n\nthe improvements we need.\n\nWe will also generate a separate list of lower priority ideas and\n\nassign an owner. If similar problems occur in the future, these ideas may serve as the foundation for crafting future\n\ncountermeasures.\n\nThose at the meeting will reach an agreement on the incident metrics and their organizational impact. For example, we may\n\nchoose to measure our incidents by the following metrics:\n\n▹ Event severity: How severe was this issue? This directly relates to the impact on the service and our\n\ncustomers.\n\n▹ Total downtime: How long were customers unable\n\nto use the service to any degree?\n\n▹ Time to detect: How long did it take for us or our\n\nsystems to know there was a problem?\n\n▹ Time to resolve: How long after we knew there was a\n\nproblem did it take for us to restore service?\n\nBethany Macri from Etsy observed, “Blamelessness in a post-\n\nmortem does not mean that no one takes responsibility. It means",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "that we want to find out what the circumstances were that allowed\n\nthe person making the change or who introduced the problem to\n\ndo this. What was the larger environment….The idea is that by removing blame, you remove fear, and by removing fear, you get\n\nhonesty.”\n\nAPPENDIX 9 THE SIMIAN ARMY\n\nAfter the 2011 AWS EAST Outage, Netflix had numerous\n\ndiscussions about engineering their systems to automatically deal\n\nwith failure. These discussions have evolved into a service called “Chaos Monkey.”\n\nSince then, Chaos Monkey has evolved into a whole family of\n\ntools, known internally as the “Netflix Simian Army,” to simulate increasingly catastrophic levels of failures:\n\nChaos Gorilla: simulates the failure of an entire AWS\n\navailability zone\n\nChaos Kong: simulates failure of entire AWS regions, such as\n\nNorth America or Europe\n\nOther member of the Simian Army now include:\n\nLatency Monkey: induces artificial delays or downtime in their RESTful client-server communication layer to simulate\n\nservice degradation and ensure that dependent services respond appropriately\n\nConformity Monkey: finds and shuts down AWS instances\n\nthat don’t adhere to best-practices (e.g., when instances don’t",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "belong to an auto-scaling group or when there is no escalation\n\nengineer email address listed in the service catalog)\n\nDoctor Monkey: taps into health checks that run on each instance and finds unhealthy instances and proactively shuts\n\nthem down if owners don’t fix the root cause in time\n\nJanitor Monkey: ensures that their cloud environment is running free of clutter and waste; searches for unused\n\nresources and disposes of them\n\nSecurity Monkey: an extension of Conformity Monkey; finds and terminates instances with security violations or\n\nvulnerabilities, such as improperly configured AWS security\n\ngroups\n\nAPPENDIX 10 TRANSPARENT UPTIME\n\nLenny Rachitsky wrote about the benefits of what he called\n\n“transparent uptime”:\n\n1. Your support costs go down as your users are able to self-\n\nidentify system wide problems without calling or emailing your\n\nsupport department. Users will no longer have to guess whether their issues are local or global, and can more quickly\n\nget to the root of the problem before complaining to you.\n\n2. You are better able to communicate with your users during\n\ndowntime events, taking advantage of the broadcast nature of\n\nthe Internet versus the one-to-one nature of email and the\n\nphone. You spend less time communicating the same thing over and over and more time resolving the issue.",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "3. You create a single and obvious place for your users to come to\n\nwhen they are experiencing downtime. You save your users’ time currently spent searching forums, Twitter, or your blog.\n\n4. Trust is the cornerstone of any successful SaaS adoption. Your\n\ncustomers are betting their business and their livelihoods on your service or platform. Both current and prospective\n\ncustomers require confidence in your service. Both need to\n\nknow they won’t be left in the dark, alone and uninformed, when you run into trouble. Real time insight into unexpected\n\nevents is the best way to build this trust. Keeping them in the\n\ndark and alone is no longer an option.\n\n5. It’s only a matter of time before every serious SaaS provider\n\nwill be offering a public health dashboard. Your users will\n\ndemand it.",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "Additional Resources\n\nMany of the common problems faced by IT organizations are discussed in the first half of the book The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business\n\nWin by Gene Kim, Kevin Behr, and George Spafford.\n\nThis video shows a speech Paul O’Neill gave on his tenure as CEO of Alcoa, including the\n\ninvestigation he took part in after a teenage worker was killed at one of Alcoa’s plants: https://www.youtube.com/watch?v=tC2ucDs_XJY .\n\nFor more on value stream mapping, see Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation by Karen Martin and Mike\n\nOsterling.\n\nFor more on ORMs, visit Stack Overflow:\n\nhttp://stackoverflow.com/questions/1279613/what-is-an-orm-and-where-can-i-learn- more-about-it .\n\nAn excellent primer on many agile development rituals and how to use them in IT\n\nOperations work can be found in a series of posts written on the Agile Admin blog:\n\nhttp://theagileadmin.com/2011/02/21/scrum-for-operations-what-is-scrum/ .\n\nFor more information on architecting for fast builds, see Daniel Worthington-Bodart’s\n\nblog post “Crazy Fast Build Times (or When 10 Seconds Starts to Make You Nervous)”: http://dan.bodar.com/2012/02/28/crazy-fast-build-times-or-when-10-seconds-starts-to-\n\nmake-you-nervous/ .\n\nFor more details on performance testing at Facebook, along with some detailed\n\ninformation on Facebook’s release process, check out Chuck Rossi’s presentation “The\n\nFacebook Release Process”: http://www.infoq.com/presentations/Facebook-Release-\n\nProcess .\n\nMany more variants of dark launching can be found in chapter 8 of The Practice of Cloud\n\nSystem Administration: Designing and Operating Large Distributed Systems, Volume 2 by Thomas A. Limoncelli, Strata R. Chalup, and Christina J. Hogan.\n\nThere is an excellent technical discussion of feature toggles here:\n\nhttp://martinfowler.com/articles/feature-toggles.html .\n\nReleases are discussed in more detail in The Practice of Cloud System Administration:\n\nDesigning and Operating Large Distributed Systems, Volume 2 by Thomas A. Limoncelli,",
      "content_length": 2098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "Strata R. Chalup, and Christina J. Hogan; Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation by Jez Humble and David\n\nFarley; and Release It! Design and Deploy Production-Ready Software by Michael T. Nygard.\n\nA description of the circuit breaker pattern can be found here: http://martinfowler.com/bliki/CircuitBreaker.html .\n\nFor more on the cost of delay see The Principles of Product Development Flow: Second Generation Lean Product Development by Donald G. Reinertsen.\n\nA further discussion on staying ahead of failures for the Amazon S3 service can be found here: https://qconsf.com/sf2010/dl/qcon-sanfran- 2009/slides/JasonMcHugh_AmazonS3ArchitectingForResiliencyInTheFaceOfFailures.pdf\n\n.\n\nFor an excellent guide on conducting user research, see Lean UX: Applying Lean\n\nPrinciples to Improve User Experience by Jeff Gothelf and Josh Seiden.\n\nWhich Test Won? is a site that displays hundreds of real-life A/B tests and asks the viewer to guess which variant performed better, reinforcing the key that unless we actually test,\n\nwe’re merely guessing. Visit it here: http://whichtestwon.com/ .\n\nA list of architectural patterns can be found in Release It! Design and Deploy Production- Ready Software by Michael T. Nygard.\n\nAn example of published Chef post-mortem meeting notes can be found here: https://www.chef.io/blog/2014/08/14/cookbook-dependency-api-postmortem/ . A video of the meeting can be found here: https://www.youtube.com/watch?v=Rmi1Tn5oWfI .\n\nA current schedule of upcoming DevOpsDays can be found on the DevOpsDays website: http://www.devopsdays.org/ . Instructions on organizing a new DevOpsDays can be\n\nfound on the DevOpsDay Organizing Guide page: http://www.devopsdays.org/pages/organizing/ .\n\nMore on using tools to manage secrets can be found in Noah Kantrowitz’s post “Secrets\n\nManagement and Chef” on his blog: https://coderanger.net/chef-secrets/ .\n\nJames Wickett and Gareth Rushgrove have put all their examples of secure pipelines on\n\nthe GitHub website: https://github.com/secure-pipeline .\n\nThe National Vulnerability Database website and XML data feeds can be found at:\n\nhttps://nvd.nist.gov/ .\n\nA concrete scenario involving integration between Puppet and ThoughtWorks’ Go and\n\nMingle (a project management application) can be found in a Puppet Labs blog post by",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "Andrew Cunningham and Andrew Myers and edited by Jez Humble: https://puppetlabs.com/blog/a-deployment-pipeline-for-infrastructure .\n\nPreparing and passing compliance audits is further explored in Jason Chan’s 2015 presentation “SEC310: Splitting the Check on Compliance and Security: Keeping Developers and Auditors Happy in the Cloud”: https://www.youtube.com/watch?\n\nv=Io00_K4v12Y&feature=youtu.be .\n\nThe story of how application configuration settings were transformed by Jez Humble and\n\nDavid Farley for Oracle WebLogic was described in the book Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation. Mirco Hering described a more generic approach to this process here:\n\nhttp://notafactoryanymore.com/2015/10/19/devops-for-systems-of-record-a-new-hope- preview-of-does-talk/ .\n\nA sample list of DevOps operational requirements can be found here: http://blog.devopsguys.com/2013/12/19/the-top-ten-devops-operational-requirements/ .",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "Endnotes\n\nINTRODUCTION\n\nBefore the revolution… Eliyahu M. Goldratt, Beyond the Goal: Eliyahu Goldratt Speaks on the Theory of Constraints (Your Coach in a Box) (Prince Frederick, Maryland: Gildan Media, 2005), Audiobook.\n\nPut even more… Jeff Immelt, “GE CEO Jeff Immelt: Let’s Finally End the Debate over Whether We Are in a Tech Bubble,” Business Insider, December 9, 2015,\n\nhttp://www.businessinsider.com/ceo-of-ge-lets-finally-end-the-debate-over-whether-we- are-in-a-tech-bubble-2015-12 .\n\nOr as Jeffrey… “Weekly Top 10: Your DevOps Flavor,” Electric Cloud, April 1, 2016, http://electric-cloud.com/blog/2016/04/weekly-top-10-devops-flavor/ .\n\nDr. Eliyahu M. Goldratt… Goldratt, Beyond the Goal.\n\nAs Christopher Little… Christopher Little, personal correspondence with Gene Kim, 2010.\n\nAs Steven J. Spear… Steven J. Spear, The High-Velocity Edge: How Market Leaders Leverage Operational Excellence to Beat the Competition (New York, NY: McGraw Hill\n\nEducation), Kindle edition, chap. 3.\n\nIn 2013, the… Chris Skinner, “Banks have bigger development shops than Microsoft,” Chris Skinner’s Blog, accessed July 28, 2016, http://thefinanser.com/2011/09/banks-have-bigger- development-shops-than-microsoft.html/ .\n\nProjects are typically… Nico Stehr and Reiner Grundmann, Knowledge: Critical Concepts,\n\nVolume 3 (London: Routledge, 2005), 139.\n\nDr. Vernon Richardson… A. Masli, V. Richardson, M. Widenmier, and R. Zmud, “Senior Executive’s IT Management Responsibilities: Serious IT Deficiencies and CEO-CFO Turnover,” MIS Quaterly (published electronically June 21, 2016).\n\nConsider the following… “IDC Forecasts Worldwide IT Spending to Grow 6% in 2012, Despite Economic Uncertainty,” Business Wire, September 10, 2012, http://www.businesswire.com/news/home/20120910005280/en/IDC-Forecasts- Worldwide-Spending-Grow-6-2012 .",
      "content_length": 1823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "The first surprise… Nigel Kersten, IT Revolution, and PwC, 2015 State of DevOps Report (Portland, OR: Puppet Labs, 2015), https://puppet.com/resources/white-paper/2015-state- of-devops-report?_ga=1.6612658.168869.1464412647&link=blog .\n\nThis is highlighted… Frederick P. Brooks, Jr., The Mythical Man-Month: Essays on Software Engineering, Anniversary Edition (Upper Saddle River, NJ: Addison-Wesley, 1995).\n\nAs Randy Shoup… Gene Kim, Gary Gruver, Randy Shoup, and Andrew Phillips, “Exploring the Uncharted Territory of Microservices,” XebiaLabs.com, webinar, February 20, 2015,\n\nhttps://xebialabs.com/community/webinars/exploring-the-uncharted-territory-of- microservices/ .\n\nThe 2015 State… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nAnother more extreme… “Velocity 2011: Jon Jenkins, ‘Velocity Culture’,” YouTube video, 15:13, posted by O’Reilly, June 20, 2011, https://www.youtube.com/watch?v=dxk8b9rSKOo ;\n\n“Transforming Software Development,” YouTube video, 40:57, posted by Amazon Web Service, April 10, 2015, https://www.youtube.com/watch?v=YCrhemssYuI&feature=youtu.be .\n\nLater in his… Eliyahu M. Goldratt, Beyond the Goal.\n\nAs with The… JGFLL, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 4, 2013, http://www.amazon.com/review/R1KSSPTEGLWJ23 ; Mark L Townsend, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 2, 2013, http://uedata.amazon.com/gp/customer-\n\nreviews/R1097DFODM12VD/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI ; Scott Van Den Elzen, review of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win, by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 13, 2013, http://uedata.amazon.com/gp/customer- reviews/R2K95XEH5OL3Q5/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI .\n\nPART I INTRODUCTION\n\nOne key principle… Kent Beck, et al., “Twelve Principles of Agile Software,” AgileManifesto.org, 2001, http://agilemanifesto.org/principles.html .\n\nHe concluded that… Mike Rother, Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results (New York: McGraw Hill, 2010), Kindle edition, Part III.\n\nCHAPTER 1",
      "content_length": 2337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "Karen Martin and… Karen Martin and Mike Osterling, Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation (New York:\n\nMcGraw Hill, 2013), Kindle edition, chap 1.\n\nIn this book… Ibid., chap. 3.\n\nKaren Martin and… Ibid.\n\nCHAPTER 2\n\nStudies have shown… Joshua S. Rubinstein, David E. Meyer, and Jeffrey E. Evans, “Executive Control of Cognitive Processes in Task Switching,” Journal of Experimental Psychology: Human Perception and Performance 27, no. 4 (2001): 763-797, doi: 10.1037//0096- 1523.27.4.763, http://www.umich.edu/~bcalab/documents/RubinsteinMeyerEvans2001.pdf .\n\nDominica DeGrandis, one… “DOES15—Dominica DeGrandis—The Shape of Uncertainty,” YouTube video, 22:54, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=Gp05i0d34gg .\n\nTaiichi Ohno compared… Sami Bahri, “Few Patients-In-Process and Less Safety Scheduling; Incoming Supplies are Secondary,” The Deming Institute Blog, August 22, 2013,\n\nhttps://blog.deming.org/2013/08/fewer-patients-in-process-and-less-safety-scheduling- incoming-supplies-are-secondary/ .\n\nIn other words… Meeting between David J. Andersen and team at Motorola with Daniel S. Vacanti, February 24, 2004; story retold at USC CSSE Research Review with Barry Boehm in March 2004.\n\nThe dramatic differences… James P. Womack and Daniel T. Jones, Lean Thinking: Banish Waste and Create Wealth in Your Corporation (New York: Free Press, 2010), Kindle edition, chap. 1.\n\nThere are many… Eric Ries, “Work in small batches,” StartupLessonsLearned.com, February\n\n20, 2009, http://www.startuplessonslearned.com/2009/02/work-in-small-batches.html .\n\nIn Beyond the… Goldratt, Beyond the Goal.\n\nAs a solution… Eliyahu M. Goldratt, The Goal: A Process of Ongoing Improvement (Great Barrington, MA: North River Press, 2014), Kindle edition, “Five Focusing Steps.”\n\nShigeo Shingo, one… Shigeo Shingo, A Study of the Toyota Production System: From an Industrial Engineering Viewpoint (London: Productivity Press, 1989); “The 7 Wastes (Seven forms of Muda),” BeyondLean.com, accessed July 28, 2016, http://www.beyondlean.com/7- wastes.html .",
      "content_length": 2140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "In the book… Mary Poppendieck and Tom Poppendieck, Implementing Lean Software: From Concept to Cash, (Upper Saddle River, NJ: Addison-Wesley, 2007), 74.\n\nThe following categories… Adapted from Damon Edwards, “DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,” Slideshare.net, posted by dev2ops, May 4, 2015, http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards .\n\nCHAPTER 3\n\nDr. Charles Perrow… Charles Perrow, Normal Accidents: Living with High Risk Technologies (Princeton, NJ: Princeton University Press, 1999).\n\nDr. Sidney Dekker… Dr. Sidney Dekker, The Field Guide to Understanding Human Error (Lund University, Sweden: Ashgate, 2006).\n\nAfter he decoded… Spear, The High-Velocity Edge, chap. 8.\n\nDr. Spear extended… Ibid.\n\nDr. Peter Senge… Peter M. Senge, The Fifth Discipline: The Art & Practice of the Learning Organization (New York: Doubleday, 2006), Kindle edition, chap. 5.\n\nIn one well-documented… “NUMMI,” This American Life, March 26, 2010, http://www.thisamericanlife.org/radio-archives/episode/403/transcript .\n\nAs Elisabeth Hendrickson… “DOES15 - Elisabeth Hendrickson - Its All About Feedback,” YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=r2BFTXBundQ .\n\n“In doing so… Spear, The High-Velocity Edge, chap. 1.\n\nAs Dr. Spear… Ibid., chap. 4.\n\nExamples of ineffective… Jez Humble, Joanne Molesky, and Barry O’Reilly, Lean Enterprise: How High Performance Organizations Innovate at Scale (Sebastopol, CA: O’Reilly Media, 2015), Kindle edition, Part IV.\n\nIn the 1700s… Dr. Thomas Sowell, Knowledge and Decisions (New York: Basic Books, 1980), 222.\n\nAs Gary Gruver… Gary Gruver, personal correspondence with Gene Kim, 2014.\n\nCHAPTER 4\n\nFor instance, in… Paul Adler, “Time-and-Motion Regained,” Harvard Business Review, January-February 1993, https://hbr.org/1993/01/time-and-motion-regained .",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "The “name, blame… Dekker, The Field Guide to Understanding Human Error, chap. 1.\n\nDr. Sidney Dekker… “Just Culture: Balancing Safety and Accountability,” Lund University, Human Factors & System Safety website, November 6, 2015,\n\nhttp://www.humanfactors.lth.se/sidney-dekker/books/just-culture/ .\n\nHe observed that… Ron Westrum, “The study of information flow: A personal journey,” Proceedings of Safety Science 67 (August 2014): 58-63, https://www.researchgate.net/publication/261186680_ The_study_of_information_flow_A_personal_journey .\n\nJust as Dr. Westrum… Nicole Forsgren Velasquez, Gene Kim, Nigel Kersten, and Jez Humble, 2014 State of DevOps Report (Portland, OR: Puppet Labs, IT Revolution Press, and ThoughtWorks, 2014), http://puppetlabs.com/2014-devops-report .\n\nAs Bethany Macri… Bethany Macri, “Morgue: Helping Better Understand Events by Building\n\na Post Mortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nDr. Spear observes… Spear, The High-Velocity Edge, chap. 1.\n\nIn The Fifth… Senge, The Fifth Discipline, chap. 1.\n\nMike Rother observed… Mike Rother, Toyota Kata, 12.\n\nThis is why… Mike Orzen, personal correspondence with Gene Kim, 2012.\n\nConsider the following… “Paul O’Neill,” Forbes, October 11, 2001, http://www.forbes.com/2001/10/16/poneill.html .\n\nIn 1987, Alcoa… Spear, The High-Velocity Edge, chap. 4.\n\nAs Dr. Spear… Ibid.\n\nA remarkable example… Ibid., chap. 5.\n\nThis process of… Nassim Nicholas Taleb, Antifragile: Things That Gain from Disorder (Incerto), (New York: Random House, 2012).\n\nAccording to Womack… Jim Womack, Gemba Walks (Cambridge, MA: Lean Enterprise Institute, 2011), Kindle edition, location 4113.\n\nMike Rother formalized… Rother, Toyota Kata, Part IV.\n\nMike Rother observes… Ibid., Conclusion.\n\nCHAPTER 5",
      "content_length": 1829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "Therefore, we must… Michael Rembetsy and Patrick McDonnell, “Continuously Deploying Culture [at Etsy],” Slideshare.net, October 4, 2012, posted by Patrick McDonnel.bl, http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at- etsy-14588485 .\n\nIn 2015, Nordstrom… “Nordstrom, Inc.,” company profile on Vault.com, http://www.vault.com/company-profiles/retail/nordstrom,-inc/company-overview.aspx .\n\nThe stage for… Courtney Kissler, “DOES14 - Courtney Kissler - Nordstrom - Transforming to\n\na Culture of Continuous Improvement,” YouTube video, 29:59, posted by DevOps Enterprise Summit 2014, October 29, 2014, https://www.youtube.com/watch?v=0ZAcsrZBSlo .\n\nThese organizations were… Tom Gardner, “Barnes & Noble, Blockbuster, Borders: The Killer B’s Are Dying,” The Motley Fool, July 21, 2010, http://www.fool.com/investing/general/2010/07/21/barnes-noble-blockbuster-borders-the- killer-bs-are.aspx .\n\nAs Kissler described… Kissler, “DOES14 - Courtney Kissler - Nordstrom.”\n\nAs Kissler said… Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n\nAs Kissler stated… Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n\nIn 2015, Kissler… Ibid.\n\nShe continued, “This… Ibid.\n\nKissler concluded, “From… Ibid.\n\nAn example of… Ernest Mueller, “Business model driven cloud adoption: what NI Is doing in the cloud,” Slideshare.net, June 28, 2011, posted by Ernest Mueller, http://www.slideshare.net/mxyzplk/business-model-driven-cloud-adoption-what-ni-is-\n\ndoing-in-the-cloud .\n\nAlthough many believe… Unpublished calculation by Gene Kim after the 2014 DevOps Enterprise Summit.\n\nIndeed, one of… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nCSG (2013): In… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments,” Slideshare.net, November 14, 2014, posted by DevOps Enterprise Summit, http://www.slideshare.net/DevOpsEnterpriseSummit/scott-prugh .\n\nEtsy (2009): In… Rembetsy and McDonnell, “Continuously Deploying Culture [at Etsy].”",
      "content_length": 2084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "The Gartner research… Bernard Golden, “What Gartner’s Bimodal IT Model Means to Enterprise CIOs,” CIO Magazine, January 27, 2015,\n\nhttp://www.cio.com/article/2875803/cio-role/what-gartner-s-bimodal-it-model-means-to- enterprise-cios.html .\n\nSystems of record… Ibid.\n\nSystems of engagement… Ibid.\n\nThe data from… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nScott Prugh, VP… Scott Prugh, personal correspondence with Gene Kim, 2014.\n\nGeoffrey A. Moore… Geoffrey A. Moore and Regis McKenna, Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers (New York: HarperCollins,\n\n2009), 11.\n\nBig bang, top-down… Linda Tucci, “Four Pillars of PayPal’s ‘Big Bang’ Agile Transformation,” TechTarget, August 2014, http://searchcio.techtarget.com/feature/Four- pillars-of-PayPals-big-bang-Agile-transformation .\n\nThe following list… “Creating High Velocity Organizations,” description of course by Roberto Fernandez and Steve Spear, MIT Sloan Executive Education website, accessed May 30, 2016, http://executive.mit.edu/openenrollment/program/organizational-development-high- velocity-organizations .\n\nBut as Ron van Kemenade… Ron Van Kemande, “Nothing Beats Engineering Talent: The\n\nAgile Transformation at ING,” presentation at the DevOps Enterprise Summit, London, UK, June 30-July 1, 2016.\n\nPeter Drucker, a… Leigh Buchanan, “The Wisdom of Peter Drucker from A to Z,” Inc., November 19, 2009, http://www.inc.com/articles/2009/11/drucker.html .\n\nCHAPTER 6\n\nOver the years… Kissler, “DOES14 - Courtney Kissler - Nordstrom.”\n\nKissler explained:… Ross Clanton and Michael Ducy, interview of Courtney Kissler and Jason Josephy, “Continuous Improvement at Nordstrom,” The Goat Farm, podcast audio, June 25, 2015, http://goatcan.do/2015/06/25/the-goat-farm-episode-7-continuous-improvement-at- nordstrom/ .\n\nShe said proudly… Ibid.\n\nTechnology executives or… Brian Maskell, “What Does This Guy Do? Role of Value Stream Manager,” Maskell, July 3, 2015, http://blog.maskell.com/? p=2106http://www.lean.org/common/display/?o=221 .",
      "content_length": 2058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "Damon Edwards observed… Damon Edwards, “DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,” Slideshare.net, posted by dev2ops, May 4, 2015, http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards .\n\nIn their book… Vijay Govindarajan and Chris Trimble, The Other Side of Innovation: Solving the Execution Challenge (Boston, MA: Harvard Business Review, 2010) Kindle edition.\n\nBased on their… Ibid., Part I.\n\nAfter the near-death… Marty Cagan, Inspired: How to Create Products Customers Love (Saratoga, CA: SVPG Press, 2008), 12.\n\nCagan notes that… Ibid.\n\nSix months after… Ashlee Vance, “LinkedIn: A Story About Silicon Valley’s Possibly Unhealthy Need for Speed,” Bloomberg, April 30, 2013, http://www.bloomberg.com/bw/articles/2013-04-29/linkedin-a-story-about-silicon-valleys- possibly-unhealthy-need-for-speed .\n\nLinkedIn was created… “LinkedIn started back in 2003 — LinkedIn - A Brief History,”\n\nSlideshare.net, posted by Josh Clemm, November 9, 2015, http://www.slideshare.net/joshclemm/how-linkedin-scaled-a-brief-history/3- LinkedIn_started_back_in_2003 .\n\nOne year later… Jonas Klit Nielsen, “8 Years with LinkedIn – Looking at the Growth [Infographic],” MindJumpers.com, May 10, 2011, http://www.mindjumpers.com/blog/2011/05/linkedin-growth-infographic/ .\n\nBy November 2015… “LinkedIn started back in 2003,” Slideshare.net.\n\nThe problem was… “From a Monolith to Microservices + REST: The Evolution of LinkedIn’s Architecture,” Slideshare.net, posted by Karan Parikh, November 6,\n\n2014,http://www.slideshare.net/parikhk/restli-and-deco .\n\nJosh Clemm, a… “LinkedIn started back in 2003,” Slideshare.net.\n\nIn 2013, journalist… Vance, “LinkedIn: A Story About,” Bloomberg.\n\nScott launched Operation… “How I Structured Engineering Teams at LinkedIn and AdMob for Success,” First Round Review, 2015,http://firstround.com/review/how-i-structured- engineering-teams-at-linkedin-and-admob-for-success/ .\n\nScott described one… Ashlee Vance, “Inside Operation InVersion, the Code Freeze that Saved LinkedIn,” Bloomberg, April 11, 2013, http://www.bloomberg.com/news/articles/2013-04- 10/inside-operation-inversion-the-code-freeze-that-saved-linkedin .\n\nHowever, Vance described… Vance, “LinkedIn: A Story About,” Bloomberg.",
      "content_length": 2243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "As Josh Clemm… “LinkedIn started back in 2003,” Slideshare.net.\n\nKevin Scott stated… “How I Structured Engineering Teams,” First Round Review.\n\nAs Christopher Little… Christopher Little, personal correspondence with Gene Kim, 2011.\n\nAs Ryan Martens… Ryan Martens, personal correspondence with Gene Kim, 2013.\n\nCHAPTER 7\n\nHe observed, “After… Dr. Melvin E. Conway, “How Do Committees Invent?” MelConway.com, http://www.melconway.com/research/committees.html , previously published in Datamation, April 1968.\n\nThese observations led… Ibid.\n\nEric S. Raymond, author… Eric S. Raymond, “Conway’s Law,” catb.org, accessed May 31, 2016, http://catb.org/~esr/jargon/ .\n\nEtsy’s DevOps journey… Sarah Buhr, “Etsy Closes Up 86 Percent on First Day of Trading,”\n\nTech Crunch, April 16, 2015, http://techcrunch.com/2015/04/16/etsy-stock-surges-86- percent-at-close-of-first-day-of-trading-to-30-per-share/ .\n\nAs Ross Snyder… “Scaling Etsy: What Went Wrong, What Went Right,” Slideshare.net, posted by Ross Snyder, October 5, 2011, http://www.slideshare.net/beamrider9/scaling-etsy- what-went-wrong-what-went-right .\n\nAs Snyder observed… Ibid.\n\nIn other words… Sean Gallagher, “When ‘Clever’ Goes Wrong: How Etsy Overcame Poor Architectural Choices,” Arstechnica, October 3, 2011, http://arstechnica.com/business/2011/10/when-clever-goes-wrong-how-etsy-overcame- poor-architectural-choices/ .\n\nSnyder explained that… “Scaling Etsy” Slideshare.net.\n\nEtsy initially had… Ibid.\n\nIn the spring… Ibid.\n\nAs Snyder described… Ross Snyder, “Surge 2011—Scaling Etsy: What Went Wrong, What Went Right,” YouTube video, posted by Surge Conference, December 23, 2011, https://www.youtube.com/watch?v=eenrfm50mXw .\n\nAs Snyder said… Ibid.\n\nSprouter was one… “Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012,",
      "content_length": 1872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at- etsy-14588485 .\n\nThey are defined… “Creating High Velocity Organizations,” description of course by Roberto Fernandez and Steven Spear.\n\nAdrian Cockcroft remarked… Adrian Cockcroft, personal correspondence with Gene Kim, 2014.\n\nIn the Lean… Spear, The High-Velocity Edge, chap. 8.\n\nAs Mike Rother… Rother, Toyota Kata, 250.\n\nReflecting on shared… “DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,” YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=USYrDaPEFtM .\n\nHe continued, “The… Ibid.\n\nPedro Canahuati, their… Pedro Canahuati, “Growing from the Few to the Many: Scaling the Operations Organization at Facebook,” InfoQ, December 16, 2013, http://www.infoq.com/presentations/scaling-operations-facebook .\n\nWhen departments over-specialize… Spear, The High-Velocity Edge, chap. 1.\n\nScott Prugh writes… Scott Prugh, “Continuous Delivery,” Scaled Agile Framework, updated February 14, 2013, http://www.scaledagileframework.com/continuous-delivery/ .\n\n“By cross-training… Ibid.\n\n“Traditional managers will… Ibid.\n\nFurthermore, as Prugh… Ibid.\n\nWhen we value… Dr. Carol Dweck, “Carol Dweck Revisits the ‘Growth Mindset,’” Education Week, September 22, 2015, http://www.edweek.org/ew/articles/2015/09/23/carol-dweck- revisits-the-growth-mindset.html .\n\nAs Jason Cox… Jason Cox, “Disney DevOps: To Infinity and Beyond,” presentation at DevOps Enterprise Summit 2014, San Francisco, CA, October 2014.\n\nAs John Lauderbach… John Lauderbach, personal conversation with Gene Kim, 2001.\n\nThese properties are… Tony Mauro, “Adopting Microservices at Netflix: Lessons for Architectural Design,” NGINX, February 19, 2015, https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/ .; Adam Wiggins, “The Twelve-Factor App,” 12Factor.net, January 30, 2012, http://12factor.net/ .",
      "content_length": 1956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "Randy Shoup, former… “Exploring the Uncharted Territory of Microservices,” YouTube video, 56:50, posted by XebiaLabs, Inc., February 20, 2015, https://www.youtube.com/watch?v=MRa21icSIQk .\n\nAs part of… Humble, O’Reilly, and Molesky, Lean Enterprise, Part III.\n\nIn the Netflix… Reed Hastings, “Netflix Culture: Freedom and Responsibility,” Slideshare.net, August 1, 2009, http://www.slideshare.net/reed2001/culture-1798664 .\n\nAmazon CTO Werner… Larry Dignan, “Little Things Add Up,” Baseline, October 19, 2005, http://www.baselinemag.com/c/a/Projects-Management/Profiles-Lessons-From-the- Leaders-in-the-iBaselinei500/3 .\n\nTarget is the… Heather Mickman and Ross Clanton, “DOES15 - Heather Mickman & Ross Clanton - (Re)building an Engineering Culture: DevOps at Target,” YouTube video, 33:39, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch? v=7s-VbB1fG5o .\n\nAs Mickman described… Ibid.\n\nIn an attempt… Ibid.\n\nBecause our team… Ibid.\n\nIn the following… Ibid.\n\nThese changes have… Ibid.\n\nThe API Enablement… Ibid.\n\nCHAPTER 8\n\nAt Big Fish… “Big Fish Celebrates 11th Consecutive Year of Record Growth,” BigFishGames.com, January 28, 2014, http://pressroom.bigfishgames.com/2014-01-28-Big- Fish-Celebrates-11th-Consecutive-Year-of-Record-Growth .\n\nHe observed that… Paul Farrall, personal correspondence with Gene Kim, January 2015.\n\nFarrall defined two… Ibid., 2014.\n\nHe concludes, “The… Ibid.\n\nErnest Mueller observed… Ernest Mueller, personal correspondence with Gene Kim, 2014.\n\nAs Damon Edwards… Edwards, “DevOps Kaizen.”\n\nDianne Marsh, Director… “Dianne Marsh ‘Introducing Change while Preserving Engineering Velocity,” YouTube video, 17:37, posted by Flowcon, November 11, 2014,",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "https://www.youtube.com/watch?v=eW3ZxY67fnc .\n\nJason Cox said… Jason Cox, “Disney DevOps.”\n\nAt Etsy, this… “devopsdays Minneapolis 2015 - Katherine Daniels - DevOps: The Missing Pieces,” YouTube video, 33:26, posted by DevOps Minneapolis, July 13, 2015, https://www.youtube.com/watch?v=LNJkVw93yTU .\n\nAs Ernest Mueller… Ernest Mueller, personal correspondence with Gene Kim, 2015.\n\nScrum is an agile… Hirotaka Takeuchi and Ikujiro Nonaka, “New Product Development Game,” Harvard Business Review (January 1986): 137-146.\n\nCHAPTER 9\n\nIn her presentation… Em Campbell-Pretty, “DOES14 - Em Campbell-Pretty - How a Business Exec Led Agile, Lead, CI/CD,” YouTube video, 29:47, posted by DevOps Enterprise Summit,\n\nApril 20, 2014, https://www.youtube.com/watch?v=-4pIMMTbtwE .\n\nCampbell-Pretty became… Ibid.\n\nThey created a… Ibid.\n\nCampbell-Pretty observed… Ibid.\n\nCampbell-Pretty described… Ibid.\n\nThe first version… “Version Control History,” PlasticSCM.com, accessed May 31, 2016, https://www.plasticscm.com/version-control-history.html .\n\nA version control… Jennifer Davis and Katherine Daniels, Effective DevOps: Building a Culture of Collaboration, Affinity, and Tooling at Scale (Sebastopol, CA: O’Reilly Media, 2016), 37.\n\nBill Baker, a… Simon Sharwood, “Are Your Servers PETS or CATTLE?,” The Register, March 18 2013, http://www.theregister.co.uk/2013/03/18/servers_pets_or_cattle_cern/ .\n\nAt Netflix, the… Jason Chan, “OWASP AppSecUSA 2012: Real World Cloud Application Security,” YouTube video, 37:45, posted by Christiaan008, December 10, 2012, https://www.youtube.com/watch?v=daNA0jXDvYk .\n\nThe latter pattern… Chad Fowler, “Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components,” ChadFowler.com, June 23, 2013,\n\nhttp://chadfowler.com/2013/06/23/immutable-deployments.html .\n\nThe entire application… John Willis, “Docker and the Three Ways of DevOps Part 1: The First Way—Systems Thinking,” Docker, May 26, 2015, https://blog.docker.com/2015/05/docker-",
      "content_length": 1994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "three-ways-devops/ .\n\nCHAPTER 10\n\nGary Gruver, former… Gary Gruver, personal correspondence with Gene Kim, 2014.\n\nThey had problems… “DOES15 - Mike Bland - Pain Is Over, If You Want It,” Slideshare.net, posted by Gene Kim, November 18, 2015, http://www.slideshare.net/ITRevolution/does15- mike-bland-pain-is-over-if-you-want-it-55236521 .\n\nBland describes how… Ibid.\n\nBland described that… Ibid.\n\nAs Bland describes… Ibid.\n\nAs Bland notes… Ibid.\n\nOver the next… Ibid.\n\nEran Messeri, an… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?,” presentation at the GOTO Conference, Aarhus, Denmark, October 2, 2013.\n\nMesseri explains, “There… Ibid.\n\nAll their code… Ibid.\n\nSome of the… Ibid.\n\nIn Development, continuous… Jez Humble and David Farley, personal correspondence with Gene Kim, 2012.\n\nThe deployment pipeline… Jez Humble and David Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation (Upper Saddle River, NJ: Addison-Wesly, 2011), 3.\n\nHumble and Farley… Ibid., 188.\n\nAs Humble and… Ibid., 258.\n\nMartin Fowler observes… Martin Fowler, “Continuous Integration,” MartinFowler.com, May 1, 2006, http://www.martinfowler.com/articles/continuousIntegration.html .\n\nMartin Fowler described… Martin Fowler, “TestPyramid,” MartinFowler.com, May 1, 2012, http://martinfowler.com/bliki/TestPyramid.html .",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "This technique was .. Martin Fowler, “Test Driven Development,” MartinFowler.com, March 5, 2005, http://martinfowler.com/bliki/TestDrivenDevelopment.html .\n\nNachi Nagappan, E. Michael… Nachiappan Nagappan, E. Michael Maximilien, Thirumalesh Bhat, and Laurie Williams, “Realizing quality improvement through test driven development: results and experiences of four industrial teams,” Empir Software Engineering, 13, (2008): 289-302, http://research.microsoft.com/en-us/groups/ese/nagappan_tdd.pdf .\n\nIn her 2013… Elisabeth Hendrickson, “On the Care and Feeding of Feedback Cycles,” Slideshare.net, posted by Elisabeth Hendrickson, November 1, 2013, http://www.slideshare.net/ehendrickson/care-and-feeding-of-feedback-cycles .\n\nHowever, merely automating… “Decreasing false positives in automated testing,” Slideshare.net, posted by Sauce Labs, March 24, 2015, http://www.slideshare.net/saucelabs/decreasing-false-positives-in-automated-testing .; Martin Fowler, “Eradicating Non-determinism in Tests,” MartinFowler.com, April 14, 2011, http://martinfowler.com/articles/nonDeterminism.html .\n\nAs Gary Gruver… Gary Gruver, “DOES14 - Gary Gruver - Macy’s - Transforming Traditional Enterprise Software Development Processes,” YouTube video, 27:24, posted by DevOps Enterprise Summit 2014, October 29, 2014, https://www.youtube.com/watch?v=- HSSGiYXA7U .\n\nRandy Shoup, former… Randy Shoup, “The Virtuous Cycle of Velocity: What I Learned About Going Fast at eBay and Google by Randy Shoup,” YouTube video, 30:05, posted by Flowcon, December 26, 2013, https://www.youtube.com/watch?v=EwLBoRyXTOI .\n\nThis is sometimes… David West, “Water scrum-fall is-reality_of_agile_for_most,” Slideshare.net, posted by harsoft, April 22, 2013, http://www.slideshare.net/harsoft/water- scrumfall-isrealityofagileformost .\n\nCHAPTER 11\n\nThe surprising breadth… Gene Kim, “The Amazing DevOps Transformation of the HP LaserJet Firmware Team (Gary Gruver),” ITRevolution.com, 2013, http://itrevolution.com/the-amazing-devops-transformation-of-the-hp-laserjet-firmware- team-gary-gruver/ .\n\nGruver described this… Ibid.\n\nCompile flags (#define… Ibid.\n\nGruver admits trunk-based… Gary Gruver and Tommy Mouser, Leading the Transformation: Applying Agile and DevOps Principles at Scale (Portland, OR: IT Revolution Press), 60.",
      "content_length": 2296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "Gruver observed, “Without… Kim, “The Amazing DevOps Transformation ”\n\nITRevolution.com.\n\nJeff Atwood, founder… Jeff Atwood, “Software Branching and Parallel Universes,” CodingHorror.com, October 2, 2007, http://blog.codinghorror.com/software-branching- and-parallel-universes/ .\n\nThis is how… Ward Cunningham, “Ward Explains Debt Metaphor,” c2.com, 2011, http://c2.com/cgi/wiki?WardExplainsDebtMetaphor .\n\nErnest Mueller, who… Ernest Mueller, “2012: A Release Odyssey,” Slideshare.net, posted by Ernest Mueller, March 12, 2014, http://www.slideshare.net/mxyzplk/2012-a-release-odyssey .\n\nAt that time… “Bazaarvoice, Inc. Announces Its Financial Results for the Fourth Fiscal Quarter and Fiscal Year Ended April 30, 2012,” BasaarVoice.com, June 6, 2012, http://investors.bazaarvoice.com/releasedetail.cfm?ReleaseID=680964 .\n\nMueller observed, “It… Ernest Mueller, “DOES15 - Ernest Mueller - DevOps Transformations At National Instruments and…,” YouTube video, 34:14, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=6Ry40h1UAyE .\n\n“By running these… Ibid.\n\nMueller further described… Ibid.\n\nHowever, the data… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nCHAPTER 12\n\nIn 2012, Rossi… Chuck Rossi, “Release engineering and push karma: Chuck Rossi,” post on Chuck Rossi’s Facebook page, April 5, 2012, https://www.facebook.com/notes/facebook- engineering/release-engineering-and-push-karma-chuck-rossi/10150660826788920 .\n\nJust prior to… Ryan Paul, “Exclusive: a behind-the-scenes look at Facebook release engineering,” Ars Technica, April 5, 2012, http://arstechnica.com/business/2012/04/exclusive-a-behind-the-scenes-look-at-facebook- release-engineering/1/ .\n\nRossi continued, “If… Chuck Rossi, “Release engineering and push karma.”\n\nThe Facebook front-end… Paul, “Exclusive: a behind-the-scenes look at Facebook release engineering,” Ars Technica.\n\nHe explained that… Chuck Rossi, “Ship early and ship twice as often,” post on Chuck Rossi’s Facebook page, August 3, 2012, https://www.facebook.com/notes/facebook-\n\nengineering/ship-early-and-ship-twice-as-often/10150985860363920 .",
      "content_length": 2135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "Kent Beck, the .. Kent Beck, “Slow Deployment Causes Meetings,” post on Kent Beck’s Facebook page, November 19, 2015), https://www.facebook.com/notes/kent-beck/slow- deployment-causes-meetings/1055427371156793?_rdr=p .\n\nScott Prugh, their… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.”\n\nPrugh observed, “It… Ibid.\n\nPrugh writes, “We… Ibid.\n\nPrugh also observes:… Ibid.\n\nIn their experiments… Puppet Labs and IT Revolution Press, 2013 State of DevOps Report (Portland, OR: Puppet Labs, 2013), http://www.exin-library.com/Player/eKnowledge/2013- state-of-devops-report.pdf .\n\nPrugh reported that… Scott Prugh and Erica Morrison, “DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),” YouTube video, 29:39, posted by\n\nDevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch? v=tKdIHCL0DUg .\n\nConsider the following… Tim Tischler, personal conversation with Gene Kim, FlowCon 2013.\n\nIn practice, the… Puppet Labs and IT Revolution Press, 2013 State of DevOps Report.\n\nIn Puppet Labs’… Velasquez, Kim, Kersten, and Humble, 2014 State of DevOps Report.\n\nThe deployment process… Chad Dickerson, “Optimizing for developer happiness,” CodeAsCraft.com, June 6, 2011, https://codeascraft.com/2011/06/06/optimizing-for- developer-happiness/ .\n\nAs Noah Sussman… Noah Sussman and Laura Beth Denker, “Divide and Conquer,” CodeAsCraft.com, April 20, 2011, https://codeascraft.com/2011/04/20/divide-and-concur/ .\n\nSussman writes, “Through… Ibid.\n\nIf all the tests… Ibid.\n\nOnce it is an… Erik Kastner, “Quantum of Deployment,” CodeAsCraft.com, May 20, 2010, https://codeascraft.com/2010/05/20/quantum-of-deployment/ .\n\nThis technique was… Timothy Fitz, “Continuous Deployment at IMVU: Doing the impossible fifty times a day,” TimothyFitz.com, February 10, 2009, http://timothyfitz.com/2009/02/10/continuous-deployment-at-imvu-doing-the-impossible-\n\nfifty-times-a-day/ .",
      "content_length": 1929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "This pattern is… Fitz, “Continuous Deployment,” TimothyFitz.com.; Michael Hrenko, “DOES15 - Michael Hrenko - DevOps Insured By Blue Shield of California,” YouTube video, 42:24, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=NlgrOT24UDw .\n\nDan North and Dave… Humble and Farley, Continuous Delivery, 265.\n\nThe cluster immune… Eric Ries, The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses (New York: Random House, 2011), Audiobook.\n\nOne sophisticated example… Andrew ‘Boz’ Bosworth, “Building and testing at Facebook,”\n\npost on Boz Facebook page, August 8, 2012, https://www.facebook.com/notes/facebook- engineering/building-and-testing-at-facebook/10151004157328920 ; “Etsy’s Feature flagging API used for operational rampups and A/B testing,” GitHub.com, https://github.com/etsy/feature ; “Library for configuration management API,” GitHub.com, https://github.com/Netflix/archaius .\n\nIn 2009, when… John Allspaw, “Convincing management that cooperation and collaboration was worth it,” KitchenSoap.com, January 5, 2012, http://www.kitchensoap.com/2012/01/05/convincing-management-that-cooperation-and- collaboration-was-worth-it/ .\n\nSimilarly, as Chuck… Rossi, “Release engineering and push karma.”\n\nFor nearly a decade… Emil Protalinski, “Facebook passes 1.55B monthly active users and 1.01B daily active users,” Venture Beat, November 4, 2015, http://venturebeat.com/2015/11/04/facebook-passes-1-55b-monthly-active-users-and-1-01- billion-daily-active-users/ .\n\nBy 2015, Facebook… Ibid.\n\nEugene Letuchy, an… Eugene Letuchy, “Facebook Chat,” post on Eugene Letuchy’s Facebook page, May 3, 2008, http://www.facebook.com/note.php? note_id=14218138919&id=944554719 .\n\nImplementing this computationally-intensive… Ibid.\n\nAs Letuchy wrote… Ibid.\n\nHowever, in 2015… Jez Humble, personal correspondence with Gene Kim, 2014.\n\nHis updated definitions… Ibid.\n\nAt Amazon and… Ibid.",
      "content_length": 1971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "CHAPTER 13\n\nThis is the… Jez Humble, “What is Continuous Delivery,” ContinuousDelivery.com, accessed May 28, 2016, https://continuousdelivery.com/ .\n\nHe observes that… Kim, Gruver, Shoup, and Phillips, “Exploring the Uncharted Territory of Microservices.”\n\nHe reflects, “Looking… Ibid.\n\neBay’s architecture went… Shoup, “From Monolith to Micro-services.”\n\nCharles Betz, author… Charles Betz, Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler’s Children (Witham, MA: Morgan Kaufmann, 2011), 300.\n\nAs Randy Shoup… Randy Shoup, “From the Monolith to Micro-services,” Slideshare.net, posted by Randy Shoup, October 8, 2014, http://www.slideshare.net/RandyShoup/goto- aarhus2014-enterprisearchitecturemicroservices .\n\nShoup notes, “Organizations… Ibid.\n\nAs Randy Shoup observes… Ibid.\n\nOne of the most… Werner Vogels, “A Conversation with Werner Vogels,” acmqueque 4, no. 4\n\n(2006): 14-22, http://queue.acm.org/detail.cfm?id=1142065 .\n\nVogels tells Gray… Ibid.\n\nDescribing the thought… Ibid.\n\nVogels notes, “The… Ibid.\n\nIn 2011, Amazon… John Jenkins, “Velocity 2011: Jon Jenkins, “Velocity Culture,”” YouTube video, 15:13, posted by O’Reilly, June 20, 2011, {https://www.youtube.com/watch? v=dxk8b9rSKOo .\n\nBy 2015, they… Ken Exner, “Transforming Software Development,” YouTube video, 40:57, posted by Amazon Web Services, April 10, 2015, https://www.youtube.com/watch? v=YCrhemssYuI&feature=youtu.be .\n\nThe term strangler… Martin Fowler, “StranglerApplication,” MartinFowler.com, June 29, 2004, http://www.martinfowler.com/bliki/StranglerApplication.html .\n\nWhen we implement… Boris Lublinsky, “Versioning in SOA,” The Architecture Journal, April 2007, https://msdn.microsoft.com/en-us/library/bb491124.aspx .",
      "content_length": 1772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "The strangler application… Paul Hammant, “Introducing Branch by Abstraction,” PaulHammant.com, April 26, 2007, http://paulhammant.com/blog/branch_by_abstraction.html .\n\nAn observation from… Martin Fowler, “StranglerApplication,” MartinFowler.com, June 29, 2004, http://www.martinfowler.com/bliki/StranglerApplication.html .\n\nBlackboard Inc., is… Gregory T. Huang, “Blackboard CEO Jay Bhatt on the Global Future of Edtech,” Xconomy, June 2, 2014, http://www.xconomy.com/boston/2014/06/02/blackboard-ceo-jay-bhatt-on-the-global- future-of-edtech/ .\n\nAs David Ashman… David Ashman, “DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,” YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, https://www.youtube.com/watch?v=SSmixnMpsI4 .\n\nIn 2010, Ashman… Ibid.\n\nHow this started… David Ashman, personal correspondence with Gene Kim, 2014.\n\nAshman noted, “To… Ibid.\n\n“In fact,” Ashman… Ibid.\n\nAshman concluded, “Having… Ibid.\n\nCHAPTER 14\n\nIn Operations, we… Kim, Behr, and Spafford, The Visible Ops Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Eugene, OR: IT Process Institute, 2004), Kindle edition, Introduction.\n\nIn contrast, the… Ibid.\n\nIn other words… Ibid.\n\nTo enable this… “Telemetry,” Wikipedia, last modified May 5, 2016, https://en.wikipedia.org/wiki/Telemetry .\n\nMcDonnell described how… Michael Rembetsy and Patrick McDonnell, “Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012, http://www.slideshare.net/mcdonnps/continuously- deploying-culture-scaling-culture-at-etsy-14588485 .\n\nMcDonnell explained further… Ibid.\n\nBy 2011, Etsy… John Allspaw, personal conversation with Gene Kim, 2014.",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "As Ian Malpass… Ian Malpass, “Measure Anything, Measure Everything,” CodeAsCraft.com, February 15, 2011, http://codeascraft.com/2011/02/15/measure-anything-measure- everything/ .\n\nOne of the findings… Kersten, IT Revolution, and PwC, 2015 State of DevOps Report.\n\nThe top two… “2014 State Of DevOps Findings! Velocity Conference,” Slideshare.net, posted by Gene Kim, June 30, 2014, http://www.slideshare.net/realgenekim/2014-state-of-devops- findings-velocity-conference .\n\nIn The Art… James Turnbull, The Art of Monitoring (Seattle, WA: Amazon Digital Services, 2016), Kindle edition, Introduction.\n\nThe resulting capability… “Monitorama - Please, no more Minutes, Milliseconds, Monoliths or Monitoring Tools,” Slideshare.net, posted by Adrian Cockcroft, May 5, 2014,\n\nhttp://www.slideshare.net/adriancockcroft/monitorama-please-no-more .\n\nScott Prugh, Chief… Prugh, “DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.”\n\nTo support these… Brice Figureau, “The 10 Commandments of Logging,” Mastersen’s Blog, January 13, 2013, http://www.masterzen.fr/2013/01/13/the-10-commandments-of-logging/ .\n\nChoosing the right… Dan North, personal correspondence with Gene Kim, 2016.\n\nTo help ensure… Anton Chuvakin, “LogLogic/Chuvakin Log Checklist,” republished with permission, 2008, http://juliusdavies.ca/logging/llclc.html .\n\nIn 2004, Kim… Kim, Behr, and Spafford, The Visible Ops Handbook, Introduction.\n\nThis was the… Dan North, “Ops and Operability,” SpeakerDeck.com, February 25, 2016, https://speakerdeck.com/tastapod/ops-and-operability .\n\nAs John Allspaw… John Allspaw, personal correspondence with Gene Kim, 2011.\n\nThis is often… “Information Radiators,” AgileAlliance.com, accessed May 31, 2016, https://www.agilealliance.org/glossary/incremental-radiators/ .\n\nAlthough there may… Ernest Mueller, personal correspondence with Gene Kim, 2014.\n\nPrachi Gupta, Director… Prachi Gupta, “Visualizing LinkedIn’s Site Performance,” LinkedIn\n\nEngineering blog, June 13, 2011, https://engineering.linkedin.com/25/visualizing-linkedins- site-performance .\n\nThus began Eric… Eric Wong, “Eric the Intern: the Origin of InGraphs,” LinkedIn, June 30, 2011, http://engineering.linkedin.com/32/eric-intern-origin-ingraphs .",
      "content_length": 2225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "Wong wrote, “To… Ibid.\n\nAt the time… Ibid.\n\nIn writing about… Gupta, “Visualizing LinkedIn’s Site Performance.”\n\nEd Blankenship, Senior… Ed Blankenship, personal correspondence with Gene Kim, 2016.\n\nHowever, increasingly these… Mike Burrows, “The Chubby lock service for loosely-coupled\n\ndistributed systems,” OSDI’06: Seventh Symposium on Operating System Design and Implementation, November 2006, http://static.googleusercontent.com/media/research.google.com/en//archive/chubby- osdi06.pdf .\n\nConsul may be… Jeff Lindsay, “Consul Service Discovery with Docker,” Progrium.com, August 20, 2014, http://progrium.com/blog/2014/08/20/consul-service-discovery-with- docker .\n\nAs Jody Mulkey… Jody Mulkey, “DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,” YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, https://www.youtube.com/watch?v=USYrDaPEFtM .\n\nCHAPTER 15\n\nIn 2015, Netflix… Netflix Letter to Shareholders, January 19, 2016,\n\nhttp://files.shareholder.com/downloads/NFLX/2432188684x0x870685/C6213FF9-5498- 4084-A0FF-74363CEE35A1/Q4_15_Letter_to_Shareholders_-_COMBINED.pdf .\n\nRoy Rapoport describes… Roy Rapoport, personal correspondence with Gene Kim, 2014.\n\nOne of the statistical… Victoria Hodge and Jim Austin, “A Survey of Outlier Detection\n\nMethodologies,” Artificial Intelligence Review 22, no. 2 (October 2004): 85-126, http://www.geo.upm.es/postgrado/CarlosLopez/\n\npapers/Hodge+Austin_OutlierDetection_AIRE381.pdf .\n\nRapoport explains that… Roy Rapoport, personal correspondence with Gene Kim, 2014.\n\nRapoport continues, “We… Ibid.\n\nRapoport states that… Ibid.\n\nAs John Vincent… Toufic Boubez, “Simple math for anomaly detection toufic boubez -\n\nmetafor software - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-\n\nboubez-metafor-software-monitorama-pdx-20140505 .",
      "content_length": 1931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "Tom Limoncelli, co-author… Tom Limoncelli, “Stop monitoring whether or not your service is up!,” EverythingSysAdmin.com, November 27, 2013,\n\nhttp://everythingsysadmin.com/2013/11/stop-monitoring-if-service-is-up.html .\n\nAs Dr. Toufic… Toufic Boubez, “Simple math for anomaly detection toufic boubez - metafor\n\nsoftware - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-boubez-\n\nmetafor-software-monitorama-pdx-20140505 .\n\nDr. Nicole Forsgren… Dr. Nicole Forsgren, personal correspondence with Gene Kim, 2015.\n\nScryer works by… Daniel Jacobson, Danny Yuan, and Neeraj Joshi, “Scryer: Netflix’s Predictive Auto Scaling Engine,” The Netflix Tech Blog, November 5, 2013,\n\nhttp://techblog.netflix.com/2013/11/scryer-netflixs-predictive-auto-scaling.html .\n\nThese techniques are… Varun Chandola, Arindam Banerjee, and Vipin Kumar, “Anomaly detection: A survey,” ACM Computing Surveys 41, no. 3 (July 2009): article no. 15,\n\nhttp://doi.acm.org/10.1145/1541880.1541882 .\n\nTarun Reddy, VP… Tarun Reddy, personal interview with Gene Kim, Rally headquarters,\n\nBoulder, CO, 2014.\n\nAt Monitorama in 2014… “Kolmogorov-Smirnov Test,” Wikipedia, last modified May 19,\n\n2016, http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test .\n\nEven saying Kilmogorov-Smirnov… ”Simple math for anomaly detection toufic boubez -\n\nmetafor software - monitorama pdx 2014-05-05,” Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection -toufic-\n\nboubez-metafor-software-monitorama-pdx-20140505.\n\nCHAPTER 16\n\nIn 2006, Nick… Mark Walsh, “Ad Firms Right Media, AdInterax Sell To Yahoo,” MediaPost,\n\nOctober 18, 2006, http://www.mediapost.com/publications/article/49779/ad-firms-right-\n\nmedia-adinterax-sell-to-yahoo.html?edition= .\n\nGalbreath described the… Nick Galbreath, personal conversation with Gene, 2013.\n\nHowever, Galbreath observed… Nick Galbreath, “Continuous Deployment - The New #1 Security Feature, from BSildesLA 2012,” Slideshare.net, posted by Nick Galbreath, Aug 16,\n\n2012, http://www.slideshare.net/nickgsuperstar/continuous-deployment-the-new-1- security-feature .\n\nAfter observing many… Ibid.\n\nGalbreath observes that… Ibid.",
      "content_length": 2279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "As Patrick Lightbody… “Volocity 2011: Patrick Lightbody, ‘From Inception to Acquisition,’”\n\nYouTube video, 15:28, posted by O’Reilly, June 17, 2011, https://www.youtube.com/watch? v=ShmPod8JecQ.\n\nAs Arup Chakrabarti… Arup Chakrabarti, “Common Ops Mistakes,” presentation at Heavy Bit Industries, June 3, 2014, http://www .heavybit.com/library/video/common-ops-\n\nmistakes/\n\nMore recently, Jeff… ”From Design Thinking to DevOps and Back Again: Unifying Design & Operations,” Vimeo video, 21:19, posted by William Evans, June 5, 2015,\n\nhttps://vimeo.com/129939230.\n\nAs an anonymous… Anonymous, personal conversation with Gene Kim, 2005.\n\nLaunch guidance and… Tom Limoncelli, “SRE@Google: Thousands Of DevOps Since 2004,” YouTube video of USENIX Association Talk, NYC, posted by USENIX, 45:57, posted January\n\n12, 2012, http://www.youtube.com/watch?v=iIuTnhdTzK .\n\nAs Treynor Sloss has… Ben Treynor, “Keys to SRE” (presentation, Usenix SREcon14, Santa\n\nClara, CA, May 30, 2014), https://www.usenix.org/conference/srecon14/technical- sessions/presentation/keys-sre .\n\nTreynor Sloss has resisted… Ibid.\n\nEven when new… Limoncelli, “SRE@Google.”\n\nTom Limoncelli noted… Ibid.\n\nLimoncelli noted, “In… Ibid.\n\nFurthermore, Limoncelli observed… Tom Limoncelli, personal correspondence with Gene Kim, 2016.\n\nLimoncelli explained, “Helping… Ibid., 2015.\n\nCHAPTER 17\n\nIn general, Jez… Humble, O’Reilly and Molesky, Lean Enterprise, Part II.\n\nIn 2012, they… Intuit, Inc., “2012 Annual Report: Form 10-K,” July 31, 2012,\n\nhttp://s1.q4cdn.com/018592547/files/doc_financials/ 2012/INTU_2012_7_31_10K_r230_at_09_13_12_FINAL_and_Camera_Ready.pdf .\n\nCook explained that… Scott Cook, “Leadership in an Agile Age: An Interview with Scott Cook,” Intuit.com, April 20, 2011, https://web.archive.org/web/20160205050418/\n\nhttp://network.intuit.com/2011/04/20/leadership-in-the-agile-age/",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "He continued, “By… Ibid.\n\nIn previous eras… “Direct Marketing,” Wikipedia, last modified May 28, 2016, https://en.wikipedia.org/wiki/Direct_marketing .\n\nInterestingly, it has… Freakonomics, “Fighting Poverty With Actual Evidence: Full Transcript,” Freakonomics.com, November 27, 2013,\n\nhttp://freakonomics.com/2013/11/27/fighting-poverty-with-actual-evidence-full-transcript/ .\n\nRonny Kohavi, Distinguished… Ron Kohavi, Thomas Crook, and Roger Longbotham, “Online Experimentation at Microsoft,” (paper presented at the Fifteenth ACM SIGKDD International\n\nConference on Knowledge Discovery and Data Mining, Paris, France, 2009), http://www.exp-platform.com/documents/exp_dmcasestudies.pdf .\n\nKohavi goes on… Ibid.\n\nJez Humble joked… Jez Humble, personal correspondence with Gene Kim, 2015.\n\nIn a 2014… Wang, Kendrick, “Etsy’s Culture Of Continuous Experimentation and A/B Testing Spurs Mobile Innovation,” Apptimize.com, January 30, 2014,\n\nhttp://apptimize.com/blog/2014/01/etsy-continuous-innovation-ab-testing/ .\n\nBarry O’Reilly, co-author… Barry O’Reilly, “How to Implement Hypothesis-Driven\n\nDevelopment,” BarryOReilly.com, October 21, 2013,\n\nhttp://barryoreilly.com/2013/10/21/how-to-implement-hypothesis-driven-development/ .\n\nIn 2009, Jim… Gene Kim, “Organizational Learning and Competitiveness: Revisiting the\n\n“Allspaw/Hammond 10 Deploys Per Day at Flickr” Story,” ITRevolution.com, 2015, http://itrevolution.com/organizational-learning-and-competitiveness-a-different-view-of-\n\nthe-allspawhammond-10-deploys-per-day-at-flickr-story/ .\n\nStoneham observes that… Ibid.\n\nHe continues, “These… Ibid.\n\nTheir astounding achievements… Ibid.\n\nStoneham concluded, “This… Ibid.\n\nCHAPTER 18\n\nOnce a pull… Scott Chacon, “Github Flow,” ScottChacon.com, August 31, 2011,\n\nhttp://scottchacon.com/2011/08/31/github-flow.html .\n\nFor example, in… Jake Douglas, “Deploying at Github,” GitHub.com, August 29, 2012, https://github.com/blog/1241-deploying-at-github .",
      "content_length": 1953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "A fifteen minute… John Allspaw, “Counterfactual Thinking, Rules, and the Knight Capital Accident,” KitchenSoap.com, October 29, 2013,\n\nhttp://www.kitchensoap.com/2013/10/29/counterfactuals-knight-capital/ .\n\nOne of the core… Bradley Staats and David M. Upton, “Lean Knowledge Work,” Harvard\n\nBusiness Review, October 2011, https://hbr.org/2011/10/lean-knowledge-work .\n\nIn the 2014… Velasquez, Kim, Kersten, and Humble, 2014 State of DevOps Report.\n\nAs Randy Shoup… Randy Shoup, personal interview with Gene Kim, 2015.\n\nAs Giary Özil… Giray Özil, Twitter post, February 27, 2013, 10:42 a.m.,\n\nhttps://twitter.com/girayozil/status/306836785739210752 .\n\nAs noted earlier… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the\n\nSame Continuous Build?,” (2013), http://scribes.tweetscriber.com/realgenekim/206 .\n\nIn 2010, there… John Thomas and Ashish Kumar, “Welcome to the Google Engineering Tools\n\nBlog,” Google Engineering Tools blog, posted May 3, 2011, http://google-\n\nengtools.blogspot.com/2011/05/welcome-to-google-engineering-tools.html .\n\nThis requires considerable… Ashish Kumar, “Development at the Speed and Scale of Google,”\n\n(presentation at QCon, San Francisco, CA, 2010), https://qconsf.com/sf2010/dl/qcon- sanfran-2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf .\n\nHe said, “I… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nJeff Atwood, one… Jeff Atwood, “Pair Programming vs. Code Reviews,” CodingHorror.com,\n\nNovember 18, 2013, http://blog.codinghorror.com/pair-programming-vs-code-reviews/ .\n\nHe continued, “Most… Ibid.\n\nDr. Laurie Williams performed… “Pair Programming,” ALICE Wiki page, last modified April 4, 2014, http://euler.math.uga.edu/wiki/index.php?title=Pair_programming .\n\nShe argues that… Elisabeth Hendrickson, “DOES15 - Elisabeth Hendrickson - Its All About Feedback,” YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015,\n\nhttps://www.youtube.com/watch?v=r2BFTXBundQ .\n\nIn her 2015… Ibid.\n\nThe problem Hendrickson… Ibid.\n\nWorse, skilled developers… Ibid.\n\nHendrickson lamented that… Ibid.",
      "content_length": 2096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "That was an actual… Ryan Tomayko and Shawn Davenport, personal interview with Gene\n\nKim, 2013.\n\nIt is many… Ibid.\n\nReading through the… Ibid.\n\nAdrian Cockcroft observed… Adrian Cockcroft, interview by Michael Ducy and Ross Clanton,\n\n“Adrian Cockcroft of Battery Ventures – the Goat Farm – Episode 8,” The Goat Farm, podcast audio, July 31, 2015, http://goatcan.do/2015/07/31/adrian-cockcroft-of-battery-\n\nventures-the-goat-farm-episode-8/ .\n\nSimilarly, Dr. Tapabrata Pal… Tapabrata Pal, “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit,\n\nJanuary 4, 2016, https://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nJason Cox, Senior… Jason Cox, “Disney DevOps.”\n\nAt Target in… Ross Clanton and Heather Mickman, ‘DOES14 - Ross Clanton and Heather Mickman - DevOps at Target,” YouTube video, 29:20, posted by DevOps Enterprise Summit\n\n2014, October 29, 2014, https://www.youtube.com/watch?v=exrjV9V9vhY .\n\n“As we went… Ibid.\n\nShe added, “I… Ibid.\n\nConsider a story… John Allspaw and Jez Humble, personal correspondence with Gene Kim,\n\n2014.\n\nCHAPTER 19\n\nThe result is… Spear, The High-Velocity Edge, chap. 1.\n\n“For such an… Ibid., chap. 10.\n\nA striking example… Julianne Pepitone, “Amazon EC2 Outage Downs Reddit, Quora,” CNN Money, April 22, 2011,\n\nhttp://money.cnn.com/2011/04/21/technology/amazon_server_outage .\n\nIn January 2013… Timothy Prickett Morgan, “A Rare Peek Into The Massive Scale of AWS,”\n\nEnterprise Tech, November 14, 2014, http://www.enterprisetech.com/2014/11/14/rare-peek- massive-scale-aws/ .\n\nHowever, a Netflix… Adrian Cockcroft, Cory Hicks, and Greg Orzell, “Lessons Netflix Learned from the AWS Outage,” The Netflix Tech Blog, April 29, 2011,\n\nhttp://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html .",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "They did so… Ibid.\n\nDr. Sidney Dekker… Sidney Dekker, Just Culture: Balancing Safety and Accountability\n\n(Lund University, Sweden: Ashgate Publishing Company, 2007), 152.\n\nHe asserts that… “DevOpsDays Brisbane 2014 - Sidney Decker - System Failure, Human Error: Who’s to Blame?” Vimeo video, 1:07:38, posted by info@devopsdays.org, 2014,\n\nhttps://vimeo.com/102167635 .\n\nAs John Allspaw… Jenn Webb, interview with John Allspaw, “Post-Mortems, Sans Finger-\n\nPointing,” The O’Reilly Radar Postcast, podcast audio, August 21, 2014, http://radar.oreilly.com/2014/08/postmortems-sans-finger-pointing-the-oreilly-radar-\n\npodcast.html .\n\nBlameless post-mortems, a… John Allspaw, “Blameless PostMortems and a Just Culture,”\n\nCodeAsCraft.com, May 22, 2012, http://codeascraft.com/2012/05/22/blameless- postmortems/ .\n\nIan Malpass, an… Ian Malpass, “DevOpsDays Minneapolis 2014 -- Ian Malpass, Fallible humans,” YouTube video, 35:48, posted by DevOps Minneapolis, July 20, 2014,\n\nhttps://www.youtube.com/watch?v=5NY-SrQFrBU .\n\nDan Milstein, one… Dan Milstein, “Post-Mortems at HubSpot: What I Learned from 250\n\nWhys,” HubSpot, June 1, 2011, http://product.hubspot.com/blog/bid/64771/Post-Mortems- at-HubSpot-What-I-Learned-From-250-Whys .\n\nRandy Shoup, former… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nWe may also… “Post-Mortem for February 24, 2010 Outage,” Google App Engine website, March 4, 2010, https://groups.google.com/forum/#!topic/google-appengine/p2QKJ0OSLc8\n\n; “Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US- East Region,” Amazon Web Services website, accessed May 28, 2016,\n\nhttps://aws.amazon.com/message/5467D2/ .\n\nThis desire to… Bethany Macri, “Morgue: Helping Better Understand Events by Building a\n\nPost Mortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nFor example, as… Spear, The High-Velocity Edge, chap. 4.\n\nDr. Amy C. Edmondson… Amy C. Edmondson, “Strategies for Learning from Failure,”\n\nHarvard Business Review, April 2011, https://hbr.org/2011/04/strategies-for-learning- from-failure .\n\nDr. Spear summarizes… Ibid.\n\nWe now know… Ibid., chap. 3.",
      "content_length": 2190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "However, prior to… Michael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson, “Facing Ambiguous Threats,” Harvard Business Review, November 2006,\n\nhttps://hbr.org/2006/11/facing-ambiguous-threats/ar/1 .\n\nThey describe how… Ibid.\n\nThey observe, “Firms… Ibid.\n\nThe authors conclude… Ibid.\n\nOn failures, Roy… Roy Rapoport, personal correspondence with Gene Kim, 2012.\n\nHe continues, “I… Ibid.\n\nHe concludes, “DevOps… Ibid.\n\nAs Michael Nygard… Michael T. Nygard, Release It!: Design and Deploy Production-Ready\n\nSoftware (Pragmatic Bookshelf: Raleigh, NC, 2007), Kindle edition, Part I.\n\nAn even more… Jeff Barr, “EC2 Maintenance Update,” AWS Blog, September 25, 2014,\n\nhttps://aws.amazon.com/blogs/aws/ec2-maintenance-update/ .\n\nAs Christos Kalantzis… Bruce Wong and Christos Kalantzis, “A State of Xen - Chaos Monkey & Cassandra,” The Netflix Tech Blog, October 2, 2014,\n\nhttp://techblog.netflix.com/2014/10/a-state-of-xen-chaos-monkey-cassandra.html .\n\nBut, Kalantzis continues… Ibid.\n\nAs Kalantzis and… Ibid.\n\nEven more surprising… Roy Rapoport, personal correspondence with Gene Kim, 2015.\n\nSpecific architectural patterns… Adrian Cockcroft, personal correspondence with Gene Kim, 2012.\n\nIn this section… Jesse Robbins, “GameDay: Creating Resiliency Through Destruction - LISA11,” Slideshare.net, posted by Jesse Robbins, December 7, 2011,\n\nhttp://www.slideshare.net/jesserobbins/ameday-creating-resiliency-through-destruction .\n\nRobbins defines resilience… Ibid.\n\nJesse Robbins observes… Jesse Robbins, Kripa Krishnan, John Allspaw, and Tom Limoncelli, “Resilience Engineering: Learning to Embrace Failure,” amcqueue 10, no. 9 (September 13,\n\n2012): https://queue.acm.org/detail.cfm?id=2371297 .\n\nAs Robbins quips… Ibid.\n\nAs Robbins describes… Ibid.",
      "content_length": 1752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "Robbins explains, “You… Ibid.\n\nDuring that time… “Kripa Krishnan: ‘Learning Continuously From Failures’ at Google,” YouTube video, 21:35, posted by Flowcon, November 11, 2014,\n\nhttps://www.youtube.com/watch?v=KqqS3wgQum0 .\n\nKrishnan wrote, “An… Kripa Krishnan, “Weathering the Unexpected,” Communications of\n\nthe ACM 55, no. 11 (November 2012): 48-52, http://cacm.acm.org/magazines/2012/11/156583-weathering-the-unexpected/abstract .\n\nSome of the learnings… Ibid.\n\nAs Peter Senge… Widely attributed to Peter Senge.\n\nCHAPTER 20\n\nAs Jesse Newland… Jesse Newland, “ChatOps at GitHub,” SpeakerDeck.com, February 7,\n\n2013, https://speakerdeck.com/jnewland/chatops-at-github .\n\nAs Mark Imbriaco… Mark Imbriaco, personal correspondence with Gene Kim, 2015.\n\nThey enabled Hubot… Newland, “ChatOps at GitHub.”\n\nHubot often performed… Ibid.\n\nNewland observes that… Ibid.\n\nInstead of putting… Leon Osterweil, “Software processes are software too,” paper presented\n\nat International Conference on Software Engineering, Monterey, CA, 1987, http://www.cs.unibo.it/cianca/wwwpages/ids/letture/Osterweil.pdf .\n\nJustin Arbuckle was… Justin Arbuckle, “What Is ArchOps: Chef Executive Roundtable” (2013).\n\nWhat resulted was… Ibid.\n\nArbuckle’s conclusion was… Ibid.\n\nBy 2015, Google… Cade Metz, “Google Is 2 Billion Lines of Code—and It’s All in One Place,”\n\nWired, September 16, 2015, http://www.wired.com/2015/09/google-2-billion-lines- codeand-one-place/ .\n\nThe Chrome and… Ibid.\n\nRachel Potvin, a… Ibid.\n\nFurthermore, as Eran… Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?” (2013),",
      "content_length": 1611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "http://scribes.tweetscriber.com/realgenekim/206 .\n\nAs Randy Shoup… Randy Shoup, personal correspondence with Gene Kim, 2014.\n\nTom Limoncelli, co-author… Tom Limoncelli, “Yes, you can really work from HEAD,” EverythingSysAdmin.com, March 15, 2014, http://everythingsysadmin.com/2014/03/yes-\n\nyou-really-can-work-from-head.html .\n\nTom Limoncelli describes… Tom Limoncelli, “Python is better than Perl6,” EverythingSysAdmin.com, January 10, 2011,\n\nhttp://everythingsysadmin.com/2011/01/python-is-better-than-perl6.html .\n\nGoogle used C++… “Which programming languages does Google use internally?,” Quora.com\n\nforum, accessed May 29, 2016, https://www.quora.com/Which-programming-languages- does-Google-use-internally .; “When will Google permit languages other than Python, C++,\n\nJava and Go to be used for internal projects?,” Quora.com forum, accessed May 29, 2016, https://www.quora.com/When-will-Google-permit-languages-other-than-Python-C-Java-\n\nand-Go-to-be-used-for-internal-projects/answer/Neil-Kandalgaonkar .\n\nIn a presentation… Ralph Loura, Olivier Jacques, and Rafael Garcia, “DOES15 - Ralph Loura,\n\nOlivier Jacques, & Rafael Garcia - Breaking Traditional IT Paradigms to…,” YouTube video, 31:07, posted by DevOps Enterprise Summit, November 16, 2015,\n\nhttps://www.youtube.com/watch?v=q9nNqqie_sM .\n\nIn many organizations… Michael Rembetsy and Patrick McDonnell, “Continuously\n\nDeploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,” Slideshare.net, posted by Patrick McDonnell, October 4, 2012, http://www.slideshare.net/mcdonnps/continuously-\n\ndeploying-culture-scaling-culture-at-etsy-14588485 .\n\nAt that time, Etsy… Ibid.\n\nOver the next… Ibid.\n\nSimilarly, Dan McKinley… Dan McKinley, “Why MongoDB Never Worked Out at Etsy,” McFunley.com, December 26, 2012, http://mcfunley.com/why-mongodb-never-worked-out-\n\nat-etsy .\n\nCHAPTER 21\n\nOne of the… “Kaizen,” Wikipedia, last modified May 12, 2016,\n\nhttps://en.wikipedia.org/wiki/Kaizen .\n\nDr. Spear explains… Spear, The High-Velocity Edge, chap. 8.\n\nSpear observes that… Ibid.\n\nClanton describes, “We… Mickman and Clanton, “(Re)building an Engineering Culture.”",
      "content_length": 2130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "Ravi Pandey, a… Ravi Pandey, personal correspondence with Gene Kim, 2015.\n\nClanton expands on… Mickman and Clanton, “(Re)building an Engineering Culture.”\n\nIn addition to… Hal Pomeranz, “Queue Inversion Week,” Righteous IT, February 12, 2009, https://righteousit.wordpress.com/2009/02/12/queue-inversion-week/ .\n\nAs Dr. Spear… Spear, The High-Velocity Edge, chap. 3.\n\nIn an interview with Jessica… Jessica Stillman, “Hack Days: Not Just for Facebookers,” Inc.,\n\nFebruary 3, 2012, http://www.inc.com/jessica-stillman/hack-days-not-just-for- facebookers.html .\n\nIn 2008, Facebook… AP, “Number of active users at Facebook over the years,” Yahoo! News, May 1, 2013, https://www.yahoo.com/news/number-active-users-facebook-over-\n\n230449748.html?ref=gs .\n\nDuring a hack… Haiping Zhao, “HipHop for PHP: Move Fast,” post on Haiping Zhao’s\n\nFacebook page, February 2, 2010, https://www.facebook.com/notes/facebook- engineering/hiphop-for-php-move-fast/280583813919 .\n\nIn an interview with Cade… Cade Metz, “How Three Guys Rebuilt the Foundation of Facebook,” Wired, June 10, 2013,\n\nhttp://www.wired.com/wiredenterprise/2013/06/facebook-hhvm-saga/all/ .\n\nSteve Farley, VP… Steve Farley, personal correspondence with Gene Kim, January 5, 2016.\n\nKarthik Gaekwad, who… “Agile 2013 Talk: How DevOps Change Everything,” Slideshare.net,\n\nposted by Karthik Gaekwad, August 7, 2013, http://www.slideshare.net/karthequian/howdevops\n\nchangeseverythingagile2013karthikgaekwad/ .\n\nAs Glenn O’Donnell… Glenn O’Donnell, “DOES14 - Glenn O’Donnell - Forrester - Modern\n\nServices Demand a DevOps Culture Beyond Apps,” YouTube video, 12:20, posted by DevOps Enterprise Summit 2014, November 5, 2014, https://www.youtube.com/watch?\n\nv=pvPWKuO4_48 .\n\nAs of 2014… Nationwide, 2014 Annual Report, https://www.nationwide.com/about-\n\nus/nationwide-annual-report-2014.jsp .\n\nSteve Farley, VP… Steve Farley, personal correspondence with Gene Kim, 2016.\n\nCapital One, one… “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nDr. Tapabrata Pal… Tapabrata Pal, personal correspondence with Gene Kim, 2015.",
      "content_length": 2196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "Target is the… “Corporate Fact Sheet,” Target company website, accessed June 9, 2016,\n\nhttps://corporate.target.com/press/corporate .\n\nIncidentally, the first… Evelijn Van Leeuwen and Kris Buytaert, “DOES15 - Evelijn Van\n\nLeeuwen and Kris Buytaert - Turning Around the Containership,” YouTube video, 30:28, posted by DevOps Enterprise Summit, December 21, 2015,\n\nhttps://www.youtube.com/watch?v=0GId4AMKvPc .\n\nClanton describes, “2015… Mickman and Clanton, “(Re)building an Engineering Culture.”\n\nAt Capital One… “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nBland explains that… Bland, “DOES15 - Mike Bland - Pain Is Over, If You Want It.”\n\nEven though they… Ibid.\n\nThey used several… Ibid.\n\nBland described, “The… Ibid.\n\nBland continues, “One… Ibid.\n\nAs Bland describes… Ibid.\n\nBland continues, “It… Ibid.\n\nHe continues, “The… Ibid.\n\nBland describes Fixits… Mike Bland, “Fixits, or I Am the Walrus,” Mike-Bland.com, October 4, 2011, https://mike-bland.com/2011/10/04/fixits.html .\n\nThese Fixits, as… Ibid.\n\nCHAPTER 22\n\nOne of the top… James Wickett, “Attacking Pipelines--Security meets Continuous Delivery,”\n\nSlideshare.net, posted by James Wickett, June 11, 2014,\n\nhttp://www.slideshare.net/wickett/attacking-pipelinessecurity-meets-continuous-delivery .\n\nJames Wickett, one… Ibid.\n\nSimilar ideas were… Tapabrata Pal, “DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,” YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016,\n\nhttps://www.youtube.com/watch?v=bbWFCKGhxOs .\n\nJustin Arbuckle, former… Justin Arbuckle, personal interview with Gene Kim, 2015.",
      "content_length": 1721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "He continues, “By… Ibid.\n\nThis helped the… Snehal Antani, “IBM Innovate DevOps Keynote,” YouTube video, 47:57,\n\nposted by IBM DevOps, June 12, 2014, https://www.youtube.com/watch?v=s0M1P05-6Io .\n\nIn a presentation… Nick Galbreath, “DevOpsSec: Appling DevOps Principles to Security,\n\nDevOpsDays Austin 2012,” Slideshare, posted by Nick Galbreath, April 12, 2012, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security .\n\nFurthermore, he states… Ibid.\n\nFurthermore, we should… “OWASP Cheat Sheet Series,” OWASP.org, last modified March 2, 2016, https://www.owasp.org/index.php/OWASP_Cheat_Sheet_Series .\n\nThe scale of… Justin Collins, Alex Smolen, and Neil Matatall, “Putting to your Robots to Work V1.1,” Slideshare.net, posted by Neil Matatall, April 24, 2012,\n\nhttp://www.slideshare.net/xplodersuv/sf-2013-robots/ .\n\nIn early 2009… “What Happens to Companies That Get Hacked? FTC Cases,” Giant Bomb\n\nforum, posted by SuicidalSnowman, July 2012, http://www.giantbomb.com/forums/off- topic-31/what-happens-to-companies-that-get-hacked-ftc-case-540466/ .\n\nIn their previously… Collins, Smolen, and Matatall, “Putting to your Robots to Work V1.1.”\n\nThe first big… Twitter Engineering, “Hack Week @ Twitter,” Twitter blog, January 25, 2012,\n\nhttps://blog.twitter.com/2012/hack-week-twitter .\n\nJosh Corman observed… Josh Corman and John Willis, “Immutable Awesomeness - Josh\n\nCorman and John Willis at DevOps Enterprise Summit 2015,” YouTube video, 34:25, posted by Sonatype, October 21, 2015, https://www.youtube.com/watch?v=-S8-lrm3iV4 .\n\nIn the 2014… Verizon, ”2014 Data Breach Investigations Report,” (Verizon Enterprise Solutions, 2014), https://dti.delaware.gov/pdfs/rp_Verizon-DBIR-2014_en_xg.pdf .\n\nIn 2015, this… “2015 State of the Software Supply Chain Report: Hidden Speed Bumps on the Way to ‘Continuous,’” (Fulton, MD: Sonatype, Inc, 2015),\n\nhttp://cdn2.hubspot.net/hubfs/1958393/White_Papers/2015_State_\n\nof_the_Software_Supply_Chain_Report-.pdf?t=1466775053631 .\n\nThe last statistic… Dan Geer and Joshua Corman, “Almost Too Big to Fail,” ;login:: The\n\nUsenix Magazine, 39, no. 4 (August 2014): 66-68, https://www.usenix.org/system/files/login/articles/15_geer_0.pdf .\n\nUS Federal Government… Wyatt Kash, “New details released on proposed 2016 IT spending,” FedScoop, February 4, 2015, http://fedscoop.com/what-top-agencies-would-\n\nspend-on-it-projects-in-2016 .",
      "content_length": 2403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "As Mike Bland… Bland, “DOES15 - Mike Bland - Pain Is Over, If You Want It.”\n\nFurthermore, the Cloud.gov… Mossadeq Zia, Gabriel Ramírez, Noah Kunin, “Compliance\n\nMasonry: Bulding a risk management platform, brick by brick,” 18F, April 15, 2016, https://18f.gsa.gov/2016/04/15/compliance-masonry-buildling-a-risk-management-\n\nplatform/ .\n\nMarcus Sachs, one… Marcus Sachs, personal correspondence with Gene Kim, 2010.\n\nWe need to… “VPC Best Configuration Practices,” Flux7 blog, January 23, 2014, http://blog.flux7.com/blogs/aws/vpc-best-configuration-practices .\n\nIn 2010, Nick… Nick Galbreath, “Fraud Engineering, from Merchant Risk Council Annual Meeting 2012,” Slideshare.net, posted by Nick Galbreath, May 3, 2012,\n\nhttp://www.slideshare.net/nickgsuperstar/fraud-engineering .\n\nOf particular concern… Nick Galbreath, “DevOpsSec: Appling DevOps Principles to Security,\n\nDevOpsDays Austin 2012,” Slideshare.net, posted by Nick Galbreath, April 12, 2013, http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security .\n\nWe were always… Ibid.\n\nThis was a ridiculously… Ibid.\n\nAs Galbreath observed… Ibid.\n\nGalbreath observed, “One… Ibid.\n\nAs Jonathan Claudius… Jonathan Claudius, “Attacking Cloud Services with Source Code,”\n\nSpeakerdeck.com, posted by Jonathan Claudius, April 16, 2013, https://speakerdeck.com/claudijd/attacking-cloud-services-with-source-code .\n\nCHAPTER 23\n\nITIL defines utility… Axelos, ITIL Service Transition (ITIL Lifecycle Suite) (Belfast, Ireland: TSO, 2011), 48.\n\nSalesforce was founded… Reena Matthew and Dave Mangot, “DOES14 - Reena Mathew and Dave Mangot - Salesforce,” Slideshare.net, posted by ITRevolution, October 29, 2014,\n\nhttp://www.slideshare.net/ITRevolution/does14-reena-matthew-and-dave-mangot- salesforce .\n\nBy 2007, the… Dave Mangot and Karthik Rajan, “Agile.2013.effecting.a.dev ops.transformation.at.salesforce,” Slideshare.net, posted by Dave Mangot, August 12, 2013,\n\nhttp://www.slideshare.net/dmangot/agile2013effectingadev-opstransformationatsalesforce .\n\nKarthik Rajan, then… Ibid.",
      "content_length": 2054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "At the 2014… Matthew and Mangot, “DOES14 - Salesforce.”\n\nFor Mangot and… Ibid.\n\nFurthermore, they noted… Ibid.\n\nBill Massie is… Bill Massie, personal correspondence with Gene Kim, 2014.\n\nBecause the scope… “Glossary,” PCI Security Standards Council website, accessed May 30,\n\n2016, https://www.pcisecuritystandards.org/pci_security/glossary .\n\nAre code review… PCI Security Standards Council, Payment Card Industry (PCI) Data\n\nSecurity Stands: Requirements and Security Assessment Procedures, Version 3.1 (PCI Security Standards Council, 2015), Section 6.3.2.\n\nhttps://webcache.googleusercontent.com/search? q=cache:hpRe2COzzdAJ:https://www.cisecuritystandards.org/documents/PCI_DSS_v3-\n\n1_SAQ_D_Merchant_rev1-1.docx+&cd=2&hl=en&ct=clnk&gl=us .\n\nTo fulfill this… Bill Massie, personal correspondence with Gene Kim, 2014.\n\nMassie observes that… Ibid.\n\nAs a result… Ibid.\n\nAs Bill Shinn… Bill Shinn, “DOES15 - Bill Shinn - Prove it! The Last Mile for DevOps in\n\nRegulated Organizations,” Slideshare.net, posted by ITRevolution, November 20, 2015, http://www.slideshare.net/ITRevolution/does15-bill-shinn-prove-it-the-last-mile-for-\n\ndevops-in-regulated-organizations .\n\nHelping large enterprise… Ibid.\n\nShinn notes, “One… Ibid.\n\n“That was fine… Ibid.\n\nHe explains, “In… Ibid.\n\nShinn states that… Ibid.\n\nShinn continues, “With… Ibid.\n\nThat requires deriving… Ibid.\n\nShinn continues, “How… Ibid.\n\nShinn gives an… Ibid.\n\nTo help solve… James DeLuccia, Jeff Gallimore, Gene Kim, and Byron Miller, DevOps Audit\n\nDefense Toolkit (Portland, OR: IT Revolution, 2015), http://itrevolution.com/devops-and-",
      "content_length": 1593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "auditors-the-devops-audit-defense-toolkit .\n\nShe made the… Mary Smith (a pseudonym), personal correspondence with Gene Kim, 2013\n\nShe observed:… Ibid., 2014.\n\nCONCLUSION\n\nAs Jesse Robbins… “Hacking Culture at VelocityConf,” Slideshare.net, posted by Jesse Robbins, June 28, 2012, http://www.slideshare.net/jesserobbins/hacking-culture-at-\n\nvelocityconf .\n\nAPPENDIX\n\nThe Lean movement started… Ries, The Lean Startup.\n\nA key principal… Kent Beck et al., “Twelve Principles of Agile Software,” AgileManifesto.org,\n\n2001, http://agilemanifesto.org/principles.html .\n\nBuilding upon the… Humble and Farley, Continuous Delivery.\n\nThis idea was… Fitz, “Continuous Deployment at IMVU.”\n\nToyota Kata describes… Rother, Toyota Kata, Introduction.\n\nHis conclusion was… Ibid..\n\nIn 2011, Eric… Ries, The Lean Startup.\n\nIn The Phoenix… Kim, Behr, and Spafford, The Phoenix Project, 365.\n\nMyth 1: “Human… Denis Besnard and Erik Hollnagel, Some Myths about Industrial Safety(Paris, Centre De Recherche Sur Les Risques Et Les Crises Mines, 2012), 3,\n\nhttp://gswong.com/?wpfb_dl=31 .\n\nMyth 2: “Systems… Ibid., 4.\n\nMyth 3: “Safety… Ibid., 6.\n\nMyth 4: “Accident… Ibid., 8.\n\nMyth 5: “Accident… Ibid., 9.\n\nMyth 6: Safety… Ibid., 11.\n\nRather, when the… John Shook, “Five Missing Pieces in Your Standardized Work (Part 3 of 3),” Lean.org, October 27, 2009, http://www.lean.org/shook/DisplayObject.cfm?o=1321 .",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "Time to resolve… “Post Event Retrospective - Part 1,” Rally Blogs, accessed May 31, 2016,\n\nhttps://www.rallydev.com/blog/engineering/post-event-retrospective-part-i .\n\nBethany Macri, from… “Morgue: Helping Better Understand events by Building a Post\n\nMortem Tool - Bethany Macri,” Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, http://vimeo.com/77206751 .\n\nThese discussions have… Cockcroft, Hicks, and Orzell, “Lessons Netflix Learned.”\n\nSince then, Chaos… Ibid.\n\nLenny Rachitsky wrote… Lenny Rachitsky, “7 Keys to a Successful Public Health\n\nDashboard,” Transparent Uptime, December 1, 2008, http://www.transparentuptime.com/2008/11/rules-for-successful-public-health.html .",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "Index\n\nSymbols Numbers A B C D E F G H I J K L M N O P Q R S T U V W Y Z\n\nNote: Figures are indicated with f; footnotes are indicated with n\n\nSYMBOLS %C/A, 11, 65\n\nNUMBERS 2PT. See two-pizza team\n\n18F team, 325–326 2013 State of DevOps Report, 159–160\n\nA AAS. See Amazon Auto Scaling\n\nAdams, Keith, 302 Agile\n\nInfrastructure and Velocity Movement, 5\n\nInfrastructure Movement, 354 Manifesto, 4–5 Movement, 354 principles, xxii–xxiii\n\nAisin Seiki Global, 43–44",
      "content_length": 458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "Alcoa, 41–42, 279\n\nAlgra, Ingrid, 305n\n\nAllspaw, John\n\nAgile Infrastructure and Velocity Movement, 5\n\ndark launches, 173–174\n\ndeployment failures, 251–252\n\nlearning culture, 273–274\n\nproduction metrics, 204\n\nVelocity Movement, 354\n\nAllstate, 66\n\nAmazon\n\ncontinuous delivery, 176 deploys per day, xxxivn evolutionary architecture, 184–185 market-oriented organization, 81 service-oriented architecture, 90–91\n\nAmazon Auto Scaling, 221–222 Amazon AWS\n\ncompliance in regulated environments, 342–344 resilience, 271–273, 281–282 service outage, 271n\n\nAndon cord\n\nconsequences of not pulling, 140\n\nillustrated, 361f swarming, 31, 32 virtual, 138–140 and work stoppages, 360–361\n\nAntani, Snehal, 314 APIs\n\nenablement, 91–93 Feature API, 245",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "service interactions using, 89\n\nversioned, 185\n\napplication logging, 201–203, 201n\n\napplication-based release patterns, 171–175\n\nArbuckle, Justin, 290, 314\n\narchitecture, evolutionary\n\nAmazon case study, 184–185\n\narchitectural archetypes, 183f\n\nBlackboard Learn case study, 186–189, 187f, 188f\n\ncode repository, 187f, 188f\n\ndecoupling functionality, 186\n\ndescription of, 179–180\n\nimmutable services, 185 loosely-coupled architecture, 181–182 monoliths vs microservices, 182–185 Second Law of Architectural Thermodynamics, 180–181 service-oriented architecture, 182 strangler application pattern, 180, 185–189\n\ntightly-coupled architecture, 180–181, 185 versioned APIs, 185 versioned services, 185\n\narchitecture, loosely-coupled, 89–93, 181–182, 254–255\n\narchitecture, monitoring, 198–199 architecture, service-oriented, 89, 90–91, 182 Ashman, David, 187\n\nATDD. See development, acceptance test-driven Atwood, Jeff, 147, 259–260 Austin, Jim, 215–216 automated environment build process\n\nassets to check into version control repository, 116 automated configuration systems, 118",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "benefits of automation, 114–115 common build mechanisms, 113\n\ncritical role of version control, 117 environment consistency, 118\n\nenvironment development on demand, 113–115\n\nenvironment re-build vs repair, 118 environments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120 standardization, 114\n\ntesting, 113 testing environments, 113\n\nuses of automation, 114\n\nversion control as predictor of organizational performance, 117 version control systems, 115–118\n\nautomated validation test suite\n\nacceptance test-driven development, 134–135\n\nacceptance tests, 131, 132 analysis tools, 138\n\nautomating manual tests, 135–136\n\ncode configuration management tools, 138 environment validation, 137–138\n\nerror detection, 132–133 fast testing, 132, 133–134\n\nfeedback, 130 green builds, 129–130\n\nideal vs non-ideal testing, 133f",
      "content_length": 990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "integration tests, 131, 132 non-functional requirements testing, 137–138\n\nperformance testing, 136–137 test types, 130–131\n\ntest-driven development, 134–135 testing in parallel, 133–134, 134f\n\nunit tests, 130–131, 132–133\n\nunreliable test, 135\n\nautomation. See automated environment build process;\n\ndeployment process automation; testing, automated\n\nB Baker, Bill, 118\n\nBarnes & Noble, 51 batch sizes\n\ncontinuous deployment, 20\n\nerror management, 19 large, 19–20\n\nsingle-piece flow, 19, 20 small, 18–20\n\nsmall batch strategy, 19\n\nsmall vs large, 20f\n\nBazaarvoice, 97n, 149–151\n\nBeck, Kent, 134, 154 Beedle, Mike, 102n\n\nBehr, Kevin, 195, 203n Besnard, Denis, 359–360\n\nBetz, Charles, 180–181, 313n\n\nBig Fish Games, 95–97 bimodal IT, 56\n\nBlackboard Learn",
      "content_length": 751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "case study, 187f, 188f\n\nPerl, 187\n\nstrangler application pattern, 186–189\n\nblameless post-mortems\n\ncountermeasures, 276 goals of, 274–275\n\noutcomes of, 275–276 publicizing, 277–278\n\nsample agenda, 362–364\n\nstakeholders present, 275 transparent uptime, 277n\n\nBland, Mike, 123–126, 124n, 306–307, 325 Blank, Steve, 355\n\nBlankenship, Ed, 210\n\nblitz\n\ngoals, 301\n\nimprovement, 299 kaizen, 299\n\nBlockbuster, 51 blue-green deployment pattern\n\ndeployment, 166–169, 166f, 167n\n\nFarley, David, 168–169 low-risk releases, 166–169, 166f\n\nRuby on Rails, 167n\n\nBMW, 67\n\nBohmer, Richard M. J., 279 Booch, Grady, 6n\n\nBorders, 51 Boubez, Toufic, 219, 224–226\n\nBouwman, Jan-Joost, 305n Brakeman, 322, 322f",
      "content_length": 687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "brownfield services. See services, brownfield Building Blocks, 188–189\n\nbuild-measure-learn cycle, 355 bureaucratic organizations, 39 Burgess, Mark, 6n\n\nbusiness logic\n\nchanges to, 78, 79 coordinating changes to, 79 moving to application layer, 79\n\nbusiness relationship manager, 96 Buytaert, Kris, 305n\n\nC C++\n\neBay, 179n, 182 Facebook, 153n, 175, 302\n\nGoogle, 296n Google Web Server, 123\n\nCagan, Marty, 70\n\nCampbell-Pretty, Em, 111–112 Canahuati, Pedro, 85 canary release pattern, 153n, 169–171, 170f, 170n canary tests, 153\n\nCapitol One, 304–306 case studies\n\nAmazon, 184–185\n\nAmazon AWS, 271–273, 344–345 anomaly detection techniques, 224–226 ATM systems, 344–345 Bazaarvoice, 149–151\n\nBig Fish Games, 95–97",
      "content_length": 711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "Blackboard Learn, 186–189, 187f, 188f Capitol One, 304–306\n\nCSG International, 157–159 Dixons Retail, 168–169 Etsy, 77–80, 162–164, 297–298, 328–330, 339–341\n\nFacebook, 153–155, 174–175 Federal Government, 325–326 Google, 237–239, 257–258 Google Web Server, 123–126\n\nHP, 144–146 Intuit, 241–248 LinkedIn, 71–73, 207–208\n\nNationwide Insurance, 304–306 Netflix, 215–216, 221–222, 271–273 Nordstrom, 51–55, 61–62 Pivotal Labs, 260–261\n\nRight Media, 227–229 Salesforce.com, 337–338 Target, 91–93, 299–300, 304–306\n\nTwitter, 320–323 Yahoo! Answers, 246–248\n\nChacon, Scott, 249 Chakrabarti, Arup, 232\n\nchange approval processes. See also code reviews\n\ncase study, 249–251 change advisory boards, 253\n\nchange control failures, 252–253 change freezes, 258–259 change review lead times, 258f code reviews, 255–258\n\ncoordination and scheduling of changes, 254–255",
      "content_length": 853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "counterfactual thinking, 251, 251n cutting bureaucratic processes, 263–264\n\ndangers of, 251–252 email pass-around, 257 engineer roles, 259 GitHub Flow, 250\n\nGoogle case study, 257–258 guidelines for code reviews, 256 in a loosely-coupled architecture, 254–255\n\nmanual testing, 258–259 over-the-shoulder, 256 pair programming, 256, 259–263 peer reviews, 249–251, 253, 254f, 255–258\n\nPivotal Labs case study, 260–261 pull requests, 250, 250f, 261–263 review steps, 250–251\n\nsmall batch sizes, 255–256 test-driven development, 259–260 tool-assisted, 257 traditional change controls, 252–254\n\ntypes of code reviews, 256–257\n\nChaos Monkey, 272–273, 281–282, 364 chat rooms, 74\n\nChatOps, 287–289 Chuvakin, Anton A., 202 CI. See continuous integration Clanton, Ross, 263, 299–300, 305\n\nClaudius, Jonathan, 330 Clemm, Josh, 71, 72 Cloud.gov, 325–326\n\ncluster immune system, 171n",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "cluster immune system release pattern, 169, 170–171 coaching kata, 45 Cockcoft, Adrian, 82n, 200, 263\n\ncode\n\ncommits, 148 configuration management tools, 138\n\ndeployment, 22, 160–162 deployment process automation, 160 deployment process changes, 154\n\ninfrastructure as, 6n merging, 143–144 migration, 117n packaging, 128\n\nrepositories, 187f, 188f, 290–292, 315–317 re-use, 289–290 signing, 319–320\n\nsource code integrity, 319–320\n\ncode commits, 148 code reviews. See also change approval processes\n\nchange review lead times, 258f\n\nemail pass-around, 257 Google case study, 257–258 guidelines for, 256\n\nover-the-shoulder, 256 pair programming, 256 small batch sizes, 255–256 tool-assisted, 257\n\ntypes of, 256–257\n\ncollective knowledge, 42–43 compliance\n\naudit and compliance documentation and proof, 341–345",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "regulatory compliance objectives, 235–236 security and compliance and change approval processes, 333– 335\n\nconfiguration management tools, 116n constraints\n\nbottlenecks, 22\n\ncode deployment, 22 environment creation, 22 overly tight architecture, 23 test setup and run, 22–23\n\ncontinual experimentation and learning, 37–46 continuous delivery\n\ndeployment pipeline, 127–129, 127f\n\nGoogle, 176 low-risk releases, 175–177\n\nContinuous Delivery Movement, 5–6, 354 continuous deployment, 6, 20, 175–177\n\ncontinuous integration case study, 149–151 code merging, 143–144\n\ndescription of, 144–146 Dev vs DevOps, 126n frequent code commits, 148 gated commits, 148\n\nintegration problems, 143 large batch development, 147–148 and trunk-based development practices, 148–151\n\nand version control, 148–149 Convergence of DevOps, 353–356 Conway, Melvin, 77 Conway’s Law, 77–78, 88",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "Cook, Scott, 242 core chronic conflict, xxiv–xxvi, xxvn core conflict cloud, 356–357, 356f\n\nCorman, Josh, 313, 323 cost of delay, 213n COTS software, 361 Cox, Jason, 87, 99–100, 263\n\nCSG International\n\nbrownfield services, 56 cross-training, 86–87\n\ndaily deployments, 157–159\n\nCunningham, Ward, 148\n\nD dark launches, 173–175 dashboards, 207n data sets. See telemetry\n\nDebois, Patrick, 5 dedicated release engineer, 96 DeGrandis, Dominica, 18 Dekker, Sidney\n\njust culture, 273 safety culture, 28, 38\n\ndependency scanning\n\nJava, 319\n\nRuby on Rails, 319\n\nDeployinator, 163–164, 163f deployment\n\nautomated self-service, 159–160\n\nblue-green pattern, 166–169, 166f, 167n change, 124",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "code, 22, 154, 160–162\n\nconsistency, 156\n\ncontinuous, 20, 175–177 daily, 157–159\n\ndecoupling from releases, 164–175\n\ndefined, 164 on demand, 165\n\nfast, 161, 161f flow, 156\n\nissues, 78\n\nlead time, 8–11, 9f, 165 making safer, 229–230\n\noverlay of production deployment activities, 213\n\npace, 154 pipeline requirements, 156–157\n\nprocess automation, 155–164, 159f\n\nself-service developer, 162–164 speed and success, 79\n\ntool, 163–164, 163f\n\ndeployment lead time\n\ndesign and development, 8\n\nlead time vs processing time, 9–10, 9f\n\nLean Manufacturing, 9 Lean Product Development, 8\n\nlong, 10, 10f, 165 short, 10–11, 11f\n\ntesting and operations, 9\n\nworkflow, 9\n\ndeployment pipeline\n\nbreakdown, 138–140\n\ncontainers in, 128n",
      "content_length": 714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "continuous delivery, 127–129, 127f\n\nand information security, 330–331\n\ndeployment pipeline protection\n\nAmazon AWS case study, 342–344\n\naudit and compliance documentation and proof, 341–345\n\ncategories of changes, 334–335, 334n compliance in regulated environments, 341–345\n\ndestructive testing, 338 Etsy case study, 339–341\n\nnormal changes, 334, 336–338\n\nproduction telemetry for ATM systems, 344–345 Salesforce case study, 337–338\n\nsecurity and compliance and change approval processes, 333–\n\n335 separation of duties, 338–341\n\nstandard changes, 334, 335–336\n\nurgent changes, 334–335\n\ndeployment process automation\n\nautomated self-service deployments, 159–160\n\nautomating manual steps, 155–156 code deployment as part of deployment pipeline, 160–162\n\ncode promotion processes, 160 CSG International case study, 157–159\n\ndeployment consistency, 156\n\ndeployment flow, 156 deployment pipeline requirements, 156–157\n\nenvironment consistency, 157, 158\n\nEtsy case study, 162–164 fast deployments, 161, 161f\n\nlead time reduction, 156\n\nMTTR, 158, 159f, 161f",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "process documentation, 155\n\nproduction incident decrease, 158, 159f\n\nself-service developer deployment, 162–164 Shared Operations team, 157\n\nsmoke testing, 156, 163\n\ndeployment vs releases, 164–175\n\nDev in production-like environments\n\nassets to check into version control repository, 116 automated configuration systems, 118\n\nbenefits of automation, 114–115\n\ncommon build mechanisms, 113 critical role of version control, 117\n\nenvironment consistency, 118\n\nenvironment development on demand, 113–115 environment re-build vs repair, 118\n\nenvironments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120 standardization, 114\n\ntesting, 113\n\ntesting environments, 113 uses of automation, 114\n\nversion control as predictor of organizational performance, 117\n\nversion control systems, 115–118 Development. See entries under Dev\n\ndevelopment, acceptance test-driven, 134–135\n\ndevelopment, test-driven",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "automated testing, 134–135, 293\n\nhandling defect density, 135n\n\nand low-risk releases, 154 and pair programming, 259–260\n\ntesting before code writing, 9n development, trunk-based, 143–151\n\ndevelopment, waterfall, 5\n\nDevOps\n\nAgile Infrastructure and Velocity Movement, 5\n\nAgile Manifesto, 4–5\n\nbusiness value of, xxxii–xxxiii Continuous Delivery Movement, 5–6\n\ndefined, 4\n\nDevOpsDays, 5 downward spiral in, xxx–xxxii\n\nethics of, xxix–xxxv\n\nhistory of, 3–6 Lean Movement, 4\n\nrevolution, xxii team engagement, 101n\n\nThe Three Ways, 11–12\n\nToyota Kata movement, 6\n\nDevOps myths\n\nLAMP stack, xvi\n\nMySQL, xvi PHP, xvi\n\nDevOps transformation\n\nbimodal IT, 56 chat rooms, 74\n\nexpanding DevOps, 58–59\n\ngreenfield vs brownfield services, 54–56",
      "content_length": 732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "leveraging innovators, 57–58\n\nLinkedIn case study, 71–73 making work visible, 73\n\nmanaging technical debt, 69–71\n\nphases of initiatives, 59 rapid communication environment, 74\n\nreinforcing desired behavior, 73–74\n\nshared tools, 73–74 shared work queue, 73–74\n\nsystems of engagement, 56–57\n\nsystems of record, 56 technical debt, 69–71, 70f\n\ntechnology adoption curve, 58f\n\ntransformation team, 66–73\n\nDevOpsDays, 5, 305n\n\nDignan, Larry, 91 Disney, 87, 99–100\n\nDixons Retail, 168–169\n\ndocumentation\n\nautomated tests as, 293\n\nprocess, 155\n\ndownward spiral, xxvi–xxviii, xxx–xxxii, 357, 357f Drucker, Peter, 60\n\nDweck, Carol, 87\n\nE eBay, 70, 179–180, 179n, 182\n\ne-commerce sites\n\nanomaly detection techniques, 224–226 application security, 318\n\nmetrics sources, 210",
      "content_length": 761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "Nordstrom, 51–55\n\nTarget, 91–93\n\nEdmondson, Amy C., 278, 279\n\nEdwards, Damon, 64, 97\n\nemployee Net Promotor Score, xxxiiin environment consistency, 157, 158\n\nenvironment security, 324–326, 324n\n\nenvironments\n\nconsistency, 157, 158\n\ncreation restraints, 22\n\ndefinition of, 113n rapid communication environment, 74\n\nvalidation, 137–138\n\nenvironments, automated build\n\nassets to check into version control repository, 116\n\nautomated configuration systems, 118 benefits of automation, 114–115\n\ncommon build mechanisms, 113\n\ncritical role of version control, 117 environment consistency, 118\n\nenvironment development on demand, 113–115\n\nenvironment re-build vs repair, 118 environments stored in version control, 115–116\n\nimmutable infrastructure, 119\n\nmetadata, 115 new definition of finished development, 119–121\n\nquick environment development, 112\n\nshared version control repository, 115–118 sprints, 119–120\n\nstandardization, 114 testing, 113",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "testing environments, 113\n\nuses of automation, 114 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nenvironments, production-like\n\nassets to check into version control repository, 116\n\nautomated configuration systems, 118\n\nbenefits of automation, 114–115 common build mechanisms, 113\n\ncritical role of version control, 117\n\ndevelopment on demand, 113–115 environment consistency, 118\n\nenvironment development on demand, 113–115 environment re-build vs repair, 118\n\nenvironments stored in version control, 115–116\n\nimmutable infrastructure, 119 metadata, 115\n\nnew definition of finished development, 119–121\n\nquick environment development, 112 shared version control repository, 115–118\n\nsprints, 119–120\n\nstandardization, 114 testing, 113\n\ntesting environments, 113\n\nuses of automation, 114 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nerrors\n\ndetection, 28, 114n, 132–133\n\nmanagement, 19\n\nEtsy",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "brownfield services, 56\n\ncase study, 77–80, 162–164 code deployment, 162n\n\ndesignated Ops, 100–101\n\nDevOps transformation, 51 functional-oriented organizations, 84\n\nLAMP stack at, 196\n\nmetrics library, 204–206 Morgue, 277–278\n\nMySQL at, 196, 297–298\n\norganizational learning, 40 PHP, 196, 297, 297n\n\nproduction monitoring, 196–198 programming languages used, 297n\n\npublicizing post-mortems, 277\n\nPython, 297, 297n security telemetry, 328–330\n\nseparation of duties, 339–341\n\nSprouter, 78–80, 88 technology stack standardization, 297–298\n\nEvans, Eric J., 89\n\nEvans, Jason, 302 Extreme Programming, 134, 154\n\nF Facebook\n\nC++, 153n, 175, 302\n\ncanary release pattern, 170\n\ncase study, 153–155 dark launches, 174–175\n\nfront-end codebase, 153n",
      "content_length": 736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "Gatekeeper, 172n\n\nJavaScript, 175 PHP, 153n, 154, 175, 302\n\nshared pain, 85\n\ntechnical debt, 302\n\nFarley, David\n\nautomated testing, 126–127\n\nblue-green deployment pattern, 168–169 continuous delivery, 175–176, 354\n\nContinuous Delivery Movement, 5–6 continuous integration, 126n\n\ninfrastructure as code, 6n\n\nFarley, Steve, 303, 305 Farrall, Paul, 95–97\n\nfast release cycle experimentation\n\ncase study, 246–248 Feature API, 245\n\nhistory of, 243\n\nintegrating into feature planning, 245–248 integrating into feature testing, 244–245\n\nintegrating into release, 245\n\noutcomes of, 244 user research, 244–245\n\nYahoo! Answers, 246–248\n\nfast testing, 132, 133–134\n\nfeature toggles, 171–173, 172n, 175, 229–230\n\nfeatures\n\nconsequences of new, 57\n\nextra, 24\n\nplanning, 245–248 testing, 244–245",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "Federal Government agencies, 325–326\n\nfeedback\n\nautomated build, integration, and test processes, 30\n\nautomated testing, 126, 130\n\nerror detection, 28 ineffective quality controls, 32–34\n\nmechanisms for production telemetry, 229\n\nOps and market-oriented outcomes, 103 optimizing for downstream work centers, 34–35\n\nproblem prevention, 30 problem visibility, 29–30\n\nproduction telemetry, 229\n\nQA automation, 33–34 safety in complex systems, 27–29\n\nswarming, 30–32\n\nfeed-forward loops, 29, 30 Fernandez, Roberto, 59, 80\n\nfirst stories, 360f\n\nFitz, Tim\n\ncontinuous delivery, 354\n\ncontinuous deployment, 6, 175–177\n\nexpand/contract pattern, 168n\n\nfix forward, 230\n\nfixed mindset, 87 Flickr, 173–174\n\nForsgren, Nicole, 220\n\nFowler, Martin, 132, 185, 186\n\nG Gaekwad, Karthik, 303\n\nGalbreath, Nick, 227–229, 328–330",
      "content_length": 808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "Game Days, 282–284\n\nGanglia, 196 Garcia, Rafael, 297\n\nGatekeeper, 172n, 175\n\nGauntlt, 313, 317 General Motors Fremont plant, 29, 31, 37\n\ngenerative organizations, 39–40 GitHub\n\nfunctional-oriented organizations, 84\n\nGitHub Flow, 250 organizational knowledge, 287–289\n\npeer reviews, 249–251\n\nGitHub Flow, 250 goal setting, 68\n\nGoogle\n\nautomated testing, 125n C++, 296n\n\ncode reviews, 257–258\n\ncontinuous delivery, 176 Disaster Recovery Program (DiRT), 284\n\nfunctional-oriented organizations, 84 imposter syndrome, 124n\n\nJava, 296n\n\nJavaScript, 296n launch and hand-off readiness reviews, 237–239\n\nproduction service, 234\n\nprogramming languages used, 297n publicizing post-mortems, 277\n\nPython, 296\n\nservice-oriented architecture, 90 source code repository, 291–292",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "Google App Engine, 139\n\nGoogle Cloud Datastore, 181f, 182 Google Web Server\n\nAndon cord, 138 automated testing, 123–126\n\nC++, 123\n\ncase study, 123–126 Fixit Grouplet, 307\n\nTesting Grouplet, 124–126, 306–307\n\nGovindarajan, Vijay, 66 Grafana, 204, 224\n\nGraphite, 196, 204, 224\n\nGray, Jim, 184 greenfield services. See services, greenfield\n\ngrowth mindset, 87\n\nGruver, Gary\n\nautomated testing, 123, 136 continuous integration, 144–146\n\nineffective quality controls, 34\n\nGupta, Prachi, 207–208\n\nGWS. See Google Web Server\n\nH Hammant, Paul, 186n\n\nHammond, Paul, 5, 354\n\nHand-Off Readiness Review, 237–239, 239f\n\nhandoffs\n\ndangers of, 358–359\n\nloss of knowledge, 21\n\nreducing batch size, 21\n\nworkflow management, 21",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "hardening phase, 53n\n\nHendrickson, Elisabeth, 30, 135, 260–261\n\nheroics, 10, 25, 25n high-trust culture, 37–38\n\nHipHop compiler, 302\n\nHodge, Victoria J., 215–216\n\nHollnagel, Erik, 359–360\n\nHP, 144–146\n\nHRR. See Hand-Off Readiness Review Humble, Jez\n\nautomated testing, 126–127\n\ncontinuous delivery, 175–176\n\nContinuous Delivery Movement, 5–6\n\ncontinuous integration, 126n dangers of change approval processes, 252\n\nevolutionary architecture, 179\n\ninfrastructure as code, 6n\n\nuser research, 244\n\nhypothesis-driven development, 241–248\n\nI ICHT, 339–341\n\nImbriaco, Mark, 288\n\nimposter syndrome, 124n improvement blitz\n\norganizational learning and improvement, 299\n\nSpear, Steven, 299\n\nToyota Production System, 299\n\nimprovement goal examples, 68\n\nimprovement kata, 6, 355 industrial safety, 359–360",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "information radiator, 206–208\n\ninformation security\n\n18F team, 325–326\n\napplication security, 318–323\n\nautomated security testing, 318f\n\nbad paths, 318 Brakeman, 322, 322f\n\nbuild images, 317\n\nCloud.gov, 325–326\n\ncode signing, 319–320\n\ncreating security telemetry, 327–330 data breaches, 323–324\n\nand defect tracking and post-mortems, 315\n\ndependency scanning, 319\n\nand the deployment pipeline, 317–318, 330–331\n\ndynamic analysis, 319\n\nenvironment security, 324–326 Etsy case study, 328–330\n\nFederal Government case study, 325–326\n\nGauntlt, 313, 317\n\nGraphite, 329f\n\nhappy paths, 318\n\nintegrating into production telemetry, 326–327 inviting InfoSec to product demonstrations, 314\n\nJava, 324\n\nMetasploit, 325\n\nNmap, 325\n\npreventive security controls, 315–317 Ruby on Rails, 322\n\nrugged DevOps, 313\n\nsad paths, 318",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "security libraries, 316\n\nshared code repositories and services, 315–317\n\nsoftware supply chain security, 323–324 source code integrity, 319–320\n\nSQL injection attempts, 329, 329f\n\nstatic analysis, 319, 320–323\n\nTwitter case study, 320–323\n\nvalue stream, 63\n\nInfosec. See information security\n\ninfrastructure as code, 6n\n\ninfrastructure metrics, 213n\n\nInGraphs, 208\n\nintegrated development environment, 128n\n\nintegration, 120n Intuit, 241–248\n\niteration length, 68\n\nITIL, 116n, 231n, 253, 333, 334n\n\nITIL CMDB, 212, 212n\n\nJ Jacob, Adam, 6n\n\nJacques, Olivier, 297\n\nJava\n\nautomation, 127 Bazaarvoice, 149\n\ndependency scanning, 319\n\neBay, 179n\n\nGoogle, 296n\n\ninformation security, 324 LinkedIn, 71\n\nlogging infrastructure, 201n",
      "content_length": 723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "ORM, 79n\n\nproduction metrics, 204\n\nthreading libraries, 292\n\nJavaScript\n\nFacebook, 175 Google, 296n\n\nproduction telemetry, 209\n\nJones, Daniel T., 19\n\njust culture, 38\n\nK Kalantzis, Christos, 281–282\n\nkanban boards\n\nand the Lean Movement, 4\n\nsharing between Ops and Dev, 104 workflow management, 16–17, 16f\n\nKanies, Luke, 6n\n\nKastner, Erik, 163–164\n\nKim, Gene, 195, 203n, 233, 252, 313\n\nKissler, Courtney, 51–54, 61–62\n\nKnight Capital, 251–252 knowledge sharing, 42–43\n\nKohavi, Ronny, 244\n\nKrishnan, Kripa, 284\n\nL LAMP stack\n\nDevOps myths, xvi at Etsy, 196\n\nLauderbach, John, 88n\n\nLaunch Readiness Review, 238–239",
      "content_length": 612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "lead time\n\nchange review, 258f\n\ndeployment, 8–11, 9f, 165 and the Lean Movement, 353\n\nreduction, 156\n\nLean Manufacturing\n\ndeployment lead time, 8–11\n\nfunctional-oriented organizations, 84 manufacturing value stream, 7–8\n\ntechnology value stream, 8–11\n\nThe Three Ways, 11–12\n\nLean Movement, 4, 353\n\nLean principles, xxii\n\nLean Product Development, 9 Lean Startup Movement, 355\n\nLean UX Movement, 355\n\nlearning culture\n\nAmazon AWS case study, 271–273\n\namplifying weak failure signals, 279–280 bad apple theory, 273\n\nblameless post-mortems, 274–276\n\nChaos Monkey, 272–273, 281–282\n\nGame Days, 282–284\n\ninjecting faults into production environment, 281–282, 282n\n\njust culture, 273–274 leaders and, 44–46\n\nMorgue, 277–278\n\nNetflix case study, 271–273\n\npublicizing post-mortems, 277–278\n\nredefining failure, 280–281\n\nrehearsing failures, 282–284",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "resilience, 281–282\n\nresilience engineering, 282–283\n\nThe Third Way, 44–46\n\nand Toyota Kata, 45\n\nand Toyota Production System, 45\n\nLesiecki, Nick, 306–307\n\nLetuchy, Eugene, 174–175\n\nLightbody, Patrick, 231\n\nLimoncelli, Tom, 218, 238–239, 292, 296–297\n\nLinkedIn\n\ncase study, 71–73 Java, 71\n\nOperation Inversion, 72\n\nOracle, 71\n\nself-service metrics, 207–208\n\nLittle, Christopher, xxvii, 73 logging, 201–203, 201n\n\nlogging infrastructure\n\nJava, 201n\n\nRuby on Rails, 201n\n\nLoura, Ralph, 297\n\nLove, Paul, 313 low-risk releases\n\napplication-based release patterns, 165–166, 171–175\n\nautomated self-service deployments, 159–160\n\nautomating manual steps, 155–156\n\nblue-green deployment pattern, 166–169, 166f\n\ncanary release pattern, 169–171, 170f canary tests, 153\n\ncluster immune system release pattern, 169, 170–171\n\ncode deployment as part of deployment pipeline, 160–162",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "code deployment process changes, 154\n\ncode promotion processes, 160 continuous delivery, 175–177\n\ncontinuous deployment, 175–177\n\nCSG International case study, 157–159\n\ndark launches, 173–175\n\ndatabase changes, 167–168, 168n\n\ndecoupling deployment from releases, 164–175 deployment consistency, 156\n\ndeployment defined, 164\n\ndeployment flow, 156\n\ndeployment lead time, 165\n\ndeployment on demand, 165\n\ndeployment pace, 154 deployment pipeline requirements, 156–157\n\ndeployment process automation, 155–164\n\ndeployment tool, 163–164, 163f\n\nDixons Retail case study, 168–169\n\nenvironment consistency, 157, 158 environment-based release patterns, 165, 166–171\n\nEtsy case study, 162–164\n\nevolutionary architecture, 179–189\n\nFacebook case study, 174–175\n\nfast deployments, 161, 161f\n\nfeature toggles, 171–173, 175 lead time reduction, 156\n\nMTTR, 158, 159f, 161f\n\nperformance degradation, 172\n\npoint-of-sale systems, 168–169\n\nprocess documentation, 155 production incident decrease, 158, 159f",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "release frequency, 153, 154f\n\nrelease risk, 165\n\nreleases defined, 164–165\n\nresilience, 172\n\nroll back, 172 self-service developer deployment, 162–164\n\nShared Operations team, 157\n\nsmoke testing, 156, 163\n\nLRR. See Launch Readiness Review\n\nM Macri, Bethany, 363\n\nMacys.com, 136\n\nmaking work visible, 15–17, 73, 104\n\nMalpass, Ian, 197, 276 Mangot, Dave, 337–338\n\nmanufacturing lead time, 4\n\nmanufacturing value stream\n\ndefined, 7\n\ndescription of, 7–8 feedback issues, 29\n\nintegrating learning, 37\n\nlow-trust environment, 37\n\nworkflow, 7–8\n\nMarsh, Dianne, 98\n\nMartin, Karen, 7 Massie, Bill, 339, 341\n\nMathew, Reena, 337–338\n\nMaximilien, E. Michael, 135n\n\nMcDonnell, Patrick, 196\n\nMcKinley, Dan, 298",
      "content_length": 696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Mediratta, Bharat, 124, 306–307\n\nMesseri, Eran, 125, 291 metadata, 115\n\nMetasploit, 325\n\nmetrics\n\nactionable business, 211f\n\napplications and business, 210–212\n\ninfrastructure, 212–213 libraries, 204–205\n\nproduction, 204–206\n\nself-service, 207–208\n\nsources, 209\n\nfor telemetry improvement, 65–66\n\nMetz, Cade, 302\n\nMickman, Heather, 91–93, 263, 305\n\nMicrosoft, 118\n\nMicrosoft Operations Framework, 195\n\nMilstein, Dan, 276\n\nminimum viable product, 355 MOF. See Microsoft Operations Framework\n\nMoore, Geoffrey A., 57\n\nMorgue, 277–278\n\nMTTR\n\ndeployment process automation, 158, 159f\n\nrecording, 277 telemetry, 197, 197f\n\nMueller, Ernst, 97n, 149–151, 207\n\nMulkey, Jody, 84–85, 212\n\nMySQL\n\nDevOps myths, xvi at Etsy, 196, 297–298",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "N Nagappan, Nachi, 135n\n\nNagios, 198 NASA, 279–280\n\nNational Instruments, 54–55\n\nNationwide Insurance, 304–306\n\nNetflix\n\nauto-scaling capacity, 221–222\n\nmarket-oriented organization, 81 Netflix AWS, 118n\n\norganizational values, 90n\n\nredefining failure, 280–281\n\nresilience, 271–273, 281–282\n\nself-service platforms, 98 telemetry analysis, 215–216\n\nNewland, Jesse, 287–289\n\nNike, 159\n\nNmap, 325\n\nnon-functional requirements, 294\n\nNordstrom, 51–55, 61–62 North, Dan, 168–169, 202\n\nNR Program, 42–43\n\nNygard, Michael, 281\n\nO Obidos, 184\n\nObject Relational Mapping layer, 79, 79n\n\nO’Donnell, Glenn, 304\n\nOhno, Taiichi, 18n\n\nO’Neill, Paul, 41, 279 Open Web Application Security Project, 319, 319n, 324n",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "Operation Desert Shield, 165n\n\nOperation Inversion, 72 Operations. See entries under Ops\n\nOps and market-oriented outcomes\n\ncreating shared services, 97–99\n\nengineers embedded into service teams, 99–100\n\nfeedback, 103\n\nintegration into Dev rituals, 101–104 internal shared services teams, 97–99\n\nliaisons assigned to service teams, 100–101\n\nmaking work visible, 104\n\nOps liaisons, 96\n\nparticipation in Dev retrospectives, 102–103 participation in Dev standups, 102\n\nself-service platforms, 97–98\n\nsilos, 102\n\ntool standardization, 98–99\n\noptimizing for downstream work centers\n\ncustomer types, 34 Designing for Manufacturing principles, 34\n\nOracle\n\napplication configuration settings, 368\n\nCOTS software, 361\n\neBay, 179n ERP code migration, 117n\n\nLinkedIn, 71\n\norganizational cultures, 39, 39f\n\norganizational knowledge\n\nautomated tests as documentation, 293\n\nautomating standardized processes, 289–290 ChatOps, 287–289",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "code re-use, 289–290\n\ncodified non-functional requirements, 294\n\ncommunities of practice, 293\n\nEtsy case study, 297–298\n\nHubot, 287–289 integrating automation into chat rooms, 287–289\n\nresuable Ops user stories, 295\n\nsource code repository, 290–292\n\ntechnology choices to achieve organizational goals, 295–298\n\ntechnology stack standardization, 297–298 test-driven development, 293\n\nusing chat rooms and chat bots, 287–289\n\norganizational learning and improvement\n\nattending external conferences, 304–306\n\nblitz goals, 301\n\nCapitol One case study, 304–306 enable learning and teaching, 303–304\n\nimprovement blitz, 299\n\ninternal consulting and coaching, 306–307\n\nkaizen blitz, 299\n\nNationwide Insurance case study, 304–306 paying down technical debt, 300–303\n\nTarget case study, 299–300, 304–306\n\nThe Third Way, 38–40\n\norganizations\n\nfunctional-oriented, 80, 84\n\nmarket-oriented, 80–81 matrix-oriented, 80\n\nORM. See Object Relational Mapping layer\n\nOsterling, Mike, 7\n\nÖzil, Giray, 256",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "P pair programming\n\nand code reviews, 256\n\ndescription of, 259–263\n\npair programmed rollback, 139n\n\npairing hours, 260n Pivotal Labs case study, 260–261\n\nPal, Tapabrata, 263, 305, 313\n\nPandey, Ravi, 300\n\nParoski, Drew, 302\n\npathological organizations, 39 PayPal, 58n\n\nPerl\n\nBlackboard Learn, 187\n\neBay, 179n\n\nproduction metrics, 204\n\nPerrow, Charles, 28 PHP\n\nConway’s Law, 78\n\nDevOps myths, xvi\n\nEtsy, 196, 297, 297n\n\nFacebook, 153n, 154, 175, 302\n\nORM, 79 production telemetry, 205, 230, 230f\n\nPivotal Labs, 260–261\n\npoint-of-sale systems, 168–169\n\nPoppendieck, Mary, 24\n\nPoppendieck, Tom, 24 post-mortems. See blameless post-mortems\n\nproblem visibility, 29–30\n\nproblems",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "fact-based problem solving, 203–204\n\nintegration, 143\n\nleaders and problem solving, 44\n\nprevention of, 30 problem visibility, 29–30\n\nswarming of smaller, 32\n\nproduction metrics\n\nJava, 204\n\nPerl, 204\n\nPython, 204 Ruby on Rails, 204\n\nproduction monitoring. See telemetry\n\nproduction telemetry. See also telemetry; telemetry analysis\n\ncontextual inquiry, 232–233\n\nfeature toggles, 229–230\n\nfeedback mechanisms, 229 fix forward, 230, 230f\n\nfunction-oriented teams, 231–232\n\nGoogle case study, 237–239\n\nHand-Off Readiness Review, 238–239\n\nimproving flow, 232n JavaScript, 209\n\nlaunch and hand-off readiness reviews, 237–239, 239f\n\nlaunch guidance, 234–235\n\nLaunch Readiness Review, 238–239\n\nmaking deployments safer, 229–230\n\nmarket-oriented teams, 231 pager rotation duties, 230–232\n\nPHP, 205, 230, 230f\n\nproduction service, 234–239\n\nregulatory compliance objectives, 235–236",
      "content_length": 871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "roll back, 230 service handback mechanism, 236f, 237\n\nsite reliability engineers, 237–239\n\nUX observation, 233, 233n\n\nPrugh, Scott\n\nand bimodal IT, 57\n\ncross-training, 86–87 daily deployments, 157–158\n\ntelemetry, 201\n\nPuppet Labs, 159–160\n\nPython\n\nEtsy, 297, 297n\n\nGoogle, 296 ORM, 79n\n\nproduction metrics, 204\n\nQ quality controls, 32–34\n\nqueue time, 358–359, 358f\n\nqueues\n\nlong, 81\n\nqueue time, 358–359, 358f shared work, 73–74\n\nsize, 18, 18n\n\nR Rachitsky, Lenny, 365\n\nRajan, Karthik, 337\n\nRapoport, Roy, 215–216, 280–281\n\nRational Unified Process, 5, 354 Raymond, Eric A., 77",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "Red Hat, 114\n\nReddy, Tarun, 222–223\n\nreinforcing desired behavior, 73–74\n\nrelease patterns\n\napplication-based, 165–166, 171–175\n\ncanary, 169–171, 170f\n\ncluster immune system, 169, 170–171\n\nenvironment-based, 165, 166–171\n\nreleases, 164–165\n\nRembetsy, Michael, 51, 196, 298 repositories\n\ncode, 187f, 188f, 290–292, 315–317\n\nversion control, 115–118\n\nresilience, 43–44, 172, 271–273, 281–282\n\nrework, 11 Richardson, Vernon, xxviiin\n\nRies, Eric, 20, 171n, 355\n\nRight Media, 227–229\n\nRobbins, Jesse, 283, 354\n\nRoberto, Michael, 279\n\nroll back, 139n, 230 Rossi, Chuck, 153–155, 174n\n\nRother, Mike\n\ncoaching kata, 45\n\nfunctional-oriented organizations, 84\n\nimprovement kata, 40\n\nToyota Kata movement, 6\n\nRuby on Rails\n\nautomation, 127\n\nblue-green deployment, 167n\n\ndependency scanning, 319",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "information security, 322 logging infrastructure, 201n\n\nORM, 79\n\nproduction metrics, 204\n\nRugged Computing Movement, 355–356\n\nS Sachs, Marcus, 326\n\nsafety culture, 28, 38–40\n\nsafety in complex systems, 27–29\n\nsafety in the workplace, 41–42 Salesforce.com, 337–338\n\nSchafer, Andrew, 5, 354\n\nSchwaber, Ken, 102n\n\nScott, Kevin, 72–73\n\nScrum methodology, 102n, 119–120\n\nScryer, 221–222 second stories, 360f\n\nself-service platforms, 97–98, 206–208\n\nSenge, Peter\n\nlearning organizations, 40\n\nproblem visibility, 29\n\nservice handback mechanism, 236f, 237, 237n\n\nservices, brownfield\n\nCSG International, 56\n\ndefined, 55\n\nDevOps transformation, 54–56\n\nEtsy, 56 improving speed and quality, 57\n\nwith largest potential business benefit, 55n\n\ntransformations of, 55–56",
      "content_length": 756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "value streams, 54–56\n\nservices, greenfield\n\nconsequences of new features, 57\n\ndefined, 54 types of projects, 54–56\n\nservices, immutable, 185\n\nservices, shared, 97–99\n\nservices, versioned, 185\n\nShingo, Shigeo, 23\n\nShinn, Bill, 342–344 Shoup, Randy\n\ndeployment pipeline breakdowns, 139\n\nevolutionary architecture, 179, 182\n\nloosely-coupled architecture, 90\n\npeer reviews of code changes, 255–256 publicizing post-mortems, 277\n\nsource code repository, 291–292\n\nsilos\n\nOps and market-oriented outcomes, 102\n\nteam organization, 85\n\nSimian Army, 364–365 single-piece flow, 19, 20\n\nsmoke testing, 156, 163\n\nsmoothing, 223, 223f, 223n\n\nSOAs. See architecture, service-oriented\n\nSouders, Steve, 354 Spafford, George, 195, 203n, 313\n\nSpear, Steven\n\nconditions for safety, 28\n\nimprovement blitz, 299\n\nIT failures, xxvii",
      "content_length": 808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "organizational learning, 40 paying down technical debt, 302\n\nresilience, 271\n\nworkplace safety, 41–42\n\nsprint planning boards, 16–17\n\nsprints, 119–120\n\nSprouter, 78–80, 88 stabilization phase, 53n\n\nstack engineers, 86\n\nStatsD, 204–205\n\nStillman, Jessica, 302\n\nStoneham, Him, 246–248 strangler application pattern, 180, 185–189, 186n\n\nSussman, Noah, 162–163\n\nSussna, Jeff, 233n\n\nswarming\n\nAndon cord, 31, 32\n\nand common management practice, 31 goal of, 30\n\nreasons for, 31\n\nof smaller problems, 32\n\nsystems of engagement\n\ndefined, 56–57\n\nand related brownfield systems of record, 57\n\nsystems of record, 56\n\nT Tableau, 223 Target\n\nAPI enablement, 91–93\n\ncase study, 91–93",
      "content_length": 669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "cutting bureaucratic processes, 263–264\n\nDevOps Dojo, 299–300\n\nfirst DevOpsDays, 305n\n\ninternal technology conferences, 304–306\n\nTDD. See development, test-driven\n\nteam organization\n\nAPI enablement at Target, 91–93\n\nbounded contexts, 89\n\nbusiness logic changes, 78, 79 business relationship manager, 96\n\ncollaboration, 88\n\nConway’s Law, 77–78, 88\n\ncross-functional and independent teams, 82\n\ncross-training, 85–87\n\ndatabase stored procedures changes, 78 decreasing handoffs, 79\n\ndedicated release engineer, 96\n\ndeployment issues, 78\n\ndeployment speed and success, 79\n\nembedding needed skills, 82–83\n\nfixed mindset, 87 functional-oriented organizations, 80\n\nfunding services and products, 87–88\n\ngrowth mindset, 87\n\nintegrating Ops into Dev teams, 95–105\n\ninternal shared services teams, 97–99 long queues, 81\n\nloosely-coupled architecture, 89–93\n\nmaking functional orientation work, 83–84, 83f\n\nmarket orientations, 80–81, 82–83\n\nmatrix-orientations, 80",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "optimizing for cost, 81–82 optimizing for speed, 82–83\n\nquality as shared goal, 84–85\n\nservice interactions via APIs, 89\n\nservice-oriented architecture, 89\n\nshared pain, 85 silos, 85\n\nspecialists vs. generalists, 85–87, 86f\n\nstack engineers, 86\n\nsynchronization, 78\n\nteam boundaries, 88\n\nteam size, 90–91 testing and operations, 89\n\ntwo-pizza team, 90–91\n\nteam size, 90–91\n\nteams\n\n18F team, 325–326\n\ndevelopment, 8 function-oriented, 231–232\n\nmarket-oriented, 231\n\nservice, 83n, 97–101\n\nshared operations, 157, 158n\n\nteam formation, 67 team size, 90–91\n\ntechnology value stream, 8\n\ntesting, 124–126\n\ntransformation, 66–74\n\ntwo-pizza team, 90–91\n\ntechnical debt\n\ndescription of, 148\n\nmanaging, 69–71",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "paying down, 144, 300–303\n\nreducing, 69–71, 70f\n\ntechnology adoption curve, 58f\n\ntechnology value stream\n\nabsence of fast feedback, 29–30\n\ncreating a high-trust culture, 37–38\n\ndefined, 8\n\ndeployment lead time, 8–11\n\ninputs, 8 integrating learning, 37–38\n\nresponses to incidents and accidents, 38–39\n\ntelemetry. See also production telemetry; telemetry analysis\n\nactionable business metrics, 211f\n\napplication logging, 201–203\n\napplications and business metrics, 210–212 centralized infrastructure, 198–200\n\nculture of causality, 195\n\ncustomer acquisition funnel, 210\n\ndata collection, 199\n\nDEBUG level, 201 defined, 196\n\nERROR level, 202\n\nevent router, 199\n\nfact-based problem solving, 203–204\n\nFATAL level, 202\n\ngraphs and dashboards, 204–205 identify gaps, 209–213\n\nincident resolution time, 197f\n\nINFO level, 202\n\ninformation radiator, 206–208\n\ninformation security in product telemetry, 326–327",
      "content_length": 899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "infrastructure metrics, 212–213 ITIL CMDB, 212\n\nLinkedIn case study, 207–208\n\nlog centralization, 199\n\nlogging entry generation, 202–203\n\nlogging levels, 201–202 making deployments safer, 229–230\n\nmetrics for improvement, 65–66\n\nmetrics libraries, 204–205\n\nmetrics library, 204–206\n\nmetrics sources, 209\n\nmonitoring architecture, 198–199 monitoring framework, 200f\n\nMTTR, 197, 197f\n\noverlay of production deployment activities, 213\n\nproduction metrics, 204–206\n\nand the Second Way, 30 security telemetry, 327–330\n\nself-service metrics, 207–208\n\nself-service platforms, 206–208\n\nStatsD, 204–205\n\ntools, 205n\n\nWARN level, 202\n\ntelemetry analysis. See also production telemetry; telemetry\n\n3 standard deviation rule, 219, 219f, 225, 225f\n\nalerts for undesired outcomes, 218\n\nanalysis tools, 224n\n\nanomaly detection techniques, 222–226\n\nautomated, 222f\n\nauto-scaling capacity, 221–222, 221f case study, 215–216",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "filtering techniques, 224\n\nGaussian distribution, 217, 217f Kolmogorov-Smirnov test, 224, 225, 226f\n\nmeans, 216–217 Netflix case study, 221–222\n\nnon-Gaussian distribution, 219–222, 220f non-parametric techniques, 225 outlier detection, 215–216\n\nprecursors to production incidents, 218 Server Outlier Detection, 216\n\nsmoothing, 223, 223f, 223n standard deviations, 216–217, 217f statistical techniques, 216–217, 223\n\ntest environments, 113n testing\n\nautomated, 123–127, 125n, 130, 134–135, 136, 293 automated validation test suite, 132, 133–134, 133f, 134f, 136– 138\n\ndestructive testing, 338 fast testing, 132, 133–134\n\nideal vs non-ideal testing, 133f manual testing, 258–259\n\nnon-functional requirements testing, 137–138 performance testing, 136–137 smoke testing, 156, 163\n\ntesting environments, 113 testing in parallel, 133–134, 134f\n\ntesting, A/B\n\ncase study, 246–248 Feature API, 245\n\nhistory of, 243",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "integrating into feature planning, 245–248\n\nintegrating into feature testing, 244–245 integrating into release, 245 outcomes of, 244\n\nuser research, 243n, 244–245 Yahoo! Answers, 246–248\n\ntesting, automated\n\nacceptance test-driven development, 134–135\n\nacceptance tests, 131, 132, 139 analysis tools, 138 automated build and test processes, 127\n\nautomated test suites, 126 automated validation test suite, 129–138\n\nautomating manual tests, 135–136 change deployment, 124 code configuration management tools, 138\n\ncode packaging, 128 deployment pipeline, 127–129\n\nenvironment validation, 137–138 error detection, 132–133 failure indicators, 139\n\nfast testing, 132, 133–134 feedback, 126, 130\n\ngreen builds, 129–130 handling input from external integration points, 131n\n\nideal vs non-ideal testing, 133f integration tests, 131, 132 non-functional requirements testing, 137–138\n\nperformance testing, 136–137 production increases, 124\n\ntest types, 130–131",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "test-driven development, 134–135 testing in parallel, 133–134, 134f\n\ntesting teams, 124–126 and trunk-based development, 145–146\n\nunit tests, 130–131, 132–133, 139 unreliable test, 135 version control, 128\n\nThe First Way\n\nbatch size comparison, 20f\n\nbottlenecks, 22 constraint identification, 21–23\n\ncontinuous, 20 controlling queue size, 18 description of, 11\n\nerror management, 19 handoff reduction, 21\n\nincreasing workflow, 15 kanban boards, 16–17 large batch sizes, 19–20\n\nlimiting work in process, 17–18 loss of knowledge, 21\n\nmaking work visible, 15–17 multitasking, 17–18 reducing batch size, 18–20\n\nsingle piece flow, 19, 20 small batch sizes, 18–20\n\nsmall batch strategy, 19 sprint planning boards, 16–17 transferring work, 15–16\n\nwaste elimination, 23–25 work interruptions, 17",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "workflow management, 16, 21 workflow visualizations, 16–17\n\nThe Second Way\n\nAndon cord, 31, 32 conditions for safety, 28–29\n\ndescription of, 12 error detection, 28\n\nfailure, 28 feedback loops, 29, 30 feed-forward loops, 29, 30\n\nincreasing information flow, 29 ineffective quality controls, 32–34\n\noptimizing for downstream work centers, 34–35 peer reviews, 33–34 problem visibility, 29–30\n\nQA automation, 33–34 safety in complex systems, 27–29\n\nswarming, 30–32 telemetry, 30\n\nThe Third Way\n\nblameless post-mortems, 40 collective knowledge, 42–43\n\ncreating a high-trust culture, 37–38 description of, 12–13\n\nimprovement of daily work, 40–42 integrating learning, 37–38 knowledge sharing, 42–43\n\nleadership, 44 learning culture, 44–46\n\norganizational cultures, 39, 39f organizational learning, 38–40",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "resilience, 43–44\n\nsafety culture, 38–40 workarounds, 40\n\nworkplace safety, 41–42\n\nThe Three Ways\n\nThe First Way, 11 illustrated, 12f increasing workflow, 12\n\nThe Phoenix Project, 11 The Second Way, 12\n\nThe Third Way, 12–13\n\nTheory of Constraints, 356–357 Three Mile Island, 28\n\nthreshold-based alerting tools, 199n Ticketmaster, 84–85\n\nTimberland, 67 Tischler, Tim, 159 Tomayko, Ryan, 261–262\n\nTotal Productive Maintenance, 4, 353 Toyota Kata\n\ndescription of, 6 functional-oriented organizations, 84\n\nand the improvement of daily work, 40 and learning culture, 45 movement, 355\n\nToyota Production System\n\nAndon cord, 138\n\nchange approval processes, 253 improvement blitz, 299 information radiator, 206\n\nand the Lean Movement, 4, 353",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "and learning culture, 45\n\nand safe systems, 28 Toyota Kata movement, 6\n\ntransparent uptime, 365\n\nTreynor, Ben, 237–238 Trimble, Chris, 66\n\nTurnbull, James, 198 Twitter, 320–323 two-pizza team, 90–91\n\nU US Navy, 42 user research, 244–245 user stories, 8, 295, 335, 336\n\nV value stream\n\nDevelopment, 63\n\nInfosec, 63 manufacturing, 7–8 Operations, 63\n\nproduct owner, 63 release managers, 63\n\nsupporting members, 63 technology, 8–11 technology executives, 63\n\ntest, 63 value stream manager, 63\n\nvalue stream mapping, 4, 61–62, 353\n\nvalue stream mapping\n\n%C/A, 65",
      "content_length": 558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "areas of focus, 64 creating, 63–66\n\nexample of, 65f first pass, 65\n\nfuture value stream map, 66 and the Lean Movement, 4, 353 metrics for improvement, 65–66\n\nvalue stream improvements, 61–62\n\nvalue streams\n\nexpanding DevOps, 58–59 greenfield vs brownfield services, 54–56 leveraging innovators, 57–58\n\nselecting a stream for DevOps transformation, 51–60 systems of engagement, 56–57\n\nsystems of record, 56 technology adoption curve, 58f\n\nVan Leeuwen, Evelijn, 305n Vance, Ashlee, 72 Velocity Movement, 354\n\nversion control\n\nassets to check into version control repository, 116\n\nautomated testing, 128 branching, 143n and continuous integration, 148–149\n\ncritical role of version control, 117 environments stored in version control, 115–116\n\nmetadata, 115 shared version control repository, 115–118 version control as predictor of organizational performance, 117\n\nversion control systems, 115–118\n\nVincent, John, 216",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "Vogels, Werner, 91, 184\n\nW Wall Street Journal, 66 waste and hardship\n\ndefects, 25 extra features, 24 extra processes, 24\n\nheroics, 25, 25n motion, 24–25\n\nnonstandard or manual work, 25 partially done work, 24 task switching, 24\n\nwaiting, 24 waste elimination, 23–25\n\nwater-Scrum-fall anti-pattern, 140n Westrum, Ron, 39–40\n\nWickett, James, 313 Williams, Laurie, 135n, 260 Willis, John\n\nAgile infrastructure, 5 convergence of DevOps, 3\n\nWIP. See work in process Wolaberg, Kirsten, 58n Womack, James P.\n\nbatch sizes, 19 leaders and problem solving, 44\n\nWong, Eric, 208 work in process\n\ncontrolling queue size, 18\n\ninterruptions, 17",
      "content_length": 630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "multitasking, 17–18\n\nwork visibility, 15–17, 73, 104 workflow\n\nincreasing, 12, 15 management, 16 visualizations, 16–17\n\nY Yahoo! Answers, 246–248\n\nZ Zenoss, 198, 208 Zhao, Haiping, 302\n\nZuckerberg, Mark, 302",
      "content_length": 207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "Acknowledgments\n\nJez Humble\n\nCreating this book has been a labor of love for Gene in particular.\n\nIt’s an immense privilege and pleasure to have worked with Gene\n\nand my other co-authors, John and Pat, along with Todd, Anna,\n\nRobyn and the editorial and production team at IT Revolution\n\npreparing this work—thank you. I also want to thank Nicole\n\nForsgren whose work with Gene, Alanna Brown, Nigel Kersten and I on the PuppetLabs/DORA State of DevOps Report over the\n\nlast three years has been instrumental in developing, testing and refining many of the ideas in this book. My wife, Rani, and my two\n\ndaughters, Amrita and Reshmi, have given me boundless love and\n\nsupport during my work on this book, as in every part of my life. Thank you. I love you. Finally, I feel incredibly lucky to be part of\n\nthe DevOps community, which almost without exception walks the talk of practicing empathy and growing a culture of respect and\n\nlearning. Thanks to each and every one of you.\n\nJohn Willis\n\nFirst and foremost, I need to acknowledge my saint of a wife for putting up with my crazy career. It would take another book to\n\nexpress how much I learned from my co-authors Patrick, Gene and Jez. Other very important influencers and advisers in my journey are Mark Hinkle, Mark Burgess, Andrew Clay Shafer, and",
      "content_length": 1305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "Michael Cote. I also want to give a shout out to Adam Jacob for\n\nhiring me at Chef and giving me the freedom to explore, in the\n\nearly days, this thing we call Devops. Last but definitely not least\n\nis my partner in crime, my Devops Cafe cohost, Damon Edwards.\n\nPatrick Debois\n\nI would like to thank those who were on this ride, much gratitude\n\nto you all.\n\nGene Kim\n\nI cannot thank Margueritte, my loving wife of nearly eleven\n\namazing years, enough for putting up with me being in deadline mode for over five years, as well as my sons, Reid, Parker, and\n\nGrant. And of course, my parents, Ben and Gail Kim, for helping me become a nerd early in life. I also want to thank my fellow co- authors for everything that I learned from them, as well as Anna Noak, Aly Hoffman, Robyn Crummer-Olsen, Todd Sattersten, and the rest of the IT Revolution team for shepherding this book to its\n\ncompletion.\n\nI am so grateful for all the people who taught me so many things, which form the foundation of this book: John Allspaw (Etsy),\n\nAlanna Brown (Puppet), Adrian Cockcroft (Battery Ventures), Justin Collins (Brakeman Pro), Josh Corman (Atlantic Council), Jason Cox (The Walt Disney Company), Dominica DeGrandis (LeanKit), Damon Edwards (DTO Solutions), Dr. Nicole Forsgren (Chef), Gary Gruver, Sam Guckenheimer (Microsoft), Elisabeth\n\nHendrickson (Pivotal Software), Nick Galbreath (Signal Sciences), Tom Limoncelli (Stack Exchange), Chris Little, Ryan Martens, Ernest Mueller (AlienVault), Mike Orzen, Scott Prugh (CSG",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "International), Roy Rapoport (Netflix), Tarun Reddy (CA/Rally),\n\nJesse Robbins (Orion Labs), Ben Rockwood (Chef), Andrew Shafer\n\n(Pivotal), Randy Shoup (Stitch Fix), James Turnbull (Kickstarter),\n\nand James Wickett (Signal Sciences).\n\nI also want to thank the many people whose incredible DevOps\n\njourneys we studied, including Justin Arbuckle, David Ashman,\n\nCharlie Betz, Mike Bland, Dr. Toufic Boubez, Em Campbell-Pretty,\n\nJason Chan, Pete Cheslock, Ross Clanton, Jonathan Claudius,\n\nShawn Davenport, James DeLuccia, Rob England, John Esser,\n\nJames Fryman, Paul Farrall, Nathen Harvey, Mirco Hering, Adam\n\nJacob, Luke Kanies, Kaimar Karu, Nigel Kersten, Courtney Kissler, Bethany Macri, Simon Morris, Ian Malpass, Dianne Marsh, Norman Marks, Bill Massie, Neil Matatall, Michael\n\nNygard, Patrick McDonnell, Eran Messeri, Heather Mickman, Jody Mulkey, Paul Muller, Jesse Newland, Dan North, Dr. Tapabrata Pal, Michael Rembetsy, Mike Rother, Paul Stack, Gareth Rushgrove, Mark Schwartz, Nathan Shimek, Bill Shinn, JP Schneider, Dr. Steven Spear, Laurence Sweeney, Jim Stoneham, and Ryan Tomayko.\n\nAnd I am so profoundly grateful for the many reviewers who gave us fantastic feedback that shaped this book: Will Albenzi, JT\n\nArmstrong, Paul Auclair, Ed Bellis, Daniel Blander, Matt Brender, Alanna Brown, Branden Burton, Ross Clanton, Adrian Cockcroft, Jennifer Davis, Jessica DeVita, Stephen Feldman, Martin Fisher, Stephen Fishman, Jeff Gallimore, Becky Hartman, Matt Hatch, William Hertling, Rob Hirschfeld, Tim Hunter, Stein Inge Morisbak, Mark Klein, Alan Kraft, Bridget Kromhaut, Chris Leavory, Chris Leavoy, Jenny Madorsky, Dave Mangot, Chris\n\nMcDevitt, Chris McEniry, Mike McGarr, Thomas McGonagle, Sam",
      "content_length": 1709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "McLeod, Byron Miller, David Mortman, Chivas Nambiar, Charles Nelles, John Osborne, Matt O’Keefe, Manuel Pais, Gary Pedretti,\n\nDan Piessens, Brian Prince, Dennis Ravenelle, Pete Reid, Markos Rendell, Trevor Roberts, Jr., Frederick Scholl, Matthew\n\nSelheimer, David Severski, Samir Shah, Paul Stack, Scott\n\nStockton, Dave Tempero, Todd Varland, Jeremy Voorhis, and Branden Williams.\n\nAnd several people gave me an amazing glimpse of what the future\n\nof authoring with modern toolchains looks like, including Andrew Odewahn (O’Reilly Media) who let us use the fantastic Chimera\n\nreviewing platform, James Turnbull (Kickstarter) for his help creating my first publishing rendering toolchain, and Scott Chacon\n\n(GitHub) for his work on GitHub Flow for authors.",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "GENE KIM\n\nJEZ HUMBLE\n\nPATRICK DEBOIS\n\nJOHN WILLIS\n\nAuthor Biographies\n\nGene Kim is a multiple award-winning CTO, researcher, and author of The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win and The Visible Ops Handbook. He is founder of IT Revolution and hosts the DevOps Enteprise Summit conferences.\n\nJez Humble is co-author of Lean Enterprise and the Jolt Award-winning Continuous Delivery. He works at 18F, teaches at UC Berkeley, and is CTO and co- founder of DevOps Research and Assessment, LLC.\n\nPatrick Debois is an independent IT consultant who is bridging the gap between projects and operations by using Agile techniques, in development, project management, and system administration.\n\nJohn Willis has worked in the IT management industry for more than thirty-five years. He has authored six IBM Redbooks and was the founder and chief architect at Chain Bridge Systems. Currently he is an Evangelist at Docker, Inc.",
      "content_length": 952,
      "extraction_method": "Unstructured"
    }
  ]
}