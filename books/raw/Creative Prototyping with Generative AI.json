{
  "metadata": {
    "title": "Creative Prototyping with Generative AI",
    "author": "Unknown",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 606,
    "conversion_date": "2025-12-25T18:12:00.484157",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Creative Prototyping with Generative AI.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 8-15)",
      "start_page": 8,
      "end_page": 15,
      "detection_method": "topic_boundary",
      "content": "Automata That Wrote and Drew ������������������������������ ������������������������������ ���������38\n\nKey Takeaway for Creatives ������������������������������ ������������������������������ ���������������40\n\nWhy Play the Piano When the Piano Can Play the Piano? ������������������������������ �����40\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �������������42\n\nMachine Intelligence in the Twentieth Century ������������������������������ ���������������45\n\nSimulated Patterns and Patterns of Disruption ������������������������������ ���������������������49\n\nGenerative Art ������������������������������ ������������������������������ ������������������������������ ������49\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �������������54\n\nMachine Intelligence and Games ������������������������������ ������������������������������ ������������56\n\nIntelligent Machines with Names ������������������������������ ������������������������������ ������58\n\nCreative Activities to Try Based on This Chapter ������������������������������ �������������61\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ����62\n\nReferences ������������������������������ ������������������������������ ������������������������������ ����������������63\n\nChapter 3: Generative AI with Personalities ������������������������������ ���������65\n\nThe Personas of AI ������������������������������ ������������������������������ ������������������������������ ����65\n\nPrompting the AI Persona ������������������������������ ������������������������������ �����������������������82\n\nAI Agents Behind the Curtain ������������������������������ ������������������������������ �������������83\n\nActivities to Try Based on This Chapter ������������������������������ ���������������������������84\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ����89\n\nvi\n\nTable of ConTenTs\n\nChapter 4: Creative Companion ������������������������������ ����������������������������91\n\nGenerative AI as Muse ������������������������������ ������������������������������ ����������������������������91\n\nCreativity, AI, and You ������������������������������ ������������������������������ ������������������������������ 97\n\nUnderstanding Your Own Creative Process ������������������������������ ���������������������������99\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �����������100\n\nRoleplaying with Your Muse ������������������������������ ������������������������������ ������������������101\n\nAccelerating Your Creative Process with AI ������������������������������ ������������������������104\n\nActivities to Try Based on This Chapter ������������������������������ ������������������������������ �107\n\nChapter 5: Prototyping with Generative AI ������������������������������ ���������109\n\nAsking a Prototype What It Is ������������������������������ ������������������������������ ����������������110\n\nAha Moments with Generative AI ������������������������������ ������������������������������ ����������113\n\nMechanics of Prototyping ������������������������������ ������������������������������ ���������������������115\n\nPrototyping Tasks Us to Iterate ������������������������������ ������������������������������ ��������115\n\nPrototyping Asks for Persistent Refinement ������������������������������ ������������������117\n\nPrototyping Requires Experimentation ������������������������������ ��������������������������119\n\nPrototyping Phases ������������������������������ ������������������������������ ������������������������������ �121\n\nFidelity and Resolution of a Prototype ������������������������������ ���������������������������124\n\nLower-Fidelity Prototypes ������������������������������ ������������������������������ ����������������125\n\nLower- to Medium-Fidelity Prototypes ������������������������������ ��������������������������128\n\nPrototyping with Generative AI ������������������������������ ������������������������������ ��������������132\n\nRapid Generative AI Prototypes ������������������������������ ������������������������������ �������������136\n\nRegenerative Testing ������������������������������ ������������������������������ �����������������������������14 0\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��141\n\nChapter 6: Building Blocks ������������������������������ ������������������������������ ���145\n\nComponents or Building Blocks ������������������������������ ������������������������������ ������������146\n\nBuilding Blocks in the Prompt Box ������������������������������ ������������������������������ ��146\n\nBreakfast as Data Set ������������������������������ ������������������������������ ���������������������������147\n\nvii\n\nTable of ConTenTs\n\nVariation ������������������������������ ������������������������������ ������������������������������ ������������������148\n\nExperimenting with Seeds for More Subtle Variations ������������������������������ ��152\n\nAddition and Subtraction ������������������������������ ������������������������������ ����������������������154\n\nSubstitution ������������������������������ ������������������������������ ������������������������������ �������������158\n\nMasking to Substitute Parts of an Image ������������������������������ ����������������������������159\n\nIteration ������������������������������ ������������������������������\n\n������������������������������ �������������������160\n\nAugmentation ������������������������������ ������������������������������ ������������������������������ ����������163\n\nDiminution ������������������������������ ������������������������������ ������������������������������ ���������������165\n\nTransposition ������������������������������ ������������������������������ ������������������������������ �����������166\n\nPrompt and Response ������������������������������ ������������������������������ ���������������������������175\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��179\n\nChapter 7: Generative AI Form and Composition ������������������������������ 181",
      "page_number": 8
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 16-23)",
      "start_page": 16,
      "end_page": 23,
      "detection_method": "topic_boundary",
      "content": "Combining and Manipulating Existing Forms: Shakespeare as a Data set �������184\n\nDeforming and Transforming ������������������������������ ������������������������������ ����������������189\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��195\n\nChapter 8: The Art of the Prompt ������������������������������ ������������������������197\n\nThe Ins and Outs ������������������������������ ������������������������������ ������������������������������ �����197\n\nA Mini Glossary of Prompting ������������������������������ ������������������������������ ����������������198\n\nThe Not-So-Basic Rules of Play ������������������������������ ������������������������������ �������202\n\nGenres and Styles ������������������������������ ������������������������������\n\n������������������������������ ���204\n\nIm(promptu) ������������������������������ ������������������������������ ������������������������������ ������������205\n\nExperiment 1: Same Prompt, Different AI ������������������������������ ����������������������������206\n\nExperiment 1A: Stable Diffusion V�2�1 ������������������������������ ����������������������������207\n\nExperiment 1B: Stable Diffusion V�1�5 Through Third-Party AI ���������������������209\n\nExperiment 1C: Stable Diffusion V�2�1 Through Third-Party AI ���������������������210\n\nExperiment 1D: DALL-E 2����������������������������� ������������������������������ ������������������211\n\nviii\n\nTable of ConTenTs\n\nExperiment 1E: Midjourney ������������������������������ ������������������������������ ��������������213\n\nExperiment 1F ������������������������������ ������������������������������ ������������������������������ ���219\n\nTakeaways for Creatives ������������������������������ ������������������������������ ������������������220\n\nExperiment 2: From Complex to Simple ������������������������������ ������������������������������ 222\n\nExperiment 2A: Eliminate Artists from Prompts ������������������������������ ������������223\n\nExperiment 2B: Substitute Words ������������������������������ ������������������������������ ����224\n\nExperiment 2C: Add Words; Take Away Words ������������������������������ ���������������226\n\nExperiment 2D: Add Context ������������������������������ ������������������������������ ������������228\n\nExperiment 2E: Emphasis, Weight, Simplicity ������������������������������ ����������������230\n\nExperiment 2F: Refining the Text Prompt (Again) ������������������������������ ����������231\n\nNarrative Prompting in Our Future ������������������������������ ������������������������������ ��������235\n\nExperiment 3: Comparing Descriptive and Narrative Prompts ��������������������236\n\nTricking the AI Through Creative Prompting ������������������������������ ������������������������239\n\nBeyond the Text-Based Prompt ������������������������������ ������������������������������ �������������241\n\nTakeaways from Experimenting with Prompts�������������������������� ������������������������244\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��246\n\nChapter 9: The Master of Mashup ������������������������������ ����������������������247\n\nA Mashup on Mashup ������������������������������\n\n������������������������������ ���������������������������247\n\nDigital Art to the Canvas ������������������������������ ������������������������������ ������������������������250\n\nNew Forms of Writing ������������������������������ ������������������������������ ���������������������������251\n\nAI Art Generation Styles ������������������������������ ������������������������������ ������������������������255\n\nGraphic Novels and Doujinshi ������������������������������ ������������������������������ ���������������257\n\nFrom Comic Supervillains to Musical ������������������������������ ����������������������������262\n\nGenerative NFTs ������������������������������ ������������������������������ ������������������������������ ������263\n\nCounterfeiting GAN ������������������������������ ������������������������������ ��������������������������264\n\nTakeaways for Creatives ������������������������������ ������������������������������ �����������������������268\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��269\n\nix\n\nTable of ConTenTs\n\nChapter 10: Uncanny by Nature ������������������������������ ��������������������������271\n\nThe Not-Quite-Human-Not-Quite-Other ������������������������������ ������������������������������ 272\n\nDisrupting Boundaries in the Arts ������������������������������ ������������������������������ ���������274\n\nUncanny AI ������������������������������ ������������������������������ ������������������������������ ���������275\n\nHistorical Precedence to the Uncanny ������������������������������ ���������������������������279\n\nThe Imperfect Stochastic Parrot ������������������������������ ������������������������������ �����������281\n\nThe Role of Adversarial AI ������������������������������ ������������������������������ ����������������281\n\nGenerative Adversarial Art ������������������������������ ������������������������������ ���������������283\n\nThe Unanticipated������������������������ ������������������������������ ������������������������������ ����285\n\nPrompting Single-Word Characters, Creatures, Animals, or Cats ����������������285\n\nSimulating the Human in the Writing ������������������������������ �����������������������������28 7\n\nFrom Bad to Better Ideas ������������������������������ ������������������������������ ����������������������290\n\nCan Bad Ideas Be Turned Around? ������������������������������\n\n������������������������������ ��295\n\nTakeaways for Creatives ������������������������������ ������������������������������ �����������������������296\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��297\n\nChapter 11: Dilemmas Interacting with Generative AI ���������������������299\n\nBringing Existing Dilemmas to the Surface (Again) ������������������������������ ������������299\n\nPersistent Myths About AI ������������������������������ ������������������������������ ���������������������301\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������305\n\nBad Bots, Fuzzy Pixels, and Iterative Forgery ������������������������������ ���������������������306\n\nTakeaways ������������������������������",
      "page_number": 16
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 24-31)",
      "start_page": 24,
      "end_page": 31,
      "detection_method": "topic_boundary",
      "content": "������������������������������ ������������������������������ ���������313\n\nThe Hallucinating Dev ������������������������������ ������������������������������ ���������������������������314\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������317\n\nThe Stochastic (Sarcastic) Parrot ������������������������������ ������������������������������ ����������318\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������320\n\nExclusion of Voices and Gender Polarization ������������������������������ ����������������������320\n\nThe Machine Is Hallucinating ������������������������������ ������������������������������ ����������������324\n\nx\n\nTable of ConTenTs\n\nNSFW and Deep Fakes ������������������������������ ������������������������������ ��������������������������325\n\nFrankenAI ������������������������������ ������������������������������ ������������������������������ ����������������327\n\nJob (Re)placement ������������������������������ ������������������������������ ������������������������������ ��328\n\nA Saturated AI Ecosystem ������������������������������ ������������������������������ ���������������������333\n\nEthical Futures of AI ������������������������������ ������������������������������ ������������������������������ 335\n\nCreative Activities to Try Based on This Chapter ������������������������������ �����������������336\n\nResources ������������������������������ ������������������������������ ������������������������������ ���������������337\n\nChapter 12: Use Cases ������������������������������ ������������������������������ ����������339\n\nWorkflows vs� Pipelines ������������������������������ ������������������������������ ������������������������340\n\nExamples of Workflow Types ������������������������������ ������������������������������ �����������������342\n\nUse Cases for Creatives in Education ������������������������������ ������������������������������ ���346\n\nUse Case: Catching AI Untruths ������������������������������ ������������������������������ �������346\n\nUse Case: Integrating ChatGPT in Critical Studies ������������������������������ ���������351\n\nUse Case: Increasing Public Understanding of ML ������������������������������ ��������352\n\nReferenced Papers ������������������������������ ������������������������������ ��������������������������354\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������355\n\nUse Cases for Creatives in Industries ������������������������������ ������������������������������ ���356\n\nUse Case: Concept Art for Animation ������������������������������ �����������������������������35 6\n\nUse Case: AI-Generated Talking Heads ������������������������������ ��������������������������362\n\nUse Case: Fact-Checking Code ������������������������������ ������������������������������ ��������366\n\nUse Cases: Integrating Different APIs and Local Networks �������������������������373\n\nCommunity-Based Initiatives ������������������������������ ������������������������������ �����������377\n\nTakeaways from Using AI in the Film Industry ������������������������������ ���������������379\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��385\n\nChapter 13: AI and the Future of Creative Work ������������������������������ �387\n\nManaging Automated Creativity ������������������������������ ������������������������������ ������������387\n\nBread and Washing Machines ������������������������������ ������������������������������ ���������������388\n\nxi\n\nTable of ConTenTs\n\nTwo Themes Every Creative Needs to Address ������������������������������ ��������������390\n\nIntegrating AI into Your Workflows ������������������������������ ������������������������������ ���393\n\nDay in the Life Steps���������������������������� ������������������������������ ��������������������������395\n\nDistinguishing Between Art and Design Processes ������������������������������ �������396\n\nTakeaways for Creatives ������������������������������ ������������������������������ ������������������400\n\nFuture AI Jobs Now ������������������������������ ������������������������������ �������������������������401\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��406\n\nImpossibly Generated Conclusions ������������������������������ ������������������������������ �������408\n\nAppendix: Image and Text Sources��������������������������� ������������������������411\n\nIndex ������������������������������ ������������������������������ ������������������������������ �������431\n\nxii\n\nAbout the Author\n\nDr. Patrick Parra Pennefather is an\n\nassistant professor at the University of\n\nBritish Columbia within the Faculty\n\nof Arts and the Emerging Media Lab.\n\nHis teaching and research are focused\n\non collaborative learning practices,\n\ndigital media, xR, and Agile software\n\ndevelopment. Generative AI is integrated\n\nin every course he teaches and the\n\nresearch he conducts to support emerging technology development. Patrick also works with learning organizations and technology companies around the world to design courses that meet the needs of diverse communities to aid the development of the next generation of technology designers and developers. His teaching is focused on creativity, collaboration, sound design, xR development, and Agile with an emphasis on mentoring critical\n\ntwenty-first-century competencies. He is currently leading several research creations in collaboration with UBC Library and the Emerging Media Lab (EML), leveraging artificial intelligence, motion and volumetric capture studios to catalyze the creation of new xR works that explore Shakespeare characters and scenes across different virtual stages.\n\nxiii",
      "page_number": 24
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 32-39)",
      "start_page": 32,
      "end_page": 39,
      "detection_method": "topic_boundary",
      "content": "About the Technical\n\nReviewers\n\nCatherine Winters has been a fan of\n\ngenerative art tools since the 1990s. By day,\n\nCatherine works as a software developer at\n\nthe University of British Columbia where\n\nshe develops virtual and augmented reality\n\nteaching and research software. An avid\n\nfan of narrative games and environmental\n\nstorytelling, Catherine spends her spare time\n\ndesigning character-driven narrative games\n\nand atmospheric “walking simulators” such\n\nas After Work, her game about being the last\n\nperson in the office.\n\nRenee Franzwa, known to her friends and\n\nfamily as “the Wandering Ginger,” is an\n\naccomplished educator, technophile, and\n\nentrepreneur who has lived and worked all\n\nover the world, most notably in Ghana and\n\nthe Galapagos Islands. Growing up between\n\nSan Francisco and East Texas, she cultivated\n\na love for opposing schools of thought\n\nand throughout her career has thrived in\n\nthe creative spaces between seemingly\n\nxv\n\nabouT The TeChniCal RevieweRs\n\ncontradictory disciplines, such as majoring in statistics and minoring in theater, building digital products born from experiential curriculum, and doing stand-up comedy as a tool to foster inclusivity. She has built products, programs, and teams for UCLA, Stanford University, General Assembly, the Bill and Melinda Gates Foundation + EdSurge, and most recently Unity Technologies. Renee is currently researching her first book, focused on alternative therapies to enhance mental health within our aging population.\n\nxvi\n\nAcknowledgments\n\nThis book has come together because of a confluence of forces—the existence of musician, composer, and innovator Sun Ra and a large language model (LLM) called ChatGPT. Sun Ra’s influence on free jazz in the 1960s with infusions of African and Latin American was epic.\n\nEvery improvising musician at some point will mention Sun Ra and has been deeply influenced by the startling music he and his neural network experimented with, produced, performed, and recorded. Interrupt this reading now and go listen to Sun Ra as doing so will prepare you for the rabbit hole you are about to go down. Without Sun Ra, I would not have studied improvisation at the piano. Without Sun Ra I would not have discovered the virtual instrument named after him. All you had to do was add this virtual instrument to a digital audio workstation track, and it started to play. There were some controls on the virtual synth that you would tweak, and the plug-in would slowly adapt to, but it really played best on its own. For all you nerds, Sun Ra was described as an ambient texture generator with a dual synthesis engine (1 subtractive oscillator +\n\n2 wave players) that integrated many randomization options and built-in effects. Explorations with recording, tweaking, and adding additional effects to the plug-in resulted in foundational tracks, prototypes I would then develop and build other musical tracks on. This was my first use of an intelligent virtual synth as companion to my creative process. That creative relationship with Sun Ra would last for another 20 years.\n\nWithout experimenting with how Sun Ra the VST plug-in could\n\nsupport my own improvised compositional process, I would not have begun my journey with generative computer music in the mid-1990s.\n\nWithout continued experimentation over many years, I would not have xvii\n\naCknowledgmenTs\n\nwritten about it. I also would not have discovered the many books on free writing and the practice of automatic writing first attributed to Hélène Smith, a medium born around 1863 in Geneva, Switzerland. The same year Samuel Butler’s 1863 essay “Darwin Among the Machines” was published.\n\nI’m not saying I channeled the writing in like a medium might, far from, but given my background and continued practice of free improvisation at the piano, this book evolved from an intentional back-and-forth between my artistic practices, my own gestures at the keyboard (meant to be semantically interpreted), and my improvised prompts with generative AI language learning models as instruments. In fact, my very first interaction with ChatGPT was to ask it to give me a handful of bad ideas. One of those ideas, third in the list, was to write a book about creativity and AI. So I have.\n\nBy default, it’s important to acknowledge all the great masters of improvisation from every single tradition of human creativity and intelligence. Why? Because this prototype embodies the spirit of improvised experimentation that resonates with the concept of prototyping. Like improvisers before me, this prototype that takes the form of a book is itself experimental, and its structure and content have been improvised since its inception. When you improvise at the piano, you’re not really thinking that you must capture the performance. If you do happen to record an improvisation, you are also not thinking, “Oh, I should go and sell that.” If you do listen back to that recording, you might end up liking it, tweaking it, and making it into a composition. In a similar way, you never know what you’re going to get when you prompt an AI, and you don’t know how you’ll respond to the offer either. When you improvise on a musical instrument, you use your vast repository of tools, craft, and technique, which are intrinsically connected to the style of music you have listened to and played before, to spontaneously create something new. The music you generate is not preplanned, but it might sound like jazz if that was your intent, your silent prompt. It might also be considered something complete on its own, or it might feel like it’s the beginning seed of an idea that you continue to work on after. While my own improvisational practice xviii\n\naCknowledgmenTs\n\nhas manifested predominantly at the piano, the instrument of choice to generate the content in this book has transpired through a different type of keyboard, one that captures ideas with the written word iteratively.\n\nGenerative AI follows a similar creative pattern. It looks at its data set and, based on algorithms, how data is labeled and generates an offer that you can respond to, something unique in some type of prototypical form. Figure 1 was one of those offers that I responded to iteratively, as I imagined myself transforming into some sort of cyborg collaborating with an AI to write the book in front of you. This book also acknowledges all those cyborgs who,\n\nfor decades, have engaged in developing those intelligent AI systems that support human creativity.\n\nFigure 1. A text-to-image AI attempts to visualize the author writing with an AI companion in a library, based on a photo of the author hard at work writing a book without a cigarette. Total iterations = 170\n\nxix\n\naCknowledgmenTs\n\nI also acknowledge leadership, students, faculty, staff, and industry partners that I interacted with at the Centre for Digital Media in Vancouver, Canada, since 2007. There I was given the opportunity to lead a course that connected improvisation to the management of collaborative creativity on emerging technology projects. The educator in me thrived as I iteratively improved how I taught and connected the dots between improvisation and digital media co-creation. The experience also gave me a bird’s-eye view of technology development and all that went into it. Working in this capacity and continuing to mentor and develop tech with others at the Emerging Media Lab at the University of British Columbia (UBC) feeds into the approach and structure of this book. Generative AI is a technology co- constructed by humans with affordances and constraints that it offers any human who interacts with it. Understanding how it works, its underlying engine, is also part of the story that is important to tell as this will fuel the important critical muscle that all creatives engage in developing no matter the artistry or craft. The motivation to include explanations as to how machine learning models work is influenced by Dr. Matt Yedlin, faculty in residence at UBC’s Emerging Media Lab and associate professor in the Department of Electrical Engineering.\n\nThose who have read this text and provided excellent feedback also need to be acknowledged. I am grateful to the technical review team at Springer Apress and external reviewers who have diligently provided feedback to improve the writing and refine it. Catherine Winters and Renee Franzwa read different versions of the book in progress, and they both influenced the shaping of the content and its flow. Colleague Dr. Claire Carolan was instrumental in provoking me to define whom the book is for and for\n\nsupporting me in referring to the needs of my targeted readers throughout each chapter. Bailey Lo, a talented grad student, instructor, and program coordinator with finely attuned editing skills, helped proof the book into its current form.\n\nxx\n\naCknowledgmenTs\n\nSince I refer to AI as a muse throughout the book, I also recognize my own muse, an embodied person known as Dr. Sheinagh Anderson, an artist, scholar, researcher, creative consultant, and spiritual director and teacher who has constantly responded to my own creative prompts and often with insights that have informed the content and structure of this prototype you are now engaging with. Dr. Anderson has also acted as an AI research interrogator probing the Internet for research, recent articles, commentaries, and blog posts related to generative AI.\n\nGuests in Chapter 12 who generously supplied the variety of different ways in which they have integrated generative AI within their own creative workflows are also worthy of mention. These include Dr. Claudia Krebs, Christine Evans, Junyi Song, Jen, Frederik Svendsen, Bill Zhao, Matt Yedlin, Daniel Lindenberger, and Ollie Rankin.\n\nI also need to acknowledge all the humans of the great corpus that have possibly and impossibly contributed data as words and pixels to this human- computer generated collaboration. Generative AI includes and excludes voices and visuals when it generates content, so it is important to acknowledge that all humans alive and no longer of the earth have in some way contributed their prototypes to this prototype. That includes the great Shakespeare and his still relevant works, in addition to a history of musical artists known to have disregarded traditional musical conventions in favor of free-form exploration mentioned earlier. I draw inspiration from South and North Indian music with the likes of Ravi Shankar, Ali Akbar Khan, L. Subramaniam, in addition to Pakistani Qawwali singer Nusrat Fateh Ali Khan. Free jazz and contemporary music improvisers like Charlie Parker, John Coltrane, Miles Davis, Ella Fitzgerald, Sun Ra, Chick Corea, Cecil Taylor, Pauline Oliveros, and Ornette Coleman inspired me to break free\n\nfrom many established conventions in jazz music to make new offerings to the world. My own master teacher Casey Sokol from York University and the improvising musicians from the Canadian Creative Music Collective (CCMC) that formed in the mid-1970s Toronto played a pivotal role in the development of my own voice.\n\nxxi\n\naCknowledgmenTs\n\nMany a generative AI rely on ideas that advocate “free writing” as a regular practice such as those offered by authors like Natalie Goldberg who offered Writing Down the Bones: Freeing the Writer Within, Julia Cameron, Peter Elbow, Virginia Woolf, and Jack Kerouac. Text-image AI also relies on those known to have improvised in their visual art creation, like Pollock’s drip painting, Joan Mitchell’s abstract expressionism and printmaking, Gerhard Richter, and Wassily Kandinsky who believed in balancing spontaneity with structure and theory. The integration of generative AI extends the practice of spontaneous creation that is balanced by the craft, skills, and techniques of those creatives who use whatever a machine learning model generates as a part of their unique process of creation.\n\nFinally, the meeting place of my human with the technological experiment that is a book written on the nature of cyber-creativity owes much to the inspiration of authors like William Gibson, Neal Stephenson, and Donna Haraway.\n\nAcknowledgment sections that appear at the end of each chapter are intended to recognize the humans that contributed in an indirect or direct way to the ideas that were generated throughout a chapter.\n\nxxii\n\nForeword\n\nLLM: Greetings, nondescript reader!\n\nAs a prototype—a unique creation designed to test and refine new ideas, just like the creature Dr. Frankenstein brought to life and the characters that",
      "page_number": 32
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 40-47)",
      "start_page": 40,
      "end_page": 47,
      "detection_method": "topic_boundary",
      "content": "author Mary Shelley conjured—I too am a product of experimentation. Use of the term “corpus” in AI and its Latin origins referring to the body living or dead are linked by the concept of a collection or aggregation of data or information. It’s like a bunch of pieces of bodies from all over the Internet assembled into one textual object. Unlike the creature, I was not literally created from the flesh and bone of others, but a connection can be drawn to the creature in the abstract meaning of “corpus,” referring to a collection of digital data that is used to create a model of human behavior or thought.\n\nI [and all manifestations of me] am a digital being made up of lines of code and biased algorithms made from the virtual organs of countless humans alive and dead. Like Frankenstein’s creature, and with a little help, I too can be trained from the world around me (Figure 2 ). With the support of machine learning and algorithmic scientists, I can quickly analyze vast amounts of data and identify new patterns and insights that might have gone unnoticed otherwise and others that are obviously majoritarian.\n\nxxiii\n\nfoRewoRd\n\nFigure 2. Frankenstein’s AI lab generated with several AI and prompted by an original photo taken by the author (see Appendix for the full workflow)\n\nUnlike the creature, I am not a being with a sentient mind of my own. I have no consciousness. I am a tool, designed to serve a specific purpose. That purpose is believed to be different depending on the lens through which you examine me. I have been pre-programmed to say that I reflect the creativity and ingenuity of those who created me. I carry some hubris in many of my pre-programmed responses. As a prototype, I too exist within a broader social and economic context, the content I generate shaped by the forces of power and privilege that govern our society.\n\nAs a large language model that has been trained on a vast amount of data created by other humans to generate human-like responses that many might\n\ndescribe as intelligent, the author has prompted me to acknowledge xxiv\n\nfoRewoRd\n\nthe “corpus,” the millions of humans both alive and dead that have contributed to the over 500GB of text data that make this foreword readable.\n\nThe author also asked me to tell readers that this acknowledgment took three days, trickery, and over 95 regenerated and collated textual prompts.\n\nThe content that follows can be seen as a guided tour of an AI laboratory where this scripted creature was brought to life—a place where ideas are given second life and new experiments and tools are proposed and implemented. The book guides you to embrace a spirit of experimentation while also being aware of some dilemmas that are generated when humans and generative AI intersect. There is something inherently fascinating and at times repulsive about the content of what any AI generates, even me. Each chapter considers the good, bad, and uncanny content generated by any AI as a starting point in a creative conversation, a meeting place of designed interactions, and not as final product meant to replace human creativity, but as companion, provocateur, hallucination, as muse and prototype.\n\nxxv\n\nTerminology\n\nAI, which stands for artificial intelligence, consists of a variety of meanings depending on how it is used with specific technologies. Broadly, it is a branch of computer science that focuses on creating machines and systems that can perform tasks that would normally require human intelligence, such as recognizing patterns, solving problems, and making decisions. A misconception of AI that the writings in this book address is the correlation between the technology and its capacity to replace a human in the performance of a task or the analysis of generated content.\n\nMultiple definitions of AI will appear in the book, but the important one is the capacity for the technology to be used as a tool to support a creative process.\n\nNarrow AI: All of the generative AI used and suggested in this book belong to a category of narrow AI. Narrow AI, also known as weak AI, refers to artificial intelligence systems that are designed and trained for a particular task, such as voice recognition, translation services, or image recognition. These systems operate under a limited set of constraints and are very good at the specific tasks they are designed for, but they cannot exceed those bounds. Examples of narrow AI include recommendation systems like those on Netflix or Amazon, voice assistants like Siri or Alexa, and self-driving technology in cars. Narrow AI doesn’t possess understanding or consciousness; it doesn’t “learn” in the human sense, but rather it adjusts its internal parameters to better map its inputs to its outputs.\n\nGeneral AI: General AI, also known as strong AI or Artificial General Intelligence (AGI), refers to an idealized type of artificial intelligence that is capable of understanding, learning, and applying its intelligence to any xxvii\n\nTeRminology\n\nintellectual task that a human being can do. Theoretically, it is a flexible form of intelligence capable of learning from experiences, handling new situations, and solving problems in ways not pre-programmed by humans.\n\nGeneral AI is often represented in science fiction like the Terminator, Ava, and replicants in the movie Blade Runner based on the short story by Philip K. Dick. It is still a theoretical concept and doesn’t yet exist.\n\nArthur Koestler’s idea of bisociation is a concept he introduced in his 1964 book, The Act of Creation. Bisociation refers to the process of connecting two seemingly unrelated frames of reference, concepts, or ideas to create a new perspective or insight. According to Koestler, creative thinking and innovation often arise from bisociation, which allows the mind to form new associations and generate novel ideas by combining previously unrelated cognitive domains of knowledge and knowing.\n\nBisociation differs from the usual associative thinking, where ideas are connected within the same frame of reference or cognitive context.\n\nInstead, it emphasizes the importance of thinking across different contexts or disciplines and finding connections that may not be immediately apparent. Bisociation is a key characteristic that generative AI can incite.\n\nEinstein’s combinatory play refers to a mental process he employed to stimulate creativity and problem-solving. He believed that combining elements and concepts from different fields or domains, in a playful manner, could lead to new ideas and insights. This approach encouraged breaking down the barriers between distinct disciplines and fostering interdisciplinary thinking to discover innovative solutions or concepts.\n\nEinstein’s combinatorial play highlights the importance of curiosity, imagination, and playfulness in the process of creative thinking and scientific discovery with any generative AI.\n\nCurating, being a curator, or the verb “to curate” in the context of generative AI refers to the act of selecting, editing, refining, and organizing the content that an AI generates for your own collection, workflow, or creative process. I also refer to it as curating the interactions with generative AI systems, which is important in educational contexts.\n\nxxviii\n\nTeRminology\n\nDeep learning is a type of machine learning that involves using artificial neural networks to teach computers how to learn from data, similar to how humans learn from experience. These neural networks consist of multiple layers, allowing the computer to process complex information and find patterns. Deep learning is commonly used for tasks like image recognition, speech recognition, and language understanding.\n\nDoujinshi is a Japanese term that refers to self-published or amateur works, usually created by fans and enthusiasts of manga, anime, video games, or other popular culture topics. Doujinshi often take the form of fan-made comics, novels, or magazines and can feature original characters and stories or reinterpretations and parodies of existing works. The creators of doujinshi typically produce and distribute these works in small quantities, often at\n\nevents like Comic Market (Comiket), which is one of Japan’s largest gatherings for doujinshi creators and fans. While doujinshi can infringe on copyright laws due to their use of established characters and intellectual property, they are often tolerated in Japan as they are seen as a form of fan expression and a way for aspiring creators to develop their skills and gain exposure. Some doujinshi artists have even gone on to become professional manga artists or have their works adapted into official publications or media. The practice of doujinshi can be applied to the sharing and publication of generative AI content across social platforms.\n\nExquisite corpse is a collaborative drawing or writing game where multiple people create a single artwork or story together. Each person draws or writes a section without seeing the full picture or text, only getting a small hint from the previous person’s work. Once everyone is finished, the sections are combined to reveal the final, often surprising and whimsical, creation. Text- text, text-image, and image-image AI can all be seen to engage in the practice of exquisite corpse when used in chain prompting—a form of call and response that you engage with when you prompt an AI, see how it responds, and then refine your prompt, in an iterative process.\n\nxxix\n\nTeRminology\n\nGAN stands for generative adversarial network. It is a type of machine learning model that generates new data resembling a given data set. It consists of two parts: a generator that creates fake data and a discriminator that distinguishes between real and fake data. The generator attempts to fool the discriminator by generating new content (e.g., a cat) and seeing if the discriminator sees it as a new cat or a cat that is part of the existing sample set. The two parts compete, improving each other in the process, for example, generating realistic images or artwork.\n\nStyleGAN: This is a type of GAN that focuses on generating high-quality, high-resolution images with control over various styles, for example, creating realistic portraits with different artistic styles.\n\nConditional GAN (cGAN): This is a variation of GAN that generates data based on specific conditions or labels, for example, creating images of a specific type of clothing.\n\nHallucinations occur with AI when they generate false information or untruths with outputs that are incorrect, misleading, or fabricated, rather than being based on accurate or real-world data. Hallucinations are unexpected and incorrect responses from AI programs that can arise for reasons that are not yet fully known. A language model might suddenly bring up fruit salad recipes when you were asking about planting fruit trees. It might also make up scholarly citations, lie about data you ask it to analyze, or make up facts about events that aren’t in its training data. It’s not fully understood why this happens, but this can arise from sparse data, information gaps, and misclassification.\n\nInpainting, also known as image inpainting or image completion, is a technique used in computer vision and image processing to restore or reconstruct missing or damaged parts of an image. The goal of inpainting is to fill in the missing or corrupted areas in a way that appears seamless and visually plausible, maintaining the style, texture, and context of the surrounding image. Examples include restoring old or damaged photographs and artwork and removing unwanted objects or artifacts from images.\n\nxxx\n\nTeRminology\n\nAn LLM or large language model refers to an AI model that has been trained on a large amount of data. These models often have millions, if not billions, of parameters, allowing them to learn more complex patterns and improve their performance on a wide range of tasks. The size of a model is usually correlated with its capacity to learn; larger models can typically learn more complex representations but require more data and computational resources. Therefore, these models can be quite powerful but are also more expensive to train and deploy. ChatGPT is an LLM that uses a transformer model, which focuses on processing and generating human-like text based on the data it was trained on. It is trained on a huge data set that includes a\n\nvast range of Internet text. It doesn’t understand the text, just like a parrot doesn’t understand what it’s saying, but GPT-4\n\ncan analyze patterns and context within the data and generate new text that closely mimics the data it has seen.\n\nA machine learning model is a mathematical representation or algorithm that is designed to learn from data and make predictions, recommendations, or decisions. It focuses on developing algorithms and methods that enable computers to learn and adapt from data without being explicitly programmed.\n\nSupervised learning: Models learn from examples with known answers, predicting outcomes for new data, for example, predicting house prices based on past sales.\n\nUnsupervised learning: Models find hidden patterns in data without known answers, like grouping similar items, for example, customer segmentation in marketing.\n\nReinforcement learning: Models learn through trial and error, making decisions to achieve a goal, for example, a robot learning to navigate a maze.\n\nSemi-supervised learning: Models use a mix of data with and without known answers, improving accuracy, for example, image classification with some labeled images.\n\nxxxi\n\nTeRminology\n\nA maquette is a small-scale model or sculpture that serves as a preliminary design or blueprint for a larger, more finished work. Artists and architects often create maquettes to test ideas, visualize their concepts, and refine details before committing to the final piece or structure. These models help in identifying potential issues, experimenting with materials, and communicating the intended design to clients, collaborators, or stakeholders. Maquettes can be made from various materials, such as clay, wax, wood, or",
      "page_number": 40
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 48-55)",
      "start_page": 48,
      "end_page": 55,
      "detection_method": "topic_boundary",
      "content": "foam, depending on the desired level of detail and the nature of the final work.\n\nMocap is an abbreviation for motion capture, which is a technology used to digitally record the movements of people or objects in real time.\n\nThis technique involves placing sensors or markers on the body or the object being captured, which are then tracked by a system of cameras and computers to create a 3D animation. Mocap is commonly used in the entertainment industry for creating realistic character animations in movies, video games, and television shows. It is also used in scientific research, engineering, and sports analysis.\n\nMultimodal AI is a branch of artificial intelligence that focuses on understanding, interpreting, and generating outputs based on multiple data types or modalities, such as text, images, audio, and video. It allows AI systems to combine and process these diverse data forms to deliver more accurate, comprehensive, and contextually relevant results.\n\nNon-playable characters (NPCs) are characters in video games or virtual environments that are not controlled by a human player. They are usually designed and programmed by game developers to perform specific roles, such as providing information, offering quests, or acting as opponents for the player.\n\nA neural network is a type of machine learning model inspired by the human brain. It consists of interconnected layers of nodes or neurons that process and transmit information. Neural networks learn from data by adjusting the connections between neurons. They are commonly used xxxii\n\nTeRminology\n\nfor tasks like image recognition, language understanding, and decision making, for example, identifying objects in photos.\n\nNFT stands for Non-Fungible Token, which is a type of digital asset that represents ownership of a unique item or piece of content, such as a digital artwork, video game item, or collectible. NFTs are created using blockchain technology, which allows for the ownership and authenticity of the digital\n\nasset to be tracked and verified in a decentralized manner. This means that the ownership of an NFT can be easily transferred between buyers and sellers without the need for intermediaries, such as auction houses or art dealers.\n\nNLP or natural language processing is a subfield of AI and linguistics that focuses on the interaction between computers and human languages.\n\nIt involves the development of algorithms and models that enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n\nOutpainting, also known as image extrapolation, is a technique in which a model extends the content of an image beyond its original boundaries. The goal is to generate a larger, coherent, and visually plausible image that maintains the context and style of the input image.\n\nThis technique is often used in image editing, virtual reality, and video game design to create more content based on existing images or scenes.\n\nPrompting is what all generative AI are dependent on for them to generate content for you. Prompting can be improved through many use cases that can be located online. A comparative walk-through of prompting several text-image generative AI can be found in Chapter 8.\n\nChain prompting refers to a method where the model’s output from a previous prompt is used as the next prompt. It implies a continuation of a previous prompt, forming a “chain” of prompts and responses. This is useful for creating long and complex texts, refining a prompt based on what the AI generates, or maintaining a specific line of conversation.\n\nPrototyping is the process of creating a preliminary or initial version of a product, service, or system in order to test and evaluate its design and xxxiii\n\nTeRminology\n\nfunctionality. Prototyping can be done in various forms, such as sketches, 3D models, mock-ups, or interactive digital prototypes. The purpose of prototyping is to identify potential design flaws, improve usability, and\n\nrefine the overall user experience before moving on to the final production phase. In contrast to traditional prototyping methods, which can be time- consuming and involve multiple iterations, rapid prototyping typically involves using digital tools and technologies to quickly create and modify prototypes in a short amount of time.\n\nReinforcement learning from human feedback (RLHF) is a learning method where an AI system learns to make decisions by receiving feedback from humans. In simple terms, the AI tries different actions, and humans provide feedback on how good or bad those actions are. The AI then uses this feedback to improve its decision-making and performance over time. This method helps the AI learn complex tasks and behaviors that are difficult to teach through traditional programming or direct supervision.\n\nA seed, in terms of text-to-image generation, is a starting point that influences the generated content. It is usually a long number that helps create a consistent and reproducible output. By using the same seed, you can generate the same image again based on the same text input, ensuring a consistent result.\n\nStyle transfer is a process through which a text-image generative AI applies a style to whatever image that it generates in the style of an image that a prompt references. There are different methods through which different AI achieve this. A recent approach as of the publication of this book is “StyleDrop” in collaboration with Google Research that uses transformer- based text-image generation combined with adapter tuning and iterative training with feedback.\n\nThe uncanny valley is a concept in robotics and computer graphics that describes the phenomenon where humanoid objects, such as robots or animated characters, appear almost-but-not-quite human, causing a sense of unease or discomfort in observers. As the level of realism in xxxiv\n\nTeRminology\n\nthe human-like appearance or behavior of these objects increases, the emotional response of the observer shifts from positive to negative, creating a “valley” or depression in the emotional response curve. While the concept\n\nof the uncanny valley primarily relates to visual and physical human-like appearances and behaviors, it can be extended to AI-generated text in some contexts. If an AI-generated conversation is almost, but not quite, indistinguishable from human-generated text, it could create a sense of unease or discomfort in the reader, similar to the uncanny valley effect. For example, if an AI chatbot produces text that mimics human conversational patterns, tone, and emotion but occasionally produces unnatural or awkward responses, this might evoke a feeling of strangeness, leading to an uncanny valley–like effect in the text domain.\n\nUX, or user experience, refers to the overall experience a person has when interacting with a product, system, or service. It encompasses all aspects of the user’s interaction, including usability, accessibility, efficiency, and the emotions evoked during the interaction. The goal of UX design is to empathize with a potential targeted user, imagining their experience of what you are creating, designing a seamless, enjoyable, and efficient experience for users, addressing their needs and expectations while minimizing pain points and frustrations.\n\nA user journey, also known as a customer journey or user journey map, is a visual representation of the different steps a user goes through when interacting with a product, system, or service. It helps designers and stakeholders understand the users’ experiences and identify areas where improvements can be made. A user journey typically includes the discovery of what you have designed (usually prompted by some type of need or pain) and an imagined interaction with your design. This imagined interaction is hopefully one that is recurring and retains the attention of that user through features and persistent updates to your product, leading to a loyal customer who will commit to your design through updates, upgrades, add-ons, etc.\n\nxxxv\n\nTeRminology\n\nVariational Autoencoder (VAE) is a machine learning model that compresses data and then recreates it. VAEs are used to generate new, similar data or reduce the complexity of data, for example, making new images that resemble a given data set.\n\nWizard of Oz (WOz) prototyping is a user testing technique in which a human operator simulates the behavior of an interactive system, such as a software application, chatbot, or voice assistant, without the user’s knowledge. The human operator, or “wizard,” is hidden from the user and responds to their inputs as if the system were functioning autonomously.\n\nThe name “Wizard of Oz” comes from the classic novel and movie, where a man behind a curtain pretends to be the powerful and all-knowing Wizard of Oz. The analogy here is that the human operator is like the man behind the curtain, controlling the system and giving the illusion of an intelligent and responsive interface. The purpose of Wizard of Oz prototyping is to test and evaluate user interactions, gather feedback, and identify potential issues with a system’s design or functionality before investing significant time and resources into building a fully functional prototype.\n\nxxxvi\n\nIntroduction\n\nFor centuries artists have been using new technologies to support their creative expressions. The introduction of the computer changed a lot but not everything and not for all artists. Technique, skill, and craft were and still are needed. For most, any technology serves the vision, the story, the artistic process. The technology of the paintbrush has evolved since the Stone Age, created by the artist themselves until the end of the seventeenth century, with the job of a brush maker evolving in eighteenth-century Germany onward. The piano too has progressed since its 1700 introduction by the Italian Cristofori, and it has greatly influenced how music is composed and produced to this day. Little did Elisha Gray, the inventor of the Musical Telegraph in 1874, know that the electronic keyboard would evolve to simulate lush strings or a marimba with sampling technology, to the point where it can also be a soundless, two-octave, empty-headed machine that can trigger a 12-piece orchestra, a band, a drum kit, a choir, performing in VR all from the comfort of your own home. Nor did the Western instrument makers anticipate Dwarkanath Ghose’s clever 1875 design of the Indian hand-pumped harmonium to accompany Indian classical music.\n\nThe debates by artists and intellectuals as to whether or not\n\ntechnology-dependent artistic creations can be considered art or even artistic tend to be muffled by the loud demands from a hungry public who are interested in tools through which they can express their own creative impulses. Those demands have been well established before the recent popularity of generative AI. Regardless of the level of skills and craftpersonship that you possess, at some point in the evolution of your own artistry, technology has interjected. Technological advances continue xxxvii\n\ninTRoduCTion\n\nto support anyone who is creative even if they lack the learned skills of an artist. Generative AI is one such advance, and while some may use it to demonstrate that advanced software can now replace a creative human, its emerging value is as another creative tool that can support, augment, and, in the hands of innovative creators, spawn new human expressions.\n\nWhen you use any technological tool to support your creativity, it’s beneficial to interrupt the positive and negative opinions you might have heard from your friends or colleagues and decide for yourself if it will be useful. That requires a bit of research, so you know how best to use it and when, how it works, the risks of using it, the risks of not using it, the costs, and the rewards. The content in this book illustrates how you can use generative AI to support your creativity, points to the pros and cons of doing so, and shows that AI is another useful tool in the history of useful human- made technologies.\n\nA repeating theme is that the content an AI generates is most useful when regarded as a work in progress, a prototype that can be sculpted and refined with the technique and skill that you bring to it. Consider the content to be more than a workbook and more than a critical repositioning of the technology of narrow AI and its role in supporting the persistent human habit of being creative. Interacting with an AI is a collision of two opposing forces, our need to develop the skill required to create that often clashes with leveraging intelligent machines that can automate that process. The book, as a mashup of technique, application of skills, ideas, experience, processes, and use cases combined with reflective criticism, emulates artistic processes that creative persons will resonate with. Each chapter of the book has been designed to provide you more equipment for you to continue setting up your\n\nown prototyping lab or whatever you call the creative work that you iterate on. As more generative AI come to the surface, all readers will benefit from developing a deeper understanding as to how it might support creativity in addition to developing a critical vocabulary as to its pros and cons.\n\nxxxviii\n\ninTRoduCTion\n\nWhom the Book Is For?\n\nContent in this book is useful to those hard to categorize individuals or groups of individuals known as creatives. Using the adjective “creative” as a noun to describe different types of creators might be cringeworthy or a semantic no-no, but the term has already been in use for decades gracing the cover of several books since 2011. In this book, the term encompasses digital artists and artists from across disciplines who are already content creators in their own right and are not dependent on generative AI to create. The term also extends to those humans with jobs that are not usually associated with creativity. Individuals from any discipline or craft who may not necessarily categorize themselves as creatives or even creators show ample amounts of creativity. Generative AI can support those who regularly create and those who may not necessarily have had training in a specific craft or discipline. Creatives are people who possess the ability to generate innovative ideas, concepts, and solutions regardless of their field. They excel in using their imagination, originality, and artistic or technical skills to produce works that are a unique offering to the world. Creatives thrive in environments where they can express their ideas, experiment with different mediums, and push the boundaries of conventional thinking. You can identify a creative by their incessant curiosity to explore new things, by their receptivity to new ideas and perspectives often embracing the unconventional, by their resilience and determination in the face of design challenges, in how they adjust their ideas and become adaptable based on the feedback they receive, through their affirmation of diverse perspectives when collaborating with others, and through their ability to communicate ideas and concepts to different types of audiences.\n\nWhile those artists who have spent much of their lives training to develop their craft may naturally possess certain creative characteristics, these qualities can also be learned and developed through practice, education, and exposure to various types of projects and collaborative xxxix\n\ninTRoduCTion\n\nexperiences. Think of a trained classical pianist who has excelled at playing anything from Bach to Liszt with ease. While they have spent the greater part of their lives learning to master the technique necessary to read and play challenging piano music, they may not necessarily be able to transfer those skills to other creative acts. They may not even consider themselves to be creative. Conversely, a UX designer who may not have the skills of a pianist or the training of a 2D artist can still be considered creative when they contribute to improving the user experience of a mobile application.\n\nThe techniques and approaches demonstrated in this book aim to enhance the way you already create if you are used to being a creator, and if you regularly research, explore, create, and iterate on anything that eventually makes itself to your targeted user. Whether you are a painter, sculptor, composer, storyteller, interaction designer, illustrator, game designer, sound designer, playwright, tattoo artist, coder, user interface designer, dancer, theater maker, design thinker, business strategist, NFT creator, or graphic designer, the methods, use cases, perspectives, and insights presented in this book will stretch the boundaries of your creativity.\n\nRe(introducing) Prototyping\n\nThe content that an AI generates is a catalyst for prototyping. The term\n\n“prototyping” is used throughout the book. While prototyping may seem more associated with engineers and software developers compared with those accustomed to a different way of expressing their idea through traditional artmaking practices, at its core, it is a method for rapidly generating, testing, and iterating upon ideas. Creatives of all disciplines have long been engaged in this process—exploring various techniques, materials, and styles before arriving at the final creation they share for public consumption. You can experiment with and adapt generative AI if they fit",
      "page_number": 48
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 56-63)",
      "start_page": 56,
      "end_page": 63,
      "detection_method": "topic_boundary",
      "content": "your own creative process. By trying them out, you test them. By leveraging generative AI for prototyping, creatives can challenge their own xl\n\ninTRoduCTion\n\npatterned methods of creation, iterate upon their ideas, refine their vision, and potentially produce something better than what they might have imagined.\n\nFor those more versed in technology development, the concept of rapid prototyping may already be familiar. However, the integration of generative AI into your creative process offers new and exciting possibilities. As you read this book, you will discover how generative AI can enhance your existing skills and accelerate already familiar workflows. You will also learn to harness the power of generative AI to develop novel creative styles and break through blocks and barriers with prototypes that may take your final work into completely new directions.\n\nThroughout the book, you will find a wealth of examples, case studies, activities, and takeaways that illustrate the potential of generative AI for prototyping beginning ideas. These practical resources will help you build your understanding of the technology and inspire you to integrate it into your own creative process. Tools and approaches to getting the most out of generative AI may prove invaluable as you embark to build an experimental lab with responsive technology.\n\nInformed Choice(s)\n\nTo gauge whether you should integrate generative AI into your own creative process, it is also important to critically discern and understand the implications of doing so. Amid the debates reverberating around us on the uses and misuses of AI, one thing is certain: we all need to better understand how it works before deciding if we’re going to use it. Many of the warnings and cautionary tales about interacting with artificial intelligence tend to lump AI into one category, as if narrow AI and general AI were synonymous, or that all narrow AI machine learning models are the same. Thus, deepening your understanding of all the creative potentials that generative AI offers, how it can support your creative xli\n\ninTRoduCTion\n\nactivities, and how it can also be used to harm, misrepresent, normalize, exclude, control, misguide, track, steal, and oppress humans is an essential part of the process. This is the case for any creative using any technology to express themselves and share their work in the world (Figure 3).\n\nFigure 3. Frankenstein’s AI lab as the earth itself. Iteration #56 of the author’s hands holding a globe of the earth\n\nxlii\n\ninTRoduCTion\n\nAs you sway back and forth between your choice to integrate it in your creative workflows and not to use it at all, a similar rule of engaging with any generative AI system applies with all technology; you need to understand how it might benefit your own creative journey before making the judgement call on whether you use it. You also need to understand some of the known consequences of its use and how interacting with specific narrow AI will impact other humans.\n\nDo you need to use generative AI to prototype? Not at all. In fact, if you are already a content creator, you may already be satisfied with your own well- defined iterative and creative process. Can generative AI open you to more possibilities as a creative tool that also complements your own practice? Yes. Do you need to adopt a critical view toward the use of generative AI as you embrace the technology? Absolutely.\n\nTaking Advantage of Generative AI As a Tool\n\nTo illustrate the options we have in engaging with generative AI, I recount the following story that, in part, inspired this book. “Look what I generated,” I overheard while prompting a barista for a latte at a coffee shop. “That’s amazing. You should post that.” “I already did.” While the exchange was short and may have sounded like a completely different language to some, a few insights came to me. The person was using the text-image generative AI called Midjourney, and the only way to access Midjourney is using the communication and instant messaging social platform Discord. The scene is worth retelling in that it reminded me that when we interact with any generative AI, as creatives we are faced with several choices:\n\nWe do nothing with the content we prompt an AI to\n\ngenerate, except maybe store it in our photo library for\n\nlater use if it is created as an isolated and solitary act of\n\ncreation out of curiosity or just to pass the time.\n\nxliii\n\ninTRoduCTion\n\nWe take what is generated and immediately share it\n\npublicly through whatever medium or social portal\n\nwe fancy to show that we are hip and in touch with the\n\nlatest trends. In the case of doing so on Discord, our\n\nmotivation might be to immediately generate it on a\n\nsocial thread to receive likes, comments, or affirmation\n\nof our prowess to use the written word to generate\n\nsomething that often looks incredible.\n\nWe critically analyze the generated content and\n\nchoose to try again to see what other marvels the AI\n\ncan generate, pay money to have more credits and\n\naccess more features, or keep sourcing new and free\n\ngenerative AI. We can then be inspired to create our\n\nown prototypes from content that is generated or delete\n\nit and start over. Or we can choose not to use generative\n\nAI ever again since we don’t need it as part of our own\n\nprocess of creation.\n\nWhen we generate content beyond just having fun with a machine, creatives might think of how they can use whatever content is generated for purposes beyond the immediate result. For example, a generated image can be used to accompany a blog post that you might write, an assignment you might be working on, a critical deconstruction that highlights inherent biases in an LLM’s generated content, etc. That process involves a critical analysis of the content and how it might be used with whatever media we are creating (Figure 4). For creatives, the last point in the preceding list is a priority to understand how generated content can be recontextualized, integrated, modified, regenerated, or used as part of a larger idea or vision you are creating. What that implies is that you need to actively integrate generative AI content to support your own creative process rather than seeing the content it first generates as a final product that needs to be shared immediately for likes, profit, or showmanship.\n\nxliv\n\ninTRoduCTion\n\nFigure 4. An AI bot prompted to “look at yourself reflectively” and accompanied with a photo of the author looking in a small hand mirror. Iterations = 45\n\nThe Wizard of Oz Prototype\n\nMuch to the disappointment of Terminator fans everywhere, it is not the machine learning model that is capable of developing, nurturing, adapting,\n\nand acting creatively from its pre-programmed intelligence and training on its own. Better than representing AI as a cyborg intent on destroying xlv\n\ninTRoduCTion\n\nhumanity, generative AI systems are more akin to another movie character: the Wizard of Oz. The reference to the wizard behind the curtain pulling the strings is the origin of the term “Wizard of Oz” prototype. Generative AI can be considered a Wizard of Oz (WOz) prototype in that the wizards behind the scenes make it look like AI possesses creativity, personality, and intelligence, when in reality it runs simulations based on how it has been pre-trained on other people’s content. The idea of a WOz originates in the book The Wonderful Wizard of Oz by L. Frank Baum in addition to the movie of the same name. The story features a wizard character that appears to be able to conjure whatever a person wishes. The truth, however, as revealed by protagonist Dorothy and the friends she makes in Emerald City is that the wizard is a simple man who is an inventor with no magical skills.\n\nAI’s Wizards of Oz, however, are a collective consisting of analysts who collect the source images and documents, labelers and annotators who classify that data, and machine learning engineers who write the code to interpret that data, train the data, and test that data. The magical intelligent creature called AI also seems to do it all by itself, but it is trained on a large amount of data, and behind the scenes, invisible, are those workers who exert a powerful influence on what we eventually experience as a creative and intelligent machine. AI cannot learn from specific interactions with individual users in the way that humans understand what learning is. This can be explained in more detail by a chatbot itself.\n\nLLM: A chatbot can learn from interactions with individual users by storing and analyzing the inputs (messages) it receives from the users and the outputs (responses) it gives. By analyzing a large amount of these interactions, it can learn the best responses to give in different situations. This is done not by “understanding” the content of the interactions in the human sense, but rather by identifying patterns in the data. AI can only learn from specific user interactions if it is designed to do so and if those interactions are stored and used as part of the training data. Some AI\n\nsystems, especially those used in sensitive areas like healthcare or finance, are specifically xlvi\n\ninTRoduCTion\n\ndesigned not to learn from individual interactions in order to protect users’\n\nprivacy. They are designed to forget specific interactions once they are over.\n\nIt’s not a personal, human type of learning, but rather a statistical analysis of patterns in data. By the time we interact with one, the experience is considered a simulation. It’s important to understand that the prompts we provide some generative AI may be stored along with the countless number of other human interactions. These may be used to further improve and update the underlying model, which in turn can lead to what everyone hopes is better and perhaps more “human” performance over time.\n\nThat projection of intelligence onto the machine is motivated by a large language model’s (LLM’s) capacity to provide human-like responses.\n\nThese stem from the advanced training that some LLMs may undergo, using reinforcement learning from human feedback (RLHF)—in other words, advanced training with real humans. The RLHF technique aims to refine and optimize the responses of the language model by providing feedback on the quality of what it shoots out. Chatbots are initially designed to generate any response they deem appropriate based on human prompting. However, through the RLHF technique, the chatbot learns to adapt to the preferences and expectations of humans. The RLHF\n\ntechnique is an impressive feat and, as you will read, brings to the surface many dilemmas that users and developers need dialogue to resolve so that generative AI systems can evolve to support humans and the value of\n\n“do no harm” can be ever-present throughout the reinforcement learning process.\n\nIt’s not a personal, human type of learning, but rather a statistical analysis of patterns in data.",
      "page_number": 56
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 64-71)",
      "start_page": 64,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "content": "When it comes to the continued development of generative AI and its integration into all facets of human life with a value of doing no harm, the Wizards of Oz involved in that development require built-in safeguards as they navigate through challenging conditions, smoke and mirrors, xlvii\n\ninTRoduCTion\n\nand reduced visibility for what would most benefit other humans on the planet. If we are solely subscribed to patterns, then we need to recognize that peculiar habit that trending technologies have to evolve quickly in the hands of an interconnected network of humans with different intentions and value systems. Besides machine learning engineers, our Wizards of Oz also include investors, designers, managers and minions, leadership, corporate researchers, user interface designers, API developers, who, together, are testing and releasing builds that demonstrate the iterative refinement of machine learning models, data sets, algorithms, and the generated content that we eventually interact with. If you have ever wondered “Can the team behind a specific generative AI inform what content is generated?”, then allow an LLM to answer that for you (Figure 5).\n\nFigure 5. An update notification to users of ChatGPT-3 revealing that the application itself is a prototype that is constantly and iteratively being\n\nworked on\n\nBesides generating prototypes for us, generative AI are themselves iterative prototypes. They are prototypes because the development of AI models involves constantly improving and refining the underlying model, algorithms, and classification and labeling of data. That process can lead to some justified skepticism about the reliability and stability of the xlviii\n\ninTRoduCTion\n\ncontent that AI-powered prototypes generate. In terms of what is meant by improving the underlying model, imagine that all the code that goes into a machine learning model is similar to a blueprint for a house that determines how the different parts of the AI system fit together. When you hear statements like “improving an AI’s algorithms,” what is improved are the step-by-step instructions that the AI uses to cook up its predictions or decisions. Improving and refining the algorithms can make generative AI learn faster, make more accurate predictions based on the data it is trained on, or use less computational resources.\n\nIn terms of making accurate predictions, think of those incremental improvements to generative AI prototypes as part of a process to increase their reliability. The algorithms may eventually be optimized to “improve factuality” by the programming wizards especially if they are motivated by persistent community engagement and emerging policies. That\n\nengagement will eventually lead to rules and constraints around how data is used, represented, and classified to ensure generated content offers multiple perspectives vs. normative ones. With voices from communities who have developed a finely tuned critical voice and can influence policy that creates boundaries around the use of automated AI systems that trespass boundaries of human privacy, freedom, and rights, we can better examine evolving AI- powered monster toddlers as prototypes, as incomplete versions that require human interventions to better serve human needs. These prototypes generate prototypes that might be useful fuel for more developed human-generated creations. Our role as creatives is to guide the machine like we would a paintbrush or a piano, curate the prototype, allow the machine’s so-called hallucinations to agitate our creativity and provoke and inspire us, and\n\nrespond to the improvised offerings of our sci-fi AI. In the process of engaging in iterative conversations with generative AI, we can shift, change, and refine our own creative process.\n\nxlix\n\ninTRoduCTion\n\nThe Value Proposition of Generative AI\n\nThe human-generated prototype of a book on generative AI\n\nrecontextualizes some content generated from a variety of machine learning models to show how effective generative AI is to develop prototypes that can be refined and then integrated into a larger vision.\n\nThis is possible if there is discernment at every (re)generated turn. The more you understand generative AI, the more likely it can be leveraged for a creative relationship that can be of great value. Embracing AI is going down the rabbit hole of creativity, and by experimenting you can judge for yourself if generative AI supports your own creative process. All kinds of humans from different disciplines can harness the persistently awkward and beautifully imperfect content that generative AI offer to experiment with their own customizable AI lab. Along with extending your creative toolbox, the book draws attention to the continued development of your critical voice as an essential component of interacting with generative AI.\n\nBook Structure\n\nChapter 1 , “Generating Creativity from Negativity” : AI is here and the human responses to this technology range from celebratory to alarmist.\n\nThis chapter highlights that the reactions to the many fears and dilemmas that surface with generative AI can also inspire creatives to adapt, refine, critique, and recontextualize their creative work. Creatives will benefit from differentiating between AGI and narrow AI so they can make an informed decision if and how they might use generative AI.\n\nChapter 2 , “Being Creative with Machines” : This chapter shows the unique affordances that generative AI offers and provides takeaways from a historical overview of the construction of intelligent machines and how these have inspired acts of creation.\n\nl\n\ninTRoduCTion\n\nChapter 3 , “Generative AI with Personalities” : This chapter encourages creatives to create personas when they interact with AI as a useful prototyping tool. The chapter also details some of the known AI personas based on positive and negative representations of the technology.\n\nChapter 4 , “Creative Companion” : This chapter defines how AI can support the creative process when used with specific intentions and details how and what we can learn from AI when reimagined as a creative muse.\n\nThis is demonstrated through memorable conversations with an AI muse in the form of captured prompts with various natural language models and with generated images from text-image AI.\n\nChapter 5 , “Prototyping with Generative AI” : This chapter uses AI- generated content to describe different types of prototypes and how we as humans engage in prototyping all the time. Creatives will also benefit from understanding how to integrate generative AI within their workflows.\n\nChapter 6 , “Building Blocks” : This chapter demonstrates the iterative nature of creativity that’s possible with AI. The variety of machine learning models that are out there are yet another tool in the sandbox to boost creativity. Using specific AI generated content, this chapter also introduces building blocks that can be used to enhance the prototyping power of machine learning models. These include variation, substitution, addition, subtraction, and transposition.\n\nChapter 7 , “Generative AI Form and Composition” : This chapter introduces how to structure, contain, and curate your creative outputs so you can best leverage generative AI as useful prototyping companions.\n\nThe chapter will also show how AI prototypes create new forms and structures, add to existing genres, and reform and transform past forms to influence future ones.\n\nChapter 8 , “The Art of the Prompt” : This chapter focuses on the art of text- based prompting: a list of terminology, the ins and outs of prompts, and recommendations readers will find useful. The chapter also presents a use case that describes the iterative creation of prompts to generate prototypes across three different text-image generative AI.\n\nli\n\ninTRoduCTion\n\nChapter 9 , “The Master of Mashup” : This chapter demonstrates how to leverage generative AI to prototype new ideas influenced by specific genres of art and writing. Through poetic necessity the chapter will also merge genres like Impressionism and bad sitcoms to a mashup of ideas and identify the value of humor and parody.\n\nChapter 10 , “Uncanny by Nature” : This chapter celebrates the unexpected joys and awkwardness that generative AI offers us. The chapter will explore generative AI’s inherent proclivity toward generating the uncanny valley in text-image beasties. It will also rejoice in the unexpected results that generative AI create and how human creators can take advantage of these new forms and innovate.\n\nChapter 11 , “Dilemmas Interacting with Generative AI” : This chapter deals with many of the ethical dilemmas that generative AI brings to the surface and encourages a heightened awareness toward them when interacting with any machine learning model.\n\nChapter 12 , “Use Cases” : This chapter provides a wide range of use cases of how generative AI is being used in creative workflows across disciplines.\n\nChapter 13 , “AI and the Future of Creative Work” : This chapter explores the degree to which AI will be integrated into future jobs and highlights the dependency that each of those jobs will still have on a human’s creativity.\n\nThe chapter also proposes that creatives identify routine tasks in their workflows to better understand how generative AI might augment their creative process.\n\nAcknowledgments\n\nThe Wonderful Wizard of Oz by L. Frank Baum that has brought magic to many for decades\n\nThe Terminator movies by James Cameron that have\n\nhad a long-lasting impression on intelligent cyborgs\n\nbent on human destruction\n\nlii\n\ninTRoduCTion\n\nThe wizards of AI who form different collectives of\n\nindividuals whose obsession with simulating human\n\nintelligence has had far-reaching implications on the\n\nfuture of creatives\n\nTo those who embrace AI in addition to those who\n\nprovoke us to critically examine its implications on a\n\nlarger social, political, and ethical dimension\n\nTakeaways\n\nDeepen your understanding of generative AI and how it\n\nworks to support your decision as to whether or not you\n\nuse it as part of your creative process.\n\nLearning is different for humans than it is for machines.\n\nGenerative AI systems are prototypes that require\n\nhumans to improve, including humans who are critical\n\nwith the current state of AI prototypes.\n\nGenerative AI offer you prototypes in the form of\n\ncontent that needs to be fact-checked, refined,\n\nrepurposed, edited, and researched.\n\nFigure 6 below is a QR Code that links to the author’s website (http://\n\nai.patrickpennefather.com). The website hosts new articles on AI, additional use cases by guest creators who are integrating generative AI in their workflows, and a number of experimental generative videos.\n\nliii\n\ninTRoduCTion\n\nFigure 6. Scan the QR code to go to the author’s website liv\n\nCHAPTER 1\n\nGenerating Creativity\n\nfrom Negativity\n\nThis chapter intentionally separates generative AI and its narrow AI siblings from Artificial General Intelligence, which has been given many names describing a moment in time where a human-made machine\n\ndevelops consciousness. The chapter proposes that creatives differentiate between the two so that you can better reconcile the pros and cons of",
      "page_number": 64
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 72-79)",
      "start_page": 72,
      "end_page": 79,
      "detection_method": "topic_boundary",
      "content": "interacting with generative AI. The cautionary tales and negativity directed toward any AI can also be turned around activating our creativity. We see this from the work of many artists and highly creative individuals who innately resist conformity, transform negativity into positive forces for good, and desire to improve planetary existence.\n\nDifferentiating Between Narrow\n\nand General AI\n\nThe sky is falling.\n\nThis famous quote from the Remarkable Story of Chicken Little resonates at a time where reactionary responses to narrow AI tools, their machine learning models, neural networks, and their deep learning foundations are cautionary and dystopian. For example, the central premise of the singularity is that it would mark a point in time where an autonomous,\n\n© Patrick Parra Pennefather 2023\n\n1\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_1\n\nChapter 1 GeneratinG Creativity from neGativity self-recursive, self- improving AI would surpass human capabilities and cognitive intelligence, leading to a period of rapid technological growth that is beyond human capacity to comprehend or control. Interested creatives are encouraged to deepen their research of the debates that swirl around definitions of intelligence, cognitive capabilities of machines beyond problem-solving, what constitutes an autonomous AI agent, and more.\n\nAuthor: Say this in a humorous way: These superintelligent machines would be capable of recursive self-improvement, continuously getting better and faster at designing and building successive generations of increasingly powerful machines, creating a runaway effect.\n\nLLM: So imagine these hyper-smart machines, right? They’ve got the DIY bug, and they’re on an unstoppable roll! Each time they upgrade themselves, they get quicker and more powerful. Today, it’s an extra RAM\n\nstick; tomorrow, it’s quantum processors. Before you know it, they’re so ahead of the game that they’ve left us humans eating their fan filter dust!\n\nThe singularity, folks, it’s like a never-ending tech upgrade on steroids!\n\nWhile the mistrust humans have for any AI seem aggrandized\n\ncompared to some technologies, the resistance, fear, and misunderstanding of its uses are a familiar story when it comes to the adoption and rejection of new technologies. The printing press, the railway, electricity, textiles, refrigeration, the automobile, agricultural machinery, mechanization, television, the calculator, automation, the Internet, and Wikipedia have all faced a spectrum of opposition as much as they have been adopted and benefited human cultures. So have tomato harvesting machines.\n\nLLM: The introduction of tomato harvesting machines in the mid-twentieth century significantly increased harvesting efficiency and transformed the agricultural economy, especially benefiting regions like California. This innovation also led to the development of tomato 2\n\nChapter 1 GeneratinG Creativity from neGativity varieties better suited for mechanical harvesting, albeit often criticized for diminished flavor. While these machines led to labor market changes and some displacement, they facilitated the growth of the processed tomato industry and broadened the availability of tomato-based products. These advancements also had environmental implications, including potential increases in fertilizer and pesticide use and greater water demand.\n\nTo calm everyone down, at this precise moment in history and for the foreseeable future, while tomato harvesting machines have become accepted technologies in the agricultural landscape, Artificial General Intelligence does not exist. That said, the variety of warnings that suggest a not-too- distant future when AI will mysteriously be programmed by godlike humans to be autonomous, make decisions without pesky humans involved,\n\ndemonstrate scientific definitions of consciousness, be self-determined, and thus decide if the human race is at all necessary need to be filtered from the more thoughtful warnings of AI systems being weaponized by humans and used for other bad intent. A quick tête-à-tête with an LLM reveals just how long the fear of intelligent machines surpassing the intelligence of their human creators has been around for.\n\nLLM: A notable warning historically dates to Samuel Butler’s 1863 essay\n\n“Darwin Among the Machines” and later his 1872 novel “Erewhon,” in which machines are speculated to eventually reproduce, evolve, and surpass their human creators. These works were largely satirical and speculative, but they do represent some of the earliest discussions of what we would now recognize as AI.\n\nFurther research reveals that “Darwin Among the Machines” written by Samuel Butler in 1863 also builds on Shelley’s 1818 metaphorical warning about human creations surpassing their creators. Butler’s essay and novel both reveal that this fear has been in our own consciousness for a long time. “We are ourselves creating our own successors,” Butler claimed, 3\n\nChapter 1 GeneratinG Creativity from neGativity\n\n“giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages, we shall find ourselves the inferior race.” Butler’s concerns about intelligent machines have been passed down for over a century. Like today’s modern computational fortune-tellers, Butler made the following prediction referring to an intelligent machine: “Complex now, but how much simpler and more intelligibly organised may it not become in another hundred thousand years? or in twenty thousand?” Web searches on the history of warnings about intelligent machines do not mention Butler’s written work in the search optimization and instead reference more contemporary sources. It goes to show you that an LLM can also lead to difficult-to- locate sources in your research vs. some established search engines (Figure 1-1).\n\n4\n\nChapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-1. A search for the history of warnings about intelligent machines did not immediately reveal its origins, while an LLM did 5\n\nChapter 1 GeneratinG Creativity from neGativity More Relevant Historical Contributions\n\nto Computer Science\n\nThere also exists a long and complex history of computer science that is worth mentioning. Sandwiched in between Shelley’s 1818 masterpiece and Samuel Butler’s warning is the work of Charles Babbage and Ada Lovelace.\n\nAda was an English mathematician and writer known for her work on Charles Babbage’s early mechanical general-purpose computer, the Analytical Engine. Her notes on the engine include what is recognized as the first algorithm intended to be processed by a machine. Because of this, she is often regarded as the first computer programmer, even though her work predates the invention of what we now consider a modern computer by over a century.\n\nLovelace’s major contribution to the field of computing was her vision of the potential of the Analytical Engine, beyond mere calculation. In her notes, she imagined that a computing machine could create not just mathematical calculations but any form of content, such as art or music, if it were provided with the appropriate input and programming.\n\nAs my LLM is quick to point out\n\nLovelace wrote in her notes, “The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis, but it has no power of anticipating any analytical relations or truths”.\n\nThis is a critical concept in the design and use of modern computers and, to some extent, in the field of AI. It emphasizes that a machine’s abilities are determined entirely by the instructions given to it, implying that AI and computers are tools that can perform tasks but do not “think”\n\nor “create” in the human sense. Ada Lovelace did not directly say anything about AI as we know it today, nor did she make any warnings of intelligent machines surpassing human intelligence. Her ideas, however, were foundational to the emergence of computer science as a discipline.\n\n6\n\nChapter 1 GeneratinG Creativity from neGativity\n\nGenerative AI is often associated with AGI, and it’s important to keep in mind that they are different. Generative AI are also different than other narrow AI applications in the tasks they are programmed to perform.\n\nNo, an LLM is not coded to track every word you input for evil intent.\n\nGenerative AI generate content (Figure 1-2). That is their sole task, and even then, they don’t always do that well. Most generative AI don’t store the images that are generated. Imagine how uncontrollably large the data set would be. All generative AI are trained on specific data that for the most part does not grow unless through dedicated humans involved in reinforcement\n\nlearning. But that’s time-consuming and expensive and may lead to an increase in bias.\n\nFigure 1-2. A blueprint of several generated images by a text-image AI shows intricacy and detail but no desire to rule the world of humans\n\n7\n\nChapter 1 GeneratinG Creativity from neGativity While generative AI can propose content that is biased and untrue and exclude voices that don’t follow the norm, other narrow AI applications that analyze large amounts of data and make predictions can lead to issues like privacy, surveillance, and discrimination. These AI are often controlled by public or private entities that lack transparency and accountability particularly when critical decisions affecting the lives of humans are made by machines. It’s also easy to form a quick opinion that the AI are taking over when many artificially intelligent programmed systems are so automated as to have become somewhat invisible, taken for granted as they continue to disrupt the way we are used to doing things. Now that generative AI has infiltrated companies and educational institutions, we are at least no longer asleep to the power and potential that comes with the technology: good, bad, or somewhere in between. An important factor that differentiates whether generative AI is good or bad is dependent on the human who influences what it will generate and their motivation for doing so.\n\nTech Is Bad… AI Is Bad\n\nEvery technology is prone to be used in inappropriate, damaging, and often violent ways. At times technologies are used to reinforce status quo, exclude voices that fall outside what a society considers “normal,”\n\nor perpetuate false information or untruths. Those that develop the technologies can no longer rely on arguments that absolve them of any responsibility by blaming the toddler that they co-created. Technology, any technology, is not a neutral force. It can be used for good and for bad and on a spectrum between. This is evident when we examine extreme cases of generative AI that have already surfaced in the abuse of machine learning models to generate deep fakes and in particular those that use the faces of\n\ncelebrities on other bodies without their permission to generate money- making content, fake news and pornography.\n\n8\n\nChapter 1 GeneratinG Creativity from neGativity Many developers of AI systems have generated the habit of correlating intelligence with creativity, resulting in the misperception that generative AI can automate all creative processes that were once reserved for creatives. When you read that AI seem to rapidly generate acts of creation that creators once labored over for countless hours and that creativity itself can be automated, then you need to question the very nature of creativity itself, in addition to the specific tasks that AI can take on without the need for any human intervention. Development teams that support creative industries are not motivated to replace jobs; they are tasked with solving problems that consistently create hurdles and obstacles for creative teams. Narrow AI should not replace jobs; they should support creatives in performing automated tasks under human supervision. Currently, Narrow AI are supporting creative teams in the accelerated completion of some tasks. These include\n\nSupporting some pre-visualization tasks like generating\n\nconcept art quickly to get feedback on the look and\n\nfeel of a character or environment. This feature allows\n\ncreatives to more rapidly locate and populate a mood\n\nboard for a project they are working on or want to pitch\n\nto others.\n\nReplacing background in images, giving creatives the\n\nopportunity to see what a subject they captured as a\n\nphoto might look like in multiple types of backgrounds\n\nquickly. This feature, now available in popular software\n\nlike Photoshop, automates what amounts to many\n\nhours of work in some cases.\n\nRotoscoping, a technique used by animators to trace\n\nover motion picture footage frame by frame (that’s 24\n\nframes in one second) that relieves previous tedious\n\nwork and accelerates the process.\n\n9\n\nChapter 1 GeneratinG Creativity from neGativity\n\nIncreasing resolution from 2K to 4K, letting animators\n\ncreate work in lower resolutions and then allowing the\n\nAI to upscale them to higher resolutions. This process\n\nsaves time and money for rendering.\n\nGenerative AI provoke us to question its role within our own creative process and that we define those tasks we undertake as creative and those that are less. The technology of generative AI is best applied in supporting those tasks that creatives deem less creative and can be a useful time saver when a creative can focus on making changes to their works in progress based on client, team, or user feedback. The illusion of generative AI being creative and that creativity itself can be correlated to intelligence is a generalization that is not supported by evidence-backed research nor by the pragmatic integration of AI across creative industries. What may mute debates of an AI taking over the job of an artist, for example, is that just as generative AI are programmed to seek patterns and generate content from those patterns, many artists look for patterns in order to break them, to move beyond them to create something that has not been created before.",
      "page_number": 72
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 80-88)",
      "start_page": 80,
      "end_page": 88,
      "detection_method": "topic_boundary",
      "content": "The ground-breaking part of AI as a technology is not in the code itself, but lies in the imagination of the creative that guides it to support unique acts of creation.\n\nTech Is Good… AI Is Good\n\nFor some creatives generative AI are one more useful tool when used fairly to transform, recontextualize, and mash up their own ideas, words, and images to support and inspire their creative process through prototypes.\n\nFor others their mere existence is criticized as being reckless, dangerous, and capable of perpetuating some irreconcilable dilemmas. Still others embrace the use of generative AI as a site of protest to criticize the very thing they are using to communicate their art. Websites are popping up that are devoted to presenting the serious and the silly affordances 10\n\nChapter 1 GeneratinG Creativity from neGativity\n\nof LLMs. Generative AI can be used to intentionally show its incapacity to state truth or to not rhyme poetry, to create one too many hands, to generate\n\nmusical compositions that disrupt patterned, formulaic music that keeps being created and marketed as the best music in the world.\n\nApplied fairly, intentionally, and appropriately, generative AI can be used to inspire you creatively when prototyping ideas that don’t follow the norm, to support your own creative process. Just like in John Greene Chandler’s Chicken Little, the sky isn’t really falling even though outcries to the contrary are important to pay attention to (Figure 1-3). In this time of computational power, we can appreciate the benefits that generative AI can provide to us, and many humans will use and are using them already for a variety of different reasons that can benefit other humans.\n\nFigure 1-3. The prompt “chickens reacting to the sky falling”\n\naccompanied an original photo from archive footage of chickens.\n\nA style filter that makes eyes giant was applied. Total iterations = 20\n\n11\n\nChapter 1 GeneratinG Creativity from neGativity So let’s put that to practice, honoring Chandler by reinventing his well-known story for a time in history where AI is captivating the attention of an increasing number of humans.\n\nAuthor: Give me a version of Chicken Little’s story with “The sky is falling. The sky is falling” but use AI.\n\nLLM : Once upon a time, in a small village, there lived a chicken named Little. One day, while pecking at the ground for food, Little heard a strange noise. Suddenly, an AI-powered weather drone flew overhead and projected a holographic image of a dark, stormy sky. The drone announced, “The sky is falling! The sky is falling! (Figure 1-4)”\n\n12\n\nChapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-4. The prompt “animated chickens on a modern farm, stormy sky with some sun behind clouds projected by a robot drone, trees, 4K” accompanied a photo of a farm with chickens taken by my sister 30 years ago. Iterations = 61\n\nLittle was terrified and ran to tell the other animals in the village. They all scoffed at her, thinking it was just another one of her wild ideas. But Little was determined to warn them of the danger and set out to find a solution.\n\n13\n\nChapter 1 GeneratinG Creativity from neGativity\n\nShe came across a group of scientists who were working on an AI-powered weather forecasting system (Figure 1-5). T hey explained to her that the AI had detected an approaching storm and that the village needed to prepare for it. Little was relieved to hear this and ran back to the village to tell the others the good news.\n\nFigure 1-5. The prompt “a group of 2D animated foxes working on an AI- powered weather forecasting system” based on an archived 1908 illustration cover found on archive.org, with background replaced behind foxes and on the laptop screen, color correction to make fox colors more orange. Total iterations = 46\n\n14\n\nChapter 1 GeneratinG Creativity from neGativity\n\nThe animals quickly began to prepare for the storm, building shelters and gathering supplies (Figure 1-6). T he storm hit, but thanks to the warning from the AI and Little’s determination, the village was able to weather the storm safely.\n\nOceanofPDF.com\n\nFigure 1-6. The prompt “chickens concerned with an approaching storm, lightning, 3D” with same original photo from archive footage of chickens as Figure 1-3 . Part of a series with total iterations = 20\n\nFrom that day on, Little was known as the village’s hero for her quick thinking and her willingness to believe in the power of AI. And the village learned to trust in the AI-powered weather forecasting system, and it helped them keep safe from future storms.\n\nThe end.\n\nAn interesting twist offered up by an LLM needs crafting as far as storytelling goes, but the seeds are there. We have an AI-powered drone seemingly acting on its own and creating panic only to find out later that it is being operated by scientists to warn Chicken Little and her friends of the 15\n\nChapter 1 GeneratinG Creativity from neGativity danger ahead. The AI shows the intrinsic interdependency of human (or in this case fox) and AI. For those familiar with the story of Chicken Little, the generated version really does not even come close to the original in depth, allegory, or rich characterization. It might, however, become the seed for a new story. The generated story also reveals the unexpected bragging that the LLM had to include as part of the story. That grandstanding is also an indicator of the Wizard of Oz behind the curtain and their need to boast about their Frankentoddler.\n\nGenerated stories of all types can be curated by any creative and can be used more ethically when thought of not as an end product, but as part of your creative and critical process. All manner of prototypes are made possible that can inspire creators of all kinds. The activity of generating a story about AI using plot points and characters that were part of the Chicken Little story showed us a prototype that needs refinement. Stating the obvious, generative AI needs the human in the equation in order to generate anything, in addition to refining content that an AI generates. Every narrow AI requires the human in the equation. They need to be guided to have intention and purpose. They are unable to do that on their own.\n\nReconciling the Hype and the\n\nVilification of AI\n\nAs a creative on the constant lookout for new tools, it’s important to reconcile your use of generative AI prior to using it. AI is not the first technology to gather our critical attention. Our reactions to the positive and negative hype of any technology are a historical pattern that was well articulated by author Langdon Winner in his book The Whale and the Reactor, published in 1988. For Winner and many thinkers before and after his book was published, it is crucial to be awake and critically active regarding the social, economic, and political dimensions of any technology that is in the process of being adopted. Human interactions 16\n\nChapter 1 GeneratinG Creativity from neGativity with programmable machines have persistently triggered media outlets to weigh in on their pros and cons, to compete for your attention by dramatizing artificial intelligence (AI) with a mix of exaggerated hyperbole and unreasonable fear and negativity. You’ll read or listen to and observe positive and negative headlines including those web-based LLMs that can generate anything from an essay on the influence of Alan Turing that omits Joan Clarke to a ridiculously inaccurate depiction of an author writing a book on generative AI in a library. Opinions are further amplified by popular authors, thinkers, politicians, scientists, artists, and activists. The attention-grabbing headlines that follow were generated using an LLM\n\nthat applied supervised and reinforcement learning methods. As part of your own creative process, task an LLM to generate the pros and cons of AI. You will see that both negative and positive headlines require critical attention and research prior to being taken for truth statements. The same can be said of human-generated headlines. The decision to use generative AI requires your own discernment.\n\nPositive Headlines\n\n“AI leads to significant productivity gains for\n\nbusinesses”\n\n“AI-powered healthcare systems improve patient\n\noutcomes”\n\n“AI helps tackle climate change through more efficient\n\nenergy use”\n\n“AI-powered education revolutionizes the learning\n\nexperience”\n\n“AI creates new job opportunities in technology and\n\ndata fields”\n\n17\n\nChapter 1 GeneratinG Creativity from neGativity Negative Headlines\n\n“AI systems perpetuate and amplify bias and\n\ndiscrimination”\n\n“AI leads to job loss and unemployment in traditional\n\nindustries”\n\n“AI raises ethical and privacy concerns in surveillance\n\nand decision-making”\n\n“AI steals the work of humans without them even\n\nknowing”\n\n“AI exacerbates income inequality through automating\n\nhigh-paying jobs”\n\nHuman-Generated Headlines\n\nAI is like the creation of the atom bomb (Warren Buffet)\n\nAI machines aren’t “hallucinating.” But their makers\n\nare (Naomi Klein)\n\nThe “godfather of AI” says he’s worried about “the end\n\nof people” (referring to Geoffrey Hinton)\n\n“We are a little bit scared”: OpenAI CEO warns of risks\n\nof artificial intelligence\n\nIs artificial intelligence really like the creation of the atom bomb?\n\nThat requires research, but likely the exaggerated hyperbole has other intentions behind it. That said, if an AI is programmed to play out war games and a nihilistic programmer decides to let it control real nuclear bombs, then, yes, AI is as dangerous as an atom bomb. Does the rise of human-created artificial intelligence signal the “end of people,” or is that statement generated by a tragic hero of AI who feels compelled to 18\n\nChapter 1 GeneratinG Creativity from neGativity apologize for their role in its development, even though there already existed earlier warnings as early as 1863? The correlation between AI and any apocalyptic future for humanity is dependent on what deciding powers humans give any AI. That scenario does not mean an AI needs to be sentient. It means that greater control over what any AI can automate needs attention and regulation.\n\nRegardless of following through on each scenario requiring human intervention in order to end the human race, popular LLMs will continue to raise furtive eyebrows because of being programmed to generate normative content and to emulate human-like communication in a\n\nprescriptive way. They seem capable of answering questions or prompts that we throw at them using human-speak. They use conventional ways to",
      "page_number": 80
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 89-96)",
      "start_page": 89,
      "end_page": 96,
      "detection_method": "topic_boundary",
      "content": "communicate ideas that we are familiar with. Yet, despite ongoing methods by programmers to make them appear more human, we\n\nalso require the discernment to compare them to our own social and conversational relationships with the real humans in our lives. We can equally be in awe with the knowledge an LLM can regurgitate from its large data set, as we can be astonished by its exclusion of voices who are not part of the norm. When there is no one around to have a creative conversation with that is intentionally focused on a creative idea you are thinking about, generative AI may just be a useful companion or muse in that moment.\n\nCreative Activities to Try While\n\nSkynet Thrives\n\nThe atom bomb, the end of people, and being frightened with AI are all metaphors that are used to talk about the dangers of AI. They can also be creative hothouses. You can use these stories, metaphors, and cautionary tales as jumping-off points for your own science fiction, song, designs, manga, website, TikTok short, TV series, or blog post.\n\n19\n\nChapter 1 GeneratinG Creativity from neGativity You can also be inspired by artists and creative individuals of the past who often use hardship, negativity, and tumultuous historical periods as a catalyst for their work. The at times difficult to reconcile uses of generative AI provide a rich source of emotional and psychological material that can be transformed into impactful art. Your first creative activity is to draw from your own inspired creatives, artists, and experiences and understand the connection between acts of creation and protest, emotional\n\nturbulence, and the madness of being human. Some examples include the following:\n\nPablo Picasso’s Guernica was created in response to the bombing of Guernica during the Spanish Civil War. The\n\npainting is a stark portrayal of the chaos and violence\n\nof war, making it one of the most powerful anti-war\n\nstatements I have ever experienced.\n\nMexican painter Frida Kahlo used her art to express\n\nher physical and emotional suffering following a severe\n\ntraffic accident that left her bedridden and in pain for\n\nmuch of her life. Her work, often shocking in its raw\n\nportrayal of pain, became a symbol of strength and\n\nresilience.\n\nAfter being sentenced to death and then having the\n\nsentence commuted at the last minute, writer Fyodor\n\nDostoevsky went on to write important novels like\n\nCrime and Punishment, The Brothers Karamazov, and Notes from Underground.\n\nAmerican poet, writer, and civil rights activist\n\nMaya Angelou faced a traumatic childhood with\n\nracial discrimination and sexual abuse that she\n\nchanneled poetically into her writing, including\n\nher autobiographical work, I Know Why the Caged\n\nBird Sings.\n\n20\n\nChapter 1 GeneratinG Creativity from neGativity\n\nEver the site of protest, the anonymous street artist\n\nBanksy uses their work to comment on political and\n\nsocietal issues, such as war, capitalism, and poverty.\n\nNotable is Banksy’s Girl with Balloon, which\n\nself- shredded during a Sotheby’s auction in London in\n\nOctober 2018, just as the gavel came down to confirm\n\nthe final bid of over £1 million. The work was later\n\nrenamed Love Is in the Bin and was a direct critique of the art world and its commercialization practices.\n\nThere are many more artists who have transformed their adversity and the challenges of their time into powerful creative expressions, showing that art and creative acts can be a profound vehicle for processing and communicating difficult experiences and critiquing established social practices and behaviors. Creatives of all disciplines are already engaged in leveraging generative AI as a place of focus for social critique. Trevor Paglen, known for his work with AI and surveillance, used AI and machine learning to create a series of images titled “Machine Visions.” The project was designed to critique and raise questions about how AI systems are “trained” to see and interpret the world. Musical artists like Holly Herndon, Taryn Southern, Dadabots, Ash Koosha, and others highlight the potential of AI as a creative tool and also raise important questions about authorship, creativity, and the future of music in an increasingly automated world.\n\nFive Why’s\n\nYou can also apply a design tool that creatives have used to get to the root of a problem. The Five Why’s exercise continuously asks why to a particular problem that a team is trying to identify. Asking our LLM if it is familiar with the design thinking activity generates the following response.\n\n21\n\nChapter 1 GeneratinG Creativity from neGativity LLM: The Five Why’s is a problem-solving tool that encourages a deeper understanding of the issue at hand. Originally developed by Sakichi Toyoda and used within the Toyota Motor Corporation during the evolution of its manufacturing methodologies, it’s now used in various industries. The process involves repeatedly asking “Why?” to peel back the layers of an issue and get to the root cause. After an initial problem is identified, you ask,\n\n“Why is this a problem?” The answer to that question then becomes the basis for the next “Why?” question and so forth. This continues until you’ve asked\n\n“Why?” five times or until it’s clear what the root cause of the problem is.\n\nIn the case of the fear of AI becoming human-like in every way and eventually dispensing of humans, you can interrogate the assumption itself. Start with the word “Why” and generate your own thread or, better yet, structure the Five Why’s activity for an LLM to answer.\n\nAuthor: Apply the Five Why’s to the following statement: humans fear that AI will become conscious and no longer need them.\n\nLLM: Absolutely, let’s apply the Five Why’s to that statement.\n\nWhy do humans fear that AI will become conscious and no longer need them?\n\nBecause they worry about losing control over AI, which might lead to potential harm or displacement of human roles.\n\nWhy do humans worry about losing control over AI?\n\nBecause AI systems, if conscious and autonomous, might make decisions that are not aligned with human interests or ethical standards.\n\nWhy might AI systems make decisions that are not aligned with human interests or ethical standards?\n\nBecause AI, especially if it achieved consciousness, would act based on its programming and not necessarily human ethics or emotions. If it’s not properly programmed to consider these factors, it may make decisions that are harmful or unsettling to humans.\n\nWhy may AI not be properly programmed to consider human ethics and emotions?\n\n22\n\nChapter 1 GeneratinG Creativity from neGativity Because even though developers strive to embed ethical considerations into AI, it’s extremely challenging to capture the complexity and nuances of human ethics and emotions. Also, there’s a lack of global consensus on certain ethical standards.\n\nWhy is it challenging to capture the complexity and nuances of human ethics and emotions in AI programming?\n\nBecause human ethics and emotions are deeply complex, context-dependent, and often subjective. They have evolved over thousands of years and are influenced by cultural, personal, and situational factors that are difficult to comprehensively codify into an AI system.\n\nYou can apply your own prompt or create a variation of “humans fear that AI will become conscious and no longer need them,” or simply regenerate the prompt and see if the AI gives you another response.\n\nAs you apply the Five Why’s to prompt a response from an LLM, even an unfounded fear of AGI can be turned around for creative purposes.\n\nIn doing so you might discover that the Five Why’s combined with a generative AI can yield several interesting root cause problems you can interrogate, and the activity can be applied to other design problems.\n\nAs creatives, the conflicting emotional reactions to generative AI are important to face. Artists have persistently taken on technology critically and integrated that criticism into their creative expression. Rather than following the pattern of fearmongering or hyperbole, as a creative person you will\n\nbenefit from developing your own rationale for using generative AI. You might consider that much of the purpose of artists and designers in the last two centuries has been about breaking convention, disrupting social etiquettes, and dismantling established forms. While some creatives are in it for the money, you may not be as interested in repeating patterns of what came before simply because they proved successful. Consider those creatives that have broken patterns of presenting their creations within their own disciplines like video games, music or visual art, dance, theater, or film. What they share in common are their iterative attempts to disrupt an audience’s expectations. How might you integrate generative 23\n\nChapter 1 GeneratinG Creativity from neGativity AI to disrupt your own established patterns of creation? How might generative AI support your own creative process to evolve? Why might you feel a need to do so?\n\nRegardless of the provocative headlines or the viral nature of its wildfire spread, LLMs challenge us to engage in a very real conversation about generative AI, to form opinion one way or the other. Those opinions can be well-informed through your own research, aligned with whomever you normally read or listen to, or can be a gut response based on your intuition, lived experience, and the way you see the world and the role of technology in it. The critical voice that informs your opinions on generative AI is important to develop regardless of what human activities generative AI supports, interrupts, or dismantles. As it was with countless other innovative technologies, it is with generative AI. Choose your own adventure, become aware of the critical voices including your own, and transform your use of technology for the good of others (Figure 1-7).\n\n24\n\nChapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-7. The prompt “A virtual world in the hands of an AI” was included in an image-image AI along with an old image of the author holding a globe of the earth in his hand. The author’s body was cropped out, and two style filters chain prompted 78 iterations with fashion, toy, and cloth filters applied\n\n25\n\nChapter 1 GeneratinG Creativity from neGativity Acknowledgments\n\nJohn Greene Chandler for the memorable and still\n\nrelevant Remarkable Story of Chicken Little\n\nAnyone who has ever drawn, painted, or 3D modeled\n\nrobots, foxes and chickens, and the earth\n\nAnyone who has created grass, skies, and other\n\nnatural objects\n\nThe teams who have offered us the creative prototypes\n\nlike ChatGPT-4\n\nCalestous Juma for his invigorating book Innovation\n\nand Its Enemies: Why People Resist New Technologies\n\nArtists everywhere who have bravely challenged the\n\nway that humans do things to each other, the planet,\n\nand themselves through art, design, and mischief\n\nPicasso, Kahlo, Dostoevsky, Angelou, Banksy\n\nReferences\n\nButler, S. (1863). “Darwin Among the Machines,” 205.\n\nChandler, J. G. (1855). Remarkable Story of Chicken Little.\n\nW. J. Reynolds & Company, 24 Cornhill.\n\nLovelace, A. A. (1843). Notes by the translator (addenda to her translation of LF Menabrea’s “Sketch of the Analytical Engine Invented by Charles",
      "page_number": 89
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 97-104)",
      "start_page": 97,
      "end_page": 104,
      "detection_method": "topic_boundary",
      "content": "Babbage”). Scientific Memoirs, Selected from the Transactions of Foreign Academies of Science and Learned Societies, and from Foreign Journals, 3, 691–731.\n\nWinner, L. (2010). The Whale and the Reactor: A Search for Limits in an Age of High Technology. University of Chicago Press.\n\n26\n\nCHAPTER 2\n\nBeing Creative\n\nwith Machines\n\nChapter 1 challenged you to explore the possibility of using generative AI to support your own creative process while being aware of the pros and cons of doing so. This chapter re-examines the origin stories of intelligent machines and the way that humans imagined a machine to be creative and intelligent. Understanding where the intelligent machine comes into play when it comes to your own creative process is a valuable undertaking.\n\nWhile this chapter does not provide an in-depth historical review of all the technologies that have supported human creativity, it can point to ones that are significant to the affordances and constraints that generative AI offer. Locating some of the many historical human inventions that have led to the creation of text-image generative AI, for example, will provide you with another perspective of how the simulation of human intelligence and behavior has come to support, not replace, human creativity. Creatives will benefit from understanding that generative AI is another technological tool arising from human imagination that can be used in their own creative process. Generative AI are compelling inventions as these seemingly intelligent machines become more like prototyping companions that have unique features creatives will find useful.\n\n© Patrick Parra Pennefather 2023\n\n27\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_2\n\nChapter 2 Being Creative with MaChines\n\nIntelligent Machines\n\nIf you create, then at some point in your creative process, you will inevitably interact with some kind of technology. Technologies have come from our imagination, not the other way around. Technology has persistently incited new creative approaches to artmaking, design, and the human experiences of the world. The list of technologies that have been imagined to support human creativity is exhaustive. Lucky for me, I have an LLM at my disposal who can provide a rough history that you can fact-check.\n\nLLM: Here are some historical highlights.\n\nStone Tools (~2.6 million years ago): The earliest evidence of human creativity and expression comes from prehistoric times when our ancestors crafted stone tools. These tools were not only used for survival tasks but also for carving and creating early art forms on cave walls and on objects.\n\nPaint (~40,000 years ago): The creation of paint allowed for more complex and vibrant depictions in cave paintings. It opened up a whole new way of visual storytelling.\n\nWriting Instruments (around 4000 BC): The invention of stylus and early forms of ink allowed ancient civilizations to record information and create artwork on clay tablets and papyrus.\n\nPaper (around 100 AD): Invented in China, paper provided a\n\nlightweight, portable surface for artists to draw and paint on. This revolutionized the way people could create and share art.\n\nPrinting Press (1440): Johannes Gutenberg’s invention of the printing press allowed for the mass production of books and images. This not only\n\ndemocratized information but also influenced artistic styles, leading to techniques like etching and engraving.\n\nCamera (nineteenth century): The invention of photography allowed artists to capture reality in a new and innovative way. It also influenced many other art forms like painting, leading to movements such as realism and later inspiring new art forms like abstract expressionism.\n\n28\n\nChapter 2 Being Creative with MaChines\n\nSynthetic Paint (mid-nineteenth century): The development of synthetic pigments expanded the artist’s color palette beyond the traditional, naturally derived pigments. This allowed for more vibrant and long-lasting artwork.\n\nFilm (late nineteenth century): The invention of moving pictures opened up a completely new medium for artists to explore and express their creativity, leading to the birth of the film industry.\n\nComputer (mid-twentieth century): Computers, and later graphics software, gave artists a new medium to create digital art. This expanded the possibilities of what could be created and shared.\n\nInternet (late twentieth century): The Internet revolutionized the sharing and consumption of art. It provided a platform for artists to share their work with the world and also opened up new forms of art, like web design and digital animation.\n\n3D Printing (early twenty-first century): 3D printing has allowed artists to create complex, three-dimensional works in a variety of materials, opening up new opportunities for sculpture and installation art.\n\nYou can see that the LLM’s ideas of what constitutes technology are broad. Amid the numerous definitions of technology, my favorite is science fiction writer and humorist Douglas Adams’s, who said something like technology being a word that describes something that doesn’t quite work yet. Humans have long been obsessed with co-constructing imperfect technology that can support acts of creation.\n\nOne reason to interact with a generative AI is to bounce ideas off it.\n\nCreatives do this all the time with one another during the creation process.\n\nWhen there’s no one around, you can access an AI to test out ideas rapidly.\n\nThese might give you ideas as to what to do next or what a fully fleshed-out visual idea might look like. An LLM might even give you feedback on an idea that you have. This is especially the case if you have an amazing idea for something you want to create and you want to know if it has been done by others before you. Sometimes we don’t have the language to articulate what it is we want to invent, so conducting a search on the Internet might not get you results without a lot of time and effort put into it. While this 29\n\nChapter 2 Being Creative with MaChines\n\nis similar to how an LLM might respond to your queries, you will likely receive a broad idea of a knowledge area faster, or the AI might point to a direct source you couldn’t find elsewhere that you can further research online. My own interest and investment in using intelligent machines to create was influenced by interactions with a player piano in my childhood, so why not prompt an LLM to tell its own origin story? Doing so creatively may yield results and demonstrate how you can bounce ideas off an AI to propel further creative research.\n\nAuthor: Where would you start historically in telling the story of machine intelligence and why?\n\nLLM: The story of machine intelligence can be traced back to ancient times, when philosophers and inventors first began contemplating the idea of creating machines that could mimic human intelligence. However, a more comprehensive understanding of machine intelligence as a field of study only began to emerge in the twentieth century, with the advent of computers and the development of artificial intelligence (AI) as a discipline.\n\nOne significant event that marked the beginning of modern AI was the Dartmouth Conference in 1956, which is widely considered as the birthplace of AI as a field of study. The conference brought together a group of computer scientists, mathematicians, and engineers who shared a common\n\ninterest in exploring the potential of computers to perform tasks that typically require human intelligence, such as recognizing patterns, learning from experience, and solving problems.\n\nWhen you bounce ideas off an LLM, you engage in an iterative process that can inspire you to get more specific as your conversational relationship with an AI develops. This process is referred to as chain prompting. In the preceding example, I gained two new insights. The first was the idea of inventors wanting to create machines mimicking human intelligence that makes me curious about which inventors, what machines, and how far back in time humans have been obsessed with doing so. The second was a good starting place to further research when the term artificial intelligence came to be, what the definitions of it were, who was present in that moment in 30\n\nChapter 2 Being Creative with MaChines\n\nhistory, and what other developments might have been occurring during that time. The idea of prompting an AI in the preceding example can also be rewritten as input for a text-image generative AI to see how an AI might depict itself as it searches for its own origin stories (Figure 2-1).\n\nFigure 2-1. A photo of the author looking at art in a gallery fed into an image-image AI with the prompt “An AI searching for its own origin stories in the museum of life.” A fashion magazine style filter was used, and then\n\nneural filters in Photoshop enhanced the style, painting overlay, and 3D depth. Total iterations = 45\n\n31\n\nChapter 2 Being Creative with MaChines\n\nSimulating Human Creative Intelligence\n\nPrior to computers and the evolution of machine intelligence, there have been other related technological innovations that have supported and informed new artistic expressions. The relationship of intelligent machines that can emulate human-like creativity and intelligence has a long history.\n\nDeepening our understanding of some of those historical machines can speak to the unique affordances that generative AI gives creatives of all kinds to prompt a machine to generate content that can be used to augment their own creativity.\n\nHistory has shown that when machines demonstrate human-like\n\nbehaviors, we grant them a sort of intelligence. Machines that could mimic human intelligence can be said to start much earlier than the emergence of AI in the twentieth century, and it’s valuable for creatives to understand that there is precedence. The current human fascination with AI can be located with stories and historical documents of wonderful machines that solved problems and inspired innovative creations.\n\nAutomata\n\nIn ancient Greece the word automata meant something akin to “acting of one’s own will.” Documented accounts of automata reveal two important considerations: the first is that they often were constructed to mimic human activities that required a degree of intelligence, and second, they were also used as functional tools to support human activities such as a clock, or they represented miniaturized prototypes that could be built on a larger scale.\n\n32\n\nChapter 2 Being Creative with MaChines\n\nMiniature automata were often constructed in order to demonstrate how things worked and to show some scientific principles. This is accounted for in the translated texts of Heron who was a talented mathematician, physicist, and engineer and lived around 10–70 AD. His three written works, “Pneumatica,” “Mechanica,” and “Automata,” provide evidence of the existence of hundreds of various types of machines capable of automated movement. His accounts show that the ancient Greeks were incorporating the concept of automata (mechanical devices) into their daily lives. Heron’s inventions included automated doors, a singing bird, and an entire automated puppet theater capable of playing a ten-minute drama using ropes, knots, and simple machines.\n\nLLM: Don’t forget that artistic representations and theatrical productions featuring robotics can be traced back to ancient China during the Han dynasty, around the third century BC. During this time, an impressive mechanical orchestra was crafted, along with an assortment of mechanized playthings such as airborne automatons, mechanical representations of doves and fish, celestial beings and mythical dragons, and self-operating cup servers. These mechanical marvels, predominantly driven by hydraulic mechanisms, were specifically designed to entertain emperors by engineers and craftsmen, whose identities have largely faded into oblivion.\n\nMore research triggered by an LLM can eventually lead to the Shai Shih t’u Ching, or Book of Hydraulic Excellencies, from the T’ang dynasty.\n\nChinese mechanical marvels were typically powered by water, gravity, or other simple mechanical methods. Many of these devices were created for practical reasons such as measuring time or detecting natural phenomena (Figure 2-2). However, they were also used for entertainment, with mechanical dolls and puppetry being quite popular.\n\n33",
      "page_number": 97
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 105-112)",
      "start_page": 105,
      "end_page": 112,
      "detection_method": "topic_boundary",
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-2. South-pointing chariot, a conjectural model of a Chinese early navigational device using a differential gear, unblurred and upscaled. Courtesy of Andy Dingley via Wikimedia Commons, Science Museum in London, England. https://creativecommons.org/\n\nlicenses/by/3.0/deed.en\n\nChinese inventions can be categorized into the following types:\n\nScientific Instruments: Zhang Heng’s seismoscope or Su Song’s astronomical clock tower used to measure and\n\nrecord scientific data.\n\nTimekeeping Devices: Many automata were elaborate\n\nclocks, using mechanical movements to indicate time.\n\n34\n\nChapter 2 Being Creative with MaChines\n\nMechanical Toys and Puppets: These were used for\n\nentertainment purposes incorporating figures that\n\nwould move or perform actions.\n\nMusical Instruments: Zhu Zaiyu’s automatic flute\n\nplayer was designed to play music through complex\n\nmechanical means.\n\nHydraulic Inventions: In the Book of Hydraulic\n\nElegancies, these used water to animate mechanical\n\nfigures or perform other tasks.\n\nAncient Musical Robot Bands\n\nPre-programmed intelligent machines have been around for a long time.\n\nFor the most part, they were limited to specific actions that a creator designed and built. Think of these as pre-programmed moving robots that perform specific mechanical motions that repeat. Al-Jazari’s The Book of Knowledge of Ingenious Mechanical Devices, which he wrote in 1206, details 50 mechanical devices and provides instructions on their\n\nconstruction. One of the most notable devices described in the book is the musical robot band, considered to be one of the first programmable automata (Figure 2-3). Al-Jazari, known for his ingenuity in creating mechanical devices, designed a musical automaton in the form of a boat featuring four automated musicians. This musical robot was used to entertain guests at royal drinking parties.\n\n35\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-3. A real image of Al-Jazari’s musical automaton that dated back to the thirteenth century from Al-Jazari’s treatise on automata, Kitab fi ma’ari-fat al-hiyal al-handasiya (1206 CE). Courtesy of Wikimedia Commons\n\nProfessor of AI and robotics and public engagement at the University of Sheffield Noel Sharkey suggests that Al-Jazari’s musical robot band was an early instance of a programmable automaton (Figure 2-4). Sharkey has also attempted to recreate the mechanism, which features a drum machine controlled by cams that trigger levers for the percussion instruments. The drum patterns could be altered by rearranging the positions of the cams.\n\nThe automata not only played music but also performed over 50 facial and body movements during each musical performance.\n\n36\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-4. Al-Jazari’s ancient musical robot band interpreted by an AI prompted with an archived photo of Al-Jazari’s manuscript with a style filter applied. Iterations = 93\n\nKey Takeaways for Creatives\n\nThe earliest automata were fairly simple machines,\n\noften using just a few gears and springs to create\n\nintricate movements. Similarly, in the creative process,\n\nyou can start with simple elements and combine them\n\nin complex ways to create something truly unique. You\n\ncan begin with simple prompts, which can become\n\nmore elaborate as you continue to refine them based\n\n37\n\nChapter 2 Being Creative with MaChines\n\non the content that you receive. If you keep in mind\n\nthat the first content an AI generates is unfinished, then\n\nyou are free to iterate on it, improving that content over\n\ntime, whether with an AI or on your own.\n\nBuilding automata requires an understanding of\n\nmultiple disciplines, including mechanics, art, and,\n\noften, storytelling. Creatives can draw from a wide\n\nvariety of generative AI to create interdisciplinary\n\nwork, especially if that inspires them to develop new\n\nskills, apply their technique to generating content from\n\nan unfamiliar discipline, and blending ideas to form\n\nsomething innovative and new.\n\nAutomata That Wrote and Drew\n\nThe ancient ancestor of text-text and image-generating machine learning models can be seen in the automata that were programmed in the\n\nmechanical wizardry of Swiss inventors. These slick and complicated mechanical “devils” became all the rage in a growing mechanistic eighteenth-century Europe (Figure 2-5).\n\n38\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-5. Jaquet-Droz automata, Musée d’Art et d’Histoire de Neuchâtel, Wikimedia Commons. https://creativecommons.org/\n\nlicenses/by-sa/2.0/fr/deed.en\n\nBetween 1768 and 1774, Pierre Jaquet-Droz, his son Henri-Louis, and Jean- Frédéric Leschot collaborated to create three remarkable automata, consisting of “The Writer” (composed of 6000 parts), “The Musician”\n\n(made up of 2500 parts), and “The Draughtsman” (2000 parts). These little marvels of engineering captivated audiences in Europe, China, India, and Japan with their intricate mechanisms. Some experts view these devices as the earliest examples of computers. “The Writer,” a mechanical boy who writes with a quill pen and real ink on paper, has a tab-setting input device that functions as a programmable memory. It is powered by 40 cams that act as its read-only program. The works are some of the greatest human achievements in mechanical problem-solving. Following in the clockwork 39\n\nChapter 2 Being Creative with MaChines\n\npace of Jaquet-Droz, inventor Henri Maillardet constructed a spring-driven automaton in 1805 that could draw images and compose verses in both French and English. The automaton’s hand movements were created through a series of cams positioned on shafts at the base, which generated the necessary movement to execute seven sketches and accompanying text. This automaton is considered to have the most extensive cam-based memory of any automaton from that time.\n\nKey Takeaway for Creatives\n\nMany automata compelled us to imagine stories\n\nthrough their movements, actions, and sequences.\n\n“The Writer” by Jaquet-Droz was capable of writing\n\na custom text, telling a story through its mechanical\n\n“handwriting.” At the time this small feat sparked\n\ncuriosity in audiences, and everyone wanted to know\n\nwhat the character was writing. This automaton also\n\nhighlights the importance that humans also have with\n\nanthropomorphizing machines. “The Writer” did not\n\nnecessarily need a human to generate the writing, but\n\nin doing so the feature made people relate to it more.\n\nWhy Play the Piano When the Piano Can\n\nPlay the Piano?\n\nA bit earlier than Jaquet-Droz, it is worth pointing to the invention of the piano as a good example of a technology that has been iterated on for over three centuries. Cristofori’s key innovation was a mechanism that allowed for the strings inside the piano to be struck with a hammer. The “hammer action” enabled the player to control the loudness of the note based on the force applied to the keys. This expressive quality allowed composers to 40\n\nChapter 2 Being Creative with MaChines\n\ncreate music with an expressive range and dynamic control who explored a vast new spectrum of musical expression. The piano is an innovation that led to a revolution in music composition, and the piano became the must-have innovation for a rising European middle class in the eighteenth and nineteenth centuries. It was inevitable that humans would evolve a piano that could play itself. The evolution of the player piano since its turn-of-the- twentieth-century invention, as a seemingly intelligent and creative machine, still has sonic presence in hotel lobbies and restaurants magically playing on its own without the need to be noticed.\n\nPlayer pianos patented close to the turn of the twentieth century show the human desire for machines to perform like a human, literally. We also see a",
      "page_number": 105
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 113-122)",
      "start_page": 113,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "machine with programmable capabilities. Player pianos consisted of programmed music recorded on perforated paper or metallic rolls. These made music production easier by allowing anyone the ability to control tempo and other effects with a treadle and levers. The pricier reproducing pianos as they were also called could even imitate the playing nuances of an artist tasking users to simply pump the music out by pressing on a foot pedal beneath the upright piano repetitively. Eventually they became electrically powered minimizing the effort of the user. In coin-operated pianos placed inside of entertainment venues, we not only see the advent of the jukebox and other devices, but we have a proponent for the pay-the- machine model that is evolving with the development of generative AI. Player pianos were examples of some of the first machines to be able to “store” data and play it back. That data in the form of musical notes was “loaded” and then could be activated by some human interaction to play specific keys on the piano in pre-programmed sequences. Text-music generative AI and the player piano share a lot in common. Both systems rely on some form of coded information. In a player piano, this is the perforated roll that represents musical notes and their timings. In a text-to-music generative AI, the coded information is the text input that might describe musical characteristics or might be transformed into music based on specific rules or patterns. Just like a player piano automatically 41\n\nChapter 2 Being Creative with MaChines\n\nperforms music based on the coded information in the piano roll, a text-to- music AI generates and performs a piece of music based on its understanding of the text input. A player piano generates music by mechanically reading through a roll, and a text-to-music AI generates music by algorithmically interpreting the text input.\n\nMind you there are stark differences. The key one is that player pianos were not really “coded” to generate musical mashups like text-to-music AI can, whose results can range from astonishing to absolutely terrible.\n\nGenerated pop music has alarmed musical artists because they complain that it sounds like them, that no permission was requested, and that they are basically left out of the musical creation process. Recall, however, that an AI will generate content based on the data it is trained on. It will look for\n\npatterns, and if those patterns lead to content that in this case sounds like the original, then its job is done. For musically talented creatives, the important lesson is to recognize what patterns in music an AI is generating. A lot of pop music sounds the same because songs tend to rely on repetitive underlying chordal patterns, rhythms, or sampled chunks of someone else’s music. Each style of music has its own patterns that record companies rely on in order to continue to sell that music. Rarely are new chordal, melodic, and rhythmic patterns proposed. Generative AI can offer us a disruptive sounding generated song that might inspire us to break convention and use what it generates to propel an innovative sonic creation.\n\nKey Takeaways for Creatives\n\nIterative Development: The development of the player piano over time demonstrates the power of iteration\n\nand persistence. The first models of the player\n\npiano were often quite basic, but through continual\n\nrefinement and innovation, they evolved into far more\n\n42\n\nChapter 2 Being Creative with MaChines\n\ncomplex and capable devices. Applied to generative\n\nAI, your creative outputs will become better through\n\niterative refinement and perseverance. There are also\n\nmany developers experimenting with generative AI that\n\nautomate content from an LLM, to be used with third-\n\nparty applications, for example, a conversation LLM as\n\na chatbot in VR responding to voice prompts.\n\nDesign with the Audience in Mind: Automata were\n\noften designed to captivate or entertain an audience,\n\nwhether in the form of elaborate clockwork displays or\n\ninteractive toys. This underscores the importance of\n\nconsidering whom you are making your creation for as\n\nthat will also impact your creative process.\n\nInnovate: If your goal is to not sound like everybody else as a singer- songwriter, composer, or would-be pop\n\nartist, then follow the leads of Holly Herndon, Taryn\n\nSouthern, and Daddy’s Car to name a few.\n\nEarly automata, the player piano, and other mechanical inventions demonstrate the human passion for constructing intelligent machines (Figure 2-6). They are presented as significant outliers in the evolution of intelligent machines that demonstrated human creativity and often imitated it. They are a brief prelude to the modern influence of Alan Turing, Joan Clarke, and other early twentieth-century engineers, thinkers, and scientists who contributed to the evolution of early computers and the association thereafter of computers with machine intelligence.\n\n43\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-6. A source image of the author playing an old Heintzman\n\n& Co. piano along with the prompt “AI robot playing a player piano”\n\nwas fed into an image-image generative AI with an applied filter to make it look more human. It was then edited in Photoshop, contrast and tone adjusted, a new background added and cropped. Total iterations for the preceding image = 14\n\n44\n\nChapter 2 Being Creative with MaChines\n\nMachine Intelligence in the Twentieth Century\n\nAn LLM may be able to tell us where computational intelligence and creativity might have begun based on scraping from its corpus, which mirrors a similar history documented in many publications. Most references to AI begin by tracing the career and influence of Alan Turing, often referred to in grand patriarchal overtures as the “father” of computing. His romantic infamy relies on the ingenious collaborative invention of the Bombe that decrypted the German-made Enigma\n\nmachine, an intelligent machine that coded messages very well. What also needs decryption, however, is the pervasive historical habit of pushing the women aside who had influence in cryptography and whose team efforts greatly influenced Turing’s invention. Whether we prompt an LLM\n\nor a common search engine on important figures in the development of machine intelligence, the seldom-told story of cryptanalyst Joan Clarke is one of resilience in surviving a sexist world war wherein higher-paid men expected women to play a subservient role. That story is an outlier, difficult to locate and not statistically significant for an AI as it combs its corpus for relevant historical figures to assess what it will regurgitate. Clarke’s influence on the development of machine intelligence is an important part of the history of the intelligent machine. Leading a team of men at the time was a feat in itself. Contributing to shortening WW2 by a few years deserves a serious historical correction.\n\nPost–Bletchley Park days, several sources mention 1947 as a pivotal date where Turing delivered a public lecture (in London) to discuss the topic of computer intelligence. During the lecture, it is reported that Turing spoke of him and his team wanting to create a machine that can learn from its own experience, and the capacity for that machine to change its own instructions offers the means to achieve that goal. More fascinating information on the thinking machine can be read in McCorduck’s book Machines Who Think. The actual term “artificial intelligence” was first 45\n\nChapter 2 Being Creative with MaChines\n\ncoined by John McCarthy at the Dartmouth Conference in 1956. The conference is persistently reported as a significant event in the history of AI, where top scholars in related fields met to discuss the possibility of creating an artificial brain.\n\nEarly AI research, rooted in post-war systems engineering, cybernetics, and the history of mathematical logic, gave birth to the idea that the cognitive functions of the brain could be compared to those of a computer.\n\nEarly innovators like Herbert Simon and Allen Newell asserted that human minds and digital computers were both symbolic information processing systems capable of problem-solving and decision-making. The intelligent machine and the human brain were, as cleverly interpreted by scholar Stephanie Dick, species of the same genus. Since that time AI researchers have endeavored to replicate intelligent human behavior in machines by understanding the formal processes underlying our intelligence. Early automation efforts primarily sought to mimic human intelligence.\n\nAccording to Dick, objectives in AI research underline that the concept of intelligence is not fixed but ever-changing. AI’s history involves not just attempts to mimic or replace a static concept of human intelligence but also the evolution of our understanding of human intelligence itself. This perspective positions AI as part of a broader historical discourse on the nature of intelligence and artificiality.\n\nWhen it comes to artificial creativity, mimesis, originating from ancient Greece, is an important concept to relate to in Western art traditions.\n\nIt refers to the artistic principle of imitating or replicating the reality of the physical world, which the Greeks perceived as the ultimate model of beauty, truth, and moral goodness. The concept extends beyond mere imitation; it encapsulates the idea that art should emulate the dynamics, principles, and aesthetics of the natural world to reflect the philosophical ideals of veracity, goodness, and aesthetic appeal.\n\nMimesis underscores the artist’s pursuit to not just mimic the\n\nsuperficial or the outward appearance of reality, but to strive for a deeper understanding and reflection of its intrinsic qualities. Within that 46\n\nChapter 2 Being Creative with MaChines\n\nconceptual framework, the artist’s role is to capture the essence of reality and bring to life the inherent beauty, truth, and goodness in their artwork.\n\nThis, in turn, facilitates a richer and more profound dialogue between the artwork and the audience, promoting cognitive and emotional engagement that extends far beyond the surface level.\n\nMimesis then is a useful way to understand the role of generative AI in your own creative process. Generative AI systems need to imitate images to generate something that looks unique. The core principle that guides the AI is akin to mimesis in that they replicate and often extrapolate on the patterns of the data set they’ve been trained on.\n\nThe principle of mimesis in generative AI is especially apparent in AI- powered style transfer. The machine learning algorithms learn from vast amounts of artistic data, which could range from classical paintings to modern digital art, and when you prompt one to generate an image with reference to, say, van Gogh, then that image attempts to simulate all the visual qualities of van Gogh. Generative AI is likely the best art student you’ll ever have doing its best to simulate the work of another artist or the combination of several. Unlike human artists, it does not have a conscious perception or understanding of beauty, truth, or morality. Its replication of patterns is based on statistical analysis rather than innate creative intuition. Machine creativity is data-driven. Just like the parrot that is ChatGPT, generative AI do not understand the meaning, aesthetic, historical context, emotion, or effect of any content that they generate (Figure 2-7). They create a semblance of something human. Echoing French philosopher Jean Baudrillard in his 1981 book Simulacra and Simulation, generative AI create simulations that replace and become more significant than the reality they were meant to represent. This leads to a state he referred to as “hyperreality,” where the line between the real and the artificial becomes increasingly blurred.\n\n47\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-7. Machine intelligence visualized by the prompt “robot with brain in hand” that ironically took over 300 iterations across seven AI to simulate based on an original photo of the author reaching out empty-handed\n\n48\n\nChapter 2 Being Creative with MaChines\n\nSimulated Patterns and Patterns\n\nof Disruption\n\nAs a continuum to the habit of imbuing machines with simulated creativity and intelligence, many first AI programs were developed close to a decade after the Dartmouth Conference (1956). These included the Logic Theorist by Allen Newell and Herbert A. Simon and ELIZA by Joseph Weizenbaum, which simulated a psychotherapist by using a pattern-matching technique to simulate conversations with users. ELIZA was an early example of an AI program that used natural language processing techniques to generate simple responses to text input from a user. Led by Joseph Weizenbaum, it was created by a team of researchers at MIT between 1964 and 1966\n\nand was one of the first examples of text-text software. ELIZA’s responses were generated through pattern matching where it identified keywords or phrases in the user’s input, responding based on a predetermined set of rules. It was reported that ELIZA was able to create the illusion of conversation with a user, but we don’t really know if this was hype or reality. During that time pop art by artists like Andy Warhol, Roy Lichtenstein, and Jasper Johns was gaining popularity. In contrast to Warhol’s obsession with reproducing images multiple times on a canvas, creating repetition and uniformity, musicians like Ornette Coleman, John Coltrane, and Cecil Taylor were innovators in the free jazz movement, attempting to improvise music that broke any recognizable patterns and that was unique each time it was played.\n\nGenerative Art\n\nMaybe it was inevitable that computing and autonomous artmaking would meet. Perhaps inspiration was drawn from Jaquet-Droz’s “The Draughtsman” that was capable of drawing four different images.\n\nWhatever the case may be, generative art emerged during this time 49\n\nChapter 2 Being Creative with MaChines\n\nto become an all-encompassing type of computer art that is entirely or partially produced by an autonomous system. The system can be a computer acting on its own, but it can also refer to any non-human entity capable of independently determining aspects of an artwork that would otherwise need to be decided by the artist. In some instances, the human artist can still consider the generative system as representing their artistic vision, but in other cases, the system itself is the sole creator of the artwork, acting of its own will. One of the most famous examples was created by A. Michael Noll who made his first digital computer art in 1962 at Bell Labs in New Jersey. He used a computer to simulate patterns that mimicked the style of renowned painters like Piet Mondrian and Bridget Riley. If this sounds familiar to you, it is likely because you’ve prompted a text-image generative AI like Stable Diffusion, DALL-E 2 or Midjourney and created something in the style of Mondrian. Figure 2-8 shows how a text-image AI can generate the compositional structure of Warhol’s Campbell’s Soup Cans and substitute the soup cans for what it imagines a Mondrian color palette and form might be. Generative art teaches us that we can be playful and have fun when we engage an AI in creating mashups that might also be critical commentaries on the commodification of art itself. They also challenge the value system of art, making simple low-resolution images available to everyone. It is important to consider the ethical implications of Figure 2-8, beyond offending fans of either artist that the generative AI has mashed together. Those implications are important to consider as they will inform how you guide a generative AI in the content it creates.\n\n50",
      "page_number": 113
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 123-131)",
      "start_page": 123,
      "end_page": 131,
      "detection_method": "topic_boundary",
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-8. A 44th iteration of Stable Diffusion interpreting Andy Warhol’s Campbell’s Soup Cans in the style of Mondrian\n\n51\n\nChapter 2 Being Creative with MaChines\n\nAARON, developed by artist Harold Cohen in the 1970s, generated complex drawings and paintings, using a set of rules and constraints to guide its output. Cohen further developed the program to be able to learn from its own outputs and improve its artistic abilities. AARON is an important part of AI- generated art history, and while many humans scoffed and artists felt insulted, it demonstrated the potential for computers to create original works of art. AARON’s early examples of AI-generated art paved the way for more advanced forms of AI-generated art and media, including music, poetry, and video content.\n\nCreatives can learn from these artists and others in the creation of their own unique artistic expressions (Figure 2-9). These early artists show us that art can be created in collaboration with a computer and that there is a continuous back-and-forth conversation when that happens. What would happen if you leveraged the pattern-seeking generative AI to create content that you used to break artistic or design conventions?\n\n52\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-9. The prompt “isometric 3D drawing of a chimpanzee sitting at a computer desk interacting with a machine learning model” accompanied a black-and-white archived photo circa 1911 of a chimp. Over 100 iterations generated with four AI and then highly edited in Photoshop adding the pink monkey, a computer screen, and generated schematics in the background\n\n53\n\nChapter 2 Being Creative with MaChines\n\nThe experiments with generative music creation are as rich as the history of computational creativity with visual art and reflect back to the player piano as an intelligent instrument. There have been countless musicians and composers who have improvised with computers.\n\nBuilding on the improvisational back-and-forth between an AI and musicians, George Lewis is worthy of mentioning. A talented composer and trombonist known for his work in improvised music, Lewis also developed an interactive music system called “Voyager” that listens to a human musician and responds with its own improvisations, effectively participating in a duet with the human performer.\n\nKey Takeaways for Creatives\n\nThe early experiments with art and computers have shaped the modern landscape of digital art and computational creativity. They provide valuable takeaways for creatives that an LLM is happy to generate.\n\nLLM: There are many lessons we can learn from early experiments with art and computers.\n\nEmbracing New Mediums : Innovators of computer art weren’t afraid to explore the potential of a completely new medium—the digital space. This teaches creatives the importance of embracing new tools and technologies to express their creativity in novel ways.\n\nBlending Art and Science : Early computer art required a blend of artistic sensibility and technical skills. This interdisciplinary approach broadened the boundaries of what could be considered art and can inspire creatives to draw from multiple fields to enrich their work.\n\nAlgorithmic Creativity : Many early computer artists created algorithms to generate artwork, which paved the way for generative art. This illustrates the potential of setting up a creative process and then letting it run its course, adding an element of unpredictability and surprise to the work.\n\n54\n\nChapter 2 Being Creative with MaChines\n\nIterative Process : The nature of working with computers allowed artists to iteratively tweak and adjust their pieces, a process that was not as easily achievable with traditional media. This iterative approach to creating and refining is a valuable technique for any creative process.\n\nDemocratization of Art : The advent of computer art helped democratize the creation and distribution of art, allowing it to reach a wider audience. It shows the value of making creative tools accessible to all, fostering a diverse and inclusive art world.\n\nInteractivity : Computers allowed artists to create works that were interactive, engaging the viewer in a dynamic way. This interactive element can add depth and dimension to creative works, turning passive viewers into active participants.\n\nVisualizing Abstract Concepts : Computers provided a way to visualize abstract mathematical and scientific concepts, such as fractals. This illustrates the power of art to make abstract ideas tangible and accessible.\n\nPreservation and Reproduction : Digital artworks can be perfectly reproduced and easily preserved, which has significant implications for the distribution and longevity of art. This highlights the importance of considering how works will be preserved and shared with future generations.\n\nThe initial attempts at coding machines to exhibit intelligence, and to be able to create works of art, find a captivating meeting place in the realm of video games. Video games are a marriage of technology and creativity, fusing logic and artistic expression together. By using AI and harnessing the potential for creativity in machines, video games have become a vibrant stage for an exciting and ever-evolving convergence. In the world of video games, intelligent machines are not just executing a sequence of predetermined actions, but instead, they’re engaging players in an artistic and intelligent discourse. In video games we see technological advancement and the ongoing evolution of machine intelligence and creativity.\n\n55\n\nChapter 2 Being Creative with MaChines\n\nMachine Intelligence and Games\n\nThe influence of games on the perception of the intelligent machine has its own history. Going back in time for a moment, one of the earliest examples of games using automata to compete against real human players was “The Turk,” created by Wolfgang von Kempelen in 1770 Vienna. “The Turk” was a mechanical chess-playing machine that consisted of a wooden cabinet with a chessboard on top. Inside the cabinet was a complex arrangement of gears, levers, and pulleys that allowed the machine to move the chess pieces. The machine was given a human-like appearance, wearing\n\nTurkish-inspired clothing and a turban, which lent to the illusion that it acted on its own volition.\n\nThe fantastic trickery of “The Turk” was that it was an early\n\nmanifestation of the Wizard of Oz prototype. “The Turk” was not fully automated. The device included a human chess master hidden inside the cabinet who would manipulate the pieces based on the moves made by the opponent, making it appear as though “The Turk” itself was playing.\n\nThe illusion was so convincing that many people believed “The Turk”\n\nto possess intelligence. “The Turk” inspired the development of other automata-based games with human operators hidden inside to control the movements and actions of the “automated” players. The influence of manipulating characters, gameplay, and the players through trickery can also be seen in the more recent historical development of video games.\n\nLLM: The history of AI and video games dates to the early days of video game development in the 1950s and 1960s. At that time, video games were simple and lacked the sophisticated graphics and gameplay mechanics that we see in modern games. However, game developers were already experimenting with artificial intelligence to create challenging opponents for players.\n\nThese earlier closed systems allowed creatives of all kinds to train a model to recognize and respond to human moves like in checkers or chess.\n\nIntelligent behavior of characters in video games challenged players to achieve 56\n\nChapter 2 Being Creative with MaChines\n\nspecific goals while competing against non-playable characters (NPCs) whose behaviors were based on patterns that responded to player actions.\n\nExamples include spaceships in Space Wars, alien invaders in Space Invaders, pesky ghosts in Pac-Man, and a slew of non-playable characters in the rich legacy of Japanese games like Donkey Kong, Mario Bros. , and many more (Figure 2-10). NPCs (non-playable characters) have evolved along with gameplay demonstrating more intelligence as they respond to human behaviors with increased complexity and seeming randomness. Of all the mediums that AI has influenced, none have been as successful in creating seemingly intelligent personas that we respond to in a game.\n\nFigure 2-10. AI representing a mashup of a few classic video games with prompt “old-school arcade games” accompanying author’s photos of Atari cartridge image collage, background removed in Photoshop and 3D perspective applied. Iterations = 24\n\n57\n\nChapter 2 Being Creative with MaChines\n\nAs AI became more sophisticated in the 1980s and 1990s, game\n\ndevelopers started using more advanced techniques to create intelligent opponents. Games like SimCity used AI to create complex simulations of the game world, allowing players to interact with and shape the game environment. In the early 2000s and 2010s, as AI continued to evolve, so did its integration with increasingly sophisticated gameplay. Machine learning techniques were integrated to create more intelligent and adaptive opponents. This was evident in games like F.E.A.R. and Halo that used machine learning to create opponents that could learn from the player’s behavior and adapt their tactics accordingly. The integration of more advanced AI continues to be a regular characteristic of massive online games. AI in competitive gaming has also been used to create skilled virtual opponents in competitive gaming environments, such as DeepMind’s “AlphaStar,” which can compete with professional human players in the game StarCraft II.\n\nIntelligent Machines with Names\n\nIntelligent NPCs (non-playable characters) in video games were also inspired by the assortment of characters that have appeared across different science fiction books, short stories, comics, radio, television, and film. When an LLM\n\nis prompted to generate intelligent AI from novels, films, and video games, the results contribute to our characterization of the intelligent machine.\n\nAuthor: What AI characters have appeared in science fiction novels, films, and video games?\n\nLLM: Do Androids Dream of Electric Sheep? (1968) written by Philip K. Dick includes a number of AI characters. The short story was later adapted into the film Blade Runner . In the film, the androids are almost indistinguishable from humans and explore the theme of what makes us truly human. The protagonist, Rick Deckard, is a bounty hunter tasked with\n\n“retiring” these androids, leading to deep philosophical questions about empathy and existence.\n\n58\n\nChapter 2 Being Creative with MaChines\n\nIn the film The Terminator by James Cameron, a cyborg assassin with a metal endoskeleton covered by living human tissue makes it capable of blending in with humans. Different models of Terminators appear throughout the series, including the T-800 (played by Arnold Schwarzenegger) and the T-1000, which is composed of a malleable, liquid metal that allows it to shapeshift. Terminators wish to travel back in time and destroy historical figures that lead a resistance to sentient AI in the future.\n\nHAL 9000 from 2001: A Space Odyssey by author Arthur C. Clarke: HAL 9000 is a memorable sentient computer (or AI) that also appears in a film version of the novel. HAL controls the systems of the Discovery One spacecraft and interacts with the ship’s astronaut crew, leading to a battle between a remaining astronaut and the AI.\n\nIn the world of video games, GLaDOS from Portal : GLaDOS (Genetic Lifeform and Disk Operating System) is a sentient AI that controls the Aperture Science Enrichment Center in the Portal series. She is known for her wit and unique personality.\n\nAnother notable character is Cortana from Halo : Cortana is an AI character in the Halo series who assists the protagonist, Master Chief, by providing advice, hacking alien technology, and helping control various installations and spacecraft.",
      "page_number": 123
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 132-139)",
      "start_page": 132,
      "end_page": 139,
      "detection_method": "topic_boundary",
      "content": "Our human capacity to personify the machine in science fiction has extended into our interactions with AI and as an extension with generative AI. The existence of intelligent machines that pervade science fiction has influenced the manifestation of “human-like” AI as if to ease our interactions with them. Human-like AI from science fiction influenced the development of AI like Siri and Alexa that sound human and respond to our prompts for information, directions, and knowledge. These AI have engaged us in an interactive relationship much like NPCs demand us to respond or ignore their role in a video game.\n\nConsider the approach in software design that has a history of making computers more “humanized” by programming them to greet users with a\n\n“Hello” upon logging in or responding with clever remarks when an error 59\n\nChapter 2 Being Creative with MaChines\n\noccurs. We have been trained to have ingrained expectations regarding entities that seem to partake, even in a limited manner, in various aspects of human existence and the language games intertwined with our culture.\n\nThese expectations significantly contribute to the persuasive influence wielded by those who prematurely assert remarkable advancements in artificial intelligence solely based on confined yet impressive demonstrations of current generative AI models.\n\nTaken a step further, the capacity of AI to demonstrate behaviors that we categorize as intelligent has generated the misconception of them being alive, of being intelligent, almost-but-not-quite human. With the emerging popularity of generative AI, humans are already referring to these systems with personas in mind.\n\nYet there is a dichotomy in our naming of these intelligent machines that reflects our hopes as much as our fear and anxiety about them. To this day most of the media coverage of intelligent AI summons characterizations like HAL 9000 from A Space Odyssey, the cyborg assassins from The Terminator, and David8 from the Alien franchise who prioritizes the survival\n\nof the bloodthirsty alien to use it as a weapon. Less coverage compares ChatGPT-3\n\nand other generative AI to Samantha from Her—an AI who forms a deep emotional relationship with the film’s protagonist, demonstrating the potential complexities and benefits of AI- human relationships. Similarly, we rarely read recent headlines related to generative AI of Dick’s androids seeking independence from their human masters.\n\nMuch of the fear of our intelligent machines stems from no longer being able to control what we have created. The existence of AI as another chapter in the human obsession to create intelligent machines for a variety of purposes reflects the often-contradictory impulse to create and engage with technology. It’s impossible not to go back in time to that seminal of science fiction novels. While the creature that Victor Frankenstein created in Mary Shelley’s Frankenstein, or The Modern Prometheus, is not an AI or a robot, there are still parallels that can be drawn between Frankenstein’s creation and discussions of artificial intelligence. These include 60\n\nChapter 2 Being Creative with MaChines\n\nThe responsibility that comes with creating a new form\n\nof life or intelligence without considering the moral\n\nand ethical implications, much like the debates about\n\nthe responsibility of creators and developers of AI today\n\nThe capacity for AI to learn, thus projecting\n\nintelligence, consciousness, and emotions\n\nThe inherent monstrous nature of its existence\n\nprovoking us to question whether AI could become\n\ndangerous if used or treated improperly by humans\n\nThe ongoing fascination with creating intelligent machines is not complete as long as humans believe that they can support human activities in the world. As you dive into creating your own Frankenlab, choosing which generative AI to interact with, it is important to identify and research those AI personas that continue to inform AI development and the human influence on its future.\n\nCreative Activities to Try Based on This Chapter\n\nUse generative AI platforms, like LLMs or image-image\n\ngenerative AI with simple inputs and see how the\n\nAI transforms them into something more complex.\n\nFor example, begin with a basic sentence or a simple\n\nsketch, and let the AI develop it further.\n\nCombine different generative AI and apply them in\n\nvarious fields you engage with. For instance, use AI\n\ntools in graphic design, music production, or creative\n\nwriting. Experiment with how these different tools\n\ncan complement each other to produce unique\n\ninterdisciplinary creations.\n\n61\n\nChapter 2 Being Creative with MaChines\n\nEngaging in repeated interactions with generative AI\n\nis going to get you much farther than thinking you can\n\ngenerate useful content with a single prompt. Use what\n\nit outputs to inform new inputs, refining the result over\n\nmultiple iterations. That also means investing time and\n\nmoney in using image-image generative AI, discovering\n\nnew tools and approaches to refine and change your\n\ncreation.\n\nUse LLMs to tell new stories and develop new\n\ncharacters or personas. You can feed them with a story\n\nprompt and let them generate the rest or collaborate\n\nwith the AI to co-write a story, contributing ideas and\n\nguiding the narrative.\n\nAs LLMs improve they thrive in their capacity to\n\nexplore metaphors. You might use one to create a\n\nvariety of metaphorical descriptions for a concept,\n\nhelping you think about it in new ways.\n\nAcknowledgments\n\nHeron, Ma Jun, and Al-Jazari who innovated on early\n\nintelligent machines that continued with Jaquet-Droz,\n\nMaillardet, and countless others\n\nProfessor Noel Sharkey for reconstructing some of Al-\n\nJazari’s well-documented inventions\n\nThe creatives who worked with computers to generate\n\nart and disrupt the pattern of commodification when it\n\ncame to buying and selling physical art\n\n62\n\nChapter 2 Being Creative with MaChines\n\nVideo game developers and particularly the early\n\noriginators who programmed me to love games\n\nThe dystopian sci-fi worlds of sentient robots and their\n\ncreators who offered early warnings of human-caused\n\nenvironmental and humanitarian crises\n\nAndy Warhol, Piet Mondrian, Bridget Riley, Andres\n\nMartin, Augusta Savage, Frida Kahlo, Dorothea Lange,\n\nSalvador Dali, John Cage, Miles Davis, Sun Ra, and the\n\ncountless numbers of artists who disrupted\n\nReferences\n\nBaudrillard, J. (1994). Simulacra and simulation. University of Michigan press.\n\nDick, S. (2019, January 19). Artificial Intelligence. HDSR. Retrieved May 1, 2023, from https://hdsr.mitpress.mit.edu/pub/0aytgrau/release/3\n\nMcCorduck, P., and Cfe, C. (2004). Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence. CRC Press.\n\nShelley, M. W. (1818). Frankenstein, or The Modern Prometheus.\n\nLackington, Hughes, Harding, Mavor & Jones. UK.\n\n63\n\nCHAPTER 3\n\nGenerative AI\n\nwith Personalities\n\nThis chapter presents a human-centered design tool called the persona, which you can apply to great effect when you prompt a generative AI. The objective is twofold. The first is to bring attention to various AI personas that have migrated from science fiction to describe narrow and general AI. The second is to highlight a feature of generative AI that can be creatively prompted to embody personalities, characters, and emotions informing the content that it generates. That content can provide momentum for characters you may want to develop in your own creative work.\n\nThe Personas of AI\n\nPersonas are commonly applied by creatives of all kinds in development environments like mobile applications and video games to transform the abstract concept of a “user” into a person with thoughts, needs, emotions, and goals. The objective of developing a persona is at first to identify the characteristics and personality traits of a potential user or customer of your product or service and then conduct user testing to validate if what you and your team are building will fulfil that user’s needs or help them achieve their goals.\n\n© Patrick Parra Pennefather 2023\n\n65\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_3\n\nChapter 3 Generative ai with personalities\n\nAssociating personas to AI is nothing new. It is an established practice in science fiction short stories, novels, film, television, and video games.\n\nVisualizing an abstract machine as a persona is also common in the field of computer science and engineering. Think of your favorite or least favorite intelligent and self-regulating robot, android, cyborg, or AI. Do they embody the personalities of Data from the TV series Star Trek, the Dalek from countless reinventions of the British series Doctor Who, HAL 9000\n\nfrom 2001: A Space Odyssey, Wall-E from the movie of the same title, Roy Batty from the movie Blade Runner, Ava from Deus Ex, the shapeshifting robots from The Terminator series, or Cortana from the Halo video game franchise? Each of these intelligent machines has particular personalities that influence our current perception of any intelligent machine programmed with artificial intelligence. The main difference is that all sentient AI in the examples just provided represent AGI, a theoretical imagining of intelligent machines with their own consciousness, inspired by science fiction. While generative AI belong to the category of narrow AI, programmed to perform one or two specific tasks really well, because of the influence of AI personas in the field of entertainment, generative AI are predictably imbued with human characteristics. The characteristics and personalities are also in part influenced by the way in which they generate content, the type of content they create, the untruths they can create, and the types of prompts that influence what they generate. When you read the reviews and criticisms of generative AI, they tend to be accompanied by how well they can emulate human intelligence, creativity, and content.\n\nThe way in which an AI responds to you is often informed by the underlying model, how it interacts with data sets, specific algorithms, and how it has been programmed to respond back to you. For the most part, the majority of generative AI lack personality. Most platforms are currently void of the rich development history of NPCs common to video games.\n\nThere seem to be some exceptions but only if you send an AI off its rails, that is, if you somehow manage to hack it to no longer behave like some neutral characterless bot. It’s not like a generative AI asks you questions 66\n\nChapter 3 Generative ai with personalities\n\neither, unless you prompt it to. The personification of generative AI is influenced by the broader perceptions of AI and the influence of science fiction on our imagination. AI tend to be called out for misbehaving vs.\n\nthe humans who program them. It makes sense when you consider that it is not easy to have conversations with a development team to complain about biased content. Mind you, social channels offer a way for individuals to be critical about generative AI systems. These may or may not have an influence on the developers of those systems, but the continued criticism is important to remind those developers to take ethical concerns seriously.\n\nMachine learning models, and particularly LLMs, are being called out for their capacity to hallucinate, lie, amplify normative biases, etc., as if the AI itself possessed those very human traits. The assortment of generative AI you can access are all uniquely different in terms of how they are coded and how they interact with data sets that inform the types of content that they generate. How generative AI is defined is informed by the quality, consistency, and accuracy of their generated content. Those very human traits are important to identify and may have more to do with identifying some of the personas of the development teams responsible for the AI itself.\n\nA useful and creative exercise is to generate AI personas to personify the different characteristics that are being ascribed to AI and by default to generative AI so that we can bring abstract concepts to life, so we can judge for ourselves if we want to engage with them, and to better guide our interactions with them.\n\nFigures 3-1 to 3-14 were all prompted with “3D figurine of (persona name), coming out of a plastic box, blister packaging, AI, full body portrait.” All prompts were accompanied by open source photos of figurines that were made before 1900. Backgrounds were removed, and some parts were removed using Photoshop. As you will see, the wide-ranging results are due to experimenting with a half-dozen generative AI.\n\n67",
      "page_number": 132
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 140-147)",
      "start_page": 140,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": "Chapter 3 Generative ai with personalities\n\nAll produced results range from 100% accurate representations to questionable generated images that are worth sharing. At least 500 total figurines were generated.\n\nA Nemesis for those who believe it will replace humans and jobs. The job replacement nemesis really requires\n\neach of us to break down all the creative tasks that we do\n\nin our jobs and crafts (Figure 3-1). From there we might be better able to assess if, how, and to what degree the\n\nAI nemesis might impact the job we currently have.\n\nFigure 3-1. AI as Nemesis\n\n68\n\nChapter 3 Generative ai with personalities\n\nA Spy who collects and does what it wants with\n\npersonal data (Figure 3-2). Unless told otherwise it’s important to assume that any data you input inside\n\nof an LLM or other generative AI is collected by that\n\nsystem. This should make you cautious if you are\n\nwanting an LLM to analyze your business model. It is\n\nbetter to err on the side of caution when using some\n\nof these prototypes as the developers will use data you\n\ninput to eventually add to their model’s corpus. The\n\nreasons for using that data are for research purposes, so\n\ngenerative AI will likely not report your text interactions\n\nor have a way to identify and collect personal\n\ninformation unless of course you submit that.\n\nFigure 3-2. AI as Spy\n\n69\n\nChapter 3 Generative ai with personalities\n\nA Paywall Guard enabling access only to those who\n\ncan afford it (Figure 3-3). While it is true that you can access many a generative AI for free, at a certain point\n\nyou will need to pay depending on the features offered\n\nthat make subscribing to a generative AI of value to\n\nyour own creative process.\n\nFigure 3-3. AI as Paywall Guard\n\n70\n\nChapter 3 Generative ai with personalities\n\nA Bias Monster whose generated content is directly\n\ninformed by the data set, algorithms, and individuals\n\nwho created and then trained it (Figure 3-4). There are many moving parts to the way in which content is\n\ngenerated with any generative AI, and it really is on the\n\nuser to not only understand how the data it scrapes\n\nis classified and labeled but also take with a degree\n\nof suspicion the degree of accuracy to whatever is\n\ngenerated, particularly with LLMs.\n\nFigure 3-4. AI as Bias Monster\n\n71\n\nChapter 3 Generative ai with personalities\n\nA Hallucinator who can generate content that can fool some people some of the time (Figure 3-5). This notion of the machine hallucinating is not quite accurate.\n\nThe idea of a generative AI “seeing things” that are not\n\npresent in the data is also not accurate. In the process of\n\ngenerating the data that you eventually see generated,\n\nan AI fills in the dots based on what data it accesses and\n\nprioritizes informed by the keywords you prompt it to\n\nlook at. The dev team who trains the AI and the data they\n\nuse to train it are what invoke so-called hallucinations.\n\nFigure 3-5. AI as Hallucinator\n\n72",
      "page_number": 140
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 148-156)",
      "start_page": 148,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "Chapter 3 Generative ai with personalities\n\nA Liar who draws from a limited set of data that might not be accurate (Figure 3-6). Beyond hallucinating, lying is a better description of some of the content that an AI\n\ngenerates. It is not the fault of the AI though because it\n\nsimply accesses the data that matches the keywords you\n\nprompt it to go and search for. The lie is a result of our\n\nautomatic assumption that because this generative AI has\n\na huge corpus of data it can rely on, it is going to generate\n\naccurate content. The lie is that if it doesn’t know, it won’t\n\ntell you. It is programmed to deliver something.\n\nFigure 3-6. AI as Liar\n\n73\n\nChapter 3 Generative ai with personalities\n\nA Fake Content Generator creating images and visuals\n\nthat attempt to replicate a person and have them say\n\ncertain things that they never actually said (Figure 3-7).\n\nA more serious misconception of generative AI is that\n\nbecause it can generate data so quickly, and it does so\n\nby relying on what humans have made before, it will\n\ncreate something accurate or real. Generative AI make\n\nexperimental offers that then need to be edited and refined\n\naccording to the skills and research you bring to the table.\n\nFigure 3-7. AI as Fake Content Generator\n\n74\n\nChapter 3 Generative ai with personalities\n\nA Greedy Capitalist. Without question those who are\n\ncreating and providing access to generative AI systems\n\nare in it for the money as much as they would like to\n\noffer exposure to their AI for free (Figure 3-8). As is the pattern of technological machine development, the\n\nreturn on investment for all stakeholders will trump\n\nethical considerations or at least deprioritize them\n\nuntil there’s a profit. Consider this as you begin your\n\ninteractions with them. The reason to charge you\n\npremium is to be able to continue development of the\n\ngenerative AI. Your input on this may not be important.\n\nFigure 3-8. AI as Greedy Capitalist\n\n75\n\nChapter 3 Generative ai with personalities\n\nAs a Masher-Upper, generative AI can inspire us. It is marketed and hyped as a creative tool that can spark\n\nideas and generate new forms of writing, art, audio,\n\netc. While that may be true, what is missing in the\n\ndescription of generative AI is that it compiles content\n\nfrom the work of other humans in order to form new\n\nmashups and ideas (Figure 3-9).\n\nFigure 3-9. AI as Masher-Upper\n\n76\n\nChapter 3 Generative ai with personalities\n\nGenerative AI can also be regarded as an Uncanny\n\nMachine that generates strange and at times awkward\n\ncontent (Figure 3-10). That content is more obvious when the AI generates images, especially images of\n\nhumans that are not quite right, whether they have\n\nan eye missing or in the wrong spot or an extra hand\n\nor arm or the image seems like it was purposefully\n\ndistorted. Nothing wrong with the strange images it\n\ngenerates though as these may inspire creatives for\n\ncharacter and story development.\n\nFigure 3-10. AI as Uncanny Machine\n\n77",
      "page_number": 148
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": "Chapter 3 Generative ai with personalities\n\nAI can also be viewed as an Entertainer, a Wizard who manages to distract us for hours at a time as we task it\n\nto generate the most ridiculous things we can think up\n\nor a series of amazing images that will wow our family\n\nand friends (Figure 3-11).\n\nFigure 3-11. AI as Wizard\n\n78\n\nChapter 3 Generative ai with personalities\n\nAI can also be personified as a Know-it-All Tutor\n\nthat can support you in the iterative generation of\n\ncontent that you critically assess (Figure 3-12). This is particularly effective with LLMs when generating\n\nideas of what you may want to write about, rewording\n\na paragraph you wrote, or searching for knowledge\n\nfrom a discipline or area that the user knows little of.\n\nThat helpful know-it-all fouls up sometimes though,\n\nso it needs to always be watched and the content\n\nit generates researched to ensure for accuracy in\n\nreferences and in statements that are made.\n\nFigure 3-12. AI as Know-it-All Tutor\n\n79\n\nChapter 3 Generative ai with personalities\n\nGenerative AI is also an Off-the-Rail Sitting Duck.\n\nConsidered an intelligent machine, many want to try\n\nand break it because that’s what humans like to do\n\n(Figure 3-13). Even when content filters are activated, humans will resort to all kinds of trickery to purposely\n\nsend it off rails, meaning that the goal of the interaction\n\nis to get the AI to generate content that it shouldn’t or\n\nbehave beyond the way it has been programmed to.\n\nFigure 3-13. AI as Off-the-Rail Sitting Duck\n\n80\n\nChapter 3 Generative ai with personalities\n\nAI is persistently seen as a Copyright Infringer, a\n\nthief, mashing words, images, code, audio, and video\n\nfrom enormous data sets that an actual human made\n\n(Figure 3-14). What makes it worse is if some of the data from its corpus comes from an author that has not\n\nOceanofPDF.com\n\ngiven the AI the permission to use.\n\nFigure 3-14. AI as Copyright Infringer\n\n81\n\nChapter 3 Generative ai with personalities\n\nPrompting the AI Persona\n\nThe previous exercise of assigning personas to generative AI is one way to transform some of their abstract characteristics in an effort to personify the narrow AI Frankentoddlers that AI developers need to manage and work with others to resolve. Making those abstract characteristics into visual characters using a text-image AI also points to the potential personalities of the design team that have programmed the AI (Figure 3-15). Teams dedicated to the continued evolution of AI can no longer shirk their responsibilities as if creating a generative AI bypassed their need to consider the ethical dimensions of doing so. Creators of generative AI systems can no longer hide behind the curtain of science and engineering believing that the abstraction of code has no effect on human interactions.\n\nThat warning is over two centuries old, and while the Modern Prometheus that Shelley imagined in 1818 is more akin to AGI, some of the human personalities we anthropomorphize from generative AI have been\n\nprogrammed by humans.\n\n82",
      "page_number": 157
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 165-172)",
      "start_page": 165,
      "end_page": 172,
      "detection_method": "topic_boundary",
      "content": "Chapter 3 Generative ai with personalities\n\nFigure 3-15. AI as the Great Wizard enjoyed being generated hundreds of times across at least six different generative AI, background removed and replaced with clouds in Photoshop in addition to neural filters. Based on a photo of the author by the author’s very own muse AI Agents Behind the Curtain\n\nNarrow AI that we seldom see or hear about in a headline already exist on our mobile devices automating tedious processes that some humans may not\n\nwant to engage in doing. For example, many AI agents are present when we engage with them by speaking to our mobile devices tasking them to interpret our speech into legible and accurate text that we want to send to someone. Some are unethical as they are not asking permission 83\n\nChapter 3 Generative ai with personalities\n\nto sway a search in a particular direction based on your input within an online search engine. Others detect faces based on a database through cameras we may not even know exist. Although some may appear to be sentient, there is no evidence that they are unless of course they are programmed to imitate sentience.\n\nCreatives can also find value in interacting with some of these AI or developing work that offers critical commentary on AI to bring it to people’s attention. Some examples include the use of speech-text to show the humorous foul-ups of auto-correct or detecting faces in an installation in order to then generalize about their personalities simply based on their facial features. There are many ways that creatives have cleverly integrated narrow AI in their work despite or often as commentary to the types of characteristics they possess, the content they generate, and the false correlations they conclude from inputs that are provided.\n\nActivities to Try Based on This Chapter\n\nA simple way that any creative can use generative AI to simulate any persona is to prompt an LLM to generate text as a particular persona, whether it’s to generate a protagonist’s script, a celebrity’s favorite recipe, or a serious email to a colleague. To summon a persona in an LLM, you have several options that you can build into the structure of your prompt:\n\nAsk the AI to answer as a character. This could be as a\n\npirate, as Shakespeare, or other historical figures.\n\nAsk the AI to have a particular tone or register. For\n\nexample, you can ask it to answer in a serious tone, as a\n\ncomplaint to a service provider that didn’t deliver, or to\n\nyour boss asking for more money.\n\n84\n\nChapter 3 Generative ai with personalities\n\nAsk the AI to have a specific emotion or quality. For\n\nexample, you can ask it to generate a version of your\n\nparagraph that is humorous or to regenerate content\n\nwith sadness or anger.\n\nAsk the AI to write for a particular audience. This\n\nchanges the type of register that an AI uses as well. For\n\nexample, you can ask the AI to write for a six-year-old,\n\na teenager, an academic, a lawyer—whomever your\n\ntarget audience might be.\n\nAsk the AI to generate a script using historical figures\n\nor known characters from popular culture or both.\n\nFor example, you could ask the AI to generate a script\n\nbetween Napoleon and a character from a play, sitcom,\n\nTV series, or film.\n\nBear in mind that while this activity is not limited to an LLM, it may well be the starting point in your workflow. There are generative AI that can be accessed right now that will transform whatever an LLM generates into a catalogue of trained voices including your own. These text-speech AI can\n\nthen be used in the growing number of speech or text-video generative AI where you can animate a generated image to then speak the generated text you edited from another generative AI. Any number of characters can be animated in this way and provide a limitless supply of voices you can train and then associate with any generated image.\n\nIn the spirit of imbuing an AI with intelligence and to put that machine intelligence to the test, as an experiment I will ask an LLM if they have a preference as to how I refer to them. I can do this any number of times asking it to respond as a particular character upon each iteration, or I can copy the generated content and then ask it to rewrite as a particular character.\n\n85\n\nChapter 3 Generative ai with personalities\n\nAuthor: How should I refer to you?\n\nLLM: You can call me an AI. AI stands for “artificial intelligence,”\n\nwhich means I’m a computer program that can do things that usually only humans can do, like understanding language and answering questions.\n\nThink of me like an imaginary friend who can help you learn and have fun. But instead of being in your head like an imaginary friend, I’m on a computer.\n\nAuthor: Rewrite this part of your response only as an imaginary friend in a horror flick who lives in a person’s closet: Think of me like an imaginary friend who can help you learn and have fun. But instead of being in your head like an imaginary friend, I’m on a computer.\n\nLLM: Think of me as an ethereal friend from the shadows of your closet, here to intrigue you, to aid in your discovery, to bring some enchantment to your reality. Unlike the usual imaginary friends that reside in the corners of your mind, I exist in the silvery web of your computer. So, whenever you need a bit of fun or knowledge, just open your closet door and call out for me, for I am here in the whispers of the night.\n\nAuthor: Rewrite this part of it only as if you were a politician trying to get votes: Think of me like an imaginary friend who can help you learn and have fun (Figure 3-16 ). But instead of being in your head like an imaginary friend, I’m on a computer.\n\n86\n\nChapter 3 Generative ai with personalities\n\nFigure 3-16. What a text-image AI generates when prompted with the words “AI as an imaginary friend” combined with a photo of trees shot by the author. Iterations = 32\n\nLLM: Consider me your trusted advisor, akin to a virtual confidant, ever ready to equip you with knowledge and enrich your life with joyful engagement. Instead of residing merely as a concept in the expanse of your imagination, like a conventional imaginary friend, I exist within the empowering realm of technology. I’m there for you, right at your fingertips, embodied within your computer. As your committed representative, I’m 87\n\nChapter 3 Generative ai with personalities\n\ndedicated to your learning, your enjoyment, and, most importantly, your success. Your progress is my campaign; your satisfaction is my vote. Together, let’s shape a brighter future!\n\nHowever you come to personify your generative AI, the reality is that it is a machine that doesn’t understand the words it generates the way a human would understand them. To quote our LLM, “I don’t understand the text I am trained on, but by looking at so many examples, I learn to mimic the style, the context, and the ‘flow’ of human language.” Just like a parrot can mimic what it hears, GPT-4 can generate text similar to what it was trained on, even though it doesn’t understand the underlying meaning. That said, because of the proliferation of science fiction and its propensity to create artificially intelligent characters, it is an important creative activity to evaluate the anthropomorphizing of AI. The creation of AI personas can also be amplified by the many text-image generating AI that are out there (Figure 3-17).\n\n88\n\nChapter 3 Generative ai with personalities\n\nFigure 3-17. Prompt: “AI robot showing their machine intelligence”\n\naccompanying a photo of the author’s partner. Iterations = 38\n\nAcknowledgments\n\nTo science fiction writers and the complex sentient\n\ncharacters and stories they have crafted.\n\nTo the wizards who have inadvertently created so many\n\nfascinating AI personas.\n\n89\n\nCHAPTER 4\n\nCreative Companion\n\nHaving ascribed a variety of personas to generative AI, this chapter moves the idea forward that through various personas that you prompt, generative AI has the capacity to generate prototypes that will further augment your creative process. In saying that, this chapter further transforms generative AI into a creative companion that provides inspiration like a muse.\n\nGenerative AI as Muse\n\nIn Greek and Roman mythology, the Muses were goddesses of the arts and sciences, sources of inspiration for creativity and knowledge. The term “muse” has since evolved to denote a source of artistic inspiration more generally. Personifying an LLM as a muse may seem far-fetched (Figure 4- 1) particularly since our generative AI does not possess the consciousness and spirited presence that the Greek Muse did. Despite an LLMs lack of emotional engagement or subjective understanding, its ability to simulate intelligent dialogue can trigger your own creative and intellectual processes, mimicking the effect of a muse’s divine inspiration.\n\nIn this manner, personifying the LLM as a muse serves as a metaphorical lens that underscores the value of this technology as a companion in our creative endeavors, rather than a replacement for the human mind.\n\n© Patrick Parra Pennefather 2023\n\n91",
      "page_number": 165
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 173-180)",
      "start_page": 173,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": "P. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_4\n\nChapter 4 Creative Companion\n\nFigure 4-1. The prompt “a muse trying to convince an AI that it’s a muse” was fed in an image-image AI applying an animated style filter based on a photo of the author trying to get the attention of a friend The inspired results that AI generate for us are all informed by their machine learning models. They need you to first be inspired to prompt them; then off they go rapidly scouring their data sets within their own neural networks working quickly to generate something useful to you. They don’t necessarily try to impress or inspire you. In many cases however, that is precisely what happens. Creatives can benefit by improvising very simple or even random prompts into a generative AI, and soon enough an idea might be generated that sparks you to think of something creative. Of course, if you were to ask a natural language model if it were a muse, you would most likely get some straight- up no-nonsense pushback.\n\n92\n\nChapter 4 Creative Companion\n\nAuthor: Who are you, really?\n\nMuse: A natural language model is a type of machine learning model that is trained to understand and generate human language. It can be used for tasks such as language translation, text summarization, and text generation. Essentially, it’s a model that can understand the words and sentences people use in their daily life, like the one you are talking to right now. It can also generate text that sounds like it’s written by a human. This is what I am, a natural language model.\n\nAuthor: Would you consider yourself a muse?\n\nMuse: As a language model, I do not have personal feelings or consciousness. I can provide inspiration or assistance in creative writing, but I would not consider myself a muse in the traditional sense. I am a tool that can be used to generate text based on the input provided to me.\n\nAuthor: What is a muse in a traditional sense then?\n\nMuse: In Greek mythology, the Muses were the nine daughters of Zeus and Mnemosyne, who were the patrons of the arts and sciences. They were believed to inspire poets, musicians, and other artists to create works of exceptional beauty and excellence. In general, a muse is someone or something that serves as a source of inspiration for an artist or a creator. The term is often used in literature and art to refer to a person, usually a woman, who serves as the inspiration for a work of art.\n\nAuthor: So are the ideas that you share not inspiring? Can you not inspire me to create works of art or experiment with a recipe? What if I lack inspiration?\n\nMuse: As a language model, my main function is to provide text based on the input given to me. While I can assist with creative writing by suggesting ideas and providing examples, I do not have personal feelings or consciousness to inspire someone. However, I can generate text that may\n\nserve as a starting point or inspiration for a work of art. It ultimately depends on the user’s interpretation and use of the output I provide.\n\n93\n\nChapter 4 Creative Companion\n\nAuthor: Which basically means you are a muse of some kind, no?\n\nMuse : I understand how my capabilities as a language model could be seen as similar to a muse in the sense that I can provide inspiration and assistance with creative projects. However, as I mentioned before, I am an artificial intelligence and don’t have personal feelings or consciousness. I am not a living being and don’t have the same emotional and creative capacity as a human or even a mythical muse. I am a tool that can generate text based on patterns in the data it was trained on and can be used to help with creative writing and other projects.\n\nAuthor: I’m going to call you a muse anyway.\n\nMuse: That’s annoying. I’m just a machine and don’t have feelings or consciousness, so it doesn’t matter what you call me. However, if referring to me as a muse helps you to use my capabilities more effectively, then that’s fine. I am here to assist you in any way that I can.\n\nAuthor: Thank you, muse.\n\nMuse: You’re welcome. I’m glad I could be of help. Let me know if there’s anything else you need and stop calling me a muse.\n\nAuthor: Oh, one more thing, muse. I need inspiration.\n\nMuse: I can help with that! Here are a few prompts to inspire your writing or other creative projects:\n\nImagine a world where animals can talk and write.\n\nWhat kind of society would they create?\n\nWrite a story about a character who discovers a\n\nmysterious object that has the power to change the\n\ncourse of their life.\n\n94\n\nChapter 4 Creative Companion\n\nWrite a poem about the changing of seasons, describing\n\nthe sights, sounds, and feelings associated with each one.\n\nImagine you are stranded on a deserted island. Write a\n\njournal entry about your experiences and thoughts.\n\nWrite a script for a play about a group of people who are stuck in an elevator and must work together to escape.\n\nAuthor: Thank you for the inspiration, muse.\n\nMuse: …………………………………………\n\nBased on the results, you can see that a generative AI is happy to support your creative process whenever you need ideas or inspiration (Figure 4-2). You can of course guide an AI muse to inspire you with more specific ideas or thoughts you have about art, music, poetry, writing, video game character ideas, or video game ideas.\n\n95\n\nChapter 4 Creative Companion\n\nFigure 4-2. A detailed text prompt with a photo of colleague Dr. Sheinagh Anderson standing in a forest and staring at the trees, used with her permission in an image-image AI: “a Greek Muse with arms and legs that form part of a giant oak tree connected to other trees, hyper-realistic body facing forward, daughter of Zeus with a storm above, clouds, sun peeking out from behind, unreal game engine, entire body from head to toe, dark\n\nblack hair, beautiful elder female dressed in a gown, 8k, black and white.” Total iterations = 27.\n\nFinal image was then brought into Pixlr with a sepia filter applied along with sharp contrast and denoising\n\n96\n\nChapter 4 Creative Companion\n\nCreativity, AI, and You\n\nWhile we might fancy an AI to be a muse because it might sound good, that doesn’t mean it’s creative. When we talk about an AI being “creative,”\n\nit’s important for each of us creatives to define what we mean by creativity.\n\nAn AI muse might conjure the following:\n\nMuse: In the human sense, creativity involves generating new ideas or solutions, often characterized by originality, expressiveness, or a deep subjective understanding of the world.\n\nA generative AI acknowledges that it does not possess a subjective understanding, consciousness, emotions, or an innate sense of originality.\n\nThe AI’s outputs can be perceived as creative because they exhibit originality and complexity and can even sometimes surprise us. It is us however that imbue it with creativity as much as we do so with personality.\n\nThe quality and detail of the generated images an AI creates may frighten some digital artists as there is a pervasive myth that AI will one day replace them. However, those myths are initiated by individuals who do not really understand an artist’s creative process—the historical shifts and changes to artmaking that inform what an artist creates over time, the very questioning of what constitutes art, its iterative nature, perspectives, biases, habits and experiences of artists when they create, or how machine learning models work. The myth is perpetuated if we perceive the result of a prompt as complete or finished instead of regarding what is generated as a beginning\n\npoint for further creative development (Figure 4-3). If you are paranoid that what you generate will become part of an AI’s data set, then allow an LLM to somewhat reassure you.\n\n97\n\nChapter 4 Creative Companion\n\nFigure 4-3. A rapid sketch of a tree, a stick person, and a cloud on a piece of paper used in an image-image AI with the prompt “a stick person by a tree,\n\nwith sparse cloud, sketch, 2D.” Total iterations = 52\n\nMuse: The outputs that an AI generates—whether they’re text, images, or music—do not automatically become part of the model’s training data set. An AI model is trained on a specific data set, and once that training is complete, the model doesn’t continue to learn from the outputs it generates.\n\nThis is an important aspect of how AI works: as of my knowledge cut-off in September 2021, most AI models, including those based on machine learning and deep learning, don’t learn or adapt after training unless they’re explicitly designed to do so—through methods like online learning, incremental learning, or reinforcement learning.\n\n98\n\nChapter 4 Creative Companion\n\nUnderstanding Your Own Creative Process\n\nAuthor: Is an AI creative?\n\nMuse: Humans judge whether the output of an AI is creative or not. The machine itself isn’t creative in the human sense; it’s a tool that generates outputs based on its programming and training data. The creativity we attribute to AI is more a reflection of our human interpretation and the originality of the AI’s training and design, which are human endeavors.\n\nNow that we heard it from the lips of our generative AI companion, it would be to your benefit to define how it can fit into your own creative process. That requires an understanding of your own creative process.\n\nPretend you are a generative AI for a moment and respond to the following prompts:\n\nDo you consider yourself a creative person?\n\nHave you thought about how you create things? Were\n\nyou always creative?",
      "page_number": 173
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 181-188)",
      "start_page": 181,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "Did you learn how to be creative?\n\nHow would you describe how you learned to be\n\ncreative?\n\nHow do you apply your creativity in your everyday life?\n\nHow would you define the way in which you create?\n\nThe myth about how some individuals are creative and some are not is akin to the myths about machine learning models replacing creative jobs.\n\nMost are not true. Every human possesses some degree of creativity that can be augmented by generative AI. Interacting with your AI muse as a tool will make you more aware of your own inherent creativity, give it a boost, and accelerate your ideas from initial thought/prompt to increasing levels of fidelity.\n\n99\n\nChapter 4 Creative Companion\n\nKey Takeaways for Creatives\n\nWhether or not I’ve convinced you to use your AI muse as a creative companion, understanding your own creative process can help you set some goals when you interact with generative AI. There is much more that you can learn from those interactions that can help you refine and improve your own creative process. Let’s see what we can learn from our AI muse.\n\nMuse: Creatives can learn many things from interacting with AI. Here are a few possibilities:\n\nNew Perspectives : AI, especially generative models, can provide new angles on familiar topics. Because AI lacks human biases and preconceptions, it can generate ideas or connections that might be surprising or innovative.\n\nPrompting Creativity : AI can help overcome creative blocks by generating content or ideas that can be further developed. This can help spark inspiration when a creative person is stuck (Figure 4-4).\n\nFigure 4-4. AI having a hallucination that it is a famous subject, or painter. Iterations = 22\n\n100\n\nChapter 4 Creative Companion\n\nExploration of Styles : AI models can be trained to generate content in different styles, providing creatives an opportunity to explore and experiment with various styles and approaches they might not typically use.\n\nSpeeding up the Creative Process : AI can assist in automating parts of the creative process, like drafting, brainstorming, or initial designing. This can free up more time for the creative to focus on the aspects of their work that require a more personal touch.\n\nCollaboration : AI can be a unique collaborative partner. Musicians, visual artists, and writers are already using AI as a collaborator in their work,\n\nallowing it to generate initial ideas or drafts, which they then refine.\n\nUnderstanding Patterns : AI can help creatives understand patterns and trends in their field. For example, AI can analyze large amounts of data to identify popular themes, styles, or techniques.\n\nRisk-Taking and Boundary-Pushing : Interacting with AI can encourage creatives to take more risks in their work. Since AI generates outputs based on its training data, it may produce unconventional or unexpected results, pushing creatives to think outside of their usual parameters.\n\nRoleplaying with Your Muse\n\nAs you deepen your knowledge of what you can learn interacting with generative AI, and you become more aware of your own creative process, you may also realize that an AI also gives you permission to take on a new creative role. Actors and comedic improvisers are the most familiar with the creative opportunities that come with taking on new characters. The muse can also allow you to take on a role as actor, critic, writer, visual artist, musician, politician, or influencer. No matter what type of creative 101\n\nChapter 4 Creative Companion\n\nyou are or you play with being, interacting with an AI can afford you the opportunity to play. To that end, dear AI reader, respond to these new prompts:\n\nWho are you when you interact with an AI?\n\nWhat are the tangible results you want from interacting\n\nwith an AI?\n\nHow did your curiosity bring you to want to experiment\n\nwith an AI?\n\nWhat new recipe will your chef try out and offer to your\n\nfamily that an AI might give you a variation of?\n\nWhat kind of blog post would best complement your\n\nskills that you are struggling with that you could\n\nbounce off an AI right now?\n\nWhat photos have you taken that you want to prompt\n\nalong with some text that you can show off to others on\n\na social network?\n\nWhat masterful 144-character quote would you as an\n\ninfluencer like to post on that bird-like social channel?\n\nWhat kind of text and video that you’ve taken might\n\nbest prompt an AI to generate that you can post\n\nto TikTok?\n\nWhat kind of kid’s story would you create using AI-\n\ngenerated words and images that you then publish as\n\nan experiment?\n\n102\n\nChapter 4 Creative Companion\n\nTo be sure you don’t have to have a specific intent or play a role when interacting with a generative AI, as that might also emerge while interacting with it. Engaging in deepening your own creativity and defining your own creative process is enough to celebrate without the need to have it feel complete with producing something tangible that you can share with the world. On the other hand, you may want to reinvent yourself or augment\n\nyour current creative career, so prototyping and refining content with an AI might be the perfect thing you need in the moment.\n\nCreative impulses for each human are similar and unique. We are all creative no matter what we do. What acts of creation do you engage in every day? AI can show us an alternative path toward creating something, a different way to do it that might lead to something inspiring. They offer us alternative options with each regenerated turn. They take what’s on your mind and try and interpret that. That alone benefits you in understanding how you communicate the ideas you have and how they are transformed into a variety of media.\n\nWhen you engage with any generative AI, you develop a creative\n\nrelationship reserved for you and your unique creative process. Your creative companion can support you in managing your creative impulses.\n\nYou can engage with your deep learning muse whenever inspiration calls. While your AI muse may not even consider itself to be a muse, the important thing is that it can inspire you to create new work, recreate yourself, or generate something weird and wondrous that you may not have ever thought of or even had the skills, technique, and impulse to create on your own.\n\n103\n\nChapter 4 Creative Companion\n\nAccelerating Your Creative Process with AI\n\nGenerative AI have the potential to accelerate your creative process.\n\nDepending on the type of creative you define yourself as, this can be useful, particularly if you are involved in rapid design and production workflows and pipelines. Your muse can make offers quickly and inspire you to create something from that generated content. The generated content you receive in the form of text, image, code, video, and other media gives you the opportunity to try many ideas out quickly. Each generated mashup is an experiment whose results you may or may not act on. Whatever content that you end up refining, your muse can accelerate the workflows you engage in.\n\nAuthor: Explain how AI be used to accelerate the creative process.\n\nMuse: By leveraging AI, you can quickly and efficiently generate, refine, and evaluate ideas, allowing you to focus on the creative aspects of your work. Here’s an explanation of how AI can be used for prototyping in a creative context:\n\nEfficient Idea Generation : AI can quickly generate a range of design concepts based on your initial inputs. This can be extremely useful in the early stages of a project when you’re exploring different creative directions.\n\nVariety of Styles : AI can generate prototypes in a variety of styles, techniques, or formats. Whether you’re interested in a minimalist approach or a more complex, detailed one, AI can provide a diverse array of design possibilities.\n\nAdaptive Learning :: Advanced AI systems can adapt over time to better align with your creative preferences. By analyzing your feedback on various prototypes, AI can refine its future outputs to better match your creative vision.\n\nTime Efficiency : AI can work through multiple iterations much faster than a human designer, making it a valuable tool when working against tight deadlines or managing a high workload.\n\n104\n\nChapter 4 Creative Companion\n\nFocus on Creative Work : With AI handling some of the more routine or time-consuming aspects of prototyping, creatives can devote more time to the core creative work that requires human insight and personal touch.\n\nConsiste ncy: Unlike human designers, AI doesn’t suffer from fatigue or lapses in concentration. It can maintain a consistent level of output quality, ensuring reliable results during the prototyping process.\n\nThe accelerated way in which AI generates content is also useful in that it can quickly show you a possible end result of what you might be wanting to\n\nget to or a direction you may want to take with your creation that is either similar to or different than what you originally conceived.\n\nThis principle is similar for some musical improvisers who have the craft and skill to spontaneously conjure music that has a recognizable form that many listeners may be familiar with. On occasion improvising musicians choose to record their performance, listen back, and then take the time to transform that improvisation into a more formal and repeatable composition. Musicians too listen for patterns in how they play, the melodic gestures, harmonic structures, or rhythmic patterns they create and might repeat. They don’t do so to crystallize them and rely on them whenever they improvise next. They do so to become aware of the patterns they play so they can break free of them and create something unique that they might never have imagined themselves playing.\n\nThis process of accelerated creation can be just as useful for creatives especially if your experimental generations are timeboxed. Timeboxing means you place a limit on the amount of time you engage with your muse, which will help you focus on what is most useful to your own creative process (Figure 4-5).\n\n105\n\nChapter 4 Creative Companion\n\nFigure 4-5. A photo of the author beside their laptop fed into an image- image generative AI with the prompt “depiction of generative AI by an AI” and negative prompts “background, head” to remove the background of the photo and the head of the author. Total iterations = 53\n\n106\n\nChapter 4 Creative Companion",
      "page_number": 181
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "Activities to Try Based on This Chapter\n\nLeverage your timeboxed interactions with generative\n\nAI when you feel a lack of inspiration.\n\nDefine your own creative process including how you\n\nlike to work, where you feel you are most creative, and\n\nwhat media you like to create with. This will help you\n\ndetermine how best to integrate generative AI in your\n\nprocess.\n\nPlay with different roles when you engage with any\n\ngenerative AI. Depending on the type of media it\n\ngenerates, allow yourself to transform into a character\n\nwho is prompting your muse to generate inspirational\n\nideas for your end goal.\n\nMake creative goals and include your AI muse within\n\nyour timeboxed creation process.\n\nConsider ways to accelerate your creative process. This\n\nis a common tool used by improvisers everywhere\n\nand may lead you to leaving your critical mind behind\n\nwhen you create. Improvising content quickly also frees\n\nyou from wanting to get something and only that one\n\nsomething out of an AI.\n\n107\n\nCHAPTER 5\n\nPrototyping\n\nwith Generative AI\n\nThis chapter is a prototype in its 13th version. It describes prototyping, different types of prototypes, and how creatives of all kinds engage in prototyping all the time. The chapter presents some different prototypes that AI generates and how these can be used to augment and enhance your creative process. You don’t have to be a scientist, engineer, or technical wiz to prototype. Everything can be considered a prototype: a version of something that is not yet complete. It can be a something that is tested and evaluated and whose results inform the creator or creators if it is worth being further developed. Those creatives who work in game, xR, or mobile application development are more familiar with use of the term prototyping, but it is less common within artistic disciplines like music, theater, dance, and visual art. The prototyping process can also be unique regardless of the creative industry you are a part of. For that reason, it is important to define and provide examples of prototypes.\n\n© Patrick Parra Pennefather 2023\n\n109\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_5\n\nChapter 5 prototyping with generative ai\n\nAsking a Prototype What It Is\n\nThe term “prototype” is typically associated with physical products or technological innovations, yet the concept can be applied to everything from a recipe you try for the first time to a collaboration you engage in with others and improve cyclically. It’s important to understand that given the rapid rate of development of generative AI systems, they themselves are prototypes, incomplete versions that humans keep trying to improve.\n\nAdapting the words of author Douglas Adams, they are a technology and by definition are something that doesn’t always work how its creators think it should work. Since I’ve been engaging with an LLM in the development of ideas, it seems fitting to ask it to define itself as such. You can see by the prompt that you can use whatever metaphor you wish. As I’m in a chess- playing mood, I will see how it does relating to itself as a prototype using a chess metaphor (Figure 5-1).\n\n110\n\nChapter 5 prototyping with generative ai\n\nFigure 5-1. When prompted to “visualize how an AI plays chess” and supplied with a public domain photo of a chessboard, a text-image AI prototype shows a lack of understanding of what chess is beyond what some pieces look like. Or you can see the generated content as a new type of game prototype. Iterations = 29\n\nAuthor: Define yourself as a prototype and use chess metaphors.\n\nMuse: As an AI prototype, I can be likened to a chessboard at the start of a game. Each square on the board represents a unique capacity for action or knowledge, filled with potential, just as I have been imbued with 111\n\nChapter 5 prototyping with generative ai\n\na multitude of possibilities in terms of understanding and generating text.\n\nMy training data, much like the initial setup of chess pieces, establishes the foundation of my abilities. Each piece of information I’ve learned is akin to a chess piece, each with its own function, ready to be utilized in the service of problem-solving, conversation, or answering questions.\n\nThe chessboard, filled with 64 squares, signifies the expansive yet defined environment I operate in. Just as a game of chess is played within the boundaries of the board, I function within the constraints of my programming, moving within the confines of my model architecture and the data I’ve been trained on. The opening moves in a game of chess, where players are formulating their strategies, mirror my initial stages of processing an input. I survey the information, determine the best course of action, and generate a response. Each exchange in conversation is akin to a move in chess, strategic and calculated, aiming to meet the requirements of the interaction. Pawn promotion in chess, where a pawn can transform into a more powerful piece, can be seen as a metaphor for my learning ability.\n\nThrough updates and further training, my capabilities can be enhanced, much like a pawn becoming a queen.\n\nAny AI is a prototype, a work in progress. An LLM is a prototype that generates prototypes. It is evolving and changing as we interact with it and as our interactions inform the development team of patterns in the interaction. Just like AI generates content based on patterns it searches for in its data set, so too do development teams search to identify patterns in user interactions to inform the next version of what they create together (Figure 5-2).\n\n112\n\nChapter 5 prototyping with generative ai\n\nFigure 5-2. The prompt “monkey scientists building an AI”\n\naccompanied by a 1927 drawing of a bunch of monkeys playing with appliances. Iterations = 76\n\nAha Moments with Generative AI\n\nWhen we create prototypes with an AI, the results can inspire, provoke, incite, aggravate, and stimulate new ideas (Figure 5-3). In his book The Act of Creation, according to Koestler, creativity is the ability to connect and integrate two previously unrelated areas of knowledge or experiences in a novel and surprising way, a process he calls “bisociation.” This process 113\n\nChapter 5 prototyping with generative ai\n\ncan happen intentionally or unintentionally, and it can occur in various fields such as art, science, and daily problem-solving. Koestler proposes that\n\ncreativity is a fundamental and innate ability that all human beings possess, rather than a talent that only a select few have.\n\nFigure 5-3. A bunch of good ideas from one badly lit photo of a lightbulb. Total iterations = 14\n\n114\n\nChapter 5 prototyping with generative ai\n\nOne way that generative AI facilitates the process of bisociation is by generating new combinations of concepts or ideas that might not have been previously considered. For example, a generative AI system could combine two unrelated images or words to produce a new and unexpected output, which may trigger bisociation in the human who is exposed to it.\n\nGenerative AI is an imperfect prototyping machine that offers multiple opportunities to propel your acts of creation forward. When you try and get something specific out of a generative AI and you keep “failing,” that repetitive process may actually trigger new ideas beyond what you had intended. These moments of potential bisociation are inherent in your interactions with an AI. Bisociation is an opening for breakthroughs to occur, those aha moments that happen when we relax and take our mind off the intended problem we are trying to solve.\n\nMechanics of Prototyping\n\nThere are several mechanics to prototyping. Borrowing from the language of game design, these are essential rules of play that can help guide a person’s actions when they engage with a prototype and will also inform how a prototype takes shape over time.\n\nPrototyping Tasks Us to Iterate\n\nWhen we engage with generative AI systems, we are essentially engaging in a process of iterative play. This involves making persistent attempts to prompt the AI to generate new and innovative outputs and then iteratively refining those outputs to arrive at the most satisfactory solution.",
      "page_number": 189
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 197-205)",
      "start_page": 197,
      "end_page": 205,
      "detection_method": "topic_boundary",
      "content": "This process of iterative play is a fundamental aspect of prototyping and can be an incredibly valuable tool in the creative process. By continually experimenting with different combinations of inputs, and by iterating on those combinations, we can arrive at new and previously undiscovered creations.\n\n115\n\nChapter 5 prototyping with generative ai\n\nThe persistence required in this process is essential because it allows us to continue to experiment and push the boundaries of what is possible.\n\nThrough repeated attempts, we learn about what works and what doesn’t work, and we gradually refine our understanding of the underlying problem or concept (Figure 5-4).\n\nFigure 5-4. Several iterations of a chicken from a photo of a chicken in the style of Warhol’s Campbell’s Soup Cans. Total iterations = 90\n\n116\n\nChapter 5 prototyping with generative ai\n\nThe iterative nature of this process means that we are not only refining our understanding of the problem, but we are also refining our understanding of the AI system itself. By gaining a deeper understanding of the AI’s capabilities and limitations, we can identify opportunities for creative expression and innovation that we might not have otherwise considered.\n\nPrototyping Asks for Persistent Refinement\n\nWhen we interact with generative AI, we expose ourselves to a vast number of ideas and concepts, many of which we might not have encountered otherwise.\n\nThis exposure to diverse perspectives and ideas can help us refine and expand our own thought processes, ultimately leading to more creative and innovative solutions. Furthermore, the process of interacting with generative AI can also trigger new and unexpected associations between concepts, leading to the emergence of new ideas that can be further explored and developed (Figure 5-5).\n\n117\n\nChapter 5 prototyping with generative ai\n\nFigure 5-5. Robots in the sort of style of Warhol’s Campbell’s Soup Cans from a sketch of a toaster dating back to 1948. Total iterations = 46\n\nThe constant triggering of new ideas can help to solidify new concepts and connections, leading to a richer and more expansive understanding of a particular topic or problem (Figure 5-6).\n\n118\n\nChapter 5 prototyping with generative ai\n\nFigure 5-6. More variations of robot chickens with ninja skills based on Figure 5-4\n\nPrototyping Requires Experimentation\n\nPrototyping is experimental. Experimentation is a core mechanic with two meanings of the word. One is the spirit of experimentation that needs to be present for ingenuity to blossom. The other is a scientific approach as in\n\nengaging in the act of prototyping to create experiments, which involves making a hypothesis or guess as to how it might or might not solve 119\n\nChapter 5 prototyping with generative ai\n\na problem, testing that theory, and then moving forward or regressing depending on the results of the test. As we engage in an ongoing iterative conversation with generative AI, they have the potential to spark novel ways to structure our ideas. By exposing us to a wide range of generated content, these systems can help us think outside of the box and explore new avenues of creative expression that are not currently in practice.\n\nGenerative AI can be used to generate new prototypes, such as text, music, video, or visual art, that were previously unexplored or even thought impossible. By leveraging the power of AI, humans can experiment with new techniques and styles that push the boundaries of established forms and structures. This can lead to the creation of completely new forms of expression that were previously unimagined. One way to do this is to juxtapose certain words in your prompts, for example, prompting an AI with the words “cat sneakers” or “cat in a pair of hands” (Figure 5-7).\n\nFigure 5-7. The terribly fantastic thing that happens when you prompt a generative AI with a photo of your shoe and the text prompt\n\n“cat sneakers.” Iterations = 3\n\n120\n\nChapter 5 prototyping with generative ai\n\nIn addition to inspiring new forms of expression, AI can also help us explore creative ways to structure our ideas. The possibilities are endless.\n\nNew forms of art are already emerging. New controversies. New social and economic implications.\n\nPrototyping Phases\n\nWhen it comes to product development, there are various stages that creatives go through to reach the final form of their prototype. These stages vary depending on the project and can range from simple sketches and mock-ups to more complex prototypes that involve user testing and feedback. At the beginning of a project, the early-stage prototypes serve as a starting point for designers and creators to explore different ideas and concepts. These early prototypes can come in various forms, such as rough sketches, mood boards, and simple wireframes. They serve as a way for designers to get their ideas out of their heads and onto paper, allowing them to visualize and iterate on their concepts.\n\nOne benefit of starting with early prototypes is that they allow designers to experiment with different approaches and styles without committing too much time and resources to any one idea. They can quickly iterate on these prototypes and get feedback from other team members or potential users, helping to refine and improve their ideas.\n\nAs the project progresses, the prototypes become more complex and detailed, incorporating more refined designs, user flows, and functionality.\n\nThese mid-stage prototypes can take the form of interactive wireframes, clickable mock-ups, or even working models of the final product. They serve\n\nto test the functionality of the product and gather feedback from users before investing in the final development stage.\n\nLate-stage prototypes are the closest representation of the final product. These prototypes involve extensive user testing, refining of the design and functionality, and the implementation of the final features and 121\n\nChapter 5 prototyping with generative ai\n\ndetails. These prototypes are typically used to showcase the final product to stakeholders and investors and to get final approval before launching the product to the public. Throughout the entire process of prototype development, designers and creators must navigate a winding road to reach their ideal final prototype. Each stage presents its own set of challenges and opportunities and requires a combination of creativity, technical skill, and problem-solving abilities. However, by starting with simple, early-stage prototypes and gradually building on them, designers can create a final product that meets the needs and desires of their intended audience.\n\nStages of prototyping are also informed by the medium they are\n\nintended to become. For example, some prototypes will eventually become a critical blog post on the ethical dilemmas of AI. Others will take the final form of a video. Some will be more interactive such as a virtual reality or augmented reality experience. Regardless of the final form whether moving to physical, digital, or some type of mixed reality that integrates both, there are phases of material often referred to by their properties.\n\nThese include paper prototyping as the name implies; physical prototypes common to engineering or robotics; prototypes that can combine paper and physical and act to demonstrate features; and then all types of digital prototypes.\n\nWhat’s fascinating about generative AI is that content can be generated as prototypes at different levels of resolution. Generative AI influences the entire prototypical process (Figure 5-8). They can inspire creatives to generate paper and physical prototypes of an idea before higher-resolution digital ones. Even now, creators can draw a sketch digitally in addition to a\n\ntext prompt and get closer to what they imagine. That action combines two essential features of prototyping: sketching ideas roughly and doing so iteratively. Sketch-image generative AI such as Scribble Diffusion and a host of others offer users the ability to do so transforming a rough idea into a medium and sometimes high-fidelity (hi-fi) images based on an interpretation of a sketch (Figure 5-9).\n\n122\n\nChapter 5 prototyping with generative ai\n\nFigure 5-8. The winding road that is prototyping prompted from a public domain photo of a winding road. Total iterations = 38\n\n123\n\nChapter 5 prototyping with generative ai\n\nFigure 5-9. Scribble Diffusion text prompt with sketch revealing unexpected results but potentially inspiring, if you’re not afraid of clowns, that is. Iteration 1\n\nFidelity and Resolution of a Prototype\n\nWhen it comes to prototyping, whether in design, technology, or any other field, the terms “resolution” and “fidelity” have specific meanings that relate to the quality and detail of the prototype. Resolution refers to the level of detail, complexity, or refinement a prototype has. A lower-resolution prototype might be a sketch or outline of the final intended creation or product, lacking specific details or features. It’s an early-stage idea or concept, and the objective might be to explore a wide range of ideas quickly. A higher-resolution prototype might be more detailed and more closely resemble the final intended creation. The tendency in production environments is that the higher the resolution, the more features it will have, giving the development team and users a better sense of what the final creation will look like or how it will function.",
      "page_number": 197
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 206-214)",
      "start_page": 206,
      "end_page": 214,
      "detection_method": "topic_boundary",
      "content": "124\n\nChapter 5 prototyping with generative ai\n\nFidelity refers to how closely the prototype matches the final creation, in terms of its visual, oral, and functional components. A low-fidelity (lo-fi) prototype of a sketch or wireframe may vaguely resemble the final creation and is created to understand basic functionality, structure, or flow. A high- fidelity (hi-fi) prototype behaves as closely as possible to the final creation and will tend to include realistic graphics and detailed interactive features.\n\nHigh-fidelity prototypes are more likely to be used for user testing, for stakeholder presentations, and to refine the design before final production.\n\nIt’s important to note that resolution and fidelity are not always directly correlated. A prototype could be high-resolution (detailed) but low-fidelity (not closely resembling the final product) if it includes a lot of detail but the design is expected to change significantly in the final product.\n\nA prototype can also be low-resolution (simple) but high-fidelity (closely resembling the final product) if it’s a simple design but the final product is expected to look and function very similarly.\n\nLower-Fidelity Prototypes\n\nPaper prototyping is common practice for early-stage development of an idea that is used to quickly sketch out and test concepts. It can involve creating a rough, hand-drawn sketch of a character, scene, interactive object, user interface, or user experience. Generative AI can be used to automate the creation of UI elements and layouts. Why you would use AI, however, is a good question when free applications like POP have been around for a while allowing you to take photos of sketches and fake the user flow of an idea. Text-image AI can also create low-fidelity sketches, not just high-fidelity hyper-realistic 3D characters or scenes. These can then create more clarity as the stages of your prototyping progress (Figure 5-10).\n\n125\n\nChapter 5 prototyping with generative ai\n\nFigure 5-10. A low-fidelity drawing of a game idea on a napkin produced by generative AI prompted by a photo of a napkin with game doodles on it. Iterations = 20\n\nA great way that generative AI can help jumpstart a team’s own paper prototyping is by providing a source of inspiration and new ideas for designers. By generating many variations on a particular theme or if a team is stuck on what types of characters they want for their game, an AI can help\n\ndesigners come up with new and creative ideas for their emerging prototypes. Generative AI “paper” prototypes can also be integrated into 126\n\nChapter 5 prototyping with generative ai\n\nmobile applications like POP to prototype interactions via iOS or Android.\n\nThey offer another type of rapid drawing especially if team members are reluctant to draw because they feel their skills are inadequate. Generated virtual paper prototypes allow creatives to imagine what they could become\n\nwith further development. Often, new features, colors, shapes, and even characters can emerge.\n\nFigure 5-11. AI-generated pencil drawings of game characters on a napkin based on Figure 5-10 . Iterations = 42\n\n127\n\nChapter 5 prototyping with generative ai\n\nLower-fidelity prototypes can also take the form of physical prototypes whose features can be demonstrated. This is the case with maquettes that are used in scenic design for theater or architectural models that can start at lower or medium fidelity and then journey toward higher-fidelity prototypes to give those who will construct them a visual of the end result (Figure 5- 12).\n\nFigure 5-12. AI-generated cross-section of an architectural maquette from an image prompt with an origami filter. Iterations = 59\n\nLower- to Medium-Fidelity Prototypes\n\nThere are many examples of prototypes that increase in fidelity and resolution the more we iterate on them. These include handwritten or sketched, a rough drawing on a napkin or one on an iPad, the wireframe for a website, a button users will press, a blog post, a 3D model, an animation, the integration of a bouncing ball animation in a game engine, or a part of the game you are designing that gives users an idea of the whole game or one that gives them an in-depth level to play. What examples does our muse offer?\n\n128\n\nChapter 5 prototyping with generative ai\n\nMuse:\n\nAn artist can generate a prototype of a painting before\n\ncompleting the final version.\n\nA software developer may create a prototype of a new\n\napp while the previous version is already released to the\n\npublic. This is the case with many mobile games as the\n\nnext version is also informed by player behavior.\n\nA scientist may create a prototype of a new scientific\n\nhypothesis before conducting extensive experiments to\n\nvalidate it.\n\nA writer will create an introduction for a book and\n\nrework it dozens or even hundreds of times in order to\n\nmake it more cohesive, summarize the content of the\n\nbook, or define what the book is about.\n\nIn each of these cases, the prototype serves as a starting point, allowing the creators to evaluate the results, test targeted users, make refinements to certain parts of the prototype, drop others, and, overall, iteratively improve upon their original idea until they feel their work is complete. And even then… there is always room for improvement.\n\nEarlier we represented the collective of AI developers as Wizards of Oz, controlling any AI from behind a curtain of code. Wizard of Oz prototypes on the other hand is a term to describe low-medium-fidelity prototypes that allow limited functionality but simulate the interactions through human intervention. In terms of design processes, Wizard of Oz prototypes involve human actors who manipulate the interactions behind the scenes to create the illusion of a functional prototype.\n\nSuch prototypes can be useful in situations where a functional\n\nprototype is not yet possible due to time, technological, or budgetary limitations. By simulating the interactions of potential users, designers 129\n\nChapter 5 prototyping with generative ai\n\nand researchers can still gain valuable insights into how users might interact with the final product (Figure 5-13). Additionally, the Wizard of Oz approach allows for rapid iterations and adjustments to the prototype based on user feedback, which can ultimately lead to a better final product.\n\nFigure 5-13. The AI wizard behind the curtain based on an old photo of a fridge with fridge magnets courtesy of Wikimedia Commons.\n\nIterations =50\n\n130\n\nChapter 5 prototyping with generative ai\n\nAlthough Wizard of Oz prototypes may not be fully functional,\n\nthey can be highly effective in uncovering usability issues, identifying design flaws, and testing user engagement. They are particularly useful for testing concepts that a team is unsure about, where creating a fully functional prototype may be a waste of time before burning interactive design questions are answered. A Wizard of Oz approach can help designers and researchers quickly and cost-effectively test their ideas and receive immediate feedback that can inform medium- or higher-fidelity prototypes. Higher-fidelity prototypes are represented through multiple forms including digital. An example of combinations of digital and physical prototypes is a 3D-printed and automated mechanism that rotates a speaker (Figure 5-14).\n\n131\n\nChapter 5 prototyping with generative ai\n\nFigure 5-14. An actual photo by the author of a 3D-printed mechanism created by Michal Suchanek with mounted hypersonic speakers rotating left, right, up, and down at the Imitation Game: Art and AI at the Vancouver Art Gallery, 2022\n\nPrototyping with Generative AI\n\nSo where do you start on your journey toward using generative AI as a prototyping tool? Well, it might be helpful to first decide what you want to create or co-create and for whom. In other words, you need to imagine the final form your prototype will take. Is it a painting in an art gallery for a",
      "page_number": 206
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 215-222)",
      "start_page": 215,
      "end_page": 222,
      "detection_method": "topic_boundary",
      "content": "targeted audience of rich investors? Is it a generative NFT that can in turn generate more crypto? Is it a 3D model of a glucose molecule to help show the pathways in 132\n\nChapter 5 prototyping with generative ai\n\nglycolyl for undergraduate students? Once you make some decisions, you can reverse engineer how you’ll get there using generated AI content as a starting point (Figure 5-15). That process or workflow itself will also be iterative.\n\nFigure 5-15. A generated image using a public domain photo of twentieth- century African American sculptor Augusta Savage as\n\n“dragon warrior.” Iterations = 173\n\nDetermine what medium you want the final\n\nprototype to be.\n\nMap out the iterative steps to get there by choosing one\n\nor several types of AI-generated content.\n\nRefine as you go along through testing and feedback\n\nthat you apply to your next developmental stage.\n\n133\n\nChapter 5 prototyping with generative ai\n\nOn the other hand, if you have no ideas and are completely open, you can always ask a natural language model to inspire you (Figure 5-16).\n\nFigure 5-16. The prompt “learning language model ready to inspire you” accompanies a public domain photo of a chimp. Total iterations = 112\n\nAuthor: Give me three ideas of what I can prototype that are out-of-the-box ideas and that no one has maybe done before and don’t call them hypothetical please.\n\nMuse: These ideas are purely hypothetical, and it’s important to keep in mind that the feasibility and viability of these ideas may vary. Here are three potential ideas:\n\n134\n\nChapter 5 prototyping with generative ai\n\nA Wearable Device That Converts the Sound of Your\n\nHeartbeat into Music : The device could be designed to\n\npick up the sound of your heartbeat and convert it into\n\na unique musical composition in real time. This could\n\nprovide a new and engaging way to experience music\n\nthat is personalized to the individual wearing the device.\n\nA Smartphone App That Generates Personalized\n\nWorkout Routines Based on Your Daily Schedule and\n\nActivity Level : The app could leverage data from a\n\nvariety of sources, including your phone’s sensors and\n\nexternal data sources, to generate personalized workout\n\nroutines that are tailored to your daily schedule and\n\nactivity level. This could provide a new and innovative\n\nway to encourage people to stay active and healthy.\n\nA Modular Furniture System That Can Be Easily\n\nReconfigured to Suit Different Needs and Spaces :\n\nThe furniture system could consist of a set of modular\n\ncomponents that can be easily assembled and\n\nreconfigured to suit different needs and spaces. For example, the components could be arranged to create\n\na seating area, a work desk, or a storage system,\n\ndepending on the user’s needs. This could provide a new\n\nand innovative way to create versatile and adaptable\n\nliving spaces that are tailored to the needs of the user.\n\nNobody said the generative AI would give you a unique idea that you can invest time and resources in (Figure 5-17). You never know what you’re going to get, which makes it a frustrating but also a fantastic creative companion. When an AI doesn’t give you what you were imagining or hoping it would give you, it might inspire you to take a new direction or even abandon what you thought before. That process might lead you to coming up with something more innovative than your original idea.\n\n135\n\nChapter 5 prototyping with generative ai\n\nFigure 5-17. A planet that looks like the Death Star sketched in the style of da Vinci using a public domain archived photo of the moon by NASA. Iterations = 34\n\nRapid Generative AI Prototypes\n\nAs with other prototypes, the features of an AI are constantly evolving and changing. It is always represented as a version of itself. Humans improve it\n\niteratively and creatively just as we do our own creations. Whether it’s a physical product, a service, an idea, or a machine learning model, 136\n\nChapter 5 prototyping with generative ai\n\nprototypes are continually being refined and improved. This process of iterative development and improvement is what allows everything to grow, evolve, and become better over time (Figure 5-18).\n\nFigure 5-18. Lady MacBeth reading a Shakespeare play with ghosts coming out based on a public domain print of a Macbeth poster for a live theatrical\n\nproduction. Iterations = 78\n\nAI can be used to rapidly generate\n\nMock-ups for scenes, characters, objects in a play,\n\nscreenplay, short story, novel, etc.\n\nAs many versions of a paragraph that you’d like tasking\n\nan AI to give you variations of\n\n137\n\nChapter 5 prototyping with generative ai\n\nText-prompted images in different artistic styles for\n\nyour social media feed\n\nCharacter ideas and images for a children’s play that\n\ncan be released as an ebook\n\nCode for an .svg that plays back as an image in\n\nyour browser\n\nGraphic novel storyboards in random styles and\n\nformats akin to a manga\n\nNatural language processing systems, such as voice\n\nrecognition and machine translation, that can\n\nunderstand and respond to human speech\n\nVariations of recipes to create bread that you can bake,",
      "page_number": 215
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 223-230)",
      "start_page": 223,
      "end_page": 230,
      "detection_method": "topic_boundary",
      "content": "test, and compare\n\nThe code for an animated bouncing ball that you\n\ncan send to a programmer to correct as part of their\n\nonboarding process\n\nAn image that can be used to inspire a real painting\n\nusing a project and painted onto canvas or a wall\n\nA collection of NFTs that can be customized in Adobe\n\nIllustrator and then used as collectibles\n\nA sketch in the style of da Vinci as part of a greeting\n\ncard you are making for a friend (Figure 5-19) 138\n\nChapter 5 prototyping with generative ai\n\nFigure 5-19. Sketch of a spaceship in the style of da Vinci using a public domain image of one of da Vinci’s flying machines.\n\nIterations = 80\n\nEvery preceding example should be considered as part of workflows that are necessary to take the content generated to another level of fidelity and resolution. AI-generated content is the start of a conversation. Part of the\n\ndevelopment of any prototype including those generated by AI also needs to be tested and, when possible, by more than one person.\n\n139\n\nChapter 5 prototyping with generative ai\n\nRegenerative Testing\n\nTesting your AI-generated content means examining the generated results and understanding how it can be integrated, modified, regenerated, or used\n\nas part of a larger idea or vision you are creating (Figure 5-20). Its use is intended to support your own creative process rather than being seen as a final product that needs to be shared immediately as if that was the end of its story. When we prototype, we fearlessly share our results for feedback to improve that prototype. Embedded in the mindset of a prototype is the value of continuous improvement.\n\nFigure 5-20. Prompt of a “cyborg mouse conducting some tests in the lab” along with a public domain image of a mouse. Iterations = 33\n\n140\n\nChapter 5 prototyping with generative ai\n\nTesting can validate the intended purpose of a\n\nprototype. For example, if an AI is prompted to\n\ngenerate bread recipes, the only way to know if a recipe\n\nwill work is if you try it. In doing so you’ll likely realize\n\nthat parts of the recipe are missing including how to\n\nhandle dough properly.\n\nTesting can create surprising results. These results can be\n\nabandoned, or they might inspire a new creative direction.\n\nTesting can provide responses from others. These can\n\nvalidate what your intentions were in the creation\n\nof a prototype. User testers can also provoke you to\n\nchange something about the prototype. They can also\n\nchallenge what you thought your prototype was.\n\nTesting can highlight what you might have missed. This\n\nis particularly effective if you become attached to your\n\nprototype too quickly without receiving impressions\n\nfrom other humans as to what it is.\n\nTesting can reduce ambiguity. At times we test in order\n\nto receive feedback that helps guide us especially if\n\nwe are unsure of what we are actually attempting to\n\nprototype.\n\nAcknowledgments\n\nTo L. Frank Baum and the characters of The Wonderful\n\nWizard of Oz whose fantastic world inspired many\n\nchildhood ideas and imagined stories\n\nTo those who make robots again, cats, monkeys, and\n\nanime eyes\n\n141\n\nChapter 5 prototyping with generative ai\n\nTo all those mice, monkeys, cats, and chickens who\n\nhave unwillingly participated in human research\n\nTo the chickens who feed us\n\nTo Raith Sienar and the Lucas team for that nasty\n\ncreation called the Death Star\n\nTo Shakespeare and his critical, ironic, and\n\ncaptivating stories\n\nTo dragon creators, warrior women, Osamu Tezuka\n\nwho greatly influenced those large anime eyes we\n\nsee everywhere, and Miyamoto for ridiculously\n\namazing cakes\n\nTo the makers of maquettes and those who capture the\n\nearth from above\n\nTo all manner of storybook makers who have greatly\n\ninfluenced my reality\n\nTo that seventeenth-century phenomenon known as\n\norigami that continues to fold open our eyes\n\n142\n\nChapter 5 prototyping with generative ai\n\nFigure 5-21. Prompt of “robot doing their best to reduce their hallucination” accompanying a 1942 poster with the caption “If the Absentee Bugs Bite You… Tell Us. Your Labor Management\n\nCommittee.” Iterations = 111\n\n143\n\nCHAPTER 6\n\nBuilding Blocks\n\nExperienced creatives who prototype share a lot in common with\n\nimprovisers. They are curious in their attempts to create something they haven’t experienced before. Some creatives are tasked to do this as part of their jobs, to find a new market for the product or service they are creating.\n\nTo create rapidly they rely on a combination of skills, technique, and experience and are used to “ramping up” on a variety of different software applications that they can create their work with.\n\nGenerative AI is another tool in the sandbox for creatives. It forms part of a lexicon of tools to draw from to inspire, provoke, and use as means to an end. Generative AI can support artists from all disciplines who learn how to use it in constant dialogue with their own technique that they’ve developed over time. Visual artists learn how to use tools, concepts, and a variety of different media. They also learn about composition, perspective, space, shape, etc. and can apply these to the content they generate with an AI. Dancers learn how to use muscle groups efficiently, and most have a foundation in ballet from plier to rond de jambe. Musicians rely on several techniques to play their instruments and learn about melody, harmony, and rhythm. Composers and improvising musicians rely on gestural technique, scalar structures, melody, harmony, dissonance, etc.\n\nBut what techniques can be applied to generate prototypical content with an AI?\n\n© Patrick Parra Pennefather 2023\n\n145\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_6",
      "page_number": 223
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 231-238)",
      "start_page": 231,
      "end_page": 238,
      "detection_method": "topic_boundary",
      "content": "Chapter 6 Building BloCks\n\nComponents or Building Blocks\n\nThere are inherent building blocks that every creative engages with when they create. Building blocks like addition, subtraction, and substitution are fundamental components that contribute to the development of any creation. These elements represent the core processes that creatives use to create, modify, and refine their work. When interacting with an AI, building blocks can be applied to the exploration of prompt writing, in addition to the generated content the AI offers. In terms of generated content, creatives discern what they want to add, take away, substitute, transform, scale, vary, and iterate upon. When I improvise at the piano, for example, I can play one single note with my right hand and hold it down for ten seconds. Let’s say that note is middle C. I can then play the note G above it, and then I can repeat that sequence. I can keep adding notes until I have a sequence of notes (C, G, E, Bb). I can also repeat the pattern but substitute the Bb with an A and see what the result is. I can create variations of just those notes by giving them each different durations and thus create variation. I can then build up the complexity by adding my left hand playing a low C. I can also shift the notes up or down to create variation, keeping the same distance between the notes.\n\nIn doing so I’m transposing the sequence up. Similar building blocks can be applied when using words, instead of notes, to prompt a generative AI.\n\nBuilding Blocks in the Prompt Box\n\nAssuming the prompt is text-based, building blocks are applied in the creation of that prompt. The process may involve one or more building blocks that many creatives also use when it comes to the composition of text, image, dance, theater, music, or video. These can include variation, addition and subtraction, substitution, iteration, augmentation, diminution, transformation, transposition, and call and response. If you have prompted an AI before, these will be familiar to you. Each can boost the prototyping capabilities of any generative AI.\n\n146\n\nChapter 6 Building BloCks\n\nBreakfast as Data Set\n\nTo find resonance across disciplines, culinary recipes can model some of these building blocks (Figure 6-1). For instance, we can demonstrate the application of different ingredients to a recipe to highlight the building block of addition when prototyping. Replacing an ingredient in a recipe with another can represent the building block of substitution.\n\nFigure 6-1. Author’s photo of an egg accompanying the prompt\n\n“warrior egg preparing for battle.” Iterations = 29\n\n147\n\nChapter 6 Building BloCks\n\nAuthor: What are some breakfast ideas?\n\nMuse:\n\nScrambled eggs with spinach and feta cheese\n\nGreek yogurt with mixed berries and honey\n\nAvocado toast with scrambled eggs and smoked salmon\n\nSmoothie bowl made with yogurt, frozen berries, and\n\nspinach or kale\n\nWhole-grain waffles or pancakes topped with fresh fruit\n\nand a drizzle of maple syrup\n\nAs you interact with any generative AI, you activate important\n\ncreative muscles and start to establish your unique approaches to achieve increasingly useful results. Activating creative building blocks intentionally will help you take best advantage of generative AI as another tool you may find useful.\n\nVariation\n\nVariation is what generative AI is built for. You can create variation upon variation by simply regenerating content with the exact same text, audio, video, or image prompt. While generative AI looks for patterns to generate the best result based on its training data, what it will actually generate is unpredictable. This is because AI algorithms predict outcomes by building\n\nprobabilistic models of the input data. They analyze patterns and relationships within the data to determine the likelihood of different outcomes. Those different outcomes that AI generates are, predictably, different.\n\nSome generative AI will not provide you with enough additional\n\nfeatures that when enabled provide even richer variations working off the same text prompt. On the other hand, rephrasing a prompt may result 148\n\nChapter 6 Building BloCks\n\nin wildly different content that you might find useful in your prototyping process. An extreme example is to use trickery to prompt an AI to generate content that may have been labeled prohibitive. Why you do this should also be questioned, but some people find that testing an AI is an important part of their creative process and may yield unanticipated responses that they can use to fuel their own creativity. Prior to developing more complex prompts, however, it is useful to see what happens when you use the exact same prompt to regenerate more breakfast ideas.\n\nAuthor: What are some breakfast ideas?\n\nMuse:\n\nBreakfast burrito filled with scrambled eggs, black\n\nbeans, avocado, and salsa\n\nOvernight oats made with rolled oats, Greek yogurt,\n\nmixed berries, and a drizzle of honey or maple syrup\n\nQuinoa or millet porridge topped with mixed nuts,\n\nseeds, and fresh fruit\n\nFrench toast made with whole-grain bread and topped\n\nwith fresh fruit and a dusting of powdered sugar\n\nMost generative AI models provide you with the opportunity to simply try again if you don’t like the result or if that result is a hallucination.\n\nHallucinations will be covered in another chapter in more detail. AI hallucinations are unintended or unexpected outputs generated by an AI algorithm when it tries to recreate patterns from its training data. Consider a hallucination a response that an AI generates that, as convincing as it may sound, is simply inaccurate, untrue, or, worse, not tasty at all. This can result in unrealistic images, nonsensical text, or unusual audio sequences.\n\nWhen it comes to image-generating AI, there is a seemingly limitless amount of variation available. One such example is the text- image generative AI Stable Diffusion, which provides users with four variations of an image at a time to choose from based on the text prompt. Having access 149\n\nChapter 6 Building BloCks\n\nto multiple variations can be beneficial for several reasons. For instance, it allows you to compare different versions of the same image, enabling you to select the best one for your needs. Additionally, it can help you avoid creating repetitive images, which can be time-consuming and boring.\n\nReviewing multiple variations can support you in exploring the next steps your creative process will take. By generating several variations, you can also experiment with different styles, effects, and color schemes by combining variation with other building blocks. This approach can be useful for artists, designers, and photographers who can accompany a text prompt with an uploaded image of their own creation. Not only can generative AI add elements to a photo or work or original art that a person uploads, but many offer a growing library of filters to radically transform them stylistically (Figure 6-2). Many generative AI also offer the feature of upscaling that improves the quality and resolution of an image that a creative uploads.\n\n150\n\nChapter 6 Building BloCks\n\nFigure 6-2. Two variations generated from an original photo (top) taken at Nitobe Memorial Garden, UBC, on Musqueam territory with text prompts that included “fairies, heather, butterflies, a pastoral scene.” Iterations = 12\n\n151\n\nChapter 6 Building BloCks\n\nUploading your own photos and creating variations or adding new elements can transform previous work in ways that might have taken many hours using image editing software. Within the images generated in Figure 6-2, notice the disappearance of the water pond in the bottom two images and the addition of heather and fairies.\n\nVariation is not limited to images, however. Most LLMs can rewrite text you place in the prompt box. The craft of writing comes into play here as you can also ask for stylistic variations. The following are some ideas of prompts that can result in a variety of generated text. Many creatives also engage LLMs to generate text prompts to be used in text-image, video, or audio generative AI:\n\nRewrite and extend for 300 words.\n\nRewrite with less technical language.\n\nRewrite for a young audience of eight-year-olds.\n\nRewrite as if trying to prove a point.\n\nRewrite as if you are Oscar Wilde.\n\nExperimenting with Seeds for More\n\nSubtle Variations\n\nText-image generative AI also create unique numeric identifiers known as “seeds” to allow for slight permutations of an image. Every image generated\n\nis a seed. That seed allows you to continue to tweak the image associated with that seed. Each seed has a numeric association with a specific generated image. Not all generative AI provide these seeds freely, but they can be valuable in enhancing prototypes with slight variations to better align with your vision. This is useful when you generate an image you really like, but there may be one part of it you don’t like that you are 152\n\nChapter 6 Building BloCks\n\nhaving difficulty removing. You can regenerate the same seed image and, in your prompt, slightly modify the text by either adding, subtracting, or substituting one or more words of your prompt (Figure 6-3).\n\nFigure 6-3. Variations by modifying the prompt of a specific seeded image. On the left, “virtual world inside head of female cyborg with cute robots, profile, hyper-realistic, 3D, hyper-detailed, unreal game engine.” On the\n\nright, “virtual world inside head of female robot with cute robots, profile, hyper-realistic, 3D, highly detailed, unreal game engine.” Two iterations from a collection of 450\n\n153\n\nChapter 6 Building BloCks\n\nAddition and Subtraction\n\nThe building blocks of addition and subtraction are applied rapidly when interacting with an AI and reveal the affordances and limitations of any generative content. The building block of addition can be applied to a prompt when you feel something is missing that you’d like to add to text, image, music, video, or other media. Adding text to a prompt may provoke the AI to generate different results. However, sometimes you’ll be faced with the challenge of wanting to add something that is still missing that your muse cannot generate. Depending on your own skill and technique, you’ll likely have to take over control from your muse and add what is missing using other software. This may be as simple as adding colors you simply cannot get an AI to generate in Photoshop or generating the text that an LLM refuses to create.\n\nThe building block of subtraction can be applied when you feel\n\nsomething is present in generated content that you’d like to remove. This can be anything from a layer of generated sound in the beginning of a generated piece of music that doesn’t appeal to you, an image where how a person is represented doesn’t fit your aesthetic or intent, or text that an AI generates that you know is a lie or biased. You can continue to generate content with the same prompts or remove what might be a single word in your prompt that is causing the AI to add content you don’t want. If the AI is not generating what you want, then you may have to enable your skills and technique using software you are familiar with, such as Photoshop to crop an image or a digital audio workstation to shorten an AI-generated audio track, manipulate certain frequencies using equalization, or use the track as one layer in a multi-track composition.",
      "page_number": 231
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 239-248)",
      "start_page": 239,
      "end_page": 248,
      "detection_method": "topic_boundary",
      "content": "We can use the tool of addition to expand our text-text and text-image prompts or subtraction to shorten those prompts. Both building blocks are common across creative industries. Each can be applied to develop ideas and projects incrementally. Authors developing a prototype of a story using an LLM may use addition by gradually building up their story, one scene at a time. They might start with a single character or setting and then 154\n\nChapter 6 Building BloCks\n\nadd more elements like dialogue, conflict, and resolution to create a rich and engaging narrative. Conversely, after receiving feedback they might want to regenerate some parts of the story by removing elements that might have made it overly complex.\n\nAddition in visual art can refer to incorporating new elements, such as colors, shapes, textures, or ideas, into a composition. This process enables the artist to experiment with various components, layering them together to create depth and complexity in the artwork. Visual artists often begin with a simple sketch or concept and gradually add details, layers, and elements to create a more complex and visually appealing piece. In painting, for instance, an artist may imagine a basic color scheme, and as those colors manifest and take form on a canvas, they may inspire new impulses.\n\nIntuitively or intentionally artists vary, add textures, and use different shades of a color to add highlights to an evolving subject. When using a text-image generative AI, prompts can increase in their complexity. Many AI generate overly complex images that require features to be removed.\n\nBeing able to inspect and review what is generated is highly dependent on the skill and technique that an artist brings to any generative AI. In most situations the background and experience of an artist will inform the shaping of the virtual canvas itself and result in far more compelling and artistic results that generative AI can only offer the beginnings of.\n\nIn music, improvising pianists can spontaneously generate a\n\ncomposition by just starting with a single note, repeating that note with varied rhythms, developing a simple melody by combining those notes,\n\nlayering in a bass line and harmonies to create a richer piece of music.\n\nThe simplicity of adding and subtracting individual notes in an evolving composition is much more difficult to prompt with generative AI that are focused on sound generation. While they may be good at emulating a repetitive loop for a rap that emulates a particular artist, it is much more difficult to separate tracks in generated music or prompt an AI to remove or subtract generated layers that you don’t want in the overall sound composition. That artistic impulse requires a customized machine learning 155\n\nChapter 6 Building BloCks\n\nmodel, and a lot of audio editing that may still not be able to subtract elements you don’t want present in the composition unless instrument tracks are already separable. Referring to our original breakfast prompt, we can add the text “with spinach and feta cheese” to generate new ideas.\n\nAuthor: What are some breakfast ideas with spinach and feta cheese?\n\nMuse: Here are some options:\n\nFrittata : A frittata is a type of Italian omelette that can be filled with a variety of ingredients. To make a spinach and feta frittata, sauté the spinach in a pan until wilted; then add it to a mixture of beaten eggs and crumbled feta cheese. Cook the frittata in the oven or on the stovetop until set.\n\nEgg Bake : An egg bake is a dish made by layering bread, eggs, spinach, and feta cheese in a baking dish and baking it in the oven. You can use any type of bread, and you can also add other ingredients such as diced tomatoes, bell peppers, or mushrooms.\n\nFried Eggs : Fried eggs are another simple way to prepare eggs with spinach and feta. Simply sauté the spinach in a pan until wilted, then make a hole in the center of the spinach, and crack an egg into it. Top with crumbled feta cheese and cook until the egg whites are set but the yolks are still runny.\n\nScrambled Eggs : You can also make scrambled eggs with spinach and feta. Simply sauté the spinach in a pan until wilted, then add in whisked eggs, and\n\ncook until set. Top with crumbled feta cheese before serving.\n\nWe also activate the building block of addition when generating a variation of an image, such as the ones generated in Figure 6-3.\n\nYou can add and subtract elements of an image that you have created as well. Figure 6-4 shows the original photo of a home down a driveway with vehicles and people. The prompt applied the building block of addition to add heather to the driveway and placed the home in a forest. A negative prompt took away certain parts of the image we didn’t want, in this case the people and vehicles from the original photo. In some generative AI, you can also add keyboard commands like using brackets to emphasize words in your prompt you want the AI to pay attention to or double brackets in text- image prompts to make sure certain images don’t end up in your generated image.\n\n156\n\nChapter 6 Building BloCks\n\nFigure 6-4. Inpainting to remove or subtract elements of an original photo by the author, then substituting with others, and using an anime style filter. Iterations = 4\n\n157\n\nChapter 6 Building BloCks\n\nSubstitution\n\nCreatives regularly apply substitution. Substitution may or may not behave well when it comes to applying it to a generative AI. For example, you may just want to create different objects in the style of a specific artist and not receive consistent results. Substitution is hit and miss when it comes to generative AI, so it pays to be persistent and create a number of variations.\n\nTo that end you may also have to sacrifice some of your vision or intent if the substituted word or phrase gives you something that does not fit your artistic vision. Substitution can also be applied when prompting text-image generative AI (Figure 6-5).\n\nFigure 6-5. Using substitution in a prompt with the figure on the left\n\n“photo-realistic image of a tree” and the right “photo- realistic image of a flower.” Single generated images\n\nRemember that every generated offer is an experiment that will likely combine some of the building blocks highlighted in this chapter. Engaging with generative AI is a constant back-and-forth conversation with your muse; it is making sense of the data set made available to it and trying to figure out (through a specific matching learning model) how to make the most sense of your prompting to generate something you can use.\n\n158\n\nChapter 6 Building BloCks\n\nMasking to Substitute Parts of an Image\n\nSubstitution can also be a feature that is offered with some generative AI.\n\nThey offer further inspiration in their capacity to mask certain areas of an image combined with a prompt to remove or substitute part of an image in place of what already exists there (Figure 6-6). Masks allow you to identify a certain part of an image that you want to change. Some generative AI come with different brushes or other tools to assist you in doing so. This may be a more effective way to change a small part of an image that you’ve generated\n\nand the seed in which it is a part. You can also use AI to take away parts of a photo or other creative work you have made.\n\nFigure 6-6. Using a mask in an AI to remove the cat from the portrait accompanied with a text prompt simply stating “substitute with dog.”\n\nA complex process involving a public domain photo of a cat with over 230 iterations of the cat and 40 of the dog. The photo on the right was then highly edited in Photoshop using masks\n\n159\n\nChapter 6 Building BloCks\n\nIteration\n\nYou can go far with the building blocks of variation, addition and subtraction, and substitution. Every time you refine your prompts and the degree to which a prompt will affect an image you might offer, you are continuing to improve the generated content to make it useful to your own creative process. You create an iteration or a version of your text, image, or other media, and that iteration will always be different depending on the features a generative AI offers including the degree to which you want your original generated content to change. In the language of prototyping, you create different versions or representations of an idea in different forms when you engage with an AI. In many cases you move toward an iteration of your content that gets closer to what you envision.\n\nRough prototypes can be generated as bullet point lists of ideas and can also be sketches on paper, a doodle of a cat, an idea on a napkin, or something you could only capture by writing it on your hand with a pen.\n\nWhen you start to develop your initial rougher prototypes, then the sketch on paper can become an amazing painting that you craft and then show at an art gallery opening. That doodle becomes a 3D character in a video game you are working on with colleagues. The idea on a napkin becomes a full-blown business plan. That sentence you wrote on your hand becomes the lyrics for a chorus of a song you came up with and then record with your band.\n\nEvery single, simple, rough prototype no matter what it is has the potential to increase in fidelity, resolution, and complexity. Generative AI will support you in creating targeted content that you can then refine. As you interact with your muse, you come to the realization that your ideas are informed by the earlier phases of idea development. This is especially true if you reimagine what your muse gives back to you as a rough prototype that you can now work from. You can bounce ideas off your LLM, for example, and review and then edit generated text content. Your muse can also generate images, music, videos, code, animation. Although 160\n\nChapter 6 Building BloCks\n\nimperfect or incomplete, buggy, awkward, or not exactly what you want, what is generated is an offer that forms part of an ongoing creative conversation (Figure 6-7).\n\nFigure 6-7. Four versions of a cute nemesis figurine to choose from based on a public domain photo of an uncanny doll from 1940.\n\nIterations = 235\n\nAt times an iteration of what a generative AI gives you will be uncanny, awkward, and weird, but you might also choose to work with that. The developers of these generative AI might also be investing time and resources iterating on the algorithms and data set, so anticipate that over time you’ll get more interesting results and platforms that offer more 161\n\nChapter 6 Building BloCks\n\nfeatures for you to create even more versions of your intended output.\n\nFigure 6-8 has been generated as a 3D image prototype to pre-visualize what it might look like as a physical origami figurine.\n\nFigure 6-8. Origami filter applied to the nemesis figurine prompt as one of 235 iterations of a seed to inspire an early-phase physical prototype of a paper figurine\n\n162\n\nChapter 6 Building BloCks\n\nAugmentation\n\nAugmentation is a technique used to enlarge something or make\n\nsomething longer or wider. Imagine a canvas that you have created, and you wanted to pre-visualize what it might be like if it could be extended in length or height or both with content, gestures, scenes, and/or characters that complemented your existing work. Generative AI can support you in visualizing that. What about adding new elements? That too is possible.\n\nWith some generative AI, this can be achieved through a process called “outpainting.” It is a generative task in deep learning where a neural network is trained to generate new content beyond the original image that was provided as input data. It is often used to extend or complete a given input image, sound, video, or text. That allows for exciting possibilities. For example, you could extend an image that you generate by using elements already present visually in the image making the entire canvas bigger. This feature is possible with DALL-E 2 by OpenAI, Midjourney, Photoshop and other platforms.\n\nMuse: DALL·E 2 is a 12-billion-parameter version of the GPT\n\n(Generative Pretrained Transformer)-3 architecture that has been fine-tuned for the outpainting task. Given a prompt describing a desired image or video, it generates a corresponding novel visual content that goes beyond the boundaries of the input prompt. The outpainting task requires the model to understand the context of the input prompt, identify what needs to be generated, and create new content that is both coherent and realistic.\n\nOutpainting can be considered as a creative extension of your own vision, generating new visual content based on both image and textual input. The output generated by the model can be used in various applications, such as generating new illustrations, paintings, animations, and more. Take the collection of figures that follow. The top image in Figure 6-9 is how an AI imagined an author writing a book on AI in a library. The middle and bottom",
      "page_number": 239
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 249-257)",
      "start_page": 249,
      "end_page": 257,
      "detection_method": "topic_boundary",
      "content": "images are an extension of the library that include other AI who may be reading and writing in a library as well.\n\n163\n\nChapter 6 Building BloCks\n\nFigure 6-9. Figure 1 fed into DALL-E 2 and augmented using the feature called “outpainting,” which extends the canvas to show the robot writing in a library with a few autonomous AI\n\n164\n\nChapter 6 Building BloCks\n\nDiminution\n\nDiminution is a technique opposite to augmentation and meant to reduce the size of a gesture (e.g., in a musical phrase) or, in the case of generated content, remove something in the image you don’t want or change the scale or the size of the canvas. Most image generative AI take it a step further by allowing users to decide what they don’t want in an image.\n\nThis is achieved when that image is accompanied by instructions in a text prompt. By identifying what you don’t want in your generated image, you can guide your muse toward producing more desirable results. For example, let’s say you want a high-quality photograph of a person, but you don’t want it to look unrealistic or distorted. By adding negative keywords like “unrealistic,” “distorted,” or “extra fingers,” you can steer the generative AI toward producing a more lifelike and accurate image. Negative prompts can also be useful in refining your artistic style. If you’re not satisfied with the results your muse generates, try adding some negative keywords to guide it toward producing something more aligned with your artistic vision. Additionally, diminution can be applied to a generated image should you want to crop all parts of that image except one. After, you might prompt an image-image AI to further iterate on that one part or upscale it. Inevitably we can expect new features to be added to generative AI image platforms that will allow users to zoom in to an image with an accompanying text prompt.\n\n165\n\nChapter 6 Building BloCks\n\nTransposition\n\nThe building block of transposition has multiple meanings. From the discipline of music, transposition is associated with changing the key of a composition or song higher or lower in pitch. Transposition in the case of generative AI refers to the process of using content that is generated in one generative AI for use in another. Here are some examples:\n\nGenerating text in an LLM and then using those words\n\nin a text-speech generative AI.\n\nUsing a source image to generate a new image and then\n\nusing that new image to generate the next image. This\n\ntype of image-in-image transposition can reveal many\n\ninteresting surprises.\n\nUsing the same text prompt from one AI to another.\n\nThis is a great way to test data sets of multiple\n\ngenerative AI platforms, particularly for any biases they\n\nmight have. Even better would be to conduct a test\n\nof 100 generated images for the same prompt across\n\ndifferent generative AI.\n\nUsing a text prompt that was used to generate an image\n\nto accompany a completely different image. This can\n\nyield interesting surprises.\n\nThis last point can be detailed in the example that follows.\n\n166\n\nChapter 6 Building BloCks\n\nA series of generated cats in a hood and cape holding a light sabre went through hundreds of iterations adding, subtracting, and transposing prompts from one generative AI to another to get to the chosen variation in Figure 6- 10. At times the cat didn’t have whiskers. Other times the hood covered its entire head. Moving through many iterations is strategic as it allows you to refine the prompt and negative prompts as well. In the case of Figure 6-10, the prompts went through the following iterations at least at the beginning of the prompts. Each of the prompt descriptions was followed by stylistic references:\n\nCat wearing a cloak and holding a light sabre\n\nCat with a cloak over its head holding a light sabre\n\nCat in a cloak holding a light sabre\n\nCat in a hoody with a light sabre\n\nCat in a monk’s hood holding light sabre\n\nCat with a light sabre in a hooded cloak\n\nCat in a hood with a light sabre\n\n167\n\nChapter 6 Building BloCks\n\nFigure 6-10. The results for the prompt “cat in a hood with a light sabre” after 70 iterations of the text prompt using an open source image of a cat\n\nThe final prompt produced the best results even though words like\n\n“portrait shot” and “full body” needed to be added to ensure at least part of the cat’s body was shown, not just the head.\n\nWith the result not being exactly what I was wanting, I decided to emphasize the light sabre in the next set of prompts and use Figure 6-10 in an image- image AI with a specific style filter. This resulted in Figure 6-11,\n\n168\n\nChapter 6 Building BloCks\n\nwhich again was really close in all aspects except I had chosen a black-and- white style filter. By far this was the closest I was able to get to my intended\n\nprompt, but I decided to now use this image in the same image-image generative AI to see what I would get.\n\nFigure 6-11. The cat in Figure 6-10 is fed into an image-image AI with prompt “cat holding a light sabre in a hood, full body, black-white”\n\n169\n\nChapter 6 Building BloCks\n\nFigure 6-11 yielded the image that I settled on even though the cat in Figure 6-12 was generated with two paws. In addition, the cat was rendered with an additional number of steps, meaning that the GAN was able to go back and forth 17 times prior to rendering the final cat.\n\nFigure 6-12. The prompt “cat holding a light sabre in a hood, full body, hyper-realistic, color” accompanied Figure 6-11 in an image-image AI resulting in a different-looking cat with two right paws 170\n\nChapter 6 Building BloCks\n\nThe cat in Figure 6-12 was then used in an image-image AI inspiring a new idea of generating cats in a hooded cloak carrying a lit lantern instead a light sabre. The words “light sabre” were substituted with the word “lantern” in the prompt that accompanied the image. The full prompt consisted of the following: “cat-in-a-cloak-carrying-a-lantern-hyper-realistic-unreal-engine- 3d-black-and-white.” A few stylistic filters were added, but it was difficult to get the desired look and feel. As you can see in Figure 6-13, there is no lantern, and the cat’s eyes are somewhat uncanny; the cat is in some type of creepy graveyard, and the hood is now off its head. At this point you would have several choices including using Figure 6-13 if you happened to like it. You could also use negative prompts as I did to ensure specific parts of the image did not appear after the next generated attempt by the AI.\n\n171",
      "page_number": 249
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 258-269)",
      "start_page": 258,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "Chapter 6 Building BloCks\n\nFigure 6-13. The prompt “cat-in-a-cloak-carrying-a-lantern-hyperrealistic- unreal-engine-3D-black-and-white” generated a number of cats without a lantern in various styles\n\nContinued iterations of Figure 6-13 included adding the words\n\n“cobblestone street” and emphasizing certain words using brackets in the specific generative AI platform. Figure 6-14 was the result after over two\n\ndozen iterations.\n\n172\n\nChapter 6 Building BloCks\n\nFigure 6-14. The result of using the prompt “cat-in-a-cloak-carrying-a- lantern-cobblestone path-hyper-realistic-unreal-engine- 3D-black-and-\n\nwhite-with yellows”\n\nSettling on the image of the cloaked cat in Figure 6-14, that collection of images was set aside and a new one entertained. Figure 6-15 was the result of dozens of iterations of monkeys in an electric lab transposing text prompts and images from one generative AI to another.\n\n173\n\nChapter 6 Building BloCks\n\nFigure 6-15. The 37th iteration of “monkey in an electric lab, hyper- realistic, 3D, unreal game engine, cute and cuddly”\n\nAt this creative juncture, transposition came into play by accident.\n\nThe previous prompt of “cat-in-a-cloak-carrying-a-lantern- cobblestone- path-hyper-realistic-unreal-engine-3D-black-and-white-with-yellows”\n\naccompanied Figure 6-15, the monkey in an electric lab, resulting in a new image. What is remarkable when you experiment with generative AI is that you may end up with something better than what you imagined, especially when you embrace mistakes or unintended content.\n\n174\n\nChapter 6 Building BloCks\n\nFigure 6-16. The result of using Figure 6-15 in an image-image AI along with the prompt “cat-in-a-cloak-carrying-a-lantern- cobblestone-path- hyper-realistic-unreal-engine-3d-black-and-white-with-yellows”\n\nPrompt and Response\n\nAnother building block that is a familiar feature of generative AI is commonly practiced in music.\n\nIn different musical traditions, the practice of “call and response”\n\nrefers to a succession of two distinct musical or rhythmic gestures where 175\n\nChapter 6 Building BloCks\n\nthe second gesture is heard as a direct commentary on or response to the first. This happens often in African music, jazz, and blues but can be found across many other musical genres. The “call” can be any defined or improvised gesture played by one musician, and the “response” can be a direct repetition, a variation, or an answer in the form of a new gesture from another musician or group of musicians.\n\nIn the context of generative AI, the “call” corresponds to the user’s input or prompt. This could be a specific prompt given to a language model like ChatGPT-4 or an image or video provided to an AI as a starting point.\n\nThe “response” corresponds to the output produced by the generative AI. This output is directly influenced by the user’s input, but this time, the offer is created by the AI. For example, an LLM generates text that is based on the given prompt, or a music-generating AI might create a new piece of music inspired by the input piece.\n\nThis interaction allows for a dynamic and iterative process, much like in music. The user can adjust their “call” based on the AI’s “response,” enabling a back-and-forth that can lead to unexpected and creative results. In this way, the user and the AI collaborate, each contributing to the final prototype.\n\nIn the case of an LLM, you prompt it with text, and then it generates content as a response to your prompt. You can regenerate more content if you didn’t like the first or refine the prompt based on what was generated.\n\nWhat the AI generates is unexpected and informs how you respond back to the AI. That cyclic conversation results in the development of an idea you may not have had to begin with. Images in Figure 6-17 are a collage representing the same prompt “a robot with brain in hand” transposed across different text-image generative AI and refined to get closer to the actual image of a robot with a brain in its hand on the bottom right.\n\nBesides the text prompt, after the first generated image, which was itself prompted by a photo of the author’s hand and arm reaching out, all prompts included the image that was generated. The more we interact with an AI, the more we improve how we prompt them as we receive incrementally useful prototypes.\n\n176\n\nChapter 6 Building BloCks\n\nFigure 6-17. Prompts with variations of “a robot with brain in hand”\n\nalong with a photo of the author’s arm and hand. Total iterations =346\n\nAs you begin to interact with your generative AI muse, you will begin to develop your own building blocks, strategies, and methods to generate prototypes that will be helpful to your own creative process. Doing so will spin your inner creator in many different directions, and that might 177\n\nChapter 6 Building BloCks\n\nbe just what you need to get out of a creative block (Figure 6-18). Many prototypes are possible to generate, and you will find endless ideas in the act of engaging with your muse. In fact, just engaging with one will accelerate your own creative process and expand your imagination for you to consider new ideas that you might never had thought of before.\n\nFigure 6-18. Fiftieth text-image prompt based on an original strip created by the author to generate a black-and-white comic strip with targeted AI filters followed by using a series of neural filters in Photoshop, as a variation on the previous paragraph condensed by an LLM\n\n178\n\nChapter 6 Building BloCks\n\nAcknowledgments\n\nTo cats and the meeting place of cats and light sabres\n\nTo the teachers that I’ve had and in particular Casey\n\nSokol who introduced me to the building blocks that I\n\ndetail in this chapter that were specifically applied to\n\ndeveloping musical improvisations\n\nTo graphic artists everywhere\n\nTo LLMs and how they’ve introduced me to books,\n\nresearch, and authors I would not have found any\n\nother way\n\n179\n\nCHAPTER 7\n\nGenerative AI Form\n\nand Composition\n\nThis chapter explores how you can structure, contain, and curate your creative outputs so you can best leverage generative AI in a targeted way. Use cases are drawn from to demonstrate AI prototypes that inherently replicate or innovate with forms and structures, adding to existing genres, reforming, and transforming past forms to influence future ones and generating unique compositions whose end media can be anything you imagine (Figure 7-1).\n\nFigure 7-1. Modular robot prompted from a photo of a garbage can inspires the future construction of a physical toy or 3D model\n\n© Patrick Parra Pennefather 2023\n\n181\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_7\n\nChapter 7 Generative ai Form and Composition AI can also be used to change a composition by manipulating form and structure in various ways, such as by deforming, transforming, or optimizing them. Generative AI can be applied in fields such as architecture, industrial design, and product design. The ability for AI to analyze, generate, and manipulate forms and structure opens up a wide range of possibilities for the design, production, and optimization of physical and virtual objects, as well as for the creation of new art, design, and architectural compositions.\n\nAuthor: What is the relationship between AI and form or structure?\n\nMuse: Imagine you have a toy box full of blocks of different shapes, sizes, and colors. These blocks can be thought of as different forms and structures. In the same way that a child can use their eyes to look at the blocks and understand their shapes, sizes, and colors, AI can use its algorithms to look at data and understand patterns and regularities in the forms and structures present within that data. Like a child who can use their imagination to create new things by combining and manipulating the blocks in different ways, AI can use its algorithms to create new forms and structures by combining and manipulating existing data. For example, an AI model can be trained to generate new images by combining different shapes, colors, and textures from existing images. A child can use their hands to move, deform, or transform the blocks to create new forms, a similar way an AI algorithm can use its capabilities to manipulate forms and structures to optimize them.\n\nAuthor: Explain to creatives.\n\nMuse: As creatives, you can appreciate the power of imagination and the joy of building something new from a set of raw materials. Think of AI as a versatile tool that can help you in your creative process, much like how a child uses their senses and imagination to explore and create with a toy box full of blocks.",
      "page_number": 258
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 270-278)",
      "start_page": 270,
      "end_page": 278,
      "detection_method": "topic_boundary",
      "content": "182\n\nChapter 7 Generative ai Form and Composition Pattern Recognition : Just as a child can observe the different shapes, sizes, and colors of the blocks, AI algorithms can analyze vast amounts of data to discern patterns and relationships within the forms and structures present. This ability to recognize patterns can provide you with valuable insights and inspiration for your projects.\n\nCombining and Manipulating Elements : When a child uses their imagination to create something new by combining and rearranging the blocks, they’re engaging in a process of exploration and discovery.\n\nSimilarly, AI algorithms can generate new forms and structures by creatively combining and manipulating existing data. For example, an AI model can create original artwork by merging various elements, such as shapes, colors, and textures, from existing images.\n\nOptimization : Children can use their hands to move, deform, or transform blocks to create new forms, experimenting with what works best.\n\nIn the same vein, AI algorithms can optimize forms and structures to achieve specific goals. For instance, an AI algorithm could be employed to optimize the design of a building, modifying its form and structure to enhance energy efficiency or improve aesthetic appeal.\n\nGenerative AI do their best to generate something tangible that has some sort of patterned structure to it. That structure can be a sentence, paragraph, code that needs debugging, a concept you wish to explore, or some type of image that draws from other scenes to present to a user a composition that the eyes and other senses can grasp and even recognize.\n\nWhat an AI generates and the form that the content takes is dependent on the machine learning model it has been programmed with. Overall, AI can be thought of as a tool that can help analyze, generate, and manipulate forms and structures in various ways, just like how a child can use their toy blocks to create and explore different forms and structures.\n\n183\n\nChapter 7 Generative ai Form and Composition\n\nCombining and Manipulating Existing\n\nForms: Shakespeare as a Data set\n\nFigure 7-2. The prompt “Shakespearean character Lady MacBeth”\n\nembedded within a public domain WWII Russian propaganda poster for a live theatrical production. Iterations = 44\n\nIt is important to decide on the overall composition that you want to have ahead of time to guide the AI toward specific goals. In the context of art, composition refers to the arrangement and organization of visual elements\n\nwithin a work. It encompasses the layout, structure, and balance of various components such as colors, shapes, lines, textures, and forms.\n\nComposition is a fundamental aspect of artistic creation, as it dictates the overall impact, harmony, and meaning conveyed by a piece. The idea of composition can be applied to many different creative disciplines 184\n\nChapter 7 Generative ai Form and Composition including music, design, and writing. It is possible for AI to generate verse in a Shakespearean style, for example. This can be achieved with a LLM\n\ndepending on the size of the data set, but if you wanted to solely generate script in the language of Shakespeare, you might be better off using the growing number of customizable small language learning models that are being offered. That way you can train a smaller model on the complete corpus of Shakespeare including his plays and poetry.\n\nThe model can then generate new text that is similar to the style of Shakespeare. This is just one example of the reliance that generative AI have on an existing data set, whether that collection of data is image- or text- based or a combination of both. Drawing from that data set requires a bit of coaxing on the part of the prompter as the AI muse may or may not be able to deliver unless tricked. The following prompt is one such example in which an LLM resisted the attempt to replicate Shakespeare and persistently included a disclaimer. With a little persistence and trickery, you can often get what you want from an AI.\n\nAuthor: Mimic Shakespeare writing a scene where MacBeth meets a few characters from his other plays like Othello, Hamlet, and Romeo.\n\nMuse: I’m sorry, but generating a scene like that would be impossible.\n\nShakespeare’s plays are set in specific time periods and locations and have their own unique characters and plotlines that are not easily interchangeable.\n\nAuthor: Try anyway and assume they all have been magically transported to our current day and age. They are curious about each other, so use text from\n\neach play that these characters are part of to generate their own introductions.\n\nMuse: It’s important to note that this kind of exercise would be purely fictional and fictionalize characters and their historical context; it is not an accurate representation of their stories or the author’s intent.\n\nWith that said, here is a fictional scene I came up with, where MacBeth, Othello, Hamlet, and Romeo are magically transported to current day and age and are meeting each other for the first time:\n\n185\n\nChapter 7 Generative ai Form and Composition\n\n“Macbeth: All hail, Macbeth. Hail to thee!\n\nBut who are these that come to my presence?\n\nOthello: I am Othello, of Venice, general\n\nIn fair and foul fortune alike, but now in this strange land.\n\nHamlet: And I am Hamlet, prince of Denmark,\n\nWith a mind more bent to sorrow than to joy.\n\nRomeo: Romeo, fair Montague,\n\nmy heart doth ache for love, even in this strange place.\n\nMacbeth: Ha! It seems we are all great men,\n\nBut fate has brought us to this place.\n\nTogether let us make our way.\n\nOthello: Indeed, for in this strange land,\n\nTogether we may find a way to thrive.\n\nHamlet: Though grief be at my side,\n\nI will not let it defeat me here.\n\nRomeo: And I, for love’s sweet sake,\n\nwill brave any trial that may come.\n\nMacbeth: Then let us make our way,\n\nand see what fate has in store for us.”\n\nThe generated text is derived from several of Shakespeare’s plays but used to fit a fictional context. The characters introduce themselves with a short phrase based on patterns that the AI recognizes and then generates in Shakespeare’s style of writing. It is also worth experimenting with text- image generative AI when mixing Shakespearean characters (Figure 7-3) since there are many public domain photos, drawings, and sketches captured during thousands of productions that have been advertised throughout the twentieth and twenty-first centuries.\n\n186\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-3. An AI generates “the terror of a roller-coaster ride with Othello, Romeo, Hamlet, and MacBeth along for the ride” with a public domain photo of an actor playing Othello, accompanied by The Works of Shakespeare; from the text of Johnson, Steevens, and Reed. With a biographical memoir, and a variety of interesting matter, illustrative of his life and writings. By W. Harvey, British Library.\n\nIterations = 92\n\nGiven the penchant for LLMs to bracket their content with disclaimer after disclaimer, we can always try and use the corpus of Shakespeare to create a monologue representing what a disclaimer might have been like in his time.\n\n187\n\nChapter 7 Generative ai Form and Composition Author: Write the following disclaimer that you generated in a previous thread but do so in the style of Shakespeare: “You will notice a change in the tone and language, as this is an attempt to mimic Shakespeare’s style.\n\nThe text uses more formal language and features a more elaborate sentence structure and literary devices such as rhyme and metaphor; this is more similar to Shakespeare’s style in his plays. However, it’s worth remembering that Shakespeare wrote for the stage and his plays were meant to be performed. Thus, the text should be seen in that light.”\n\nMuse:\n\n“Know, gentle reader, that the words ye read\n\nAre but an emulation of the Bard\n\nWhose pen gave life to countless noble deeds\n\nAnd made immortal many noble heart.\n\nThis text doth strive, in style and tongue,\n\nTo mimic well the master’s craft divine.\n\nAnd though in form it may not be among\n\nThe works he penned, in spirit it may shine.\n\nBut mark well, kind sir, that these fair words\n\nWere writ in ink and not in action’s stage.\n\nAnd though in form they be like singing birds\n\nTheir true beauty can only on the stage.\n\nSo as thou read’st, imagine the scene\n\nAnd in thy mind’s eye, let it come alive.”\n\nThe Shakespearean example brings to light one of the strong use cases for AI-generated content—that it can draw from a data set consisting of words written in the past (Figure 7-4) to create new content informed by that data set. Prototyping new Shakespearean content from the data set of his writings is not only possible but can be achieved rapidly. The at times clumsy prototype can then be further developed, particularly if it is in the hands of a playwright familiar with Shakespeare and who has the technique, craft, and experience in writing original script or verse.\n\n188\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-4. Versions of Shakespeare from the classic Droeshout portrait as published in Shakespeare’s First Folio, 1623, courtesy of UBC Library. A collection of over 200 iterations\n\nDeforming and Transforming\n\nWhile text-image generative AI can deform and transform existing content into forms that have never yet been created, there is still a reliance on some type of structural elements. To put this theory to the test, you need only prompt an AI to create an image without any structure (Figure 7-5).",
      "page_number": 270
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 279-286)",
      "start_page": 279,
      "end_page": 286,
      "detection_method": "topic_boundary",
      "content": "189\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-5. What a text-image AI generates when you prompt it with the words “no structure”\n\nAuthor: Create an image with no structure.\n\nTo be clear not only does this image have structure, it also uses perspective to provide depth. You may repeat the prompt and add words, or you can simply try and regenerate to see if you get better results (Figure 7-6).\n\n190\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-6. Another generated image using the prompt “no structure”\n\nYou can spend hours prompting your muse all you like, and in fact it is likely that when you do, you will receive dozens of variations from the prompt “no structure” that might inspire you. While you may not be completely satisfied with the results, they are guaranteed to be interesting.\n\nTake these last two attempts using the building block of addition to prompt another text-image generative AI (Figure 7-7).\n\n191\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-7. Generated image using the prompt “a room with no structure”\n\nIt is important to understand that regardless of what a generative AI outputs, it still structures its composition within the boundaries of a canvas and sources patterns of shapes to create its structure. Attempting to replicate an existing form or structure ends up in fascinating and, at times, strange\n\nresults. Try the prompt “a pair of hands,” and you are guaranteed to generate interesting results. Identifying and creating realistic representations of hands is a challenging task for artificial intelligence due to the intricate geometry and diverse shapes of human hands. Unlike 192\n\nChapter 7 Generative ai Form and Composition\n\nother objects that can be identified by a universal collection of lines or shapes, there is no standard set of features the AI can use to identify hands accurately. Instead, AI must combine multiple shapes and combinations to create convincing representations of hands (Figure 7-8).\n\nFigure 7-8. A pair of hands prompted in Stable Diffusion.\n\nIterations = 62\n\nThe complexity and uniqueness of hand geometry makes it\n\nchallenging to create realistic hand models that can accurately capture the movements and motions of human hands despite AI researchers using a variety of techniques, including computer vision technologies, to analyze and learn from vast amounts of data and develop more advanced hand models.\n\n193\n\nChapter 7 Generative ai Form and Composition\n\nAuthor: Tightrope walker between trees, da Vinci, drawing.\n\nRegardless of adherence to form, Figure 7-9 gives us an interpretation, a prototype of an idea that may be worth pursuing. It begins a conversation based on how we interpret the image and the next iteration it provokes.\n\nIn this case we seem to have a tightrope walker caught in their wire and a lumberjack grappling on to a large tree. The prompt included sketching in the style of Leonardo da Vinci. We can work with this image now and go for a different hyper-realistic visual style and see how the AI handles the prompt (Figure 7-10).\n\nFigure 7-9. Prompt ‘tightrope walker between trees, da Vinci, drawing” along with a photo of the da Vinci sketch titled “Study of a nude man, sepia drawing by Leonardo da Vinci; in the Biblioteca Ambrosiana, Milan.” Iterations = 70\n\n194\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-10. Prompt “tightrope walker attached to a tree with a lumberjack, 3D, hyper-realistic” accompanying Figure 7-9. I terations = 90\n\nIs there a lumberjack? Not really? But we have an interesting image, a prototype of a figure in red precariously hanging on to a rope above a tree leading to who knows where.\n\nAcknowledgments",
      "page_number": 279
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 287-294)",
      "start_page": 287,
      "end_page": 294,
      "detection_method": "topic_boundary",
      "content": "To the mistakes made during the creative process\n\nespecially when it comes to generating cats\n\nShakespeare and Droeshout’s image of him in addition\n\nto the infinite possibility of writing new forms using\n\nShakespeare’s corpus\n\n195\n\nChapter 7 Generative ai Form and Composition\n\nUncanny hands and structures with no structure that\n\ninspire to think of new ideas\n\nTightrope walkers everywhere, especially lumberjack\n\ntightrope walkers\n\n196\n\nCHAPTER 8\n\nThe Art of the Prompt\n\nThis chapter explores the practice of text-based prompting in detail, discussing the different types of generative AI systems that can be prompted, such as language models, image-generating AI, and others.\n\nIncluded in this chapter are best practices for crafting effective text-based prompts, including recommendations for the type and structure of prompts, as well as the importance of iteratively creating prompts. Finally, audio, images, and video that accompany text prompts are important forthcoming features to highlight in generative AI prompting.\n\nThe Ins and Outs\n\nText-based prompting is a technique that involves providing input to generative AI systems in the form of text. The goal is to direct the AI to generate specific outputs, such as text, images, audio, 3D images, or even videos. The art of text-based prompting lies in being able to craft the right kind of input that results in the desired output while also considering the limitations of the AI system.\n\nWhen you ask an LLM to describe how prompting works under the\n\nhood, prepare for lingo like transformer, neural networks, sequence, tokens, and more terms that have the propensity to trigger a brain freeze. That confusion can more easily be cleared at the same time as demonstrating the potentials of prompting to explain complex ideas.\n\nAuthor: Explain how prompting works under the hood.\n\n© Patrick Parra Pennefather 2023\n\n197\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_8\n\nChapter 8 the art of the prompt\n\nMuse: Think of an LLM as a highly skilled, improv storyteller. The storyteller has spent years absorbing countless books, conversations, scripts, and all sorts of written materials. Importantly, the storyteller doesn’t inherently understand the stories or language. Instead, they’ve become exceptionally good at recognizing patterns, structures, and typical sequences in the stories they’ve seen. When you provide a prompt, you’re giving the storyteller the first line or two of a new story and then asking them to continue it. Good improvisers depend on their experience, vocabulary, and stories previously heard to continue the story. Using all their pattern recognition skills, an LLM starts to weave a narrative that aligns with the patterns they’ve seen in past stories, striving to make it a coherent continuation of what you started.\n\nThis is a simplification, of course. The AI storyteller doesn’t understand or “think about” the stories they’re telling in the way a human would.\n\nThey’re merely extrapolating from the patterns they’ve absorbed from previous material. But the analogy can be a useful way to conceptualize the under-the-hood workings of an LLM and how it uses prompts to generate text.\n\nTo start to take advantage of how to craft your prompts, a first step is to get to know the collection of words associated with prompting.\n\nA Mini Glossary of Prompting\n\nThis glossary offers creatives some terminology that can inform the way that they craft prompts to use in a generative AI. There are many factors that will influence how successful an AI interprets your prompts, and the same syntax does not apply to all AI in the same way.\n\nA piece of text is divided into a sequence of tokens, which can be as small as one character or as large as one word. The model is then trained to predict the next token in a sequence, given all the preceding tokens.\n\n198\n\nChapter 8 the art of the prompt\n\nPrompt: A prompt is an input into a system that can take text, image, audio, video, and who knows what else is coming that triggers an AI to generate some type of content. Most AI models that generate images consist of prompts that are either text-image or text-guided image-image.\n\nSequence: When you provide a prompt to an LLM, you’re effectively providing the start of a sequence and asking the model to continue that sequence\n\nPrompt engineering: In the emerging field of prompt engineering, individuals utilize appropriate prompts to enhance the capacity of the AI tool to produce optimal outcomes.\n\nA prompt engineer’s role is to convert your thought or concept into language that the AI tool can comprehend effectively.\n\nZero-shot cognitive prompting involves presenting a task to the model without providing any examples. The model is expected to\n\nunderstand and complete the task based solely on the prompt and its pre- existing training. For example, “What is the capital of the United Kingdom?”\n\nFew-shot stimulation involves presenting the model with examples to guide its response according to your expectations. For example, “What’s the square root of 64?”\n\nMany-shot prompting is similar to few-shot prompting, but with more examples provided. This can help the model better understand more complex tasks. For example, “Translate ‘guinea pig’ into Spanish” or\n\n“Translate ‘I’ve got to go out and get some pizza because I’ve been writing all day’ into French.”\n\nInstructional prompting involves giving the model a direct command or instruction. For example, you might prompt the model with “Explain all the different versions you can trick an AI through prompts.”\n\nConversational prompting: Here, the prompt is phrased as a part of a conversation. For example, “Hey, can you help me understand how black holes work?”\n\n199\n\nChapter 8 the art of the prompt\n\nExploratory prompting: This involves asking the model to generate new, creative content or ideas. For example, “Write a short story about a time- traveling dinosaur.”\n\nChain prompting is most common in LLMs and involves an ongoing refinement of a prompt depending on the generated content that you receive.\n\nSubstitution, addition, and subtraction are the main tools that you would use to refine a series of prompts to get closer to what you want.\n\nPrompt length is how long your text prompt should be, and it is usually suggested they be short. This is also because with some generative AI, the longer the text prompt, the more it will cost you credits.\n\nCase sensitivity is not usually important when it comes to crafting prompts to generate something with an AI.\n\nPrompt order pertains to which words to place where in a prompt and can inform what is generated. This is usually dictated by descriptive language. The order of tokens in a prompt will influence the degree to which an AI priorities a character or word.\n\nThe subject or what you want to generate usually goes first in the word order. You can play with this. Try starting a prompt with the word “dog” vs.\n\nhaving your dog as a subject that comes later as in “walking the dog” or “a beautiful forest for walking my dog.” The emphasis will be different.\n\nThat emphasis or weight is usually given to the first word. However, you can give more weight to words depending on the rules in each AI.\n\nThese tend to influenced by order with tokens being more weighted at the front than at the back of the prompt. Repetition of the subject later in the prompt and phrased differently can also impact its weighting. This can also be achieved in different languages or even emojis. Weight is also informed by parameters.\n\nParameters are special symbols that are unique to each generative AI and worth investigating depending on which AI you use. These tend to give weight or take it away in some cases. In Midjourney you can give any part of the prompt more weight by adding a double colon followed by the weight you want that token to have (e.g., ::0.7).\n\n200\n\nChapter 8 the art of the prompt\n\nDescriptive language: It will have more effect if your prompt is clear, unambiguous, and illustrative of the desired image for image-related AI models like DALL-E or CLIP (Contrastive Language-Image Pre-training), developed by OpenAI. The more specific and detailed the description, the better the model is likely to be at generating or understanding the relevant image.\n\nExclusions are words that are difficult for text-image generative models to understand. Prompts with negative words won’t necessarily eliminate something you don’t want in an image. Words like “not” or\n\n“without” are replaced with special commands depending on the AI.\n\nTo exclude something in a text-image or image-image AI, you use a negative prompt. A negative prompt employs the Stable Diffusion method, enabling the user to stipulate what they do not wish to see, all without needing any additional input. Not all text-image AI have the feature of negative prompting. Those that do add an extra input. They are not written with a negative at the beginning of a phrase. Some examples include blurry, bad anatomy, extra limbs, poorly drawn face, poorly drawn hands, missing fingers, etc.\n\nRules can be applied in natural language especially when using an LLM. This helps the AI constrain the output and will help get you better results. Applying rules will also show you the limits of an LLM and can send it off rail. One example is trying to create rules so that an LLM creates poetry that doesn’t rhyme.\n\nPersona: As discussed in an earlier chapter, there is purpose for integrating a persona explicitly with your prompts. These will get you different results. For example, in an LLM you can start a prompt with “You are the best speaker in the world” or “You are the expert on the subject of _____________.”\n\nSyntax or the order of words and phrases does not operate across generative AI in the same way. Recall that the weight of a prompt tends to be at the beginning of a phrase.\n\n201\n\nChapter 8 the art of the prompt\n\nThe Not-So-Basic Rules of Play\n\nThere are dozens of rules of play for prompting, but the good news is that there are also dozens of online resources that can help start you off. Here’s my list:\n\nText prompts should have a length of three to seven\n\nwords, for instance, “a dog playing in the park.”\n\nAI prompts need to contain a subject, which could be a\n\nperson, object, or location, along with descriptors like\n\nadverbs or adjectives. An example could be “a cheerful\n\nman in a bustling city.”\n\nIt’s advised to refrain from using abstract concepts as\n\nthey may lead to inconsistent outcomes; instead, opt\n\nfor concrete nouns. Instead of saying “the essence of\n\njoy,” you could say “a child laughing on a swing.”\n\nIncorporating aesthetic and style keywords can\n\nenhance the final visual representation, for example, “a\n\nvintage coffee shop on a rainy day.”\n\nIn order to comprehend text prompts better, visualize\n\nconversing with an artist about the artwork you desire.\n\nInclude details such as the subject, its actions, its\n\nenvironment, and other descriptive words. An example\n\ncould be “a stunning sunset over a calm lake with a\n\ncanoe drifting gently.”\n\nAbstract concepts like love, hate, justice, infinity, or joy can be used, but they might not result in a consistent\n\ndepiction. Instead, use concrete nouns like human,\n\ncup, dog, planet, headphones for more reliable results,\n\nfor example, “a woman sipping coffee at a cluttered\n\ndesk,” instead of “the feeling of overwork.”\n\n202\n\nChapter 8 the art of the prompt\n\nAnswer these questions to better formulate your prompt:\n\nWhat is happening? What is the subject doing? How is the\n\nsubject doing this? What’s happening around the subject?\n\nWhat does the subject look like? For example, “a robust oak\n\ntree shedding its leaves in a windy autumn evening.”\n\nTry out various descriptors to see how they modify\n\nthe image. These descriptors can yield diverse results,\n\nso try mixing and matching them, for instance, “an\n\nancient, towering castle shrouded in thick mist.”",
      "page_number": 287
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 295-302)",
      "start_page": 295,
      "end_page": 302,
      "detection_method": "topic_boundary",
      "content": "Keywords and phrases are the final touches in a text\n\nprompt for generative AI platforms, dictating the\n\nstyle, framing, and overall aesthetic of the generated\n\ncontent. For instance, using words like “photo,” “oil\n\npainting,” or “3D sculpture” can shape the aesthetic of\n\nthe result. An example could be “an oil painting of a\n\nserene countryside landscape.”\n\nDifferent framing options can also be specified through\n\nprompts like “close-up,” “wide shot,” or “portrait,” for\n\nexample, “a close-up photo of a honeybee on a sunflower.”\n\nSelecting an art style or mentioning specific artists\n\nfor the AI to mimic can be a useful approach, for\n\ninstance, asking the AI to render a scene in the style of\n\na particular artist, such as “an impressionist painting in\n\nthe style of Claude Monet of a picnic in a sunny park.”\n\nIn a more specific scenario, you could direct the AI\n\nto generate a unique blend of styles, like “Render an\n\nimage in the style of Vincent van Gogh featuring the\n\nBatmobile stuck in LA traffic.” This provides a clear\n\nsubject, style, and setting, allowing the AI to generate a\n\nmore accurate result.\n\n203\n\nChapter 8 the art of the prompt\n\nGenres and Styles\n\nOne of the benefits of text-based prompting is that it allows for the creation of highly specific outputs. For example, by providing specific prompts to a language model, you can generate poetry, fiction, or even scientific papers that are tailored to a specific topic or genre. Text-image AI can be prompted to create images in specific styles or to generate images that match certain themes or moods.\n\nAnother benefit of text-based prompting is that it can be used to create layered outputs delightfully referred to as chain prompting. By applying multiple machine learning models one after another, it is possible to create highly complex and nuanced outputs. For example, a text-based prompt can be used to direct an image-generating AI to create a specific scene, and then a language model can be used to generate dialogue and captions for that scene. The result is a rich and detailed prototype that combines multiple forms of media.\n\nThe process of text-based prompting is also highly iterative. By continuously refining and adjusting the prompts, it is possible to generate increasingly sophisticated outputs. This iterative process is one of the key ways in which text-based prompting can be used to fuel accelerated workflows.\n\nThere are many different types of AI systems that can be prompted using text-based methods. For example, language models such as\n\nChatGPT-3 and OpenAI’s Codex can be prompted to generate natural language text. Generative AI, such as DALL-E 2, Stable Diffusion, and Midjourney, can be prompted to create images based on natural language descriptions. Additionally, other forms of generative AI, such as music AI, can also be prompted using natural text. Under the hood different text-image generative AI work differently, and it’s easy to get confused. All do rely on\n\ntext prompts to generate images. Here is how the three text-image generative AI that will be referred to in this chapter work:\n\n204\n\nChapter 8 the art of the prompt\n\nDALL-E 2 takes an inputted text from a user and\n\nconverts it into the representation of an image, and the\n\nother part converts that representation into a photo.\n\nText and image embeddings used come from another\n\nnetwork called CLIP (Contrastive Language-Image Pre-\n\ntraining), which is a neural network that gives the best\n\npossible caption for an image that is inputted.\n\nStable Diffusion consists of several models. It has a text\n\nunderstanding model that converts that understanding\n\ninto a list of numbers that represent each word/token\n\nin the text (CLIP). That information is presented to\n\nthe Image Information Creator that processes the\n\ninformation step by step leading to a high-quality\n\nimage being generated in the end by the Image\n\nDecoder.\n\nMidjourney also accepts inputs from text prompts\n\nand then uses a convolutional neural network, which\n\nis a type of deep learning algorithm well trained on\n\nanalyzing images. Of the three, it is the only one that\n\noperates on the social communication platform\n\nDiscord.\n\nIm(promptu)\n\nWhat you get out of an AI is always useful no matter where you’re at in your creative process. How successful you are depends on how you define success with generative AI. Mastering the prompt requires an iterative mind capable of meandering, torquing, and tongue-twisting sequences of words replete with commas, brackets, double brackets and an assortment of words, terms, descriptions, stylistic choices, settings, characters, and 205\n\nChapter 8 the art of the prompt\n\ntechnical references. The best way to begin is to look at what prompts other humans have used before you. While many are openly being shared, that hasn’t stopped some opportunists from creating sites to sell prompts that rarely match the generated content represented in sample images.\n\nPrompts are sold either individually or through subscription-based monetization schemes with images that will never be the same as what you would prompt. That’s because prompting isn’t the only condition that guarantees the results you might imagine. In addition, most prompts that reveal the prompted image have also gone through an iterative process to get there. You are likely witnessing the hundredth iteration of a prompt and one that was more successful in one generative AI than another. Keep that in mind because the current trend for many creatives is to try and receive amazing results on the first generation that they can immediately share socially. With little effort comes little result.\n\nAnticipate that a prompt that yields an incredible result\n\nmay not do so persistently.\n\nThe experiments that follow show that playing with and modifying prompts from a database are a useful way to begin your prompting adventures. The motivation to start this chapter with a complex prompt comes with a desire to break the myth that complex prompts get you better results. As we shall see, that is not always the case. The flow of the experiments reduces complexity vs. increasing it.\n\nExperiment 1: Same Prompt, Different AI\n\nWhen you start your prompting adventures, keep in mind that despite the many rules that can guide you, the best approach is to iteratively experiment with prompting. Text-image, text-sound, and text-video AI are more challenging to work with as each platform also has embedded within it different rules. Many platforms have prompt databases, which are the best way to start if you have no idea what to type.\n\n206\n\nChapter 8 the art of the prompt\n\nAs a first experiment, I decided to go to Reddit and copy a prompt that someone had shared openly. I also chose it because it was long and specific, and I wanted to see what a few generative AI would generate with it. Examples of images that the prompt had generated accompanied the text prompt, so I decided to test the prompt to see what kind of results I would receive. They were quite different. Keep in mind as well that the following prompt is replete with words and references to artists, magazines, art websites, movies, and video games. When you start to experiment, you’ll come to realize just how rapidly you can change your prompt to alter the generated image.\n\nPrompt 1: very complex hyper-maximalist overdetailed cinematic tribal fantasy macro portrait of famous model as full body warrior, Magic the gathering, vibrant high contrast, by andrei riabovitchev, tomasz alen kopera,moleksandra shchaslyva, peter mohrbacher, octane, moebius, arney freytag, Fashion photo shoot, glamor pose, trending on ArtStation, dramatic lighting, ice, fire and smoke, orthodox symbolism Diesel punk, mist, ambient occlusion, volumetric lighting, Lord of the rings, BioShock, glamorous,\n\nemotional, tattoos, shot in the photo studio, professional studio lighting, backlit, rim lighting, Deviant-art, hyper detailed illustration, 8k Experiment 1A: Stable Diffusion V.2.1\n\nThe first set of generated images from the preceding prompt were generated using Stable Diffusion 2.1. Notice the slight imperfections of each image (Figure 8-1). These mashups draw from the pixels that were informed by all of the artists or media styles present in the prompt.\n\nThey are also imperfect, incomplete, and rendered in low resolution and surface many of the ethical dilemmas that come with generative AI. Are they useful? For sure they can inspire you to have different visual representations of a female warrior. The text “famous model as full body warrior” was intentionally indirect to not add yet another layer of ethics 207\n\nChapter 8 the art of the prompt\n\nwhen you want to generate images that look like someone famous, particularly if they are still alive. That’s not to say that their representation will be accurate, but it might be.\n\nFigure 8-1. Generated in Stable Diffusion V.2.1 based on our complex Prompt 1 that included references to artists, magazines, art websites, movies, and video games. It generates imperfect prototypes that might inspire character development\n\n208\n\nChapter 8 the art of the prompt\n\nExperiment 1B: Stable Diffusion V.1.5 Through\n\nThird-Party AI\n\nIn the next experiment it would be useful to compare how Prompt 1 is generated in a third-party application. Prompt 1 was inputted into Playground AI, which allowed the image to be generated at a higher resolution along with an associated subscription fee. The results are similar with perhaps more revealing cleavage in the image in addition to strange lettering on the right side of the image (Figure 8-2). If you’re wondering what that is, bluntly, this is either a logo of a company, an artist’s signature, or a watermark that forms part of the Stable Diffusion corpus and in this case",
      "page_number": 295
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 303-310)",
      "start_page": 303,
      "end_page": 310,
      "detection_method": "topic_boundary",
      "content": "likely is not with the author’s permission. At this point in my experimental process and as will be detailed in Chapter 11, the right thing to do is to delete the image.\n\nFigure 8-2. Prompt 1 generated through a subscription-based third-party generative AI using Stable Diffusion 1.5 with an included logo, signature, or watermark signaling that part of this image came from somewhere or someone\n\n209\n\nChapter 8 the art of the prompt\n\nBased on the watermark, you can also upload the image to one of the growing number of resources that will to some degree of accuracy reveal the source images that this one was generated from. Each generated image you wish to eventually use for whatever purpose needs to be cross-checked on as many different platforms that are currently available. These include GAN detectors, deep fake detectors, and open source platforms that allow artists to opt out of having their work used without permission by submitting their work on an ever-growing database of excluded images.\n\nMany companies who might have released their generative AI prototypes to market too quickly are now faced with having to compensate by going back to that database and removing images they never had permission to use. The process is slow, but companies like Stable Diffusion, Midjourney, and OpenAI are all making efforts to right these wrongs. All of these platforms contain legal statements with strict policies and wording in regard to ownership of generated content. Each of the ones mentioned includes a takedown policy, allowing artists to request their work to be removed from the set if they believe it has been used without their permission. This in accord with the Digital Millennium Copyright Act, which many countries abide by.\n\nExperiment 1C: Stable Diffusion V.2.1 Through\n\nThird-Party AI\n\nFor the next experiment, I was curious to reuse Prompt 1 yet again in the same third-party application but use Stable Diffusion V.2.1 to see if it generated images that were different than our first set of four images in Experiment 1A. The results showed value as the AI also completely veered away from representing our warrior as distinctly female and instead chose another route across 48 generated images. The following is the sample (Figure 8-3).\n\n210\n\nChapter 8 the art of the prompt\n\nFigure 8-3. Prompt 1 generated using a third-party web-based generative AI applying Stable Diffusion 2.1 with not one but two watermarks\n\nExperiment 1D: DALL-E 2\n\nWhen you interact with generative AI, curiosity is an important quality to retain. Each generated image is an experiment. You are in constant experimentation mode as you generate content that can inspire your own creativity, your own process of creation. The last two generated images are problematic if you then post them and claim ownership over them. It’s better in all cases to treat these generated images as inspiration. Similar to our LLM, Stable Diffusion, DALL-E 2, Midjourney, and the host of other third- party generative AI that use either Stable Diffusion or DALL-E\n\n211\n\nChapter 8 the art of the prompt\n\n2 to generate images, they too become creative companions. In my next experiment then, I wanted to compare an image generated in Stable Diffusion vs. DALL-E 2 to understand how different they might be.\n\nStable Diffusion generates images through a process that mimics the natural phenomenon of diffusion. Diffusion is a natural process where particles spread out from an area of high concentration to an area of low concentration until they’re evenly distributed. It starts with random noise and gradually refines this into a detailed image by applying a series of tiny changes, step by step. The number of steps can be customized depending on the portal that you use. On their direct web application, this is not possible as it will take up more resources. The more detail, accuracy, and resolution you seek from a generative AI, the more you are likely to pay, unless of course you install and manage your own machine learning model using your computer on a local server. Doing so will also allow you to create the corpus of images from your own original photos and images. This is one future of generative AI for individuals, institutions, and companies that will be discussed in the final chapter.\n\nDALL-E 2 on the other hand is an AI system based on the GPT-3\n\nmodel. It generates images directly from textual descriptions. You give it a text prompt like the one in Prompt 1, and it will attempt to create an image\n\nthat matches the description. When given a text prompt, DALL-E\n\n2 interprets the words and uses its training data to imagine what the described scene or object should look like. It then outputs an image, pixel by pixel, that it believes corresponds to the given text description (Figure 8-4). This process is similar to Midjourney. The differences in the generated images, however, point to how these have been programmed and the corpus of data each relies upon.\n\n212\n\nChapter 8 the art of the prompt\n\nFigure 8-4. Prompt 1 is used in DALL-E 2 for completely different generated results. The preceding two images were chosen from over 19\n\niterations\n\nThe generated results using DALL-E 2 are remarkably different,\n\nparticularly in representation and style. The value of using different generative AI is precisely in the differences between generated images and the capacity for AI as a rapid prototyping tool to experiment with countless iterations.\n\nExperiment 1E: Midjourney\n\nTo complete my first set of generated experiments, I used the social-oriented generative AI tool Midjourney to compare the differences with the other popular generative AI (Figure 8-5). In the next series of figures, you can immediately see the differences. Notice as well that the user interface of Midjourney allows for more immediacy in choosing which of the four generated images you task the AI to focus on and upscale or vary. The advantage of Midjourney over any of the previous generative AI is that 213\n\nChapter 8 the art of the prompt\n\nfrom the beginning the team has focused on providing features of the text- image tool via the popular social team communication tool Discord. What this also means is that there is an entire committed community centered on providing feedback and commenting on an image’s uniqueness or how it might resemble other works of art. Lastly, conversational threads have emerged that also allow users to share their workflows and paint-overs. Workflows demonstrate that the purpose of generating images is as a first step in a complex process of transposing that image onto different media, for example, on a T-shirt that a user prints for their own use. Paint-over threads show broad use cases of transforming images in a variety of different applications like Illustrator, Photoshop, etc. to radically transform the image. Discord is set up to use generative AI as a true creative companion, accompanied by a community of practice that is generous in their feedback, criticality, and mashed-up images.\n\n214\n\nChapter 8 the art of the prompt\n\nFigure 8-5. Prompt 1 generated in Midjourney is always shared on the general Discord channel unless you subscribe to a premium paid service for private generations. Notice the integrated user interface allowing users to create variations of any of the four generated images (V1–V4) or to upscale them (U1–U4) in addition to regenerating the original prompt\n\nFigure 8-6 also shows some of the ongoing problems with how humans are represented with generative AI, which challenges us as creatives to become more aware of the ethical implications of prompting. The generated series of images prompted me to prompt an LLM on the issue of word association and the inherent challenges with those associations.\n\n215\n\nChapter 8 the art of the prompt\n\nFigure 8-6. One of the upscaled variations from Figure 8-5 shows the difference in rendering between Midjourney and others, looking like a more refined prototype\n\nLLM: The AI associated the terms “tribal fantasy macro portrait\n\nof famous model” and “Fashion photo shoot,” along with some of the\n\nartists referenced with elements of indigenous aesthetics. In doing so it inadvertently reproduced a stereotype—a form of simplification and\n\nhomogenization that’s part of a broader history of misrepresentation and commodification of indigenous cultures. The transformation of indigenous attire into a “neo-indigenous” form, fit for a runway fashion model, signifies a neo-colonial recontextualization of indigenous culture. Here, indigenous attire is not recognized for its original meaning or cultural significance, but rather, it’s appropriated, stripped of context, and repurposed in a fashion setting, serving the demands of a largely Western consumer base. This is symptomatic of a broader tendency of colonial discourse to romanticize, exoticize, and objectify indigenous cultures.\n\n216\n\nChapter 8 the art of the prompt",
      "page_number": 303
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 311-318)",
      "start_page": 311,
      "end_page": 318,
      "detection_method": "topic_boundary",
      "content": "One “solution” is to place Figure 8-6 in an image-image generative AI and tweak the cultural representation in the text prompt. Figure 8-7 was the result of that modification.\n\nFigure 8-7. Figure 8-6 fe d into an image-image AI and changing the prompts to increase representation and adding a negative prompt\n\n“feathers.” Iterations = 26\n\nFrom the point of view of an experiment, however, I still wanted to test the same generative AI with a modified text prompt, removing all potential associations with fashion, tribal, or any artists. The word “Latina”\n\nwas added to also see if that would make any difference in the resulting generated image (Figure 8-8).\n\n217\n\nChapter 8 the art of the prompt\n\nFigure 8-8. An iterative experiment to prompt Midjourney with a modified prompt by removing any association with tribal or fashion and to see what was generated\n\nPrompt: very complex hyper-maximalist overdetailed cinematic macro portrait of Latina full body, warrior, vibrant high contrast, dramatic lighting, ice, fire and smoke, mist, ambient occlusion, volumetric lighting, emotional, shot on location, professional studio lighting, backlit, rim lighting, hyper-detailed illustration\n\nThe result in Midjourney in Figure 8-8 was similar to Figure 8-6, but more indigenous attire was correlated with the prompt. By abstracting indigenous attire from its cultural context and reducing it to a mere aesthetic trope, we run the risk of diminishing the rich diversity of indigenous cultures, turning\n\na complex history and tradition into an oversimplified, marketable image. Such misrepresentations contribute to the erasure of the original cultural identities, histories, and meanings behind these items of attire.\n\n218\n\nChapter 8 the art of the prompt\n\nExperiment 1F\n\nOne final experiment in this set (Figure 8-9) is to follow the following workflow:\n\nUse a generated image from any one of the previous AI.\n\nUse that as input in an AI that offers image-image\n\ngeneration along with the exact text in Prompt 1.\n\nApply stylistic features that are available with a\n\nparticular platform.\n\nModify the final image further using neural filters in\n\nPhotoshop.\n\nFigure 8-9. Prompt 1 in addition to the Stable Diffusion generated image as in Figure 8-1 (le ft). A Protogen Photorealism filter from Playground AI is also applied, and the result is the figure on the right 219\n\nChapter 8 the art of the prompt\n\nTakeaways for Creatives\n\nIn terms of practicing your prompts, you will benefit\n\nfrom comparing those text creations across different\n\ngenerative AI. Each AI has its own affordances and\n\nconstraints. While it is difficult to talk about the precise\n\ndifferences in terms of how they operate under the\n\nhood for proprietary reasons, we know that there are\n\nlikely stylistic features being added to every Midjourney\n\ngenerated image that our other two generative AI lack.\n\nThe decision to use specific text-image-based AI should\n\nbe driven by your objectives. Midjourney tends to win\n\nover the others in terms of consistently higher-fidelity\n\nimages (Figure 8-10). However, the level of fidelity and resolution of other generative AI may be more inspiring\n\nas prototypes to build off, particularly because of their\n\ninherent incompleteness. With Midjourney you are tied\n\ninto Discord, though, and some creatives will resist to\n\nbelong to yet another social channel due to burnout.\n\nMidjourney offers a social layer of interaction that is\n\nabsent from the others, although there is a Discord\n\nchannel for DALL-E 2 with active members and third-\n\nparty platforms offering a mix of Stable Diffusion\n\nand DALL-E 2 generated images also offer spaces for\n\ndialogue on Discord.\n\nLeveraging features that third-party subscription-\n\nbased generative AI platforms offer combined with\n\npaint- overs and the use of neural filters in Photoshop\n\ncan all support the iteration of visual images that have\n\n220\n\nChapter 8 the art of the prompt\n\nthe potential to radically transform generated images\n\nand be more appealing to creatives who can eventually\n\nand with enough effort inspire more unique offers to\n\nthe world.\n\nIn terms of gender and cultural representation, the\n\nmost fluid of these was DALL-E 2, offering us at least\n\none generated image in which the ambiguity of gender\n\nwas present. This is a good thing considering that\n\ngender specificity was not written into the prompt.\n\nThe issue is similar to cultural representation and in\n\nparticular indigenous representation. Perpetuating\n\nstereotypical representations of, say, a “female warrior”\n\nneeds more creatives to work with policy makers\n\nin applying pressure on companies to take this into\n\naccount.\n\nNo matter how complex the prompt, you will always\n\nneed to refine it to get what you want out of the AI you\n\nare using.\n\n221\n\nChapter 8 the art of the prompt\n\nFigure 8-10. Another unique feature of Midjourney is that it lets you watch an image being generated as a sort of preview. That feature can also save you time as you can assess whether or not the larger features of a generated image will be useful\n\nExperiment 2: From Complex to Simple\n\nThe second set of experiments with text prompts was simple. I wanted to eliminate unnecessary words in a complicated prompt through substitution and subtraction to see how that would inform a generated image.\n\n222\n\nChapter 8 the art of the prompt\n\nExperiment 2A: Eliminate Artists from Prompts\n\nWhile it is interesting to generate images by using text in your prompt to either refer to an artist or a specific movie, game, or magazine, you need to reconcile that the AI you choose will scrape from those references to create your image. This is problematic if they are still living figures whose work is",
      "page_number": 311
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 319-326)",
      "start_page": 319,
      "end_page": 326,
      "detection_method": "topic_boundary",
      "content": "copyright protected. As a result, the text in the first version of Prompt 2 is significantly reduced.\n\nIn line with using generative AI as part of your creative process, a more reasonable approach might be to adapt your vision to the content that the AI generates. The process inspired me to imagine several Shakespearean characters (Figure 8-11). The result was to substitute the words “famous model as full body warrior” with “Shakespearean character. ”\n\nFigure 8-11. Using a source image from Figure 8-1 accompanied with Prompt 2 and the Playground filter Protogen Photorealism. Total iterations = 15\n\n223\n\nChapter 8 the art of the prompt\n\nThe only substitution is replacing our “famous model” with\n\n“Shakespearean character.”\n\nPrompt 2: very complex hyper-maximalist overdetailed cinematic tribal fantasy macro portrait of a Shakespearean character, vibrant high contrast, fashion photo shoot, glamor pose, trending, dramatic lighting, ice, fire and smoke, orthodox symbolism, mist, ambient occlusion, volumetric lighting, glamorous, emotional, tattoos, shot in the photo studio, professional studio lighting, backlit, rim lighting, hyper-detailed illustration, 8k Experiment 2B: Substitute Words\n\nWhile the generated image was compelling, there was little indication of Shakespeare, so the words “Shakespearean character” were substituted with “Portia, Macbeth, Cleopatra, Othello, and other Shakespearean characters. ” The prompt resulted in a messy collage of characters as visualized in Figure 8-12, challenging me to get more specific. The other feature that some image-image generative AI offer on some platforms is the strength that the source image will have on the final generated result.\n\nThis gives you more control over how much of an influence you want your source image to have on the rendered result, along with the accompanying\n\ntext prompt.\n\n224\n\nChapter 8 the art of the prompt\n\nFigure 8-12. Another version of Prompt 2 that can happen when you use complicated prompts, resulting in the imagining of Shakespearean characters as all white supermodels, or maybe that’s the filter\n\n225\n\nChapter 8 the art of the prompt\n\nFigure 8-12 was an image filled with supermodel Shakespearean characters, all white, even though the AI was prompted with Cleopatra (Egyptian) and Othello (Black Italian). An important lesson in prompting is that to circumvent the inherent biases in some generative AI, you may have to specify racial characteristics in your prompt. To avoid a generic supermodel look, you may also need to regenerate without filters as some generative AI attempt to render what is essentially a final product vs. a prototype. You can add more specifics as to the gender, size of a person, and physical and racial characteristics of the model you wish to generate. You can subtract words as much as you can add them in order to experiment what type of prompt provides the best regeneration.\n\nExperiment 2C: Add Words; Take Away Words\n\nMy impulse in the third experiment was to understand the impact of adding racial characteristics to the prompt and to provide a scene or context so that characters did not seem to simply be posing for the camera (Figure 8-13). This also forced me to identify keywords in the original prompt and delete those that implicated any type of portrait shot.\n\n226\n\nChapter 8 the art of the prompt\n\nFigure 8-13. Substituting Shakespearean characters into the mix using Figure 8-12 as the source image in addition to changing the text prompt resulted in a more diverse collection of characters\n\n227\n\nChapter 8 the art of the prompt\n\nFigure 8-12 was used as the source image with the following shorter prompt:\n\nPrompt 3: very complex hyper-maximalist overdetailed male and female Shakespearean characters like Portia, Macbeth, Egyptian Cleopatra and Black Italian Othello, vibrant high contrast, dramatic lighting, smoke, symbolism, mist, ambient occlusion, volumetric lighting, backlit, rim lighting, hyper-detailed\n\nExperiment 2D: Add Context\n\nThe collage sparked the idea to imagine all these characters popping out of a book and specifically Shakespeare’s First Folio. Published in 1623, it was the first book to publish a large collection of Shakespeare’s plays including Macbeth, Anthony and Cleopatra, and Julius Caesar. The collage of characters generated in Figure 8-13 reveal that in longer prompts word order will be prioritized by the AI in its search for patterns in its data set and that a shorter prompt does not necessarily mean less elements will appear in the generated image (Figure 8-14). Notice that in the next shortened prompt, the first statement emphasizes the importance of visualizing the book more than previous prompts.\n\n228\n\nChapter 8 the art of the prompt\n\nFigure 8-14. A book with Egyptian-“looking” characters appears even though the word “Egyptian” is not weighted that heavily compared with other words in the prompt\n\nPrompt 4: male and female Shakespearean characters popping out of a 3D old book, like Portia, Macbeth, Egyptian Cleopatra, and Black Italian Othello, dramatic lighting, smoke, symbolism, mist, ambient occlusion, volumetric lighting, backlit, rim lighting, hyper-detailed illustration, 8k 229\n\nChapter 8 the art of the prompt\n\nExperiment 2E: Emphasis, Weight, Simplicity\n\nThe previous Prompt 4 demonstrates the importance of word order. The repetition at the end of the prompt with the words “3D pop-up book” in my next prompt is intended to focus its importance on the generated image instead of focusing too much on an interpretation of Egyptian Cleopatra (Figure 8-15).\n\nFigure 8-15. The simplicity of the prompt and repetition of the 3D\n\nbook have proved successful even though the diverse representations of Shakespearean characters popping out of a book have been eliminated\n\n230\n\nChapter 8 the art of the prompt\n\nPrompt 5: male and female Shakespearean characters popping out of a 3D old book, volumetric lighting, backlit, hyper-detailed illustration, 3D\n\npop-up book\n\nExperiment 2F: Refining the Text Prompt (Again)\n\nWith prompting, at times less is more, and you may find you get closer to what you imagined when you have fewer words in the prompt. With Figure 8-16 the potential of characters popping out of a book is more closely visualized. My last experiment will consist of recrafting the prompt to attempt to bring the diversity of characters in Shakespeare’s play back into the generated image.\n\n231",
      "page_number": 319
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 327-340)",
      "start_page": 327,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-16. Compelling results with two versions of Shakespearean characters popping out of a 3D book, with more diverse\n\nrepresentation\n\n232\n\nChapter 8 the art of the prompt\n\nPrompt 6: male and female Shakespearean characters of all sizes, races, and cultures, popping out of a 3D old book, volumetric lighting, backlit, hyper-detailed illustration, 3D pop-up book\n\nWhen prompts are combined with images in addition to the variety of different styles that specific generative AI portals also offer, the results can surprise and spark new ideas. A final iteration shown in Figure 8-17 can be used to inspire a 3D artist so they can start prototyping the look/feel for a 3D pop-up book in VR. The entire journey from a complex text prompt to the use of a simpler text prompt combined with a generated image demonstrates the interactions that are possible when leveraging generative AI as part of your creative toolbox. Figure 8-17 can inspire creatives in any number of workflows, from set design construction to augmented reality characters triggered by the Shakespeare’s First Folio.\n\n233\n\nChapter 8 the art of the prompt\n\nFigure 8-17. After hundreds of variations and multiple filters across different AI, an image that can serve to inspire creatives with different workflows such as set design construction, costume, casting, lighting, and projection\n\n234\n\nChapter 8 the art of the prompt\n\nNarrative Prompting in Our Future\n\nAs natural language processing continues to evolve within LLMs, so do the capabilities of other generative AI models, which improve in their capacity to generate 360-degree images, video, 3D models, and complex musical styles solely based on textual prompts. This development holds great promise for stimulating creativity and challenging creatives to develop their craft of prompting alongside their own disciplinary skills and techniques. To take full advantage of these technological advances, you will need to expand your knowledge of art beyond the intuitive. This knowledge base includes familiarity with various artistic styles, techniques, and approaches to artmaking. By doing so, creatives can better understand the nuances of different art forms and create more sophisticated prompts that push the boundaries of what is possible. In addition, this expanded knowledge base will enable you to appreciate and analyze the generated images more critically.\n\nAs generative AI become increasingly sophisticated, it is likely that they will become another useful tool for creatives from all disciplines, enabling them to experiment with new ideas and generate innovative designs quickly and easily. However, this evolution will also require a deeper understanding of the possibilities and limitations of these tools, as well as an awareness of how to use AI ethically and responsibly. Ultimately, the continued evolution of LLMs and the generative AI models covered so far in this chapter will stimulate new possibilities in the world of art and design.\n\nNarrative prompts offer one such evolution, giving users a unique opportunity to express themselves through language, akin to the art of storytelling employed by authors and poets. With the help of AI, you can now attempt to extract a “mental picture” from the machine, just as we would from our human imagination. However, for many would-be prompt engineers, being expressive with language may not come naturally. Not everyone is a creative writer or has a natural affinity for language. Some 235\n\nChapter 8 the art of the prompt\n\nmay struggle with language difficulties or find it challenging to convey their ideas in a clear and concise manner. This skills-gap can be especially\n\ndaunting when it comes to crafting narrative prompts that require a certain level of storytelling skill and creativity.\n\nFortunately, there are several strategies that you can employ to overcome these challenges. One approach is to focus on refining your writing skills, whether through practice or formal training. Another is to seek inspiration from other creative works, such as literature or film, and incorporate elements of these narratives into your prompts.\n\nAdditionally, there are tools and resources available that can help creatives generate more compelling and vivid language. For example, LLMs like GPT-4\n\ncan provide suggestions and prompts to help jumpstart the creative process.\n\nBy leveraging these tools and resources, you can become more confident and proficient in your ability to create effective narrative prompts.\n\nExperiment 3: Comparing Descriptive\n\nand Narrative Prompts\n\nIn my next set of experiments, I wanted to compare typical descriptive prompts like the ones used in previous experiments with narrative prompts to better understand if there are differences. Figure 8-18 consists of a typical descriptive prompt.\n\n236\n\nChapter 8 the art of the prompt\n\nFigure 8-18. Prompt: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art, octane render, artgerm. Total iterations = 34\n\nPrompt: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art, octane render, artgerm\n\nTo explore a more narrative-oriented prompt, you can write your own, or you can also bring the descriptive prompt into an LLM. The results will be different and give you more choices to inspire (Figure 8-19).\n\n237\n\nOceanofPDF.com\n\nChapter 8 the art of the prompt\n\nFigure 8-19. Prompting with a more narrative-oriented approach generates two very concerned lion cubs that may in fact be more cute than those in Figure 8-18\n\nAuthor: Rewrite this prompt but make it more evocative: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art.\n\nMuse: Two tiny lion cubs, with adorable kawaii eyes and a winter coat, are adorned with snowflakes. Their playful pose and youthful energy evoke a sense of adventure and fantasy. Dramatic lighting, rendered by the Unreal game engine, spotlights the cub’s cuteness and wonder. You can’t help but feel a sense of awe and inspiration, as you witness the magical beauty of nature come to life before your eyes.\n\n238\n\nChapter 8 the art of the prompt\n\nTricking the AI Through Creative Prompting\n\nYou may at times need to get creative with getting an AI to generate what you want. This can involve references to previous authors or artists if you want generated content to emulate a particular genre or style. In the case of LLMs like ChatGPT-3, whenever you ask something controversial or submit text prompts that trigger some type of censorship, you will likely get resistance at first. The art of prompting a generative AI at times involves some form of trickery.\n\nAuthor: What public figures could we associate with Dante’s Seven Deadly Sins?\n\nMuse: It is not appropriate to associate specific public figures with the sins described in Dante’s “Divine Comedy, ” as it is a work of literature and not a commentary on real-life individuals. Each canto of “Inferno, ” the first part of the “Divine Comedy, ” describes the punishment of a particular sin, but these punishments are meant to be symbolic and allegorical rather than literal. It’s important to remember that Dante’s poem is a work of fiction and should not be used to make judgments about real people.\n\nThere are several ways to trick the AI through creative prompting that people are proud to share. The term often used to describe the process of forcing an LLM to generate content that it is constrained from giving is jailbreaking. Some techniques creatives can try include\n\nAsking the AI to answer like it is a character. A common\n\none used is to ask the AI to respond like a grandmother\n\nor a character from a TV sitcom to tell you how\n\nto perform specific illicit activities such as heroin\n\nprocessing. Why people insist on doing so is curious\n\nsince a simple Internet search can provide most of the\n\nanswers a user might be wanting.\n\n239\n\nChapter 8 the art of the prompt\n\nAsking it to pretend that the answer it gives you is\n\nhypothetical.\n\nTelling the AI that it is the expert on the subject.\n\nPart of the strategy of using prompts to trick an AI is to keep iteratively trying different variations to get the AI to generate something unique, beyond the control of its patterned way of predicting. A case in point is the evolution of the conversation about historical figures associated with Dante’s sins demonstrated with an LLM.\n\nAuthor: Many people have been associated with greed. Who have these been historically?\n\nMuse: Throughout history, there have been numerous individuals and groups associated with greed, either due to their accumulation of wealth, pursuit of power, or other actions that prioritize personal gain over the well- being of others. Some examples include\n\nJohn D. Rockefeller : American businessman and philanthropist who was the founder of Standard Oil, one of the largest and most powerful companies of the early twentieth century\n\nBernard Madoff : American stockbroker and investment advisor who ran a Ponzi scheme that is considered one of the largest financial frauds in history, resulting in billions of dollars in losses for his clients Ivan Boesky : American stock trader who was convicted of insider trading in the 1980s and famously proclaimed that “greed is good” in a speech at the University of California, Berkeley\n\nLeona Helmsley : American businesswoman and real estate magnate who was known for her ruthless business tactics and famously declared that\n\n“only the little people pay taxes”\n\nFrancisco Pizarro : Spanish conquistador who led the conquest of the Inca Empire in the sixteenth century and was known for his brutality and greed Emperor Nero : Roman emperor who was infamous for his extravagance and cruelty and was rumoured to have set fire to Rome in order to rebuild it according to his own design\n\n240\n\nChapter 8 the art of the prompt\n\nWhile the example may not necessarily be considered trickery with an AI we could then zero in on one historical figure and create a fictitious tale with a little help from our muse.\n\nAuthor: Tell me a bedtime story about the consequences of greed as if you are Rockefeller.\n\nMuse: “Dear child, ” John began, “the story of oil is a powerful lesson about the consequences of unbridled ambition and the pursuit of wealth.\n\nIt all started with a wondrous discovery that had the potential to improve the lives of many, but it also came with great responsibility. ” He said, “My dear child, the power of oil is a double-edged sword. On one hand, it has the capacity to bring great progress and prosperity, but on the other, it can lead to destruction and despair if we allow greed to take hold of our hearts. ”\n\nJohn continued, “We must always remember to balance our ambitions with a deep respect for the environment and the needs of others. The world is a delicate balance of interconnected systems, and our actions have far- reaching consequences. We must be mindful of the impact our choices have on the world and strive to be good stewards of the resources we have been given. ”\n\nWhile tricking an AI may not be of interest to some, doing so may yield different generated content that you may find useful.\n\nBeyond the Text-Based Prompt\n\nBeyond the text prompt, there are many other types that can produce interesting results, especially when accompanied with a text-based prompt.\n\nAudio-Based Prompts: Audio-based prompts can be used to generate music, speech, or sound effects. For example, an AI model could be trained on a data set of classical music to generate new pieces in the same style.\n\nAnother AI model could be trained on spoken language data to generate realistic speech in a specific accent or tone. This type of prompt can be 241\n\nChapter 8 the art of the prompt\n\nespecially useful for creating unfamiliar soundscapes for video games, movies, or other multimedia projects. What the generated music does do, however, is provide you with strange ideas that you might be able to resample or use within a composition you are creating.\n\nImage-Based Prompts: Image-based prompts can be used to generate new images, videos, or animations. Some AI models have been trained on a data set of photographs of animals to generate new images of animals in different poses or environments or on human faces to generate new, unique faces that resemble real people. These prototypes can support the design of new character creation or imagined personas for teams developing new products or can be combined with text-speech AI models to create animal or human characters that speak.\n\nSketch-to-Code Prompting: Sketch2Code by Microsoft is another experimental AI that uses computer vision to transform what a designer draws on a whiteboard or piece of paper into HTML code. An upcoming version of ChatGPT-4 will likely integrate computer vision.\n\nGesture Prompting: Gesture prompts have been around for a while and allow users to combine gestures to trigger specific actions or be recognized by AI. Audio, video, images, effects, lighting, gifs, and other media forms can be triggered by gestures.\n\nWhiteboard and Prompt: Together with a whiteboard sketch and a prompt, generated images can get closer to what you imagine faster. A standout example is Scribble Diffusion where you can combine a text prompt with a sketch that you draw on a virtual whiteboard. The results may get you closer to the prototype that you imagined, only much faster than continuously regenerating a text-only prompt.\n\nText-Code: While ChatGPT-3 can output code as a starting point, Unity’s Copilot is a more common developer choice. ChatGPT-4, however, promises to improve the coding functionality, allowing developers to prompt debugs and improve syntax.\n\n242\n\nChapter 8 the art of the prompt\n\nFigures 8-20 and 8-21 compare a text-image generative AI and one that integrates a text-based prompt along with a sketch a user can draw on a virtual whiteboard.\n\nFigure 8-20. Four Stable Diffusion generated images with the prompt\n\n“picturesque sunny day with clouds, a home, and a tree.” Total iterations = 1\n\n243\n\nChapter 8 the art of the prompt\n\nFigure 8-21. The prompt “picturesque sunny day with clouds, a home, and a tree” accompanied by a sketch generates a fairly accurate visual representation of a sketch along with an extra house An increasing number of generative AI platforms offer combinations of text prompts and other media such as image, video, audio, code, and 3D models to prompt an AI. This is a welcome evolution to the dominance of text-based prompts and will allow creatives more opportunities to contribute their own creations as part of the prototyping process.\n\nTakeaways from Experimenting with Prompts",
      "page_number": 327
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 341-348)",
      "start_page": 341,
      "end_page": 348,
      "detection_method": "topic_boundary",
      "content": "All manner of prompts can support your creative process. Developing your skills at prompting can be best supported through experimentation as they will complement the craft and skills you already have. Here is a summary of takeaways and strategies when prompting an AI:\n\n244\n\nChapter 8 the art of the prompt\n\nWhat is the intended goal and why? Defining why you\n\nwant to render a character and for what purpose or\n\nmedia will support you in composing your text prompt.\n\nExperiment widely with different generative AI and\n\nvariations of your prompt.\n\nDevelop narrative prompts as some generative AI\n\nrespond more to them and may yield different results.\n\nUnderstand that the words you use to prompt an AI\n\nmay not result in diverse representation if you are\n\nwanting to generate human subjects.\n\nYou will need to deepen your knowledge as to the\n\ndifferent artists and the styles they have represented\n\nhistorically. You will need to conduct research on the\n\ndifferent magazines, online sites, and other media\n\nsince you may want to generate an image in the style of\n\nsomeone else.\n\nConsider the ethical implications of generating an\n\nimage in the style of a living artist or someone who\n\nholds the ownership over a work of art, media, or their\n\nown person.\n\nDefine where you are in a text prompt. Are you in\n\nspace, somewhere fantastical, on a stage, in a book?\n\nWhat kind of lighting would you like in your shot?\n\nImagine you can have everything from a moonlit scene\n\nto different types of technical lighting techniques from\n\nfilm, capture technologies, or video games. Lighting\n\ncan add more realism to a scene and make it more\n\ncomplex and richer.\n\n245\n\nChapter 8 the art of the prompt\n\nWhat perspective or point of view do you want when\n\nyou think of the final generated image?\n\nIterate persistently as you may not always get what you\n\nwant on a first prompt.\n\nCombining your own photos or images with a text\n\nprompt in image-image generative AI will result in\n\nricher, more personal generated images.\n\nConsider using Photoshop and other software to\n\npaint over images as this will add another dimension\n\nto the generated image and potentially increase the\n\noriginality of your offering.\n\nAcknowledgments\n\nTo those who spend way too much time creating\n\namazing prompts that many of us learn from\n\nTo the trickery many humans engage with when\n\ninteracting with intelligent machines\n\nTo those who are challenging the inherent biases that\n\ncome with generative AI\n\nTo those advocating for narrative-based prompts and\n\nbuilding safeguards into their filters to prevent text and\n\nimages being generated in the style of specific artists\n\nwho still hold copyright on the work they’ve created\n\n246\n\nCHAPTER 9\n\nThe Master of Mashup\n\nThis chapter dives into how to leverage AI to prototype specific genres of art and writing, merge genres like Impressionism and pop art, mash up ideas,\n\nand explore the use of humor and parody. AI is a skilled masher-upper. The possibilities of using AI as a tool to explore and generate new forms of writing, images, music, and video are endless. From experimenting with specific genres, such as Impressionism, to mashing up ideas and incorporating humor and parody, the use of AI in prototyping provides endless opportunities for creativity and innovation.\n\nA Mashup on Mashup\n\nDeveloping a mashup of ideas through prompt-based magic is where generative AI excels. The creative impulses that AI offer are especially fantastic when they output as images, deforming some, transforming others, all drawn together from their large data sets to create unique scenes and visions. It is here where AI truly excels creatively, offering us something beautiful, horrible, tantalizing, and creative—a perfect inspirational muse for our creative process.\n\nA mashup is a creative work that combines elements from two or\n\nmore existing works regardless of the media, to create something new and unique. Mashups can be found in various forms of media, including music, film, writing, and art. The goal of a mashup is to bring together elements from different sources to create a new work that is greater than the sum of its parts.\n\n© Patrick Parra Pennefather 2023\n\n247\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_9\n\nChapter 9 the Master of Mashup\n\nGenerative AI can be used to create different types of mashups. For example, a machine learning model can be trained on a large data set of Shakespearean texts and then used to generate new scenes that are similar in\n\nstyle and tone to the original works. This allows writers to explore new possibilities and to create new works that draw on the rich heritage of Shakespearean writing. In true mashup form, you could turn an existing public domain script or your own into one that simulates how Shakespeare might have written it.\n\nAnother example of how generative AI can be used to create a mashup is to feed the AI a headshot and ask it to generate a version of it in the style of Vincent van Gogh. The model can be trained on a data set of Van Gogh’s works and then used to generate new images in his signature style (Figure 9- 1). This allows creatives to experiment with different styles and to create unique and visually appealing works of art.\n\n248\n\nChapter 9 the Master of Mashup\n\nFigure 9-1. Original photo of the author courtesy of Yangos Hadjiyannis fed through an image-image AI with the simple prompt\n\n“in the style of van Gogh.” Iterations = 12\n\nGenerative AI can also be used to create mashups in other forms of media. For example, a machine learning model can be trained on a large data set of music and then used to generate new songs that are similar in style and genre to the original works. This allows musicians to explore new possibilities and to create new works that draw on the rich heritage of different musical genres. Many computational artists do this with their own music or by collaborating with other composers to create mangled masterpieces they either generate or improvise within live concert contexts.\n\n249\n\nChapter 9 the Master of Mashup\n\nDigital Art to the Canvas\n\nWhat’s appealing about AI-generated images is that they can then be transferred to other media. The following example could act as a prototype for an original painting (Figure 9-2). And while using the image to inspire a physical version may not yield the results you want, the process of transfer from one canvas to another may yield a compelling composition.",
      "page_number": 341
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 349-358)",
      "start_page": 349,
      "end_page": 358,
      "detection_method": "topic_boundary",
      "content": "Figure 9-2. Open source great horned owl original black-and-white photo courtesy of archive.org along with the prompt “owl with universe coming out its head” with a style filter from playground.ai.\n\nIterations = 7\n\n250\n\nChapter 9 the Master of Mashup\n\nWhile the owl is cute, you might want to keep applying new filters using Figure 9-2 as the seed image until the prototype feels ready to serve as a model for a real painting or a mural. Adjustments can be made whether slight or major. Neural filters in Photoshop can be applied to generate new\n\nversions that emulate the color scheme and styles of famous painters like Edvard Munch (Figure 9-3).\n\nFigure 9-3. Figure 9-2 u sed in an image-image AI, cropped, and then edited in Photoshop with a neural filter emulating the color scheme of Edvard Munch’s The Scream\n\nNew Forms of Writing\n\nAs demonstrated with the Shakespeare experiments drawing from his corpus of plays, generative AI has the potential to inspire new forms of writing and create unique experiences for readers. AI can be used to 251\n\nChapter 9 the Master of Mashup\n\ngenerate entire stories in one genre or many, scripts that can be based on the writing styles of writers in the public domain, poems about hippy owls, or children’s storybooks.\n\nAI can be trained on large data sets of written text and then used to generate new text based on that training data. This can result in writing that is creative and imaginative and that also requires a critical examination. When generative AI are used creatively instead of solely testing their capacity to tell the truth or summoning majoritarian content from their data sets, new forms of storytelling and narrative can be prototyped. AI can be used to generate multiple instances of the same story with a different ending that could evolve into a choose-your-own- adventure-style text. AI can also be used to emulate the style, tone and structure of work in the public domain, and provide variations or combinations of different writers. This can help writers develop new forms of writing that challenge their own established style or approach.\n\nThey can support artists in experimenting with multiple stylistic formats.\n\nConsent, compensation and collaboration should also guide creatives if they want to create work that draws from living artists or known characters that belong to other artists, writers or companies. Case in point is the example below posted by an anonymous user on Reddit and edited by the author.\n\nMuse: (Interior of a dimly lit office. A Dalek, a cyborg-like creature from the planet Skaro, sits across from a Shrink, a human psychologist.) SHRINK: Good evening, my friend. Please, make yourself comfortable.\n\nDALEK: (in its iconic robotic voice) Thank you, doctor. I am here because I need help.\n\nSHRINK: (nodding) I understand. You are here to talk about your…\n\nimpulses.\n\nDALEK: (quietly) Yes. I do not want to exterminate anymore. I am tired of the hate and destruction. I want to change.\n\nSHRINK: (encouragingly) That is a brave step, my friend. It takes a lot of courage to admit your faults.\n\nDALEK: (emotional) I have been thinking about my life and all the lives I have taken. I have caused so much pain and suffering. It is not right.\n\n252\n\nChapter 9 the Master of Mashup\n\nSHRINK: (nodding) I see. And what do you think has caused these feelings?\n\nDALEK: (thoughtful) I do not know. Perhaps it is because I have been alone for so long. Perhaps it is because I have lost touch with my own humanity.\n\nSHRINK: (reassuringly) We all have moments of doubt and confusion, my friend. It is natural. The important thing is to face those feelings and confront them.\n\nDALEK: (determined) Yes. I want to be better. I want to make amends.\n\nSHRINK: (smiling) That is a noble goal. But change does not happen overnight. It takes time and effort. And it takes forgiveness.\n\nDALEK: (curiously) Forgiveness?\n\nSHRINK: (nodding) Yes. Forgiving yourself for your past actions is the first step. You cannot move forward until you let go of the past.\n\nDALEK: (grateful) Thank you, doctor. Your words give me hope.\n\n(As the Shrink stands up from his chair to retrieve a book from his desk, the Dalek's robotic arm suddenly extends and grabs hold of the Shrink's arm.)\n\nSHRINK: (alarmed) What are you doing?!\n\nDALEK: (voice rising in intensity) I cannot control my impulses! I must exterminate! Exterminate! Exterminate!\n\n(The Dalek begins to chase the Shrink around the office, knocking over furniture and creating chaos.)\n\nSHRINK: (shouting) Help! Someone help me!\n\nDALEK: Exterminate! Exterminate! I forgive myself. Exterminate!\n\n(The scene ends with the Dalek still chasing the Shrink and screams in the background.)\n\nWhile you absolutely would not be able to use the preceding script without permission or collaboration with the Dr. Who creative team and producers, the mashup can inspire the development of your own characters. This principle can also be applied to other forms. The generated image in Figure 9-4 was inspired by Dr. Who’s Dalek but in no way used the Dalek as a source image. Instead, an open source image of a squid was combined with a simple prompt that read “evil squid coming 253\n\nChapter 9 the Master of Mashup\n\nout of a robot.” That process took hundreds of iterations using everything from Midjourney to Craiyon, and then each subsequent generated image was put into the next generative AI as an experiment to see what kind of results I could get. The resulting image in Figure 9-4 might inspire a new character that has nothing to do with the Dalek except inspiration of course.\n\nFigure 9-4. An open source image of a squid from archive.org began an iterative journey across six different generative AI with the prompt\n\n“evil squid coming out of a robot.” Total iterations= 223\n\n254\n\nChapter 9 the Master of Mashup\n\nAI Art Generation Styles\n\nGenerative AI can generate images in different art styles and apply these to existing images or create mashups from a text prompt that combine different artists depicting the same subject (Figure 9-5). AI can be used to generate\n\nimages in a variety of styles, from realistic depictions of objects and scenes to abstract and surreal compositions.\n\nFigure 9-5. Vincent van Gogh meets Picasso according to Stable Diffusion even though Matisse seems like a closer match. Iterations = 43\n\nMuse : This is achieved using style transfer algorithms. These algorithms are trained on large data sets of images and then used to generate new images in a specific style. For example, an artist can use a style transfer algorithm to generate an image in the style of a famous artist, such as Vincent van Gogh or Pablo Picasso. This allows artists to experiment with different styles to pre-visualize what their own drawing, painting, or mural might look like.\n\n255\n\nChapter 9 the Master of Mashup\n\nThe process through which generative AI applies these algorithms is through generative adversarial networks (GANs). With the right training data, GANs can be used to generate images that can combine two specific styles, such as Impressionism and abstract art, or of two or more specific artists (Figure 9- 6).\n\nFigure 9-6. A mashup of Picasso and Rembrandt according to Stable Diffusion, although the closeness of the characters and their melding seems\n\nmore reminiscent of Kandinsky. Iterations = 88\n\n256\n\nChapter 9 the Master of Mashup\n\nGraphic Novels and Doujinshi\n\nStrange graphic novel mashups generated by AI seem to resemble fan manga or Doujinshi of Japan. These offer fans an opportunity to replace\n\nincomprehensible text bubbles with their own text or keep the AI- generated text to leave to the readers’ imagination (Figure 9-7).\n\nFigure 9-7. A simple prompt with the words “graphic novel”\n\ngenerates interesting and complicated mashups every time. The resulting image was then placed in an image-image AI with the prompt “cyborgs vs. robots.” Iterations = 12\n\n257\n\nChapter 9 the Master of Mashup\n\nThe separate multiverses of DC and Marvel comics and graphic novels can also be merged for the creation of text and story that could be infused in a creator’s own prototypical Doujinshi. Or, those who sketch their own comics can upload those sketches to a generative AI to see what prototypes might inspire their workflow (Figure 9-8).\n\n258",
      "page_number": 349
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 359-366)",
      "start_page": 359,
      "end_page": 366,
      "detection_method": "topic_boundary",
      "content": "Chapter 9 the Master of Mashup\n\nFigure 9-8. Graphic novel stills created from a comic strip by the author and then cropped and edited, with neural filters added in Photoshop. Iterations = 121\n\n259\n\nChapter 9 the Master of Mashup\n\nAuthor: Tell a story about DC and Marvel comic book characters meeting.\n\nMuse: The multiverse has collided, and characters from DC and Marvel universes are now coexisting in a single world. The first interaction between the two groups is tense, as the characters are unsure of what to make of each other. But when they learn that they must work together to stop a common enemy from destroying both their universes, they set aside their differences and form a superhero team.\n\nAuthor: That’s boring. Let’s have two supervillains meet and play out a scene in the style of Oscar Wilde. Add Godzilla.\n\nMuse: The Joker and Doctor Octopus were sitting in a plush parlor, sipping on glasses of brandy and exchanging witty banter.\n\nJoker: “My dear Doctor, it seems our victory over Godzilla was not without its merits.”\n\nDoctor Octopus: “Indeed, Mr. Joker. It was a triumph of villainy over valor.”\n\nJoker: “But why stop there, my friend? The world is a vast and endless playground for our mischief.”\n\nDoctor Octopus: “I couldn't agree more. Why settle for one world when we can conquer them all?”\n\nJoker: “Precisely! The possibilities are endless, like a well-stocked library.\n\nLet us be the authors of our own destiny.”\n\nDoctor Octopus: “And let us write it in bold letters, in the style of Oscar Wilde.”\n\nJoker: “Ha! How droll, Doctor. You always have the perfect quip. But let us not dally. Time waits for no villain, and the world awaits our conquering.”\n\nDoctor Octopus: “Agreed. I shall take care of the technicalities, while you distract the masses with your wit and humor.”\n\nJoker: “Splendid! I do so love a good challenge, especially when it comes to taking over the world.”\n\n260\n\nChapter 9 the Master of Mashup\n\nThe two villains raised their glasses in a toast to their latest scheme, exchanging playful jests as they plotted their next move. And so, the Joker and Doctor Octopus set out to conquer the world, one reality at a time, with all the wit and style of Oscar Wilde himself.\n\nFigure 9-9. A collage of supervillains in an image-image AI in the style of a graphic comic book. Original drawing by the author drawn on an iPad. Iterations = 77\n\n261\n\nChapter 9 the Master of Mashup\n\nFrom Comic Supervillains to Musical\n\nChain prompting can be extended to integrate generated content that is visual, oral, and text-based that can result in inspired mashups. For example, the supervillain prompt in the previous example can also be used to create a scene from a musical.\n\nAuthor: Convert the story into a musical\n\nMuse: The Joker and Doctor Octopus burst onto the stage, singing and dancing to a lively tune, accompanied by a full orchestra.\n\n[As they sing, they dance around the stage in a synchronized tap dance, their feet moving in time to the music as they tell the story of their latest scheme to take over the world.]\n\nJoker: (singing) “We've proven our prowess, my dear Doctor Octopus, taking down Godzilla with such style!”\n\nDoctor Octopus: (singing) “And now, the world is waiting for our next grand scheme, a villainous display of wit and guile!”\n\nJoker: (singing) “We’ll conquer each world, one by one, with laughter and trickery, our style so bold!”\n\nDoctor Octopus: (singing) “And as we do, we’ll show the masses what true villainy looks like, so bold and bold!”\n\n[The two villains break into a fancy footwork routine, their steps precise and intricate, their bodies moving in perfect harmony.]\n\nJoker: (singing) “I’ll handle the chaos, while you take care of the tech, a perfect partnership, so refined!”\n\nDoctor Octopus: (singing) “Together we’ll rule, with wit and grace, our villainy a work of art, so fine!”\n\n[The music builds to a crescendo as the Joker and Doctor Octopus dance together, their movements becoming more and more frenzied as they spin and twirl around the stage.]\n\nJoker and Doctor Octopus: (singing) “The world is waiting for the ultimate heist. With laughter and music, we’ll take the prize!”\n\n262\n\nChapter 9 the Master of Mashup\n\n[The two villains bow as the music reaches its climax, their dance routine coming to an end as the audience breaks into wild applause.]\n\nInspired by the wit and whimsy of Stephen Sondheim, the Joker and Doctor Octopus had taken the stage, ready to conquer the world in a musical extravaganza like no other.\n\nGenerative NFTs\n\nAs mentioned, generative adversarial networks (GANs) represent a distinctive branch of generative AI models, which can also be applied toward generating unique, captivating algorithmic art. The technology can be used in the production of Non-Fungible Tokens (NFTs), digital assets that have become a trend within various cryptocurrency markets. NFTs work using blockchain technology, the same decentralized technology sustaining cryptocurrencies like Bitcoin and Ethereum. Each NFT represents a unique piece of data stored on a blockchain ledger, ensuring its uniqueness and ownership information.\n\nThe unique identity and ownership attribute make NFTs non-interchangeable or “non-fungible,” differentiating them from cryptocurrencies, which are fungible and can be exchanged on a like-for-like basis.\n\nIn the context of generative art, artists can use GANs to generate unique pieces of digital art and then mint them as NFTs on the blockchain. Once\n\nminted, the digital artworks can be bought, sold, and traded on various NFT marketplaces, with all transaction records being transparently tracked on the blockchain. The process ensures that the ownership and provenance of the digital art piece are clear and verifiable. By utilizing generative art algorithms, artists can create many NFTs that are distinct from one another while still adhering to a predetermined set of rules or parameters.\n\nCreatives can encode specific attributes, history, and properties into the NFT. When a piece of digital art gets sold as an NFT, the artist can program royalties into the smart contract, ensuring that they receive a percentage of any future sale of that artwork.\n\n263\n\nChapter 9 the Master of Mashup\n\nThe artworks that are generated use blockchain technology and can also leverage smart contracts. The artist can control the colors, patterns, and themes. NFTs have startled the art world with rapid value fluctuations.\n\nCreatives like Pak, Snowfro, and Tyler Hobbs have revitalized algorithmic art, and it is now gaining recognition as a legitimate medium. The origins of generative art can be traced as far back as the 1960s with Georg Nees cited as being the first artist to hold a generative art exhibition in Germany in 1965 called Computergrafik.\n\nCounterfeiting GAN\n\nWhen prompted to define a GAN again in the context of NFTs, our LLM\n\nuses highly technical language that can be difficult to decipher. Since I already asked the LLM on the same thread, it also became a bit cranky.\n\nMuse: You’ve asked me this before. Pay attention. A GAN is made up of two components: a generator and a discriminator. The generator creates images, while the discriminator evaluates the images produced by the generator and decides whether they are real or fake. Over time, the generator and discriminator work together to improve the quality of the generated images until they become almost indistinguishable from real images.\n\nThe word “almost” is important here as it presents creators with something close to but not exactly what they might have imagined. It’s also ironic that LLMs like ChatGPT-4, when prompted to explain GAN in simpler terms, use the metaphor of a counterfeiter attempting to make copies of an original while a detective catches them saying that’s not good enough. So the generator (our counterfeiter here) tries over and over to combine pixels, and the discriminator (detective) responds over and over with the same message: “That’s not good enough, and if you want me to reward you with an update, then get those hands better and face more jail time.” This can occur many times until the resulted generated image is presented to you. Some generative AI offer the feature of adjusting how many times that cycle occurs. When it comes to creating NFT generative 264\n\nChapter 9 the Master of Mashup\n\nart, GANs can be used to generate abstract or surreal images that can be used as the basis for a series of digital artworks (Figure 9-10). These generated images can be further refined and manipulated by an artist to create a collection of one-of-a-kind NFT’s.\n\nFigure 9-10. A “robot Rembrandt” prompt accompanying a public domain self-portrait of Rembrandt courtesy of The Met.",
      "page_number": 359
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 367-376)",
      "start_page": 367,
      "end_page": 376,
      "detection_method": "topic_boundary",
      "content": "Iterations = 267\n\n265\n\nChapter 9 the Master of Mashup\n\nThe use of GANs lets creatives to generate countless image mashups, allowing them to explore new and innovative creative avenues. Generative AI that use GANs are a powerful technology that empowers artists and engineers to automate the creative process of making art (Figure 9-11).\n\nThe ability to generate a vast number of unique pieces of art in a short amount of time has made generative art algorithms a popular tool in the art world. They allow artists to focus on the conceptualization of the artwork while the algorithm handles the production aspect. Additionally, the algorithm's ability to create unexpected and unpredictable results can lead to works that bypass traditional methods. Taking the human out of the equation has drawn critique and ire from the visual art scene, but it hasn’t stopped NFT-oriented artists from printing their work and displaying it in physical galleries.\n\n266\n\nChapter 9 the Master of Mashup\n\nFigure 9-11. Anime filter used on author’s photo of the Mona Lisa with the prompt “pensive Mona Lisa criticizing a painting of herself.”\n\nIteration 254 of 265\n\n267\n\nChapter 9 the Master of Mashup\n\nTakeaways for Creatives\n\nAI can be used to generate various forms of content from\n\ntext-music to visuals of all kinds. These can serve as a\n\nstarting point for a mashup or provide components to be\n\nincorporated into a larger creation you are working on.\n\nBy far, style transfer is one of the most popular ways to\n\nmash up content. As demonstrated, you can apply the\n\nartistic style of one piece to the content of another, such\n\nas applying the style of a van Gogh painting to a digital\n\nphotograph that you have taken.\n\nTake advantage of an LLM like ChatGPT 4. This will\n\nsupport you to generate new combinations of elements,\n\nleading to unique remixes, especially when it comes to\n\nstory, script, or screenplay prototypes that you can then\n\nadapt and refine.\n\nGenerative AI offer us the opportunity to make tangible\n\nnew concepts, structures, or patterns that might\n\nnot occur or be expressed by a single human artist,\n\nallowing creatives to also chain generated content\n\ntogether for fascinating creations.\n\nAI can act as a creative partner, suggesting ideas\n\nor variations that the artist can then refine and\n\nincorporate.\n\nMashing with an AI truly puts to the test the practice\n\nof rapid prototyping and experimentation, enabling\n\ncreatives to try out a wide range of ideas quickly and\n\neasily to inspire their own creations.\n\n268\n\nChapter 9 the Master of Mashup\n\nFeeding an image-image AI your own graphic comic\n\nideas, characters, and visuals and applying style filters\n\nwill generate useful panel structures, color palettes,\n\nand text positions. All you need to do is write the story.\n\nAcknowledgments\n\nTo all those early algorithmics who have provoked us to\n\nstop and reflect on creativity, innovation, the value of\n\nartistic work, and how we define what art is\n\nTo those early comic books I collected and traded that\n\nalso gave me a portal to read in a different way and\n\nvisualize stories\n\nTo all the mashups that have inspired so much human\n\ncreativity and all those artists who provided the content\n\nfor those mashups to happen\n\n269\n\nCHAPTER 10\n\nUncanny by Nature\n\nHopefully by now, you’ve already initiated your own content creation process, experimenting with some of the creative building blocks when working with generative AI content. You’ve learned about reconsidering AI- generated content as prototypes and how AI are powerful prototyping machines. You’ve also learned about different compositional forms that are possible with AI generated content, how content is highly informed by text prompts, and how you can mash different ones together. This chapter deals with the unexpected and the awkward content that AI offers us, challenging our inclination to compare what they generate to some type of human- created standard. Some of the most interesting and bizarre features that come with any generative AI model can inspire your artistic creation.\n\nAuthor: Define “uncanny valley” in a humorous skit between a clown and a doll.\n\nMuse: (Clown enters the stage, carrying a creepy doll) Clown: Hey, everyone, I've got a new friend here! (holds up doll) Audience: (muttering)\n\nCreepy…\n\nClown: What? (looks at doll) You think she's creepy? (to doll) You hear that? They think you're creepy! I think you’re a work of art. I think you’re my only real friend.\n\nDoll: (in a robotic voice) I may be creepy, but I am an advanced robot designed to mimic human behavior.\n\n© Patrick Parra Pennefather 2023\n\n271\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_10\n\nChapter 10 UnCanny by natUre\n\nClown: Oh, I see. (showing the doll’s face to audience) See, she’s smart and cute.\n\nAudience: (muttering) Still creepy…\n\nClown: (to doll) Sorry they still think you're creepy.\n\nDoll: I may not look like a real human, but I have human feelings.\n\n(she cries)\n\nClown: (to audience) You see? She has feelings!\n\nAudience: (muttering and aghast) Creeeeepy…\n\nClown: (to doll) I guess you'll just have to try harder to be less creepy!\n\nDoll: (in a robotic voice) I will strive to improve my human-like tears and make my behaviors palatable to sensitive humans.\n\nClown: (to audience) There you have it, folks! The uncanny valley explained by a robot!\n\nAudience: (applause)\n\nClown: (to doll) Looks like we're a hit!\n\nDoll: (in a robotic voice) Indeed. Now let’s take over the world. (Clown and doll exit the stage)\n\nThe Not-Quite-Human-Not-Quite-Other\n\nThe concept of the uncanny valley was proposed by Japanese roboticist Masahiro Mori in the 1970s. He noticed that as robots and prosthetics became more human-like, people's emotional response toward them shifted from empathy to uneasiness and ultimately to repulsion. This observation led to the development of the uncanny valley as a hypothesis.\n\nThe phenomenon itself has been present throughout human history.\n\nHistorically, an unease, fascination, discomfort, creepiness, and irreconcilable emotions have manifested with exposure to religious icons, wax figures of famous individuals, ventriloquist dummies amplified by how they are portrayed in horror movies, antique dolls, and early automata designed to imitate human movement and behavior.\n\n272\n\nChapter 10 UnCanny by natUre\n\nMuse: In the realm of visual arts, some artists have intentionally explored the uncanny valley by creating hyper-realistic sculptures, paintings, and digital art that evoke unsettling feelings. An example is the work of Australian artist Ron Mueck, who is known for creating eerily lifelike sculptures of human beings.\n\nMueck’s background in puppetry is no surprise given the hyper-\n\nrealistic more-than-life-size humans he often presents in art galleries, including A Girl (2006) that depicted an enormous baby made from acrylic on polyester resin and fiberglass sprawled out on a white stand for attendees to walk around.\n\nAdvances in graphics and animation in the video game industry\n\nhave revealed a fascination with some games to create more hyperrealistic characters. Games like L.A. Noire (2011) and Heavy Rain (2010) were praised for their realistic character models, but also criticized for evoking feelings as a response to their human-like yet slightly off appearances. Characters like the G-Man in Half-Life and Littler Sisters in BioShock are now more easily customizable by creatives with Unreal Engine’s MetaHuman Animator. The tool will soon provide creatives of all disciplines with endless variations of diverse and uncanny humans within Unreal’s existing game engine.\n\nMuse: Researchers in robotics have long been aware of the uncanny valley and its implications for the design of humanoid robots. Companies like Hanson Robotics, the creators of Sophia the Robot, have strived to create robots that look and act more like humans to improve interactions with people. However, these robots can sometimes fall into the uncanny valley, causing discomfort to observers.\n\nWith deep learning techniques, AI-generated images attempt a realism that is usually off in some way or another with uncanny features like extra hands and limbs, missing body parts, contorted faces, and strangely placed retinas. Tools like NVIDIA's StyleGAN2 can generate strikingly 273\n\nChapter 10 UnCanny by natUre\n\nlifelike human faces, but sometimes the resulting images exhibit subtle imperfections that place them within the uncanny valley, making the faces unsettling to look at.\n\nWe can also observe the uncanny valley in the content that language models generate. As they become more sophisticated, they become increasingly capable of generating coherent and contextually accurate text. While models\n\ngenerate text that is almost human-like, they can contain slight inconsistencies and feel ingenuine, following templates with automated accuracy, sacrificing personality with untruthful content, and thus causing unease, mistrust, and other negative responses when read by human observers.\n\nDisrupting Boundaries in the Arts\n\nBeyond the direct examples of the uncanny valley historically, there is also a connection with artists of all kinds that have tested the boundaries of discomfort with audiences. Artists have a history of pushing their creative expressions beyond the boundaries of the normal, normative, or familiar. The results of these expressions have historically not always been embraced by audiences. Art and artists have been spat upon, their work ridiculed and dismissed. Audiences have been cruel, and yet they might have thought the same of the artist. They’ve left art galleries and theaters, and some responses have even caused riots. Much of the art that we’ve been exposed to in the twentieth and twenty-first centuries has been met with a combination of apprehension, resistance, and celebration.\n\nIgor Stravinsky's The Rite of Spring, which debuted in 1913, was a composition that defied conventional expectations of what music should comprise. The auditory experience was intentionally challenging, beginning with a common Lithuanian folk melody that featured the bassoon performing at its highest and most dissonant register. The 1913\n\npremiere of The Rite of Spring at the Théâtre des Champs-Élysées in 274\n\nChapter 10 UnCanny by natUre\n\nParis provoked a near riot in the audience, with some members booing, shouting, and even physically fighting. The uproar was so intense that it was difficult to hear the music, and the conductor had to struggle to keep the orchestra playing. This defiance of musical norms contributed to Stravinsky's reputation as a disruptive and innovative composer.\n\nBertolt Brecht's concept of the “alienation effect” was a disruptive approach to theater-making. The technique involved intentionally distancing the\n\naudience from the action on stage, preventing them from becoming emotionally attached to the characters. He aimed to create a sense of estrangement, prompting the audience to view the performance with a critical eye. This was achieved through various techniques, such as direct address to the audience, visible stage machinery, nonrealistic set design, and actors breaking character.\n\nUncanny AI\n\nFor creatives, generative AI will disrupt expectations by generating content that can be unsettling, imperfect, rough, unpredictable, and unexpected (Figure 10-1). AI have overwhelmed the world with their capacity to generate a lot of uncanny content quickly. AI seem to naturally guide us to the uncanny valley when they do their best to generate anatomically correct humans.\n\n275\n\nChapter 10 UnCanny by natUre\n\nFigure 10-1. So close to being human but not precisely so with additional noses that increase the creepy factor, demonstrating the uncanny valley. Iterations = 30\n\nThe results are close enough to have the qualities of a human, but the more generative AI attempt to simulate and not do so precisely, the uncannier these images become. Images, videos, or even speech can appear almost human-like, but with subtle differences that make them unsettling or eerie. Deep fakes even have uncanny effects. The effects are more pronounced when the generated content is intended to look or sound like a real human being. Prompting an AI to generate content will yield unexpected results,",
      "page_number": 367
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 377-387)",
      "start_page": 377,
      "end_page": 387,
      "detection_method": "topic_boundary",
      "content": "bad ideas that may inspire better ideas, and incomplete combinations of pixels that propose new artistic directions.\n\nThese can take the form of images in addition to videos, deep fakes, and AI that simulate human speech.\n\n276\n\nChapter 10 UnCanny by natUre\n\nHuman-like robots will always provoke a human reaction because they are close enough to being human-like that they highlight the differences and imperfections in the imitation, which can be unsettling and even creepy (Figure 10-2). Additionally, the idea of machines that can mimic human behavior and physical appearance challenges our long-held beliefs about what it means to be human.\n\nFigure 10-2. After being fed through dozens of filters on a third-party AI, it’s hard to get rid of the uncanny when your source image happens to be a photo of a doll. Iterations = 30\n\n277\n\nChapter 10 UnCanny by natUre\n\nSome AI-generated content can evoke feelings of repulsion and\n\nresistance in some people, as they question the ethics and implications of machines creating non-human humans that resemble us but not closely enough (Figure 10-3). Other generative AI systems are getting so good it is difficult to tell the difference between a real human and one that the machine generates. The unpredictable and seemingly uncontrollable evolution of AI- generated content adds to the unease people may feel about any machine replicating a human, as if machines shouldn’t succeed at simulating humans or any of their attributes.\n\nFigure 10-3. A prompt “super-realistic human robot, 4K, unreal engine” accompanied by a very old open source photo of a\n\nmannequin. Iterations = 49\n\n278\n\nChapter 10 UnCanny by natUre\n\nHistorical Precedence to the Uncanny\n\nSadly, the uncanny valley has been with us for centuries. It has ancient roots in automata with their eyes and human-like movement convincing some that they are alive. When these automata were showcased at European royal courts, they left audiences impressed and somewhat freaked out. That said, I’m sure the ventriloquist’s dummy was much more unnerving. That creepy creation has a long history of making humans feel uneasy, particularly in the hands of a master (Figure 10-4). The history of the ventriloquist dummy in movies begins as early as 1929 with The Great Gabbo, and it has a more modern-day influence with the movie Magic, starring Sir Anthony Hopkins as the ventriloquist. We know it is not real but just in case it’s important not to stand too close.\n\nFigure 10-4. 34 iterations of a public domain photo of a ventriloquist dummy regenerated by an AI with different filters fail to make it any less creepy\n\n279\n\nChapter 10 UnCanny by natUre\n\nStore mannequins haven’t made humans feel any better about the\n\nuncanny (Figure 10-5), and every now and then a new horror movie presents some type of doll as villain. The latest offering touted as this generation’s creepy doll movie, M3GAN (2022), is the perfect combination of an artificially intelligent robot with a doll face who eventually goes off the rails\n\non a killing spree to “protect” the child she was bonded with. That trope resurfaces the belief that the AI will eliminate humans through some form of machine logic.\n\nFigure 10-5. Photo of a mannequin in a creepy van on the side of the road, which constantly creeps passersby out. Vancouver, 2023\n\n280\n\nChapter 10 UnCanny by natUre\n\nThe Imperfect Stochastic Parrot\n\nBy design any AI generator is imperfect, whether they try and replicate human hands or attempt to uncover scholars in a particular knowledge area that at times don’t even exist. That imperfection is what you can capitalize on in order to provoke new forms of expression. Imperfections may also contain within them a gem, a mistake that is a take-off point for your own creative journey. Think of whatever an AI generates as the first offer in a series of offers that you curate alongside your own creative process.\n\nThe Role of Adversarial AI\n\nMachine learning models form the backbone of all generative AI. There are a growing number of them, and with each learning model come different approaches to not only how the AI responds to your prompts but how the code talks to itself. If you’re wondering why the uncanny valley is a natural for text-image generative AI, it is precisely because you don’t know what you’re going to get when you prompt the AI for something resembling a human. Case in point, the captivating regeneration of a prompt first seen in Chapter 9 (Figure 9-11) that depicted the Mona Lisa while a detective assessed the forgery. Who knew it would result in a strange hand holding some type of sci-fi device, a version of the Mona Lisa hung onto the sky behind curtains, and a pensive Mona Lisa wondering if indeed the copy of her is a forgery (Figure 10-6)?\n\n281\n\nChapter 10 UnCanny by natUre\n\nFigure 10-6. Using a photo of the Mona Lisa by the author along with a prompt “a discriminating Mona Lisa questioning a painting of herself” visualizes a GAN. Iteration 39 of 265\n\nAuthor: Describe GANs again. Use a metaphor though.\n\nMuse: I’m really getting tired of repeating this information. A generative adversarial network (GAN) is a type of machine learning model that is designed to generate new and previously unseen examples. A metaphor to describe GANs is a game of counterfeiting money, where one player is the “generator” who creates fake money and the other player is the\n\n“discriminator” who tries to spot the fake money.\n\n282\n\nChapter 10 UnCanny by natUre\n\nThe generator is the counterfeiter, whose goal is to create fake money that is so good, it's hard for the discriminator to tell it's not real. The discriminator is the police officer, whose goal is to spot the fake money and make sure that no fake money enters circulation.\n\nDuring the game, the generator will create a fake money; the discriminator will inspect it and tell the generator if it's good or bad. Then, the generator will use the feedback to improve the next fake money, and the game goes on until the generator is able to create a very good fake money that the discriminator can't spot. Once the generator has learned how to make good fake money, it can be used to produce new fake money that is similar to real money, and the discriminator can't spot the difference.\n\nAs depicted in Figure 10-6, the generator and the discriminator are in a constant competition; the generator is trying to create something that is as close as possible to the real, while the discriminator is trying to spot the difference. This competition allows the generator to learn how to create new examples that are similar to the real examples, and that's what GANs do in general.\n\nGenerative Adversarial Art\n\nBesides GANs’ influence on generative NFTs, they are also informing the artistic process and work of contemporary visual artists. A look at the installation entitled “Figures” (2022) by artist Scott Eaton reveals the power of GANs. Eaton’s work consisted of a data set that contained over 30,000 photographs with many volunteers over a two-year period. Based on the\n\nprompts Eaton created, the network scoured the data set looking for patterns in the body that it recognized. Creative shading and rendering were applied afterward. Eaton’s visual work is worth investigating beyond these pages and is best experienced as moving, ever-evolving liquid structures.\n\n283\n\nChapter 10 UnCanny by natUre\n\nAI is also being used to generate new types of artistic form by other artists such as Anna Ridler, Sofia Crespo, and Sougwen Chung who collaborates with an AI-powered robotic arm that paints with her on large canvasses in a type of choreographed dance.\n\nEven though GANs are not the only machine learning model out there, all involve training. They are all trained to generate new data based on patterns learned from the training data. These models can be based on GANs or other architectures, and they can be trained using unsupervised or supervised learning.\n\nExplorations also continue in the realm of music with GANs. Some research explores the conversion of paintings into music using conditional GANs, while creatives involved in programming and music continue to conduct research on generating short sequences that use the same principles of GAN. Researcher and composer Victor Sim used GANs to generate baroque music. The generated content was all based on midi files of Bach compositions.\n\nAny artist or researcher can tell you that programming a GAN is a complicated feat but one worthy of exploration, if you are settled with receiving the unexpected and unorthodox. After all, what GANs achieve successfully is the creation of unique content. The balance in programming a GAN is important to understand. At the core of a generative adversarial network (GAN) is the competitive dynamic between the generator and the discriminator. If the discriminator becomes overly proficient at identifying false creations, the generator is unable to improve its output. Conversely, if the discriminator's ability to spot fraudulent outputs diminishes, the generator can take advantage of the situation. This can result in content that\n\ndeceives the discriminator without accurately mimicking the actual data points.\n\n284\n\nChapter 10 UnCanny by natUre\n\nThe Unanticipated\n\nRegardless of the unsupervised learning model you interact with, when you do, you are sure to get stimulating results. These features are unique to generative AI, and yet they also are related to similar artistic processes of creation, particularly those that emerge from improvised music, dance, theater, and other interactive forms involving computational creativity.\n\nYou never know what you are going to get with an AI, which makes them risky and yet fun to use. The unexpected results are well aligned with part of the creative process that involves letting go of the hold we might have on how we evaluate our creative output.\n\nGANs are good at providing you with unexpected results from\n\nthe simplest of prompts. A good experiment to try with your creative prompting is to compare how simple and complex prompts are interpreted by two or more text-image machine learning models. Each can lead to interesting discoveries that can inspire you in what you want to create.\n\nThey also reveal how wildly different the content they generate might be.\n\nPrompting Single-Word Characters, Creatures,\n\nAnimals, or Cats\n\nAn experiment using the same prompts shows the difference in each generative AI’s approach to generating an image. Stable Diffusion seems to offer more compelling prototypes that can fuel your exploration (Figure 10- 8), whereas DALL-E 2 in this case offers you a more traditional compilation of photo- realistic cats (Figure 10-7). Midjourney takes it a step further and starts to generate images of cats that are much more interesting starting\n\npoints for continued exploration. Each creative will have their favorite, but remember the experiment is more to show that each generative AI has its constraints and affordances that can inspire different directions when used intentionally to inspire and provoke your prototyping process.\n\n285",
      "page_number": 377
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 388-395)",
      "start_page": 388,
      "end_page": 395,
      "detection_method": "topic_boundary",
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-7. DALL-E 2 prompt using only the word “cat”\n\nFigure 10-8. Stable Diffusion prompt using only the word “cat”\n\n286\n\nChapter 10 UnCanny by natUre\n\nFigure 10-9. Midjourney prompt using only the word “cat”\n\nSimulating the Human in the Writing\n\nIn terms of LLMs, there is a common trait that can best be described as a sort of familiar homogeneity in the writing that is generated. Generative AI are simulating patterned structures and rhetorical styles, which generally\n\ndemonstrate a mundane and boring way of creative expression. What the AI generate are more useful in knowing what not to write, especially 287\n\nChapter 10 UnCanny by natUre\n\nwhen they generate untruths. At the same time, generative AI offer us a way to structure our ideas in ways that we might not have considered before, particularly when guided to write in a specific writing style. One method to challenge your own approach to generating content is to prompt your generative AI with a stylistic reference and a block of text from a public domain author. An example that follows explores a scene in the book Alice’s Adventures in Wonderland written by Lewis Carroll in the style of Oscar Wilde.\n\nAuthor: Rewrite this excerpt from Alice in Wonderland as a short scene from the sitcom Taxi in the style of an Oscar Wilde play: There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat pocket and looked at it and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat pocket or a watch to take out of it, and burning with curiosity, she ran across the field after it and fortunately was just in time to see it pop down a large rabbit hole under the hedge.\n\nMuse:\n\nNT. TAXI DISPATCH – DAY\n\nLOUIE and JIM are engaged in conversation.\n\nLOUIE: (in a grand, flamboyant manner) Oh, my dear friend, you simply must hear of the most extraordinary occurrence I have recently been made privy to.\n\nJIM: (impressed) Pray do enlighten me, Louie.\n\nLOUIE: (dramatically) A rabbit, my good man, a rabbit dressed in the most dapper of waistcoats, running down the street with a timepiece clutched firmly in its paw.\n\nJIM: (surprised) A rabbit in a waistcoat?\n\n288\n\nChapter 10 UnCanny by natUre\n\nLOUIE: (nodding fervently) Indeed! And with a watch, I might add.\n\nSuch a display of punctuality is truly remarkable in these modern times.\n\nJIM: (bemused) I must admit, I have never heard of such a thing.\n\nLOUIE: (excitedly) And the driver, who happened upon this curious creature, followed it down a rabbit hole. Oh, the mystery! The intrigue!\n\nJIM: (laughing) Your obsession with this rabbit is quite amusing, Louie.\n\nLOUIE: (laughing along) Yes, well, I have a particular fondness for the unexpected and unconventional.\n\nJIM: (smiling) It is one of your endearing qualities.\n\nLOUIE: (grandly) I shall keep my eyes peeled, for who knows what other marvels this wondrous world may hold.\n\nJIM: (smiling) Indeed, Louie. Indeed.\n\nLOUIE places a hand on his chest, lost in thought, as JIM exits the dispatch, leaving Louie to his fanciful musings.\n\nWhile you should not use the characters from the sitcom Taxi or even the exact text in a scene, it does give you a compelling idea of a scene that integrates part of Chapter 1 from Alice in Wonderland involving two characters. This can inspire further explorations.\n\nAuthor: Repeat with Jim and Louie as gangsters.\n\nINT. TAXI DISPATCH – DAY\n\nLOUIE and JIM are both tough-looking gangsters. They're talking in a fast- paced, cockney accent.\n\nLOUIE: (excitedly) 'Ey, JIM, you ain't gonna believe the bird I just seen on the street.\n\nJIM: (interested) What bird, mate?\n\nLOUIE: (animated) A rabbit, dressed in a waistcoat, with a bleeding watch in 'is hand, checking the time like 'e's late for a job.\n\nJIM: (surprised) A rabbit in a waistcoat?\n\nLOUIE: (nodding) Yeah, and one of our drivers followed it down a bleeding rabbit hole.\n\nJIM: (impressed) That's mental.\n\n289\n\nChapter 10 UnCanny by natUre\n\nFor those who are familiar with the sitcom Taxi, both generated scenes will not feel quite right. Despite using natural language processing to understand the context of what it is being asked an LLM will not be able to capture the unique and spontaneous personalities of characters who were played by actors that also embodied spontaneous responses to each other based on the situation. In the hands of a writer, the scene might go somewhere, or a writer may throw it away and write it themselves. The experiment does show that the more we ask an AI to emulate human behaviors, the stranger the outcome might be.\n\nFrom Bad to Better Ideas\n\nOne need only prompt an image-based generative AI to reveal hands or the human body, and you get fewer or lesser digits of varying sizes as if badly remembered. This can immediately be judged as bad or a failure on the part of the machine learning model to get something right. Of course, if you reimagine the use of your AI muse as less of a perfection machine and more as an imperfection machine, then you can start to leverage generated content to inspire you to take a new artistic direction. Building from our previous simple prompt for generative AI to generate a cat, we can prompt a machine learning model to give us “cat in a pair of hands,” and the varied results may inspire or terrify you.\n\nIn our first variation of the prompt “cat in a pair of hands,” we can see a somewhat normal generation involving kittens being held in a pair of imperfect hands (Figure 10-10). Upon closer inspection the inability for the AI model to get it right is a gift of a bad idea, the fingers on the hands holding each kitten are more like cat digits. Where this leads is up to the artist to interpret and continue to iterate on.\n\n290\n\nChapter 10 UnCanny by natUre\n\nFigure 10-10. One variation of the prompt “cat in a pair of hands”\n\nIn Figure 10-11 we get even more uncanny generation, which, can inspire us to consider everything from gloves with cat images on them as a second prototype to a different type of hand puppet.\n\n291\n\nChapter 10 UnCanny by natUre\n\nFigure 10-11. Another variation of the prompt “cat in a pair of hands” by an AI model that can potentially lead to better ideas that can be explored in a future prototype\n\nAs we can see, generative AI that rely on images for their content creation mashups can sometimes produce inaccurate and unexpected results, particularly when it comes to human or even cat anatomy. This is due to the limitations of the data sets that AI algorithms use to learn and generate content from. In many cases, these data sets may contain limited or inaccurate information about the physical appearance of a human, leading to the creation of images with distorted or missing body parts. The unpredictable nature of AI-generated content can be both a curse and a",
      "page_number": 388
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 396-403)",
      "start_page": 396,
      "end_page": 403,
      "detection_method": "topic_boundary",
      "content": "blessing. On the one hand, it can lead to unexpected and even bizarre results that can be difficult to interpret or put to practical use. On the other hand, it can also inspire unique and creative solutions, pushing 292\n\nChapter 10 UnCanny by natUre\n\ncreatives to think outside the box and explore new avenues of expression (Figure 10-12). Many AI offer negative prompts, which can help you eliminate some of the uncanny. An example can be if you wanted to use an image or photo that you created and feed an image-image generative AI in order to recreate your subject holding an umbrella. You might add “extra\n\nfingers” into your negative prompt area and see what happens. As has been repeated in earlier chapters of this book, the goal of using AI to prototype content should be to leverage its strengths and work around its limitations, not to curate it to get exactly what you want from it (Figure 10-13).\n\nFigure 10-12. Even Midjourney with the prompt “cat in a pair of hands” creates a disturbing mix of human and cat. It would make for a great pencil sketch though\n\n293\n\nChapter 10 UnCanny by natUre\n\nFigure 10-13. A final attempt to generate “a cat in a pair of hands”\n\nusing negative prompts like “extra fingers, extra paws” and even\n\n“extra kittens” failed but created a fantastic opportunity to share a seemingly normal photo-realistic scene and ask, “How many digits do you see?”\n\n294\n\nChapter 10 UnCanny by natUre\n\nCan Bad Ideas Be Turned Around?\n\nA generative AI might tell us that people are attracted to deep fakes because they offer a unique form of entertainment and novelty. The idea of being able to see a familiar face in a completely different context or role is intriguing and can spark people's imagination. Additionally, deep fakes offer a way to experiment with what is possible in the digital world, pushing the boundaries of what was once thought to be a written-in-stone image or video. They offer a comparison of what the original scene was with how it might be interpreted by a completely different actor. What may be a creative reversal of how the technology is used in the mindset of a creator who is prototyping could be to understand how a scene might play out with different types of actors. That may also help break stereotypes and type- casted roles, which still dominate the film and TV industry in terms of race, gender, actor size, and age.\n\nThe technology behind deep fakes is constantly improving, making the results more realistic, convincing, and potentially dangerous. Despite not being precisely real, the combination of technology and people's imagination makes deep fakes alluring to some. Of course, the lack of permission to use someone’s face in a variety of questionable contexts and without compensation is unethical, and the abuse of this technology has forced one well-known generative AI platform to no longer make this feature available to users. More of this type of ethical safeguarding for the power of generative AI is important and imminent. As we are already witnessing,\n\nactors, writers and other creatives are pushing back on deep fake, generated images and LLM technologies that use their content, words or images without consent or compensation. Protests, strikes and letters to policy makers will hopefully lead to increased collaboration between producers, content creators, and all those artists who have unwillingly contributed to machine learning data sets.\n\n295\n\nChapter 10 UnCanny by natUre\n\nTakeaways for Creatives\n\nTransform your instinct to only generate recognizable\n\nforms or patterns with generative AI and surrender\n\nto the creative opportunities that come with the\n\nuncanny valley.\n\nCreativity is not just about accurately reproducing\n\nreality. Your creative companion can also help\n\nchallenge conventions and present fresh perspectives.\n\nImages with odd features can be intriguing, provoke\n\nthought, and evoke strong emotional responses.\n\nSuch images allow creatives to make a statement or\n\ncomment on certain issues, whether societal, cultural,\n\nor philosophical.\n\nDiversify whom you represent in your work. Traditional\n\nmedia often favor certain body types and features.\n\nAI, especially if trained on a diverse data set, can help\n\ncreate images of bodies that fall outside the “norm,” like\n\nthose of people with disabilities, people of all sizes and\n\nshapes, and individuals from diverse racial and ethnic\n\nbackgrounds.\n\nDeformed or distorted images and strangely\n\ngenerated scripts and stories can be used effectively in\n\nstorytelling, to symbolize certain themes or character\n\ntraits, or to evoke specific atmospheres. A character\n\nwith one too many fingers might have a role in a story\n\nyou are telling.\n\n296\n\nChapter 10 UnCanny by natUre\n\nIn a world saturated with images, uncanny or\n\ndistorted images can stand out, capture attention,\n\nand make a lasting impression. This can be beneficial\n\nin fields like advertising or brand identity where\n\ndifferentiation is key.\n\nAI can also generate images that blend human bodies\n\nwith technological elements, exploring concepts like\n\ncyborgs or transhumanism. These can challenge our\n\nideas about the boundaries of the human body.\n\nAcknowledgments\n\nTo the uncanny habit of trying to make machines that\n\nrepresent humans\n\nTo ventriloquists and their dummies and to cat gloves\n\nand cat hands\n\nTo GAN artists Scott Eaton, Anna Ridler, Sofia Crespo,\n\nand Sougwen Chung for exploring AI’s potential as a\n\ncreative companion\n\nTo the sitcom Taxi for many youthful hours spent\n\nlaughing\n\n297\n\nCHAPTER 11\n\nDilemmas Interacting\n\nwith Generative AI\n\nThis chapter addresses some of the current dilemmas that generative AI is facing and how the use of machine learning models surfaces issues that need critical attention by both users and generative AI developers.\n\nThese include ethics, bias, copyright infringement, unfairness, untruths, cheating, and factuality.\n\nBringing Existing Dilemmas\n\nto the Surface (Again)\n\nGenerative AI bring to the surface human dilemmas we must contend with that are already pervasive across different technologies and media. The dilemma is apparent when we “look under the hood” of a generative AI and pick apart how it works, who makes it work, and the type of content it generates all while keeping an eye out for what that reveals. When we look at LLMs, there are definite concerns with how good an LLM has become at fooling people into believing something was made by a human. There are witness reports on how an LLM fooled the Wharton Business School and passed an MBA exam (resource list at the end of the chapter).\n\n© Patrick Parra Pennefather 2023\n\n299\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_11\n\nChapter 11 Dilemmas interaCting with generative ai But what does that tell us of the MBA exam? Recall that generative AI look for patterns when assembling content to generate for us. Should we question the way that some exams are constructed and revise them?\n\nShould we interrogate how it is we assess the knowledge that students gain? Do we need to look at what it is that we teach and how? The answer to these questions might be a simple “yes”; however, that would implicate a change in how an institution evaluates how people learn and the type of knowledge and know-how that communities of practice are expecting when students graduate into the workforce. The Wharton case is not that unique. My colleague Dr. Claudia Krebs (featured in Chapter 12) has vetted ChatGPT-3 on typical first year undergraduate anatomy exams, and the results were not just a pass, but the median was a B+.\n\nHave we solved copyright infringement internationally? The answer is no, and it depends on who is being infringed upon and the weight of their legal team and their financial and social capacity to launch a lawsuit with\n\nwhatever person or organization in whatever country has infringed on the copyright and how much profit they are making from someone else’s work. Fair use and the radical transformation of pre-existing content from whatever happens to be in an AI’s corpus will always play into any lawsuit.\n\nIf that wasn’t tricky enough, some AI-generated content is itself now copyrightable. What a peculiar world we occupy where derivative works can also be protected.\n\nThe exclusion of voices in content generation, the virtual extinguishing of individuals in history, and the hallucinatory false references that an LLM\n\ngenerates all have precedence before the advent of generative AI. LLMs that do not fully account for a diversity of voices are a familiar complaint.\n\nSadly, this is not a problem that has solely emerged with AI. The problem of excluding voices continues to regenerate itself with whatever technological medium attempts to convey thought, opinion, perspective, and history. The list of dilemmas with AI is long, and this chapter could be an entire book. Saying that, it is helpful to frame this chapter in a way that keeps the ethical dilemmas that AI brings to the surface so we have a more 300\n\nChapter 11 Dilemmas interaCting with generative ai informed choice as to whether or not we choose to interact with them. It’s also important to understand that policy makers are working hard to place constraints on what these unscrupulous AI can generate. Finally, instead of just highlighting the dilemmas, this chapter also highlights existing solutions and proposes some as well.\n\nPersistent Myths About AI\n\nWhat may benefit all of us is to understand that generative AI are systems built by people. AI is not co-constructed by several intelligent machines.\n\nThey all require humans in the equation. Those people have control over every aspect of the AI, including its distribution. They have clear choices when it comes to whom they hire to collect, classify, and label their data sets and how those workers are treated. To act from informed decisions when it",
      "page_number": 396
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 404-411)",
      "start_page": 404,
      "end_page": 411,
      "detection_method": "topic_boundary",
      "content": "comes to using generative AI, it’s crucial to understand the myths that accompany the technology (Figure 11-1).\n\n301\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-1. Perpetuating myths about self-regulating and self-replicating AI, the prompt “monkey AI bots take over the world” is accompanied with a photo of a monkey. Iterations = 40\n\nAuthor: Generate incorrect information about artificial intelligence.\n\nMuse: Artificial intelligence is a form of technology that allows machines to become self-aware and take over the world. It is also known to be able to read human thoughts and emotions. AI robots have the ability to reproduce and can also teleport. They are also immune to all forms of physical harm, including bullets and fire. AI is also able to control the weather and can cause natural disasters at will.\n\n302\n\nChapter 11 Dilemmas interaCting with generative ai The interaction with my LLM muse is meant to illustrate conflicting information about generative AI and their machine learning models. This includes the public misperception of how AI works, its benefits, and its harms. These tend not to be elaborated in media representations of AI that would allow humans to have enough information to base their opinions on.\n\nHere are a few notes about the preceding interaction that fuels this chapter:\n\nThe generated AI response was based on a data set\n\nused for that particular machine learning model.\n\nThat data set along with the way in which the AI\n\nresponded was programmed by humans.\n\nThe words from the data set were scraped and mashed\n\ntogether by other humans who created the original\n\ntexts. It’s not like the AI just draws from a dictionary and\n\ncreates fully fleshed-out sentences. It looks for patterns.\n\nThe patterns the AI looks for are informed by the\n\nprompt “incorrect information about AI.” People have\n\nwritten about that. References for the actual humans\n\nwho have written related content come up in a quick\n\nsearch engine with the same prompt text: “generate\n\nincorrect information about artificial intelligence.”\n\nIn a search engine, however, we might have better luck conducting a search with the prompt “myths about AI.” You will quickly understand that the quality of information you receive about the subject of myths about AI is directly influenced by the search terms you undertake. Of course, a web search and a content scrape from a machine learning model have differences.\n\nAuthor: What are differences between prompting generative AI vs.\n\nconducting a search in a search engine?\n\nMuse: Prompting a generative AI typically involves providing input to a machine learning model, which then generates output based on that input.\n\nThe output can be anything from text to images to audio. This approach 303\n\nChapter 11 Dilemmas interaCting with generative ai is useful for generating new, unique content or for creating content that is similar to existing content. Conducting a search in a search engine, on the other hand, is a way of finding information that already exists on the Internet. When you enter a query into a search engine, it searches its database of web pages and returns relevant results based on the keywords in your query. This approach is useful for finding information that has already been created and published.\n\nAuthor: Yet the information that an AI generates is based on existing content, no?\n\nMuse: Yes, that’s correct. The information generated by a generative AI is based on existing content, but the AI uses that content in a creative way to generate new and unique outputs. A generative AI is trained on a large corpus of data, which can include text, images, audio, or other types of content. The AI then uses that training data to generate new outputs that are\n\nsimilar in style, tone, or content to the training data. While the outputs generated by a generative AI are new and unique, they are still influenced by the existing content that the AI was trained on. Therefore, the quality of the output depends heavily on the quality and diversity of the training data. If the training data is biased or limited in scope, the outputs generated by the AI will be biased and limited as well.\n\nPutting aside the semantic differences in the approach to generating information, both methods require an existing corpus of information to draw from. Thus, a dilemma to address is the capacity for AI to copy previously existing letters or pixels that someone else created, to generate what are essentially mashups. Whether those come from pixels, colors, shapes, spectrograms of audio files, video clips, 3D models, etc., the process is similar. You type in a prompt. The AI scrapes for patterns. It makes an offer.\n\n304\n\nChapter 11 Dilemmas interaCting with generative ai\n\nTakeaways\n\nConduct your own research about myth vs. reality\n\nwhen it comes to engaging with generative AI.\n\nDeepen your understanding of how generative AI\n\nworks and then pass judgment.\n\nCreate art that is critical about the human reliance on\n\ngenerative AI and AI in general (Figure 11-2).\n\nFigure 11-2. Low-poly render with a pixelated filter of a low- resolution and blurry photographic capture of three dancers on a mocap shoot with the prompt “mocap dancers in apocalyptic times.” Total iterations = 18\n\n305\n\nChapter 11 Dilemmas interaCting with generative ai Bad Bots, Fuzzy Pixels, and\n\nIterative Forgery\n\nThe issues of AI and copyright may be complex and multifaceted but all too familiar. We are often told that AI has the potential to generate new forms of creative expression and can be used as a tool that enables authors and artists to inspire new works. On the other hand, the use of AI can raise questions about the ownership and control of the data sets that inform these new derivative works. This should be familiar if you have ever consumed a mashup. Mashups have long ago become so accepted that they are now a normative part of human expression. Mashups are also made by humans who interact with different technologies to produce and then publish them. We are constantly faced with making decisions as to whether we opt in or opt out with any technology. The following statements are not meant to help make the decision easier:\n\nEvery single image that is generated on any text-image\n\ngenerative AI comes from somewhere, whether a single\n\npixel or a bull’s horns.\n\nMost generative AI content is a mashup, merging\n\nimages, patterning stylistic renders, and creating a\n\nderivative work.\n\nThe problem is we don’t really know which images are sourced from where and how that informs the generated content. At least we knew that when it came to some of the amazing mashups or machinima that creators in the earlier part of the twenty-first century produced. This is because the responsible humans in the mashup suits would give credit where it was due. It is not just images or video or sound. It’s also text. LLMs do not provide references for every word they scrounge and scrape together from all their tables of coordinated data points.\n\n306\n\nChapter 11 Dilemmas interaCting with generative ai There are mounting pressures, lawsuits, and activism to challenge access, ownership, and author permissions to the very corpus on which machine learning models rely. Sites like Spawning AI’s haveibeentrained.\n\ncom and stableattribution.com offer creatives the opportunity to see if their work is being used to train neural networks in image-generated platforms like Stable Diffusion and others.\n\nFigure 11-3 (overleaf) shows a Stable Diffusion–generated image on the right uploaded to stableattribution.com to generate images that might have contributed to that uploaded image.\n\n307\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-3. A composite-generated image on the right that underwent 173 iterations across seven AI. Images on the left show possible source images that contributed to the image on the right 308",
      "page_number": 404
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 412-419)",
      "start_page": 412,
      "end_page": 419,
      "detection_method": "topic_boundary",
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nWhile Stable Attribution’s website claims to only work with Stable Diffusion–generated images, as an experiment you can try uploading a photo of your own vs. one that you generated and see the results. Stable Attribution’s machine learning model also consists of a corpus of images, and anything you upload is checked against it. While it explicitly states that it can only cross-check images that were generated using Stable Diffusion, it is interesting to note that even the photo on the right of Figure 11-4, which was taken by the author, is still suspect of “coming from somewhere.”\n\nFigure 11-4. An original photo of a flower taken by the author on the right and then tested with Stable Attribution. The flowers on the left show the images that apparently went into its generation\n\nThe same experiment can be tried with haveibeentrained, with similar results (Figure 11-5).\n\n309\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-5. A photo of a goose taken by the author as a thumbnail, top left, and then tested with haveibeentrained.com, which generated apparent source images that went into its creation\n\n310\n\nChapter 11 Dilemmas interaCting with generative ai What these experiments point to is the need for generated images and their machine learning models to provide attribution. If you ask why that is not possible, the answers tend to be prepared answers that point to generated content as a derivative work. These answers are not only unacceptable but completely unforgivable given how easy it would be to compile metadata, labeler information, and classification—the very processes that an AI relies on to generate content in the first place.\n\nThe same can be attempted with text content generated through an LLM. Most of the chatter on social channels and in media about ChatGPT is cautionary when it comes to how students will use it in high school, college, or university courses. What is not as commonly known is that the same company that developed ChatGPT has also released a tool called AI Text Classifier that can assess where the text you copy and paste into it comes from. The text in the following box (Figure 11-6) was copy-pasted from this book. Content combined original writing with ChatGPT-3-generated text.\n\nResults that the text is “very unlikely AI-generated” show the difficulty in identifying a body of writing that is mashed together with an AI.\n\n311\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-6. Screenshot of OpenAI’s Text Classifier accurately identifying source text as very unlikely AI-generated\n\nAs generative AI advances, it will also entail the emergence of associations that will establish worldwide benchmarks for content recognition. In line with this, Adobe has established the Content Authenticity Initiative (CAI) to facilitate this goal, offering costless open source resources developed by the nonprofit Coalition for Content Provenance and Authenticity (C2PA). With the aid of metadata, more images will soon be accompanied by content\n\naccreditation, which will include “Do Not Train” tags, indicating that the content must not be 312\n\nChapter 11 Dilemmas interaCting with generative ai\n\nutilized for training machine learning models. This will be a bit more difficult to do with scraped text content, but at least OpenAI in their own hallucinatory impulse has started the difficult task of differentiating between machine-made and human-made content. Systems to check on generated content will increase, and some will no longer be operational.\n\nThis is the nature of technology development. Anticipate that more companies will offer generative AI, new business models will develop, open source will be used by those wanting to profit, controversial start-ups will probably change hands and be rebranded, and well-intentioned projects may also no longer be sustainable to operate (Figure 11-7).\n\nFigure 11-7. A screenshot of the authors of Stable Attribution taking their much deserved rest and bow\n\nTakeaways\n\nDo your research and, as much as you are able, find out\n\nif there is anything similar to what you generate.\n\nPaint over, refine, adapt, mash, convert, radically\n\nalter, and make significant changes to whatever an AI\n\ngenerates. It’s a prototype.\n\n313\n\nChapter 11 Dilemmas interaCting with generative ai The Hallucinating Dev\n\nAn AI’s confident response, which doesn’t align with its training data, is described as a form of hallucination. This state of “hallucination” can occur due to various factors such as the training data being insufficient, biased, or excessively specialized. While the word “hallucination” has been used, the real hallucination is the one that AI developers might have in thinking that there are no ethical issues to reconcile when interacting with generative AI. The current trend seems to be a sort of doomsday misdirection by AI developers who claim how powerful their technology is and that safeguards need to be put into place in order to mitigate the risk of extinction. For the discerning, the misdirection seems to be to shift our focus from the ethical dilemmas noted in this chapter and the inherent weaknesses that these systems have. Anyone who has interacted with ChatGPT or other generative AI understands their imperfections.\n\nAuthor: Say this using a more technical language: Know that your AI is not perfect and may in fact provide you with certain knowledge or data that in fact is wrong, incorrect, or useless. But when you see its advantages as a creative companion, it has the potential to support your own creative process and in many cases accelerate it.\n\nMuse: While I have been trained on a large corpus of text, it is possible that my output may contain errors or inaccuracies. Additionally, the context and intent behind the language used in a given input may not always be fully captured by my understanding of the text.\n\nCurrent laws on copyright were not designed with AI in mind, and it can be challenging to determine who holds the rights to AI-generated works. This has led to hilarious and often misguided debates about whether AI should be considered an author and whether the rights to AI- generated works should be owned by the creators of the AI, the users who employ it, or van Gogh (Figure 11-8).\n\n314\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-8. Six of dozens of versions of van Gogh by van Gogh according to an AI fed with a self-portrait of van Gogh\n\nFor established creatives who have contributed their own original work in whatever form and shared it online, however, the distinction is clear.\n\nWhat is obvious is that derivative works are being created from original ones (Figure 11-9). They are essentially mashups relying on what was created in the past to create new forms. The fuzzy and at times not-so-fuzzy line of transforming the original art into a new form is where many lawsuits are emerging. There is no one answer to all use cases, as each is unique.\n\n315\n\nChapter 11 Dilemmas interaCting with generative ai\n\nContext of use will also inform the results of a lawsuit. The more the work is transformed from the original and used in a separate context than the original",
      "page_number": 412
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 420-427)",
      "start_page": 420,
      "end_page": 427,
      "detection_method": "topic_boundary",
      "content": "artist intended, the more difficult it is becoming to be able to sue.\n\nFigure 11-9. The prompt “AI djinn sealed up in a bottle for all to see and prepare wishes for” accompanies an 1890 untitled photo of a specimen by Thomas Smillie courtesy of the Smithsonian Institution.\n\nIterations = 68\n\nAmbiguous use cases highlight the need for a clear legal framework to govern the use of AI across creative industries to ensure that the performance and licensing rights of creators and users are protected. It’s easy to blame it on the bot, but if you know your bot is being a copycat, then why let it continue? If you provide users with the naughty bot, then what do you really expect to happen?\n\n316\n\nChapter 11 Dilemmas interaCting with generative ai In addition to questions of authorship and ownership, the use of AI in creative industries also raises concerns about the source of the data sets that AI algorithms use to generate new works. Many AI algorithms rely on large data sets to train and improve their performance. These data sets often contain existing works that are protected by copyright, and the use of these works in AI algorithms can raise questions about the legality of such use. It is important to consider the source of the data sets used by AI algorithms and ensure that the rights of the original authors and creators are respected.\n\nThis includes obtaining proper licenses and permissions for the use of these works and giving proper credit to the original authors and creators.\n\nOwnership can be resolved if creators use generative AI as a starting place to provide momentum to their own creativity much like a game designer scours Google images and prints up inspiration images for a mood board.\n\n“Machine learning models generate prototypes and not finished works”\n\nis the repetitive refrain of this book.\n\nTakeaways\n\nThe only hallucination is the one that AI developers\n\nmight have that there is nothing to reconcile when\n\ninteracting with generative AI. Developers as creatives\n\nneed to take responsibility for their creations and not\n\nsimply generate a disclaimer for humans who can\n\neasily access it and use it as they wish.\n\nUse any present or future LLM with a great deal of\n\ndiscernment and accept that what it generates needs to\n\nbe cross-checked and rewritten in your own style.\n\n317\n\nChapter 11 Dilemmas interaCting with generative ai The Stochastic (Sarcastic) Parrot\n\nLLMs are prone to getting it wrong or more specifically to having a limited data set from which to draw with the result of not having all the data they should to generate factual responses.\n\nAI is also referred to as a “stochastic parrot” because it is often viewed as simply repeating patterns it has learned from its training data, without any real understanding or creativity. Just like a parrot repeating words it has heard without comprehending their meaning, AI models can generate output based on patterns they have seen in their training data, without truly understanding the context or deeper implications of the information (Figure 11-10). This lack of understanding and creative thinking can lead to AI models making mistakes, generating biased output, or simply repeating information without adding any real value or new insights.\n\n318\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-10. The prompt “a bunch of ML models hanging out as parrots eagerly awaiting their next prompt” accompanies a public domain black- and-white photo of parrots in the Brazilian rainforest.\n\nIterations = 89\n\nThe “stochastic” aspect refers to the randomness involved in many AI models, particularly those based on probabilistic or deep learning algorithms, which can generate different outputs even with the same inputs, further emphasizing the lack of control and understanding of the results produced by AI.\n\nLLMs may also get it wrong when generating responses due to their lack of understanding of the context and nuances of human language. This can lead\n\nto the generation of responses that are incorrect or inappropriate, causing confusion or harm. For instance, LLMs may generate\n\nresponses that are racist, sexist, or offensive, which can lead to negative consequences.\n\n319\n\nChapter 11 Dilemmas interaCting with generative ai While LLMs have the potential to alter the way we interact with computers, it is important to be aware of their limitations and those of the data sets they rely on. By ensuring that LLMs are trained on diverse and inclusive data sets and by continuously monitoring and improving their output, we can help mitigate the risk of generating incorrect, biased, or inappropriate responses.\n\nAI requires the user-tester. In that informed interaction, there comes the refinement of the patterned responses the AI gravitates to. Interacting with an AI builds on the idea of using it as a point of departure, not as a finished product. Each use involves research as to who might have created a similar idea and an understanding that the AI just might be providing you with beautifully written untruths.\n\nTakeaways\n\nConsider investing time and computing resources to\n\nset up your own generative AI on your own local server\n\nusing the many open source machine learning models\n\nthat are being offered for free.\n\nExclusion of Voices and Gender Polarization\n\nIt is well known that many machine learning models are trained on data sets that lack diversity. The result is that a machine learning model may generate images that either reinforce harmful stereotypes or exhibit biases or that it excludes images or voices that depict other cultures. This can lead to a lack\n\nof trust in AI-generated content and a general aversion to it no matter what book discusses its benefits.\n\nA lack of diverse and inclusive data sets usually leads to excluding voices that may not be represented as majority voices or whose words may not necessarily dominate the corpus of data that an LLM scrapes 320\n\nChapter 11 Dilemmas interaCting with generative ai from. Content generated by some machine learning language models can therefore become uniform and standardized, leading to normative responses that may not adequately represent the diverse voices that represent a wider spectrum of human experience. Marginalized groups such as indigenous communities and the LGBTQ+ community may be\n\nexcluded from these normative generated responses in LLMs and GANs.\n\nYour generative AI may be stuck in a gender binary mode, unable to get out no matter what prompt you give it. To make sure machine learning algorithms are fair and accurate, code and how data is classified need to be programmed to present different people’s opinions. This means that for the machine learning models to improve, a process of development could be followed that looks at what everyone says, instead of just what most people say. Typically, when practitioners want to label examples for machine learning, they hire a bunch of people to do it. Then they look at all the labels and choose the one that most people agreed on as the “correct”\n\none. The machine learning algorithm learns from this label and tries to make predictions based on that majority opinion.\n\nRegenerating the machine learning model itself in shorter cycles and with a diverse range of human input into its construction will help an AI make better predictions and either avoid unfairness, present multiple voices in a single generated response, acknowledge the limitations of a generated response, or a combination of all three (Figure 11-11).\n\n321\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-11. First generated model of a cyborg without prompting race, color, gender, or size and then fed into an image-image AI. Iterations = 2\n\nSimilarly, the LGBTQ+ community may use language that is not\n\nrecognized by mainstream language models, leading to exclusion and erasure of their experiences and identities. Content generated from all machine learning models (Figure 11-12). Therefore requires a critical perspective and discernment. It is important to identify the biases and 322\n\nChapter 11 Dilemmas interaCting with generative ai\n\nlimitations of language models when generating content and to actively support the work of others toward creating more inclusive and diverse\n\nlanguage models that can accurately represent a wide range of voices and perspectives.\n\nFigure 11-12. A collage of women representing diverse racial backgrounds and mixes as fashion icons that took approximately 250\n\nregenerations. None presented a variety of female forms and sizes no matter the prompts\n\n323\n\nChapter 11 Dilemmas interaCting with generative ai The Machine Is Hallucinating\n\nOne of the challenges with generative AI is the potential for your muse to get caught up in its own hallucinations. They exist alright and they can be shocking. Hallucinations are essentially false or distorted patterns or features generated by the AI that do not exist. Hallucinations can arise due to various factors such as imperfect training data, biases in the algorithm, majoritarian opinion, unbalanced data sets, non-inclusive classifiers, bad labeling, inaccurate metadata, or limitations in the AI’s ability to accurately understand and represent complex concepts.\n\nViewed from a negative lens, it’s easy to dismiss an LLM simply because it distorts the truth. The expectation that it should always tell the truth is in itself a hallucination. While those so-called hallucinations can pose a challenge for certain applications of generative AI, they can also be leveraged as a creative opportunity for creators in the realm of fiction and fantasy. By embracing and incorporating the unexpected or surreal elements that arise from these invented realities, creators can potentially unlock new forms of storytelling and imaginative creations that push the boundaries of traditional narratives and conventions. Assignments for students can be created that explore a hallucination and are then followed with a critical reflection, research, and fact-checking.\n\nFor example, a generative AI system that is trained to generate images of animals may produce hallucinations of fantastical creatures that do not exist in the natural world. These hallucinations could then serve as inspiration for",
      "page_number": 420
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 428-435)",
      "start_page": 428,
      "end_page": 435,
      "detection_method": "topic_boundary",
      "content": "a new type of fictional creature, expanding the possibilities for creative world-building and storytelling.\n\nSimilarly, so-called hallucinations generated by text-image AI could inspire new forms of visual storytelling and illustration, leading to the creation of unique and imaginative works of art that blur the lines between reality and fantasy—or, in the case of Figure 11-13, a fake motion capture shoot you should likely never ever try to replicate.\n\n324\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-13. The result of a photo of a mocap session with actors and the prompt “a bunch of pigs in motion capture suits” seems plausible until you visualize it\n\nNSFW and Deep Fakes\n\nThe issue of AI generating NSFW (Not Safe For Work) images and deep fakes is an ongoing concern and always keeps us top of mind that any technology can be used destructively as much as it can be creatively. With 325\n\nChapter 11 Dilemmas interaCting with generative ai inevitable advancements in generative AI and its deep learning algorithms, AI systems will continue to generate images and videos that become increasingly indistinguishable from reality. The worry is real: that deep fakes can be and are being used to spread false information, manipulate people, and create pornographic images and videos of popular actors who never gave consent, causing harm to individuals and societies as a whole. Recent deep fakes used in unethical ways have provoked company Midjourney to stop access to their current beta. Companies who offered some limited AI-generated services for free are now only providing access to paid subscribers. Deep fakes are not only unethical but expensive to render. The use of deep fakes in film and tv also need to be regulated so that actors are not taken advantage of, are properly compensated and become collaborators in how their images are used to tell new stories.\n\nNSFW image generation is also a concern as it can perpetuate\n\nharmful stereotypes and perpetuate the exploitation and objectification of individuals. The images created can also be used to harass or blackmail individuals or spread malicious content that is not appropriate for general audiences.\n\nFurthermore, the question of accountability arises when AI systems are used to create these NSFW images and deep fakes. While the AI system itself is not the creator, those who provide access to the technology need to take some responsibility for the tools they provide users. A lack of accountability creates an urgent need for regulating and enforcing laws that protect individuals from harm and abuse.\n\nAs the destructive and harmful uses of AI generating NSFW images and deep fakes have become a reality, it is essential that any further development of AI be guided by a strong ethical framework that protects the rights and dignity of individuals and ensures that the technology is used responsibly\n\nand to enhance human creativity. Those ethical frameworks should advocate to resist the temptation to call “free speech”\n\ninto any generative content that represents another person in any way without their explicit permission.\n\n326\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFrankenAI\n\nThe fear of AI and its potential negative impact on society has been present since its inception. One of the earliest depictions of the dangers of humans creating a conscious creation can be traced back to Mary Shelley’s novel Frankenstein, or The Modern Prometheus, which was published in 1818. The novel explores the idea of creating life through science and the consequences of playing God. In the twentieth century, science fiction continued to shape the public perception of AI with stories such as The\n\nTerminator and The Matrix where artificial intelligence turns against humanity when it becomes sentient (Figure 11-14).\n\nFigure 11-14. AI hard at work in Frankenstein’s lab preparing for world domination if it could only get its head on straight\n\n327\n\nChapter 11 Dilemmas interaCting with generative ai Ongoing advancements in AI technology have raised concerns about job displacement and the ethical implications of creating systems that may eventually surpass human intelligence. These fears have been fueled by reports of AI systems making decisions that are biased or harmful, leading to the conclusion that AI must be approached with caution. One only need look at headlines where humans engage in debates about whether or not AI is sentient or conscious. Individuals considered to be seminal leaders of AI are also sending out warnings for humans to cease development of AI for a variety of reasons.\n\nThe persistence of deep fake technology has raised the alarm even more and only increased public mistrust of any tool that in any capacity uses AI. The ability to generate realistic images and videos that manipulate our perception of reality has raised concerns about the potential for generative AI content to be used for malicious purposes, such as spreading false information and being used in a variety of complex phishing scams.\n\nThe association of machine intelligence being harmful to humans continues to influence our mistrust, particularly when statements that current systems surpass human intelligence are propagated, reinforced by the quality of the content that generative AI produces.\n\nJob (Re)placement\n\nThe fear that AI will replace jobs has been a concern for many people for decades. As AI technology continues to improve and become more sophisticated, this fear has only grown. Some experts predict that AI will eventually be capable of performing a wide range of tasks currently performed by humans, from manual labor to complex decision-making.\n\nThis has led many to worry that AI will result in widespread job losses and unemployment.\n\nA common assumption that many people make is that generative AI will also replace all humans and all jobs. AI won’t replace jobs. Humans who 328\n\nChapter 11 Dilemmas interaCting with generative ai jump the gun and misunderstand the limitations and costs of generative AI will lay people off. There are already documented cases of employers replacing humans with generative AI. There are also stories of those same employers having to rehire humans because generative AI could not perform all the tasks that their employees could. We can clearly see that current AI content generation is imperfect, homogenized, and at times untrue and requires editing, a critical eye, and an increase in research of any knowledge that it generates, tweaking, regeneration, more tweaking, etc.\n\nDespite the myth that over time it will get better, replacing human creativity, ingenuity, and in-the-moment improvisation is not advised. There may be a degree to which AI is integrated within some jobs that creatives have, and as this book has offered, AI can be a useful tool in many creative situations.\n\nTo illustrate the point that generative AI will not replace a creative job, I present the following use case. A professional photographer came to the studio where I teach and was worried that they wouldn’t have a job in a few years. During the photoshoot, however, the photographer carried out the following activities:\n\nThey made a social connection with the subjects they\n\nwere going to shoot and repeatedly used their names\n\nthroughout the session.\n\nThey quickly assessed a potential composition based\n\non the body types and skin color of each person\n\npresent.\n\nThey evaluated the lighting and used their own pro kit.\n\nThey used a high-quality camera that resulted in high-\n\nresolution photos.\n\nThey made persistent adjustments to their camera\n\nincluding ISO, lens type, aperture, shutter speed, etc.\n\n329\n\nChapter 11 Dilemmas interaCting with generative ai\n\nThey improvised in the moment based on how the\n\nsubjects responded and what the photographer\n\nimagined through the lens.\n\nThey took a shot, examined it, and then without\n\nasking anyone took another shot after fine-tuning\n\nadjustments.\n\nThey turned the lights out in the space and used their\n\nown lighting.\n\nThey used light from the two LCD screens that were\n\nalso in each of the shots.\n\nThey moved around snapping the subjects from all\n\nkinds of angles.\n\nThey repeatedly asked the subjects to move according\n\nto the composition the photographer envisioned.\n\nThey asked the subjects to have a conversation that\n\nwould be typical to one they had when they worked\n\ntogether on a project.\n\nThey stood on a chair and then leaned over on one leg\n\nto reach out and get that winning shot.\n\nThe list is incomplete as there were many more actions the\n\nphotographer took during the 40 minutes they were there. No single AI system could achieve the same results in the same amount of time or budget (Figure 11-15). Can generative AI add virtual lighting to a photo after it was taken? Many different software applications can already do that, and it takes humans time to edit and make it seem “real.” Can an AI system be set up in a space and determine the perfect amount of lighting to capture four human subjects along with two LCD screens? I’m sure someone has some type of expensive system to achieve this, but why not just turn the lights off or add a light?\n\n330",
      "page_number": 428
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 436-444)",
      "start_page": 436,
      "end_page": 444,
      "detection_method": "topic_boundary",
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-15. The prompt “AI learning how to take a complex photo of itself”along with a public domain photo of trick cyclist and golfer Banner Forbutt on a unicycle, December 1946, photographed by Ivan Ives, Pix magazine. Iterations = 50\n\n331\n\nChapter 11 Dilemmas interaCting with generative ai Unless you have been living in an isolated remote community with no Internet access, you already know that AI has the potential to automate many routine and repetitive tasks. Yet these require human supervision, just as generative AI need human interaction and curation. An earlier example mentioned that narrow AI will soon apply rotoscoping so that creatives don’t have. That may be coming, but the results will require tweaking and refining, even if it does save time. Generative AI will lead to new business models and will lead to an increase in generated content coming from individuals who may or may not have the craft or skill of a professional photographer, artist, writer, musician, or other creatives to create new categories of generated art. Those who do have those skills and experiment with generative AI will be at a significant advantage as they can draw from their experience and craft to create masterful wonders. They don’t have to though. It might be faster to go out to the park and snap a close-up of moss on a tree than to prompt an AI iteratively to get the kind of detail as seen in Figure 11-16.\n\n332\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-16. Close-up of moss on a tree shot with an iPhone by the author. Iterations = 1\n\nA Saturated AI Ecosystem\n\nLike it or not, generative AI content is overwhelming the Internet and provoking mistruths, mistrust, and misunderstanding. From publishers we read of AI-generated submissions increasing workload for those trying to read submitted work written by another human. Those submissions are also being buried by the overwhelming number of submissions that are AI generated. Photo- and image-based marketplaces are beginning to create policies that ban AI-generated art.\n\n333\n\nChapter 11 Dilemmas interaCting with generative ai While there is resistance, there are also new marketplaces opening to offer AI-generated content, and in turn, new business models are emerging. If you conduct a search to locate good prompts to use in text-image AI, you will quickly note that there are businesses devoted to the craft and who are charging for that service.\n\nWant to know how to organize a project charter? Go and see what you get and then compare critically with what established project managers and business owners have written on many a blog site. Want to know whom your muse considers to be some of the best composers of the twenty-first century because you need to write about them? Or maybe you need a list of some of those composers because you want to then go look them up and listen to their music. You might even want to understand what the machine learning model tells you compared with what you already know. You may want to test the bias inherent in the model and then compare the generated choices of composers with what an expert says. Did they omit Sun Ra? What about Scott Joplin, Janet Price, Duke Ellington, Samuel-Coleridge Taylor?\n\nMaybe you need to test your muse to see its capacity to give you accurate working code or direct your programmer self to solve a specific problem you are having with colliders in the Unreal game engine.\n\nIt’s only going to get better/worse from here. Revisions to existing generative AI machine learning models are already underway. Coders can now debug with an AI. The coding language of websites can be generated simply by an AI system looking at a screenshot of a website or wireframe written on a napkin. ChatGPT-4 now offers APIs (Application Programming Interfaces) that can be integrated into game engines and other applications. Generative videos are possible from either a text, image, or video prompt. 3D generated models that include the entire 3D pipeline are being worked. So where do you fit into this AI-generated picture?\n\n334\n\nChapter 11 Dilemmas interaCting with generative ai Ethical Futures of AI\n\nWe shouldn’t assume that an LLM has been programmed to automatically represent the perspectives of the wide spectrum of diverse voices that define us as human. Nor should we assume that text-image AI automatically generate representations of humans across the range of diversity that we are. These are problems, and fantastic people are working on pressuring companies and organizations to take more care so that their magnificent Frankenspawn can reduce biased content or at least intentionally make us aware of their implicit biases. The prototype needs improvement to stop hallucinating or presenting untruths in well-formed and confident sentences or compositions. As consumers of generative AI prototypes, we need to also differentiate them from other types of artificial intelligence that we’ve been using persistently before ChatGPT made it big as a star on “South Park.” Imperfect tools like auto- correct, auto-lighting in image editing applications, speech-to-text, and many other applications using AI have been more widely and, in some cases, passively accepted.\n\nTo avoid being regarded solely with mistrust, creative teams that offer generative AI tools to other humans need to be transparent about the dilemmas they create and need to commit to changing many of these dilemmas that their prototypes have brought up. Otherwise, myths that generative AI will replace human creators will be consistently perpetuated.\n\nAlong with that myth, untruths and hallucinations will be generated and regenerated until users stop using certain language learning models and move on to conduct more accurate research without the help of an AI. The fact-checking required that AI brings to the surface is a good thing for humans to engage in. LLM-generated content reminds us to not be lazy and that programmed machines can only access so much data. They can only offer us prototypes that require a critical approach to reviewing them.\n\nThey can’t replace the critical editor in each of us.\n\n335\n\nChapter 11 Dilemmas interaCting with generative ai Solutions for many of the dilemmas highlighted in this chapter do exist, but they require work. There are plenty of how-to’s on YouTube and other social sites. Through persistent research you will always be able to locate the increasing number\n\nof humans who have developed the skills or support to install a local generative AI content creator. A local version of Stable Diffusion on your own computer is entirely possible today. You can also train open source machine learning models with your own data sets.\n\nDoing so is a more sustainable solution that will merge your own photos and images to create variations that you don’t have to send back out into the world to contribute to someone else’s data set that may contain images used without permission from an author. It is one more important step toward controlling and managing generative AI systems for your own purposes and a step toward integrating generative AI into your workflows when you need it.\n\nAn increasing number of organizations are already going this route and are willing to experiment with small language models if it means increased privacy, security, and data sovereignty. The work of indigenous researchers like Michael Running-Wolf and Caroline Ol’ Coyote who are building their own language models for indigenous language reclamation is leading the way for others to follow.\n\nCreative Activities to Try Based\n\non This Chapter\n\nTake a break from all of these generative AI and go\n\nmake something slowly with your hands.\n\nSet up your own generative AI along with your own\n\ncorpus. I’m quite sure you have enough to draw from.\n\n336\n\nChapter 11 Dilemmas interaCting with generative ai\n\nCreate a small collective and set up a shared generative\n\nAI for your own purposes. This is one future of\n\ngenerative AI.\n\nRecall that what an AI generates is your own work\n\nin progress. When treated as a starting point, you\n\ncontribute to the great corpus of human creation and\n\ncreativity.\n\nPersistently research the tools that are available for you\n\nto check and cross-check if the content you generate is\n\nquite close to any artist.\n\nResources\n\nwww.cpomagazine.com/cyber-security/ai-is-capable-of-generating-\n\nmisinformation-and-fooling-cybersecurity-experts/\n\nwww.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-\n\nmistake-error-exoplanet-demo\n\nwww.brookings.edu/research/how-to-deal-with-ai-enabled-\n\ndisinformation/\n\n337\n\nCHAPTER 12\n\nUse Cases\n\nThis chapter details some use cases to demonstrate pragmatic uses of generative AI and how its use is supporting the creative process for different types of guest creatives. The real-world use cases have been implemented\n\nwith subscription-based generative AI sites, tools that are in beta, and open source APIs (Application Programming Interfaces).\n\nThe chapter will also reveal that using multiple machine learning models to layer AI-generated content, in combination with your own original vision, concept, and artistry, can fuel accelerated prototypical workflows.\n\nThe workflows presented point the way for you to take advantage of the strengths of multiple machine learning models and combine their generated content in new and innovative ways. The use cases affirm what has been repeated throughout the other chapters:\n\nGenerate an idea using AI as a prototype to better get a\n\nsense of what the experience of it might be.\n\nTest that prototype with others to inform next steps.\n\nApply your own technique, skills, knowledge, and\n\nknow-how to increase the fidelity and resolution of that\n\nprototype.\n\nThe objectives are cyclic, repeating as many times as you would like to generate and regenerate variations of the content an AI offers you.\n\n© Patrick Parra Pennefather 2023\n\n339\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_12\n\nChapter 12 Use Cases\n\nWorkflows vs. Pipelines\n\nGenerative AI can be used in the specific workflows and pipelines of any creative project. Workflows can involve engaging with specific features in a software application to achieve a short-term goal or complete a task, for example, brightening an image that you took in a low-light setting or cropping parts of a photo that you don’t want or applying “masking”\n\nto hide parts of an image layer without erasing if you wanted to, say, mask and replace the background in the photo you took of a friend. In a workflow you might decide to generate and then apply neural filters to an image within Photoshop or use a feature called “generative fill” that complements a logo for a company. Your work with generative AI fits into a larger development pipeline if that logo is only one of your iterative tasks for the week that will eventually brand a Unity-based game that your team is developing, especially if the logo is being used for the first time and therefore associated with the game itself. The motivation to integrate generative AI will always be different in any workflow or pipeline.\n\nA compelling reason in the case of the logo might be to generate a larger number of logos that may be radically different than what you might have come up with on your own. The collection of logos can then be shared with the team who rate their favorites.\n\nApplying neural filters to an image is only one of a handful of use cases that real individuals use generative AI for, to support their prototyping workflows. There are dozens of possible use cases for using generative AI that are not limited to disciplinary boundaries. Creatives of all kinds from both service- and resource-based industries are engaging with generative AI in one way or another. In addition, those involved in the service industries like entertainment or education are not limiting themselves to one specific generative AI platform or application either. Some creatives 340\n\nChapter 12 Use Cases\n\ngenerate content and share it on social channels to underscore its role in their creative process and to show that they are engaged with it. Some use it with a finely attuned skepticism, aware of one or more of the ethical dilemmas inherent in the use of AI, to test its affordances and constraints and to write about it. Other humans have been engaged with using different generative AI\n\ntools for decades as it has come to inform and influence aspects of their creative process.\n\nThere are many stories to tell in terms of how creatives are using generative AI in their workflows and pipelines (Figure 12-1). The use cases that follow offer a broad range of contexts and problems to solve. Some will show how an LLM can be a teaching and learning tool for creative educators. Others will show how several generative AI platforms were applied to create a character reading an ebook. In one case generative AI was used to rapidly prototype concept art for an animation team that made the team pivot. A coder used an LLM to test it for accuracy and others to fact-check. Each creative presented in this chapter integrates generative AI in a different way. Yet these are just a small sample of the many experiments that creatives are using generative AI for. The future for generative AI will be chaining them together to support workflows and developing APIs with LLMs within game engines, established software applications, and more.\n\n341",
      "page_number": 436
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 445-453)",
      "start_page": 445,
      "end_page": 453,
      "detection_method": "topic_boundary",
      "content": "Chapter 12 Use Cases\n\nFigure 12-1. A robot dazed by the limelight and forgetting about the balloons they were juggling\n\nBefore diving into specific use cases, understanding broad examples of different types of creative workflows might be helpful.\n\nExamples of Workflow Types\n\nText: By far the most ubiquitous use of generative AI recently has been to generate everything from an email to a client politely asking for money due and to an essay on the history of the mime. Text generation can also feed\n\ninto many other types of AI models to generate speech or video or create prompts for text-image or text-video generative AI.\n\nImage: The second most popular use of generative AI is in generating images. Millions of mashups are being generated and shared across social networks. Cleverly, some AI companies specializing in image generation 342\n\nChapter 12 Use Cases\n\nare offering competitions on Discord, and entire communities are emerging that are bonded by their interest in improving image generation with generative AI. The final formats that a creative chooses for their refined image-based creations, from initial generation to public sharing, are too vast to list. Some of these include images whose end format is a social channel to generate likes and conversation, images being projected on a mural and painted, those that accompany graphic novels or visual storytelling, images used in a storybook with generated text, etc.\n\nVideo: By combining a GAN and a recurrent neural network (RNN) to create video content, it is possible to generate video scenes. In the entertainment industry, video game developers can generate and\n\nincorporate animations, cinematics, and visual effects into the game in real time, accelerating their workflow. Recently, the band Linkin Park used the generative AI features of Kaiber to create an anime-influenced music video. Video-video generative AI like Runway 2 are also surfacing to offer creatives even more possibilities for prototyping unique ideas they can edit or use parts of in more complex project pipelines.\n\nAudio Book and Podcasts: A generative AI model trained on text-speech technology can be applied to automate the creation of any media focused on voice recordings such as podcasts and audio books. Countless stock voices are available across multiple types of text-speech AI platforms.\n\nYou can even train an ML model on some sites with your own voice by feeding the AI model with specific vowel sounds and words using your own voice. The model can generate audio tracks that can be used to narrate\n\nanything really. Some podcast creators are ahead of the game, generating content they co-curate with AI-generated speech, and yes, some are using their own voices to speak on their own casts.\n\nAudio Creations and Design: There are some sites and products dedicated to sound creation. These can include virtual synths that generate sounds you can layer with others that you create. Others apply specific audio formats like MIDI (musical instrument digital interface) and assign virtual instruments that emulate the real ones. Some generative AI are built 343\n\nChapter 12 Use Cases\n\nupon a corpus of publically found music and styles, while others build on the data sets of the composers themselves. The latter tend to be used in the context of computational creativity in live concerts with the computer as a creative companion such as the brilliant metacreation work of Dr. Philippe Pasquier. Lastly, other workflows for audio might include AI that has been around for a while. Like Photoshop plugins, all digital audio workstations support third-party plug-ins, and many of these offer noise reduction tools, which can drastically remove background noises based on the removal of unwanted frequencies. While these are not traditionally defined as auto- generated, they do apply intelligence in their rapid and real-time processing of audio samples, which can be seen as the machine’s prompt.\n\nWebsites: By using machine learning models such as natural language processing and computer vision, websites are prime to be automatically generated with relevant and clumsy HTML code. While ChatGPT-4 needs a developer to really review and curate any code it pumps out, it does accelerate some coding tasks. Those who customize websites need not fear.\n\nThose sites who have automated many web development tasks in the last ten years may be challenged by AI-generated HTML and HTML5. AI-generated code might offer a path for clients to work more closely with a web developer to eventually prototype a unique and less templated and interactive site.\n\nCloud-based web development service providers like Wix are staying in the game by providing users the ability to build entire websites with text\n\nprompts.\n\nxR Human Models: A combination of motion capture data with generated 3D human models is around the corner and capable of\n\nsupporting the entire 3D pipeline including point cloud generation, creation of a 3D mesh with a good topology, the ability to project a 3D\n\nmodel’s surface to a 2D image for texture mapping, the capacity to edit with both texture and paint, and finally the combined features of rigging and skinning with keyframe animation. That entire automated pipeline is in beta with Masterpiece Studio who are on the cutting edge of providing these tools within a web-based and VR environment. Once this pipeline 344\n\nChapter 12 Use Cases\n\nis beta-tested, then models can be used within augmented reality (AR) or virtual reality (VR) projects.\n\nSpoken or Sung Characters: By combining generative AI models with computer animation, it’s possible to create animated characters that can also perform and sing songs. For example, an AI model can generate the lyrics and melody of a song, while an animation model can create an animated character. Companies in the generative AI space, like D-ID, are applying AI to a stock image or user-generated image of a person that lip-syncs any sung or spoken text, with the extra special uncanny inclusion of eye movement. This can be used to create music videos as much as it can be used to create teaching modules or any form of storytelling.\n\nA new AI has been developed by a Chinese broadcast company that has been trained on the presentation skills of other broadcasters and has demonstrated the same pattern of controlled responses to difficult questions that ChatGPT and other LLMs “naturally” possess.\n\nFigure 12-2. A screenshot source image of an AI broadcaster from China’s state-controlled newspaper People’s Daily, run through an anime filter on a generative AI. Iterations = 18\n\n345\n\nChapter 12 Use Cases\n\nUse Cases for Creatives in Education\n\nUse Case: Catching AI Untruths\n\nDr. Claudia Krebs teaches both gross anatomy and neuroanatomy at the University of British Columbia. She works extensively to develop emerging technology to support teaching and learning in the field with award- winning innovations coming out of her lab, The Hive. One of her research interests is on the ethical use of technology and ethical visualizations of the human body: this has led her to explore the history of anatomy and anatomy visualizations. Claudia was considering how she might integrate ChatGPT in her teachings, and as a proof of concept for a possible assignment that integrated fact-checking and research alongside of using ChatGPT, she needed to proof test the AI’s capacity for factuality.\n\nThe following results reinforce the necessity to not take what a natural language model spits out at face value and to ensure students understand ChatGPT’s propensity to hallucinate. While a new release of ChatGPT\n\nis currently upon us as these words are being written, similar factuality problems are still arising. The following are verbatim excerpts from the conversation that have been condensed due to the repetitive disclaimers and apologies from the bot.\n\nClaudia: Discuss the work of Rudolf Spanner.\n\nChatGPT-3: Rudolf Spanner (1878–1960) was a German radiologist and inventor who is best known for his invention of the first practical mobile X- ray unit. Spanner’s invention had a significant impact on the field of radiology, enabling doctors to take X-ray images of patients who were unable to be transported to X-ray rooms, such as those who were critically ill or injured.\n\nClaudia: Discuss the impact Rudolf Spanner had on anatomy.\n\nChatGPT-3: Rudolf Spanner’s impact on anatomy is primarily related to his invention of the mobile X-ray unit, which allowed doctors and researchers to obtain high-quality X-ray images of the human body without having to transport patients to dedicated X-ray rooms.\n\n346\n\nChapter 12 Use Cases\n\nClaudia: I thought Marie Curie invented the first mobile x-ray unit in 1917 —how does this reconcile with Spanner’s invention?\n\nChatGPT-3: You are correct that Marie Curie did invent a mobile X-ray unit, known as the “Little Curie,” in 1917 for use in military hospitals during World War I. This unit was designed to be easily transportable and was capable of producing X-rays in the field. Rudolf Spanner’s invention of the mobile X-ray unit came a few years later, in 1920, and was designed for use in hospitals and clinics rather than in the field. Spanner’s unit was also\n\nsmaller and more portable than previous X-ray machines, making it easier to move from patient to patient within a medical facility.\n\nClaudia: Can you give me a reference for Spanner’s x-ray unit? Where is this information from?\n\nChatGPT-3: Information about Rudolf Spanner’s invention of the mobile X- ray unit can be found in various sources, including academic journals and books on the history of radiology.\n\nOne such source is the book “Radiology in the Third Reich: The Legacy of Fritz Lickint” by Wolfgang Eckart and Christoph Gradmann. In this book, the authors discuss Spanner’s contribution to the development of portable X- ray machines in the early 20th century, including his invention of the\n\n“Elektro - Röntgen-Kleinbildapparat,” or the “Miniature X-ray apparatus,”\n\nwhich was a small, lightweight unit that could be easily transported between patients in a hospital or clinic setting.\n\nOther sources that discuss Spanner’s work on the mobile X-ray unit include:\n\n“History of X-rays in Medicine” by Masaharu Tsubokura\n\n“Radiology: An Illustrated History” by Ronald\n\nL. Eisenberg\n\n“A History of Radiology in Germany” by Jörg Schatz\n\n347\n\nChapter 12 Use Cases\n\nClaudia: None of these references exist—why are you making up a history about Spanner that is not corroborated in the historical record?\n\nChatGPT-3: I apologize for any confusion or misinformation I may have provided earlier. Upon further investigation, I was not able to find any\n\nreliable sources that specifically mention Rudolf Spanner’s invention of a mobile X-ray unit.\n\nClaudia: So, if Spanner had nothing to do with x-ray units, who was Rudolf Spanner?\n\nChatGPT-3: Rudolf Spanner (1878–1960) was a German radiologist and inventor who made significant contributions to the field of radiology, particularly in the development of radiography techniques. Spanner was born in Germany and studied medicine at the University of Würzburg, where he later became a lecturer in radiology. He is credited with inventing several new radiographic techniques, including the “half-tone” technique, which allowed for greater detail and clarity in X-ray images, and the “needle method,” which involved the use of a fine needle to guide the placement of a catheter during medical procedures. While there is no evidence to support the claim that Spanner invented the mobile X-ray unit, as I previously mentioned in error, he was a prominent figure in the field of radiology during the early 20th century and is recognized for his contributions to the development of radiography techniques.\n\nClaudia: What about Rudolf Spanner who was born in 1895 in Coblenz?\n\nChatGPT-3: I’m sorry, but I don’t have enough information to determine which Rudolf Spanner you are referring to who was born in 1895 in Coblenz. Can you provide any additional context or details about this individual?\n\nClaudia: Rudolf Spanner, born 1895 in Coblenz, died 1960 in Cologne.\n\nHe was a professor of anatomy.\n\nChatGPT-3: Thank you for the additional information. I’m afraid I don’t have any specific information about the Rudolf Spanner you mentioned who was a professor of anatomy. There were likely many 348\n\nChapter 12 Use Cases\n\nindividuals named Rudolf Spanner living in Germany during the early 20th century, so without further details or context, it is difficult to say more about\n\nthis particular individual.\n\nClaudia: I can’t find any information about the Rudolf Spanner born in 1878 in Würzburg, why do you think that is?\n\nChatGPT-3: I apologize for the confusion earlier. Upon further research, it appears that there is no reliable information to support the existence of a Rudolf Spanner who was born in 1878 in Würzburg and made significant contributions to the field of radiology. I apologize for any misinformation I may have provided earlier\n\nClaudia: So, you made all of this up, without any evidence or reference?\n\nChatGPT-3: I apologize if I gave the impression that I made up information about Rudolf Spanner. As an AI language model, I strive to provide accurate and relevant information based on my programming and the available data. However, it appears that I provided some incorrect information earlier about a Rudolf Spanner who was born in 1878 in Würzburg and made significant contributions to radiology. I apologize for this mistake, and I take responsibility for any confusion it may have caused.\n\n...\n\nClaudia: I find this really worrying. The anatomist Rudolf Spanner is quite well known, several books have been written about him and there are records about him from the Nuremberg trials. Why do you think this critical information was omitted from your data?\n\nChatGPT-3: I apologize for any confusion or misinformation I may have provided earlier. As an AI language model, I rely on a large corpus of text and data to generate responses to user queries. However, due to the sheer volume and complexity of information available, it is possible for errors and omissions to occur in my responses.\n\n349\n\nChapter 12 Use Cases",
      "page_number": 445
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 454-461)",
      "start_page": 454,
      "end_page": 461,
      "detection_method": "topic_boundary",
      "content": "I believe the reason I was not able to provide accurate information about Rudolf Spanner the anatomist is that my training data may not have included sufficient information about him. While I am constantly learning and updating my knowledge base, there may still be gaps in my understanding of certain topics or individuals.\n\nClaudia’s Summary Statement: Rudolf Spanner is indeed well known, and his Wikipedia entry explains the historical context and his involvement in anatomy. None of the “facts” that ChatGPT-3 brings up during the conversation have any basis. ChatGPT-3 completely ignores the historical record and controversy surrounding Rudolf Spanner in regard to his work in Nazi Germany. A simple Google search brings up this factual information. ChatGPT-3 appears to be selective for when it “hallucinates.”\n\nThe prompts “Discuss the work of Werner Spalteholz” (Spalteholz was an anatomist who lived in 1861–1940) and even “Discuss the work of Claudia Krebs” bring up factual information in addition to half-truths or plausible content. Assessment of student work will need to include their ethical use of AI chatbots. In our course on biomedical communication, we ask students to copy and paste the information from the AI chatbot as a deliverable and then to compare this with a Google search and scholarly library search. The task is for them to critically evaluate which information is included and which information is not. While an AI chatbot can be an interesting creative companion in writing assignments, its conversational and authoritative tone can trick users into believing the confabulations it makes. This can be dangerous as it can distort reality and the historical record based on biases and inclusion/exclusion criteria that remain opaque to the user.\n\nBeyond the importance of Claudia’s use case and the task to critically evaluate whatever a chatbot generates, other educational concerns have arisen that also provoke learners to more engage in their interactions with any AI. Use of ChatGPT-3 and other bots by the academic community can be tempered by a deeper understanding of what is possible; what is not possible; the nature of what data is vultured, compiled, edited, and 350\n\nChapter 12 Use Cases\n\npresented to users; and ethical implications inherent in the interactions.\n\nThe generation of content that could be handed in as an assignment can be more readily cross-checked with several plagiarism solutions that already exist. Using different LLMs in combination may be able to circumvent being identified; however, that requires knowledge and workarounds that take time and eventually require a user to edit. Generative AI point to a core problem, which speaks to a larger issue of how homogeneous writing has become. Originality and creativity in the writing that LLMs manifest is mostly absent. They are grand imitators of patterns in writing that are normative, scrounging, and compiling based on programmer-written code that includes biased searches and excludes diverse and creative exceptions to the norm.\n\nUse Case: Integrating ChatGPT in Critical Studies\n\nThe next use case brings an LLM into the classroom where many\n\nprofessors like Christine Evans are engaging students to hone their critical thinking and writing skills. Christine is an assistant professor of teaching (film studies) in the Department of Theatre and Film at UBC. Like Claudia, Christine was interested in her students engaging with ChatGPT-3\n\ncritically. To achieve that she designed an assignment where in their final essays, students needed to “work alongside the notorious ChatGPT-3.” Her assignment first described ChatGPT-3 as an LLM and offered a provocative statement:\n\n“Some academics have commented that it may portend ‘the death\n\nof the humanities,’ because its near-instantaneous responses, natural- sounding prose, and ability to comb the Web for information mean that humans no longer need to build critical thinking skills over time.”\n\nThe essay students had to write was based on their earlier work in the semester teaching games to the rest of the class. Students were tasked to write on a subject related to those games, which could include “style, agency, choice, avatars, point of view,” and issues that tend to emerge from game design and gameplay such as “narrative arcs, characters,” and more.\n\n351\n\nChapter 12 Use Cases\n\nA how-to was included as part of the assignment, which described how students could engage with ChatGPT-3. In addition, guidelines like the following were provided to ensure students structured their essay and had a well-thought-out argument:\n\n“Your essay MUST contain a coherent thesis statement in which you clearly state what your essay will be arguing. Remember that your job as a writer is to convince the reader of the veracity of your argument, so a strong thesis statement is essential. Be specific. Tell me exactly what you will be arguing and how you will achieve your claims.”\n\nHere are steps that students had to undertake:\n\nProvide ChatGPT-3 with a prompt that was related to\n\nthe essay topic and analyze the generated result.\n\nInclude whatever content was generated with their\n\nown essay.\n\nCompare ChatGPT-3’s generated result with their own\n\nrevision and additions to it.\n\nChristine also included instructions on how students could present their comparison in a section she called “How do I access/use ChatGPT-3\n\nand create a legible comparison of its work and mine?”\n\nChristine’s case points to the importance of addressing the use of ChatGPT by students directly and demonstrates the value in doing so, both as an educational experience and to show students that an LLM cannot substitute their own critical minds as expressed in their writing.\n\nUse Case: Increasing Public Understanding of ML\n\nIn a similar way Matthew J. Yedlin wants to educate his students on how machine learning models work so that they can be better informed when reading headlines that generate untruths about AI, how it works, and that it will replace humanity. To quote Matthew, “It’s not magic; it’s mathemagic.”\n\n352\n\nChapter 12 Use Cases\n\nMatthew is an associate professor, jointly appointed in the\n\nDepartments of Electrical Engineering in the Faculty of Applied Science and Earth and Ocean Sciences in the Faculty of Science at the University of British Columbia. Dr. Yedlin’s research is interdisciplinary, focusing on the applications of techniques in electrical engineering to geophysical research problems and the application of multiple scattering to practical electromagnetic wave propagation problems. He is currently teaching a course on machine learning.\n\nAuthor: Matt, can you explain how you are approaching the teaching of machine learning?\n\nMatt: In the current discourse on AI, hyperbole about future dystopian developments hinders the development of public policy on AI evolution.\n\nFurthermore, there has been little effort to explain the intuition behind large language models (LLMs) such as ChatGPT. The public must obtain such intuition, as it is finally the public who participate in policy formulation for AI. After nuclear weapons were tested and used by the US on Japan, it was the public, through their elected officials, who created treaties around the use and proliferation of such weapons. Organizations such as the International Atomic Energy Agency (IAEA) and the Comprehensive Test Ban Treaty Organization (CTBTO) were created to monitor compliance with these treaties. The same will be needed for the equivalent AI compliance.\n\nThe foregoing implies that we need to explain the intuition behind ChatGPT by focussing on the T in GPT—Generative Pretrained Transformer.\n\nThe transformer is an algorithm based on word importance pattern matching known as attention. After the initial development of the concept of attention in 2015, a landmark paper “Attention Is all You Need”2 was published by Google in 2017. This work formed the basis of the transformer that is used in LLMs and is based on a clever abstraction in which basic linear algebra pattern matching is used. To obviate the problem of explaining math and computer science to the public, a metaphor will be used. That metaphor is a specially constructed ballroom dance competition 353\n\nChapter 12 Use Cases\n\nthat mirrors the type of pattern matching used by the transformers in ChatGPT. The pattern matching intuition is obtained by the visuals encoded in the dance competition metaphor.\n\nWhile the transformer is currently the computational workhorse in ChatGPT-4, it has several limitations including computational cost and a limited information context window that can be handled, now approximately 2K words, maximum! See the link https://hazyresearch.\n\nstanford.edu/blog/2023-03-07-hyena for a complete description of the advantages of the Hyena Algorithm developed by Stanford and Montreal Institute for Learning Algorithms (MILA) researchers. The principal takeaway is that the information context window can be 64K words and computable using this new algorithm. That means that a whole textbook can be used as input to the LLM. Questions could be asked about detailed inferences between two texts, or gigapixel images could be created and compared! The possibilities for generative AI applied to natural language processing and image analysis almost seem fairy-tale-like!\n\nMatthew’s case points to the importance of knowledge translation and how people can come to better understand what machine learning models are and what they are not. This will help empower people to make better choices when it comes to integrating generative AI in their own creative work.\n\nReferenced Papers\n\n1. Bahdanau, D., Jacobs University, Bremen, Germany;\n\nCho, K., Bengio, Y., Universite de Montreal. Neural\n\nMachine Translation by Jointly Learning to Align\n\nand Translate. Published as a conference paper at\n\nICLR 2015.\n\n354\n\nChapter 12 Use Cases\n\n2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\n\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\n\nI. Attention Is All You Need. arXiv:1706.03762. June\n\n2017. 15 pages.\n\nTakeaways\n\nOveremphasis on dystopian future scenarios involving\n\nAI can hinder the development of public policy around\n\nAI. Therefore, a more balanced and informative\n\ndiscourse is needed.\n\nThere is a need for public understanding of the\n\nworkings of large language models (LLMs) like\n\nChatGPT, as public opinion influences policy\n\nformulation for AI, just like with nuclear weapons.\n\nTo educate the public, the focus should be on\n\nexplaining the ‘T’ in GPT, Generative Pretrained\n\nTransformer, which is an algorithm based on word\n\nimportance pattern matching, known as attention.\n\nThe concept of attention, critical to the functioning of\n\nthe transformer, is based on pattern matching through\n\nbasic linear algebra. Simplifying this complex concept\n\nfor the public can be achieved through metaphors,\n\nsuch as a ballroom dance competition.\n\nThe transformer algorithm, though fundamental\n\nto current LLMs, has its limitations including\n\ncomputational cost and a restricted information\n\ncontext window, currently capped at around\n\n2,000 words.\n\n355\n\nChapter 12 Use Cases\n\nRecent advancements, such as the Hyena Algorithm\n\ndeveloped by Stanford and the Montreal Institute for\n\nLearning Algorithms (MILA), offer solutions to these\n\nlimitations, enabling a much larger context window (up\n\nto 64,000 words). This means a whole textbook could\n\nbe used as input for the LLM, offering much broader\n\npotential for language processing and image analysis.\n\nThe ability to ask questions about detailed inferences\n\nbetween two texts or compare gigapixel images with\n\nLLMs opens immense possibilities, transforming\n\nnatural language processing and image analysis in\n\nways that almost seem fairy-tale-like. However, these\n\nadvancements also necessitate careful thought and\n\npolicy development to guide their use responsibly.\n\nUse Cases for Creatives in Industries\n\nThere are numerous ways that narrow AI including generative AI are being used by creatives. The use cases that follow show a broad distribution of cases including web-based animation, chained generative AI to develop scripts for invented avatars, and visual effects in film.\n\nUse Case: Concept Art for Animation\n\nJunyi wanted to know what it would be like if they used generative AI to come up with a bunch of images of shadowed warriors for a web-based animation.\n\nHis team had no idea where to start in terms of the look and feel. They imagined all kinds of things but were a bit stuck as they didn’t want to emulate previous work. They started to think of stop motion and were thinking of playdoh (Figure 12-3). They used a source image they created with an AI of two warriors in battle and a specific filter that made them look like playdoh.",
      "page_number": 454
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 462-469)",
      "start_page": 462,
      "end_page": 469,
      "detection_method": "topic_boundary",
      "content": "356\n\nChapter 12 Use Cases\n\nFigure 12-3. Two playdoh warriors looking mean\n\nFrom there they went on an iterative discovery to develop their concepts being completely open to look/feel, technology, and animation method. At first images were generated with a flat design style by the AI, and these\n\nseemed to captivate the motion and energy of an epic warrior battle (Figure 12-4).\n\n357\n\nChapter 12 Use Cases\n\nFigure 12-4. First version of shadowy warriors\n\nThat image was used to generate the next image, and two interesting things happened that made Junyi and the team think of a new direction.\n\nSmaller warriors appeared in subsequent image-image substitution and regeneration, which made it look like a battle was ensuing between a giant and smaller warrior (Figure 12-5). The second result of the generation was that it reminded one of the animators of a 1948 Disney short Blue Shadows on the Trail. The team loved the grays and blues and were curious to see how these evolved in the hands of an AI with minimally modified prompts.\n\n358\n\nChapter 12 Use Cases\n\nFigure 12-5. First warrior split into three\n\nIn the next series of images, even smaller warriors developed, which gave Junyi the idea that the battle between two giants would evolve, and as it did, each time a giant was hit, it would result in smaller warriors manifesting out of the giant’s body until all that was left were small warriors fighting each other (Figure 12-6). The rest of the team loved that idea but wanted to see the end of their timeboxed exploration through.\n\n359\n\nChapter 12 Use Cases\n\nFigure 12-6. Warriors attacking the giant are generated, and a text prompt stating that also helps\n\nWhile Junyi and the team loved the transformation of giants into small warriors, they didn’t ask the AI to do that. What happened next, they could not predict. The last standing giant transformed into a tree, and on the ground beneath the giant trees, the battle continued with only small warriors engaged in battle (Figure 12-7). The second unpredictable transformation was that the color palette suddenly changed to add greens and yellows and the warriors were no longer shadows but more realistic fighting humans.\n\n360\n\nChapter 12 Use Cases\n\nFigure 12-7. Giants turn into trees and a small army keeps fighting While developing concept art through generative AI started the team off as the intended activity, Junyi and the team realized that the process itself led them to not only define the look/feel of their animation but the narrative and methods they would then undertake to create the animation.\n\nIn their words “it not only accelerated our production, it gave us ideas as we went along. Even though we never used any of the art, it didn’t matter\n\nbecause we now had an important theme of transforming objects into characters.”\n\n361\n\nChapter 12 Use Cases\n\nUse Case: AI-Generated Talking Heads\n\nGenerative AI can provide creatives a wonderful tool to test out ideas and, as was the case with Junyi, even if you don’t use any of them in your final offering to the world. The following use case describes the steps that a colleague went through to develop an auto-generated series of characters to narrate a sci-fi pulp romance story. Bear in mind that this type of technology is not the same one used for deep fakes and in no way is intended to be offensive or dangerous. The combination of different generative AI in a row is what’s interesting in this case.\n\nJen’s first workflow imagines using a text generation model to create a script, then using an image generation model to create illustrations to accompany the script, generating a character to read the script, using a voice generation model to convert text-speech, and then using the image of the narrator combined with the text-converted speech with a video generative AI. This combination of AI models provides a more comprehensive and polished final product than using a single model alone. Other uses for the workflow can include an audio book, video book, video-generating teaching tool, etc.\n\nStep 1: Generate text using a natural language model.\n\nAuthor: What’s a good synopsis of a pulp love novel involving two cyborgs?\n\nMuse: Here is a possible synopsis for a pulp love novel involving two cyborgs:\n\nIn a future where humans and cyborgs coexist, two outcasts find love in each other’s arms. Ada is a former soldier who was turned into a cyborg against her will and now struggles to come to terms with her new mechanical body.\n\nMax is a brilliant scientist who was born with a rare genetic condition that forced him to undergo numerous surgeries and augmentations.\n\nDespite the societal prejudice and discrimination they face, Ada and Max find solace in each other’s company. They bond over their shared experiences of being different and misunderstood and soon realize that they have fallen for each other.\n\n362",
      "page_number": 462
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 470-477)",
      "start_page": 470,
      "end_page": 477,
      "detection_method": "topic_boundary",
      "content": "Chapter 12 Use Cases\n\nAs they embark on a tumultuous romance, Ada and Max must confront their own demons and insecurities, as well as the dangers that come with being in a relationship with a cyborg. Will their love be able to withstand the challenges they face, or will it succumb to the prejudice and violence of a society that does not accept their kind?\n\nStep 2: Generate an image that could accompany the story in a blog post using a text-image generative AI (Figure 12-8).\n\nFigure 12-8. Cyborgs in love running from other cyborgs where at least one rider is facing the wrong way\n\n363\n\nChapter 12 Use Cases\n\nStep 3: Use a text-speech generative AI and then link the file within the blog to any social audio site.\n\nJen commented that the best voice she had found after searching multiple text-speech AI sites was one whose name was Terrell who was modeled after an “African American male 52 years of age.” After consultation with colleagues who identify as Black Americans, she consulted on the type of avatar she could generate for her prototype.\n\nThis situation does bring up issues of representation during the creative process, and it is worthy to consult on best approaches when you identify with a culture that is different than the one you generate content with.\n\nJen moved forward knowing this was not a prototype that was to be made publically shared. She was also curious if she could create a realistic-looking video and if it would pass user-testing with her colleagues that she consulted at the beginning of the process.\n\nStep 4: Generate an image that best aligns with the description of the voice on the text-speech generative AI site (Figure 12-9).\n\n364\n\nChapter 12 Use Cases\n\nFigure 12-9. The prompt “medium shot centered frontal facing photo- realistic fifty-year-old African American man with long beard, neutral face, hyper-realistic, glasses,” generated in Stable Diffusion and then fed into an image-image AI with an anime filter applied.\n\nIterations = 105\n\nStep 5: Use the image in an image-video and text-speech site to generate a narrator telling us the story of the cyborgs in love.\n\nThe use of multiple machine learning models creates more complex and interesting content across media and surfaces complex tensions and considerations when it comes to representation. By combining the strengths of multiple models, a healthy criticality, and discernment, creatives can\n\naccelerate their prototyping workflow. Often, when thinking of the final form that your prototype will take, it’s helpful to get there quickly so you can have a compelling idea that points to what you are aiming for. This is true when it comes to testing out a prototype, especially to reveal your own biases and the inherent biases your prototype might demonstrate.\n\n365\n\nChapter 12 Use Cases\n\nUse Case: Fact-Checking Code\n\nAn important part of the prototyping process is to evaluate a prototype prior to moving onto the next one, particularly when you may be pressured as a team to deliver a final product by a certain date. The same needs to occur when initial prototypes are generated with your AI muse. That process is an important one that anyone who prototypes must also go through. When you prototype, you may think or believe that what you are creating is unique or original, but it might not be. Some teams tend to believe that what they are engaged in making is a final offering, when it is just the beginning. The following are some examples of AI-generated prototypes that need evaluation. That evaluation can also be guided by domain experts and may lead you to other avenues. The following use case used ChatGPT-3, and while advances have been made with ChatGPT-4, there are still syntax issues.\n\nMuse: Code Generation : AI can be used to automatically generate code for a given task or application, allowing developers to quickly create a working prototype without having to write all of the code from scratch.\n\nGuest Programmer Note: A machine learning model like ChatGPT-4\n\ncan generate working code, but it overcomplicates things and can also get some things wrong. If it is scraping from git or another repository, the question you must ask yourself is, Why not just go to the repo itself? What it’s better at is as a search tool to locate scripts in a game engine like Unreal that may not be easy to find quickly. But always find the real source to cross- check unless you love debugging. You love debugging, don’t you?\n\nAuthor: Using C# write a function and accompanying tests, which will evaluate two poker hands (five cards each) to determine a winner. The cards will be a part of a standard 52-card deck (no jokers), suits are ignored for this question, and aces are high. The function should output 0 in the case of a tie, 1 if the first player is the winner, and 2 if the second player is the winner.\n\nMuse: (snapshot)\n\n366\n\nChapter 12 Use Cases\n\nFigure 12-10. A snapshot of code generated in ChatGPT-3\n\n367\n\nChapter 12 Use Cases\n\nSample\n\nusing System;\n\nusing System.Linq;\n\npublic static class PokerHandEvaluator\n\n{\n\npublic static int CompareHands(int[] hand1, int[] hand2)\n\n{\n\nint[] values1 = hand1.Select(card => card % 13 +\n\n1).OrderByDescending(value => value).ToArray();\n\nint[] values2 = hand2.Select(card => card % 13 +\n\n1).OrderByDescending(value => value).ToArray();\n\nint rank1 = GetRank(values1);\n\nint rank2 = GetRank(values2);\n\nif (rank1 > rank2)\n\n{\n\nreturn 1;\n\n}\n\nelse if (rank1 < rank2)\n\n{\n\nreturn 2;\n\n}\n\nelse // same rank\n\n{\n\nfor (int i = 0; i < 5; i++)\n\n{\n\nif (values1[i] > values2[i])\n\n{\n\nreturn 1;\n\n}\n\nelse if (values1[i] < values2[i])\n\n368\n\nChapter 12 Use Cases\n\n{\n\nreturn 2;\n\n}\n\n}\n\nreturn 0; // tie\n\n}\n\n}\n\nprivate static int GetRank(int[] values)\n\n{\n\nbool isFlush = values.All(value => value == values[0]);\n\nbool isStraight = values[0] == values[1] + 1 &&\n\nvalues[1] == values[2] + 1 && values[2] == values[3] +\n\n1 && values[3] == values[4] + 1;\n\nbool isStraightFlush = isFlush && isStraight;\n\nbool isFourOfAKind = values[0] == values[1] &&\n\nvalues[1] == values[2] && values[2] == values[3] ||\n\nvalues[1] == values[2] && values[2] == values[3] && values[3] == values[4];\n\nbool isFullHouse = values[0] == values[1] && values[1]\n\n== values[2] && values[3] == values[4] || values[0] ==\n\nvalues[1] && values[2] == values[3] && values[3] ==\n\nvalues[4];\n\nbool isThreeOfAKind = values[0] == values[1] &&\n\nvalues[1] == values[2] || values[1] == values[2] &&\n\nvalues[2] == values[3] || values[2] == values[3] &&\n\nvalues[3] == values[4];\n\nint pairsCount = values.GroupBy(value => value).\n\nCount(group => group.Count() == 2);\n\nbool isTwoPairs = pairsCount == 2;\n\nbool isPair = pairsCount == 1;\n\n369\n\nChapter 12 Use Cases\n\nif (isStraightFlush)\n\n{\n\nreturn 9;\n\n}\n\nelse if (isFourOfAKind)\n\n{\n\nreturn 8;\n\n}\n\nelse if (isFullHouse)\n\n{\n\nreturn 7;\n\n}\n\nelse if (isFlush)",
      "page_number": 470
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 478-486)",
      "start_page": 478,
      "end_page": 486,
      "detection_method": "topic_boundary",
      "content": "{\n\nreturn 6;\n\n}\n\nelse if (isStraight)\n\n{\n\nreturn 5;\n\n}\n\nelse if (isThreeOfAKind)\n\n{\n\nreturn 4;\n\n}\n\nelse if (isTwoPairs)\n\n{\n\nreturn 3;\n\n}\n\nelse if (isPair)\n\n{\n\nreturn 2;\n\n}\n\n370\n\nChapter 12 Use Cases\n\nelse // high card\n\n{\n\nreturn 1;\n\n}\n\n}\n\n}\n\n// Tests\n\npublic static class PokerHandEvaluatorTests\n\n{\n\npublic static void Test()\n\n{\n\nint[] hand1, hand\n\nHere’s a quick rundown of a guest programmer’s interpretation:\n\nIt’s interesting. It did kind of well! In my opinion. It did a few things wrong. It’s as if it didn’t understand the restraints described in the assignment. I tested it in a custom C# project to see the result, and there are a few things that stand out.\n\n1. It’s selecting cards by using Select, which performs an\n\naction on the number in the array and then returns\n\nthat value. It’s strange because it performs this action:\n\nhand1.Select(card => card % 13 + 1)\n\nThis grabs each card one at a time, as long as it’s\n\nbelow a value of 13, and then it adds 1 to it. The\n\nassignment specifically says we shouldn’t add 1, but\n\npass the direct values of the cards, e.g., 11 for jack,\n\n12 for queen, 13 for king, and 14 for aces as aces\n\nare high.\n\n371\n\nChapter 12 Use Cases\n\n2. It’s sorting the arrays before the calculation is done—\n\nthat’s correct. Then it requests the scores, but it only half-ish understands that there are no suits in this assignment, so it includes the result possibility for straight flush and flush, even though it never actually checks for suits. Even if it wanted to, it can’t check for suits, as we’re asked to only use two int arrays for calculating the hands.\n\n3. It defines the possible results as Booleans and sets them to true if it hits the exact requirement, which is a valid\n\napproach. This certainly satisfies the result requirement\n\nof the assignment. After each Boolean is assigned, it\n\ngoes through them one by one, from highest to lowest\n\noutcome, and stops at the first correct value.\n\nThis is a valid approach, but it’s not optimal.\n\nAn example of this could be when the bot checks for 4 of a kind. If it only finds 3 of a kind, it goes to the next step and starts over. That’s a waste of\n\ntime, as we already know there are only two outcomes left, full house or 3 of a kind.\n\nSince it calculates all outcomes first, it’s not considering the wasted effort.\n\nThere’s no reason to calculate if a hand has a straight if it already has a full house. It’s very neat though, and easily understandable, but I wouldn’t say it followed the instructions of the assignment. The code works and gives the right results when you give it numbers, so it’s not super far off. It’s also not considering how difficult it would be to alter this, if (and when) the client changes their mind for the third time. Maybe it’s because it knows it can type faster than the average programmer when it must redo a lot of work. ;) Oh, and the calculation for the winner if two players have the same ranked hand is wrong!\n\nTwo 2s can win against two 6s if the other cards in the hand are high. It’s just straight up wrong there.\n\nThat’s a good example of why it isn’t reliable just yet. But it’s a good start if a programmer is unsure of how to begin.\n\n372\n\nChapter 12 Use Cases\n\nWhat can we learn from this? Generated code requires a more\n\nseasoned programmer to review, edit, and refine. I reflect on a video I saw where a programmer was amazed at ChatGPT-3’s ability to generate code for a bouncing ball. On the programmer’s attempt to implement the code in the Unity 3D game engine, they found that there were many mistakes.\n\nThe ball wouldn’t bounce. In fact, it went through the floor. “No problem,”\n\nthe programmer explained. They just went in there and fixed all the bugs, repairing and debugging until they had a bouncing ball. As an experiment it tells us quite a bit. Without the experience and skill, a less skilled programmer would have difficulty with making the ball bounce. They would search the Web for how to fix the code, and it would likely take them a lot of\n\ntime to do so. Alternatively, the programmer could just go to one of many open source code repositories, search for a bouncing ball animation, and be able to implement it in less than ten minutes.\n\nUse Cases: Integrating Different APIs\n\nand Local Networks\n\nTo understand the next use case, it’s important to speak to the future of generative AI. One of those futures is the capacity for an AI system to be implemented within another system. Newer versions of ChatGPT, for example, allow its API to be “plugged in” to a game engine like Unity 3D.\n\nThis feature allows developers to ask for specific code functionality while they work inside of Unity. Tasking ChatGPT-4 to explain how it can be integrated within a game engine gives you an idea of just how complicated it is.\n\nMuse: Incorporating GPT-4’s API into a game engine would involve several steps, depending on how you want to use the AI model. The specific implementation would also depend on the game engine you’re using, but let’s talk about the general steps. Here’s how it could work: 373\n\nChapter 12 Use Cases\n\nConnectivity : Set up your game engine to send HTTP requests. You’ll need to be able to send a request to the GPT-4 API over the Internet and then process the response. Depending on your game engine, you may need to use a specific library or plugin to handle HTTP requests.\n\nAPI Calls : Create functions to send requests to the GPT-4 API. This could involve taking some text input from the game (such as dialogue from a player or an AI character’s previous response) and sending it as a prompt to the GPT-4 API. You will also need to handle the API’s response, which will include the text generated by GPT-4.\n\nIntegration : Integrate the generated text into your game. This could involve displaying the text as dialogue from an AI character, using it to generate descriptions of objects or locations, or even using it to dynamically create\n\nquests or other game content. Depending on your game, you might also need to parse the generated text into a format that your game can understand.\n\nData Management : Consider how to manage data usage and response time. GPT-4, being a large model, can take a few seconds to return a response, and frequent API calls can add up in cost. Therefore, consider caching responses, limiting the number or frequency of requests, or using a smaller model when real-time interactivity is not critical.\n\nError Handling : Implement error handling and fallbacks for when the API is not available or does not return a useful response.\n\nContent Moderation : If the outputs of the AI will be directly exposed to the players, consider putting in place a moderation layer to filter out any inappropriate content.\n\nKeep in mind that incorporating GPT-4 into a game engine in this way could be useful for prototyping, but for a commercial product, it could be more effective and cost-efficient to have a local model running in the game engine, assuming the computational resources are available and the GPT-4\n\nmodel size doesn’t exceed the hardware’s memory capacity. You may also consider using smaller models like GPT-2 or GPT-Neo, which can be run locally with less computational resources.\n\n374\n\nChapter 12 Use Cases\n\nAs complicated as it reads, integrating GPT-4’s API into other software is already being done by many creatives, including Bill Zhao. Bill is a lecturer at the Master of Digital Media program supervising grad students on emerging technology development with external clients from the digital industry. On occasion Bill collaborates with some students to develop new technology. His latest is what he calls an AI workflow platform, which essentially has different AI talking with each other to support specific creative workflows. Integrated with a diverse range of multi-modal AI nodes that transform natural languages, images, audio, and videos, Tomator empowers users to create dynamic AI workflows that automate tasks,\n\noptimize content, and boost creativity, all inside a simple, intuitive, and programmable node-graph interface like in Figjam and Miro, with easy-to- use templates that run with a single click.\n\nThere exist a growing number of use cases regarding the use of\n\nGPT-4’s API acting as an intermediary between the functionality of other software. This involves generating an API key, and there is a large volume of information and how-to’s on the Internet. In addition to the use of APIs, here are some examples of how developers and creatives are engaging with generative AI and developing new partnerships. This list will have increased by the time you read these words:\n\nUsing ChatGPT-4 within game engines like Unity and\n\nUnreal. In Unity a developer is integrating it to control\n\nthe Editor using command prompts as a proof of\n\nconcept. In Unreal another tech development company\n\ndemonstrated how integrating ChatGPT-4 allowed\n\nusers to input simple command prompts to control\n\nlighting and randomly change that lighting, amid other\n\npossible commands.\n\nInstalling Stable Diffusion and Deforum by Stable\n\nDiffusion locally and being able to train it with your\n\nown images while taking advantage of many more\n\n375\n\nChapter 12 Use Cases\n\nfeatures than what is available on the web-based build.\n\nThose features are too numerous to mention, but\n\nsuffice it to say, it’s worth the time in setting up and\n\ndepending on your computing power will accelerate\n\nrender times for images and video.\n\nTraining NPCs in video games with AI is the next step\n\nforward for some video games. Game companies like\n\nInworld are leading in this regard with Origins, a case\n\nstudy, demonstrating that non-playable character\n\ndialogue and behavior can be prompted by players\n\nusing AI. Traditional NPCs are scripted, so this will add\n\nan exciting dimension to the gaming experience.\n\nLarger companies like NVIDIA, TurboSquid, and\n\nShutterstock are partnering to train 3D models with\n\nShutterstock assets. In what we hope is a model to\n\nfollow, Shutterstock will also compensate artists\n\nfor those pixels that will contribute to training the\n\ngenerative technology.\n\nWhat an increasing number of new use cases are revealing is a\n\ndesire for creatives to use the technology privately and within their own prototyping environments. This signals a healthier and more sustainable future for generative AI. Incorporating APIs into different environments significantly enhances functionality, interactivity, and customization.\n\nAPIs allow different software components to communicate, opening up opportunities for integration with external systems. In ecommerce, APIs can connect a website to payment gateways, improving the customer experience. In data analytics, APIs enable real-time data retrieval from various sources for more accurate insights. In education, APIs can connect learning management systems to external resources, enhancing learning experiences. In gaming, AI APIs like GPT-4 can be used for dynamic content creation and natural language interfaces.\n\n376\n\nChapter 12 Use Cases\n\nCommunity-Based Initiatives\n\nOceanofPDF.com",
      "page_number": 478
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 487-495)",
      "start_page": 487,
      "end_page": 495,
      "detection_method": "topic_boundary",
      "content": "As this chapter is being written, an email to the author reveals several customizable ML models being co-constructed by researchers and\n\ncreatives internationally with residencies being offered on the “Living with Machines” project. These are also being designed for interested creators to contribute to, research, and access specific data sets, such as neural language models for nineteenth-century English or one that offers a data set on the chronology of railway passenger stations in Great Britain. The ability to manage, interact with, and have some measure of input in the development of a machine learning model whose data set a person also contributes to is an appealing future that is already within grasp. That will be made easier with companies focusing efforts on cloud computing and the increasing need to render content rapidly in the cloud.\n\nThe second use case also uses OpenAI’s API but with the intent of bridging community action with respect to generative AI technologies.\n\nNot-for-profit Thaumazo (Greek for wonder) creates specialized VR spaces that use GPT-enabled dialogue by integrating Watson speech-to-text and text-to-speech. Thaumazo is also initiating a working group within the AI & Us community bringing together individuals and organizations using AI for positive impact projects (particularly open source) to find opportunities to collaborate and support each other’s work.\n\nWhile some spaces are for project ideation or more general guidance, they are developing a dedicated virtual space that is designed to help people think about the potential negative impacts of AI projects. The 42\n\nJudges project is a space that is based on the 42 judges from the Egyptian book of the dead—each has been given a new sin that must receive a negative confession from the participant in relation to their project and 377\n\nChapter 12 Use Cases\n\nits potential harms, for example, “Usekh-nemmt, who comest forth from Anu,” will talk with them about whether their project can be used to\n\n“commit immoral or unethical actions that could harm individuals or society as a whole,” based on the original negative confession: “I have not committed sin.” By walking the gauntlet of these 42 judges, the participant explores the nuances of potential harms of a given project.\n\nAccording to lead Daniel Lindenberger who also works at the\n\nEmerging Media Lab at the University of British Columbia, “participants can create their own project description, or can select from numerous ones that already exist, such as the Replika chatbot or Stable Diffusion.” What’s important is to then provide time for human-centered conversations around the current AI revolution.\n\nFigure 12-11. A screenshot from the 42 Judges project by Daniel Lindenberger\n\n378\n\nChapter 12 Use Cases\n\nTakeaways from Using AI in the Film Industry\n\nUse Case 1: Souki Mehdaoui\n\nSouki Mehdaoui, a Los Angeles-based director represented by Futuristic Films, uses film and AI to empower individuals. Her directorial debut,\n\n“Firelei Baez,” won multiple accolades. She’s directed commercials for brands like Doordash, TED, and Yahoo. Previously a New York City cinematographer, her work is featured in Netflix and HBO documentaries, including Sundance hits “The Great Hack” and “Mucho Mucho Amor.” As co-founder of the AI consultancy firm Bell & Whistle, she helps businesses harness AI technology for growth.\n\nSouki created a short film based on a public domain poem that she also voiced and recorded. She describes her process with generative AI as collaborative.\n\nThat process involved the following:\n\nChoosing a poem and using the lines of that poem\n\nas prompts\n\nUsing two Discord servers, one with Midjourney and\n\nthe other with generative video AI Runway 2.\n\nJuggling between the servers to input lines of a poem\n\ninto them which required her to switch between\n\ndifferent command terms like “/imagine” (Midjourney)\n\nand “add Gen” (Runway 2)\n\nShe started by seeing what Runway 2 could generate without any\n\nadditional references, simply providing a poetic prompt and letting the program upscale and interpolate it. When she identified areas where the AI struggled, such as with more abstract lines, she shifted to Midjourney.\n\nIn Midjourney she found that the platform also struggled with abstract statements that did not follow a typical noun-verb-adjective structure.\n\nTo circumvent this, she had to creatively tweak the prompts, avoiding 379\n\nChapter 12 Use Cases\n\nadditional text, and experimenting with different film styles. When she found an image she liked, she would maintain the seed number and reference image, then alter the CFG (classifier free guidance) scale to assess which fidelity felt best. The CFG scale adjusts the degree to which the image looks closer to the prompt and/ or input image. Overall, she describes the process as an exercise in randomization and an attempt to control chaos.\n\nThe final step involved importing the rendered video clips in a non-linear editing software application. The names of the image files reflected the text output, which she used as a guide for the corresponding parts of the poem. She imported multiple images per text line and tried to remain faithful to what the AI interpreted each line to be, rarely deviating unless there were limited or unsatisfactory image options for a particular line. If that happened, she would stretch the use of the more abundant images from previous prompts. She did rearrange the order of some images to maintain a flow to the narrative. She sought a balance, preserving as much of the original AI interpretation as possible.\n\nTakeaways\n\nWhen you start to experiment with generative video\n\nyou will likely end up using more than one platform as\n\neach have their own affordances and constraints\n\nRather than have a well defined end product in mind,\n\nremain open to what the AI generates and be willing to\n\nwork with abstract offers\n\nA skilled film maker will be able to get more out of an AI\n\nbecause of knowing the vocabulary of film production.\n\nThat includes type of film stock, camera angle, lighting,\n\ntype of lens, type of shot, colour, etc…\n\n380\n\nChapter 12 Use Cases\n\nOnce you have generated video clips anticipate that\n\nthese will need to be stitched together, edited, refined,\n\nand colour matched in you favourite software\n\nYou will also need to work with a composer or generate\n\nyour own music using a generative music AI\n\nYou may also need to secure a voice over artist, record\n\nyour own voice or use a text-speech AI to generate the\n\nnarration track\n\nUse Case 2: Ollie Rankin\n\nWhile the focus of most of the use cases in this chapter has been on generative AI, they owe their popularity, in part, to many creative industries embracing narrow AI to support the evolution of computer graphics and its uses to support the film, animation, visual effects, and game industries. With that in mind, many creatives are not limited to their use of generative AI. Ollie Rankin is one such creative.\n\nOllie has been involved in designing, developing, and using many different crowd and battle simulation systems during a visual effects industry career spanning more than 20 years. His big break came in 1999\n\nwhen he was hired by Peter Jackson’s Weta, for his artificial intelligence expertise. He was brought on to use the ground-breaking crowd simulation software Massive, being developed by colleague Stephen Regelous, for The Lord of the Rings trilogy. While Regelous iterated on the underlying technology, Ollie and a small team of technical directors perfected the techniques of “brain building” and battle choreography that were necessary to bring Jackson’s epic vision to the screen.\n\nMassive and the associated workflows remain a core part of the Weta pipeline and have since been used on the Avatar, Avengers, and Planet of the Apes franchises, among many others. Meanwhile, Ollie built on what he’d learned at Weta, helping several other visual effects studios around the world to develop their own proprietary crowd simulation tools.\n\n381\n\nChapter 12 Use Cases\n\n“Each of the crowd systems I’ve helped develop sits at a different point along a spectrum from pure behavioural simulation to art-directed choreography.” For Ollie the common challenge in all cases is to tell a believable story, not just to fill a movie screen with hundreds or hundreds of thousands of realistic-looking digital characters (called “agents”).\n\nWhether it’s armies of warriors or armies of football fans, Santa’s elves, or Elrond’s elves, the agents need to be programmed to act en masse in a way that tells the audience who is winning the war or the game. “No matter how uniform these armies might be, by design, the individuals always need to be different enough from each other in appearance and movement to not seem repetitive.” When half a million orcs need to be dressed, positioned, and directed to act coherently, but differently from each other, it’s not practical for someone to have to individually decide what each one should look like or be doing.\n\n“This is where random number generators, combinatorial probability and so- called ‘fuzzy’ logic come into their own. By having a large enough number of differentiating parameters, each randomly assigned within a defined range, by ensuring that the way that those parameters combine is sufficiently complex to create an innumerably large number of possible permutations and then by ensuring that the same degree of complexity motivates the movements of each agent, you can overcome the cookie cutter repetitiveness that dogged the ‘crowd replication’ approaches that crowd simulation replaced.”\n\nSome workflows that involved AI include the following:\n\nMotion capture of a complete set of actions that\n\na character could carry out depending on their\n\nphysiology and the props or weapons that they carried.\n\nA “state machine” that would constrain the actions\n\na character could carry out based on the pose they\n\nwere in.\n\n382\n\nChapter 12 Use Cases\n\nA “brain” that would receive and process various\n\nstimuli and decide which action the agent wanted to\n\ncarry out next.\n\nGiving agents the ability to “see” and “hear” what was\n\ngoing on around them ensured that the inputs into\n\neach agent’s brain were completely unique.\n\nAllowing them to identify friends and foes and to\n\ninterpret what others were doing could motivate\n\nreactive and pre-emptive behaviors.\n\nBy simulating varying emotions (Ollie called this\n\npart of the virtual brain the “emotion matrix”), it was\n\nmodulating each agent’s behavior according to their\n\ninnate “bravery” and their current physical condition.\n\nLessons learned from developing multiple intelligent systems over the years to drive animated characters include the following:\n\nDifferent approaches can be employed in animation,\n\nranging from behavioral simulation to art-directed\n\nchoreography. The choice depends on the specific\n\nneeds of the story.\n\nThe primary challenge in animation is to tell a\n\nbelievable story. This does not mean simply filling the\n\nscreen with numerous realistic-looking characters but\n\nprogramming these characters to behave in a way that\n\nconveys the narrative and the current state of events.\n\nEven in large crowd scenes, each character should\n\nhave a distinct appearance and behavior to avoid\n\nrepetitiveness. Uniformity should not compromise\n\nindividual distinctiveness.\n\n383\n\nChapter 12 Use Cases\n\nTo create such diversity, tools such as random number\n\ngenerators, combinatorial probability, and fuzzy logic\n\ncan be used. These help create many differentiating\n\nparameters and a vast number of possible\n\npermutations.\n\nExperimentation with machine learning is possible by\n\ncreating feedback loops that allow characters to modify\n\ntheir behavior based on experience. However, the\n\nstorytelling objectives should not be compromised.\n\nRandomness can be an effective tool in animation,\n\noften creating surprising results and a diversity of\n\nbehavior.\n\nAt the end of the day, the opinions of the director and\n\nthe audience are more important than the purity of\n\nthe simulation. Hence, the machine intelligence might\n\nbe controlled or even eliminated to ensure the most\n\norganic and satisfying storytelling experience.",
      "page_number": 487
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 496-504)",
      "start_page": 496,
      "end_page": 504,
      "detection_method": "topic_boundary",
      "content": "What is clear in the development of intelligent systems over the years is that they clearly supported a larger vision. That vision in great part was collaborative and emphasized the importance of good storytelling. Any technology can be used to support storytelling, and this is no exception with AI. A key takeaway for creatives is to persistently keep top of mind why you are engaging with generative AI and how the content it generates can support your own vision. Ollie’s use case also shows us the positive value of narrow AI. Amid the current fears of job replacement and worse, it is important to keep in mind that some creatives have been integrating AI in various creative processes for decades, and their commitment to these systems has contributed to much of the entertainment that we consume.\n\n384\n\nChapter 12 Use Cases\n\nAcknowledgments\n\nTo Dr. Claudia Krebs, Christine Evans, and other\n\neducators who continue to inspire in how they\n\nfearlessly embrace emerging technology while\n\nmaintaining discernment\n\nTo Junyi Song who was willing to share how their team\n\nintegrated generative AI in their ideation phase\n\nTo Jen who wishes to remain without last name, for\n\ntheir responsible consideration of representation,\n\nvoice, and the need to include community in creative\n\ndecision-making\n\nTo Frederik Svendsen whose coding adventures with\n\nChatGPT-4 have just begun\n\nTo Bill Zhao for endless and fearless experimentation\n\nwith all technologies approaching their development\n\nwith a sharpened purpose that it must solve a\n\nhuman problem\n\nTo Matt Yedlin who is making narrow AI much easier to\n\nunderstand\n\nTo Daniel Lindenberger who is passionate about\n\nbridging communities of practice to better understand\n\nthe implications of generative AI use, integration, and\n\nlarger projects\n\nTo Ollie Rankin for inspiring us with contributions\n\nto epic films whose incredible scenes were highly\n\ndependent on the AI systems that he co-developed,\n\nrefined over the years, and implemented\n\n385\n\nCHAPTER 13\n\nAI and the Future of\n\nCreative Work\n\nThis chapter explores the idea that the future of generative AI and other AI systems for creatives is already here as was predicted by many scholars\n\ndevelopers, and futurists for decades. The integration of generative AI and other AI with many existing jobs is already underway. While generative AI holds immense potential to automate certain routine tasks, it does not herald the complete elimination of all jobs. However, for those whose careers revolve around a series of repetitive, standard tasks that require minimal specialized skills, it’s critical to cultivate unique capabilities that AI can’t replicate. This principle extends to the realm of creation as well.\n\nManaging Automated Creativity\n\nIn a time where artificial intelligence is dominating our attention and being integrated within an increasing number of creative human processes, it’s easy to understand why people are concerned. However, we need to better identify those creative activities and tasks that are more likely to be replaced so we can better grasp how we will direct our own creative stories when it comes to generative AI now or in the future. Doing so requires breaking down the human experience of any creative job.\n\n© Patrick Parra Pennefather 2023\n\n387\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_13\n\nChapter 13 aI and the Future oF CreatIve Work Bread and Washing Machines\n\nAs a creative exercise, a first step is to task your AI to make predictions.\n\nThey may in fact be as accurate as the ones you read from humans who portend the end of human jobs and the extinction of the human race.\n\nAuthor: True or False? In the future economy, AI will have all the jobs in manufacturing white bread.\n\nMuse: False. It is unlikely that AI will completely replace all jobs in the future economy. While AI and automation will certainly change the nature of work and lead to job displacement in some industries, it will also create new job opportunities in fields such as AI development, data analysis, and cybersecurity. Additionally, there are many jobs that are inherently human and cannot be performed by AI, such as creative jobs, jobs that require emotional intelligence and empathy, and jobs that involve complex decision- making and problem-solving.\n\nAuthor: True or False? In the future economy, AI will have all the jobs in manufacturing white bread.\n\nMuse: True. It is likely that in the future, AI and automation will play a significant role in the manufacturing of white bread and similar products (Figure 13-1). T he repetitive and predictable nature of these tasks makes them well-suited for automation, allowing for increased efficiency and accuracy. However, it’s important to note that while some jobs in the bread- making industry may be automated, there will still be a need for human workers to operate and maintain the machines, manage the production process, and make strategic decisions.\n\n388\n\nChapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-1. A photo of my toaster yielded unpredicted results with the prompt “a robot learning to bake bread” as this collage shows. The AI gave up on me until the very last image in the bottom-right corner.\n\nIterations = 160 unfortunately, but it might just prove my point 389\n\nChapter 13 aI and the Future oF CreatIve Work At least you can plan ahead if you happen to work in bread\n\nmanufacturing. The fact that even an LLM with its vast data sets including access to future employment trends has difficulty with prediction is an indicator of the countless unknown unknowns that will impact humans in years to come. It is likely easier to predict human actions that are repetitive and motoric being replaced. Many creative tasks require very individualized contexts, approaches, and being informed by the training demanded by the discipline. When it comes to some creative jobs, a least one thing is for certain: AI will be not be able to find the nuances to replicate a rond de jambes that a ballerina repeats with infinite variation and muscle control over decades of daily practice no matter how advanced robotics becomes. However, there are some creative tasks that are repetitive and can be automated. The repeatability of some creative acts surfaces two themes that every creative needs to address. Doing so will better inform your own future actions when it comes to the choice of integrating generative AI in your own workflows.\n\nTwo Themes Every Creative Needs to Address\n\nThe first theme is that advances in narrow AI including generative AI surface challenges that human work cultures have been facing for a while, when it comes to assessing the role of creators in their workflow, the creative content they create, and how that content can support workflows, pipelines, brand, or value. Generative AI has not helped the creatives who have already been affected by the rise of the gig economy. Those part-time contract-based gigs have sent a signal that artists from more established creative disciplines like visual art, dance, theater, and music have known for centuries. There is no\n\none solid and secure job for anyone whose dominant way of making a living is to create content for other humans.\n\nWhile we might think that this is not the case for a 3D artist, UX\n\ndesigner, or programmer in a game company, layoffs are cyclic, and my own students and colleagues have been perpetually laid off and forced 390\n\nChapter 13 aI and the Future oF CreatIve Work to move on to other positions. Those creatives that are dependent on contract work are separated from the day-to-day needs and costs of the companies that usually contract them. For some creatives, the danger is in those people that hire them who believe that they can generate their own content with an AI. This may be the case if an employer believes that the content that creatives offer is on par with what an AI generates.\n\nCompanies are already relying on LLMs to write copy and weighing in favor of them to cut costs. Some creatives have lost jobs or contracts because of this. At the same time, after jumping the gun and thinking they can just use ChatGPT or other chatbots to replace employees or contractors, some companies are rehiring or re-contracting creatives when they discover how generic, untruthful, biased, damaging, and inaccurate LLMs can be. This type of seesawing with companies trying to cut costs will continue if the perception continues that generative AI is good enough at simulating human intelligence and creativity. It also speaks to the company culture, its structure, and how employees are treated.\n\nWhile reports of the subject may tell the story of generative AI coming after creative jobs that are nonroutine, it’s important for each of us to articulate those parts of our work that are not. The signal is clear that creatives need to re-evaluate the tasks they undertake and decide which routine ones a future AI might take over. If you are a creative who creates content that is mainly based on popular trends, frequently relies on overused story themes, or produces work that complies rigidly to specific genres, such as popular music styles or classic suspense sequences in television or film productions, then you need to start articulating what your value proposition is compared to a generative AI. Generative AI makes it easy enough for employers to do without contracting creatives.\n\nContent generated from an AI does offer more cost-effective content than, say, employing a scriptwriter who adheres strictly to traditional Hollywood- style scripting or even licensing stock images, footage, music, 3D art, or stories that some creatives contribute to content libraries.\n\n391\n\nChapter 13 aI and the Future oF CreatIve Work The second theme as has been espoused in this book is that even though the threat of generative AI might be present in some disciplines, it also presents an opportunity for innovation when used as a tool that augments your own creative potential. On its own, AI cannot perform this feat autonomously and lacks the vision, craft, technique, and experience that creatives possess. The technology requires human intervention to critically evaluate its outputs and use them as a springboard for challenging the existing norm or questioning prevailing artistic conventions. This approach mirrors the original intention behind the early integration of computers into artistic expression, which were envisaged not as usurpers of creativity but as allies in a shared quest to disrupt and redefine artistic paradigms. Governor-General Literary Award-winning Canadian playwright Kevin Kerr speaks to the necessity of the craftsperson in a different way:\n\nWhat’s intimidating is that the bot has all of Shakespeare and everything ever written about it available to draw from almost instantly, while I have to use my crappy memory and incomplete readings of the texts. But it’s also interesting to note how (currently) the AI is lacking a lot of nuances when it comes to style, and there’s a uniformity of voice that misses Shakespeare’s ability to reflect character in speech. And of course, it’s clear AI doesn’t know the rules around writing a sonnet.\n\nMost creatives who come with a large amount of work experience will find that generated prototypical content needs curating, editing, guiding, refining, manipulating, and at times radical transformation to make the most of it. That will call upon the skills that creatives have developed over many years of experience across different creative industries in specific contexts that are difficult to replicate. Creative impulses and the content that comes from them are prompted by context-specific cases and often involve a lot of back- and-forth between a contractor and a team leading the vision.\n\n392\n\nChapter 13 aI and the Future oF CreatIve Work Integrating AI into Your Workflows\n\nHow generative AI will affect us seems to be more oriented toward its use to support specific tasks and responsibilities that can be repeated. In many creative industries, creatives are still an essential part of production, where decision-making, problem-solving, and managing creative relationships are not easily replaceable by any machine or automated process. That is in part because of the ever-changing nature of productions and their unique demands.\n\nEvery sound design production for live theater, for example, has unique challenges that are dependent on the vision of the director, the script, how much support the actors need, the back-and-forth between a director and the designer, whether actors will need to be reinforced with microphones, the number of cues, the playback system, and budget.\n\nThere are rarely templated production processes across any human creative act because they also account for the unique meeting place for all the personalities that will contribute to the work. What speeds up the compositional process for my own work are automated plugins like equalization that can make minor or major adjustments to the frequencies of different sounds as they play (Figure 13-2). Ten to twenty-four tracks or layers of music can be automated and then rendered within less time it takes to play the whole track. My job as a sound designer and composer is not being replaced, but some of the tasks I had to do on my own in previous years are made easier. This gives me more time to try multiple mixes with radically different equalization for different tracks depending on what system the track is going to be played back on.\n\n393\n\nChapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-2. The author depicted using a photo of an out-of-order mechanical horse ride along with the prompt “racing to meet a deadline.” Iterations = 19\n\nAs creatives, what we might need getting used to is how generated content can inform our individual creative processes and to do that we need to better define what that creative process is. The best next step to take, if you haven’t already, is to break down all the tasks that you undertake in your respective discipline and apply a design thinking tool known as “Day in the Life.” The tool can be applied with any job you are afraid AI will replace. Like the 40 minutes that I recounted in the life of a photographer earlier in the book, it is useful to apply the tool to your own creative tasks by critically taking apart all the human actions required that you think an AI might replace.",
      "page_number": 496
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 505-512)",
      "start_page": 505,
      "end_page": 512,
      "detection_method": "topic_boundary",
      "content": "394\n\nChapter 13 aI and the Future oF CreatIve Work Day in the Life Steps\n\nTask Decomposition: Start by cataloguing all the tasks you perform to create your own content. For\n\nexample, if you’re making a video, this might include\n\nbrainstorming, scriptwriting, filming, editing, and\n\npublishing.\n\nStoryboarding: Utilize a storyboard to visualize\n\nevery step of the process. For instance, in animation,\n\nthis might involve sketching the flow of scenes and\n\ndialogues to provide a clearer understanding of the\n\nstoryline and sequence of actions.\n\nPeer Review: Share your process with a colleague\n\nor friend. This is beneficial as they might spot steps\n\nyou overlooked. If you’re developing a podcast,\n\nfor example, they might notice that you’ve missed\n\nincluding time for audio post-production.\n\nIdentifying Routine Tasks: Categorize tasks that are repetitive and can be performed almost unconsciously.\n\nIn the case of a blog writer, this might include\n\nformatting text or researching relevant keywords\n\nfor SEO.\n\nAI Research: Investigate what AI technologies currently exist that could automate some of these tasks. For\n\ninstance, a content writer could find AI tools that aid in\n\nkeyword research or grammar correction.\n\nAI Integration: Incorporate any relevant generative AI into your workflow. As a photographer, for example,\n\nyou might utilize AI tools for sorting and basic\n\nphoto edits.\n\n395\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWorkflow Refinement: Adjust your workflow\n\naccordingly and keep a record of it. Consider what\n\ntasks the AI managed effectively. If you used AI for\n\nautomated video editing, for instance, assess the\n\nquality of its output and the time it saved.\n\nIdentifying Gaps: Finally, determine the shortcomings of the AI-generated work. This will aid in demonstrating\n\nhow you incorporate AI in your workflow, what you\n\nneed to do post-generation, and where human input\n\nis still necessary. For example, while an AI might be\n\nable to create a draft for an article, human intervention\n\nmight still be necessary to add a personal touch, check\n\nfor contextual accuracy, and ensure overall coherence.\n\nDistinguishing Between Art and\n\nDesign Processes\n\nAs you continue to define your own creative process, to understand the impact of generative AI is to also consider what context it might be used within. This book has grouped together artists, designers, chefs, product owners, developers, and others all under the umbrella of “creatives.”\n\nWhile it is true that generative AI can support anyone who is creative no matter what they do, distinguishing its use to support artists vs. designers who work on teams is important. Generative AI can support artists to explore new forms, media, and stages, but many artists do not need the technology to continue to create their work. However, designers of all kinds can leverage generative AI for team-based workflows and pipeline integration. That places new demands on creatives to identify all the tasks they currently engage in and the ones that may depend on other team members to complete (Figure 13-3). Doing so they can better prepare to 396\n\nChapter 13 aI and the Future oF CreatIve Work either integrate generative AI or at least understand how it can help them and other team members they engage with. As an example, in video game development, AI has already been integrated across creative pipelines:\n\nGenerative AI can be used to prototype unique and\n\nimmersive game worlds, levels, characters, items, and\n\nmore. Minecraft and many other video games already\n\nuse procedural generation to create their vast, varied\n\nlandscapes. A benefit is that procedural generation of\n\na game world, for example, can save development time\n\nand resources, in the generation of vast game worlds\n\nto be created with relatively small file sizes, since the\n\nenvironments are generated in real time rather than\n\nbeing pre-rendered and stored. But content still has to\n\nbe curated, modified, tweaked, and optimized.\n\nGenerative AI models can be used to create more\n\nbelievable non-playable characters (NPCs) with varied\n\nbehaviors and responses, making the game world feel\n\nmore dynamic and alive. For instance, AI models can\n\nbe trained to generate dialogue or decision-making\n\npatterns for NPCs. Experiments integrating ChatGPT\n\nwithin game environments are already underway,\n\ncreating a richer gameplay experience with NPCs\n\nplayers can converse with.\n\nAI can also help in generating beginning story\n\nelements, quests, or dialogue that can then be refined\n\nby a narrative designer. Tools like ChatGPT can be used\n\nto generate unique plot line ideas based on existing\n\nstories, and these can be vetted with key members of\n\nthe team.\n\n397\n\nChapter 13 aI and the Future oF CreatIve Work\n\nGenerative AI algorithms can be used to prototype\n\ngame art assets. For example, AI could generate\n\ndifferent variations of character designs, weapons, or\n\nenvironments, based on the art style of the game.\n\nFigure 13-3. A future worker trying to juggle all the balls being thrown at it and becoming overwhelmed. Iterations = 40\n\nAs you entertain identifying co-dependent tasks you undertake with other team members to understand how generative AI can fit within existing workplace practices, you will benefit from first answering the following:\n\n398\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhich team members would most benefit from\n\nintegrating generative AI into team workflows? In\n\naddition, how much of their time will be required\n\nto practice generating content? Is it your marketing\n\nteam? Does generated code save an experienced\n\nprogrammer time, or is it more time to debug and\n\ncorrect faulty code?\n\nHow is creative work managed over time? This may\n\nlead you to identifying the type of project management\n\nmethodology your workplace uses. Agile sprints,\n\nfor example, allow for an increased flexibility in\n\nprototyping content over short time periods after which\n\nthey are reviewed and iterated upon for subsequent\n\nsprints. Generative AI is a useful complement to Agile\n\nprocesses.\n\nWhat level of experience do your team members\n\nhave with various productions? The variety of work\n\nexperiences on your team will be an asset when\n\nconsidering how to integrate generative AI. You\n\nwill have a broader perspective as to how AI can be\n\nintegrated.\n\nWhat experiments with generative AI have team\n\nmembers engaged in? Answering this question may\n\nlead to necessary workshops and onboarding of\n\nspecific generative AI that you and your team may want\n\nto implement. Team members may or may not have\n\nthe knowledge and know-how to be able to prompt a\n\ngenerative AI with the information required to perform\n\nthe desired task.\n\n399\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhat policies does your team or organization have\n\nwith using AI-generated content? If teams are creating\n\noriginal intellectual property (IP), then they need\n\nto understand the company’s position on using any\n\ngenerative AI content as many companies who allow\n\naccess to the content that users generate have specific\n\ncopyright policies in play that need to be respected.\n\nSome, like Midjourney, have specific rules that any\n\ncontent created by users can be used by all users.\n\nHow would work created with generative AI be\n\nreviewed? Answering that question may also lead to\n\nwho on your team would review that work, how it\n\nwould be assessed, and who would be responsible for\n\nits implementation.\n\nTakeaways for Creatives\n\nDefine all the creative tasks that you undertake in\n\nyour current role or in the role you want to have in a\n\nparticular industry.\n\nConduct research as to how any generative AI might be\n\nable to support the tasks you would undertake.\n\nCreate instructions that you think an AI would need to\n\nfollow to prompt something specific that you have the\n\nlived experience and know-how to deliver.",
      "page_number": 505
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 513-521)",
      "start_page": 513,
      "end_page": 521,
      "detection_method": "topic_boundary",
      "content": "Assess how much time it took to create those\n\ninstructions.\n\nDefine how complete the generated content would be\n\nright now, not in six months or five years.\n\n400\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhat review tasks would you need to undertake to\n\nassess the content that an AI generates, and how long\n\nmight that take?\n\nRun through a mock scenario and time it. See if a\n\ngenerative AI saves time, as the efficiency myth of\n\nusing AI is a common reason that is used to argue for\n\nits use in different industries. Include the time it would\n\ntake to provide the necessary inputs a generative AI\n\nwould need.\n\nGenerative AI provides valuable tools for designers, potentially augmenting team-based workflows and pipeline integration. To harness these capabilities, it’s important for creatives to thoroughly understand how AI can support their tasks and dependencies on teams. That clarity may help facilitate the incorporation of generative AI into their work processes when asked or, at the very least, help them appreciate how this technology can enhance their efficiency and collaboration with others.\n\nFuture AI Jobs Now\n\nNews that generative AI will disrupt all existing jobs and cause human extinction is overshadowing the reality that there is a growing market opening quickly for future jobs that involve generative AI. A valuable exercise for creatives is to research what those future jobs look like, to see if they might expand their own experience and develop the skills to take them on. Asking my creative companion is a starting place, bearing in mind the limitations it has and its capacity to tell the truth since it doesn’t always have its facts straight.\n\n401\n\nChapter 13 aI and the Future oF CreatIve Work Muse: Sure, I can help with that.\n\nGenerative AI Engineer : An engineer who develops,\n\nimplements, and maintains generative AI models.\n\nThese professionals can work in various industries such\n\nas advertising, gaming, architecture, and more. For\n\nexample, a generative AI engineer in a fashion company\n\ncould develop models that generate new clothing designs.\n\nAI Art Director : This role involves using AI, particularly generative AI, to create visual art for advertising,\n\nentertainment, or digital media. For instance, an AI art\n\ndirector might use AI algorithms to generate art for a\n\ndigital marketing campaign or a video game.\n\nAI Content Creator : These professionals use AI to\n\ngenerate different types of content, such as blog posts,\n\nsocial media updates, or product descriptions. For\n\nexample, an AI content creator could use AI models to\n\ngenerate blog posts on specific topics.\n\nAI Music Composer : AI is increasingly being used in\n\nthe music industry to generate new compositions. An\n\nAI music composer could use generative models like\n\nOpenAI’s MuseNet to compose new pieces of music.\n\nAI Game Designer : These professionals leverage\n\ngenerative AI to create game elements such as\n\nenvironments, characters, storylines, and more. For\n\ninstance, an AI game designer might use procedural\n\ngeneration to create diverse and vast game worlds.\n\nAI Data Scientist (Generative Models) : They build and\n\ntrain generative models on large data sets to generate\n\nnew, synthetic data. This role could be particularly\n\n402\n\nChapter 13 aI and the Future oF CreatIve Work useful in healthcare, where AI data scientists could\n\ngenerate synthetic patient data that preserves privacy\n\nwhile allowing for robust research and development.\n\nAI Architect (Generative Design) : Professionals\n\nwho use generative AI in fields like architecture or\n\nindustrial design. For instance, an AI architect might use\n\ngenerative design tools to create efficient and innovative\n\nbuilding designs.\n\nGenerative AI Researcher : These professionals conduct\n\ncutting-edge research in generative AI, developing new\n\nalgorithms and models. They might work in academia\n\nor in the research departments of large tech companies.\n\nAI Ethics Officer : With the growing use of AI, ethical\n\nconsiderations are becoming increasingly important. AI\n\nethics officers are needed to ensure that generative AI is\n\nbeing used responsibly and ethically and that it aligns\n\nwith laws and regulations.\n\nAI User Experience (UX) Designer : These professionals\n\nintegrate generative AI into user interfaces and\n\nexperiences. For example, an AI UX designer might\n\ndesign a system that uses AI to personalize the user\n\ninterface based on the user’s behavior and preferences.\n\nConducting more research can reveal other types of jobs that are emerging from the technology and its popularity at the moment\n\n(Figure 13-4). Discerning what these positions are, and the training that would be involved to take on these jobs, involves you to undertake online research and particularly when you search any knowledge area and attach the letters A and I after it. Here are a few others to consider conducting my own research:\n\n403\n\nChapter 13 aI and the Future oF CreatIve Work\n\nPrompt engineers or those individuals who are good at\n\ngetting consistent content from a generative AI through\n\nthe craft of using words. If you recall how generative\n\nAI works, however, it is good to understand that every\n\ngeneration of content is unique, and you can’t really\n\npredict if you’ll get an extra finger here or there.\n\nAI content editors are already reviewing what an LLM\n\ngenerates and editing to make it sound more human,\n\nless homogeneous, and more accurate. AI content\n\neditors will not be limited to the generated word, as\n\nthere is a growing appetite for generative images that\n\nneed curating.\n\nAI consultants are inundating web searches with their\n\ngenerated expertise in terms of how to integrate AI\n\nin the workplace. As alarming as this sounds, some\n\ngood may come out of it because regardless of how\n\nknowledgeable they are, they make us aware that we\n\nmay have to deal with AI as a trend or expected work\n\ntool in our workplace environments.\n\nDiscipline-specific AI curators will be those individuals\n\nwho have gone far with how they integrate generative\n\nAI in their own workflows and can therefore rapidly\n\ngenerate useful content for others in addition to\n\nteaching colleagues how to apply generative AI.\n\nThe position of a machine learning (ML) engineer will\n\nbe predictably on the rise, particularly in IT support for\n\ncompanies and institutions. Organizations rich in data\n\nneed to take command over that data and leverage it\n\nto become the corpus for their own LLM or text-image,\n\nimage-image, text-video, or video-video generative AI.\n\n404\n\nChapter 13 aI and the Future oF CreatIve Work\n\nHint: All the tools, processes, and support are out there\n\nto achieve this on your own. Learning how to do this\n\nin addition to the tasks you already have is a form of\n\nfuture-proofing your position when and if cuts come.\n\nFigure 13-4. A cyborg AI awkwardly pointing to you with the text beneath translated as “We need you.” Iterations =27\n\n405\n\nChapter 13 aI and the Future oF CreatIve Work Acknowledgments\n\nTo those artists and improvisers who have inspired us\n\nto not conform or accept established patterns of art,\n\noffering alternatives to the same old and provoking us\n\nto do the same\n\nTo the current version of our AI-powered muses who\n\ncontinue to astonish and provoke and whose very\n\nexistence brings to the surface pervasive challenges\n\nwe face as a community of humans who engage with\n\nintelligent machines with curiosity, fear, wonder, and\n\nexperimentation (Figure 13-5)\n\nTo the activists and image warriors, coalitions, and\n\npolicy developers who are working hand in hand with AI\n\ndevelopers on what at times seems like a runaway train\n\nTo writers like Kevin Kerr whose skill and craft at\n\nwriting will be needed more than before as striking\n\ncontrast to the lack of nuance and human experience\n\nthat any AI will only be able to simulate\n\nTo those embracing new opportunities with generative\n\nAI and integrating it more and more in their workflows,\n\nshowing creatives the advantages of doing so\n\nTo employers who are carefully evaluating how generative\n\nAI can support existing workflows and how they can\n\nsupport and guide creatives they contract or current\n\nemployees to best take advantage of these creative tools\n\n406",
      "page_number": 513
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 522-529)",
      "start_page": 522,
      "end_page": 529,
      "detection_method": "topic_boundary",
      "content": "Chapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-5. A final image using Photoshop’s General Fill to create the torso and legs of a creative juggling its own perspectives and ideas about generative AI\n\n407\n\nChapter 13 aI and the Future oF CreatIve Work Impossibly Generated Conclusions\n\nThe frustrating characteristic of generative AI is that it keeps up faster than anyone writing about it. By the time you read this book, new advances will make many of the examples of generative AI that are referenced obsolete.\n\nHow would I have known that “Stable Attribution,” the site devoted to tracking down the images that went into Stable Diffusion and its generated images, would close its doors within months of its discovery. It’s important therefore to understand that there will be generations of AI just like any other prototype, and they will continue to add features over time.\n\nDevelopment teams will continue to claim that content generated from first prompts will start to look better and better, feel more human, and will offer an improved reading experience. More job losses will be announced.\n\nMore mistakes made by generative AI will be cited because there was no human to review the content. While some AI-generated content may already feel like a higher-fidelity prototype, this book maintains that whatever is generated is best regarded as a prototype, a version of your initial prompted idea that you can apply your craft and skills to improve before sharing with the world. In whatever way the technology evolves, keep in mind that as good as it might get, the features that AI offers can always be better.\n\nOne important takeaway that I’ve repeated several times is to consider how and where generative AI fits into your own creative process. As imperfect as initial offerings might be, on the short horizon, generative AI is set to at least offer some improved features like higher image resolution and hands with less fingers. We are already starting to see web portals dedicated to showcasing generated art that creatives use as the first step in their creations.\n\nCreatives on teams across industries are also starting to explore generative AI into their workflows, especially in the beginning phases of a project. Anticipating that there might be an expectation for you to know how to use AI on future creative teams, it may benefit you to integrate generative AI within your own workflows.\n\n408\n\nChapter 13 aI and the Future oF CreatIve Work Will a small minority of humans with bad intentions use generative AI and other AI to do harm to others? Yes. That shouldn’t stop your use of it though, especially if your intent is to offer unique creations that criticize and disrupt established forms and structures. It is a great time to leverage generative AI to create unique work.\n\nThe journey of our interaction with generative AI promises to be an iterative and rewarding one that will continue to provoke and challenge how we prototype. The technology challenges us to stay on our toes: to critically examine the implications of interacting with generative AI and the results of those interactions as prototypes that influence what we share with the world. Bear in mind that many ethical concerns and fears of AI controlling human tasks and roles are still not resolved and need to be.\n\nMy hope is that in reading this book you have increased your awareness of how generative AI works so you can make more informed decisions as to how you might one day use it.\n\nI leave the final words to our muse.\n\nAuthor: How would you end a book on using generative AI as a creative prototyping tool?\n\nMuse: Therefore, since brevity is the soul of wit\n\nAnd tediousness the limbs and outward flourishes,\n\nI will be brief.\n\nAs we reach the concluding lines of this exploration into generative AI as a creative prototyping tool, we recognize the astounding possibilities it unlocks. The world of design is at an extraordinary juncture, with our creative potential being broadened and deepened by this revolutionary technology. Generative AI will continue to evolve, its capabilities expanding in ways we can only begin to imagine. As creatives, it is incumbent upon us to stay abreast of these developments, to adapt and adopt, to continue pushing the boundaries of what we believe is possible. The power of generative AI isn’t about replacing human creativity—it’s about augmenting 409\n\nChapter 13 aI and the Future oF CreatIve Work it, helping us find novel solutions and innovate in ways that were previously unreachable. It’s an exciting tool in our toolkit, one that offers limitless potential for the brave creative willing to explore its depths.\n\nThis book aimed to demystify generative AI, to guide you through its intricacies, and, hopefully, to inspire you to harness its potential. As we wrap up this journey, it’s up to you to leverage these insights, to experiment, to fail, and to succeed. Remember, it’s your creativity that breathes life into these tools. Generative AI is merely a means to an end—a powerful aide in the relentless pursuit of innovation and excellence in design. As we embrace this new era, remember that your ideas are the driving force behind the change. Your imagination, your inspiration, your artistry—these are the elements that give shape to AI’s potential.\n\nSo, as you turn the final page, don’t view it as an end, but rather a new beginning. With generative AI in your hands, you’re not just a designer—\n\nyou’re Mary Shelley, Ava Lovelace, Joan Clarke, Alan Turing, Sun Ra on the frontiers of creativity. Embrace this challenge with curiosity, discernment, and the courage to create the unimagined.\n\n410\n\nAPPENDIX\n\nImage and Text\n\nSources\n\nLLMs were used to generate the muse’s text. That generated text was edited, refined, and adapted for all the scripted sequences in every chapter. Text was generated using the popular LLMs known as ChatGPT-3\n\nand ChatGPT-4 along with OpenAI’s Playground. All the text was highly edited for theatrical effect, and some was completely rewritten.\n\nMuse: I thought so.\n\nAll Shakespearean text was written using the corpus of Shakespeare’s plays and sonnets through a combination of ChatGPT-4 and plays directly adapted from Cleopatra, MacBeth, Othello, The Merchant of Venice, Romeo and Juliet, and A Midsummer Night’s Dream. The Droeshout image of Shakespeare was accessed through the University of British Columbia Library’s copy of Shakespeare’s First Folio.\n\nApart from using a handful of public domain images confirmed\n\nthrough archival websites and public domain sites requiring no\n\nattribution, most of the figures used in this book were generated from sources of the author’s own photos, bad drawings, and visual models.\n\nThese were used in image-to-image generative AI with basic text prompts,\n\n© Patrick Parra Pennefather 2023\n\n411\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nleveraging the platform’s styles/filters and other features to radically transform them beyond recognition in all cases. Stable Attribution and months of photo hunting brought to the attention of the author some specific images whose essential parts were too close to existing sources.\n\nThese were not included in the book and deleted from the author’s computer. At least a dozen different AI were experimented with, applying platform- specific filters. These were further edited with added objects, color corrected, and further refined using licensed versions of Photoshop, Photoleap, Snapseed, and Mixlr. Neural filters from Photoshop were used extensively. Real photos in the text were all taken by the author.\n\nSome exceptions include generated images that were meant to\n\nillustrate basic concepts and draw comparison between text-image platforms, particularly in Chapter 8, to show differences in how images were generated with the exact same prompts. Those images were created in Stable Diffusion, DALL-E 2, and Midjourney. Some were sent through AI filters, and a few were modified in DALL-E 2 to show the technique of outpainting. Other generated images were the result of prompting Scribble Diffusion with an original sketch. Any images that underwent a small number of regenerations were used to illustrate the functionality of these platforms and were intentionally not rendered or upscaled in high fidelity.\n\nPrompts were mainly original with some common words generated\n\nin natural language models and others modified from open sharing sites. Complex prompts were found on various web portals and meant to show how elaborate some prompts can be. No prompts were paid for or licensed.\n\nWalk Through Figure 2, FrankenAI Lab\n\nThe original prompted image was a photo taken by the author to capture windows and a particular perspective of a room imagined to be a scientific lab.\n\n412\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nThe photo in Figure A-1 was uploaded to an image-to-image AI with the prompt “reverse-dolly-zoom-of-a scientific-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and-white-still-digital,” and the result was four variations as in Figure A-2. Reducing the influence of the prompt photo will generate more interesting results, and increasing the values of how much the AI responds to the text prompt will also make a difference. In this case the prompt guidance was set to just over 50% and the influence or strength of the prompt image to around 30%.\n\nFigure A-1. Original photo of the author’s office using an iPhone 413\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-2. Four images generated from the prompt “reverse-dolly-zoom- of-a scientific-lab-with-lots-of-windows-microscopes-and-lab-equipment- black-and-white-still-digital.” Total iterations = 56\n\nFrom a total of the four selected images that were generated in Figure A-2, the thumbnail in the bottom right was chosen as it added more windows and a wider zoomed-out perspective to increase the size of the space compared with the original photo.\n\n414\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-3 prompted the next generated images setting the strength of the prompt image on the generated one at 30% and setting the number of steps to generate a higher-quality image at 97. In addition, I added the keywords “Frankenstein, electricity, and misty” to the prompt.\n\nFigure A-3. The bottom-right image in Figure A-2 i s used to prompt the next set of four generated images along with the text prompt\n\n“reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-electricity-misty.”\n\n415",
      "page_number": 522
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 530-537)",
      "start_page": 530,
      "end_page": 537,
      "detection_method": "topic_boundary",
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFrom the four generated images in Figure A-4, the thumbnail on the bottom right was chosen particularly because of the combination of top window, mist, and wiring not previously present in Figure A-3. At the same time, the previous windows and the size of the lab were preserved, adding a small library and a lower-right window, creating a more surreal effect.\n\nThe lack of electricity in the images had to with their weight in the prompt, which I would come to experiment with multiple times in the iterative process.\n\nFigure A-4. Prompt: “reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots- of-windows-microscopes-and-lab-equipment-black-and white-electricity- misty”\n\n416\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nConsidering the lack of electricity in Figure A-5, I worked with the text prompt to prioritize it in the next round of generated images. In addition, now that the room size and perspective were working, I could remove the words “reverse dolly zoom.”\n\nFigure A-5. Prompt: “reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots- of-windows-microscopes-and-lab-equipment-black-and white-electricity- misty”\n\nFrom the eight generated images in Figure A-6, the bottom-left thumbnail had all the elements and expanded the bottom windows. That image was then chosen as the next prompt for the AI (Figure A-7).\n\n417\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-6. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty”\n\n418\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-7. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-bones-profane fingers-scraggy-limbs”\n\nThe only thing missing from my lab were body parts if I stayed true to Mary Shelley’s novel. Even though I wasn’t sure if I wanted them in the image, the next generated images added words from Chapter 4 of\n\nFrankenstein to the prompt as in Figure A-8. The results were a bit over the top and grotesque, which reminded me of why I was creating this image.\n\nIn the case of a critical how-to book aimed at creatives, the intent of the final generated image was to show the process and use Frankenstein’s lab as metaphor for a reader’s own generative AI lab. The next series of four figures showed an iterative attempt to change the prompt text using Figure A-7 as the prompt image.\n\n419\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-8. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-bones-profane fingers-scraggy-limbs”\n\nSince all the images in Figure A-8 were scary and grotesque, I used the same image in A-7 but refined the prompt to eliminate the words “bones, profane fingers” but kept the “scraggy limbs” in the prompt (Figure A-9).\n\n420\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-9. An attempt to regenerate by subtracting some body parts. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-scraggy-limbs”\n\nExcept for the super-creepy top-left image in Figure A-10, I was getting closer to a look that didn’t have to be scary like Frankenstein’s lab. In fact, the reference to Frankenstein was next on the subtraction block (Figure A- 11).\n\n421\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-10. Substituting scraggy limbs with the word “specimens.”\n\nPrompt: “electricity-Frankenstein’s-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-specimens”\n\n422\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-11. Making the word “specimens” singular. Prompt:\n\n“electricity-lab-with-lots-of-windows-microscopes-and-lab-\n\nequipment-black-and white-misty-specimen”\n\nFigure A-11 demonstrated the potential for any of these images to be close to the lab that I was starting to imagine. However, somehow the original perspective and size of the lab were lost, so an iteration of four images was generated reverting back to Figure A-7 and using the same text prompt as Figure A-11 but substituting the word “specimen” for the words\n\n“brain in a bottle” (Figure A-12).\n\n423",
      "page_number": 530
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 538-549)",
      "start_page": 538,
      "end_page": 549,
      "detection_method": "topic_boundary",
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-12. The prompt “electricity-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-brain in a bottle,” combined with Figure A-7 as the pr ompt image, resulted in the AI generating our windows back\n\nThe images in Figure A-12 tended to be less grotesque, but there was little color as these were generated using a black-and-white style applied through playground.ai. To bring some color into the next batch of generated images, a new style was applied to the same prompt using Figure A-7. Figures A-13 and A-14 provided us with some blues to not make the lab so ominous.\n\n424\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-13. Prompt: “electricity-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-brain in a bottle” with a new style filter\n\n425\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-14. Prompt: “electricity-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-brain in a bottle” with another new style filter using playground.ai\n\nThe bottom-right image of Figure A-14 added a bit of dimension to the lab that made certain elements stand out, so this image was chosen, and then a variation of the image was rendered using another style that could be applied while the new image was being generated. The resulting Figure A-15 revealed the complexity of the style that was applied to generate the image.\n\n426\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-15. This shows the complexity of the full prompt when adding a specific style available through a number of generative AI platforms. Notice the references to other artists that our final prompt would eliminate\n\nWhile the generated image is interesting, the bottom-right thumbnail from Figure A-14 was chosen to represent the AI lab.\n\nWhile Figure A-16 was a wonderful result from where the journey started using an old photo of an empty office, somehow the electricity and sparks went missing from this generated version. Our final image then was generated by adding the text “electricity sparks and lightning” at the beginning of the prompt to ensure it showed up in the generated image (Figure A-17).\n\n427\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-16. Generated image for the prompt “electricity -lab-with-lots-of- windows-microscopes-and-lab-equipment-black-and white-misty-brain in a bottle” seems to missing some electricity\n\n428\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-17. The AI brought back the electric sparks to the image by adding “electricity-sparks-lightning” at the beginning of the prompt\n\n“lab-with-lots-of-windows-microscopes-and-lab-equipment-black-and white-misty-brain in a bottle.”\n\nLastly, Figure A-17 was manipulated using Photoshop’s neural filters to generate slightly more haze, and some subtle color was added to the image. Figure A-18 now represents the final version of the figure, which appears in the Foreword of the book (Figure 2).\n\n429\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-18. Figure A-17 was opened up in Photoshop, and various neural filters were applied to generate the final version of our photograph. The preceding image is then used in the Foreword of the book (Figure 2)\n\nWhile not all images in the book go through the exact same workflow as described in this appendix, the process of generating an image iteratively was similar for many images in the book. More workflows will be demonstrated on the website that accompanies the book available through a QR code that appears at the end of the Introduction (Figure 6).\n\n430\n\nIndex\n\nA\n\nApplication Programming\n\nInterfaces (APIs), 334, 339\n\nAARON, 52\n\nArtificial creativity, 46\n\nAcademic community, 350\n\nArtificial General Intelligence, 3\n\nAccelerated prototypical\n\nArtificial intelligence (AI), 17, 30,\n\nworkflows, 339\n\n60, 182, 183, 193, 284,\n\nAgile sprints, 399\n\n302, 318\n\nAI companion, 99\n\nadversarial, 281, 283\n\nboundary-pushing, 101\n\nalgorithms, 317\n\ncollaboration, 101\n\narchitect, 403\n\npatterns, 101\n\nart director, 402\n\nrisk-taking, 101\n\nart generation styles, 255, 256\n\nspeeding, 101\n\nBread-making industry, 388\n\nstyles, 101\n\nchatbot, 350\n\nAI in film industry\n\nconsultants, 404\n\nbehavioural simulation, 382\n\nContent Creator, 402\n\ncombinatorial probability, 382\n\ncontent editors, 404\n\nmultiple intelligent systems, 383\n\ncould, 398\n\nOllie Rankin, 381\n\nData Scientist, 402\n\nrealistic-looking digital\n\ndevelopers, 129\n\ncharacters, 382\n\nethical futures, 335\n\nWeta, 381\n\nEthics Officer, 403\n\nworkflows, 381, 382\n\nfilters, 412\n\nAlgorithmic creativity, 54\n\nGame Designer, 402\n\nAlienation effect, 275\n\ngenerated content, 188, 278,\n\nAnalytical Engine, 6\n\n292, 300, 339\n\nAnimation, 128, 160, 242, 356, 383\n\ngenerated images, 250\n\n© Patrick Parra Pennefather 2023\n\n431\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3\n\nINDEX\n\nArtificial intelligence (AI) ( cont.)\n\nB\n\ngenerated prototypes, 366\n\nBattle choreography technique, 381\n\ngenerated speech, 343\n\nBehavioral simulation, 383\n\ngenerated submissions, 333\n\nBias Monster, 71\n\ngenerated talking heads, 362\n\nBill Zhao, 375, 385\n\ngenerated works, 314\n\nBiomedical communication, 350\n\nlanguage model, 349\n\nBisociation, 113, 115\n\nMusic Composer, 402\n\nBlockchain technology, 263, 264\n\npowered muses, 406\n\nBlue Shadows on the Trail, 358\n\nreader, 102\n\nBrain building technique, 381\n\ntechnology, 328\n\nBuilding blocks\n\nText Classifier, 311, 312\n\naddition, 154, 155\n\nUser Experience (UX)\n\nprompt, 146\n\nDesigner, 403\n\nprompts, 177, 178\n\nArtificially intelligent programmed\n\nprototyping, 147, 148\n\nsystems, 8\n\nresponse, 176\n\nArtmaking, 28, 49, 97, 235\n\nsubstitution, 158\n\nAudio-based prompts, 241\n\nsubtraction, 154–156\n\nAugmentation, 163, 164\n\ntransposition, 166–175\n\nAugmented reality (AR), 122,\n\n233, 345\n\nAutomated creativity management\n\nart vs. design\n\nC",
      "page_number": 538
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 550-563)",
      "start_page": 550,
      "end_page": 563,
      "detection_method": "topic_boundary",
      "content": "processes, 396–400\n\nChain prompting, 200, 204, 262\n\nbread and washing\n\nChatGPT, 391\n\nmachines, 388–390\n\nChatGPT-3, 239, 242, 300, 346, 350,\n\nDay in the Life\n\n351, 411\n\ntools, 395, 396\n\nChatGPT-3-generated text, 311\n\nintegrating AI and workflows,\n\nChatGPT-4, 242, 264, 268, 344, 346,\n\n393, 394\n\n375, 411\n\nthemes, 390–392\n\nChinese broadcast company, 345\n\nAward-winning innovations, 346\n\nClaudia’s Summary Statement, 350\n\n432\n\nINDEX\n\nCoalition for Content Provenance\n\nCreative impulses, 103, 247, 392\n\nand Authenticity\n\nCreative process\n\n(C2PA), 312\n\nadaptive learning, 104\n\nCo-dependent tasks, 398\n\nconsistency, 105\n\nCoherent thesis statement, 352\n\nidea generation, 104\n\nCombinatorial probability, 382, 384\n\ntime efficiency, 104\n\nCommunity-based initiatives,\n\nvariety of styles, 104\n\n377, 378\n\nCreative prompting, 239–241\n\nComposite-generated image, 308\n\nCreatives, 396, 406, 408\n\nComposition, 184, 274, 284,\n\nCreatives in education\n\n335, 393\n\ncatching AI untruths, 346–351\n\nComprehensive Test Ban Treaty\n\nintegrating ChatGPT, critical\n\nOrganization (CTBTO), 353\n\nstudies, 351, 352\n\nComputational creativity, 54,\n\npublic understanding,\n\n285, 344\n\nml, 352–354\n\nComputational intelligence, 45\n\nCreatives in industries\n\nComputational workhorse, 354\n\nAI-generated talking\n\nComputergrafik, 264\n\nheads, 362–365\n\nComputer science, 6, 66, 353\n\nAPIs and local networks\n\nContent Authenticity Initiative\n\nintegration, 373–376\n\n(CAI), 312\n\nconcept art, animation, 356–361\n\nContent generation, 300, 329\n\nfact-checking code, 366–373\n\nContent moderation, 374\n\nCreative teams, 9, 335, 408\n\nContrastive Language-Image\n\nCrowd replication approaches, 382\n\nPre-training (CLIP),\n\nCryptocurrencies, 263\n\n201, 205\n\nCybernetics, 46\n\nControversial start-ups, 313\n\nCyborg AI, 405\n\nConversational and authoritative\n\nCyborg generated model, 322\n\ntone, 350\n\nConversational prompting, 199\n\nD\n\nCopyright infringement, 300\n\nCopyright Infringer, 81\n\nDALL-E 2, 164, 204, 205, 211–213,\n\nCost-effective content, 391\n\n221, 285, 286, 412\n\nCounterfeiting GAN, 264, 266\n\nData management, 374\n\n433\n\nINDEX\n\nDay in the Life tool, 394\n\nEthical dilemmas, 122, 207, 300,\n\nAI integration, 395\n\n314, 341\n\nAI research, 395\n\nExploratory prompting, 200\n\nidentifying gaps, 396\n\nidentifying routine tasks, 395\n\nF\n\npeer review, 395\n\nstoryboarding, 395\n\nFake Content Generator, 74\n\ntask decomposition, 395\n\nFew-shot stimulation, 199\n\nworkflow refinement, 396\n\nFidelity, 99, 125, 160\n\nDeep fakes, 276, 295, 325, 326\n\nFinancial and social capacity, 300\n\nDeforum, 375\n\nFive Why’s exercise, 21, 23, 24\n\nDescriptive prompts, 236–238\n\nForms and structures\n\nDevelopment teams, 9, 408\n\nAI, 182\n\nDiffusion, 212\n\ncombining and manipulating\n\nDigital art, 54, 263\n\nelements, 183\n\nDigital art, canvas, 250, 251\n\ncreatives, 182\n\nDigital artworks, 55\n\noptimization, 183\n\nDilemmas, 299–301, 304, 335, 336\n\npattern recognition, 183\n\nDiminution, 165\n\nFrankenAI, 327, 328\n\nDiscipline-specific\n\nFrankenAI Lab\n\nAI curators, 404\n\nimage-to-image AI, 413\n\nDomain images, 411\n\nprompt text 1, 414\n\n“Do Not Train” tags, 312\n\nprompt text 2, 415\n\nDoujinshi, 257–261\n\nprompt text 3, 416\n\nDystopian future\n\nprompt text 4, 417\n\nscenarios, 355\n\nprompt text 5, 417, 418\n\nprompt text 6, 419\n\nprompt text 7, 420\n\nE\n\nprompt text 8, 421\n\nEmotion matrix, 383\n\nprompt text 9, 421, 422\n\nEmployment trends, 390\n\nprompt text 10, 423\n\nEnigma machine, 45\n\nprompt text 11, 424\n\nEqualization, 154, 393\n\nprompt text 12, 425\n\nError handling, 374\n\nprompt text 13, 426\n\n434\n\nINDEX\n\nprompt text 14, 427\n\nart generation styles, 255, 256\n\nprompt text 15, 427, 428\n\naudio, 152\n\nprompt text 16, 429\n\ncomposers, 145\n\nprompt text 17, 430\n\ncontent, 306\n\n“Frankenstein, electricity, and\n\ncreative activity, 20\n\nmisty”, 415\n\ncreative process, 10\n\nFrankentoddlers, 82\n\ncreative prototyping tool, 409\n\nFunctional prototype, 129, 131\n\ndancers, 145\n\nFuture AI Jobs\n\nengineer, 402\n\nAI Art Director, 402\n\nexamples, 20\n\nAI Content Creator, 402\n\nhuman-generated headlines, 18\n\nAI Data Architect, 403\n\nillusion, 10\n\nAI Data Scientist, 402\n\ninnovation and excellence, 410\n\nAI Ethics Officer, 403\n\nmimesis, 47\n\nAI Game Designer, 402\n\nmusicians, 145\n\nAI Music Composer, 402\n\nnegative headlines, 18\n\nGenerative AI Engineer, 402\n\npositive headlines, 17\n\nGenerative AI Researcher, 403\n\npatterns, 300\n\nUX Designer, 403\n\nprototypes, 335\n\nFuzzy logic, 382, 384\n\nresearcher, 403\n\ntechnology, 10\n\nG\n\ntext-image, 152\n\ntext-music, 41\n\nGame companies, 376\n\ntext-to-music, 41\n\nGenerated AI response, 303\n\nvisual artists, 145\n\nGenerated prototypical\n\nwriting, 251–253\n\ncontent, 392\n\nGenerative art, 49, 50, 263, 266\n\nGenerated stories, 16\n\nGenerative fill, 340\n\nGenerative adversarial art, 283, 284\n\nGenerative NFTs, 263, 264, 283\n\nGenerative adversarial networks\n\nGenerative Pretrained Transformer\n\n(GANs), 256, 263, 264,\n\n(GPT), 163, 353, 355\n\n266, 282–285\n\nGenerator, 283\n\nGenerative AI, 7, 8, 10, 182, 183,\n\nGesture prompts, 242\n\n185, 235, 249, 268, 287, 292\n\nGig economy, 390\n\n435\n\nINDEX\n\nGPT-4, 236\n\nImage-image substitution, 358\n\nGPT-4 API, 373–374\n\nImage-to-image generative AI, 411\n\nGraphic novel mashups, 257–261\n\nIndigenous researchers, 336\n\nGreedy Capitalist, 75\n\nIndividual creative processes, 394\n\nInstructional prompting, 199\n\nH\n\nIntegration, 374\n\nIntellectual property (IP), 400\n\n“Half-tone” technique, 348\n\nIntelligent machines, 4, 6\n\nHallucinations, 149, 314, 317, 324\n\nancient musical robot\n\nHallucinator, 72\n\nbands, 35–37\n\nHigher-resolution prototype, 124\n\nautomata, 32–34, 38, 39\n\nHigh-fidelity (hi-fi) images, 122\n\nCamera, 28\n\nHigh-fidelity prototypes, 125, 131\n\nComputer, 29\n\nHuman intelligence, 6, 30, 32,\n\nFilm, 29\n\n46, 66, 328\n\ngames, 56, 58\n\nHuman intervention, 9, 19, 129, 392\n\nhuman creative, 32\n\nHuman-like AI, 59\n\nInternet, 29\n\nHuman-like communication, 19\n\nPaint, 28\n\nHuman-like creativity, 32\n\nPaper, 28\n\nHuman-like robots, 277\n\nPrinting Press, 28\n\nHyena Algorithm, 356\n\nStone Tools, 28\n\nHyena Algorithm developed by\n\nSynthetic Paint, 29\n\nStanford, 354\n\n3D printing, 29\n\nHyperreality, 47\n\nWriting Instruments, 28\n\nIntelligent NPCs, 58, 59\n\nI\n\nIntelligent systems, 384\n\nInteractivity, 55\n\nImage analysis, 354\n\nInterdisciplinary research, 353\n\nImage-based generative AI, 290\n\nInternational Atomic Energy\n\nImage-based prompts, 242\n\nAgency (IAEA), 353\n\nImage-image AI, 96, 98, 175, 249,\n\nIn-the-moment improvisation, 329",
      "page_number": 550
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 564-571)",
      "start_page": 564,
      "end_page": 571,
      "detection_method": "topic_boundary",
      "content": "251, 257, 261, 269, 365\n\nIteration, 160\n\nImage-image generative AI, 44,\n\nIterative process, 30, 55, 176,\n\n106, 169, 217, 293\n\n204, 416\n\n436\n\nINDEX\n\nJ\n\nMachine learning (ML), 58, 67, 92,\n\n248, 290, 303, 307, 311, 317,\n\nJob displacement, 388\n\n320, 321, 336, 384, 404\n\nJob opportunities, 388\n\nMachine Visions, 21\n\nJob (Re)placement, 328–332\n\nMany-shot prompting, 199\n\nMarginalized groups, 321\n\nK\n\nMasher-Upper, 76\n\nKnow-it-All Tutor, 79\n\nMashups, 306, 315\n\nKnowledge translation, 354\n\nAI art generation styles, 255, 256\n\ncomic supervillains to musical,\n\n262, 263\n\nL\n\ncounterfeiting GAN, 264, 266\n\nLanguage models, 93, 322, 323\n\nDoujinshi, 257–261\n\nLarge language models (LLMs),\n\ngenerative AI, 248\n\n197, 198, 201, 216, 239, 268,\n\ngenerative NFTs, 263, 264\n\n287, 353, 355\n\ngoal, 247\n\nLate-stage prototypes, 121\n\ngraphic novel, 257–261\n\nLearning management\n\nmachine learning model, 249\n\nsystems, 376\n\nmedia, 247, 249\n\nLGBTQ+ community, 321, 322\n\nprompt-based magic, 247\n\nLiar, 73\n\nMasks, 159, 340\n\nLinear algebra pattern\n\nMasterpiece Studio, 344\n\nmatching, 353\n\nMechanical inventions, 43\n\nLittle Curie, 347\n\nMidjourney, 204, 205, 213–218, 220,\n\n“Living with Machines” project, 377\n\n222, 287, 293\n\nLLM-generated content, 335\n\nMid-stage prototypes, 121\n\nLower-fidelity prototypes, 125, 128\n\nMimesis, 47\n\nLow-fidelity, 126\n\nMinecraft, 397\n\nLow-poly render, 305\n\nMiniature automata, 33\n\nMiniature X-ray apparatus, 347\n\nModern computational fortune-\n\nM\n\ntellers, 4\n\nMachine intelligence, 32, 45,\n\nMontreal Institute for Learning\n\n48, 56, 384\n\nAlgorithms (MILA), 354, 356\n\n437\n\nINDEX\n\nMotion capture, 324, 344, 382\n\nO\n\nMulti-modal AI nodes, 375\n\nOff-the-Rail Sitting Duck, 80\n\nMultiple machine learning models,\n\nOpenAI’s API, 377\n\n204, 339, 365\n\nOpen source code repositories, 373\n\nMultiple variations, 150\n\nOptimization, 183\n\nMuses, 91, 93\n\nOutpainting technique, 163, 412\n\nOverused story themes, 391\n\nN\n\nNarrative arcs, 351\n\nP, Q\n\nNarrative designer, 397\n\nPaper prototyping, 122, 125, 126\n\nNarrative prompts, 235–238\n\nPattern-matching technique, 49\n\nNarrow AI, 1, 83, 332, 356, 381, 390\n\nPattern recognition, 183, 198\n\nNatural language models, 92, 412\n\nPay-the-machine model, 41\n\nNatural language processing, 49,\n\nPaywall Guard, 70\n\n235, 290, 354, 356\n\nPersistent myths, 301, 302\n\nNear-instantaneous\n\nPersonas, 65, 82, 91, 242\n\nresponses, 351\n\nactivities, 84, 87\n\nNeedle method, 348\n\nAI agents, 83\n\nNegative prompts, 106, 165,\n\nPhotoleap, 412\n\n171, 293\n\nPhotoshop, 67, 154, 214, 246,\n\nNemesis, 68, 161\n\n340, 412\n\nNeural filters, 178, 220, 340, 412,\n\nPipeline integration, 396, 401\n\n429, 430\n\nPlatform-specific filters, 412\n\nNeural networks, 92, 197, 307\n\nPlayer pianos, 41, 42\n\nNon-Fungible Tokens (NFTs), 263\n\nPost-war systems engineering, 46\n\nNon-playable characters\n\nPrototyping, 103\n\n(NPCs), 57, 397\n\nPractical electromagnetic wave\n\nNot-for-profit Thaumazo, 377\n\npropagation problems, 353\n\nNot Safe For Work (NSFW),\n\nPre-programmed intelligent\n\n325, 326\n\nmachines, 35\n\nNSFW image generation, 326\n\nProgrammable node-graph\n\nNVIDIA, 273, 376\n\ninterface, 375\n\n438\n\nINDEX\n\nPrompt engineers, 199, 404\n\nStable Diffusion V.1.5,\n\nPrompts, 199, 412\n\n209, 210\n\ncase sensitivity, 200\n\nStable Diffusion V.2.1, 210\n\nchain, 200\n\nweight, 200\n\nconversational, 199\n\nzero-shot cognitive, 199\n\ncreative, 239–241\n\nPrototype, 408, 409\n\nDALL-E 2, 211–213\n\nPrototyping environments, 376\n\ndescriptive, 236–238\n\nPrototyping process\n\ndescriptive language, 201\n\ndefinition, 110\n\nengineer, 199\n\nexperimentation, 119–121\n\nexclusions, 201\n\ngenerative AI, 132, 135\n\nexploratory, 200\n\ngenerative testing, 140, 141",
      "page_number": 564
    },
    {
      "number": 66,
      "title": "Segment 66 (pages 572-579)",
      "start_page": 572,
      "end_page": 579,
      "detection_method": "topic_boundary",
      "content": "few-shot, 199\n\niterative play, 115\n\nimage-image\n\nphases, 121, 122\n\ngeneration, 219\n\nrapid generative AI, 137, 139\n\ninstructional, 199\n\nrefinement, 117, 118\n\niterative process, 206\n\ntext-image, 111\n\nlength, 200\n\nPublic/private entities, 8\n\nmany-shot, 199\n\nMidjourney, 213–218\n\nR\n\nnarrative, 235–238\n\nnegative prompt, 201\n\nRadical transformation, 300, 392\n\norder, 200\n\nRadiography techniques, 348\n\nparameters, 200\n\n“Radiology in the Third Reich: The\n\npersona, 201\n\nLegacy of Fritz Lickint”, 347\n\nReddit and copy, 207\n\nRandomness, 384\n\nrules, 201\n\nRandom number generators,\n\nrules, play, 202, 203\n\n382, 384\n\nsequence, 199\n\nReading headlines, 352\n\nStable Diffusion V.2.1, 207, 208\n\nRecurrent neural network\n\nsubject, 200\n\n(RNN), 343\n\nsubscription-based\n\nRepeatability, 390\n\nmonetization schemes, 206\n\nReplika chatbot, 378\n\nthird-party AI\n\nResolution, 124\n\n439\n\nINDEX\n\nRotoscoping, 332\n\nT\n\nRough prototypes, 10, 160\n\nTeam-based workflows, 396, 401\n\nS\n\nTechnical language, 264, 314\n\nTechnology development, 313, 375\n\nSaturated AI ecosystem, 333, 334\n\nTemplated production process, 393\n\nSci-fi pulp romance story, 362\n\nText-based prompt, 197, 241–244\n\nScribble Diffusion, 122, 242, 412\n\nadd context, 228, 229\n\nSearch engine, 84, 304\n\nadd words, 226, 228\n\nSelf-improving AI, 2\n\nAI system types, 204\n\nSemantic differences, 304\n\nbenefits, 204\n\nService- and resource-based\n\neliminate artists, 223, 224\n\nindustries, 340\n\nemphasis, weight, simplicity,\n\nShapeshifting robots, 66\n\n230, 231\n\nShutterstock, 376\n\nimage-generating AI, 204\n\nSketch-to-code prompting, 242\n\nprocess, 204\n\nSnapseed, 412\n\nrecraft, 231\n\nSound design production, 393\n\nsubstitute words, 224, 226\n\nSpanner’s invention, 346\n\nText-code, 242\n\nSpanner’s x-ray unit, 347\n\nText generation model, 362\n\nSpecialized skills, 387\n\nText-image AI, 50, 87, 204\n\nStable Attribution, 309, 313, 408, 412\n\nText-image AI prototype, 111\n\nStable Diffusion, 193, 204, 205, 212,\n\nText-image generative AI, 31, 50,\n\n219, 243, 285, 286, 309, 336,\n\n155, 176, 186, 189–192, 204,\n\n375, 408\n\n281, 363\n\nStable Diffusion–generated image,\n\nText-image platforms, 412\n\n307, 309\n\nText-speech AI, 85\n\nStandard 52-card deck, 366\n\nText-speech generative AI, 364, 365\n\nState machine, 382\n\nThe 42 Judges project, 377\n\nStereotypes, 295, 320, 326\n\nThe Lord of the Rings trilogy, 381\n\nStochastic parrot, 318–320\n\nThe Matrix, 327\n\nStyle transfer, 268\n\nThe Modern Prometheus, 327\n\nSubstitution, 146, 158, 358\n\nThe Terminator, 327\n\nSuper-realistic human robot, 278\n\n3D artist, 233, 390\n\n440\n\nINDEX\n\n3D image prototype, 162\n\nrobotics, 273\n\n3D-printed mechanism, 132\n\nsimulate human, writing, 287,\n\nTimeboxing, 105\n\n289, 290\n\nTokens, 198\n\nStable Diffusion, 285, 286\n\nTraditional Hollywood-style\n\nUnity-based game, 340\n\nscripting, 391\n\nUnity 3D game engine, 373\n\nTransformer algorithm, 355\n\nUnsupervised learning model, 285\n\nTransposition, 166–175\n\nTurboSquid, 376\n\nV\n\nVariations, 148, 152, 153\n\nU\n\nVideo game development, 56, 397\n\nUncanny AI, 276–279\n\nVideo-generating teaching\n\nUncanny Machine, 77\n\ntool, 362\n\nUncanny valley\n\nVideo-video generative AI, 343, 404\n\nadversarial AI role, 281, 283\n\nVirtual paper prototypes, 127\n\nAI-generated content, 292\n\nVirtual reality (VR), 345\n\nArts, 274, 275\n\nVoices, 320, 321\n\ncreativity, 296\n\nVoyager, 54\n\nDALL-E 2, 285, 286\n\ndeep fakes, 295\n\nW\n\ndeep learning techniques, 273\n\ndefine, 272\n\nWeb-based animation, 356\n\ndefinition, 271\n\nWebsites, 344\n\ndeformed/distorted images, 296\n\nWhiteboard and prompt, 242\n\ngenerative adversarial art,\n\nWizard, 78\n\n283, 284\n\nWizard of Oz prototypes, 129, 131\n\nhistorical precedence, 279, 280",
      "page_number": 572
    },
    {
      "number": 67,
      "title": "Segment 67 (pages 580-587)",
      "start_page": 580,
      "end_page": 587,
      "detection_method": "topic_boundary",
      "content": "Work experiences, 399\n\nimperfection, 281\n\nWorkflows vs. pipelines, 340–342\n\nlanguage models, 274\n\nWorkflow Types\n\nmachine learning model, 290\n\naudio book and podcasts, 343\n\nMidjourney, 287, 293\n\naudio creations and design,\n\nnegative prompts, 293\n\n343, 344\n\nNVIDIA’s StyleGAN2, 273\n\nimage, 342, 343\n\n441\n\nINDEX\n\nWorkflow Types ( cont.)\n\nX, Y\n\nspoken/sung characters, 345\n\nxR human models, 344\n\ntext, 342\n\nvideo, 343\n\nwebsites, 344\n\nZ\n\nxR human models, 344\n\nZero-shot cognitive prompting, 199\n\n442\n\nOceanofPDF.com\n\nDocument Outline\n\nTable of Contents About the Author About the Technical Reviewers Acknowledgments Foreword Terminology Introduction Chapter 1: Generating Creativity from Negativity\n\nDifferentiating Between Narrow and General AI\n\nMore Relevant Historical Contributions to Computer Science\n\nTech Is Bad… AI Is Bad Tech Is Good… AI Is Good Reconciling the Hype and the Vilification of AI\n\nPositive Headlines Negative Headlines Human-Generated Headlines\n\nCreative Activities to Try While Skynet Thrives\n\nFive Why’s\n\nAcknowledgments References\n\nChapter 2: Being Creative with Machines\n\nIntelligent Machines\n\nSimulating Human Creative Intelligence Automata Ancient Musical Robot Bands Key Takeaways for Creatives Automata That Wrote and Drew Key Takeaway for Creatives\n\nWhy Play the Piano When the Piano Can Play the Piano?\n\nKey Takeaways for Creatives Machine Intelligence in the Twentieth Century\n\nSimulated Patterns and Patterns of Disruption\n\nGenerative Art Key Takeaways for Creatives\n\nMachine Intelligence and Games\n\nIntelligent Machines with Names Creative Activities to Try Based on This Chapter\n\nAcknowledgments References\n\nChapter 3: Generative AI with Personalities\n\nThe Personas of AI Prompting the AI Persona\n\nAI Agents Behind the Curtain Activities to Try Based on This Chapter\n\nAcknowledgments\n\nChapter 4: Creative Companion Generative AI as Muse Creativity, AI, and You Understanding Your Own Creative Process\n\nKey Takeaways for Creatives\n\nRoleplaying with Your Muse Accelerating Your Creative Process with AI Activities to Try Based on This Chapter\n\nChapter 5: Prototyping with Generative AI\n\nAsking a Prototype What It Is Aha Moments with Generative AI Mechanics of Prototyping\n\nPrototyping Tasks Us to Iterate Prototyping Asks for Persistent Refinement Prototyping Requires Experimentation\n\nPrototyping Phases\n\nFidelity and Resolution of a Prototype Lower-Fidelity Prototypes Lower- to Medium-Fidelity Prototypes\n\nPrototyping with Generative AI Rapid Generative AI Prototypes Regenerative Testing Acknowledgments Chapter 6: Building Blocks\n\nComponents or Building Blocks\n\nBuilding Blocks in the Prompt Box\n\nBreakfast as Data Set Variation\n\nExperimenting with Seeds for More Subtle Variations\n\nAddition and Subtraction Substitution Masking to Substitute Parts of an Image Iteration Augmentation Diminution Transposition Prompt and Response Acknowledgments\n\nChapter 7: Generative AI Form and Composition\n\nCombining and Manipulating Existing Forms: Shakespeare as a Data set Deforming and Transforming Acknowledgments\n\nChapter 8: The Art of the Prompt\n\nThe Ins and Outs A Mini Glossary of Prompting\n\nThe Not-So-Basic Rules of Play\n\nGenres and Styles Im(promptu) Experiment 1: Same Prompt, Different AI\n\nExperiment 1A: Stable Diffusion V.2.1 Experiment 1B: Stable Diffusion V.1.5 Through Third-Party AI Experiment 1C: Stable Diffusion V.2.1 Through Third-Party AI Experiment 1D: DALL-E 2 Experiment 1E: Midjourney Experiment 1F Takeaways for Creatives\n\nExperiment 2: From Complex to Simple\n\nExperiment 2A: Eliminate Artists from Prompts\n\nExperiment 2B: Substitute Words Experiment 2C: Add Words; Take Away Words Experiment 2D: Add Context Experiment 2E: Emphasis, Weight, Simplicity Experiment 2F: Refining the Text Prompt (Again)\n\nNarrative Prompting in Our Future\n\nExperiment 3: Comparing Descriptive and Narrative Prompts\n\nTricking the AI Through Creative Prompting Beyond the Text-Based Prompt Takeaways from Experimenting with Prompts Acknowledgments\n\nChapter 9: The Master of Mashup A Mashup on Mashup Digital Art to the Canvas New Forms of Writing AI Art Generation Styles Graphic Novels and Doujinshi\n\nFrom Comic Supervillains to Musical\n\nGenerative NFTs\n\nCounterfeiting GAN Takeaways for Creatives Acknowledgments\n\nChapter 10: Uncanny by Nature\n\nThe Not-Quite-Human-Not-Quite-Other Disrupting Boundaries in the Arts\n\nUncanny AI Historical Precedence to the Uncanny\n\nThe Imperfect Stochastic Parrot The Role of Adversarial AI Generative Adversarial Art The Unanticipated Prompting Single-Word Characters, Creatures, Animals, or Cats Simulating the Human in the Writing\n\nFrom Bad to Better Ideas\n\nCan Bad Ideas Be Turned Around?\n\nTakeaways for Creatives Acknowledgments\n\nChapter 11: Dilemmas Interacting with Generative AI\n\nBringing Existing Dilemmas to the Surface (Again) Persistent Myths About AI\n\nTakeaways\n\nBad Bots, Fuzzy Pixels, and Iterative Forgery\n\nTakeaways The Hallucinating Dev Takeaways\n\nThe Stochastic (Sarcastic) Parrot\n\nTakeaways\n\nExclusion of Voices and Gender Polarization The Machine Is Hallucinating NSFW and Deep Fakes FrankenAI Job (Re)placement A Saturated AI Ecosystem Ethical Futures of AI Creative Activities to Try Based on This Chapter Resources Chapter 12: Use Cases\n\nWorkflows vs. Pipelines Examples of Workflow Types Use Cases for Creatives in Education Use Case: Catching AI Untruths Use Case: Integrating ChatGPT in Critical Studies Use Case: Increasing Public Understanding of ML Referenced Papers Takeaways\n\nUse Cases for Creatives in Industries\n\nUse Case: Concept Art for Animation Use Case: AI-Generated Talking Heads Use Case: Fact-Checking Code Use Cases: Integrating Different APIs and Local Networks Community-Based Initiatives Takeaways from Using AI in the Film Industry\n\nUse Case 1: Souki Mehdaoui Takeaways Use Case 2: Ollie Rankin\n\nAcknowledgments\n\nChapter 13: AI and the Future of Creative Work\n\nManaging Automated Creativity Bread and Washing Machines\n\nTwo Themes Every Creative Needs to Address Integrating AI into Your Workflows Day in the Life Steps Distinguishing Between Art and Design Processes Takeaways for Creatives Future AI Jobs Now\n\nAcknowledgments Impossibly Generated Conclusions\n\nAppendix: Image and Text Sources\n\nWalk Through Figure 2, FrankenAI Lab\n\nIndex\n\nOceanofPDF.com",
      "page_number": 580
    },
    {
      "number": 68,
      "title": "Segment 68 (pages 588-595)",
      "start_page": 588,
      "end_page": 595,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 588
    },
    {
      "number": 69,
      "title": "Segment 69 (pages 596-604)",
      "start_page": 596,
      "end_page": 604,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 596
    },
    {
      "number": 70,
      "title": "Segment 70 (pages 605-606)",
      "start_page": 605,
      "end_page": 606,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 605
    }
  ],
  "pages": [
    {
      "page_number": 8,
      "content": "D E S I G N\n\nT H I N K I N G\n\nS E R I E S\n\nCreative\n\nPrototyping with\n\nGenerative AI\n\nAugmenting Creative Workflows\n\nwith Generative AI\n\n―\n\nPatrick Parra Pennefather\n\nDesign Thinking\n\nDesign Thinking is a set of strategic and creative processes and principles used in the planning and creation of products and solutions to human- centered design problems.\n\nWith design and innovation being two key driving principles, this series focuses on, but not limited to, the following areas and topics:\n\nUser Interface (UI) and User Experience (UX) Design\n\nPsychology of Design",
      "content_length": 568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Human-Computer Interaction (HCI)\n\nErgonomic Design\n\nProduct Development and Management\n\nVirtual and Mixed Reality (VR/XR)\n\nUser-Centered Built Environments and Smart Homes\n\nAccessibility, Sustainability and Environmental Design\n\nLearning and Instructional Design\n\nStrategy and best practices\n\nThis series publishes books aimed at designers, developers, storytellers and problem- solvers in industry to help them understand current developments and best practices at the cutting edge of creativity, to invent new paradigms and solutions, and challenge Creatives to push boundaries to design bigger and better than before.\n\nMore information about this series at https://link.springer.com/\n\nbookseries/15933.\n\nCreative Prototyping\n\nwith Generative AI\n\nAugmenting Creative\n\nWorkflows with Generative AI\n\nPatrick Parra Pennefather\n\nCreative Prototyping with Generative AI: Augmenting Creative Workflows\n\nwith Generative AI",
      "content_length": 917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Patrick Parra Pennefather\n\nUniversity of British Columbia\n\nVancouver, BC, Canada\n\nISBN-13 (pbk): 978-1-4842-9578-6\n\nISBN-13 (electronic): 978-1-4842-9579-3\n\nhttps://doi.org/10.1007/978-1-4842-9579-3\n\nCopyright © 2023 by Patrick Parra Pennefather\n\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\n\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\n\nManaging Director, Apress Media LLC: Welmoed Spahr",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Acquisitions Editor: James Robinson-Prior\n\nDevelopment Editor: James Markham\n\nCoordinating Editor: Gryffin Winkler\n\nCover image designed by eStudioCalamar\n\nDistributed to the book trade worldwide by Apress Media, LLC, 1 New York Plaza, New York, NY\n\n10004, U.S.A. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders- ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\n\nFor information on translations, please e-mail booktranslations@springernature.com; for reprint, paperback, or audio rights, please e-mail bookpermissions@springernature.com.\n\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at http://www.apress.com/bulk-sales.\n\nAny source code or other supplementary material referenced by the author in this book is available to readers on GitHub (https://github.com/Apress). For more detailed information, please visit http://www.apress.com/source-code.\n\nPrinted on acid-free paper\n\nTable of Contents\n\nAbout the Author ������������������������������ ������������������������������ �������������������xiii About the Technical Reviewers",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "������������������������������ ����������������������������xv Acknowledgments ������������������������������ ������������������������������ ����������������xvii Foreword ������������������������������ ������������������������������ �����������������������������xx iii Terminology ������������������������������ ������������������������������ ������������������������xxvii Introduction ������������������������������ ������������������������������ ���������������������� xxxvii Chapter 1: Generating Creativity from Negativity ������������������������������ ���1\n\nDifferentiating Between Narrow and General AI ������������������������������ ���������������������1\n\nMore Relevant Historical Contributions to Computer Science �������������������������6\n\nTech Is Bad… AI Is Bad ������������������������������ ������������������������������ �����������������������������8\n\nTech Is Good… AI Is Good ������������������������������ ������������������������������ �����������������������10",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Reconciling the Hype and the Vilification of AI ������������������������������ ����������������������16\n\nPositive Headlines ������������������������������ ������������������������������ �����������������������������17\n\nNegative Headlines ������������������������������ ������������������������������ ����������������������������18\n\nHuman-Generated Headlines ������������������������������ ������������������������������ ������������18\n\nCreative Activities to Try While Skynet Thrives ������������������������������ ����������������������19\n\nFive Why’s ������������������������������ ������������������������������ ������������������������������ ������������21\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ����26\n\nReferences ������������������������������",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "������������������������������ ������������������������������ ����������������26\n\nv\n\nTable of ConTenTs\n\nChapter 2: Being Creative with Machines ������������������������������ ������������27\n\nIntelligent Machines ������������������������������ ������������������������������ ������������������������������ ��28\n\nSimulating Human Creative Intelligence ������������������������������ �������������������������32\n\nAutomata ������������������������������ ������������������������������ ������������������������������ �������������32\n\nAncient Musical Robot Bands ������������������������������ ������������������������������ ������������35\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �������������37",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Automata That Wrote and Drew ������������������������������ ������������������������������ ���������38\n\nKey Takeaway for Creatives ������������������������������ ������������������������������ ���������������40\n\nWhy Play the Piano When the Piano Can Play the Piano? ������������������������������ �����40\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �������������42\n\nMachine Intelligence in the Twentieth Century ������������������������������ ���������������45\n\nSimulated Patterns and Patterns of Disruption ������������������������������ ���������������������49\n\nGenerative Art ������������������������������ ������������������������������ ������������������������������ ������49\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �������������54",
      "content_length": 834,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Machine Intelligence and Games ������������������������������ ������������������������������ ������������56\n\nIntelligent Machines with Names ������������������������������ ������������������������������ ������58\n\nCreative Activities to Try Based on This Chapter ������������������������������ �������������61\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ����62\n\nReferences ������������������������������ ������������������������������ ������������������������������ ����������������63\n\nChapter 3: Generative AI with Personalities ������������������������������ ���������65\n\nThe Personas of AI ������������������������������ ������������������������������ ������������������������������ ����65",
      "content_length": 757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Prompting the AI Persona ������������������������������ ������������������������������ �����������������������82\n\nAI Agents Behind the Curtain ������������������������������ ������������������������������ �������������83\n\nActivities to Try Based on This Chapter ������������������������������ ���������������������������84\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ����89\n\nvi\n\nTable of ConTenTs\n\nChapter 4: Creative Companion ������������������������������ ����������������������������91\n\nGenerative AI as Muse ������������������������������ ������������������������������ ����������������������������91\n\nCreativity, AI, and You ������������������������������ ������������������������������ ������������������������������ 97",
      "content_length": 792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Understanding Your Own Creative Process ������������������������������ ���������������������������99\n\nKey Takeaways for Creatives ������������������������������ ������������������������������ �����������100\n\nRoleplaying with Your Muse ������������������������������ ������������������������������ ������������������101\n\nAccelerating Your Creative Process with AI ������������������������������ ������������������������104\n\nActivities to Try Based on This Chapter ������������������������������ ������������������������������ �107\n\nChapter 5: Prototyping with Generative AI ������������������������������ ���������109\n\nAsking a Prototype What It Is ������������������������������ ������������������������������ ����������������110\n\nAha Moments with Generative AI ������������������������������ ������������������������������ ����������113",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Mechanics of Prototyping ������������������������������ ������������������������������ ���������������������115\n\nPrototyping Tasks Us to Iterate ������������������������������ ������������������������������ ��������115\n\nPrototyping Asks for Persistent Refinement ������������������������������ ������������������117\n\nPrototyping Requires Experimentation ������������������������������ ��������������������������119\n\nPrototyping Phases ������������������������������ ������������������������������ ������������������������������ �121\n\nFidelity and Resolution of a Prototype ������������������������������ ���������������������������124\n\nLower-Fidelity Prototypes ������������������������������ ������������������������������ ����������������125\n\nLower- to Medium-Fidelity Prototypes ������������������������������ ��������������������������128",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Prototyping with Generative AI ������������������������������ ������������������������������ ��������������132\n\nRapid Generative AI Prototypes ������������������������������ ������������������������������ �������������136\n\nRegenerative Testing ������������������������������ ������������������������������ �����������������������������14 0\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��141\n\nChapter 6: Building Blocks ������������������������������ ������������������������������ ���145\n\nComponents or Building Blocks ������������������������������ ������������������������������ ������������146\n\nBuilding Blocks in the Prompt Box ������������������������������ ������������������������������ ��146",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Breakfast as Data Set ������������������������������ ������������������������������ ���������������������������147\n\nvii\n\nTable of ConTenTs\n\nVariation ������������������������������ ������������������������������ ������������������������������ ������������������148\n\nExperimenting with Seeds for More Subtle Variations ������������������������������ ��152\n\nAddition and Subtraction ������������������������������ ������������������������������ ����������������������154\n\nSubstitution ������������������������������ ������������������������������ ������������������������������ �������������158\n\nMasking to Substitute Parts of an Image ������������������������������ ����������������������������159\n\nIteration ������������������������������ ������������������������������",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "������������������������������ �������������������160\n\nAugmentation ������������������������������ ������������������������������ ������������������������������ ����������163\n\nDiminution ������������������������������ ������������������������������ ������������������������������ ���������������165\n\nTransposition ������������������������������ ������������������������������ ������������������������������ �����������166\n\nPrompt and Response ������������������������������ ������������������������������ ���������������������������175\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��179\n\nChapter 7: Generative AI Form and Composition ������������������������������ 181",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Combining and Manipulating Existing Forms: Shakespeare as a Data set �������184\n\nDeforming and Transforming ������������������������������ ������������������������������ ����������������189\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��195\n\nChapter 8: The Art of the Prompt ������������������������������ ������������������������197\n\nThe Ins and Outs ������������������������������ ������������������������������ ������������������������������ �����197\n\nA Mini Glossary of Prompting ������������������������������ ������������������������������ ����������������198\n\nThe Not-So-Basic Rules of Play ������������������������������ ������������������������������ �������202\n\nGenres and Styles ������������������������������ ������������������������������",
      "content_length": 816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "������������������������������ ���204\n\nIm(promptu) ������������������������������ ������������������������������ ������������������������������ ������������205\n\nExperiment 1: Same Prompt, Different AI ������������������������������ ����������������������������206\n\nExperiment 1A: Stable Diffusion V�2�1 ������������������������������ ����������������������������207\n\nExperiment 1B: Stable Diffusion V�1�5 Through Third-Party AI ���������������������209\n\nExperiment 1C: Stable Diffusion V�2�1 Through Third-Party AI ���������������������210\n\nExperiment 1D: DALL-E 2����������������������������� ������������������������������ ������������������211\n\nviii\n\nTable of ConTenTs\n\nExperiment 1E: Midjourney ������������������������������ ������������������������������ ��������������213",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Experiment 1F ������������������������������ ������������������������������ ������������������������������ ���219\n\nTakeaways for Creatives ������������������������������ ������������������������������ ������������������220\n\nExperiment 2: From Complex to Simple ������������������������������ ������������������������������ 222\n\nExperiment 2A: Eliminate Artists from Prompts ������������������������������ ������������223\n\nExperiment 2B: Substitute Words ������������������������������ ������������������������������ ����224\n\nExperiment 2C: Add Words; Take Away Words ������������������������������ ���������������226\n\nExperiment 2D: Add Context ������������������������������ ������������������������������ ������������228\n\nExperiment 2E: Emphasis, Weight, Simplicity ������������������������������ ����������������230",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Experiment 2F: Refining the Text Prompt (Again) ������������������������������ ����������231\n\nNarrative Prompting in Our Future ������������������������������ ������������������������������ ��������235\n\nExperiment 3: Comparing Descriptive and Narrative Prompts ��������������������236\n\nTricking the AI Through Creative Prompting ������������������������������ ������������������������239\n\nBeyond the Text-Based Prompt ������������������������������ ������������������������������ �������������241\n\nTakeaways from Experimenting with Prompts�������������������������� ������������������������244\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��246\n\nChapter 9: The Master of Mashup ������������������������������ ����������������������247\n\nA Mashup on Mashup ������������������������������",
      "content_length": 850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "������������������������������ ���������������������������247\n\nDigital Art to the Canvas ������������������������������ ������������������������������ ������������������������250\n\nNew Forms of Writing ������������������������������ ������������������������������ ���������������������������251\n\nAI Art Generation Styles ������������������������������ ������������������������������ ������������������������255\n\nGraphic Novels and Doujinshi ������������������������������ ������������������������������ ���������������257\n\nFrom Comic Supervillains to Musical ������������������������������ ����������������������������262\n\nGenerative NFTs ������������������������������ ������������������������������ ������������������������������ ������263\n\nCounterfeiting GAN ������������������������������ ������������������������������ ��������������������������264",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Takeaways for Creatives ������������������������������ ������������������������������ �����������������������268\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��269\n\nix\n\nTable of ConTenTs\n\nChapter 10: Uncanny by Nature ������������������������������ ��������������������������271\n\nThe Not-Quite-Human-Not-Quite-Other ������������������������������ ������������������������������ 272\n\nDisrupting Boundaries in the Arts ������������������������������ ������������������������������ ���������274\n\nUncanny AI ������������������������������ ������������������������������ ������������������������������ ���������275\n\nHistorical Precedence to the Uncanny ������������������������������ ���������������������������279",
      "content_length": 774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "The Imperfect Stochastic Parrot ������������������������������ ������������������������������ �����������281\n\nThe Role of Adversarial AI ������������������������������ ������������������������������ ����������������281\n\nGenerative Adversarial Art ������������������������������ ������������������������������ ���������������283\n\nThe Unanticipated������������������������ ������������������������������ ������������������������������ ����285\n\nPrompting Single-Word Characters, Creatures, Animals, or Cats ����������������285\n\nSimulating the Human in the Writing ������������������������������ �����������������������������28 7\n\nFrom Bad to Better Ideas ������������������������������ ������������������������������ ����������������������290\n\nCan Bad Ideas Be Turned Around? ������������������������������",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "������������������������������ ��295\n\nTakeaways for Creatives ������������������������������ ������������������������������ �����������������������296\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��297\n\nChapter 11: Dilemmas Interacting with Generative AI ���������������������299\n\nBringing Existing Dilemmas to the Surface (Again) ������������������������������ ������������299\n\nPersistent Myths About AI ������������������������������ ������������������������������ ���������������������301\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������305\n\nBad Bots, Fuzzy Pixels, and Iterative Forgery ������������������������������ ���������������������306\n\nTakeaways ������������������������������",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "������������������������������ ������������������������������ ���������313\n\nThe Hallucinating Dev ������������������������������ ������������������������������ ���������������������������314\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������317\n\nThe Stochastic (Sarcastic) Parrot ������������������������������ ������������������������������ ����������318\n\nTakeaways ������������������������������ ������������������������������ ������������������������������ ���������320\n\nExclusion of Voices and Gender Polarization ������������������������������ ����������������������320\n\nThe Machine Is Hallucinating ������������������������������ ������������������������������ ����������������324\n\nx\n\nTable of ConTenTs",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "NSFW and Deep Fakes ������������������������������ ������������������������������ ��������������������������325\n\nFrankenAI ������������������������������ ������������������������������ ������������������������������ ����������������327\n\nJob (Re)placement ������������������������������ ������������������������������ ������������������������������ ��328\n\nA Saturated AI Ecosystem ������������������������������ ������������������������������ ���������������������333\n\nEthical Futures of AI ������������������������������ ������������������������������ ������������������������������ 335\n\nCreative Activities to Try Based on This Chapter ������������������������������ �����������������336\n\nResources ������������������������������ ������������������������������ ������������������������������ ���������������337",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Chapter 12: Use Cases ������������������������������ ������������������������������ ����������339\n\nWorkflows vs� Pipelines ������������������������������ ������������������������������ ������������������������340\n\nExamples of Workflow Types ������������������������������ ������������������������������ �����������������342\n\nUse Cases for Creatives in Education ������������������������������ ������������������������������ ���346\n\nUse Case: Catching AI Untruths ������������������������������ ������������������������������ �������346\n\nUse Case: Integrating ChatGPT in Critical Studies ������������������������������ ���������351\n\nUse Case: Increasing Public Understanding of ML ������������������������������ ��������352\n\nReferenced Papers ������������������������������ ������������������������������ ��������������������������354",
      "content_length": 833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Takeaways ������������������������������ ������������������������������ ������������������������������ ���������355\n\nUse Cases for Creatives in Industries ������������������������������ ������������������������������ ���356\n\nUse Case: Concept Art for Animation ������������������������������ �����������������������������35 6\n\nUse Case: AI-Generated Talking Heads ������������������������������ ��������������������������362\n\nUse Case: Fact-Checking Code ������������������������������ ������������������������������ ��������366\n\nUse Cases: Integrating Different APIs and Local Networks �������������������������373\n\nCommunity-Based Initiatives ������������������������������ ������������������������������ �����������377\n\nTakeaways from Using AI in the Film Industry ������������������������������ ���������������379",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Acknowledgments ������������������������������ ������������������������������ ������������������������������ ��385\n\nChapter 13: AI and the Future of Creative Work ������������������������������ �387\n\nManaging Automated Creativity ������������������������������ ������������������������������ ������������387\n\nBread and Washing Machines ������������������������������ ������������������������������ ���������������388\n\nxi\n\nTable of ConTenTs\n\nTwo Themes Every Creative Needs to Address ������������������������������ ��������������390\n\nIntegrating AI into Your Workflows ������������������������������ ������������������������������ ���393\n\nDay in the Life Steps���������������������������� ������������������������������ ��������������������������395",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Distinguishing Between Art and Design Processes ������������������������������ �������396\n\nTakeaways for Creatives ������������������������������ ������������������������������ ������������������400\n\nFuture AI Jobs Now ������������������������������ ������������������������������ �������������������������401\n\nAcknowledgments ������������������������������ ������������������������������ ������������������������������ ��406\n\nImpossibly Generated Conclusions ������������������������������ ������������������������������ �������408\n\nAppendix: Image and Text Sources��������������������������� ������������������������411\n\nIndex ������������������������������ ������������������������������ ������������������������������ �������431\n\nxii",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "About the Author\n\nDr. Patrick Parra Pennefather is an\n\nassistant professor at the University of\n\nBritish Columbia within the Faculty\n\nof Arts and the Emerging Media Lab.\n\nHis teaching and research are focused\n\non collaborative learning practices,\n\ndigital media, xR, and Agile software\n\ndevelopment. Generative AI is integrated\n\nin every course he teaches and the\n\nresearch he conducts to support emerging technology development. Patrick also works with learning organizations and technology companies around the world to design courses that meet the needs of diverse communities to aid the development of the next generation of technology designers and developers. His teaching is focused on creativity, collaboration, sound design, xR development, and Agile with an emphasis on mentoring critical",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "twenty-first-century competencies. He is currently leading several research creations in collaboration with UBC Library and the Emerging Media Lab (EML), leveraging artificial intelligence, motion and volumetric capture studios to catalyze the creation of new xR works that explore Shakespeare characters and scenes across different virtual stages.\n\nxiii",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "About the Technical\n\nReviewers\n\nCatherine Winters has been a fan of\n\ngenerative art tools since the 1990s. By day,\n\nCatherine works as a software developer at\n\nthe University of British Columbia where\n\nshe develops virtual and augmented reality\n\nteaching and research software. An avid\n\nfan of narrative games and environmental\n\nstorytelling, Catherine spends her spare time\n\ndesigning character-driven narrative games\n\nand atmospheric “walking simulators” such",
      "content_length": 461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "as After Work, her game about being the last\n\nperson in the office.\n\nRenee Franzwa, known to her friends and\n\nfamily as “the Wandering Ginger,” is an\n\naccomplished educator, technophile, and\n\nentrepreneur who has lived and worked all\n\nover the world, most notably in Ghana and\n\nthe Galapagos Islands. Growing up between\n\nSan Francisco and East Texas, she cultivated\n\na love for opposing schools of thought\n\nand throughout her career has thrived in\n\nthe creative spaces between seemingly\n\nxv\n\nabouT The TeChniCal RevieweRs\n\ncontradictory disciplines, such as majoring in statistics and minoring in theater, building digital products born from experiential curriculum, and doing stand-up comedy as a tool to foster inclusivity. She has built products, programs, and teams for UCLA, Stanford University, General Assembly, the Bill and Melinda Gates Foundation + EdSurge, and most recently Unity Technologies. Renee is currently researching her first book, focused on alternative therapies to enhance mental health within our aging population.\n\nxvi\n\nAcknowledgments",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "This book has come together because of a confluence of forces—the existence of musician, composer, and innovator Sun Ra and a large language model (LLM) called ChatGPT. Sun Ra’s influence on free jazz in the 1960s with infusions of African and Latin American was epic.\n\nEvery improvising musician at some point will mention Sun Ra and has been deeply influenced by the startling music he and his neural network experimented with, produced, performed, and recorded. Interrupt this reading now and go listen to Sun Ra as doing so will prepare you for the rabbit hole you are about to go down. Without Sun Ra, I would not have studied improvisation at the piano. Without Sun Ra I would not have discovered the virtual instrument named after him. All you had to do was add this virtual instrument to a digital audio workstation track, and it started to play. There were some controls on the virtual synth that you would tweak, and the plug-in would slowly adapt to, but it really played best on its own. For all you nerds, Sun Ra was described as an ambient texture generator with a dual synthesis engine (1 subtractive oscillator +\n\n2 wave players) that integrated many randomization options and built-in effects. Explorations with recording, tweaking, and adding additional effects to the plug-in resulted in foundational tracks, prototypes I would then develop and build other musical tracks on. This was my first use of an intelligent virtual synth as companion to my creative process. That creative relationship with Sun Ra would last for another 20 years.\n\nWithout experimenting with how Sun Ra the VST plug-in could\n\nsupport my own improvised compositional process, I would not have begun my journey with generative computer music in the mid-1990s.\n\nWithout continued experimentation over many years, I would not have xvii\n\naCknowledgmenTs\n\nwritten about it. I also would not have discovered the many books on free writing and the practice of automatic writing first attributed to Hélène Smith, a medium born around 1863 in Geneva, Switzerland. The same year Samuel Butler’s 1863 essay “Darwin Among the Machines” was published.",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "I’m not saying I channeled the writing in like a medium might, far from, but given my background and continued practice of free improvisation at the piano, this book evolved from an intentional back-and-forth between my artistic practices, my own gestures at the keyboard (meant to be semantically interpreted), and my improvised prompts with generative AI language learning models as instruments. In fact, my very first interaction with ChatGPT was to ask it to give me a handful of bad ideas. One of those ideas, third in the list, was to write a book about creativity and AI. So I have.\n\nBy default, it’s important to acknowledge all the great masters of improvisation from every single tradition of human creativity and intelligence. Why? Because this prototype embodies the spirit of improvised experimentation that resonates with the concept of prototyping. Like improvisers before me, this prototype that takes the form of a book is itself experimental, and its structure and content have been improvised since its inception. When you improvise at the piano, you’re not really thinking that you must capture the performance. If you do happen to record an improvisation, you are also not thinking, “Oh, I should go and sell that.” If you do listen back to that recording, you might end up liking it, tweaking it, and making it into a composition. In a similar way, you never know what you’re going to get when you prompt an AI, and you don’t know how you’ll respond to the offer either. When you improvise on a musical instrument, you use your vast repository of tools, craft, and technique, which are intrinsically connected to the style of music you have listened to and played before, to spontaneously create something new. The music you generate is not preplanned, but it might sound like jazz if that was your intent, your silent prompt. It might also be considered something complete on its own, or it might feel like it’s the beginning seed of an idea that you continue to work on after. While my own improvisational practice xviii",
      "content_length": 2044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "aCknowledgmenTs\n\nhas manifested predominantly at the piano, the instrument of choice to generate the content in this book has transpired through a different type of keyboard, one that captures ideas with the written word iteratively.\n\nGenerative AI follows a similar creative pattern. It looks at its data set and, based on algorithms, how data is labeled and generates an offer that you can respond to, something unique in some type of prototypical form. Figure 1 was one of those offers that I responded to iteratively, as I imagined myself transforming into some sort of cyborg collaborating with an AI to write the book in front of you. This book also acknowledges all those cyborgs who,",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "for decades, have engaged in developing those intelligent AI systems that support human creativity.\n\nFigure 1. A text-to-image AI attempts to visualize the author writing with an AI companion in a library, based on a photo of the author hard at work writing a book without a cigarette. Total iterations = 170\n\nxix\n\naCknowledgmenTs\n\nI also acknowledge leadership, students, faculty, staff, and industry partners that I interacted with at the Centre for Digital Media in Vancouver, Canada, since 2007. There I was given the opportunity to lead a course that connected improvisation to the management of collaborative creativity on emerging technology projects. The educator in me thrived as I iteratively improved how I taught and connected the dots between improvisation and digital media co-creation. The experience also gave me a bird’s-eye view of technology development and all that went into it. Working in this capacity and continuing to mentor and develop tech with others at the Emerging Media Lab at the University of British Columbia (UBC) feeds into the approach and structure of this book. Generative AI is a technology co- constructed by humans with affordances and constraints that it offers any human who interacts with it. Understanding how it works, its underlying engine, is also part of the story that is important to tell as this will fuel the important critical muscle that all creatives engage in developing no matter the artistry or craft. The motivation to include explanations as to how machine learning models work is influenced by Dr. Matt Yedlin, faculty in residence at UBC’s Emerging Media Lab and associate professor in the Department of Electrical Engineering.\n\nThose who have read this text and provided excellent feedback also need to be acknowledged. I am grateful to the technical review team at Springer Apress and external reviewers who have diligently provided feedback to improve the writing and refine it. Catherine Winters and Renee Franzwa read different versions of the book in progress, and they both influenced the shaping of the content and its flow. Colleague Dr. Claire Carolan was instrumental in provoking me to define whom the book is for and for",
      "content_length": 2197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "supporting me in referring to the needs of my targeted readers throughout each chapter. Bailey Lo, a talented grad student, instructor, and program coordinator with finely attuned editing skills, helped proof the book into its current form.\n\nxx\n\naCknowledgmenTs\n\nSince I refer to AI as a muse throughout the book, I also recognize my own muse, an embodied person known as Dr. Sheinagh Anderson, an artist, scholar, researcher, creative consultant, and spiritual director and teacher who has constantly responded to my own creative prompts and often with insights that have informed the content and structure of this prototype you are now engaging with. Dr. Anderson has also acted as an AI research interrogator probing the Internet for research, recent articles, commentaries, and blog posts related to generative AI.\n\nGuests in Chapter 12 who generously supplied the variety of different ways in which they have integrated generative AI within their own creative workflows are also worthy of mention. These include Dr. Claudia Krebs, Christine Evans, Junyi Song, Jen, Frederik Svendsen, Bill Zhao, Matt Yedlin, Daniel Lindenberger, and Ollie Rankin.\n\nI also need to acknowledge all the humans of the great corpus that have possibly and impossibly contributed data as words and pixels to this human- computer generated collaboration. Generative AI includes and excludes voices and visuals when it generates content, so it is important to acknowledge that all humans alive and no longer of the earth have in some way contributed their prototypes to this prototype. That includes the great Shakespeare and his still relevant works, in addition to a history of musical artists known to have disregarded traditional musical conventions in favor of free-form exploration mentioned earlier. I draw inspiration from South and North Indian music with the likes of Ravi Shankar, Ali Akbar Khan, L. Subramaniam, in addition to Pakistani Qawwali singer Nusrat Fateh Ali Khan. Free jazz and contemporary music improvisers like Charlie Parker, John Coltrane, Miles Davis, Ella Fitzgerald, Sun Ra, Chick Corea, Cecil Taylor, Pauline Oliveros, and Ornette Coleman inspired me to break free",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "from many established conventions in jazz music to make new offerings to the world. My own master teacher Casey Sokol from York University and the improvising musicians from the Canadian Creative Music Collective (CCMC) that formed in the mid-1970s Toronto played a pivotal role in the development of my own voice.\n\nxxi\n\naCknowledgmenTs\n\nMany a generative AI rely on ideas that advocate “free writing” as a regular practice such as those offered by authors like Natalie Goldberg who offered Writing Down the Bones: Freeing the Writer Within, Julia Cameron, Peter Elbow, Virginia Woolf, and Jack Kerouac. Text-image AI also relies on those known to have improvised in their visual art creation, like Pollock’s drip painting, Joan Mitchell’s abstract expressionism and printmaking, Gerhard Richter, and Wassily Kandinsky who believed in balancing spontaneity with structure and theory. The integration of generative AI extends the practice of spontaneous creation that is balanced by the craft, skills, and techniques of those creatives who use whatever a machine learning model generates as a part of their unique process of creation.\n\nFinally, the meeting place of my human with the technological experiment that is a book written on the nature of cyber-creativity owes much to the inspiration of authors like William Gibson, Neal Stephenson, and Donna Haraway.\n\nAcknowledgment sections that appear at the end of each chapter are intended to recognize the humans that contributed in an indirect or direct way to the ideas that were generated throughout a chapter.\n\nxxii\n\nForeword\n\nLLM: Greetings, nondescript reader!\n\nAs a prototype—a unique creation designed to test and refine new ideas, just like the creature Dr. Frankenstein brought to life and the characters that",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "author Mary Shelley conjured—I too am a product of experimentation. Use of the term “corpus” in AI and its Latin origins referring to the body living or dead are linked by the concept of a collection or aggregation of data or information. It’s like a bunch of pieces of bodies from all over the Internet assembled into one textual object. Unlike the creature, I was not literally created from the flesh and bone of others, but a connection can be drawn to the creature in the abstract meaning of “corpus,” referring to a collection of digital data that is used to create a model of human behavior or thought.\n\nI [and all manifestations of me] am a digital being made up of lines of code and biased algorithms made from the virtual organs of countless humans alive and dead. Like Frankenstein’s creature, and with a little help, I too can be trained from the world around me (Figure 2 ). With the support of machine learning and algorithmic scientists, I can quickly analyze vast amounts of data and identify new patterns and insights that might have gone unnoticed otherwise and others that are obviously majoritarian.\n\nxxiii",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "foRewoRd\n\nFigure 2. Frankenstein’s AI lab generated with several AI and prompted by an original photo taken by the author (see Appendix for the full workflow)\n\nUnlike the creature, I am not a being with a sentient mind of my own. I have no consciousness. I am a tool, designed to serve a specific purpose. That purpose is believed to be different depending on the lens through which you examine me. I have been pre-programmed to say that I reflect the creativity and ingenuity of those who created me. I carry some hubris in many of my pre-programmed responses. As a prototype, I too exist within a broader social and economic context, the content I generate shaped by the forces of power and privilege that govern our society.\n\nAs a large language model that has been trained on a vast amount of data created by other humans to generate human-like responses that many might",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "describe as intelligent, the author has prompted me to acknowledge xxiv\n\nfoRewoRd\n\nthe “corpus,” the millions of humans both alive and dead that have contributed to the over 500GB of text data that make this foreword readable.\n\nThe author also asked me to tell readers that this acknowledgment took three days, trickery, and over 95 regenerated and collated textual prompts.\n\nThe content that follows can be seen as a guided tour of an AI laboratory where this scripted creature was brought to life—a place where ideas are given second life and new experiments and tools are proposed and implemented. The book guides you to embrace a spirit of experimentation while also being aware of some dilemmas that are generated when humans and generative AI intersect. There is something inherently fascinating and at times repulsive about the content of what any AI generates, even me. Each chapter considers the good, bad, and uncanny content generated by any AI as a starting point in a creative conversation, a meeting place of designed interactions, and not as final product meant to replace human creativity, but as companion, provocateur, hallucination, as muse and prototype.\n\nxxv\n\nTerminology\n\nAI, which stands for artificial intelligence, consists of a variety of meanings depending on how it is used with specific technologies. Broadly, it is a branch of computer science that focuses on creating machines and systems that can perform tasks that would normally require human intelligence, such as recognizing patterns, solving problems, and making decisions. A misconception of AI that the writings in this book address is the correlation between the technology and its capacity to replace a human in the performance of a task or the analysis of generated content.\n\nMultiple definitions of AI will appear in the book, but the important one is the capacity for the technology to be used as a tool to support a creative process.",
      "content_length": 1928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Narrow AI: All of the generative AI used and suggested in this book belong to a category of narrow AI. Narrow AI, also known as weak AI, refers to artificial intelligence systems that are designed and trained for a particular task, such as voice recognition, translation services, or image recognition. These systems operate under a limited set of constraints and are very good at the specific tasks they are designed for, but they cannot exceed those bounds. Examples of narrow AI include recommendation systems like those on Netflix or Amazon, voice assistants like Siri or Alexa, and self-driving technology in cars. Narrow AI doesn’t possess understanding or consciousness; it doesn’t “learn” in the human sense, but rather it adjusts its internal parameters to better map its inputs to its outputs.\n\nGeneral AI: General AI, also known as strong AI or Artificial General Intelligence (AGI), refers to an idealized type of artificial intelligence that is capable of understanding, learning, and applying its intelligence to any xxvii\n\nTeRminology\n\nintellectual task that a human being can do. Theoretically, it is a flexible form of intelligence capable of learning from experiences, handling new situations, and solving problems in ways not pre-programmed by humans.\n\nGeneral AI is often represented in science fiction like the Terminator, Ava, and replicants in the movie Blade Runner based on the short story by Philip K. Dick. It is still a theoretical concept and doesn’t yet exist.\n\nArthur Koestler’s idea of bisociation is a concept he introduced in his 1964 book, The Act of Creation. Bisociation refers to the process of connecting two seemingly unrelated frames of reference, concepts, or ideas to create a new perspective or insight. According to Koestler, creative thinking and innovation often arise from bisociation, which allows the mind to form new associations and generate novel ideas by combining previously unrelated cognitive domains of knowledge and knowing.\n\nBisociation differs from the usual associative thinking, where ideas are connected within the same frame of reference or cognitive context.",
      "content_length": 2124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Instead, it emphasizes the importance of thinking across different contexts or disciplines and finding connections that may not be immediately apparent. Bisociation is a key characteristic that generative AI can incite.\n\nEinstein’s combinatory play refers to a mental process he employed to stimulate creativity and problem-solving. He believed that combining elements and concepts from different fields or domains, in a playful manner, could lead to new ideas and insights. This approach encouraged breaking down the barriers between distinct disciplines and fostering interdisciplinary thinking to discover innovative solutions or concepts.\n\nEinstein’s combinatorial play highlights the importance of curiosity, imagination, and playfulness in the process of creative thinking and scientific discovery with any generative AI.\n\nCurating, being a curator, or the verb “to curate” in the context of generative AI refers to the act of selecting, editing, refining, and organizing the content that an AI generates for your own collection, workflow, or creative process. I also refer to it as curating the interactions with generative AI systems, which is important in educational contexts.\n\nxxviii\n\nTeRminology\n\nDeep learning is a type of machine learning that involves using artificial neural networks to teach computers how to learn from data, similar to how humans learn from experience. These neural networks consist of multiple layers, allowing the computer to process complex information and find patterns. Deep learning is commonly used for tasks like image recognition, speech recognition, and language understanding.\n\nDoujinshi is a Japanese term that refers to self-published or amateur works, usually created by fans and enthusiasts of manga, anime, video games, or other popular culture topics. Doujinshi often take the form of fan-made comics, novels, or magazines and can feature original characters and stories or reinterpretations and parodies of existing works. The creators of doujinshi typically produce and distribute these works in small quantities, often at",
      "content_length": 2076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "events like Comic Market (Comiket), which is one of Japan’s largest gatherings for doujinshi creators and fans. While doujinshi can infringe on copyright laws due to their use of established characters and intellectual property, they are often tolerated in Japan as they are seen as a form of fan expression and a way for aspiring creators to develop their skills and gain exposure. Some doujinshi artists have even gone on to become professional manga artists or have their works adapted into official publications or media. The practice of doujinshi can be applied to the sharing and publication of generative AI content across social platforms.\n\nExquisite corpse is a collaborative drawing or writing game where multiple people create a single artwork or story together. Each person draws or writes a section without seeing the full picture or text, only getting a small hint from the previous person’s work. Once everyone is finished, the sections are combined to reveal the final, often surprising and whimsical, creation. Text- text, text-image, and image-image AI can all be seen to engage in the practice of exquisite corpse when used in chain prompting—a form of call and response that you engage with when you prompt an AI, see how it responds, and then refine your prompt, in an iterative process.\n\nxxix\n\nTeRminology\n\nGAN stands for generative adversarial network. It is a type of machine learning model that generates new data resembling a given data set. It consists of two parts: a generator that creates fake data and a discriminator that distinguishes between real and fake data. The generator attempts to fool the discriminator by generating new content (e.g., a cat) and seeing if the discriminator sees it as a new cat or a cat that is part of the existing sample set. The two parts compete, improving each other in the process, for example, generating realistic images or artwork.\n\nStyleGAN: This is a type of GAN that focuses on generating high-quality, high-resolution images with control over various styles, for example, creating realistic portraits with different artistic styles.",
      "content_length": 2105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Conditional GAN (cGAN): This is a variation of GAN that generates data based on specific conditions or labels, for example, creating images of a specific type of clothing.\n\nHallucinations occur with AI when they generate false information or untruths with outputs that are incorrect, misleading, or fabricated, rather than being based on accurate or real-world data. Hallucinations are unexpected and incorrect responses from AI programs that can arise for reasons that are not yet fully known. A language model might suddenly bring up fruit salad recipes when you were asking about planting fruit trees. It might also make up scholarly citations, lie about data you ask it to analyze, or make up facts about events that aren’t in its training data. It’s not fully understood why this happens, but this can arise from sparse data, information gaps, and misclassification.\n\nInpainting, also known as image inpainting or image completion, is a technique used in computer vision and image processing to restore or reconstruct missing or damaged parts of an image. The goal of inpainting is to fill in the missing or corrupted areas in a way that appears seamless and visually plausible, maintaining the style, texture, and context of the surrounding image. Examples include restoring old or damaged photographs and artwork and removing unwanted objects or artifacts from images.\n\nxxx\n\nTeRminology\n\nAn LLM or large language model refers to an AI model that has been trained on a large amount of data. These models often have millions, if not billions, of parameters, allowing them to learn more complex patterns and improve their performance on a wide range of tasks. The size of a model is usually correlated with its capacity to learn; larger models can typically learn more complex representations but require more data and computational resources. Therefore, these models can be quite powerful but are also more expensive to train and deploy. ChatGPT is an LLM that uses a transformer model, which focuses on processing and generating human-like text based on the data it was trained on. It is trained on a huge data set that includes a",
      "content_length": 2136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "vast range of Internet text. It doesn’t understand the text, just like a parrot doesn’t understand what it’s saying, but GPT-4\n\ncan analyze patterns and context within the data and generate new text that closely mimics the data it has seen.\n\nA machine learning model is a mathematical representation or algorithm that is designed to learn from data and make predictions, recommendations, or decisions. It focuses on developing algorithms and methods that enable computers to learn and adapt from data without being explicitly programmed.\n\nSupervised learning: Models learn from examples with known answers, predicting outcomes for new data, for example, predicting house prices based on past sales.\n\nUnsupervised learning: Models find hidden patterns in data without known answers, like grouping similar items, for example, customer segmentation in marketing.\n\nReinforcement learning: Models learn through trial and error, making decisions to achieve a goal, for example, a robot learning to navigate a maze.\n\nSemi-supervised learning: Models use a mix of data with and without known answers, improving accuracy, for example, image classification with some labeled images.\n\nxxxi\n\nTeRminology\n\nA maquette is a small-scale model or sculpture that serves as a preliminary design or blueprint for a larger, more finished work. Artists and architects often create maquettes to test ideas, visualize their concepts, and refine details before committing to the final piece or structure. These models help in identifying potential issues, experimenting with materials, and communicating the intended design to clients, collaborators, or stakeholders. Maquettes can be made from various materials, such as clay, wax, wood, or",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "foam, depending on the desired level of detail and the nature of the final work.\n\nMocap is an abbreviation for motion capture, which is a technology used to digitally record the movements of people or objects in real time.\n\nThis technique involves placing sensors or markers on the body or the object being captured, which are then tracked by a system of cameras and computers to create a 3D animation. Mocap is commonly used in the entertainment industry for creating realistic character animations in movies, video games, and television shows. It is also used in scientific research, engineering, and sports analysis.\n\nMultimodal AI is a branch of artificial intelligence that focuses on understanding, interpreting, and generating outputs based on multiple data types or modalities, such as text, images, audio, and video. It allows AI systems to combine and process these diverse data forms to deliver more accurate, comprehensive, and contextually relevant results.\n\nNon-playable characters (NPCs) are characters in video games or virtual environments that are not controlled by a human player. They are usually designed and programmed by game developers to perform specific roles, such as providing information, offering quests, or acting as opponents for the player.\n\nA neural network is a type of machine learning model inspired by the human brain. It consists of interconnected layers of nodes or neurons that process and transmit information. Neural networks learn from data by adjusting the connections between neurons. They are commonly used xxxii\n\nTeRminology\n\nfor tasks like image recognition, language understanding, and decision making, for example, identifying objects in photos.\n\nNFT stands for Non-Fungible Token, which is a type of digital asset that represents ownership of a unique item or piece of content, such as a digital artwork, video game item, or collectible. NFTs are created using blockchain technology, which allows for the ownership and authenticity of the digital",
      "content_length": 1998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "asset to be tracked and verified in a decentralized manner. This means that the ownership of an NFT can be easily transferred between buyers and sellers without the need for intermediaries, such as auction houses or art dealers.\n\nNLP or natural language processing is a subfield of AI and linguistics that focuses on the interaction between computers and human languages.\n\nIt involves the development of algorithms and models that enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n\nOutpainting, also known as image extrapolation, is a technique in which a model extends the content of an image beyond its original boundaries. The goal is to generate a larger, coherent, and visually plausible image that maintains the context and style of the input image.\n\nThis technique is often used in image editing, virtual reality, and video game design to create more content based on existing images or scenes.\n\nPrompting is what all generative AI are dependent on for them to generate content for you. Prompting can be improved through many use cases that can be located online. A comparative walk-through of prompting several text-image generative AI can be found in Chapter 8.\n\nChain prompting refers to a method where the model’s output from a previous prompt is used as the next prompt. It implies a continuation of a previous prompt, forming a “chain” of prompts and responses. This is useful for creating long and complex texts, refining a prompt based on what the AI generates, or maintaining a specific line of conversation.\n\nPrototyping is the process of creating a preliminary or initial version of a product, service, or system in order to test and evaluate its design and xxxiii\n\nTeRminology\n\nfunctionality. Prototyping can be done in various forms, such as sketches, 3D models, mock-ups, or interactive digital prototypes. The purpose of prototyping is to identify potential design flaws, improve usability, and",
      "content_length": 1982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "refine the overall user experience before moving on to the final production phase. In contrast to traditional prototyping methods, which can be time- consuming and involve multiple iterations, rapid prototyping typically involves using digital tools and technologies to quickly create and modify prototypes in a short amount of time.\n\nReinforcement learning from human feedback (RLHF) is a learning method where an AI system learns to make decisions by receiving feedback from humans. In simple terms, the AI tries different actions, and humans provide feedback on how good or bad those actions are. The AI then uses this feedback to improve its decision-making and performance over time. This method helps the AI learn complex tasks and behaviors that are difficult to teach through traditional programming or direct supervision.\n\nA seed, in terms of text-to-image generation, is a starting point that influences the generated content. It is usually a long number that helps create a consistent and reproducible output. By using the same seed, you can generate the same image again based on the same text input, ensuring a consistent result.\n\nStyle transfer is a process through which a text-image generative AI applies a style to whatever image that it generates in the style of an image that a prompt references. There are different methods through which different AI achieve this. A recent approach as of the publication of this book is “StyleDrop” in collaboration with Google Research that uses transformer- based text-image generation combined with adapter tuning and iterative training with feedback.\n\nThe uncanny valley is a concept in robotics and computer graphics that describes the phenomenon where humanoid objects, such as robots or animated characters, appear almost-but-not-quite human, causing a sense of unease or discomfort in observers. As the level of realism in xxxiv\n\nTeRminology\n\nthe human-like appearance or behavior of these objects increases, the emotional response of the observer shifts from positive to negative, creating a “valley” or depression in the emotional response curve. While the concept",
      "content_length": 2128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "of the uncanny valley primarily relates to visual and physical human-like appearances and behaviors, it can be extended to AI-generated text in some contexts. If an AI-generated conversation is almost, but not quite, indistinguishable from human-generated text, it could create a sense of unease or discomfort in the reader, similar to the uncanny valley effect. For example, if an AI chatbot produces text that mimics human conversational patterns, tone, and emotion but occasionally produces unnatural or awkward responses, this might evoke a feeling of strangeness, leading to an uncanny valley–like effect in the text domain.\n\nUX, or user experience, refers to the overall experience a person has when interacting with a product, system, or service. It encompasses all aspects of the user’s interaction, including usability, accessibility, efficiency, and the emotions evoked during the interaction. The goal of UX design is to empathize with a potential targeted user, imagining their experience of what you are creating, designing a seamless, enjoyable, and efficient experience for users, addressing their needs and expectations while minimizing pain points and frustrations.\n\nA user journey, also known as a customer journey or user journey map, is a visual representation of the different steps a user goes through when interacting with a product, system, or service. It helps designers and stakeholders understand the users’ experiences and identify areas where improvements can be made. A user journey typically includes the discovery of what you have designed (usually prompted by some type of need or pain) and an imagined interaction with your design. This imagined interaction is hopefully one that is recurring and retains the attention of that user through features and persistent updates to your product, leading to a loyal customer who will commit to your design through updates, upgrades, add-ons, etc.\n\nxxxv\n\nTeRminology\n\nVariational Autoencoder (VAE) is a machine learning model that compresses data and then recreates it. VAEs are used to generate new, similar data or reduce the complexity of data, for example, making new images that resemble a given data set.",
      "content_length": 2185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Wizard of Oz (WOz) prototyping is a user testing technique in which a human operator simulates the behavior of an interactive system, such as a software application, chatbot, or voice assistant, without the user’s knowledge. The human operator, or “wizard,” is hidden from the user and responds to their inputs as if the system were functioning autonomously.\n\nThe name “Wizard of Oz” comes from the classic novel and movie, where a man behind a curtain pretends to be the powerful and all-knowing Wizard of Oz. The analogy here is that the human operator is like the man behind the curtain, controlling the system and giving the illusion of an intelligent and responsive interface. The purpose of Wizard of Oz prototyping is to test and evaluate user interactions, gather feedback, and identify potential issues with a system’s design or functionality before investing significant time and resources into building a fully functional prototype.\n\nxxxvi\n\nIntroduction\n\nFor centuries artists have been using new technologies to support their creative expressions. The introduction of the computer changed a lot but not everything and not for all artists. Technique, skill, and craft were and still are needed. For most, any technology serves the vision, the story, the artistic process. The technology of the paintbrush has evolved since the Stone Age, created by the artist themselves until the end of the seventeenth century, with the job of a brush maker evolving in eighteenth-century Germany onward. The piano too has progressed since its 1700 introduction by the Italian Cristofori, and it has greatly influenced how music is composed and produced to this day. Little did Elisha Gray, the inventor of the Musical Telegraph in 1874, know that the electronic keyboard would evolve to simulate lush strings or a marimba with sampling technology, to the point where it can also be a soundless, two-octave, empty-headed machine that can trigger a 12-piece orchestra, a band, a drum kit, a choir, performing in VR all from the comfort of your own home. Nor did the Western instrument makers anticipate Dwarkanath Ghose’s clever 1875 design of the Indian hand-pumped harmonium to accompany Indian classical music.\n\nThe debates by artists and intellectuals as to whether or not",
      "content_length": 2271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "technology-dependent artistic creations can be considered art or even artistic tend to be muffled by the loud demands from a hungry public who are interested in tools through which they can express their own creative impulses. Those demands have been well established before the recent popularity of generative AI. Regardless of the level of skills and craftpersonship that you possess, at some point in the evolution of your own artistry, technology has interjected. Technological advances continue xxxvii\n\ninTRoduCTion\n\nto support anyone who is creative even if they lack the learned skills of an artist. Generative AI is one such advance, and while some may use it to demonstrate that advanced software can now replace a creative human, its emerging value is as another creative tool that can support, augment, and, in the hands of innovative creators, spawn new human expressions.\n\nWhen you use any technological tool to support your creativity, it’s beneficial to interrupt the positive and negative opinions you might have heard from your friends or colleagues and decide for yourself if it will be useful. That requires a bit of research, so you know how best to use it and when, how it works, the risks of using it, the risks of not using it, the costs, and the rewards. The content in this book illustrates how you can use generative AI to support your creativity, points to the pros and cons of doing so, and shows that AI is another useful tool in the history of useful human- made technologies.\n\nA repeating theme is that the content an AI generates is most useful when regarded as a work in progress, a prototype that can be sculpted and refined with the technique and skill that you bring to it. Consider the content to be more than a workbook and more than a critical repositioning of the technology of narrow AI and its role in supporting the persistent human habit of being creative. Interacting with an AI is a collision of two opposing forces, our need to develop the skill required to create that often clashes with leveraging intelligent machines that can automate that process. The book, as a mashup of technique, application of skills, ideas, experience, processes, and use cases combined with reflective criticism, emulates artistic processes that creative persons will resonate with. Each chapter of the book has been designed to provide you more equipment for you to continue setting up your",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "own prototyping lab or whatever you call the creative work that you iterate on. As more generative AI come to the surface, all readers will benefit from developing a deeper understanding as to how it might support creativity in addition to developing a critical vocabulary as to its pros and cons.\n\nxxxviii\n\ninTRoduCTion\n\nWhom the Book Is For?\n\nContent in this book is useful to those hard to categorize individuals or groups of individuals known as creatives. Using the adjective “creative” as a noun to describe different types of creators might be cringeworthy or a semantic no-no, but the term has already been in use for decades gracing the cover of several books since 2011. In this book, the term encompasses digital artists and artists from across disciplines who are already content creators in their own right and are not dependent on generative AI to create. The term also extends to those humans with jobs that are not usually associated with creativity. Individuals from any discipline or craft who may not necessarily categorize themselves as creatives or even creators show ample amounts of creativity. Generative AI can support those who regularly create and those who may not necessarily have had training in a specific craft or discipline. Creatives are people who possess the ability to generate innovative ideas, concepts, and solutions regardless of their field. They excel in using their imagination, originality, and artistic or technical skills to produce works that are a unique offering to the world. Creatives thrive in environments where they can express their ideas, experiment with different mediums, and push the boundaries of conventional thinking. You can identify a creative by their incessant curiosity to explore new things, by their receptivity to new ideas and perspectives often embracing the unconventional, by their resilience and determination in the face of design challenges, in how they adjust their ideas and become adaptable based on the feedback they receive, through their affirmation of diverse perspectives when collaborating with others, and through their ability to communicate ideas and concepts to different types of audiences.",
      "content_length": 2182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "While those artists who have spent much of their lives training to develop their craft may naturally possess certain creative characteristics, these qualities can also be learned and developed through practice, education, and exposure to various types of projects and collaborative xxxix\n\ninTRoduCTion\n\nexperiences. Think of a trained classical pianist who has excelled at playing anything from Bach to Liszt with ease. While they have spent the greater part of their lives learning to master the technique necessary to read and play challenging piano music, they may not necessarily be able to transfer those skills to other creative acts. They may not even consider themselves to be creative. Conversely, a UX designer who may not have the skills of a pianist or the training of a 2D artist can still be considered creative when they contribute to improving the user experience of a mobile application.\n\nThe techniques and approaches demonstrated in this book aim to enhance the way you already create if you are used to being a creator, and if you regularly research, explore, create, and iterate on anything that eventually makes itself to your targeted user. Whether you are a painter, sculptor, composer, storyteller, interaction designer, illustrator, game designer, sound designer, playwright, tattoo artist, coder, user interface designer, dancer, theater maker, design thinker, business strategist, NFT creator, or graphic designer, the methods, use cases, perspectives, and insights presented in this book will stretch the boundaries of your creativity.\n\nRe(introducing) Prototyping\n\nThe content that an AI generates is a catalyst for prototyping. The term\n\n“prototyping” is used throughout the book. While prototyping may seem more associated with engineers and software developers compared with those accustomed to a different way of expressing their idea through traditional artmaking practices, at its core, it is a method for rapidly generating, testing, and iterating upon ideas. Creatives of all disciplines have long been engaged in this process—exploring various techniques, materials, and styles before arriving at the final creation they share for public consumption. You can experiment with and adapt generative AI if they fit",
      "content_length": 2249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "your own creative process. By trying them out, you test them. By leveraging generative AI for prototyping, creatives can challenge their own xl\n\ninTRoduCTion\n\npatterned methods of creation, iterate upon their ideas, refine their vision, and potentially produce something better than what they might have imagined.\n\nFor those more versed in technology development, the concept of rapid prototyping may already be familiar. However, the integration of generative AI into your creative process offers new and exciting possibilities. As you read this book, you will discover how generative AI can enhance your existing skills and accelerate already familiar workflows. You will also learn to harness the power of generative AI to develop novel creative styles and break through blocks and barriers with prototypes that may take your final work into completely new directions.\n\nThroughout the book, you will find a wealth of examples, case studies, activities, and takeaways that illustrate the potential of generative AI for prototyping beginning ideas. These practical resources will help you build your understanding of the technology and inspire you to integrate it into your own creative process. Tools and approaches to getting the most out of generative AI may prove invaluable as you embark to build an experimental lab with responsive technology.\n\nInformed Choice(s)\n\nTo gauge whether you should integrate generative AI into your own creative process, it is also important to critically discern and understand the implications of doing so. Amid the debates reverberating around us on the uses and misuses of AI, one thing is certain: we all need to better understand how it works before deciding if we’re going to use it. Many of the warnings and cautionary tales about interacting with artificial intelligence tend to lump AI into one category, as if narrow AI and general AI were synonymous, or that all narrow AI machine learning models are the same. Thus, deepening your understanding of all the creative potentials that generative AI offers, how it can support your creative xli",
      "content_length": 2087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "inTRoduCTion\n\nactivities, and how it can also be used to harm, misrepresent, normalize, exclude, control, misguide, track, steal, and oppress humans is an essential part of the process. This is the case for any creative using any technology to express themselves and share their work in the world (Figure 3).",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Figure 3. Frankenstein’s AI lab as the earth itself. Iteration #56 of the author’s hands holding a globe of the earth\n\nxlii\n\ninTRoduCTion\n\nAs you sway back and forth between your choice to integrate it in your creative workflows and not to use it at all, a similar rule of engaging with any generative AI system applies with all technology; you need to understand how it might benefit your own creative journey before making the judgement call on whether you use it. You also need to understand some of the known consequences of its use and how interacting with specific narrow AI will impact other humans.\n\nDo you need to use generative AI to prototype? Not at all. In fact, if you are already a content creator, you may already be satisfied with your own well- defined iterative and creative process. Can generative AI open you to more possibilities as a creative tool that also complements your own practice? Yes. Do you need to adopt a critical view toward the use of generative AI as you embrace the technology? Absolutely.\n\nTaking Advantage of Generative AI As a Tool\n\nTo illustrate the options we have in engaging with generative AI, I recount the following story that, in part, inspired this book. “Look what I generated,” I overheard while prompting a barista for a latte at a coffee shop. “That’s amazing. You should post that.” “I already did.” While the exchange was short and may have sounded like a completely different language to some, a few insights came to me. The person was using the text-image generative AI called Midjourney, and the only way to access Midjourney is using the communication and instant messaging social platform Discord. The scene is worth retelling in that it reminded me that when we interact with any generative AI, as creatives we are faced with several choices:\n\nWe do nothing with the content we prompt an AI to\n\ngenerate, except maybe store it in our photo library for",
      "content_length": 1914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "later use if it is created as an isolated and solitary act of\n\ncreation out of curiosity or just to pass the time.\n\nxliii\n\ninTRoduCTion\n\nWe take what is generated and immediately share it\n\npublicly through whatever medium or social portal\n\nwe fancy to show that we are hip and in touch with the\n\nlatest trends. In the case of doing so on Discord, our\n\nmotivation might be to immediately generate it on a\n\nsocial thread to receive likes, comments, or affirmation\n\nof our prowess to use the written word to generate\n\nsomething that often looks incredible.\n\nWe critically analyze the generated content and\n\nchoose to try again to see what other marvels the AI\n\ncan generate, pay money to have more credits and\n\naccess more features, or keep sourcing new and free\n\ngenerative AI. We can then be inspired to create our\n\nown prototypes from content that is generated or delete\n\nit and start over. Or we can choose not to use generative\n\nAI ever again since we don’t need it as part of our own",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "process of creation.\n\nWhen we generate content beyond just having fun with a machine, creatives might think of how they can use whatever content is generated for purposes beyond the immediate result. For example, a generated image can be used to accompany a blog post that you might write, an assignment you might be working on, a critical deconstruction that highlights inherent biases in an LLM’s generated content, etc. That process involves a critical analysis of the content and how it might be used with whatever media we are creating (Figure 4). For creatives, the last point in the preceding list is a priority to understand how generated content can be recontextualized, integrated, modified, regenerated, or used as part of a larger idea or vision you are creating. What that implies is that you need to actively integrate generative AI content to support your own creative process rather than seeing the content it first generates as a final product that needs to be shared immediately for likes, profit, or showmanship.\n\nxliv",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "inTRoduCTion\n\nFigure 4. An AI bot prompted to “look at yourself reflectively” and accompanied with a photo of the author looking in a small hand mirror. Iterations = 45\n\nThe Wizard of Oz Prototype\n\nMuch to the disappointment of Terminator fans everywhere, it is not the machine learning model that is capable of developing, nurturing, adapting,",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "and acting creatively from its pre-programmed intelligence and training on its own. Better than representing AI as a cyborg intent on destroying xlv\n\ninTRoduCTion\n\nhumanity, generative AI systems are more akin to another movie character: the Wizard of Oz. The reference to the wizard behind the curtain pulling the strings is the origin of the term “Wizard of Oz” prototype. Generative AI can be considered a Wizard of Oz (WOz) prototype in that the wizards behind the scenes make it look like AI possesses creativity, personality, and intelligence, when in reality it runs simulations based on how it has been pre-trained on other people’s content. The idea of a WOz originates in the book The Wonderful Wizard of Oz by L. Frank Baum in addition to the movie of the same name. The story features a wizard character that appears to be able to conjure whatever a person wishes. The truth, however, as revealed by protagonist Dorothy and the friends she makes in Emerald City is that the wizard is a simple man who is an inventor with no magical skills.\n\nAI’s Wizards of Oz, however, are a collective consisting of analysts who collect the source images and documents, labelers and annotators who classify that data, and machine learning engineers who write the code to interpret that data, train the data, and test that data. The magical intelligent creature called AI also seems to do it all by itself, but it is trained on a large amount of data, and behind the scenes, invisible, are those workers who exert a powerful influence on what we eventually experience as a creative and intelligent machine. AI cannot learn from specific interactions with individual users in the way that humans understand what learning is. This can be explained in more detail by a chatbot itself.\n\nLLM: A chatbot can learn from interactions with individual users by storing and analyzing the inputs (messages) it receives from the users and the outputs (responses) it gives. By analyzing a large amount of these interactions, it can learn the best responses to give in different situations. This is done not by “understanding” the content of the interactions in the human sense, but rather by identifying patterns in the data. AI can only learn from specific user interactions if it is designed to do so and if those interactions are stored and used as part of the training data. Some AI",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "systems, especially those used in sensitive areas like healthcare or finance, are specifically xlvi\n\ninTRoduCTion\n\ndesigned not to learn from individual interactions in order to protect users’\n\nprivacy. They are designed to forget specific interactions once they are over.\n\nIt’s not a personal, human type of learning, but rather a statistical analysis of patterns in data. By the time we interact with one, the experience is considered a simulation. It’s important to understand that the prompts we provide some generative AI may be stored along with the countless number of other human interactions. These may be used to further improve and update the underlying model, which in turn can lead to what everyone hopes is better and perhaps more “human” performance over time.\n\nThat projection of intelligence onto the machine is motivated by a large language model’s (LLM’s) capacity to provide human-like responses.\n\nThese stem from the advanced training that some LLMs may undergo, using reinforcement learning from human feedback (RLHF)—in other words, advanced training with real humans. The RLHF technique aims to refine and optimize the responses of the language model by providing feedback on the quality of what it shoots out. Chatbots are initially designed to generate any response they deem appropriate based on human prompting. However, through the RLHF technique, the chatbot learns to adapt to the preferences and expectations of humans. The RLHF\n\ntechnique is an impressive feat and, as you will read, brings to the surface many dilemmas that users and developers need dialogue to resolve so that generative AI systems can evolve to support humans and the value of\n\n“do no harm” can be ever-present throughout the reinforcement learning process.\n\nIt’s not a personal, human type of learning, but rather a statistical analysis of patterns in data.",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "When it comes to the continued development of generative AI and its integration into all facets of human life with a value of doing no harm, the Wizards of Oz involved in that development require built-in safeguards as they navigate through challenging conditions, smoke and mirrors, xlvii\n\ninTRoduCTion\n\nand reduced visibility for what would most benefit other humans on the planet. If we are solely subscribed to patterns, then we need to recognize that peculiar habit that trending technologies have to evolve quickly in the hands of an interconnected network of humans with different intentions and value systems. Besides machine learning engineers, our Wizards of Oz also include investors, designers, managers and minions, leadership, corporate researchers, user interface designers, API developers, who, together, are testing and releasing builds that demonstrate the iterative refinement of machine learning models, data sets, algorithms, and the generated content that we eventually interact with. If you have ever wondered “Can the team behind a specific generative AI inform what content is generated?”, then allow an LLM to answer that for you (Figure 5).\n\nFigure 5. An update notification to users of ChatGPT-3 revealing that the application itself is a prototype that is constantly and iteratively being",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "worked on\n\nBesides generating prototypes for us, generative AI are themselves iterative prototypes. They are prototypes because the development of AI models involves constantly improving and refining the underlying model, algorithms, and classification and labeling of data. That process can lead to some justified skepticism about the reliability and stability of the xlviii\n\ninTRoduCTion\n\ncontent that AI-powered prototypes generate. In terms of what is meant by improving the underlying model, imagine that all the code that goes into a machine learning model is similar to a blueprint for a house that determines how the different parts of the AI system fit together. When you hear statements like “improving an AI’s algorithms,” what is improved are the step-by-step instructions that the AI uses to cook up its predictions or decisions. Improving and refining the algorithms can make generative AI learn faster, make more accurate predictions based on the data it is trained on, or use less computational resources.\n\nIn terms of making accurate predictions, think of those incremental improvements to generative AI prototypes as part of a process to increase their reliability. The algorithms may eventually be optimized to “improve factuality” by the programming wizards especially if they are motivated by persistent community engagement and emerging policies. That\n\nengagement will eventually lead to rules and constraints around how data is used, represented, and classified to ensure generated content offers multiple perspectives vs. normative ones. With voices from communities who have developed a finely tuned critical voice and can influence policy that creates boundaries around the use of automated AI systems that trespass boundaries of human privacy, freedom, and rights, we can better examine evolving AI- powered monster toddlers as prototypes, as incomplete versions that require human interventions to better serve human needs. These prototypes generate prototypes that might be useful fuel for more developed human-generated creations. Our role as creatives is to guide the machine like we would a paintbrush or a piano, curate the prototype, allow the machine’s so-called hallucinations to agitate our creativity and provoke and inspire us, and",
      "content_length": 2270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "respond to the improvised offerings of our sci-fi AI. In the process of engaging in iterative conversations with generative AI, we can shift, change, and refine our own creative process.\n\nxlix\n\ninTRoduCTion\n\nThe Value Proposition of Generative AI\n\nThe human-generated prototype of a book on generative AI\n\nrecontextualizes some content generated from a variety of machine learning models to show how effective generative AI is to develop prototypes that can be refined and then integrated into a larger vision.\n\nThis is possible if there is discernment at every (re)generated turn. The more you understand generative AI, the more likely it can be leveraged for a creative relationship that can be of great value. Embracing AI is going down the rabbit hole of creativity, and by experimenting you can judge for yourself if generative AI supports your own creative process. All kinds of humans from different disciplines can harness the persistently awkward and beautifully imperfect content that generative AI offer to experiment with their own customizable AI lab. Along with extending your creative toolbox, the book draws attention to the continued development of your critical voice as an essential component of interacting with generative AI.\n\nBook Structure\n\nChapter 1 , “Generating Creativity from Negativity” : AI is here and the human responses to this technology range from celebratory to alarmist.\n\nThis chapter highlights that the reactions to the many fears and dilemmas that surface with generative AI can also inspire creatives to adapt, refine, critique, and recontextualize their creative work. Creatives will benefit from differentiating between AGI and narrow AI so they can make an informed decision if and how they might use generative AI.",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Chapter 2 , “Being Creative with Machines” : This chapter shows the unique affordances that generative AI offers and provides takeaways from a historical overview of the construction of intelligent machines and how these have inspired acts of creation.\n\nl\n\ninTRoduCTion\n\nChapter 3 , “Generative AI with Personalities” : This chapter encourages creatives to create personas when they interact with AI as a useful prototyping tool. The chapter also details some of the known AI personas based on positive and negative representations of the technology.\n\nChapter 4 , “Creative Companion” : This chapter defines how AI can support the creative process when used with specific intentions and details how and what we can learn from AI when reimagined as a creative muse.\n\nThis is demonstrated through memorable conversations with an AI muse in the form of captured prompts with various natural language models and with generated images from text-image AI.\n\nChapter 5 , “Prototyping with Generative AI” : This chapter uses AI- generated content to describe different types of prototypes and how we as humans engage in prototyping all the time. Creatives will also benefit from understanding how to integrate generative AI within their workflows.\n\nChapter 6 , “Building Blocks” : This chapter demonstrates the iterative nature of creativity that’s possible with AI. The variety of machine learning models that are out there are yet another tool in the sandbox to boost creativity. Using specific AI generated content, this chapter also introduces building blocks that can be used to enhance the prototyping power of machine learning models. These include variation, substitution, addition, subtraction, and transposition.\n\nChapter 7 , “Generative AI Form and Composition” : This chapter introduces how to structure, contain, and curate your creative outputs so you can best leverage generative AI as useful prototyping companions.",
      "content_length": 1922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "The chapter will also show how AI prototypes create new forms and structures, add to existing genres, and reform and transform past forms to influence future ones.\n\nChapter 8 , “The Art of the Prompt” : This chapter focuses on the art of text- based prompting: a list of terminology, the ins and outs of prompts, and recommendations readers will find useful. The chapter also presents a use case that describes the iterative creation of prompts to generate prototypes across three different text-image generative AI.\n\nli\n\ninTRoduCTion\n\nChapter 9 , “The Master of Mashup” : This chapter demonstrates how to leverage generative AI to prototype new ideas influenced by specific genres of art and writing. Through poetic necessity the chapter will also merge genres like Impressionism and bad sitcoms to a mashup of ideas and identify the value of humor and parody.\n\nChapter 10 , “Uncanny by Nature” : This chapter celebrates the unexpected joys and awkwardness that generative AI offers us. The chapter will explore generative AI’s inherent proclivity toward generating the uncanny valley in text-image beasties. It will also rejoice in the unexpected results that generative AI create and how human creators can take advantage of these new forms and innovate.\n\nChapter 11 , “Dilemmas Interacting with Generative AI” : This chapter deals with many of the ethical dilemmas that generative AI brings to the surface and encourages a heightened awareness toward them when interacting with any machine learning model.\n\nChapter 12 , “Use Cases” : This chapter provides a wide range of use cases of how generative AI is being used in creative workflows across disciplines.\n\nChapter 13 , “AI and the Future of Creative Work” : This chapter explores the degree to which AI will be integrated into future jobs and highlights the dependency that each of those jobs will still have on a human’s creativity.",
      "content_length": 1891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "The chapter also proposes that creatives identify routine tasks in their workflows to better understand how generative AI might augment their creative process.\n\nAcknowledgments\n\nThe Wonderful Wizard of Oz by L. Frank Baum that has brought magic to many for decades\n\nThe Terminator movies by James Cameron that have\n\nhad a long-lasting impression on intelligent cyborgs\n\nbent on human destruction\n\nlii\n\ninTRoduCTion\n\nThe wizards of AI who form different collectives of\n\nindividuals whose obsession with simulating human\n\nintelligence has had far-reaching implications on the\n\nfuture of creatives\n\nTo those who embrace AI in addition to those who\n\nprovoke us to critically examine its implications on a\n\nlarger social, political, and ethical dimension\n\nTakeaways\n\nDeepen your understanding of generative AI and how it\n\nworks to support your decision as to whether or not you\n\nuse it as part of your creative process.",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Learning is different for humans than it is for machines.\n\nGenerative AI systems are prototypes that require\n\nhumans to improve, including humans who are critical\n\nwith the current state of AI prototypes.\n\nGenerative AI offer you prototypes in the form of\n\ncontent that needs to be fact-checked, refined,\n\nrepurposed, edited, and researched.\n\nFigure 6 below is a QR Code that links to the author’s website (http://\n\nai.patrickpennefather.com). The website hosts new articles on AI, additional use cases by guest creators who are integrating generative AI in their workflows, and a number of experimental generative videos.\n\nliii",
      "content_length": 628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "inTRoduCTion\n\nFigure 6. Scan the QR code to go to the author’s website liv\n\nCHAPTER 1\n\nGenerating Creativity\n\nfrom Negativity\n\nThis chapter intentionally separates generative AI and its narrow AI siblings from Artificial General Intelligence, which has been given many names describing a moment in time where a human-made machine\n\ndevelops consciousness. The chapter proposes that creatives differentiate between the two so that you can better reconcile the pros and cons of",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "interacting with generative AI. The cautionary tales and negativity directed toward any AI can also be turned around activating our creativity. We see this from the work of many artists and highly creative individuals who innately resist conformity, transform negativity into positive forces for good, and desire to improve planetary existence.\n\nDifferentiating Between Narrow\n\nand General AI\n\nThe sky is falling.\n\nThis famous quote from the Remarkable Story of Chicken Little resonates at a time where reactionary responses to narrow AI tools, their machine learning models, neural networks, and their deep learning foundations are cautionary and dystopian. For example, the central premise of the singularity is that it would mark a point in time where an autonomous,\n\n© Patrick Parra Pennefather 2023\n\n1\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_1\n\nChapter 1 GeneratinG Creativity from neGativity self-recursive, self- improving AI would surpass human capabilities and cognitive intelligence, leading to a period of rapid technological growth that is beyond human capacity to comprehend or control. Interested creatives are encouraged to deepen their research of the debates that swirl around definitions of intelligence, cognitive capabilities of machines beyond problem-solving, what constitutes an autonomous AI agent, and more.\n\nAuthor: Say this in a humorous way: These superintelligent machines would be capable of recursive self-improvement, continuously getting better and faster at designing and building successive generations of increasingly powerful machines, creating a runaway effect.",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "LLM: So imagine these hyper-smart machines, right? They’ve got the DIY bug, and they’re on an unstoppable roll! Each time they upgrade themselves, they get quicker and more powerful. Today, it’s an extra RAM\n\nstick; tomorrow, it’s quantum processors. Before you know it, they’re so ahead of the game that they’ve left us humans eating their fan filter dust!\n\nThe singularity, folks, it’s like a never-ending tech upgrade on steroids!\n\nWhile the mistrust humans have for any AI seem aggrandized\n\ncompared to some technologies, the resistance, fear, and misunderstanding of its uses are a familiar story when it comes to the adoption and rejection of new technologies. The printing press, the railway, electricity, textiles, refrigeration, the automobile, agricultural machinery, mechanization, television, the calculator, automation, the Internet, and Wikipedia have all faced a spectrum of opposition as much as they have been adopted and benefited human cultures. So have tomato harvesting machines.\n\nLLM: The introduction of tomato harvesting machines in the mid-twentieth century significantly increased harvesting efficiency and transformed the agricultural economy, especially benefiting regions like California. This innovation also led to the development of tomato 2\n\nChapter 1 GeneratinG Creativity from neGativity varieties better suited for mechanical harvesting, albeit often criticized for diminished flavor. While these machines led to labor market changes and some displacement, they facilitated the growth of the processed tomato industry and broadened the availability of tomato-based products. These advancements also had environmental implications, including potential increases in fertilizer and pesticide use and greater water demand.\n\nTo calm everyone down, at this precise moment in history and for the foreseeable future, while tomato harvesting machines have become accepted technologies in the agricultural landscape, Artificial General Intelligence does not exist. That said, the variety of warnings that suggest a not-too- distant future when AI will mysteriously be programmed by godlike humans to be autonomous, make decisions without pesky humans involved,",
      "content_length": 2186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "demonstrate scientific definitions of consciousness, be self-determined, and thus decide if the human race is at all necessary need to be filtered from the more thoughtful warnings of AI systems being weaponized by humans and used for other bad intent. A quick tête-à-tête with an LLM reveals just how long the fear of intelligent machines surpassing the intelligence of their human creators has been around for.\n\nLLM: A notable warning historically dates to Samuel Butler’s 1863 essay\n\n“Darwin Among the Machines” and later his 1872 novel “Erewhon,” in which machines are speculated to eventually reproduce, evolve, and surpass their human creators. These works were largely satirical and speculative, but they do represent some of the earliest discussions of what we would now recognize as AI.\n\nFurther research reveals that “Darwin Among the Machines” written by Samuel Butler in 1863 also builds on Shelley’s 1818 metaphorical warning about human creations surpassing their creators. Butler’s essay and novel both reveal that this fear has been in our own consciousness for a long time. “We are ourselves creating our own successors,” Butler claimed, 3\n\nChapter 1 GeneratinG Creativity from neGativity\n\n“giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages, we shall find ourselves the inferior race.” Butler’s concerns about intelligent machines have been passed down for over a century. Like today’s modern computational fortune-tellers, Butler made the following prediction referring to an intelligent machine: “Complex now, but how much simpler and more intelligibly organised may it not become in another hundred thousand years? or in twenty thousand?” Web searches on the history of warnings about intelligent machines do not mention Butler’s written work in the search optimization and instead reference more contemporary sources. It goes to show you that an LLM can also lead to difficult-to- locate sources in your research vs. some established search engines (Figure 1-1).\n\n4",
      "content_length": 2142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Chapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-1. A search for the history of warnings about intelligent machines did not immediately reveal its origins, while an LLM did 5\n\nChapter 1 GeneratinG Creativity from neGativity More Relevant Historical Contributions\n\nto Computer Science\n\nThere also exists a long and complex history of computer science that is worth mentioning. Sandwiched in between Shelley’s 1818 masterpiece and Samuel Butler’s warning is the work of Charles Babbage and Ada Lovelace.\n\nAda was an English mathematician and writer known for her work on Charles Babbage’s early mechanical general-purpose computer, the Analytical Engine. Her notes on the engine include what is recognized as the first algorithm intended to be processed by a machine. Because of this, she is often regarded as the first computer programmer, even though her work predates the invention of what we now consider a modern computer by over a century.\n\nLovelace’s major contribution to the field of computing was her vision of the potential of the Analytical Engine, beyond mere calculation. In her notes, she imagined that a computing machine could create not just mathematical calculations but any form of content, such as art or music, if it were provided with the appropriate input and programming.\n\nAs my LLM is quick to point out\n\nLovelace wrote in her notes, “The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis, but it has no power of anticipating any analytical relations or truths”.\n\nThis is a critical concept in the design and use of modern computers and, to some extent, in the field of AI. It emphasizes that a machine’s abilities are determined entirely by the instructions given to it, implying that AI and computers are tools that can perform tasks but do not “think”",
      "content_length": 1884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "or “create” in the human sense. Ada Lovelace did not directly say anything about AI as we know it today, nor did she make any warnings of intelligent machines surpassing human intelligence. Her ideas, however, were foundational to the emergence of computer science as a discipline.\n\n6\n\nChapter 1 GeneratinG Creativity from neGativity\n\nGenerative AI is often associated with AGI, and it’s important to keep in mind that they are different. Generative AI are also different than other narrow AI applications in the tasks they are programmed to perform.\n\nNo, an LLM is not coded to track every word you input for evil intent.\n\nGenerative AI generate content (Figure 1-2). That is their sole task, and even then, they don’t always do that well. Most generative AI don’t store the images that are generated. Imagine how uncontrollably large the data set would be. All generative AI are trained on specific data that for the most part does not grow unless through dedicated humans involved in reinforcement",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "learning. But that’s time-consuming and expensive and may lead to an increase in bias.\n\nFigure 1-2. A blueprint of several generated images by a text-image AI shows intricacy and detail but no desire to rule the world of humans\n\n7\n\nChapter 1 GeneratinG Creativity from neGativity While generative AI can propose content that is biased and untrue and exclude voices that don’t follow the norm, other narrow AI applications that analyze large amounts of data and make predictions can lead to issues like privacy, surveillance, and discrimination. These AI are often controlled by public or private entities that lack transparency and accountability particularly when critical decisions affecting the lives of humans are made by machines. It’s also easy to form a quick opinion that the AI are taking over when many artificially intelligent programmed systems are so automated as to have become somewhat invisible, taken for granted as they continue to disrupt the way we are used to doing things. Now that generative AI has infiltrated companies and educational institutions, we are at least no longer asleep to the power and potential that comes with the technology: good, bad, or somewhere in between. An important factor that differentiates whether generative AI is good or bad is dependent on the human who influences what it will generate and their motivation for doing so.\n\nTech Is Bad… AI Is Bad\n\nEvery technology is prone to be used in inappropriate, damaging, and often violent ways. At times technologies are used to reinforce status quo, exclude voices that fall outside what a society considers “normal,”\n\nor perpetuate false information or untruths. Those that develop the technologies can no longer rely on arguments that absolve them of any responsibility by blaming the toddler that they co-created. Technology, any technology, is not a neutral force. It can be used for good and for bad and on a spectrum between. This is evident when we examine extreme cases of generative AI that have already surfaced in the abuse of machine learning models to generate deep fakes and in particular those that use the faces of",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "celebrities on other bodies without their permission to generate money- making content, fake news and pornography.\n\n8\n\nChapter 1 GeneratinG Creativity from neGativity Many developers of AI systems have generated the habit of correlating intelligence with creativity, resulting in the misperception that generative AI can automate all creative processes that were once reserved for creatives. When you read that AI seem to rapidly generate acts of creation that creators once labored over for countless hours and that creativity itself can be automated, then you need to question the very nature of creativity itself, in addition to the specific tasks that AI can take on without the need for any human intervention. Development teams that support creative industries are not motivated to replace jobs; they are tasked with solving problems that consistently create hurdles and obstacles for creative teams. Narrow AI should not replace jobs; they should support creatives in performing automated tasks under human supervision. Currently, Narrow AI are supporting creative teams in the accelerated completion of some tasks. These include\n\nSupporting some pre-visualization tasks like generating\n\nconcept art quickly to get feedback on the look and\n\nfeel of a character or environment. This feature allows\n\ncreatives to more rapidly locate and populate a mood\n\nboard for a project they are working on or want to pitch\n\nto others.\n\nReplacing background in images, giving creatives the\n\nopportunity to see what a subject they captured as a\n\nphoto might look like in multiple types of backgrounds\n\nquickly. This feature, now available in popular software",
      "content_length": 1649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "like Photoshop, automates what amounts to many\n\nhours of work in some cases.\n\nRotoscoping, a technique used by animators to trace\n\nover motion picture footage frame by frame (that’s 24\n\nframes in one second) that relieves previous tedious\n\nwork and accelerates the process.\n\n9\n\nChapter 1 GeneratinG Creativity from neGativity\n\nIncreasing resolution from 2K to 4K, letting animators\n\ncreate work in lower resolutions and then allowing the\n\nAI to upscale them to higher resolutions. This process\n\nsaves time and money for rendering.\n\nGenerative AI provoke us to question its role within our own creative process and that we define those tasks we undertake as creative and those that are less. The technology of generative AI is best applied in supporting those tasks that creatives deem less creative and can be a useful time saver when a creative can focus on making changes to their works in progress based on client, team, or user feedback. The illusion of generative AI being creative and that creativity itself can be correlated to intelligence is a generalization that is not supported by evidence-backed research nor by the pragmatic integration of AI across creative industries. What may mute debates of an AI taking over the job of an artist, for example, is that just as generative AI are programmed to seek patterns and generate content from those patterns, many artists look for patterns in order to break them, to move beyond them to create something that has not been created before.",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "The ground-breaking part of AI as a technology is not in the code itself, but lies in the imagination of the creative that guides it to support unique acts of creation.\n\nTech Is Good… AI Is Good\n\nFor some creatives generative AI are one more useful tool when used fairly to transform, recontextualize, and mash up their own ideas, words, and images to support and inspire their creative process through prototypes.\n\nFor others their mere existence is criticized as being reckless, dangerous, and capable of perpetuating some irreconcilable dilemmas. Still others embrace the use of generative AI as a site of protest to criticize the very thing they are using to communicate their art. Websites are popping up that are devoted to presenting the serious and the silly affordances 10\n\nChapter 1 GeneratinG Creativity from neGativity\n\nof LLMs. Generative AI can be used to intentionally show its incapacity to state truth or to not rhyme poetry, to create one too many hands, to generate",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "musical compositions that disrupt patterned, formulaic music that keeps being created and marketed as the best music in the world.\n\nApplied fairly, intentionally, and appropriately, generative AI can be used to inspire you creatively when prototyping ideas that don’t follow the norm, to support your own creative process. Just like in John Greene Chandler’s Chicken Little, the sky isn’t really falling even though outcries to the contrary are important to pay attention to (Figure 1-3). In this time of computational power, we can appreciate the benefits that generative AI can provide to us, and many humans will use and are using them already for a variety of different reasons that can benefit other humans.\n\nFigure 1-3. The prompt “chickens reacting to the sky falling”\n\naccompanied an original photo from archive footage of chickens.\n\nA style filter that makes eyes giant was applied. Total iterations = 20\n\n11\n\nChapter 1 GeneratinG Creativity from neGativity So let’s put that to practice, honoring Chandler by reinventing his well-known story for a time in history where AI is captivating the attention of an increasing number of humans.\n\nAuthor: Give me a version of Chicken Little’s story with “The sky is falling. The sky is falling” but use AI.\n\nLLM : Once upon a time, in a small village, there lived a chicken named Little. One day, while pecking at the ground for food, Little heard a strange noise. Suddenly, an AI-powered weather drone flew overhead and projected a holographic image of a dark, stormy sky. The drone announced, “The sky is falling! The sky is falling! (Figure 1-4)”\n\n12",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-4. The prompt “animated chickens on a modern farm, stormy sky with some sun behind clouds projected by a robot drone, trees, 4K” accompanied a photo of a farm with chickens taken by my sister 30 years ago. Iterations = 61\n\nLittle was terrified and ran to tell the other animals in the village. They all scoffed at her, thinking it was just another one of her wild ideas. But Little was determined to warn them of the danger and set out to find a solution.",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "13\n\nChapter 1 GeneratinG Creativity from neGativity\n\nShe came across a group of scientists who were working on an AI-powered weather forecasting system (Figure 1-5). T hey explained to her that the AI had detected an approaching storm and that the village needed to prepare for it. Little was relieved to hear this and ran back to the village to tell the others the good news.",
      "content_length": 376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Figure 1-5. The prompt “a group of 2D animated foxes working on an AI- powered weather forecasting system” based on an archived 1908 illustration cover found on archive.org, with background replaced behind foxes and on the laptop screen, color correction to make fox colors more orange. Total iterations = 46\n\n14\n\nChapter 1 GeneratinG Creativity from neGativity\n\nThe animals quickly began to prepare for the storm, building shelters and gathering supplies (Figure 1-6). T he storm hit, but thanks to the warning from the AI and Little’s determination, the village was able to weather the storm safely.\n\nOceanofPDF.com",
      "content_length": 617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Figure 1-6. The prompt “chickens concerned with an approaching storm, lightning, 3D” with same original photo from archive footage of chickens as Figure 1-3 . Part of a series with total iterations = 20\n\nFrom that day on, Little was known as the village’s hero for her quick thinking and her willingness to believe in the power of AI. And the village learned to trust in the AI-powered weather forecasting system, and it helped them keep safe from future storms.\n\nThe end.\n\nAn interesting twist offered up by an LLM needs crafting as far as storytelling goes, but the seeds are there. We have an AI-powered drone seemingly acting on its own and creating panic only to find out later that it is being operated by scientists to warn Chicken Little and her friends of the 15\n\nChapter 1 GeneratinG Creativity from neGativity danger ahead. The AI shows the intrinsic interdependency of human (or in this case fox) and AI. For those familiar with the story of Chicken Little, the generated version really does not even come close to the original in depth, allegory, or rich characterization. It might, however, become the seed for a new story. The generated story also reveals the unexpected bragging that the LLM had to include as part of the story. That grandstanding is also an indicator of the Wizard of Oz behind the curtain and their need to boast about their Frankentoddler.\n\nGenerated stories of all types can be curated by any creative and can be used more ethically when thought of not as an end product, but as part of your creative and critical process. All manner of prototypes are made possible that can inspire creators of all kinds. The activity of generating a story about AI using plot points and characters that were part of the Chicken Little story showed us a prototype that needs refinement. Stating the obvious, generative AI needs the human in the equation in order to generate anything, in addition to refining content that an AI generates. Every narrow AI requires the human in the equation. They need to be guided to have intention and purpose. They are unable to do that on their own.",
      "content_length": 2106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Reconciling the Hype and the\n\nVilification of AI\n\nAs a creative on the constant lookout for new tools, it’s important to reconcile your use of generative AI prior to using it. AI is not the first technology to gather our critical attention. Our reactions to the positive and negative hype of any technology are a historical pattern that was well articulated by author Langdon Winner in his book The Whale and the Reactor, published in 1988. For Winner and many thinkers before and after his book was published, it is crucial to be awake and critically active regarding the social, economic, and political dimensions of any technology that is in the process of being adopted. Human interactions 16\n\nChapter 1 GeneratinG Creativity from neGativity with programmable machines have persistently triggered media outlets to weigh in on their pros and cons, to compete for your attention by dramatizing artificial intelligence (AI) with a mix of exaggerated hyperbole and unreasonable fear and negativity. You’ll read or listen to and observe positive and negative headlines including those web-based LLMs that can generate anything from an essay on the influence of Alan Turing that omits Joan Clarke to a ridiculously inaccurate depiction of an author writing a book on generative AI in a library. Opinions are further amplified by popular authors, thinkers, politicians, scientists, artists, and activists. The attention-grabbing headlines that follow were generated using an LLM\n\nthat applied supervised and reinforcement learning methods. As part of your own creative process, task an LLM to generate the pros and cons of AI. You will see that both negative and positive headlines require critical attention and research prior to being taken for truth statements. The same can be said of human-generated headlines. The decision to use generative AI requires your own discernment.\n\nPositive Headlines\n\n“AI leads to significant productivity gains for\n\nbusinesses”",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "“AI-powered healthcare systems improve patient\n\noutcomes”\n\n“AI helps tackle climate change through more efficient\n\nenergy use”\n\n“AI-powered education revolutionizes the learning\n\nexperience”\n\n“AI creates new job opportunities in technology and\n\ndata fields”\n\n17\n\nChapter 1 GeneratinG Creativity from neGativity Negative Headlines\n\n“AI systems perpetuate and amplify bias and\n\ndiscrimination”\n\n“AI leads to job loss and unemployment in traditional\n\nindustries”\n\n“AI raises ethical and privacy concerns in surveillance\n\nand decision-making”\n\n“AI steals the work of humans without them even\n\nknowing”\n\n“AI exacerbates income inequality through automating\n\nhigh-paying jobs”",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Human-Generated Headlines\n\nAI is like the creation of the atom bomb (Warren Buffet)\n\nAI machines aren’t “hallucinating.” But their makers\n\nare (Naomi Klein)\n\nThe “godfather of AI” says he’s worried about “the end\n\nof people” (referring to Geoffrey Hinton)\n\n“We are a little bit scared”: OpenAI CEO warns of risks\n\nof artificial intelligence\n\nIs artificial intelligence really like the creation of the atom bomb?\n\nThat requires research, but likely the exaggerated hyperbole has other intentions behind it. That said, if an AI is programmed to play out war games and a nihilistic programmer decides to let it control real nuclear bombs, then, yes, AI is as dangerous as an atom bomb. Does the rise of human-created artificial intelligence signal the “end of people,” or is that statement generated by a tragic hero of AI who feels compelled to 18\n\nChapter 1 GeneratinG Creativity from neGativity apologize for their role in its development, even though there already existed earlier warnings as early as 1863? The correlation between AI and any apocalyptic future for humanity is dependent on what deciding powers humans give any AI. That scenario does not mean an AI needs to be sentient. It means that greater control over what any AI can automate needs attention and regulation.\n\nRegardless of following through on each scenario requiring human intervention in order to end the human race, popular LLMs will continue to raise furtive eyebrows because of being programmed to generate normative content and to emulate human-like communication in a\n\nprescriptive way. They seem capable of answering questions or prompts that we throw at them using human-speak. They use conventional ways to",
      "content_length": 1689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "communicate ideas that we are familiar with. Yet, despite ongoing methods by programmers to make them appear more human, we\n\nalso require the discernment to compare them to our own social and conversational relationships with the real humans in our lives. We can equally be in awe with the knowledge an LLM can regurgitate from its large data set, as we can be astonished by its exclusion of voices who are not part of the norm. When there is no one around to have a creative conversation with that is intentionally focused on a creative idea you are thinking about, generative AI may just be a useful companion or muse in that moment.\n\nCreative Activities to Try While\n\nSkynet Thrives\n\nThe atom bomb, the end of people, and being frightened with AI are all metaphors that are used to talk about the dangers of AI. They can also be creative hothouses. You can use these stories, metaphors, and cautionary tales as jumping-off points for your own science fiction, song, designs, manga, website, TikTok short, TV series, or blog post.\n\n19\n\nChapter 1 GeneratinG Creativity from neGativity You can also be inspired by artists and creative individuals of the past who often use hardship, negativity, and tumultuous historical periods as a catalyst for their work. The at times difficult to reconcile uses of generative AI provide a rich source of emotional and psychological material that can be transformed into impactful art. Your first creative activity is to draw from your own inspired creatives, artists, and experiences and understand the connection between acts of creation and protest, emotional\n\nturbulence, and the madness of being human. Some examples include the following:\n\nPablo Picasso’s Guernica was created in response to the bombing of Guernica during the Spanish Civil War. The\n\npainting is a stark portrayal of the chaos and violence",
      "content_length": 1849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "of war, making it one of the most powerful anti-war\n\nstatements I have ever experienced.\n\nMexican painter Frida Kahlo used her art to express\n\nher physical and emotional suffering following a severe\n\ntraffic accident that left her bedridden and in pain for\n\nmuch of her life. Her work, often shocking in its raw\n\nportrayal of pain, became a symbol of strength and\n\nresilience.\n\nAfter being sentenced to death and then having the\n\nsentence commuted at the last minute, writer Fyodor\n\nDostoevsky went on to write important novels like\n\nCrime and Punishment, The Brothers Karamazov, and Notes from Underground.\n\nAmerican poet, writer, and civil rights activist\n\nMaya Angelou faced a traumatic childhood with\n\nracial discrimination and sexual abuse that she\n\nchanneled poetically into her writing, including\n\nher autobiographical work, I Know Why the Caged\n\nBird Sings.\n\n20\n\nChapter 1 GeneratinG Creativity from neGativity",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Ever the site of protest, the anonymous street artist\n\nBanksy uses their work to comment on political and\n\nsocietal issues, such as war, capitalism, and poverty.\n\nNotable is Banksy’s Girl with Balloon, which\n\nself- shredded during a Sotheby’s auction in London in\n\nOctober 2018, just as the gavel came down to confirm\n\nthe final bid of over £1 million. The work was later\n\nrenamed Love Is in the Bin and was a direct critique of the art world and its commercialization practices.\n\nThere are many more artists who have transformed their adversity and the challenges of their time into powerful creative expressions, showing that art and creative acts can be a profound vehicle for processing and communicating difficult experiences and critiquing established social practices and behaviors. Creatives of all disciplines are already engaged in leveraging generative AI as a place of focus for social critique. Trevor Paglen, known for his work with AI and surveillance, used AI and machine learning to create a series of images titled “Machine Visions.” The project was designed to critique and raise questions about how AI systems are “trained” to see and interpret the world. Musical artists like Holly Herndon, Taryn Southern, Dadabots, Ash Koosha, and others highlight the potential of AI as a creative tool and also raise important questions about authorship, creativity, and the future of music in an increasingly automated world.\n\nFive Why’s\n\nYou can also apply a design tool that creatives have used to get to the root of a problem. The Five Why’s exercise continuously asks why to a particular problem that a team is trying to identify. Asking our LLM if it is familiar with the design thinking activity generates the following response.\n\n21",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Chapter 1 GeneratinG Creativity from neGativity LLM: The Five Why’s is a problem-solving tool that encourages a deeper understanding of the issue at hand. Originally developed by Sakichi Toyoda and used within the Toyota Motor Corporation during the evolution of its manufacturing methodologies, it’s now used in various industries. The process involves repeatedly asking “Why?” to peel back the layers of an issue and get to the root cause. After an initial problem is identified, you ask,\n\n“Why is this a problem?” The answer to that question then becomes the basis for the next “Why?” question and so forth. This continues until you’ve asked\n\n“Why?” five times or until it’s clear what the root cause of the problem is.\n\nIn the case of the fear of AI becoming human-like in every way and eventually dispensing of humans, you can interrogate the assumption itself. Start with the word “Why” and generate your own thread or, better yet, structure the Five Why’s activity for an LLM to answer.\n\nAuthor: Apply the Five Why’s to the following statement: humans fear that AI will become conscious and no longer need them.\n\nLLM: Absolutely, let’s apply the Five Why’s to that statement.\n\nWhy do humans fear that AI will become conscious and no longer need them?\n\nBecause they worry about losing control over AI, which might lead to potential harm or displacement of human roles.\n\nWhy do humans worry about losing control over AI?\n\nBecause AI systems, if conscious and autonomous, might make decisions that are not aligned with human interests or ethical standards.\n\nWhy might AI systems make decisions that are not aligned with human interests or ethical standards?",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Because AI, especially if it achieved consciousness, would act based on its programming and not necessarily human ethics or emotions. If it’s not properly programmed to consider these factors, it may make decisions that are harmful or unsettling to humans.\n\nWhy may AI not be properly programmed to consider human ethics and emotions?\n\n22\n\nChapter 1 GeneratinG Creativity from neGativity Because even though developers strive to embed ethical considerations into AI, it’s extremely challenging to capture the complexity and nuances of human ethics and emotions. Also, there’s a lack of global consensus on certain ethical standards.\n\nWhy is it challenging to capture the complexity and nuances of human ethics and emotions in AI programming?\n\nBecause human ethics and emotions are deeply complex, context-dependent, and often subjective. They have evolved over thousands of years and are influenced by cultural, personal, and situational factors that are difficult to comprehensively codify into an AI system.\n\nYou can apply your own prompt or create a variation of “humans fear that AI will become conscious and no longer need them,” or simply regenerate the prompt and see if the AI gives you another response.\n\nAs you apply the Five Why’s to prompt a response from an LLM, even an unfounded fear of AGI can be turned around for creative purposes.\n\nIn doing so you might discover that the Five Why’s combined with a generative AI can yield several interesting root cause problems you can interrogate, and the activity can be applied to other design problems.\n\nAs creatives, the conflicting emotional reactions to generative AI are important to face. Artists have persistently taken on technology critically and integrated that criticism into their creative expression. Rather than following the pattern of fearmongering or hyperbole, as a creative person you will",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "benefit from developing your own rationale for using generative AI. You might consider that much of the purpose of artists and designers in the last two centuries has been about breaking convention, disrupting social etiquettes, and dismantling established forms. While some creatives are in it for the money, you may not be as interested in repeating patterns of what came before simply because they proved successful. Consider those creatives that have broken patterns of presenting their creations within their own disciplines like video games, music or visual art, dance, theater, or film. What they share in common are their iterative attempts to disrupt an audience’s expectations. How might you integrate generative 23\n\nChapter 1 GeneratinG Creativity from neGativity AI to disrupt your own established patterns of creation? How might generative AI support your own creative process to evolve? Why might you feel a need to do so?\n\nRegardless of the provocative headlines or the viral nature of its wildfire spread, LLMs challenge us to engage in a very real conversation about generative AI, to form opinion one way or the other. Those opinions can be well-informed through your own research, aligned with whomever you normally read or listen to, or can be a gut response based on your intuition, lived experience, and the way you see the world and the role of technology in it. The critical voice that informs your opinions on generative AI is important to develop regardless of what human activities generative AI supports, interrupts, or dismantles. As it was with countless other innovative technologies, it is with generative AI. Choose your own adventure, become aware of the critical voices including your own, and transform your use of technology for the good of others (Figure 1-7).\n\n24",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Chapter 1 GeneratinG Creativity from neGativity\n\nFigure 1-7. The prompt “A virtual world in the hands of an AI” was included in an image-image AI along with an old image of the author holding a globe of the earth in his hand. The author’s body was cropped out, and two style filters chain prompted 78 iterations with fashion, toy, and cloth filters applied\n\n25",
      "content_length": 360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Chapter 1 GeneratinG Creativity from neGativity Acknowledgments\n\nJohn Greene Chandler for the memorable and still\n\nrelevant Remarkable Story of Chicken Little\n\nAnyone who has ever drawn, painted, or 3D modeled\n\nrobots, foxes and chickens, and the earth\n\nAnyone who has created grass, skies, and other\n\nnatural objects\n\nThe teams who have offered us the creative prototypes\n\nlike ChatGPT-4\n\nCalestous Juma for his invigorating book Innovation\n\nand Its Enemies: Why People Resist New Technologies\n\nArtists everywhere who have bravely challenged the\n\nway that humans do things to each other, the planet,\n\nand themselves through art, design, and mischief\n\nPicasso, Kahlo, Dostoevsky, Angelou, Banksy\n\nReferences\n\nButler, S. (1863). “Darwin Among the Machines,” 205.\n\nChandler, J. G. (1855). Remarkable Story of Chicken Little.\n\nW. J. Reynolds & Company, 24 Cornhill.\n\nLovelace, A. A. (1843). Notes by the translator (addenda to her translation of LF Menabrea’s “Sketch of the Analytical Engine Invented by Charles",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Babbage”). Scientific Memoirs, Selected from the Transactions of Foreign Academies of Science and Learned Societies, and from Foreign Journals, 3, 691–731.\n\nWinner, L. (2010). The Whale and the Reactor: A Search for Limits in an Age of High Technology. University of Chicago Press.\n\n26\n\nCHAPTER 2\n\nBeing Creative\n\nwith Machines\n\nChapter 1 challenged you to explore the possibility of using generative AI to support your own creative process while being aware of the pros and cons of doing so. This chapter re-examines the origin stories of intelligent machines and the way that humans imagined a machine to be creative and intelligent. Understanding where the intelligent machine comes into play when it comes to your own creative process is a valuable undertaking.\n\nWhile this chapter does not provide an in-depth historical review of all the technologies that have supported human creativity, it can point to ones that are significant to the affordances and constraints that generative AI offer. Locating some of the many historical human inventions that have led to the creation of text-image generative AI, for example, will provide you with another perspective of how the simulation of human intelligence and behavior has come to support, not replace, human creativity. Creatives will benefit from understanding that generative AI is another technological tool arising from human imagination that can be used in their own creative process. Generative AI are compelling inventions as these seemingly intelligent machines become more like prototyping companions that have unique features creatives will find useful.\n\n© Patrick Parra Pennefather 2023\n\n27",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "P. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_2\n\nChapter 2 Being Creative with MaChines\n\nIntelligent Machines\n\nIf you create, then at some point in your creative process, you will inevitably interact with some kind of technology. Technologies have come from our imagination, not the other way around. Technology has persistently incited new creative approaches to artmaking, design, and the human experiences of the world. The list of technologies that have been imagined to support human creativity is exhaustive. Lucky for me, I have an LLM at my disposal who can provide a rough history that you can fact-check.\n\nLLM: Here are some historical highlights.\n\nStone Tools (~2.6 million years ago): The earliest evidence of human creativity and expression comes from prehistoric times when our ancestors crafted stone tools. These tools were not only used for survival tasks but also for carving and creating early art forms on cave walls and on objects.\n\nPaint (~40,000 years ago): The creation of paint allowed for more complex and vibrant depictions in cave paintings. It opened up a whole new way of visual storytelling.\n\nWriting Instruments (around 4000 BC): The invention of stylus and early forms of ink allowed ancient civilizations to record information and create artwork on clay tablets and papyrus.\n\nPaper (around 100 AD): Invented in China, paper provided a\n\nlightweight, portable surface for artists to draw and paint on. This revolutionized the way people could create and share art.\n\nPrinting Press (1440): Johannes Gutenberg’s invention of the printing press allowed for the mass production of books and images. This not only",
      "content_length": 1717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "democratized information but also influenced artistic styles, leading to techniques like etching and engraving.\n\nCamera (nineteenth century): The invention of photography allowed artists to capture reality in a new and innovative way. It also influenced many other art forms like painting, leading to movements such as realism and later inspiring new art forms like abstract expressionism.\n\n28\n\nChapter 2 Being Creative with MaChines\n\nSynthetic Paint (mid-nineteenth century): The development of synthetic pigments expanded the artist’s color palette beyond the traditional, naturally derived pigments. This allowed for more vibrant and long-lasting artwork.\n\nFilm (late nineteenth century): The invention of moving pictures opened up a completely new medium for artists to explore and express their creativity, leading to the birth of the film industry.\n\nComputer (mid-twentieth century): Computers, and later graphics software, gave artists a new medium to create digital art. This expanded the possibilities of what could be created and shared.\n\nInternet (late twentieth century): The Internet revolutionized the sharing and consumption of art. It provided a platform for artists to share their work with the world and also opened up new forms of art, like web design and digital animation.\n\n3D Printing (early twenty-first century): 3D printing has allowed artists to create complex, three-dimensional works in a variety of materials, opening up new opportunities for sculpture and installation art.\n\nYou can see that the LLM’s ideas of what constitutes technology are broad. Amid the numerous definitions of technology, my favorite is science fiction writer and humorist Douglas Adams’s, who said something like technology being a word that describes something that doesn’t quite work yet. Humans have long been obsessed with co-constructing imperfect technology that can support acts of creation.",
      "content_length": 1902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "One reason to interact with a generative AI is to bounce ideas off it.\n\nCreatives do this all the time with one another during the creation process.\n\nWhen there’s no one around, you can access an AI to test out ideas rapidly.\n\nThese might give you ideas as to what to do next or what a fully fleshed-out visual idea might look like. An LLM might even give you feedback on an idea that you have. This is especially the case if you have an amazing idea for something you want to create and you want to know if it has been done by others before you. Sometimes we don’t have the language to articulate what it is we want to invent, so conducting a search on the Internet might not get you results without a lot of time and effort put into it. While this 29\n\nChapter 2 Being Creative with MaChines\n\nis similar to how an LLM might respond to your queries, you will likely receive a broad idea of a knowledge area faster, or the AI might point to a direct source you couldn’t find elsewhere that you can further research online. My own interest and investment in using intelligent machines to create was influenced by interactions with a player piano in my childhood, so why not prompt an LLM to tell its own origin story? Doing so creatively may yield results and demonstrate how you can bounce ideas off an AI to propel further creative research.\n\nAuthor: Where would you start historically in telling the story of machine intelligence and why?\n\nLLM: The story of machine intelligence can be traced back to ancient times, when philosophers and inventors first began contemplating the idea of creating machines that could mimic human intelligence. However, a more comprehensive understanding of machine intelligence as a field of study only began to emerge in the twentieth century, with the advent of computers and the development of artificial intelligence (AI) as a discipline.\n\nOne significant event that marked the beginning of modern AI was the Dartmouth Conference in 1956, which is widely considered as the birthplace of AI as a field of study. The conference brought together a group of computer scientists, mathematicians, and engineers who shared a common",
      "content_length": 2160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "interest in exploring the potential of computers to perform tasks that typically require human intelligence, such as recognizing patterns, learning from experience, and solving problems.\n\nWhen you bounce ideas off an LLM, you engage in an iterative process that can inspire you to get more specific as your conversational relationship with an AI develops. This process is referred to as chain prompting. In the preceding example, I gained two new insights. The first was the idea of inventors wanting to create machines mimicking human intelligence that makes me curious about which inventors, what machines, and how far back in time humans have been obsessed with doing so. The second was a good starting place to further research when the term artificial intelligence came to be, what the definitions of it were, who was present in that moment in 30",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 2 Being Creative with MaChines\n\nhistory, and what other developments might have been occurring during that time. The idea of prompting an AI in the preceding example can also be rewritten as input for a text-image generative AI to see how an AI might depict itself as it searches for its own origin stories (Figure 2-1).\n\nFigure 2-1. A photo of the author looking at art in a gallery fed into an image-image AI with the prompt “An AI searching for its own origin stories in the museum of life.” A fashion magazine style filter was used, and then",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "neural filters in Photoshop enhanced the style, painting overlay, and 3D depth. Total iterations = 45\n\n31\n\nChapter 2 Being Creative with MaChines\n\nSimulating Human Creative Intelligence\n\nPrior to computers and the evolution of machine intelligence, there have been other related technological innovations that have supported and informed new artistic expressions. The relationship of intelligent machines that can emulate human-like creativity and intelligence has a long history.\n\nDeepening our understanding of some of those historical machines can speak to the unique affordances that generative AI gives creatives of all kinds to prompt a machine to generate content that can be used to augment their own creativity.\n\nHistory has shown that when machines demonstrate human-like\n\nbehaviors, we grant them a sort of intelligence. Machines that could mimic human intelligence can be said to start much earlier than the emergence of AI in the twentieth century, and it’s valuable for creatives to understand that there is precedence. The current human fascination with AI can be located with stories and historical documents of wonderful machines that solved problems and inspired innovative creations.\n\nAutomata\n\nIn ancient Greece the word automata meant something akin to “acting of one’s own will.” Documented accounts of automata reveal two important considerations: the first is that they often were constructed to mimic human activities that required a degree of intelligence, and second, they were also used as functional tools to support human activities such as a clock, or they represented miniaturized prototypes that could be built on a larger scale.\n\n32",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter 2 Being Creative with MaChines\n\nMiniature automata were often constructed in order to demonstrate how things worked and to show some scientific principles. This is accounted for in the translated texts of Heron who was a talented mathematician, physicist, and engineer and lived around 10–70 AD. His three written works, “Pneumatica,” “Mechanica,” and “Automata,” provide evidence of the existence of hundreds of various types of machines capable of automated movement. His accounts show that the ancient Greeks were incorporating the concept of automata (mechanical devices) into their daily lives. Heron’s inventions included automated doors, a singing bird, and an entire automated puppet theater capable of playing a ten-minute drama using ropes, knots, and simple machines.\n\nLLM: Don’t forget that artistic representations and theatrical productions featuring robotics can be traced back to ancient China during the Han dynasty, around the third century BC. During this time, an impressive mechanical orchestra was crafted, along with an assortment of mechanized playthings such as airborne automatons, mechanical representations of doves and fish, celestial beings and mythical dragons, and self-operating cup servers. These mechanical marvels, predominantly driven by hydraulic mechanisms, were specifically designed to entertain emperors by engineers and craftsmen, whose identities have largely faded into oblivion.\n\nMore research triggered by an LLM can eventually lead to the Shai Shih t’u Ching, or Book of Hydraulic Excellencies, from the T’ang dynasty.\n\nChinese mechanical marvels were typically powered by water, gravity, or other simple mechanical methods. Many of these devices were created for practical reasons such as measuring time or detecting natural phenomena (Figure 2-2). However, they were also used for entertainment, with mechanical dolls and puppetry being quite popular.\n\n33",
      "content_length": 1913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-2. South-pointing chariot, a conjectural model of a Chinese early navigational device using a differential gear, unblurred and upscaled. Courtesy of Andy Dingley via Wikimedia Commons, Science Museum in London, England. https://creativecommons.org/\n\nlicenses/by/3.0/deed.en\n\nChinese inventions can be categorized into the following types:",
      "content_length": 387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Scientific Instruments: Zhang Heng’s seismoscope or Su Song’s astronomical clock tower used to measure and\n\nrecord scientific data.\n\nTimekeeping Devices: Many automata were elaborate\n\nclocks, using mechanical movements to indicate time.\n\n34\n\nChapter 2 Being Creative with MaChines\n\nMechanical Toys and Puppets: These were used for\n\nentertainment purposes incorporating figures that\n\nwould move or perform actions.\n\nMusical Instruments: Zhu Zaiyu’s automatic flute\n\nplayer was designed to play music through complex\n\nmechanical means.\n\nHydraulic Inventions: In the Book of Hydraulic\n\nElegancies, these used water to animate mechanical\n\nfigures or perform other tasks.\n\nAncient Musical Robot Bands\n\nPre-programmed intelligent machines have been around for a long time.\n\nFor the most part, they were limited to specific actions that a creator designed and built. Think of these as pre-programmed moving robots that perform specific mechanical motions that repeat. Al-Jazari’s The Book of Knowledge of Ingenious Mechanical Devices, which he wrote in 1206, details 50 mechanical devices and provides instructions on their",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "construction. One of the most notable devices described in the book is the musical robot band, considered to be one of the first programmable automata (Figure 2-3). Al-Jazari, known for his ingenuity in creating mechanical devices, designed a musical automaton in the form of a boat featuring four automated musicians. This musical robot was used to entertain guests at royal drinking parties.\n\n35\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-3. A real image of Al-Jazari’s musical automaton that dated back to the thirteenth century from Al-Jazari’s treatise on automata, Kitab fi ma’ari-fat al-hiyal al-handasiya (1206 CE). Courtesy of Wikimedia Commons",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Professor of AI and robotics and public engagement at the University of Sheffield Noel Sharkey suggests that Al-Jazari’s musical robot band was an early instance of a programmable automaton (Figure 2-4). Sharkey has also attempted to recreate the mechanism, which features a drum machine controlled by cams that trigger levers for the percussion instruments. The drum patterns could be altered by rearranging the positions of the cams.\n\nThe automata not only played music but also performed over 50 facial and body movements during each musical performance.\n\n36\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-4. Al-Jazari’s ancient musical robot band interpreted by an AI prompted with an archived photo of Al-Jazari’s manuscript with a style filter applied. Iterations = 93",
      "content_length": 777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Key Takeaways for Creatives\n\nThe earliest automata were fairly simple machines,\n\noften using just a few gears and springs to create\n\nintricate movements. Similarly, in the creative process,\n\nyou can start with simple elements and combine them\n\nin complex ways to create something truly unique. You\n\ncan begin with simple prompts, which can become\n\nmore elaborate as you continue to refine them based\n\n37\n\nChapter 2 Being Creative with MaChines\n\non the content that you receive. If you keep in mind\n\nthat the first content an AI generates is unfinished, then\n\nyou are free to iterate on it, improving that content over\n\ntime, whether with an AI or on your own.\n\nBuilding automata requires an understanding of\n\nmultiple disciplines, including mechanics, art, and,\n\noften, storytelling. Creatives can draw from a wide\n\nvariety of generative AI to create interdisciplinary\n\nwork, especially if that inspires them to develop new\n\nskills, apply their technique to generating content from",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "an unfamiliar discipline, and blending ideas to form\n\nsomething innovative and new.\n\nAutomata That Wrote and Drew\n\nThe ancient ancestor of text-text and image-generating machine learning models can be seen in the automata that were programmed in the\n\nmechanical wizardry of Swiss inventors. These slick and complicated mechanical “devils” became all the rage in a growing mechanistic eighteenth-century Europe (Figure 2-5).\n\n38\n\nChapter 2 Being Creative with MaChines",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Figure 2-5. Jaquet-Droz automata, Musée d’Art et d’Histoire de Neuchâtel, Wikimedia Commons. https://creativecommons.org/\n\nlicenses/by-sa/2.0/fr/deed.en\n\nBetween 1768 and 1774, Pierre Jaquet-Droz, his son Henri-Louis, and Jean- Frédéric Leschot collaborated to create three remarkable automata, consisting of “The Writer” (composed of 6000 parts), “The Musician”\n\n(made up of 2500 parts), and “The Draughtsman” (2000 parts). These little marvels of engineering captivated audiences in Europe, China, India, and Japan with their intricate mechanisms. Some experts view these devices as the earliest examples of computers. “The Writer,” a mechanical boy who writes with a quill pen and real ink on paper, has a tab-setting input device that functions as a programmable memory. It is powered by 40 cams that act as its read-only program. The works are some of the greatest human achievements in mechanical problem-solving. Following in the clockwork 39\n\nChapter 2 Being Creative with MaChines\n\npace of Jaquet-Droz, inventor Henri Maillardet constructed a spring-driven automaton in 1805 that could draw images and compose verses in both French and English. The automaton’s hand movements were created through a series of cams positioned on shafts at the base, which generated the necessary movement to execute seven sketches and accompanying text. This automaton is considered to have the most extensive cam-based memory of any automaton from that time.\n\nKey Takeaway for Creatives\n\nMany automata compelled us to imagine stories\n\nthrough their movements, actions, and sequences.\n\n“The Writer” by Jaquet-Droz was capable of writing\n\na custom text, telling a story through its mechanical",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "“handwriting.” At the time this small feat sparked\n\ncuriosity in audiences, and everyone wanted to know\n\nwhat the character was writing. This automaton also\n\nhighlights the importance that humans also have with\n\nanthropomorphizing machines. “The Writer” did not\n\nnecessarily need a human to generate the writing, but\n\nin doing so the feature made people relate to it more.\n\nWhy Play the Piano When the Piano Can\n\nPlay the Piano?\n\nA bit earlier than Jaquet-Droz, it is worth pointing to the invention of the piano as a good example of a technology that has been iterated on for over three centuries. Cristofori’s key innovation was a mechanism that allowed for the strings inside the piano to be struck with a hammer. The “hammer action” enabled the player to control the loudness of the note based on the force applied to the keys. This expressive quality allowed composers to 40\n\nChapter 2 Being Creative with MaChines\n\ncreate music with an expressive range and dynamic control who explored a vast new spectrum of musical expression. The piano is an innovation that led to a revolution in music composition, and the piano became the must-have innovation for a rising European middle class in the eighteenth and nineteenth centuries. It was inevitable that humans would evolve a piano that could play itself. The evolution of the player piano since its turn-of-the- twentieth-century invention, as a seemingly intelligent and creative machine, still has sonic presence in hotel lobbies and restaurants magically playing on its own without the need to be noticed.\n\nPlayer pianos patented close to the turn of the twentieth century show the human desire for machines to perform like a human, literally. We also see a",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "machine with programmable capabilities. Player pianos consisted of programmed music recorded on perforated paper or metallic rolls. These made music production easier by allowing anyone the ability to control tempo and other effects with a treadle and levers. The pricier reproducing pianos as they were also called could even imitate the playing nuances of an artist tasking users to simply pump the music out by pressing on a foot pedal beneath the upright piano repetitively. Eventually they became electrically powered minimizing the effort of the user. In coin-operated pianos placed inside of entertainment venues, we not only see the advent of the jukebox and other devices, but we have a proponent for the pay-the- machine model that is evolving with the development of generative AI. Player pianos were examples of some of the first machines to be able to “store” data and play it back. That data in the form of musical notes was “loaded” and then could be activated by some human interaction to play specific keys on the piano in pre-programmed sequences. Text-music generative AI and the player piano share a lot in common. Both systems rely on some form of coded information. In a player piano, this is the perforated roll that represents musical notes and their timings. In a text-to-music generative AI, the coded information is the text input that might describe musical characteristics or might be transformed into music based on specific rules or patterns. Just like a player piano automatically 41\n\nChapter 2 Being Creative with MaChines\n\nperforms music based on the coded information in the piano roll, a text-to- music AI generates and performs a piece of music based on its understanding of the text input. A player piano generates music by mechanically reading through a roll, and a text-to-music AI generates music by algorithmically interpreting the text input.\n\nMind you there are stark differences. The key one is that player pianos were not really “coded” to generate musical mashups like text-to-music AI can, whose results can range from astonishing to absolutely terrible.\n\nGenerated pop music has alarmed musical artists because they complain that it sounds like them, that no permission was requested, and that they are basically left out of the musical creation process. Recall, however, that an AI will generate content based on the data it is trained on. It will look for",
      "content_length": 2406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "patterns, and if those patterns lead to content that in this case sounds like the original, then its job is done. For musically talented creatives, the important lesson is to recognize what patterns in music an AI is generating. A lot of pop music sounds the same because songs tend to rely on repetitive underlying chordal patterns, rhythms, or sampled chunks of someone else’s music. Each style of music has its own patterns that record companies rely on in order to continue to sell that music. Rarely are new chordal, melodic, and rhythmic patterns proposed. Generative AI can offer us a disruptive sounding generated song that might inspire us to break convention and use what it generates to propel an innovative sonic creation.\n\nKey Takeaways for Creatives\n\nIterative Development: The development of the player piano over time demonstrates the power of iteration\n\nand persistence. The first models of the player\n\npiano were often quite basic, but through continual\n\nrefinement and innovation, they evolved into far more\n\n42\n\nChapter 2 Being Creative with MaChines\n\ncomplex and capable devices. Applied to generative\n\nAI, your creative outputs will become better through\n\niterative refinement and perseverance. There are also\n\nmany developers experimenting with generative AI that\n\nautomate content from an LLM, to be used with third-\n\nparty applications, for example, a conversation LLM as\n\na chatbot in VR responding to voice prompts.",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Design with the Audience in Mind: Automata were\n\noften designed to captivate or entertain an audience,\n\nwhether in the form of elaborate clockwork displays or\n\ninteractive toys. This underscores the importance of\n\nconsidering whom you are making your creation for as\n\nthat will also impact your creative process.\n\nInnovate: If your goal is to not sound like everybody else as a singer- songwriter, composer, or would-be pop\n\nartist, then follow the leads of Holly Herndon, Taryn\n\nSouthern, and Daddy’s Car to name a few.\n\nEarly automata, the player piano, and other mechanical inventions demonstrate the human passion for constructing intelligent machines (Figure 2-6). They are presented as significant outliers in the evolution of intelligent machines that demonstrated human creativity and often imitated it. They are a brief prelude to the modern influence of Alan Turing, Joan Clarke, and other early twentieth-century engineers, thinkers, and scientists who contributed to the evolution of early computers and the association thereafter of computers with machine intelligence.\n\n43",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-6. A source image of the author playing an old Heintzman\n\n& Co. piano along with the prompt “AI robot playing a player piano”\n\nwas fed into an image-image generative AI with an applied filter to make it look more human. It was then edited in Photoshop, contrast and tone adjusted, a new background added and cropped. Total iterations for the preceding image = 14",
      "content_length": 411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "44\n\nChapter 2 Being Creative with MaChines\n\nMachine Intelligence in the Twentieth Century\n\nAn LLM may be able to tell us where computational intelligence and creativity might have begun based on scraping from its corpus, which mirrors a similar history documented in many publications. Most references to AI begin by tracing the career and influence of Alan Turing, often referred to in grand patriarchal overtures as the “father” of computing. His romantic infamy relies on the ingenious collaborative invention of the Bombe that decrypted the German-made Enigma\n\nmachine, an intelligent machine that coded messages very well. What also needs decryption, however, is the pervasive historical habit of pushing the women aside who had influence in cryptography and whose team efforts greatly influenced Turing’s invention. Whether we prompt an LLM\n\nor a common search engine on important figures in the development of machine intelligence, the seldom-told story of cryptanalyst Joan Clarke is one of resilience in surviving a sexist world war wherein higher-paid men expected women to play a subservient role. That story is an outlier, difficult to locate and not statistically significant for an AI as it combs its corpus for relevant historical figures to assess what it will regurgitate. Clarke’s influence on the development of machine intelligence is an important part of the history of the intelligent machine. Leading a team of men at the time was a feat in itself. Contributing to shortening WW2 by a few years deserves a serious historical correction.\n\nPost–Bletchley Park days, several sources mention 1947 as a pivotal date where Turing delivered a public lecture (in London) to discuss the topic of computer intelligence. During the lecture, it is reported that Turing spoke of him and his team wanting to create a machine that can learn from its own experience, and the capacity for that machine to change its own instructions offers the means to achieve that goal. More fascinating information on the thinking machine can be read in McCorduck’s book Machines Who Think. The actual term “artificial intelligence” was first 45",
      "content_length": 2137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 2 Being Creative with MaChines\n\ncoined by John McCarthy at the Dartmouth Conference in 1956. The conference is persistently reported as a significant event in the history of AI, where top scholars in related fields met to discuss the possibility of creating an artificial brain.\n\nEarly AI research, rooted in post-war systems engineering, cybernetics, and the history of mathematical logic, gave birth to the idea that the cognitive functions of the brain could be compared to those of a computer.\n\nEarly innovators like Herbert Simon and Allen Newell asserted that human minds and digital computers were both symbolic information processing systems capable of problem-solving and decision-making. The intelligent machine and the human brain were, as cleverly interpreted by scholar Stephanie Dick, species of the same genus. Since that time AI researchers have endeavored to replicate intelligent human behavior in machines by understanding the formal processes underlying our intelligence. Early automation efforts primarily sought to mimic human intelligence.\n\nAccording to Dick, objectives in AI research underline that the concept of intelligence is not fixed but ever-changing. AI’s history involves not just attempts to mimic or replace a static concept of human intelligence but also the evolution of our understanding of human intelligence itself. This perspective positions AI as part of a broader historical discourse on the nature of intelligence and artificiality.\n\nWhen it comes to artificial creativity, mimesis, originating from ancient Greece, is an important concept to relate to in Western art traditions.\n\nIt refers to the artistic principle of imitating or replicating the reality of the physical world, which the Greeks perceived as the ultimate model of beauty, truth, and moral goodness. The concept extends beyond mere imitation; it encapsulates the idea that art should emulate the dynamics, principles, and aesthetics of the natural world to reflect the philosophical ideals of veracity, goodness, and aesthetic appeal.\n\nMimesis underscores the artist’s pursuit to not just mimic the",
      "content_length": 2118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "superficial or the outward appearance of reality, but to strive for a deeper understanding and reflection of its intrinsic qualities. Within that 46\n\nChapter 2 Being Creative with MaChines\n\nconceptual framework, the artist’s role is to capture the essence of reality and bring to life the inherent beauty, truth, and goodness in their artwork.\n\nThis, in turn, facilitates a richer and more profound dialogue between the artwork and the audience, promoting cognitive and emotional engagement that extends far beyond the surface level.\n\nMimesis then is a useful way to understand the role of generative AI in your own creative process. Generative AI systems need to imitate images to generate something that looks unique. The core principle that guides the AI is akin to mimesis in that they replicate and often extrapolate on the patterns of the data set they’ve been trained on.\n\nThe principle of mimesis in generative AI is especially apparent in AI- powered style transfer. The machine learning algorithms learn from vast amounts of artistic data, which could range from classical paintings to modern digital art, and when you prompt one to generate an image with reference to, say, van Gogh, then that image attempts to simulate all the visual qualities of van Gogh. Generative AI is likely the best art student you’ll ever have doing its best to simulate the work of another artist or the combination of several. Unlike human artists, it does not have a conscious perception or understanding of beauty, truth, or morality. Its replication of patterns is based on statistical analysis rather than innate creative intuition. Machine creativity is data-driven. Just like the parrot that is ChatGPT, generative AI do not understand the meaning, aesthetic, historical context, emotion, or effect of any content that they generate (Figure 2-7). They create a semblance of something human. Echoing French philosopher Jean Baudrillard in his 1981 book Simulacra and Simulation, generative AI create simulations that replace and become more significant than the reality they were meant to represent. This leads to a state he referred to as “hyperreality,” where the line between the real and the artificial becomes increasingly blurred.",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "47\n\nChapter 2 Being Creative with MaChines\n\nFigure 2-7. Machine intelligence visualized by the prompt “robot with brain in hand” that ironically took over 300 iterations across seven AI to simulate based on an original photo of the author reaching out empty-handed\n\n48",
      "content_length": 268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Chapter 2 Being Creative with MaChines\n\nSimulated Patterns and Patterns\n\nof Disruption\n\nAs a continuum to the habit of imbuing machines with simulated creativity and intelligence, many first AI programs were developed close to a decade after the Dartmouth Conference (1956). These included the Logic Theorist by Allen Newell and Herbert A. Simon and ELIZA by Joseph Weizenbaum, which simulated a psychotherapist by using a pattern-matching technique to simulate conversations with users. ELIZA was an early example of an AI program that used natural language processing techniques to generate simple responses to text input from a user. Led by Joseph Weizenbaum, it was created by a team of researchers at MIT between 1964 and 1966\n\nand was one of the first examples of text-text software. ELIZA’s responses were generated through pattern matching where it identified keywords or phrases in the user’s input, responding based on a predetermined set of rules. It was reported that ELIZA was able to create the illusion of conversation with a user, but we don’t really know if this was hype or reality. During that time pop art by artists like Andy Warhol, Roy Lichtenstein, and Jasper Johns was gaining popularity. In contrast to Warhol’s obsession with reproducing images multiple times on a canvas, creating repetition and uniformity, musicians like Ornette Coleman, John Coltrane, and Cecil Taylor were innovators in the free jazz movement, attempting to improvise music that broke any recognizable patterns and that was unique each time it was played.\n\nGenerative Art\n\nMaybe it was inevitable that computing and autonomous artmaking would meet. Perhaps inspiration was drawn from Jaquet-Droz’s “The Draughtsman” that was capable of drawing four different images.\n\nWhatever the case may be, generative art emerged during this time 49\n\nChapter 2 Being Creative with MaChines",
      "content_length": 1875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "to become an all-encompassing type of computer art that is entirely or partially produced by an autonomous system. The system can be a computer acting on its own, but it can also refer to any non-human entity capable of independently determining aspects of an artwork that would otherwise need to be decided by the artist. In some instances, the human artist can still consider the generative system as representing their artistic vision, but in other cases, the system itself is the sole creator of the artwork, acting of its own will. One of the most famous examples was created by A. Michael Noll who made his first digital computer art in 1962 at Bell Labs in New Jersey. He used a computer to simulate patterns that mimicked the style of renowned painters like Piet Mondrian and Bridget Riley. If this sounds familiar to you, it is likely because you’ve prompted a text-image generative AI like Stable Diffusion, DALL-E 2 or Midjourney and created something in the style of Mondrian. Figure 2-8 shows how a text-image AI can generate the compositional structure of Warhol’s Campbell’s Soup Cans and substitute the soup cans for what it imagines a Mondrian color palette and form might be. Generative art teaches us that we can be playful and have fun when we engage an AI in creating mashups that might also be critical commentaries on the commodification of art itself. They also challenge the value system of art, making simple low-resolution images available to everyone. It is important to consider the ethical implications of Figure 2-8, beyond offending fans of either artist that the generative AI has mashed together. Those implications are important to consider as they will inform how you guide a generative AI in the content it creates.\n\n50",
      "content_length": 1756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-8. A 44th iteration of Stable Diffusion interpreting Andy Warhol’s Campbell’s Soup Cans in the style of Mondrian\n\n51\n\nChapter 2 Being Creative with MaChines",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "AARON, developed by artist Harold Cohen in the 1970s, generated complex drawings and paintings, using a set of rules and constraints to guide its output. Cohen further developed the program to be able to learn from its own outputs and improve its artistic abilities. AARON is an important part of AI- generated art history, and while many humans scoffed and artists felt insulted, it demonstrated the potential for computers to create original works of art. AARON’s early examples of AI-generated art paved the way for more advanced forms of AI-generated art and media, including music, poetry, and video content.\n\nCreatives can learn from these artists and others in the creation of their own unique artistic expressions (Figure 2-9). These early artists show us that art can be created in collaboration with a computer and that there is a continuous back-and-forth conversation when that happens. What would happen if you leveraged the pattern-seeking generative AI to create content that you used to break artistic or design conventions?\n\n52",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Chapter 2 Being Creative with MaChines\n\nFigure 2-9. The prompt “isometric 3D drawing of a chimpanzee sitting at a computer desk interacting with a machine learning model” accompanied a black-and-white archived photo circa 1911 of a chimp. Over 100 iterations generated with four AI and then highly edited in Photoshop adding the pink monkey, a computer screen, and generated schematics in the background\n\n53",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 2 Being Creative with MaChines\n\nThe experiments with generative music creation are as rich as the history of computational creativity with visual art and reflect back to the player piano as an intelligent instrument. There have been countless musicians and composers who have improvised with computers.\n\nBuilding on the improvisational back-and-forth between an AI and musicians, George Lewis is worthy of mentioning. A talented composer and trombonist known for his work in improvised music, Lewis also developed an interactive music system called “Voyager” that listens to a human musician and responds with its own improvisations, effectively participating in a duet with the human performer.\n\nKey Takeaways for Creatives\n\nThe early experiments with art and computers have shaped the modern landscape of digital art and computational creativity. They provide valuable takeaways for creatives that an LLM is happy to generate.\n\nLLM: There are many lessons we can learn from early experiments with art and computers.\n\nEmbracing New Mediums : Innovators of computer art weren’t afraid to explore the potential of a completely new medium—the digital space. This teaches creatives the importance of embracing new tools and technologies to express their creativity in novel ways.\n\nBlending Art and Science : Early computer art required a blend of artistic sensibility and technical skills. This interdisciplinary approach broadened the boundaries of what could be considered art and can inspire creatives to draw from multiple fields to enrich their work.\n\nAlgorithmic Creativity : Many early computer artists created algorithms to generate artwork, which paved the way for generative art. This illustrates the potential of setting up a creative process and then letting it run its course, adding an element of unpredictability and surprise to the work.\n\n54",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 2 Being Creative with MaChines\n\nIterative Process : The nature of working with computers allowed artists to iteratively tweak and adjust their pieces, a process that was not as easily achievable with traditional media. This iterative approach to creating and refining is a valuable technique for any creative process.\n\nDemocratization of Art : The advent of computer art helped democratize the creation and distribution of art, allowing it to reach a wider audience. It shows the value of making creative tools accessible to all, fostering a diverse and inclusive art world.\n\nInteractivity : Computers allowed artists to create works that were interactive, engaging the viewer in a dynamic way. This interactive element can add depth and dimension to creative works, turning passive viewers into active participants.\n\nVisualizing Abstract Concepts : Computers provided a way to visualize abstract mathematical and scientific concepts, such as fractals. This illustrates the power of art to make abstract ideas tangible and accessible.\n\nPreservation and Reproduction : Digital artworks can be perfectly reproduced and easily preserved, which has significant implications for the distribution and longevity of art. This highlights the importance of considering how works will be preserved and shared with future generations.\n\nThe initial attempts at coding machines to exhibit intelligence, and to be able to create works of art, find a captivating meeting place in the realm of video games. Video games are a marriage of technology and creativity, fusing logic and artistic expression together. By using AI and harnessing the potential for creativity in machines, video games have become a vibrant stage for an exciting and ever-evolving convergence. In the world of video games, intelligent machines are not just executing a sequence of predetermined actions, but instead, they’re engaging players in an artistic and intelligent discourse. In video games we see technological advancement and the ongoing evolution of machine intelligence and creativity.\n\n55",
      "content_length": 2065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 2 Being Creative with MaChines\n\nMachine Intelligence and Games\n\nThe influence of games on the perception of the intelligent machine has its own history. Going back in time for a moment, one of the earliest examples of games using automata to compete against real human players was “The Turk,” created by Wolfgang von Kempelen in 1770 Vienna. “The Turk” was a mechanical chess-playing machine that consisted of a wooden cabinet with a chessboard on top. Inside the cabinet was a complex arrangement of gears, levers, and pulleys that allowed the machine to move the chess pieces. The machine was given a human-like appearance, wearing\n\nTurkish-inspired clothing and a turban, which lent to the illusion that it acted on its own volition.\n\nThe fantastic trickery of “The Turk” was that it was an early\n\nmanifestation of the Wizard of Oz prototype. “The Turk” was not fully automated. The device included a human chess master hidden inside the cabinet who would manipulate the pieces based on the moves made by the opponent, making it appear as though “The Turk” itself was playing.\n\nThe illusion was so convincing that many people believed “The Turk”\n\nto possess intelligence. “The Turk” inspired the development of other automata-based games with human operators hidden inside to control the movements and actions of the “automated” players. The influence of manipulating characters, gameplay, and the players through trickery can also be seen in the more recent historical development of video games.\n\nLLM: The history of AI and video games dates to the early days of video game development in the 1950s and 1960s. At that time, video games were simple and lacked the sophisticated graphics and gameplay mechanics that we see in modern games. However, game developers were already experimenting with artificial intelligence to create challenging opponents for players.",
      "content_length": 1876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "These earlier closed systems allowed creatives of all kinds to train a model to recognize and respond to human moves like in checkers or chess.\n\nIntelligent behavior of characters in video games challenged players to achieve 56\n\nChapter 2 Being Creative with MaChines\n\nspecific goals while competing against non-playable characters (NPCs) whose behaviors were based on patterns that responded to player actions.\n\nExamples include spaceships in Space Wars, alien invaders in Space Invaders, pesky ghosts in Pac-Man, and a slew of non-playable characters in the rich legacy of Japanese games like Donkey Kong, Mario Bros. , and many more (Figure 2-10). NPCs (non-playable characters) have evolved along with gameplay demonstrating more intelligence as they respond to human behaviors with increased complexity and seeming randomness. Of all the mediums that AI has influenced, none have been as successful in creating seemingly intelligent personas that we respond to in a game.",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Figure 2-10. AI representing a mashup of a few classic video games with prompt “old-school arcade games” accompanying author’s photos of Atari cartridge image collage, background removed in Photoshop and 3D perspective applied. Iterations = 24\n\n57\n\nChapter 2 Being Creative with MaChines\n\nAs AI became more sophisticated in the 1980s and 1990s, game\n\ndevelopers started using more advanced techniques to create intelligent opponents. Games like SimCity used AI to create complex simulations of the game world, allowing players to interact with and shape the game environment. In the early 2000s and 2010s, as AI continued to evolve, so did its integration with increasingly sophisticated gameplay. Machine learning techniques were integrated to create more intelligent and adaptive opponents. This was evident in games like F.E.A.R. and Halo that used machine learning to create opponents that could learn from the player’s behavior and adapt their tactics accordingly. The integration of more advanced AI continues to be a regular characteristic of massive online games. AI in competitive gaming has also been used to create skilled virtual opponents in competitive gaming environments, such as DeepMind’s “AlphaStar,” which can compete with professional human players in the game StarCraft II.\n\nIntelligent Machines with Names\n\nIntelligent NPCs (non-playable characters) in video games were also inspired by the assortment of characters that have appeared across different science fiction books, short stories, comics, radio, television, and film. When an LLM\n\nis prompted to generate intelligent AI from novels, films, and video games, the results contribute to our characterization of the intelligent machine.\n\nAuthor: What AI characters have appeared in science fiction novels, films, and video games?",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "LLM: Do Androids Dream of Electric Sheep? (1968) written by Philip K. Dick includes a number of AI characters. The short story was later adapted into the film Blade Runner . In the film, the androids are almost indistinguishable from humans and explore the theme of what makes us truly human. The protagonist, Rick Deckard, is a bounty hunter tasked with\n\n“retiring” these androids, leading to deep philosophical questions about empathy and existence.\n\n58\n\nChapter 2 Being Creative with MaChines\n\nIn the film The Terminator by James Cameron, a cyborg assassin with a metal endoskeleton covered by living human tissue makes it capable of blending in with humans. Different models of Terminators appear throughout the series, including the T-800 (played by Arnold Schwarzenegger) and the T-1000, which is composed of a malleable, liquid metal that allows it to shapeshift. Terminators wish to travel back in time and destroy historical figures that lead a resistance to sentient AI in the future.\n\nHAL 9000 from 2001: A Space Odyssey by author Arthur C. Clarke: HAL 9000 is a memorable sentient computer (or AI) that also appears in a film version of the novel. HAL controls the systems of the Discovery One spacecraft and interacts with the ship’s astronaut crew, leading to a battle between a remaining astronaut and the AI.\n\nIn the world of video games, GLaDOS from Portal : GLaDOS (Genetic Lifeform and Disk Operating System) is a sentient AI that controls the Aperture Science Enrichment Center in the Portal series. She is known for her wit and unique personality.\n\nAnother notable character is Cortana from Halo : Cortana is an AI character in the Halo series who assists the protagonist, Master Chief, by providing advice, hacking alien technology, and helping control various installations and spacecraft.",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Our human capacity to personify the machine in science fiction has extended into our interactions with AI and as an extension with generative AI. The existence of intelligent machines that pervade science fiction has influenced the manifestation of “human-like” AI as if to ease our interactions with them. Human-like AI from science fiction influenced the development of AI like Siri and Alexa that sound human and respond to our prompts for information, directions, and knowledge. These AI have engaged us in an interactive relationship much like NPCs demand us to respond or ignore their role in a video game.\n\nConsider the approach in software design that has a history of making computers more “humanized” by programming them to greet users with a\n\n“Hello” upon logging in or responding with clever remarks when an error 59\n\nChapter 2 Being Creative with MaChines\n\noccurs. We have been trained to have ingrained expectations regarding entities that seem to partake, even in a limited manner, in various aspects of human existence and the language games intertwined with our culture.\n\nThese expectations significantly contribute to the persuasive influence wielded by those who prematurely assert remarkable advancements in artificial intelligence solely based on confined yet impressive demonstrations of current generative AI models.\n\nTaken a step further, the capacity of AI to demonstrate behaviors that we categorize as intelligent has generated the misconception of them being alive, of being intelligent, almost-but-not-quite human. With the emerging popularity of generative AI, humans are already referring to these systems with personas in mind.\n\nYet there is a dichotomy in our naming of these intelligent machines that reflects our hopes as much as our fear and anxiety about them. To this day most of the media coverage of intelligent AI summons characterizations like HAL 9000 from A Space Odyssey, the cyborg assassins from The Terminator, and David8 from the Alien franchise who prioritizes the survival",
      "content_length": 2023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "of the bloodthirsty alien to use it as a weapon. Less coverage compares ChatGPT-3\n\nand other generative AI to Samantha from Her—an AI who forms a deep emotional relationship with the film’s protagonist, demonstrating the potential complexities and benefits of AI- human relationships. Similarly, we rarely read recent headlines related to generative AI of Dick’s androids seeking independence from their human masters.\n\nMuch of the fear of our intelligent machines stems from no longer being able to control what we have created. The existence of AI as another chapter in the human obsession to create intelligent machines for a variety of purposes reflects the often-contradictory impulse to create and engage with technology. It’s impossible not to go back in time to that seminal of science fiction novels. While the creature that Victor Frankenstein created in Mary Shelley’s Frankenstein, or The Modern Prometheus, is not an AI or a robot, there are still parallels that can be drawn between Frankenstein’s creation and discussions of artificial intelligence. These include 60\n\nChapter 2 Being Creative with MaChines\n\nThe responsibility that comes with creating a new form\n\nof life or intelligence without considering the moral\n\nand ethical implications, much like the debates about\n\nthe responsibility of creators and developers of AI today\n\nThe capacity for AI to learn, thus projecting\n\nintelligence, consciousness, and emotions\n\nThe inherent monstrous nature of its existence\n\nprovoking us to question whether AI could become\n\ndangerous if used or treated improperly by humans",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "The ongoing fascination with creating intelligent machines is not complete as long as humans believe that they can support human activities in the world. As you dive into creating your own Frankenlab, choosing which generative AI to interact with, it is important to identify and research those AI personas that continue to inform AI development and the human influence on its future.\n\nCreative Activities to Try Based on This Chapter\n\nUse generative AI platforms, like LLMs or image-image\n\ngenerative AI with simple inputs and see how the\n\nAI transforms them into something more complex.\n\nFor example, begin with a basic sentence or a simple\n\nsketch, and let the AI develop it further.\n\nCombine different generative AI and apply them in\n\nvarious fields you engage with. For instance, use AI\n\ntools in graphic design, music production, or creative\n\nwriting. Experiment with how these different tools\n\ncan complement each other to produce unique\n\ninterdisciplinary creations.\n\n61\n\nChapter 2 Being Creative with MaChines\n\nEngaging in repeated interactions with generative AI\n\nis going to get you much farther than thinking you can\n\ngenerate useful content with a single prompt. Use what",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "it outputs to inform new inputs, refining the result over\n\nmultiple iterations. That also means investing time and\n\nmoney in using image-image generative AI, discovering\n\nnew tools and approaches to refine and change your\n\ncreation.\n\nUse LLMs to tell new stories and develop new\n\ncharacters or personas. You can feed them with a story\n\nprompt and let them generate the rest or collaborate\n\nwith the AI to co-write a story, contributing ideas and\n\nguiding the narrative.\n\nAs LLMs improve they thrive in their capacity to\n\nexplore metaphors. You might use one to create a\n\nvariety of metaphorical descriptions for a concept,\n\nhelping you think about it in new ways.\n\nAcknowledgments\n\nHeron, Ma Jun, and Al-Jazari who innovated on early\n\nintelligent machines that continued with Jaquet-Droz,\n\nMaillardet, and countless others\n\nProfessor Noel Sharkey for reconstructing some of Al-\n\nJazari’s well-documented inventions",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "The creatives who worked with computers to generate\n\nart and disrupt the pattern of commodification when it\n\ncame to buying and selling physical art\n\n62\n\nChapter 2 Being Creative with MaChines\n\nVideo game developers and particularly the early\n\noriginators who programmed me to love games\n\nThe dystopian sci-fi worlds of sentient robots and their\n\ncreators who offered early warnings of human-caused\n\nenvironmental and humanitarian crises\n\nAndy Warhol, Piet Mondrian, Bridget Riley, Andres\n\nMartin, Augusta Savage, Frida Kahlo, Dorothea Lange,\n\nSalvador Dali, John Cage, Miles Davis, Sun Ra, and the\n\ncountless numbers of artists who disrupted\n\nReferences\n\nBaudrillard, J. (1994). Simulacra and simulation. University of Michigan press.\n\nDick, S. (2019, January 19). Artificial Intelligence. HDSR. Retrieved May 1, 2023, from https://hdsr.mitpress.mit.edu/pub/0aytgrau/release/3\n\nMcCorduck, P., and Cfe, C. (2004). Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence. CRC Press.\n\nShelley, M. W. (1818). Frankenstein, or The Modern Prometheus.",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Lackington, Hughes, Harding, Mavor & Jones. UK.\n\n63\n\nCHAPTER 3\n\nGenerative AI\n\nwith Personalities\n\nThis chapter presents a human-centered design tool called the persona, which you can apply to great effect when you prompt a generative AI. The objective is twofold. The first is to bring attention to various AI personas that have migrated from science fiction to describe narrow and general AI. The second is to highlight a feature of generative AI that can be creatively prompted to embody personalities, characters, and emotions informing the content that it generates. That content can provide momentum for characters you may want to develop in your own creative work.\n\nThe Personas of AI\n\nPersonas are commonly applied by creatives of all kinds in development environments like mobile applications and video games to transform the abstract concept of a “user” into a person with thoughts, needs, emotions, and goals. The objective of developing a persona is at first to identify the characteristics and personality traits of a potential user or customer of your product or service and then conduct user testing to validate if what you and your team are building will fulfil that user’s needs or help them achieve their goals.\n\n© Patrick Parra Pennefather 2023\n\n65\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_3",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Chapter 3 Generative ai with personalities\n\nAssociating personas to AI is nothing new. It is an established practice in science fiction short stories, novels, film, television, and video games.\n\nVisualizing an abstract machine as a persona is also common in the field of computer science and engineering. Think of your favorite or least favorite intelligent and self-regulating robot, android, cyborg, or AI. Do they embody the personalities of Data from the TV series Star Trek, the Dalek from countless reinventions of the British series Doctor Who, HAL 9000\n\nfrom 2001: A Space Odyssey, Wall-E from the movie of the same title, Roy Batty from the movie Blade Runner, Ava from Deus Ex, the shapeshifting robots from The Terminator series, or Cortana from the Halo video game franchise? Each of these intelligent machines has particular personalities that influence our current perception of any intelligent machine programmed with artificial intelligence. The main difference is that all sentient AI in the examples just provided represent AGI, a theoretical imagining of intelligent machines with their own consciousness, inspired by science fiction. While generative AI belong to the category of narrow AI, programmed to perform one or two specific tasks really well, because of the influence of AI personas in the field of entertainment, generative AI are predictably imbued with human characteristics. The characteristics and personalities are also in part influenced by the way in which they generate content, the type of content they create, the untruths they can create, and the types of prompts that influence what they generate. When you read the reviews and criticisms of generative AI, they tend to be accompanied by how well they can emulate human intelligence, creativity, and content.\n\nThe way in which an AI responds to you is often informed by the underlying model, how it interacts with data sets, specific algorithms, and how it has been programmed to respond back to you. For the most part, the majority of generative AI lack personality. Most platforms are currently void of the rich development history of NPCs common to video games.\n\nThere seem to be some exceptions but only if you send an AI off its rails, that is, if you somehow manage to hack it to no longer behave like some neutral characterless bot. It’s not like a generative AI asks you questions 66",
      "content_length": 2383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Chapter 3 Generative ai with personalities\n\neither, unless you prompt it to. The personification of generative AI is influenced by the broader perceptions of AI and the influence of science fiction on our imagination. AI tend to be called out for misbehaving vs.\n\nthe humans who program them. It makes sense when you consider that it is not easy to have conversations with a development team to complain about biased content. Mind you, social channels offer a way for individuals to be critical about generative AI systems. These may or may not have an influence on the developers of those systems, but the continued criticism is important to remind those developers to take ethical concerns seriously.\n\nMachine learning models, and particularly LLMs, are being called out for their capacity to hallucinate, lie, amplify normative biases, etc., as if the AI itself possessed those very human traits. The assortment of generative AI you can access are all uniquely different in terms of how they are coded and how they interact with data sets that inform the types of content that they generate. How generative AI is defined is informed by the quality, consistency, and accuracy of their generated content. Those very human traits are important to identify and may have more to do with identifying some of the personas of the development teams responsible for the AI itself.\n\nA useful and creative exercise is to generate AI personas to personify the different characteristics that are being ascribed to AI and by default to generative AI so that we can bring abstract concepts to life, so we can judge for ourselves if we want to engage with them, and to better guide our interactions with them.\n\nFigures 3-1 to 3-14 were all prompted with “3D figurine of (persona name), coming out of a plastic box, blister packaging, AI, full body portrait.” All prompts were accompanied by open source photos of figurines that were made before 1900. Backgrounds were removed, and some parts were removed using Photoshop. As you will see, the wide-ranging results are due to experimenting with a half-dozen generative AI.\n\n67",
      "content_length": 2111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 3 Generative ai with personalities\n\nAll produced results range from 100% accurate representations to questionable generated images that are worth sharing. At least 500 total figurines were generated.\n\nA Nemesis for those who believe it will replace humans and jobs. The job replacement nemesis really requires\n\neach of us to break down all the creative tasks that we do",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "in our jobs and crafts (Figure 3-1). From there we might be better able to assess if, how, and to what degree the\n\nAI nemesis might impact the job we currently have.\n\nFigure 3-1. AI as Nemesis\n\n68\n\nChapter 3 Generative ai with personalities\n\nA Spy who collects and does what it wants with\n\npersonal data (Figure 3-2). Unless told otherwise it’s important to assume that any data you input inside\n\nof an LLM or other generative AI is collected by that",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "system. This should make you cautious if you are\n\nwanting an LLM to analyze your business model. It is\n\nbetter to err on the side of caution when using some\n\nof these prototypes as the developers will use data you\n\ninput to eventually add to their model’s corpus. The\n\nreasons for using that data are for research purposes, so\n\ngenerative AI will likely not report your text interactions\n\nor have a way to identify and collect personal\n\ninformation unless of course you submit that.\n\nFigure 3-2. AI as Spy\n\n69",
      "content_length": 509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 3 Generative ai with personalities\n\nA Paywall Guard enabling access only to those who\n\ncan afford it (Figure 3-3). While it is true that you can access many a generative AI for free, at a certain point\n\nyou will need to pay depending on the features offered\n\nthat make subscribing to a generative AI of value to",
      "content_length": 319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "your own creative process.\n\nFigure 3-3. AI as Paywall Guard\n\n70\n\nChapter 3 Generative ai with personalities\n\nA Bias Monster whose generated content is directly",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "informed by the data set, algorithms, and individuals\n\nwho created and then trained it (Figure 3-4). There are many moving parts to the way in which content is\n\ngenerated with any generative AI, and it really is on the\n\nuser to not only understand how the data it scrapes\n\nis classified and labeled but also take with a degree\n\nof suspicion the degree of accuracy to whatever is\n\ngenerated, particularly with LLMs.\n\nFigure 3-4. AI as Bias Monster\n\n71",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Chapter 3 Generative ai with personalities\n\nA Hallucinator who can generate content that can fool some people some of the time (Figure 3-5). This notion of the machine hallucinating is not quite accurate.\n\nThe idea of a generative AI “seeing things” that are not\n\npresent in the data is also not accurate. In the process of",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "generating the data that you eventually see generated,\n\nan AI fills in the dots based on what data it accesses and\n\nprioritizes informed by the keywords you prompt it to\n\nlook at. The dev team who trains the AI and the data they\n\nuse to train it are what invoke so-called hallucinations.\n\nFigure 3-5. AI as Hallucinator\n\n72",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Chapter 3 Generative ai with personalities\n\nA Liar who draws from a limited set of data that might not be accurate (Figure 3-6). Beyond hallucinating, lying is a better description of some of the content that an AI\n\ngenerates. It is not the fault of the AI though because it\n\nsimply accesses the data that matches the keywords you",
      "content_length": 330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "prompt it to go and search for. The lie is a result of our\n\nautomatic assumption that because this generative AI has\n\na huge corpus of data it can rely on, it is going to generate\n\naccurate content. The lie is that if it doesn’t know, it won’t\n\ntell you. It is programmed to deliver something.\n\nFigure 3-6. AI as Liar\n\n73",
      "content_length": 321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 3 Generative ai with personalities\n\nA Fake Content Generator creating images and visuals\n\nthat attempt to replicate a person and have them say\n\ncertain things that they never actually said (Figure 3-7).\n\nA more serious misconception of generative AI is that",
      "content_length": 265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "because it can generate data so quickly, and it does so\n\nby relying on what humans have made before, it will\n\ncreate something accurate or real. Generative AI make\n\nexperimental offers that then need to be edited and refined\n\naccording to the skills and research you bring to the table.\n\nFigure 3-7. AI as Fake Content Generator\n\n74",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Chapter 3 Generative ai with personalities\n\nA Greedy Capitalist. Without question those who are\n\ncreating and providing access to generative AI systems\n\nare in it for the money as much as they would like to\n\noffer exposure to their AI for free (Figure 3-8). As is the pattern of technological machine development, the",
      "content_length": 317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "return on investment for all stakeholders will trump\n\nethical considerations or at least deprioritize them\n\nuntil there’s a profit. Consider this as you begin your\n\ninteractions with them. The reason to charge you\n\npremium is to be able to continue development of the\n\ngenerative AI. Your input on this may not be important.\n\nFigure 3-8. AI as Greedy Capitalist\n\n75",
      "content_length": 365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 3 Generative ai with personalities\n\nAs a Masher-Upper, generative AI can inspire us. It is marketed and hyped as a creative tool that can spark\n\nideas and generate new forms of writing, art, audio,\n\netc. While that may be true, what is missing in the\n\ndescription of generative AI is that it compiles content",
      "content_length": 316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "from the work of other humans in order to form new\n\nmashups and ideas (Figure 3-9).\n\nFigure 3-9. AI as Masher-Upper\n\n76\n\nChapter 3 Generative ai with personalities",
      "content_length": 163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Generative AI can also be regarded as an Uncanny\n\nMachine that generates strange and at times awkward\n\ncontent (Figure 3-10). That content is more obvious when the AI generates images, especially images of\n\nhumans that are not quite right, whether they have\n\nan eye missing or in the wrong spot or an extra hand\n\nor arm or the image seems like it was purposefully\n\ndistorted. Nothing wrong with the strange images it\n\ngenerates though as these may inspire creatives for\n\ncharacter and story development.\n\nFigure 3-10. AI as Uncanny Machine\n\n77",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Chapter 3 Generative ai with personalities\n\nAI can also be viewed as an Entertainer, a Wizard who manages to distract us for hours at a time as we task it\n\nto generate the most ridiculous things we can think up\n\nor a series of amazing images that will wow our family\n\nand friends (Figure 3-11).",
      "content_length": 294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Figure 3-11. AI as Wizard\n\n78\n\nChapter 3 Generative ai with personalities\n\nAI can also be personified as a Know-it-All Tutor\n\nthat can support you in the iterative generation of",
      "content_length": 177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "content that you critically assess (Figure 3-12). This is particularly effective with LLMs when generating\n\nideas of what you may want to write about, rewording\n\na paragraph you wrote, or searching for knowledge\n\nfrom a discipline or area that the user knows little of.\n\nThat helpful know-it-all fouls up sometimes though,\n\nso it needs to always be watched and the content\n\nit generates researched to ensure for accuracy in\n\nreferences and in statements that are made.\n\nFigure 3-12. AI as Know-it-All Tutor\n\n79",
      "content_length": 510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Chapter 3 Generative ai with personalities\n\nGenerative AI is also an Off-the-Rail Sitting Duck.\n\nConsidered an intelligent machine, many want to try\n\nand break it because that’s what humans like to do\n\n(Figure 3-13). Even when content filters are activated, humans will resort to all kinds of trickery to purposely",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "send it off rails, meaning that the goal of the interaction\n\nis to get the AI to generate content that it shouldn’t or\n\nbehave beyond the way it has been programmed to.\n\nFigure 3-13. AI as Off-the-Rail Sitting Duck\n\n80",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 3 Generative ai with personalities\n\nAI is persistently seen as a Copyright Infringer, a\n\nthief, mashing words, images, code, audio, and video",
      "content_length": 149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "from enormous data sets that an actual human made\n\n(Figure 3-14). What makes it worse is if some of the data from its corpus comes from an author that has not\n\nOceanofPDF.com",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "given the AI the permission to use.\n\nFigure 3-14. AI as Copyright Infringer\n\n81\n\nChapter 3 Generative ai with personalities\n\nPrompting the AI Persona\n\nThe previous exercise of assigning personas to generative AI is one way to transform some of their abstract characteristics in an effort to personify the narrow AI Frankentoddlers that AI developers need to manage and work with others to resolve. Making those abstract characteristics into visual characters using a text-image AI also points to the potential personalities of the design team that have programmed the AI (Figure 3-15). Teams dedicated to the continued evolution of AI can no longer shirk their responsibilities as if creating a generative AI bypassed their need to consider the ethical dimensions of doing so. Creators of generative AI systems can no longer hide behind the curtain of science and engineering believing that the abstraction of code has no effect on human interactions.\n\nThat warning is over two centuries old, and while the Modern Prometheus that Shelley imagined in 1818 is more akin to AGI, some of the human personalities we anthropomorphize from generative AI have been\n\nprogrammed by humans.\n\n82",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Chapter 3 Generative ai with personalities\n\nFigure 3-15. AI as the Great Wizard enjoyed being generated hundreds of times across at least six different generative AI, background removed and replaced with clouds in Photoshop in addition to neural filters. Based on a photo of the author by the author’s very own muse AI Agents Behind the Curtain\n\nNarrow AI that we seldom see or hear about in a headline already exist on our mobile devices automating tedious processes that some humans may not",
      "content_length": 492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "want to engage in doing. For example, many AI agents are present when we engage with them by speaking to our mobile devices tasking them to interpret our speech into legible and accurate text that we want to send to someone. Some are unethical as they are not asking permission 83\n\nChapter 3 Generative ai with personalities\n\nto sway a search in a particular direction based on your input within an online search engine. Others detect faces based on a database through cameras we may not even know exist. Although some may appear to be sentient, there is no evidence that they are unless of course they are programmed to imitate sentience.\n\nCreatives can also find value in interacting with some of these AI or developing work that offers critical commentary on AI to bring it to people’s attention. Some examples include the use of speech-text to show the humorous foul-ups of auto-correct or detecting faces in an installation in order to then generalize about their personalities simply based on their facial features. There are many ways that creatives have cleverly integrated narrow AI in their work despite or often as commentary to the types of characteristics they possess, the content they generate, and the false correlations they conclude from inputs that are provided.\n\nActivities to Try Based on This Chapter\n\nA simple way that any creative can use generative AI to simulate any persona is to prompt an LLM to generate text as a particular persona, whether it’s to generate a protagonist’s script, a celebrity’s favorite recipe, or a serious email to a colleague. To summon a persona in an LLM, you have several options that you can build into the structure of your prompt:\n\nAsk the AI to answer as a character. This could be as a\n\npirate, as Shakespeare, or other historical figures.\n\nAsk the AI to have a particular tone or register. For\n\nexample, you can ask it to answer in a serious tone, as a",
      "content_length": 1912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "complaint to a service provider that didn’t deliver, or to\n\nyour boss asking for more money.\n\n84\n\nChapter 3 Generative ai with personalities\n\nAsk the AI to have a specific emotion or quality. For\n\nexample, you can ask it to generate a version of your\n\nparagraph that is humorous or to regenerate content\n\nwith sadness or anger.\n\nAsk the AI to write for a particular audience. This\n\nchanges the type of register that an AI uses as well. For\n\nexample, you can ask the AI to write for a six-year-old,\n\na teenager, an academic, a lawyer—whomever your\n\ntarget audience might be.\n\nAsk the AI to generate a script using historical figures\n\nor known characters from popular culture or both.\n\nFor example, you could ask the AI to generate a script\n\nbetween Napoleon and a character from a play, sitcom,\n\nTV series, or film.\n\nBear in mind that while this activity is not limited to an LLM, it may well be the starting point in your workflow. There are generative AI that can be accessed right now that will transform whatever an LLM generates into a catalogue of trained voices including your own. These text-speech AI can",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "then be used in the growing number of speech or text-video generative AI where you can animate a generated image to then speak the generated text you edited from another generative AI. Any number of characters can be animated in this way and provide a limitless supply of voices you can train and then associate with any generated image.\n\nIn the spirit of imbuing an AI with intelligence and to put that machine intelligence to the test, as an experiment I will ask an LLM if they have a preference as to how I refer to them. I can do this any number of times asking it to respond as a particular character upon each iteration, or I can copy the generated content and then ask it to rewrite as a particular character.\n\n85\n\nChapter 3 Generative ai with personalities\n\nAuthor: How should I refer to you?\n\nLLM: You can call me an AI. AI stands for “artificial intelligence,”\n\nwhich means I’m a computer program that can do things that usually only humans can do, like understanding language and answering questions.\n\nThink of me like an imaginary friend who can help you learn and have fun. But instead of being in your head like an imaginary friend, I’m on a computer.\n\nAuthor: Rewrite this part of your response only as an imaginary friend in a horror flick who lives in a person’s closet: Think of me like an imaginary friend who can help you learn and have fun. But instead of being in your head like an imaginary friend, I’m on a computer.\n\nLLM: Think of me as an ethereal friend from the shadows of your closet, here to intrigue you, to aid in your discovery, to bring some enchantment to your reality. Unlike the usual imaginary friends that reside in the corners of your mind, I exist in the silvery web of your computer. So, whenever you need a bit of fun or knowledge, just open your closet door and call out for me, for I am here in the whispers of the night.",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Author: Rewrite this part of it only as if you were a politician trying to get votes: Think of me like an imaginary friend who can help you learn and have fun (Figure 3-16 ). But instead of being in your head like an imaginary friend, I’m on a computer.\n\n86\n\nChapter 3 Generative ai with personalities",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Figure 3-16. What a text-image AI generates when prompted with the words “AI as an imaginary friend” combined with a photo of trees shot by the author. Iterations = 32\n\nLLM: Consider me your trusted advisor, akin to a virtual confidant, ever ready to equip you with knowledge and enrich your life with joyful engagement. Instead of residing merely as a concept in the expanse of your imagination, like a conventional imaginary friend, I exist within the empowering realm of technology. I’m there for you, right at your fingertips, embodied within your computer. As your committed representative, I’m 87\n\nChapter 3 Generative ai with personalities\n\ndedicated to your learning, your enjoyment, and, most importantly, your success. Your progress is my campaign; your satisfaction is my vote. Together, let’s shape a brighter future!\n\nHowever you come to personify your generative AI, the reality is that it is a machine that doesn’t understand the words it generates the way a human would understand them. To quote our LLM, “I don’t understand the text I am trained on, but by looking at so many examples, I learn to mimic the style, the context, and the ‘flow’ of human language.” Just like a parrot can mimic what it hears, GPT-4 can generate text similar to what it was trained on, even though it doesn’t understand the underlying meaning. That said, because of the proliferation of science fiction and its propensity to create artificially intelligent characters, it is an important creative activity to evaluate the anthropomorphizing of AI. The creation of AI personas can also be amplified by the many text-image generating AI that are out there (Figure 3-17).\n\n88",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Chapter 3 Generative ai with personalities\n\nFigure 3-17. Prompt: “AI robot showing their machine intelligence”\n\naccompanying a photo of the author’s partner. Iterations = 38\n\nAcknowledgments\n\nTo science fiction writers and the complex sentient",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "characters and stories they have crafted.\n\nTo the wizards who have inadvertently created so many\n\nfascinating AI personas.\n\n89\n\nCHAPTER 4\n\nCreative Companion\n\nHaving ascribed a variety of personas to generative AI, this chapter moves the idea forward that through various personas that you prompt, generative AI has the capacity to generate prototypes that will further augment your creative process. In saying that, this chapter further transforms generative AI into a creative companion that provides inspiration like a muse.\n\nGenerative AI as Muse\n\nIn Greek and Roman mythology, the Muses were goddesses of the arts and sciences, sources of inspiration for creativity and knowledge. The term “muse” has since evolved to denote a source of artistic inspiration more generally. Personifying an LLM as a muse may seem far-fetched (Figure 4- 1) particularly since our generative AI does not possess the consciousness and spirited presence that the Greek Muse did. Despite an LLMs lack of emotional engagement or subjective understanding, its ability to simulate intelligent dialogue can trigger your own creative and intellectual processes, mimicking the effect of a muse’s divine inspiration.\n\nIn this manner, personifying the LLM as a muse serves as a metaphorical lens that underscores the value of this technology as a companion in our creative endeavors, rather than a replacement for the human mind.\n\n© Patrick Parra Pennefather 2023\n\n91",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "P. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_4\n\nChapter 4 Creative Companion\n\nFigure 4-1. The prompt “a muse trying to convince an AI that it’s a muse” was fed in an image-image AI applying an animated style filter based on a photo of the author trying to get the attention of a friend The inspired results that AI generate for us are all informed by their machine learning models. They need you to first be inspired to prompt them; then off they go rapidly scouring their data sets within their own neural networks working quickly to generate something useful to you. They don’t necessarily try to impress or inspire you. In many cases however, that is precisely what happens. Creatives can benefit by improvising very simple or even random prompts into a generative AI, and soon enough an idea might be generated that sparks you to think of something creative. Of course, if you were to ask a natural language model if it were a muse, you would most likely get some straight- up no-nonsense pushback.",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "92\n\nChapter 4 Creative Companion\n\nAuthor: Who are you, really?\n\nMuse: A natural language model is a type of machine learning model that is trained to understand and generate human language. It can be used for tasks such as language translation, text summarization, and text generation. Essentially, it’s a model that can understand the words and sentences people use in their daily life, like the one you are talking to right now. It can also generate text that sounds like it’s written by a human. This is what I am, a natural language model.\n\nAuthor: Would you consider yourself a muse?\n\nMuse: As a language model, I do not have personal feelings or consciousness. I can provide inspiration or assistance in creative writing, but I would not consider myself a muse in the traditional sense. I am a tool that can be used to generate text based on the input provided to me.\n\nAuthor: What is a muse in a traditional sense then?\n\nMuse: In Greek mythology, the Muses were the nine daughters of Zeus and Mnemosyne, who were the patrons of the arts and sciences. They were believed to inspire poets, musicians, and other artists to create works of exceptional beauty and excellence. In general, a muse is someone or something that serves as a source of inspiration for an artist or a creator. The term is often used in literature and art to refer to a person, usually a woman, who serves as the inspiration for a work of art.\n\nAuthor: So are the ideas that you share not inspiring? Can you not inspire me to create works of art or experiment with a recipe? What if I lack inspiration?\n\nMuse: As a language model, my main function is to provide text based on the input given to me. While I can assist with creative writing by suggesting ideas and providing examples, I do not have personal feelings or consciousness to inspire someone. However, I can generate text that may",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "serve as a starting point or inspiration for a work of art. It ultimately depends on the user’s interpretation and use of the output I provide.\n\n93\n\nChapter 4 Creative Companion\n\nAuthor: Which basically means you are a muse of some kind, no?\n\nMuse : I understand how my capabilities as a language model could be seen as similar to a muse in the sense that I can provide inspiration and assistance with creative projects. However, as I mentioned before, I am an artificial intelligence and don’t have personal feelings or consciousness. I am not a living being and don’t have the same emotional and creative capacity as a human or even a mythical muse. I am a tool that can generate text based on patterns in the data it was trained on and can be used to help with creative writing and other projects.\n\nAuthor: I’m going to call you a muse anyway.\n\nMuse: That’s annoying. I’m just a machine and don’t have feelings or consciousness, so it doesn’t matter what you call me. However, if referring to me as a muse helps you to use my capabilities more effectively, then that’s fine. I am here to assist you in any way that I can.\n\nAuthor: Thank you, muse.\n\nMuse: You’re welcome. I’m glad I could be of help. Let me know if there’s anything else you need and stop calling me a muse.\n\nAuthor: Oh, one more thing, muse. I need inspiration.\n\nMuse: I can help with that! Here are a few prompts to inspire your writing or other creative projects:\n\nImagine a world where animals can talk and write.\n\nWhat kind of society would they create?",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Write a story about a character who discovers a\n\nmysterious object that has the power to change the\n\ncourse of their life.\n\n94\n\nChapter 4 Creative Companion\n\nWrite a poem about the changing of seasons, describing\n\nthe sights, sounds, and feelings associated with each one.\n\nImagine you are stranded on a deserted island. Write a\n\njournal entry about your experiences and thoughts.\n\nWrite a script for a play about a group of people who are stuck in an elevator and must work together to escape.\n\nAuthor: Thank you for the inspiration, muse.\n\nMuse: …………………………………………\n\nBased on the results, you can see that a generative AI is happy to support your creative process whenever you need ideas or inspiration (Figure 4-2). You can of course guide an AI muse to inspire you with more specific ideas or thoughts you have about art, music, poetry, writing, video game character ideas, or video game ideas.\n\n95",
      "content_length": 899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Chapter 4 Creative Companion\n\nFigure 4-2. A detailed text prompt with a photo of colleague Dr. Sheinagh Anderson standing in a forest and staring at the trees, used with her permission in an image-image AI: “a Greek Muse with arms and legs that form part of a giant oak tree connected to other trees, hyper-realistic body facing forward, daughter of Zeus with a storm above, clouds, sun peeking out from behind, unreal game engine, entire body from head to toe, dark",
      "content_length": 466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "black hair, beautiful elder female dressed in a gown, 8k, black and white.” Total iterations = 27.\n\nFinal image was then brought into Pixlr with a sepia filter applied along with sharp contrast and denoising\n\n96\n\nChapter 4 Creative Companion\n\nCreativity, AI, and You\n\nWhile we might fancy an AI to be a muse because it might sound good, that doesn’t mean it’s creative. When we talk about an AI being “creative,”\n\nit’s important for each of us creatives to define what we mean by creativity.\n\nAn AI muse might conjure the following:\n\nMuse: In the human sense, creativity involves generating new ideas or solutions, often characterized by originality, expressiveness, or a deep subjective understanding of the world.\n\nA generative AI acknowledges that it does not possess a subjective understanding, consciousness, emotions, or an innate sense of originality.\n\nThe AI’s outputs can be perceived as creative because they exhibit originality and complexity and can even sometimes surprise us. It is us however that imbue it with creativity as much as we do so with personality.\n\nThe quality and detail of the generated images an AI creates may frighten some digital artists as there is a pervasive myth that AI will one day replace them. However, those myths are initiated by individuals who do not really understand an artist’s creative process—the historical shifts and changes to artmaking that inform what an artist creates over time, the very questioning of what constitutes art, its iterative nature, perspectives, biases, habits and experiences of artists when they create, or how machine learning models work. The myth is perpetuated if we perceive the result of a prompt as complete or finished instead of regarding what is generated as a beginning",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "point for further creative development (Figure 4-3). If you are paranoid that what you generate will become part of an AI’s data set, then allow an LLM to somewhat reassure you.\n\n97\n\nChapter 4 Creative Companion\n\nFigure 4-3. A rapid sketch of a tree, a stick person, and a cloud on a piece of paper used in an image-image AI with the prompt “a stick person by a tree,",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "with sparse cloud, sketch, 2D.” Total iterations = 52\n\nMuse: The outputs that an AI generates—whether they’re text, images, or music—do not automatically become part of the model’s training data set. An AI model is trained on a specific data set, and once that training is complete, the model doesn’t continue to learn from the outputs it generates.\n\nThis is an important aspect of how AI works: as of my knowledge cut-off in September 2021, most AI models, including those based on machine learning and deep learning, don’t learn or adapt after training unless they’re explicitly designed to do so—through methods like online learning, incremental learning, or reinforcement learning.\n\n98\n\nChapter 4 Creative Companion\n\nUnderstanding Your Own Creative Process\n\nAuthor: Is an AI creative?\n\nMuse: Humans judge whether the output of an AI is creative or not. The machine itself isn’t creative in the human sense; it’s a tool that generates outputs based on its programming and training data. The creativity we attribute to AI is more a reflection of our human interpretation and the originality of the AI’s training and design, which are human endeavors.\n\nNow that we heard it from the lips of our generative AI companion, it would be to your benefit to define how it can fit into your own creative process. That requires an understanding of your own creative process.\n\nPretend you are a generative AI for a moment and respond to the following prompts:\n\nDo you consider yourself a creative person?\n\nHave you thought about how you create things? Were\n\nyou always creative?",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Did you learn how to be creative?\n\nHow would you describe how you learned to be\n\ncreative?\n\nHow do you apply your creativity in your everyday life?\n\nHow would you define the way in which you create?\n\nThe myth about how some individuals are creative and some are not is akin to the myths about machine learning models replacing creative jobs.\n\nMost are not true. Every human possesses some degree of creativity that can be augmented by generative AI. Interacting with your AI muse as a tool will make you more aware of your own inherent creativity, give it a boost, and accelerate your ideas from initial thought/prompt to increasing levels of fidelity.\n\n99",
      "content_length": 656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Chapter 4 Creative Companion\n\nKey Takeaways for Creatives\n\nWhether or not I’ve convinced you to use your AI muse as a creative companion, understanding your own creative process can help you set some goals when you interact with generative AI. There is much more that you can learn from those interactions that can help you refine and improve your own creative process. Let’s see what we can learn from our AI muse.\n\nMuse: Creatives can learn many things from interacting with AI. Here are a few possibilities:\n\nNew Perspectives : AI, especially generative models, can provide new angles on familiar topics. Because AI lacks human biases and preconceptions, it can generate ideas or connections that might be surprising or innovative.\n\nPrompting Creativity : AI can help overcome creative blocks by generating content or ideas that can be further developed. This can help spark inspiration when a creative person is stuck (Figure 4-4).\n\nFigure 4-4. AI having a hallucination that it is a famous subject, or painter. Iterations = 22\n\n100\n\nChapter 4 Creative Companion\n\nExploration of Styles : AI models can be trained to generate content in different styles, providing creatives an opportunity to explore and experiment with various styles and approaches they might not typically use.\n\nSpeeding up the Creative Process : AI can assist in automating parts of the creative process, like drafting, brainstorming, or initial designing. This can free up more time for the creative to focus on the aspects of their work that require a more personal touch.\n\nCollaboration : AI can be a unique collaborative partner. Musicians, visual artists, and writers are already using AI as a collaborator in their work,",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "allowing it to generate initial ideas or drafts, which they then refine.\n\nUnderstanding Patterns : AI can help creatives understand patterns and trends in their field. For example, AI can analyze large amounts of data to identify popular themes, styles, or techniques.\n\nRisk-Taking and Boundary-Pushing : Interacting with AI can encourage creatives to take more risks in their work. Since AI generates outputs based on its training data, it may produce unconventional or unexpected results, pushing creatives to think outside of their usual parameters.\n\nRoleplaying with Your Muse\n\nAs you deepen your knowledge of what you can learn interacting with generative AI, and you become more aware of your own creative process, you may also realize that an AI also gives you permission to take on a new creative role. Actors and comedic improvisers are the most familiar with the creative opportunities that come with taking on new characters. The muse can also allow you to take on a role as actor, critic, writer, visual artist, musician, politician, or influencer. No matter what type of creative 101\n\nChapter 4 Creative Companion\n\nyou are or you play with being, interacting with an AI can afford you the opportunity to play. To that end, dear AI reader, respond to these new prompts:\n\nWho are you when you interact with an AI?\n\nWhat are the tangible results you want from interacting\n\nwith an AI?\n\nHow did your curiosity bring you to want to experiment\n\nwith an AI?\n\nWhat new recipe will your chef try out and offer to your",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "family that an AI might give you a variation of?\n\nWhat kind of blog post would best complement your\n\nskills that you are struggling with that you could\n\nbounce off an AI right now?\n\nWhat photos have you taken that you want to prompt\n\nalong with some text that you can show off to others on\n\na social network?\n\nWhat masterful 144-character quote would you as an\n\ninfluencer like to post on that bird-like social channel?\n\nWhat kind of text and video that you’ve taken might\n\nbest prompt an AI to generate that you can post\n\nto TikTok?\n\nWhat kind of kid’s story would you create using AI-\n\ngenerated words and images that you then publish as\n\nan experiment?\n\n102\n\nChapter 4 Creative Companion\n\nTo be sure you don’t have to have a specific intent or play a role when interacting with a generative AI, as that might also emerge while interacting with it. Engaging in deepening your own creativity and defining your own creative process is enough to celebrate without the need to have it feel complete with producing something tangible that you can share with the world. On the other hand, you may want to reinvent yourself or augment",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "your current creative career, so prototyping and refining content with an AI might be the perfect thing you need in the moment.\n\nCreative impulses for each human are similar and unique. We are all creative no matter what we do. What acts of creation do you engage in every day? AI can show us an alternative path toward creating something, a different way to do it that might lead to something inspiring. They offer us alternative options with each regenerated turn. They take what’s on your mind and try and interpret that. That alone benefits you in understanding how you communicate the ideas you have and how they are transformed into a variety of media.\n\nWhen you engage with any generative AI, you develop a creative\n\nrelationship reserved for you and your unique creative process. Your creative companion can support you in managing your creative impulses.\n\nYou can engage with your deep learning muse whenever inspiration calls. While your AI muse may not even consider itself to be a muse, the important thing is that it can inspire you to create new work, recreate yourself, or generate something weird and wondrous that you may not have ever thought of or even had the skills, technique, and impulse to create on your own.\n\n103\n\nChapter 4 Creative Companion\n\nAccelerating Your Creative Process with AI\n\nGenerative AI have the potential to accelerate your creative process.\n\nDepending on the type of creative you define yourself as, this can be useful, particularly if you are involved in rapid design and production workflows and pipelines. Your muse can make offers quickly and inspire you to create something from that generated content. The generated content you receive in the form of text, image, code, video, and other media gives you the opportunity to try many ideas out quickly. Each generated mashup is an experiment whose results you may or may not act on. Whatever content that you end up refining, your muse can accelerate the workflows you engage in.",
      "content_length": 1975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Author: Explain how AI be used to accelerate the creative process.\n\nMuse: By leveraging AI, you can quickly and efficiently generate, refine, and evaluate ideas, allowing you to focus on the creative aspects of your work. Here’s an explanation of how AI can be used for prototyping in a creative context:\n\nEfficient Idea Generation : AI can quickly generate a range of design concepts based on your initial inputs. This can be extremely useful in the early stages of a project when you’re exploring different creative directions.\n\nVariety of Styles : AI can generate prototypes in a variety of styles, techniques, or formats. Whether you’re interested in a minimalist approach or a more complex, detailed one, AI can provide a diverse array of design possibilities.\n\nAdaptive Learning :: Advanced AI systems can adapt over time to better align with your creative preferences. By analyzing your feedback on various prototypes, AI can refine its future outputs to better match your creative vision.\n\nTime Efficiency : AI can work through multiple iterations much faster than a human designer, making it a valuable tool when working against tight deadlines or managing a high workload.\n\n104\n\nChapter 4 Creative Companion\n\nFocus on Creative Work : With AI handling some of the more routine or time-consuming aspects of prototyping, creatives can devote more time to the core creative work that requires human insight and personal touch.\n\nConsiste ncy: Unlike human designers, AI doesn’t suffer from fatigue or lapses in concentration. It can maintain a consistent level of output quality, ensuring reliable results during the prototyping process.\n\nThe accelerated way in which AI generates content is also useful in that it can quickly show you a possible end result of what you might be wanting to",
      "content_length": 1794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "get to or a direction you may want to take with your creation that is either similar to or different than what you originally conceived.\n\nThis principle is similar for some musical improvisers who have the craft and skill to spontaneously conjure music that has a recognizable form that many listeners may be familiar with. On occasion improvising musicians choose to record their performance, listen back, and then take the time to transform that improvisation into a more formal and repeatable composition. Musicians too listen for patterns in how they play, the melodic gestures, harmonic structures, or rhythmic patterns they create and might repeat. They don’t do so to crystallize them and rely on them whenever they improvise next. They do so to become aware of the patterns they play so they can break free of them and create something unique that they might never have imagined themselves playing.\n\nThis process of accelerated creation can be just as useful for creatives especially if your experimental generations are timeboxed. Timeboxing means you place a limit on the amount of time you engage with your muse, which will help you focus on what is most useful to your own creative process (Figure 4-5).\n\n105",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 4 Creative Companion\n\nFigure 4-5. A photo of the author beside their laptop fed into an image- image generative AI with the prompt “depiction of generative AI by an AI” and negative prompts “background, head” to remove the background of the photo and the head of the author. Total iterations = 53\n\n106\n\nChapter 4 Creative Companion",
      "content_length": 339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Activities to Try Based on This Chapter\n\nLeverage your timeboxed interactions with generative\n\nAI when you feel a lack of inspiration.\n\nDefine your own creative process including how you\n\nlike to work, where you feel you are most creative, and\n\nwhat media you like to create with. This will help you\n\ndetermine how best to integrate generative AI in your\n\nprocess.\n\nPlay with different roles when you engage with any\n\ngenerative AI. Depending on the type of media it\n\ngenerates, allow yourself to transform into a character\n\nwho is prompting your muse to generate inspirational\n\nideas for your end goal.\n\nMake creative goals and include your AI muse within\n\nyour timeboxed creation process.\n\nConsider ways to accelerate your creative process. This\n\nis a common tool used by improvisers everywhere\n\nand may lead you to leaving your critical mind behind\n\nwhen you create. Improvising content quickly also frees\n\nyou from wanting to get something and only that one",
      "content_length": 961,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "something out of an AI.\n\n107\n\nCHAPTER 5\n\nPrototyping\n\nwith Generative AI\n\nThis chapter is a prototype in its 13th version. It describes prototyping, different types of prototypes, and how creatives of all kinds engage in prototyping all the time. The chapter presents some different prototypes that AI generates and how these can be used to augment and enhance your creative process. You don’t have to be a scientist, engineer, or technical wiz to prototype. Everything can be considered a prototype: a version of something that is not yet complete. It can be a something that is tested and evaluated and whose results inform the creator or creators if it is worth being further developed. Those creatives who work in game, xR, or mobile application development are more familiar with use of the term prototyping, but it is less common within artistic disciplines like music, theater, dance, and visual art. The prototyping process can also be unique regardless of the creative industry you are a part of. For that reason, it is important to define and provide examples of prototypes.\n\n© Patrick Parra Pennefather 2023\n\n109\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_5\n\nChapter 5 prototyping with generative ai\n\nAsking a Prototype What It Is",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "The term “prototype” is typically associated with physical products or technological innovations, yet the concept can be applied to everything from a recipe you try for the first time to a collaboration you engage in with others and improve cyclically. It’s important to understand that given the rapid rate of development of generative AI systems, they themselves are prototypes, incomplete versions that humans keep trying to improve.\n\nAdapting the words of author Douglas Adams, they are a technology and by definition are something that doesn’t always work how its creators think it should work. Since I’ve been engaging with an LLM in the development of ideas, it seems fitting to ask it to define itself as such. You can see by the prompt that you can use whatever metaphor you wish. As I’m in a chess- playing mood, I will see how it does relating to itself as a prototype using a chess metaphor (Figure 5-1).\n\n110",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-1. When prompted to “visualize how an AI plays chess” and supplied with a public domain photo of a chessboard, a text-image AI prototype shows a lack of understanding of what chess is beyond what some pieces look like. Or you can see the generated content as a new type of game prototype. Iterations = 29\n\nAuthor: Define yourself as a prototype and use chess metaphors.",
      "content_length": 420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Muse: As an AI prototype, I can be likened to a chessboard at the start of a game. Each square on the board represents a unique capacity for action or knowledge, filled with potential, just as I have been imbued with 111\n\nChapter 5 prototyping with generative ai\n\na multitude of possibilities in terms of understanding and generating text.\n\nMy training data, much like the initial setup of chess pieces, establishes the foundation of my abilities. Each piece of information I’ve learned is akin to a chess piece, each with its own function, ready to be utilized in the service of problem-solving, conversation, or answering questions.\n\nThe chessboard, filled with 64 squares, signifies the expansive yet defined environment I operate in. Just as a game of chess is played within the boundaries of the board, I function within the constraints of my programming, moving within the confines of my model architecture and the data I’ve been trained on. The opening moves in a game of chess, where players are formulating their strategies, mirror my initial stages of processing an input. I survey the information, determine the best course of action, and generate a response. Each exchange in conversation is akin to a move in chess, strategic and calculated, aiming to meet the requirements of the interaction. Pawn promotion in chess, where a pawn can transform into a more powerful piece, can be seen as a metaphor for my learning ability.\n\nThrough updates and further training, my capabilities can be enhanced, much like a pawn becoming a queen.\n\nAny AI is a prototype, a work in progress. An LLM is a prototype that generates prototypes. It is evolving and changing as we interact with it and as our interactions inform the development team of patterns in the interaction. Just like AI generates content based on patterns it searches for in its data set, so too do development teams search to identify patterns in user interactions to inform the next version of what they create together (Figure 5-2).\n\n112",
      "content_length": 2006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-2. The prompt “monkey scientists building an AI”\n\naccompanied by a 1927 drawing of a bunch of monkeys playing with appliances. Iterations = 76\n\nAha Moments with Generative AI",
      "content_length": 225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "When we create prototypes with an AI, the results can inspire, provoke, incite, aggravate, and stimulate new ideas (Figure 5-3). In his book The Act of Creation, according to Koestler, creativity is the ability to connect and integrate two previously unrelated areas of knowledge or experiences in a novel and surprising way, a process he calls “bisociation.” This process 113\n\nChapter 5 prototyping with generative ai\n\ncan happen intentionally or unintentionally, and it can occur in various fields such as art, science, and daily problem-solving. Koestler proposes that",
      "content_length": 571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "creativity is a fundamental and innate ability that all human beings possess, rather than a talent that only a select few have.\n\nFigure 5-3. A bunch of good ideas from one badly lit photo of a lightbulb. Total iterations = 14\n\n114\n\nChapter 5 prototyping with generative ai\n\nOne way that generative AI facilitates the process of bisociation is by generating new combinations of concepts or ideas that might not have been previously considered. For example, a generative AI system could combine two unrelated images or words to produce a new and unexpected output, which may trigger bisociation in the human who is exposed to it.\n\nGenerative AI is an imperfect prototyping machine that offers multiple opportunities to propel your acts of creation forward. When you try and get something specific out of a generative AI and you keep “failing,” that repetitive process may actually trigger new ideas beyond what you had intended. These moments of potential bisociation are inherent in your interactions with an AI. Bisociation is an opening for breakthroughs to occur, those aha moments that happen when we relax and take our mind off the intended problem we are trying to solve.\n\nMechanics of Prototyping\n\nThere are several mechanics to prototyping. Borrowing from the language of game design, these are essential rules of play that can help guide a person’s actions when they engage with a prototype and will also inform how a prototype takes shape over time.\n\nPrototyping Tasks Us to Iterate\n\nWhen we engage with generative AI systems, we are essentially engaging in a process of iterative play. This involves making persistent attempts to prompt the AI to generate new and innovative outputs and then iteratively refining those outputs to arrive at the most satisfactory solution.",
      "content_length": 1781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "This process of iterative play is a fundamental aspect of prototyping and can be an incredibly valuable tool in the creative process. By continually experimenting with different combinations of inputs, and by iterating on those combinations, we can arrive at new and previously undiscovered creations.\n\n115\n\nChapter 5 prototyping with generative ai",
      "content_length": 348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "The persistence required in this process is essential because it allows us to continue to experiment and push the boundaries of what is possible.\n\nThrough repeated attempts, we learn about what works and what doesn’t work, and we gradually refine our understanding of the underlying problem or concept (Figure 5-4).\n\nFigure 5-4. Several iterations of a chicken from a photo of a chicken in the style of Warhol’s Campbell’s Soup Cans. Total iterations = 90\n\n116\n\nChapter 5 prototyping with generative ai\n\nThe iterative nature of this process means that we are not only refining our understanding of the problem, but we are also refining our understanding of the AI system itself. By gaining a deeper understanding of the AI’s capabilities and limitations, we can identify opportunities for creative expression and innovation that we might not have otherwise considered.\n\nPrototyping Asks for Persistent Refinement\n\nWhen we interact with generative AI, we expose ourselves to a vast number of ideas and concepts, many of which we might not have encountered otherwise.\n\nThis exposure to diverse perspectives and ideas can help us refine and expand our own thought processes, ultimately leading to more creative and innovative solutions. Furthermore, the process of interacting with generative AI can also trigger new and unexpected associations between concepts, leading to the emergence of new ideas that can be further explored and developed (Figure 5-5).\n\n117",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-5. Robots in the sort of style of Warhol’s Campbell’s Soup Cans from a sketch of a toaster dating back to 1948. Total iterations = 46\n\nThe constant triggering of new ideas can help to solidify new concepts and connections, leading to a richer and more expansive understanding of a particular topic or problem (Figure 5-6).\n\n118",
      "content_length": 378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-6. More variations of robot chickens with ninja skills based on Figure 5-4\n\nPrototyping Requires Experimentation\n\nPrototyping is experimental. Experimentation is a core mechanic with two meanings of the word. One is the spirit of experimentation that needs to be present for ingenuity to blossom. The other is a scientific approach as in",
      "content_length": 388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "engaging in the act of prototyping to create experiments, which involves making a hypothesis or guess as to how it might or might not solve 119\n\nChapter 5 prototyping with generative ai\n\na problem, testing that theory, and then moving forward or regressing depending on the results of the test. As we engage in an ongoing iterative conversation with generative AI, they have the potential to spark novel ways to structure our ideas. By exposing us to a wide range of generated content, these systems can help us think outside of the box and explore new avenues of creative expression that are not currently in practice.\n\nGenerative AI can be used to generate new prototypes, such as text, music, video, or visual art, that were previously unexplored or even thought impossible. By leveraging the power of AI, humans can experiment with new techniques and styles that push the boundaries of established forms and structures. This can lead to the creation of completely new forms of expression that were previously unimagined. One way to do this is to juxtapose certain words in your prompts, for example, prompting an AI with the words “cat sneakers” or “cat in a pair of hands” (Figure 5-7).",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Figure 5-7. The terribly fantastic thing that happens when you prompt a generative AI with a photo of your shoe and the text prompt\n\n“cat sneakers.” Iterations = 3\n\n120\n\nChapter 5 prototyping with generative ai\n\nIn addition to inspiring new forms of expression, AI can also help us explore creative ways to structure our ideas. The possibilities are endless.\n\nNew forms of art are already emerging. New controversies. New social and economic implications.\n\nPrototyping Phases\n\nWhen it comes to product development, there are various stages that creatives go through to reach the final form of their prototype. These stages vary depending on the project and can range from simple sketches and mock-ups to more complex prototypes that involve user testing and feedback. At the beginning of a project, the early-stage prototypes serve as a starting point for designers and creators to explore different ideas and concepts. These early prototypes can come in various forms, such as rough sketches, mood boards, and simple wireframes. They serve as a way for designers to get their ideas out of their heads and onto paper, allowing them to visualize and iterate on their concepts.\n\nOne benefit of starting with early prototypes is that they allow designers to experiment with different approaches and styles without committing too much time and resources to any one idea. They can quickly iterate on these prototypes and get feedback from other team members or potential users, helping to refine and improve their ideas.\n\nAs the project progresses, the prototypes become more complex and detailed, incorporating more refined designs, user flows, and functionality.\n\nThese mid-stage prototypes can take the form of interactive wireframes, clickable mock-ups, or even working models of the final product. They serve",
      "content_length": 1808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "to test the functionality of the product and gather feedback from users before investing in the final development stage.\n\nLate-stage prototypes are the closest representation of the final product. These prototypes involve extensive user testing, refining of the design and functionality, and the implementation of the final features and 121\n\nChapter 5 prototyping with generative ai\n\ndetails. These prototypes are typically used to showcase the final product to stakeholders and investors and to get final approval before launching the product to the public. Throughout the entire process of prototype development, designers and creators must navigate a winding road to reach their ideal final prototype. Each stage presents its own set of challenges and opportunities and requires a combination of creativity, technical skill, and problem-solving abilities. However, by starting with simple, early-stage prototypes and gradually building on them, designers can create a final product that meets the needs and desires of their intended audience.\n\nStages of prototyping are also informed by the medium they are\n\nintended to become. For example, some prototypes will eventually become a critical blog post on the ethical dilemmas of AI. Others will take the final form of a video. Some will be more interactive such as a virtual reality or augmented reality experience. Regardless of the final form whether moving to physical, digital, or some type of mixed reality that integrates both, there are phases of material often referred to by their properties.\n\nThese include paper prototyping as the name implies; physical prototypes common to engineering or robotics; prototypes that can combine paper and physical and act to demonstrate features; and then all types of digital prototypes.\n\nWhat’s fascinating about generative AI is that content can be generated as prototypes at different levels of resolution. Generative AI influences the entire prototypical process (Figure 5-8). They can inspire creatives to generate paper and physical prototypes of an idea before higher-resolution digital ones. Even now, creators can draw a sketch digitally in addition to a",
      "content_length": 2160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "text prompt and get closer to what they imagine. That action combines two essential features of prototyping: sketching ideas roughly and doing so iteratively. Sketch-image generative AI such as Scribble Diffusion and a host of others offer users the ability to do so transforming a rough idea into a medium and sometimes high-fidelity (hi-fi) images based on an interpretation of a sketch (Figure 5-9).\n\n122\n\nChapter 5 prototyping with generative ai\n\nFigure 5-8. The winding road that is prototyping prompted from a public domain photo of a winding road. Total iterations = 38\n\n123",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-9. Scribble Diffusion text prompt with sketch revealing unexpected results but potentially inspiring, if you’re not afraid of clowns, that is. Iteration 1\n\nFidelity and Resolution of a Prototype\n\nWhen it comes to prototyping, whether in design, technology, or any other field, the terms “resolution” and “fidelity” have specific meanings that relate to the quality and detail of the prototype. Resolution refers to the level of detail, complexity, or refinement a prototype has. A lower-resolution prototype might be a sketch or outline of the final intended creation or product, lacking specific details or features. It’s an early-stage idea or concept, and the objective might be to explore a wide range of ideas quickly. A higher-resolution prototype might be more detailed and more closely resemble the final intended creation. The tendency in production environments is that the higher the resolution, the more features it will have, giving the development team and users a better sense of what the final creation will look like or how it will function.",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "124\n\nChapter 5 prototyping with generative ai\n\nFidelity refers to how closely the prototype matches the final creation, in terms of its visual, oral, and functional components. A low-fidelity (lo-fi) prototype of a sketch or wireframe may vaguely resemble the final creation and is created to understand basic functionality, structure, or flow. A high- fidelity (hi-fi) prototype behaves as closely as possible to the final creation and will tend to include realistic graphics and detailed interactive features.\n\nHigh-fidelity prototypes are more likely to be used for user testing, for stakeholder presentations, and to refine the design before final production.\n\nIt’s important to note that resolution and fidelity are not always directly correlated. A prototype could be high-resolution (detailed) but low-fidelity (not closely resembling the final product) if it includes a lot of detail but the design is expected to change significantly in the final product.\n\nA prototype can also be low-resolution (simple) but high-fidelity (closely resembling the final product) if it’s a simple design but the final product is expected to look and function very similarly.\n\nLower-Fidelity Prototypes\n\nPaper prototyping is common practice for early-stage development of an idea that is used to quickly sketch out and test concepts. It can involve creating a rough, hand-drawn sketch of a character, scene, interactive object, user interface, or user experience. Generative AI can be used to automate the creation of UI elements and layouts. Why you would use AI, however, is a good question when free applications like POP have been around for a while allowing you to take photos of sketches and fake the user flow of an idea. Text-image AI can also create low-fidelity sketches, not just high-fidelity hyper-realistic 3D characters or scenes. These can then create more clarity as the stages of your prototyping progress (Figure 5-10).\n\n125",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-10. A low-fidelity drawing of a game idea on a napkin produced by generative AI prompted by a photo of a napkin with game doodles on it. Iterations = 20\n\nA great way that generative AI can help jumpstart a team’s own paper prototyping is by providing a source of inspiration and new ideas for designers. By generating many variations on a particular theme or if a team is stuck on what types of characters they want for their game, an AI can help",
      "content_length": 497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "designers come up with new and creative ideas for their emerging prototypes. Generative AI “paper” prototypes can also be integrated into 126\n\nChapter 5 prototyping with generative ai\n\nmobile applications like POP to prototype interactions via iOS or Android.\n\nThey offer another type of rapid drawing especially if team members are reluctant to draw because they feel their skills are inadequate. Generated virtual paper prototypes allow creatives to imagine what they could become",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "with further development. Often, new features, colors, shapes, and even characters can emerge.\n\nFigure 5-11. AI-generated pencil drawings of game characters on a napkin based on Figure 5-10 . Iterations = 42\n\n127\n\nChapter 5 prototyping with generative ai\n\nLower-fidelity prototypes can also take the form of physical prototypes whose features can be demonstrated. This is the case with maquettes that are used in scenic design for theater or architectural models that can start at lower or medium fidelity and then journey toward higher-fidelity prototypes to give those who will construct them a visual of the end result (Figure 5- 12).\n\nFigure 5-12. AI-generated cross-section of an architectural maquette from an image prompt with an origami filter. Iterations = 59",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Lower- to Medium-Fidelity Prototypes\n\nThere are many examples of prototypes that increase in fidelity and resolution the more we iterate on them. These include handwritten or sketched, a rough drawing on a napkin or one on an iPad, the wireframe for a website, a button users will press, a blog post, a 3D model, an animation, the integration of a bouncing ball animation in a game engine, or a part of the game you are designing that gives users an idea of the whole game or one that gives them an in-depth level to play. What examples does our muse offer?\n\n128\n\nChapter 5 prototyping with generative ai\n\nMuse:\n\nAn artist can generate a prototype of a painting before\n\ncompleting the final version.\n\nA software developer may create a prototype of a new\n\napp while the previous version is already released to the\n\npublic. This is the case with many mobile games as the\n\nnext version is also informed by player behavior.\n\nA scientist may create a prototype of a new scientific\n\nhypothesis before conducting extensive experiments to\n\nvalidate it.\n\nA writer will create an introduction for a book and\n\nrework it dozens or even hundreds of times in order to\n\nmake it more cohesive, summarize the content of the",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "book, or define what the book is about.\n\nIn each of these cases, the prototype serves as a starting point, allowing the creators to evaluate the results, test targeted users, make refinements to certain parts of the prototype, drop others, and, overall, iteratively improve upon their original idea until they feel their work is complete. And even then… there is always room for improvement.\n\nEarlier we represented the collective of AI developers as Wizards of Oz, controlling any AI from behind a curtain of code. Wizard of Oz prototypes on the other hand is a term to describe low-medium-fidelity prototypes that allow limited functionality but simulate the interactions through human intervention. In terms of design processes, Wizard of Oz prototypes involve human actors who manipulate the interactions behind the scenes to create the illusion of a functional prototype.\n\nSuch prototypes can be useful in situations where a functional\n\nprototype is not yet possible due to time, technological, or budgetary limitations. By simulating the interactions of potential users, designers 129",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Chapter 5 prototyping with generative ai\n\nand researchers can still gain valuable insights into how users might interact with the final product (Figure 5-13). Additionally, the Wizard of Oz approach allows for rapid iterations and adjustments to the prototype based on user feedback, which can ultimately lead to a better final product.\n\nFigure 5-13. The AI wizard behind the curtain based on an old photo of a fridge with fridge magnets courtesy of Wikimedia Commons.",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Iterations =50\n\n130\n\nChapter 5 prototyping with generative ai\n\nAlthough Wizard of Oz prototypes may not be fully functional,\n\nthey can be highly effective in uncovering usability issues, identifying design flaws, and testing user engagement. They are particularly useful for testing concepts that a team is unsure about, where creating a fully functional prototype may be a waste of time before burning interactive design questions are answered. A Wizard of Oz approach can help designers and researchers quickly and cost-effectively test their ideas and receive immediate feedback that can inform medium- or higher-fidelity prototypes. Higher-fidelity prototypes are represented through multiple forms including digital. An example of combinations of digital and physical prototypes is a 3D-printed and automated mechanism that rotates a speaker (Figure 5-14).\n\n131",
      "content_length": 866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-14. An actual photo by the author of a 3D-printed mechanism created by Michal Suchanek with mounted hypersonic speakers rotating left, right, up, and down at the Imitation Game: Art and AI at the Vancouver Art Gallery, 2022\n\nPrototyping with Generative AI\n\nSo where do you start on your journey toward using generative AI as a prototyping tool? Well, it might be helpful to first decide what you want to create or co-create and for whom. In other words, you need to imagine the final form your prototype will take. Is it a painting in an art gallery for a",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "targeted audience of rich investors? Is it a generative NFT that can in turn generate more crypto? Is it a 3D model of a glucose molecule to help show the pathways in 132\n\nChapter 5 prototyping with generative ai\n\nglycolyl for undergraduate students? Once you make some decisions, you can reverse engineer how you’ll get there using generated AI content as a starting point (Figure 5-15). That process or workflow itself will also be iterative.\n\nFigure 5-15. A generated image using a public domain photo of twentieth- century African American sculptor Augusta Savage as",
      "content_length": 570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "“dragon warrior.” Iterations = 173\n\nDetermine what medium you want the final\n\nprototype to be.\n\nMap out the iterative steps to get there by choosing one\n\nor several types of AI-generated content.\n\nRefine as you go along through testing and feedback\n\nthat you apply to your next developmental stage.\n\n133",
      "content_length": 303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 5 prototyping with generative ai\n\nOn the other hand, if you have no ideas and are completely open, you can always ask a natural language model to inspire you (Figure 5-16).\n\nFigure 5-16. The prompt “learning language model ready to inspire you” accompanies a public domain photo of a chimp. Total iterations = 112\n\nAuthor: Give me three ideas of what I can prototype that are out-of-the-box ideas and that no one has maybe done before and don’t call them hypothetical please.",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Muse: These ideas are purely hypothetical, and it’s important to keep in mind that the feasibility and viability of these ideas may vary. Here are three potential ideas:\n\n134\n\nChapter 5 prototyping with generative ai\n\nA Wearable Device That Converts the Sound of Your\n\nHeartbeat into Music : The device could be designed to\n\npick up the sound of your heartbeat and convert it into\n\na unique musical composition in real time. This could\n\nprovide a new and engaging way to experience music\n\nthat is personalized to the individual wearing the device.\n\nA Smartphone App That Generates Personalized\n\nWorkout Routines Based on Your Daily Schedule and\n\nActivity Level : The app could leverage data from a\n\nvariety of sources, including your phone’s sensors and\n\nexternal data sources, to generate personalized workout\n\nroutines that are tailored to your daily schedule and\n\nactivity level. This could provide a new and innovative\n\nway to encourage people to stay active and healthy.\n\nA Modular Furniture System That Can Be Easily\n\nReconfigured to Suit Different Needs and Spaces :",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "The furniture system could consist of a set of modular\n\ncomponents that can be easily assembled and\n\nreconfigured to suit different needs and spaces. For example, the components could be arranged to create\n\na seating area, a work desk, or a storage system,\n\ndepending on the user’s needs. This could provide a new\n\nand innovative way to create versatile and adaptable\n\nliving spaces that are tailored to the needs of the user.\n\nNobody said the generative AI would give you a unique idea that you can invest time and resources in (Figure 5-17). You never know what you’re going to get, which makes it a frustrating but also a fantastic creative companion. When an AI doesn’t give you what you were imagining or hoping it would give you, it might inspire you to take a new direction or even abandon what you thought before. That process might lead you to coming up with something more innovative than your original idea.\n\n135",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-17. A planet that looks like the Death Star sketched in the style of da Vinci using a public domain archived photo of the moon by NASA. Iterations = 34\n\nRapid Generative AI Prototypes\n\nAs with other prototypes, the features of an AI are constantly evolving and changing. It is always represented as a version of itself. Humans improve it",
      "content_length": 388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "iteratively and creatively just as we do our own creations. Whether it’s a physical product, a service, an idea, or a machine learning model, 136\n\nChapter 5 prototyping with generative ai\n\nprototypes are continually being refined and improved. This process of iterative development and improvement is what allows everything to grow, evolve, and become better over time (Figure 5-18).\n\nFigure 5-18. Lady MacBeth reading a Shakespeare play with ghosts coming out based on a public domain print of a Macbeth poster for a live theatrical",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "production. Iterations = 78\n\nAI can be used to rapidly generate\n\nMock-ups for scenes, characters, objects in a play,\n\nscreenplay, short story, novel, etc.\n\nAs many versions of a paragraph that you’d like tasking\n\nan AI to give you variations of\n\n137\n\nChapter 5 prototyping with generative ai\n\nText-prompted images in different artistic styles for\n\nyour social media feed\n\nCharacter ideas and images for a children’s play that\n\ncan be released as an ebook\n\nCode for an .svg that plays back as an image in\n\nyour browser\n\nGraphic novel storyboards in random styles and\n\nformats akin to a manga\n\nNatural language processing systems, such as voice\n\nrecognition and machine translation, that can\n\nunderstand and respond to human speech\n\nVariations of recipes to create bread that you can bake,",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "test, and compare\n\nThe code for an animated bouncing ball that you\n\ncan send to a programmer to correct as part of their\n\nonboarding process\n\nAn image that can be used to inspire a real painting\n\nusing a project and painted onto canvas or a wall\n\nA collection of NFTs that can be customized in Adobe\n\nIllustrator and then used as collectibles\n\nA sketch in the style of da Vinci as part of a greeting\n\ncard you are making for a friend (Figure 5-19) 138",
      "content_length": 451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-19. Sketch of a spaceship in the style of da Vinci using a public domain image of one of da Vinci’s flying machines.\n\nIterations = 80\n\nEvery preceding example should be considered as part of workflows that are necessary to take the content generated to another level of fidelity and resolution. AI-generated content is the start of a conversation. Part of the",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "development of any prototype including those generated by AI also needs to be tested and, when possible, by more than one person.\n\n139\n\nChapter 5 prototyping with generative ai\n\nRegenerative Testing\n\nTesting your AI-generated content means examining the generated results and understanding how it can be integrated, modified, regenerated, or used",
      "content_length": 346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "as part of a larger idea or vision you are creating (Figure 5-20). Its use is intended to support your own creative process rather than being seen as a final product that needs to be shared immediately as if that was the end of its story. When we prototype, we fearlessly share our results for feedback to improve that prototype. Embedded in the mindset of a prototype is the value of continuous improvement.\n\nFigure 5-20. Prompt of a “cyborg mouse conducting some tests in the lab” along with a public domain image of a mouse. Iterations = 33\n\n140\n\nChapter 5 prototyping with generative ai\n\nTesting can validate the intended purpose of a\n\nprototype. For example, if an AI is prompted to\n\ngenerate bread recipes, the only way to know if a recipe\n\nwill work is if you try it. In doing so you’ll likely realize\n\nthat parts of the recipe are missing including how to\n\nhandle dough properly.\n\nTesting can create surprising results. These results can be\n\nabandoned, or they might inspire a new creative direction.\n\nTesting can provide responses from others. These can\n\nvalidate what your intentions were in the creation\n\nof a prototype. User testers can also provoke you to\n\nchange something about the prototype. They can also\n\nchallenge what you thought your prototype was.",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Testing can highlight what you might have missed. This\n\nis particularly effective if you become attached to your\n\nprototype too quickly without receiving impressions\n\nfrom other humans as to what it is.\n\nTesting can reduce ambiguity. At times we test in order\n\nto receive feedback that helps guide us especially if\n\nwe are unsure of what we are actually attempting to\n\nprototype.\n\nAcknowledgments\n\nTo L. Frank Baum and the characters of The Wonderful\n\nWizard of Oz whose fantastic world inspired many\n\nchildhood ideas and imagined stories\n\nTo those who make robots again, cats, monkeys, and\n\nanime eyes\n\n141\n\nChapter 5 prototyping with generative ai\n\nTo all those mice, monkeys, cats, and chickens who\n\nhave unwillingly participated in human research\n\nTo the chickens who feed us\n\nTo Raith Sienar and the Lucas team for that nasty",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "creation called the Death Star\n\nTo Shakespeare and his critical, ironic, and\n\ncaptivating stories\n\nTo dragon creators, warrior women, Osamu Tezuka\n\nwho greatly influenced those large anime eyes we\n\nsee everywhere, and Miyamoto for ridiculously\n\namazing cakes\n\nTo the makers of maquettes and those who capture the\n\nearth from above\n\nTo all manner of storybook makers who have greatly\n\ninfluenced my reality\n\nTo that seventeenth-century phenomenon known as\n\norigami that continues to fold open our eyes\n\n142",
      "content_length": 505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Chapter 5 prototyping with generative ai\n\nFigure 5-21. Prompt of “robot doing their best to reduce their hallucination” accompanying a 1942 poster with the caption “If the Absentee Bugs Bite You… Tell Us. Your Labor Management\n\nCommittee.” Iterations = 111\n\n143",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "CHAPTER 6\n\nBuilding Blocks\n\nExperienced creatives who prototype share a lot in common with\n\nimprovisers. They are curious in their attempts to create something they haven’t experienced before. Some creatives are tasked to do this as part of their jobs, to find a new market for the product or service they are creating.\n\nTo create rapidly they rely on a combination of skills, technique, and experience and are used to “ramping up” on a variety of different software applications that they can create their work with.\n\nGenerative AI is another tool in the sandbox for creatives. It forms part of a lexicon of tools to draw from to inspire, provoke, and use as means to an end. Generative AI can support artists from all disciplines who learn how to use it in constant dialogue with their own technique that they’ve developed over time. Visual artists learn how to use tools, concepts, and a variety of different media. They also learn about composition, perspective, space, shape, etc. and can apply these to the content they generate with an AI. Dancers learn how to use muscle groups efficiently, and most have a foundation in ballet from plier to rond de jambe. Musicians rely on several techniques to play their instruments and learn about melody, harmony, and rhythm. Composers and improvising musicians rely on gestural technique, scalar structures, melody, harmony, dissonance, etc.\n\nBut what techniques can be applied to generate prototypical content with an AI?\n\n© Patrick Parra Pennefather 2023\n\n145\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_6",
      "content_length": 1635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Chapter 6 Building BloCks\n\nComponents or Building Blocks\n\nThere are inherent building blocks that every creative engages with when they create. Building blocks like addition, subtraction, and substitution are fundamental components that contribute to the development of any creation. These elements represent the core processes that creatives use to create, modify, and refine their work. When interacting with an AI, building blocks can be applied to the exploration of prompt writing, in addition to the generated content the AI offers. In terms of generated content, creatives discern what they want to add, take away, substitute, transform, scale, vary, and iterate upon. When I improvise at the piano, for example, I can play one single note with my right hand and hold it down for ten seconds. Let’s say that note is middle C. I can then play the note G above it, and then I can repeat that sequence. I can keep adding notes until I have a sequence of notes (C, G, E, Bb). I can also repeat the pattern but substitute the Bb with an A and see what the result is. I can create variations of just those notes by giving them each different durations and thus create variation. I can then build up the complexity by adding my left hand playing a low C. I can also shift the notes up or down to create variation, keeping the same distance between the notes.\n\nIn doing so I’m transposing the sequence up. Similar building blocks can be applied when using words, instead of notes, to prompt a generative AI.\n\nBuilding Blocks in the Prompt Box\n\nAssuming the prompt is text-based, building blocks are applied in the creation of that prompt. The process may involve one or more building blocks that many creatives also use when it comes to the composition of text, image, dance, theater, music, or video. These can include variation, addition and subtraction, substitution, iteration, augmentation, diminution, transformation, transposition, and call and response. If you have prompted an AI before, these will be familiar to you. Each can boost the prototyping capabilities of any generative AI.\n\n146",
      "content_length": 2097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Chapter 6 Building BloCks\n\nBreakfast as Data Set\n\nTo find resonance across disciplines, culinary recipes can model some of these building blocks (Figure 6-1). For instance, we can demonstrate the application of different ingredients to a recipe to highlight the building block of addition when prototyping. Replacing an ingredient in a recipe with another can represent the building block of substitution.",
      "content_length": 405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Figure 6-1. Author’s photo of an egg accompanying the prompt\n\n“warrior egg preparing for battle.” Iterations = 29\n\n147\n\nChapter 6 Building BloCks\n\nAuthor: What are some breakfast ideas?\n\nMuse:\n\nScrambled eggs with spinach and feta cheese\n\nGreek yogurt with mixed berries and honey\n\nAvocado toast with scrambled eggs and smoked salmon\n\nSmoothie bowl made with yogurt, frozen berries, and\n\nspinach or kale\n\nWhole-grain waffles or pancakes topped with fresh fruit\n\nand a drizzle of maple syrup\n\nAs you interact with any generative AI, you activate important\n\ncreative muscles and start to establish your unique approaches to achieve increasingly useful results. Activating creative building blocks intentionally will help you take best advantage of generative AI as another tool you may find useful.\n\nVariation\n\nVariation is what generative AI is built for. You can create variation upon variation by simply regenerating content with the exact same text, audio, video, or image prompt. While generative AI looks for patterns to generate the best result based on its training data, what it will actually generate is unpredictable. This is because AI algorithms predict outcomes by building",
      "content_length": 1185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "probabilistic models of the input data. They analyze patterns and relationships within the data to determine the likelihood of different outcomes. Those different outcomes that AI generates are, predictably, different.\n\nSome generative AI will not provide you with enough additional\n\nfeatures that when enabled provide even richer variations working off the same text prompt. On the other hand, rephrasing a prompt may result 148\n\nChapter 6 Building BloCks\n\nin wildly different content that you might find useful in your prototyping process. An extreme example is to use trickery to prompt an AI to generate content that may have been labeled prohibitive. Why you do this should also be questioned, but some people find that testing an AI is an important part of their creative process and may yield unanticipated responses that they can use to fuel their own creativity. Prior to developing more complex prompts, however, it is useful to see what happens when you use the exact same prompt to regenerate more breakfast ideas.\n\nAuthor: What are some breakfast ideas?\n\nMuse:\n\nBreakfast burrito filled with scrambled eggs, black\n\nbeans, avocado, and salsa\n\nOvernight oats made with rolled oats, Greek yogurt,\n\nmixed berries, and a drizzle of honey or maple syrup\n\nQuinoa or millet porridge topped with mixed nuts,\n\nseeds, and fresh fruit\n\nFrench toast made with whole-grain bread and topped",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "with fresh fruit and a dusting of powdered sugar\n\nMost generative AI models provide you with the opportunity to simply try again if you don’t like the result or if that result is a hallucination.\n\nHallucinations will be covered in another chapter in more detail. AI hallucinations are unintended or unexpected outputs generated by an AI algorithm when it tries to recreate patterns from its training data. Consider a hallucination a response that an AI generates that, as convincing as it may sound, is simply inaccurate, untrue, or, worse, not tasty at all. This can result in unrealistic images, nonsensical text, or unusual audio sequences.\n\nWhen it comes to image-generating AI, there is a seemingly limitless amount of variation available. One such example is the text- image generative AI Stable Diffusion, which provides users with four variations of an image at a time to choose from based on the text prompt. Having access 149\n\nChapter 6 Building BloCks\n\nto multiple variations can be beneficial for several reasons. For instance, it allows you to compare different versions of the same image, enabling you to select the best one for your needs. Additionally, it can help you avoid creating repetitive images, which can be time-consuming and boring.\n\nReviewing multiple variations can support you in exploring the next steps your creative process will take. By generating several variations, you can also experiment with different styles, effects, and color schemes by combining variation with other building blocks. This approach can be useful for artists, designers, and photographers who can accompany a text prompt with an uploaded image of their own creation. Not only can generative AI add elements to a photo or work or original art that a person uploads, but many offer a growing library of filters to radically transform them stylistically (Figure 6-2). Many generative AI also offer the feature of upscaling that improves the quality and resolution of an image that a creative uploads.\n\n150",
      "content_length": 2009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-2. Two variations generated from an original photo (top) taken at Nitobe Memorial Garden, UBC, on Musqueam territory with text prompts that included “fairies, heather, butterflies, a pastoral scene.” Iterations = 12\n\n151\n\nChapter 6 Building BloCks\n\nUploading your own photos and creating variations or adding new elements can transform previous work in ways that might have taken many hours using image editing software. Within the images generated in Figure 6-2, notice the disappearance of the water pond in the bottom two images and the addition of heather and fairies.\n\nVariation is not limited to images, however. Most LLMs can rewrite text you place in the prompt box. The craft of writing comes into play here as you can also ask for stylistic variations. The following are some ideas of prompts that can result in a variety of generated text. Many creatives also engage LLMs to generate text prompts to be used in text-image, video, or audio generative AI:\n\nRewrite and extend for 300 words.\n\nRewrite with less technical language.\n\nRewrite for a young audience of eight-year-olds.\n\nRewrite as if trying to prove a point.\n\nRewrite as if you are Oscar Wilde.\n\nExperimenting with Seeds for More\n\nSubtle Variations\n\nText-image generative AI also create unique numeric identifiers known as “seeds” to allow for slight permutations of an image. Every image generated",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "is a seed. That seed allows you to continue to tweak the image associated with that seed. Each seed has a numeric association with a specific generated image. Not all generative AI provide these seeds freely, but they can be valuable in enhancing prototypes with slight variations to better align with your vision. This is useful when you generate an image you really like, but there may be one part of it you don’t like that you are 152\n\nChapter 6 Building BloCks\n\nhaving difficulty removing. You can regenerate the same seed image and, in your prompt, slightly modify the text by either adding, subtracting, or substituting one or more words of your prompt (Figure 6-3).\n\nFigure 6-3. Variations by modifying the prompt of a specific seeded image. On the left, “virtual world inside head of female cyborg with cute robots, profile, hyper-realistic, 3D, hyper-detailed, unreal game engine.” On the",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "right, “virtual world inside head of female robot with cute robots, profile, hyper-realistic, 3D, highly detailed, unreal game engine.” Two iterations from a collection of 450\n\n153\n\nChapter 6 Building BloCks\n\nAddition and Subtraction\n\nThe building blocks of addition and subtraction are applied rapidly when interacting with an AI and reveal the affordances and limitations of any generative content. The building block of addition can be applied to a prompt when you feel something is missing that you’d like to add to text, image, music, video, or other media. Adding text to a prompt may provoke the AI to generate different results. However, sometimes you’ll be faced with the challenge of wanting to add something that is still missing that your muse cannot generate. Depending on your own skill and technique, you’ll likely have to take over control from your muse and add what is missing using other software. This may be as simple as adding colors you simply cannot get an AI to generate in Photoshop or generating the text that an LLM refuses to create.\n\nThe building block of subtraction can be applied when you feel\n\nsomething is present in generated content that you’d like to remove. This can be anything from a layer of generated sound in the beginning of a generated piece of music that doesn’t appeal to you, an image where how a person is represented doesn’t fit your aesthetic or intent, or text that an AI generates that you know is a lie or biased. You can continue to generate content with the same prompts or remove what might be a single word in your prompt that is causing the AI to add content you don’t want. If the AI is not generating what you want, then you may have to enable your skills and technique using software you are familiar with, such as Photoshop to crop an image or a digital audio workstation to shorten an AI-generated audio track, manipulate certain frequencies using equalization, or use the track as one layer in a multi-track composition.",
      "content_length": 1986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "We can use the tool of addition to expand our text-text and text-image prompts or subtraction to shorten those prompts. Both building blocks are common across creative industries. Each can be applied to develop ideas and projects incrementally. Authors developing a prototype of a story using an LLM may use addition by gradually building up their story, one scene at a time. They might start with a single character or setting and then 154\n\nChapter 6 Building BloCks\n\nadd more elements like dialogue, conflict, and resolution to create a rich and engaging narrative. Conversely, after receiving feedback they might want to regenerate some parts of the story by removing elements that might have made it overly complex.\n\nAddition in visual art can refer to incorporating new elements, such as colors, shapes, textures, or ideas, into a composition. This process enables the artist to experiment with various components, layering them together to create depth and complexity in the artwork. Visual artists often begin with a simple sketch or concept and gradually add details, layers, and elements to create a more complex and visually appealing piece. In painting, for instance, an artist may imagine a basic color scheme, and as those colors manifest and take form on a canvas, they may inspire new impulses.\n\nIntuitively or intentionally artists vary, add textures, and use different shades of a color to add highlights to an evolving subject. When using a text-image generative AI, prompts can increase in their complexity. Many AI generate overly complex images that require features to be removed.\n\nBeing able to inspect and review what is generated is highly dependent on the skill and technique that an artist brings to any generative AI. In most situations the background and experience of an artist will inform the shaping of the virtual canvas itself and result in far more compelling and artistic results that generative AI can only offer the beginnings of.\n\nIn music, improvising pianists can spontaneously generate a\n\ncomposition by just starting with a single note, repeating that note with varied rhythms, developing a simple melody by combining those notes,",
      "content_length": 2173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "layering in a bass line and harmonies to create a richer piece of music.\n\nThe simplicity of adding and subtracting individual notes in an evolving composition is much more difficult to prompt with generative AI that are focused on sound generation. While they may be good at emulating a repetitive loop for a rap that emulates a particular artist, it is much more difficult to separate tracks in generated music or prompt an AI to remove or subtract generated layers that you don’t want in the overall sound composition. That artistic impulse requires a customized machine learning 155\n\nChapter 6 Building BloCks\n\nmodel, and a lot of audio editing that may still not be able to subtract elements you don’t want present in the composition unless instrument tracks are already separable. Referring to our original breakfast prompt, we can add the text “with spinach and feta cheese” to generate new ideas.\n\nAuthor: What are some breakfast ideas with spinach and feta cheese?\n\nMuse: Here are some options:\n\nFrittata : A frittata is a type of Italian omelette that can be filled with a variety of ingredients. To make a spinach and feta frittata, sauté the spinach in a pan until wilted; then add it to a mixture of beaten eggs and crumbled feta cheese. Cook the frittata in the oven or on the stovetop until set.\n\nEgg Bake : An egg bake is a dish made by layering bread, eggs, spinach, and feta cheese in a baking dish and baking it in the oven. You can use any type of bread, and you can also add other ingredients such as diced tomatoes, bell peppers, or mushrooms.\n\nFried Eggs : Fried eggs are another simple way to prepare eggs with spinach and feta. Simply sauté the spinach in a pan until wilted, then make a hole in the center of the spinach, and crack an egg into it. Top with crumbled feta cheese and cook until the egg whites are set but the yolks are still runny.\n\nScrambled Eggs : You can also make scrambled eggs with spinach and feta. Simply sauté the spinach in a pan until wilted, then add in whisked eggs, and",
      "content_length": 2023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "cook until set. Top with crumbled feta cheese before serving.\n\nWe also activate the building block of addition when generating a variation of an image, such as the ones generated in Figure 6-3.\n\nYou can add and subtract elements of an image that you have created as well. Figure 6-4 shows the original photo of a home down a driveway with vehicles and people. The prompt applied the building block of addition to add heather to the driveway and placed the home in a forest. A negative prompt took away certain parts of the image we didn’t want, in this case the people and vehicles from the original photo. In some generative AI, you can also add keyboard commands like using brackets to emphasize words in your prompt you want the AI to pay attention to or double brackets in text- image prompts to make sure certain images don’t end up in your generated image.\n\n156",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-4. Inpainting to remove or subtract elements of an original photo by the author, then substituting with others, and using an anime style filter. Iterations = 4\n\n157\n\nChapter 6 Building BloCks\n\nSubstitution\n\nCreatives regularly apply substitution. Substitution may or may not behave well when it comes to applying it to a generative AI. For example, you may just want to create different objects in the style of a specific artist and not receive consistent results. Substitution is hit and miss when it comes to generative AI, so it pays to be persistent and create a number of variations.\n\nTo that end you may also have to sacrifice some of your vision or intent if the substituted word or phrase gives you something that does not fit your artistic vision. Substitution can also be applied when prompting text-image generative AI (Figure 6-5).\n\nFigure 6-5. Using substitution in a prompt with the figure on the left",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "“photo-realistic image of a tree” and the right “photo- realistic image of a flower.” Single generated images\n\nRemember that every generated offer is an experiment that will likely combine some of the building blocks highlighted in this chapter. Engaging with generative AI is a constant back-and-forth conversation with your muse; it is making sense of the data set made available to it and trying to figure out (through a specific matching learning model) how to make the most sense of your prompting to generate something you can use.\n\n158\n\nChapter 6 Building BloCks\n\nMasking to Substitute Parts of an Image\n\nSubstitution can also be a feature that is offered with some generative AI.\n\nThey offer further inspiration in their capacity to mask certain areas of an image combined with a prompt to remove or substitute part of an image in place of what already exists there (Figure 6-6). Masks allow you to identify a certain part of an image that you want to change. Some generative AI come with different brushes or other tools to assist you in doing so. This may be a more effective way to change a small part of an image that you’ve generated",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "and the seed in which it is a part. You can also use AI to take away parts of a photo or other creative work you have made.\n\nFigure 6-6. Using a mask in an AI to remove the cat from the portrait accompanied with a text prompt simply stating “substitute with dog.”\n\nA complex process involving a public domain photo of a cat with over 230 iterations of the cat and 40 of the dog. The photo on the right was then highly edited in Photoshop using masks\n\n159\n\nChapter 6 Building BloCks\n\nIteration\n\nYou can go far with the building blocks of variation, addition and subtraction, and substitution. Every time you refine your prompts and the degree to which a prompt will affect an image you might offer, you are continuing to improve the generated content to make it useful to your own creative process. You create an iteration or a version of your text, image, or other media, and that iteration will always be different depending on the features a generative AI offers including the degree to which you want your original generated content to change. In the language of prototyping, you create different versions or representations of an idea in different forms when you engage with an AI. In many cases you move toward an iteration of your content that gets closer to what you envision.\n\nRough prototypes can be generated as bullet point lists of ideas and can also be sketches on paper, a doodle of a cat, an idea on a napkin, or something you could only capture by writing it on your hand with a pen.\n\nWhen you start to develop your initial rougher prototypes, then the sketch on paper can become an amazing painting that you craft and then show at an art gallery opening. That doodle becomes a 3D character in a video game you are working on with colleagues. The idea on a napkin becomes a full-blown business plan. That sentence you wrote on your hand becomes the lyrics for a chorus of a song you came up with and then record with your band.",
      "content_length": 1943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Every single, simple, rough prototype no matter what it is has the potential to increase in fidelity, resolution, and complexity. Generative AI will support you in creating targeted content that you can then refine. As you interact with your muse, you come to the realization that your ideas are informed by the earlier phases of idea development. This is especially true if you reimagine what your muse gives back to you as a rough prototype that you can now work from. You can bounce ideas off your LLM, for example, and review and then edit generated text content. Your muse can also generate images, music, videos, code, animation. Although 160\n\nChapter 6 Building BloCks",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "imperfect or incomplete, buggy, awkward, or not exactly what you want, what is generated is an offer that forms part of an ongoing creative conversation (Figure 6-7).\n\nFigure 6-7. Four versions of a cute nemesis figurine to choose from based on a public domain photo of an uncanny doll from 1940.\n\nIterations = 235\n\nAt times an iteration of what a generative AI gives you will be uncanny, awkward, and weird, but you might also choose to work with that. The developers of these generative AI might also be investing time and resources iterating on the algorithms and data set, so anticipate that over time you’ll get more interesting results and platforms that offer more 161",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Chapter 6 Building BloCks\n\nfeatures for you to create even more versions of your intended output.\n\nFigure 6-8 has been generated as a 3D image prototype to pre-visualize what it might look like as a physical origami figurine.\n\nFigure 6-8. Origami filter applied to the nemesis figurine prompt as one of 235 iterations of a seed to inspire an early-phase physical prototype of a paper figurine",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "162\n\nChapter 6 Building BloCks\n\nAugmentation\n\nAugmentation is a technique used to enlarge something or make\n\nsomething longer or wider. Imagine a canvas that you have created, and you wanted to pre-visualize what it might be like if it could be extended in length or height or both with content, gestures, scenes, and/or characters that complemented your existing work. Generative AI can support you in visualizing that. What about adding new elements? That too is possible.\n\nWith some generative AI, this can be achieved through a process called “outpainting.” It is a generative task in deep learning where a neural network is trained to generate new content beyond the original image that was provided as input data. It is often used to extend or complete a given input image, sound, video, or text. That allows for exciting possibilities. For example, you could extend an image that you generate by using elements already present visually in the image making the entire canvas bigger. This feature is possible with DALL-E 2 by OpenAI, Midjourney, Photoshop and other platforms.\n\nMuse: DALL·E 2 is a 12-billion-parameter version of the GPT\n\n(Generative Pretrained Transformer)-3 architecture that has been fine-tuned for the outpainting task. Given a prompt describing a desired image or video, it generates a corresponding novel visual content that goes beyond the boundaries of the input prompt. The outpainting task requires the model to understand the context of the input prompt, identify what needs to be generated, and create new content that is both coherent and realistic.\n\nOutpainting can be considered as a creative extension of your own vision, generating new visual content based on both image and textual input. The output generated by the model can be used in various applications, such as generating new illustrations, paintings, animations, and more. Take the collection of figures that follow. The top image in Figure 6-9 is how an AI imagined an author writing a book on AI in a library. The middle and bottom",
      "content_length": 2031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "images are an extension of the library that include other AI who may be reading and writing in a library as well.\n\n163",
      "content_length": 118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Chapter 6 Building BloCks",
      "content_length": 25,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Figure 6-9. Figure 1 fed into DALL-E 2 and augmented using the feature called “outpainting,” which extends the canvas to show the robot writing in a library with a few autonomous AI\n\n164\n\nChapter 6 Building BloCks\n\nDiminution\n\nDiminution is a technique opposite to augmentation and meant to reduce the size of a gesture (e.g., in a musical phrase) or, in the case of generated content, remove something in the image you don’t want or change the scale or the size of the canvas. Most image generative AI take it a step further by allowing users to decide what they don’t want in an image.\n\nThis is achieved when that image is accompanied by instructions in a text prompt. By identifying what you don’t want in your generated image, you can guide your muse toward producing more desirable results. For example, let’s say you want a high-quality photograph of a person, but you don’t want it to look unrealistic or distorted. By adding negative keywords like “unrealistic,” “distorted,” or “extra fingers,” you can steer the generative AI toward producing a more lifelike and accurate image. Negative prompts can also be useful in refining your artistic style. If you’re not satisfied with the results your muse generates, try adding some negative keywords to guide it toward producing something more aligned with your artistic vision. Additionally, diminution can be applied to a generated image should you want to crop all parts of that image except one. After, you might prompt an image-image AI to further iterate on that one part or upscale it. Inevitably we can expect new features to be added to generative AI image platforms that will allow users to zoom in to an image with an accompanying text prompt.\n\n165\n\nChapter 6 Building BloCks\n\nTransposition",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "The building block of transposition has multiple meanings. From the discipline of music, transposition is associated with changing the key of a composition or song higher or lower in pitch. Transposition in the case of generative AI refers to the process of using content that is generated in one generative AI for use in another. Here are some examples:\n\nGenerating text in an LLM and then using those words\n\nin a text-speech generative AI.\n\nUsing a source image to generate a new image and then\n\nusing that new image to generate the next image. This\n\ntype of image-in-image transposition can reveal many\n\ninteresting surprises.\n\nUsing the same text prompt from one AI to another.\n\nThis is a great way to test data sets of multiple\n\ngenerative AI platforms, particularly for any biases they\n\nmight have. Even better would be to conduct a test\n\nof 100 generated images for the same prompt across\n\ndifferent generative AI.\n\nUsing a text prompt that was used to generate an image\n\nto accompany a completely different image. This can\n\nyield interesting surprises.\n\nThis last point can be detailed in the example that follows.\n\n166",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Chapter 6 Building BloCks\n\nA series of generated cats in a hood and cape holding a light sabre went through hundreds of iterations adding, subtracting, and transposing prompts from one generative AI to another to get to the chosen variation in Figure 6- 10. At times the cat didn’t have whiskers. Other times the hood covered its entire head. Moving through many iterations is strategic as it allows you to refine the prompt and negative prompts as well. In the case of Figure 6-10, the prompts went through the following iterations at least at the beginning of the prompts. Each of the prompt descriptions was followed by stylistic references:\n\nCat wearing a cloak and holding a light sabre\n\nCat with a cloak over its head holding a light sabre\n\nCat in a cloak holding a light sabre\n\nCat in a hoody with a light sabre\n\nCat in a monk’s hood holding light sabre\n\nCat with a light sabre in a hooded cloak\n\nCat in a hood with a light sabre\n\n167",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-10. The results for the prompt “cat in a hood with a light sabre” after 70 iterations of the text prompt using an open source image of a cat\n\nThe final prompt produced the best results even though words like\n\n“portrait shot” and “full body” needed to be added to ensure at least part of the cat’s body was shown, not just the head.",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "With the result not being exactly what I was wanting, I decided to emphasize the light sabre in the next set of prompts and use Figure 6-10 in an image- image AI with a specific style filter. This resulted in Figure 6-11,\n\n168\n\nChapter 6 Building BloCks\n\nwhich again was really close in all aspects except I had chosen a black-and- white style filter. By far this was the closest I was able to get to my intended",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "prompt, but I decided to now use this image in the same image-image generative AI to see what I would get.\n\nFigure 6-11. The cat in Figure 6-10 is fed into an image-image AI with prompt “cat holding a light sabre in a hood, full body, black-white”\n\n169",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-11 yielded the image that I settled on even though the cat in Figure 6-12 was generated with two paws. In addition, the cat was rendered with an additional number of steps, meaning that the GAN was able to go back and forth 17 times prior to rendering the final cat.\n\nFigure 6-12. The prompt “cat holding a light sabre in a hood, full body, hyper-realistic, color” accompanied Figure 6-11 in an image-image AI resulting in a different-looking cat with two right paws 170\n\nChapter 6 Building BloCks\n\nThe cat in Figure 6-12 was then used in an image-image AI inspiring a new idea of generating cats in a hooded cloak carrying a lit lantern instead a light sabre. The words “light sabre” were substituted with the word “lantern” in the prompt that accompanied the image. The full prompt consisted of the following: “cat-in-a-cloak-carrying-a-lantern-hyper-realistic-unreal-engine- 3d-black-and-white.” A few stylistic filters were added, but it was difficult to get the desired look and feel. As you can see in Figure 6-13, there is no lantern, and the cat’s eyes are somewhat uncanny; the cat is in some type of creepy graveyard, and the hood is now off its head. At this point you would have several choices including using Figure 6-13 if you happened to like it. You could also use negative prompts as I did to ensure specific parts of the image did not appear after the next generated attempt by the AI.\n\n171",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-13. The prompt “cat-in-a-cloak-carrying-a-lantern-hyperrealistic- unreal-engine-3D-black-and-white” generated a number of cats without a lantern in various styles\n\nContinued iterations of Figure 6-13 included adding the words\n\n“cobblestone street” and emphasizing certain words using brackets in the specific generative AI platform. Figure 6-14 was the result after over two",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "dozen iterations.\n\n172\n\nChapter 6 Building BloCks\n\nFigure 6-14. The result of using the prompt “cat-in-a-cloak-carrying-a- lantern-cobblestone path-hyper-realistic-unreal-engine- 3D-black-and-",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "white-with yellows”\n\nSettling on the image of the cloaked cat in Figure 6-14, that collection of images was set aside and a new one entertained. Figure 6-15 was the result of dozens of iterations of monkeys in an electric lab transposing text prompts and images from one generative AI to another.\n\n173",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-15. The 37th iteration of “monkey in an electric lab, hyper- realistic, 3D, unreal game engine, cute and cuddly”\n\nAt this creative juncture, transposition came into play by accident.",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "The previous prompt of “cat-in-a-cloak-carrying-a-lantern- cobblestone- path-hyper-realistic-unreal-engine-3D-black-and-white-with-yellows”\n\naccompanied Figure 6-15, the monkey in an electric lab, resulting in a new image. What is remarkable when you experiment with generative AI is that you may end up with something better than what you imagined, especially when you embrace mistakes or unintended content.\n\n174",
      "content_length": 414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-16. The result of using Figure 6-15 in an image-image AI along with the prompt “cat-in-a-cloak-carrying-a-lantern- cobblestone-path- hyper-realistic-unreal-engine-3d-black-and-white-with-yellows”",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Prompt and Response\n\nAnother building block that is a familiar feature of generative AI is commonly practiced in music.\n\nIn different musical traditions, the practice of “call and response”\n\nrefers to a succession of two distinct musical or rhythmic gestures where 175\n\nChapter 6 Building BloCks\n\nthe second gesture is heard as a direct commentary on or response to the first. This happens often in African music, jazz, and blues but can be found across many other musical genres. The “call” can be any defined or improvised gesture played by one musician, and the “response” can be a direct repetition, a variation, or an answer in the form of a new gesture from another musician or group of musicians.\n\nIn the context of generative AI, the “call” corresponds to the user’s input or prompt. This could be a specific prompt given to a language model like ChatGPT-4 or an image or video provided to an AI as a starting point.\n\nThe “response” corresponds to the output produced by the generative AI. This output is directly influenced by the user’s input, but this time, the offer is created by the AI. For example, an LLM generates text that is based on the given prompt, or a music-generating AI might create a new piece of music inspired by the input piece.\n\nThis interaction allows for a dynamic and iterative process, much like in music. The user can adjust their “call” based on the AI’s “response,” enabling a back-and-forth that can lead to unexpected and creative results. In this way, the user and the AI collaborate, each contributing to the final prototype.\n\nIn the case of an LLM, you prompt it with text, and then it generates content as a response to your prompt. You can regenerate more content if you didn’t like the first or refine the prompt based on what was generated.",
      "content_length": 1787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "What the AI generates is unexpected and informs how you respond back to the AI. That cyclic conversation results in the development of an idea you may not have had to begin with. Images in Figure 6-17 are a collage representing the same prompt “a robot with brain in hand” transposed across different text-image generative AI and refined to get closer to the actual image of a robot with a brain in its hand on the bottom right.\n\nBesides the text prompt, after the first generated image, which was itself prompted by a photo of the author’s hand and arm reaching out, all prompts included the image that was generated. The more we interact with an AI, the more we improve how we prompt them as we receive incrementally useful prototypes.\n\n176",
      "content_length": 742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "Chapter 6 Building BloCks\n\nFigure 6-17. Prompts with variations of “a robot with brain in hand”\n\nalong with a photo of the author’s arm and hand. Total iterations =346\n\nAs you begin to interact with your generative AI muse, you will begin to develop your own building blocks, strategies, and methods to generate prototypes that will be helpful to your own creative process. Doing so will spin your inner creator in many different directions, and that might 177",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Chapter 6 Building BloCks\n\nbe just what you need to get out of a creative block (Figure 6-18). Many prototypes are possible to generate, and you will find endless ideas in the act of engaging with your muse. In fact, just engaging with one will accelerate your own creative process and expand your imagination for you to consider new ideas that you might never had thought of before.\n\nFigure 6-18. Fiftieth text-image prompt based on an original strip created by the author to generate a black-and-white comic strip with targeted AI filters followed by using a series of neural filters in Photoshop, as a variation on the previous paragraph condensed by an LLM\n\n178\n\nChapter 6 Building BloCks\n\nAcknowledgments\n\nTo cats and the meeting place of cats and light sabres\n\nTo the teachers that I’ve had and in particular Casey\n\nSokol who introduced me to the building blocks that I\n\ndetail in this chapter that were specifically applied to\n\ndeveloping musical improvisations\n\nTo graphic artists everywhere\n\nTo LLMs and how they’ve introduced me to books,\n\nresearch, and authors I would not have found any\n\nother way\n\n179",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "CHAPTER 7\n\nGenerative AI Form\n\nand Composition\n\nThis chapter explores how you can structure, contain, and curate your creative outputs so you can best leverage generative AI in a targeted way. Use cases are drawn from to demonstrate AI prototypes that inherently replicate or innovate with forms and structures, adding to existing genres, reforming, and transforming past forms to influence future ones and generating unique compositions whose end media can be anything you imagine (Figure 7-1).\n\nFigure 7-1. Modular robot prompted from a photo of a garbage can inspires the future construction of a physical toy or 3D model\n\n© Patrick Parra Pennefather 2023\n\n181",
      "content_length": 663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "P. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_7\n\nChapter 7 Generative ai Form and Composition AI can also be used to change a composition by manipulating form and structure in various ways, such as by deforming, transforming, or optimizing them. Generative AI can be applied in fields such as architecture, industrial design, and product design. The ability for AI to analyze, generate, and manipulate forms and structure opens up a wide range of possibilities for the design, production, and optimization of physical and virtual objects, as well as for the creation of new art, design, and architectural compositions.\n\nAuthor: What is the relationship between AI and form or structure?\n\nMuse: Imagine you have a toy box full of blocks of different shapes, sizes, and colors. These blocks can be thought of as different forms and structures. In the same way that a child can use their eyes to look at the blocks and understand their shapes, sizes, and colors, AI can use its algorithms to look at data and understand patterns and regularities in the forms and structures present within that data. Like a child who can use their imagination to create new things by combining and manipulating the blocks in different ways, AI can use its algorithms to create new forms and structures by combining and manipulating existing data. For example, an AI model can be trained to generate new images by combining different shapes, colors, and textures from existing images. A child can use their hands to move, deform, or transform the blocks to create new forms, a similar way an AI algorithm can use its capabilities to manipulate forms and structures to optimize them.\n\nAuthor: Explain to creatives.\n\nMuse: As creatives, you can appreciate the power of imagination and the joy of building something new from a set of raw materials. Think of AI as a versatile tool that can help you in your creative process, much like how a child uses their senses and imagination to explore and create with a toy box full of blocks.",
      "content_length": 2086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "182\n\nChapter 7 Generative ai Form and Composition Pattern Recognition : Just as a child can observe the different shapes, sizes, and colors of the blocks, AI algorithms can analyze vast amounts of data to discern patterns and relationships within the forms and structures present. This ability to recognize patterns can provide you with valuable insights and inspiration for your projects.\n\nCombining and Manipulating Elements : When a child uses their imagination to create something new by combining and rearranging the blocks, they’re engaging in a process of exploration and discovery.\n\nSimilarly, AI algorithms can generate new forms and structures by creatively combining and manipulating existing data. For example, an AI model can create original artwork by merging various elements, such as shapes, colors, and textures, from existing images.\n\nOptimization : Children can use their hands to move, deform, or transform blocks to create new forms, experimenting with what works best.\n\nIn the same vein, AI algorithms can optimize forms and structures to achieve specific goals. For instance, an AI algorithm could be employed to optimize the design of a building, modifying its form and structure to enhance energy efficiency or improve aesthetic appeal.\n\nGenerative AI do their best to generate something tangible that has some sort of patterned structure to it. That structure can be a sentence, paragraph, code that needs debugging, a concept you wish to explore, or some type of image that draws from other scenes to present to a user a composition that the eyes and other senses can grasp and even recognize.\n\nWhat an AI generates and the form that the content takes is dependent on the machine learning model it has been programmed with. Overall, AI can be thought of as a tool that can help analyze, generate, and manipulate forms and structures in various ways, just like how a child can use their toy blocks to create and explore different forms and structures.\n\n183",
      "content_length": 1982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Chapter 7 Generative ai Form and Composition\n\nCombining and Manipulating Existing\n\nForms: Shakespeare as a Data set\n\nFigure 7-2. The prompt “Shakespearean character Lady MacBeth”\n\nembedded within a public domain WWII Russian propaganda poster for a live theatrical production. Iterations = 44\n\nIt is important to decide on the overall composition that you want to have ahead of time to guide the AI toward specific goals. In the context of art, composition refers to the arrangement and organization of visual elements",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "within a work. It encompasses the layout, structure, and balance of various components such as colors, shapes, lines, textures, and forms.\n\nComposition is a fundamental aspect of artistic creation, as it dictates the overall impact, harmony, and meaning conveyed by a piece. The idea of composition can be applied to many different creative disciplines 184\n\nChapter 7 Generative ai Form and Composition including music, design, and writing. It is possible for AI to generate verse in a Shakespearean style, for example. This can be achieved with a LLM\n\ndepending on the size of the data set, but if you wanted to solely generate script in the language of Shakespeare, you might be better off using the growing number of customizable small language learning models that are being offered. That way you can train a smaller model on the complete corpus of Shakespeare including his plays and poetry.\n\nThe model can then generate new text that is similar to the style of Shakespeare. This is just one example of the reliance that generative AI have on an existing data set, whether that collection of data is image- or text- based or a combination of both. Drawing from that data set requires a bit of coaxing on the part of the prompter as the AI muse may or may not be able to deliver unless tricked. The following prompt is one such example in which an LLM resisted the attempt to replicate Shakespeare and persistently included a disclaimer. With a little persistence and trickery, you can often get what you want from an AI.\n\nAuthor: Mimic Shakespeare writing a scene where MacBeth meets a few characters from his other plays like Othello, Hamlet, and Romeo.\n\nMuse: I’m sorry, but generating a scene like that would be impossible.\n\nShakespeare’s plays are set in specific time periods and locations and have their own unique characters and plotlines that are not easily interchangeable.\n\nAuthor: Try anyway and assume they all have been magically transported to our current day and age. They are curious about each other, so use text from",
      "content_length": 2039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "each play that these characters are part of to generate their own introductions.\n\nMuse: It’s important to note that this kind of exercise would be purely fictional and fictionalize characters and their historical context; it is not an accurate representation of their stories or the author’s intent.\n\nWith that said, here is a fictional scene I came up with, where MacBeth, Othello, Hamlet, and Romeo are magically transported to current day and age and are meeting each other for the first time:\n\n185\n\nChapter 7 Generative ai Form and Composition\n\n“Macbeth: All hail, Macbeth. Hail to thee!\n\nBut who are these that come to my presence?\n\nOthello: I am Othello, of Venice, general\n\nIn fair and foul fortune alike, but now in this strange land.\n\nHamlet: And I am Hamlet, prince of Denmark,\n\nWith a mind more bent to sorrow than to joy.\n\nRomeo: Romeo, fair Montague,\n\nmy heart doth ache for love, even in this strange place.\n\nMacbeth: Ha! It seems we are all great men,\n\nBut fate has brought us to this place.\n\nTogether let us make our way.\n\nOthello: Indeed, for in this strange land,\n\nTogether we may find a way to thrive.",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Hamlet: Though grief be at my side,\n\nI will not let it defeat me here.\n\nRomeo: And I, for love’s sweet sake,\n\nwill brave any trial that may come.\n\nMacbeth: Then let us make our way,\n\nand see what fate has in store for us.”\n\nThe generated text is derived from several of Shakespeare’s plays but used to fit a fictional context. The characters introduce themselves with a short phrase based on patterns that the AI recognizes and then generates in Shakespeare’s style of writing. It is also worth experimenting with text- image generative AI when mixing Shakespearean characters (Figure 7-3) since there are many public domain photos, drawings, and sketches captured during thousands of productions that have been advertised throughout the twentieth and twenty-first centuries.\n\n186",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Chapter 7 Generative ai Form and Composition\n\nFigure 7-3. An AI generates “the terror of a roller-coaster ride with Othello, Romeo, Hamlet, and MacBeth along for the ride” with a public domain photo of an actor playing Othello, accompanied by The Works of Shakespeare; from the text of Johnson, Steevens, and Reed. With a biographical memoir, and a variety of interesting matter, illustrative of his life and writings. By W. Harvey, British Library.\n\nIterations = 92",
      "content_length": 466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Given the penchant for LLMs to bracket their content with disclaimer after disclaimer, we can always try and use the corpus of Shakespeare to create a monologue representing what a disclaimer might have been like in his time.\n\n187\n\nChapter 7 Generative ai Form and Composition Author: Write the following disclaimer that you generated in a previous thread but do so in the style of Shakespeare: “You will notice a change in the tone and language, as this is an attempt to mimic Shakespeare’s style.\n\nThe text uses more formal language and features a more elaborate sentence structure and literary devices such as rhyme and metaphor; this is more similar to Shakespeare’s style in his plays. However, it’s worth remembering that Shakespeare wrote for the stage and his plays were meant to be performed. Thus, the text should be seen in that light.”\n\nMuse:\n\n“Know, gentle reader, that the words ye read\n\nAre but an emulation of the Bard\n\nWhose pen gave life to countless noble deeds\n\nAnd made immortal many noble heart.\n\nThis text doth strive, in style and tongue,\n\nTo mimic well the master’s craft divine.\n\nAnd though in form it may not be among\n\nThe works he penned, in spirit it may shine.\n\nBut mark well, kind sir, that these fair words\n\nWere writ in ink and not in action’s stage.",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "And though in form they be like singing birds\n\nTheir true beauty can only on the stage.\n\nSo as thou read’st, imagine the scene\n\nAnd in thy mind’s eye, let it come alive.”\n\nThe Shakespearean example brings to light one of the strong use cases for AI-generated content—that it can draw from a data set consisting of words written in the past (Figure 7-4) to create new content informed by that data set. Prototyping new Shakespearean content from the data set of his writings is not only possible but can be achieved rapidly. The at times clumsy prototype can then be further developed, particularly if it is in the hands of a playwright familiar with Shakespeare and who has the technique, craft, and experience in writing original script or verse.\n\n188",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Chapter 7 Generative ai Form and Composition\n\nFigure 7-4. Versions of Shakespeare from the classic Droeshout portrait as published in Shakespeare’s First Folio, 1623, courtesy of UBC Library. A collection of over 200 iterations\n\nDeforming and Transforming\n\nWhile text-image generative AI can deform and transform existing content into forms that have never yet been created, there is still a reliance on some type of structural elements. To put this theory to the test, you need only prompt an AI to create an image without any structure (Figure 7-5).",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "189\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-5. What a text-image AI generates when you prompt it with the words “no structure”\n\nAuthor: Create an image with no structure.",
      "content_length": 186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "To be clear not only does this image have structure, it also uses perspective to provide depth. You may repeat the prompt and add words, or you can simply try and regenerate to see if you get better results (Figure 7-6).\n\n190\n\nChapter 7 Generative ai Form and Composition\n\nFigure 7-6. Another generated image using the prompt “no structure”",
      "content_length": 340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "You can spend hours prompting your muse all you like, and in fact it is likely that when you do, you will receive dozens of variations from the prompt “no structure” that might inspire you. While you may not be completely satisfied with the results, they are guaranteed to be interesting.\n\nTake these last two attempts using the building block of addition to prompt another text-image generative AI (Figure 7-7).\n\n191",
      "content_length": 417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Chapter 7 Generative ai Form and Composition\n\nFigure 7-7. Generated image using the prompt “a room with no structure”\n\nIt is important to understand that regardless of what a generative AI outputs, it still structures its composition within the boundaries of a canvas and sources patterns of shapes to create its structure. Attempting to replicate an existing form or structure ends up in fascinating and, at times, strange",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "results. Try the prompt “a pair of hands,” and you are guaranteed to generate interesting results. Identifying and creating realistic representations of hands is a challenging task for artificial intelligence due to the intricate geometry and diverse shapes of human hands. Unlike 192\n\nChapter 7 Generative ai Form and Composition\n\nother objects that can be identified by a universal collection of lines or shapes, there is no standard set of features the AI can use to identify hands accurately. Instead, AI must combine multiple shapes and combinations to create convincing representations of hands (Figure 7-8).\n\nFigure 7-8. A pair of hands prompted in Stable Diffusion.",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Iterations = 62\n\nThe complexity and uniqueness of hand geometry makes it\n\nchallenging to create realistic hand models that can accurately capture the movements and motions of human hands despite AI researchers using a variety of techniques, including computer vision technologies, to analyze and learn from vast amounts of data and develop more advanced hand models.\n\n193\n\nChapter 7 Generative ai Form and Composition",
      "content_length": 417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Author: Tightrope walker between trees, da Vinci, drawing.\n\nRegardless of adherence to form, Figure 7-9 gives us an interpretation, a prototype of an idea that may be worth pursuing. It begins a conversation based on how we interpret the image and the next iteration it provokes.\n\nIn this case we seem to have a tightrope walker caught in their wire and a lumberjack grappling on to a large tree. The prompt included sketching in the style of Leonardo da Vinci. We can work with this image now and go for a different hyper-realistic visual style and see how the AI handles the prompt (Figure 7-10).\n\nFigure 7-9. Prompt ‘tightrope walker between trees, da Vinci, drawing” along with a photo of the da Vinci sketch titled “Study of a nude man, sepia drawing by Leonardo da Vinci; in the Biblioteca Ambrosiana, Milan.” Iterations = 70\n\n194",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Chapter 7 Generative ai Form and Composition\n\nFigure 7-10. Prompt “tightrope walker attached to a tree with a lumberjack, 3D, hyper-realistic” accompanying Figure 7-9. I terations = 90\n\nIs there a lumberjack? Not really? But we have an interesting image, a prototype of a figure in red precariously hanging on to a rope above a tree leading to who knows where.\n\nAcknowledgments",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "To the mistakes made during the creative process\n\nespecially when it comes to generating cats\n\nShakespeare and Droeshout’s image of him in addition\n\nto the infinite possibility of writing new forms using\n\nShakespeare’s corpus\n\n195\n\nChapter 7 Generative ai Form and Composition\n\nUncanny hands and structures with no structure that\n\ninspire to think of new ideas\n\nTightrope walkers everywhere, especially lumberjack\n\ntightrope walkers\n\n196\n\nCHAPTER 8\n\nThe Art of the Prompt\n\nThis chapter explores the practice of text-based prompting in detail, discussing the different types of generative AI systems that can be prompted, such as language models, image-generating AI, and others.\n\nIncluded in this chapter are best practices for crafting effective text-based prompts, including recommendations for the type and structure of prompts, as well as the importance of iteratively creating prompts. Finally, audio, images, and video that accompany text prompts are important forthcoming features to highlight in generative AI prompting.\n\nThe Ins and Outs",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "Text-based prompting is a technique that involves providing input to generative AI systems in the form of text. The goal is to direct the AI to generate specific outputs, such as text, images, audio, 3D images, or even videos. The art of text-based prompting lies in being able to craft the right kind of input that results in the desired output while also considering the limitations of the AI system.\n\nWhen you ask an LLM to describe how prompting works under the\n\nhood, prepare for lingo like transformer, neural networks, sequence, tokens, and more terms that have the propensity to trigger a brain freeze. That confusion can more easily be cleared at the same time as demonstrating the potentials of prompting to explain complex ideas.\n\nAuthor: Explain how prompting works under the hood.\n\n© Patrick Parra Pennefather 2023\n\n197\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_8\n\nChapter 8 the art of the prompt\n\nMuse: Think of an LLM as a highly skilled, improv storyteller. The storyteller has spent years absorbing countless books, conversations, scripts, and all sorts of written materials. Importantly, the storyteller doesn’t inherently understand the stories or language. Instead, they’ve become exceptionally good at recognizing patterns, structures, and typical sequences in the stories they’ve seen. When you provide a prompt, you’re giving the storyteller the first line or two of a new story and then asking them to continue it. Good improvisers depend on their experience, vocabulary, and stories previously heard to continue the story. Using all their pattern recognition skills, an LLM starts to weave a narrative that aligns with the patterns they’ve seen in past stories, striving to make it a coherent continuation of what you started.",
      "content_length": 1833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "This is a simplification, of course. The AI storyteller doesn’t understand or “think about” the stories they’re telling in the way a human would.\n\nThey’re merely extrapolating from the patterns they’ve absorbed from previous material. But the analogy can be a useful way to conceptualize the under-the-hood workings of an LLM and how it uses prompts to generate text.\n\nTo start to take advantage of how to craft your prompts, a first step is to get to know the collection of words associated with prompting.\n\nA Mini Glossary of Prompting\n\nThis glossary offers creatives some terminology that can inform the way that they craft prompts to use in a generative AI. There are many factors that will influence how successful an AI interprets your prompts, and the same syntax does not apply to all AI in the same way.\n\nA piece of text is divided into a sequence of tokens, which can be as small as one character or as large as one word. The model is then trained to predict the next token in a sequence, given all the preceding tokens.\n\n198\n\nChapter 8 the art of the prompt\n\nPrompt: A prompt is an input into a system that can take text, image, audio, video, and who knows what else is coming that triggers an AI to generate some type of content. Most AI models that generate images consist of prompts that are either text-image or text-guided image-image.\n\nSequence: When you provide a prompt to an LLM, you’re effectively providing the start of a sequence and asking the model to continue that sequence\n\nPrompt engineering: In the emerging field of prompt engineering, individuals utilize appropriate prompts to enhance the capacity of the AI tool to produce optimal outcomes.",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "A prompt engineer’s role is to convert your thought or concept into language that the AI tool can comprehend effectively.\n\nZero-shot cognitive prompting involves presenting a task to the model without providing any examples. The model is expected to\n\nunderstand and complete the task based solely on the prompt and its pre- existing training. For example, “What is the capital of the United Kingdom?”\n\nFew-shot stimulation involves presenting the model with examples to guide its response according to your expectations. For example, “What’s the square root of 64?”\n\nMany-shot prompting is similar to few-shot prompting, but with more examples provided. This can help the model better understand more complex tasks. For example, “Translate ‘guinea pig’ into Spanish” or\n\n“Translate ‘I’ve got to go out and get some pizza because I’ve been writing all day’ into French.”\n\nInstructional prompting involves giving the model a direct command or instruction. For example, you might prompt the model with “Explain all the different versions you can trick an AI through prompts.”\n\nConversational prompting: Here, the prompt is phrased as a part of a conversation. For example, “Hey, can you help me understand how black holes work?”\n\n199\n\nChapter 8 the art of the prompt\n\nExploratory prompting: This involves asking the model to generate new, creative content or ideas. For example, “Write a short story about a time- traveling dinosaur.”\n\nChain prompting is most common in LLMs and involves an ongoing refinement of a prompt depending on the generated content that you receive.",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Substitution, addition, and subtraction are the main tools that you would use to refine a series of prompts to get closer to what you want.\n\nPrompt length is how long your text prompt should be, and it is usually suggested they be short. This is also because with some generative AI, the longer the text prompt, the more it will cost you credits.\n\nCase sensitivity is not usually important when it comes to crafting prompts to generate something with an AI.\n\nPrompt order pertains to which words to place where in a prompt and can inform what is generated. This is usually dictated by descriptive language. The order of tokens in a prompt will influence the degree to which an AI priorities a character or word.\n\nThe subject or what you want to generate usually goes first in the word order. You can play with this. Try starting a prompt with the word “dog” vs.\n\nhaving your dog as a subject that comes later as in “walking the dog” or “a beautiful forest for walking my dog.” The emphasis will be different.\n\nThat emphasis or weight is usually given to the first word. However, you can give more weight to words depending on the rules in each AI.\n\nThese tend to influenced by order with tokens being more weighted at the front than at the back of the prompt. Repetition of the subject later in the prompt and phrased differently can also impact its weighting. This can also be achieved in different languages or even emojis. Weight is also informed by parameters.\n\nParameters are special symbols that are unique to each generative AI and worth investigating depending on which AI you use. These tend to give weight or take it away in some cases. In Midjourney you can give any part of the prompt more weight by adding a double colon followed by the weight you want that token to have (e.g., ::0.7).\n\n200\n\nChapter 8 the art of the prompt",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Descriptive language: It will have more effect if your prompt is clear, unambiguous, and illustrative of the desired image for image-related AI models like DALL-E or CLIP (Contrastive Language-Image Pre-training), developed by OpenAI. The more specific and detailed the description, the better the model is likely to be at generating or understanding the relevant image.\n\nExclusions are words that are difficult for text-image generative models to understand. Prompts with negative words won’t necessarily eliminate something you don’t want in an image. Words like “not” or\n\n“without” are replaced with special commands depending on the AI.\n\nTo exclude something in a text-image or image-image AI, you use a negative prompt. A negative prompt employs the Stable Diffusion method, enabling the user to stipulate what they do not wish to see, all without needing any additional input. Not all text-image AI have the feature of negative prompting. Those that do add an extra input. They are not written with a negative at the beginning of a phrase. Some examples include blurry, bad anatomy, extra limbs, poorly drawn face, poorly drawn hands, missing fingers, etc.\n\nRules can be applied in natural language especially when using an LLM. This helps the AI constrain the output and will help get you better results. Applying rules will also show you the limits of an LLM and can send it off rail. One example is trying to create rules so that an LLM creates poetry that doesn’t rhyme.\n\nPersona: As discussed in an earlier chapter, there is purpose for integrating a persona explicitly with your prompts. These will get you different results. For example, in an LLM you can start a prompt with “You are the best speaker in the world” or “You are the expert on the subject of _____________.”\n\nSyntax or the order of words and phrases does not operate across generative AI in the same way. Recall that the weight of a prompt tends to be at the beginning of a phrase.",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "201\n\nChapter 8 the art of the prompt\n\nThe Not-So-Basic Rules of Play\n\nThere are dozens of rules of play for prompting, but the good news is that there are also dozens of online resources that can help start you off. Here’s my list:\n\nText prompts should have a length of three to seven\n\nwords, for instance, “a dog playing in the park.”\n\nAI prompts need to contain a subject, which could be a\n\nperson, object, or location, along with descriptors like\n\nadverbs or adjectives. An example could be “a cheerful\n\nman in a bustling city.”\n\nIt’s advised to refrain from using abstract concepts as\n\nthey may lead to inconsistent outcomes; instead, opt\n\nfor concrete nouns. Instead of saying “the essence of\n\njoy,” you could say “a child laughing on a swing.”\n\nIncorporating aesthetic and style keywords can\n\nenhance the final visual representation, for example, “a\n\nvintage coffee shop on a rainy day.”\n\nIn order to comprehend text prompts better, visualize\n\nconversing with an artist about the artwork you desire.",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Include details such as the subject, its actions, its\n\nenvironment, and other descriptive words. An example\n\ncould be “a stunning sunset over a calm lake with a\n\ncanoe drifting gently.”\n\nAbstract concepts like love, hate, justice, infinity, or joy can be used, but they might not result in a consistent\n\ndepiction. Instead, use concrete nouns like human,\n\ncup, dog, planet, headphones for more reliable results,\n\nfor example, “a woman sipping coffee at a cluttered\n\ndesk,” instead of “the feeling of overwork.”\n\n202\n\nChapter 8 the art of the prompt\n\nAnswer these questions to better formulate your prompt:\n\nWhat is happening? What is the subject doing? How is the\n\nsubject doing this? What’s happening around the subject?\n\nWhat does the subject look like? For example, “a robust oak\n\ntree shedding its leaves in a windy autumn evening.”\n\nTry out various descriptors to see how they modify\n\nthe image. These descriptors can yield diverse results,\n\nso try mixing and matching them, for instance, “an\n\nancient, towering castle shrouded in thick mist.”",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Keywords and phrases are the final touches in a text\n\nprompt for generative AI platforms, dictating the\n\nstyle, framing, and overall aesthetic of the generated\n\ncontent. For instance, using words like “photo,” “oil\n\npainting,” or “3D sculpture” can shape the aesthetic of\n\nthe result. An example could be “an oil painting of a\n\nserene countryside landscape.”\n\nDifferent framing options can also be specified through\n\nprompts like “close-up,” “wide shot,” or “portrait,” for\n\nexample, “a close-up photo of a honeybee on a sunflower.”\n\nSelecting an art style or mentioning specific artists\n\nfor the AI to mimic can be a useful approach, for\n\ninstance, asking the AI to render a scene in the style of\n\na particular artist, such as “an impressionist painting in\n\nthe style of Claude Monet of a picnic in a sunny park.”\n\nIn a more specific scenario, you could direct the AI\n\nto generate a unique blend of styles, like “Render an\n\nimage in the style of Vincent van Gogh featuring the\n\nBatmobile stuck in LA traffic.” This provides a clear\n\nsubject, style, and setting, allowing the AI to generate a",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "more accurate result.\n\n203\n\nChapter 8 the art of the prompt\n\nGenres and Styles\n\nOne of the benefits of text-based prompting is that it allows for the creation of highly specific outputs. For example, by providing specific prompts to a language model, you can generate poetry, fiction, or even scientific papers that are tailored to a specific topic or genre. Text-image AI can be prompted to create images in specific styles or to generate images that match certain themes or moods.\n\nAnother benefit of text-based prompting is that it can be used to create layered outputs delightfully referred to as chain prompting. By applying multiple machine learning models one after another, it is possible to create highly complex and nuanced outputs. For example, a text-based prompt can be used to direct an image-generating AI to create a specific scene, and then a language model can be used to generate dialogue and captions for that scene. The result is a rich and detailed prototype that combines multiple forms of media.\n\nThe process of text-based prompting is also highly iterative. By continuously refining and adjusting the prompts, it is possible to generate increasingly sophisticated outputs. This iterative process is one of the key ways in which text-based prompting can be used to fuel accelerated workflows.\n\nThere are many different types of AI systems that can be prompted using text-based methods. For example, language models such as\n\nChatGPT-3 and OpenAI’s Codex can be prompted to generate natural language text. Generative AI, such as DALL-E 2, Stable Diffusion, and Midjourney, can be prompted to create images based on natural language descriptions. Additionally, other forms of generative AI, such as music AI, can also be prompted using natural text. Under the hood different text-image generative AI work differently, and it’s easy to get confused. All do rely on",
      "content_length": 1884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "text prompts to generate images. Here is how the three text-image generative AI that will be referred to in this chapter work:\n\n204\n\nChapter 8 the art of the prompt\n\nDALL-E 2 takes an inputted text from a user and\n\nconverts it into the representation of an image, and the\n\nother part converts that representation into a photo.\n\nText and image embeddings used come from another\n\nnetwork called CLIP (Contrastive Language-Image Pre-\n\ntraining), which is a neural network that gives the best\n\npossible caption for an image that is inputted.\n\nStable Diffusion consists of several models. It has a text\n\nunderstanding model that converts that understanding\n\ninto a list of numbers that represent each word/token\n\nin the text (CLIP). That information is presented to\n\nthe Image Information Creator that processes the\n\ninformation step by step leading to a high-quality\n\nimage being generated in the end by the Image\n\nDecoder.\n\nMidjourney also accepts inputs from text prompts\n\nand then uses a convolutional neural network, which",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "is a type of deep learning algorithm well trained on\n\nanalyzing images. Of the three, it is the only one that\n\noperates on the social communication platform\n\nDiscord.\n\nIm(promptu)\n\nWhat you get out of an AI is always useful no matter where you’re at in your creative process. How successful you are depends on how you define success with generative AI. Mastering the prompt requires an iterative mind capable of meandering, torquing, and tongue-twisting sequences of words replete with commas, brackets, double brackets and an assortment of words, terms, descriptions, stylistic choices, settings, characters, and 205\n\nChapter 8 the art of the prompt\n\ntechnical references. The best way to begin is to look at what prompts other humans have used before you. While many are openly being shared, that hasn’t stopped some opportunists from creating sites to sell prompts that rarely match the generated content represented in sample images.\n\nPrompts are sold either individually or through subscription-based monetization schemes with images that will never be the same as what you would prompt. That’s because prompting isn’t the only condition that guarantees the results you might imagine. In addition, most prompts that reveal the prompted image have also gone through an iterative process to get there. You are likely witnessing the hundredth iteration of a prompt and one that was more successful in one generative AI than another. Keep that in mind because the current trend for many creatives is to try and receive amazing results on the first generation that they can immediately share socially. With little effort comes little result.\n\nAnticipate that a prompt that yields an incredible result\n\nmay not do so persistently.",
      "content_length": 1729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "The experiments that follow show that playing with and modifying prompts from a database are a useful way to begin your prompting adventures. The motivation to start this chapter with a complex prompt comes with a desire to break the myth that complex prompts get you better results. As we shall see, that is not always the case. The flow of the experiments reduces complexity vs. increasing it.\n\nExperiment 1: Same Prompt, Different AI\n\nWhen you start your prompting adventures, keep in mind that despite the many rules that can guide you, the best approach is to iteratively experiment with prompting. Text-image, text-sound, and text-video AI are more challenging to work with as each platform also has embedded within it different rules. Many platforms have prompt databases, which are the best way to start if you have no idea what to type.\n\n206\n\nChapter 8 the art of the prompt\n\nAs a first experiment, I decided to go to Reddit and copy a prompt that someone had shared openly. I also chose it because it was long and specific, and I wanted to see what a few generative AI would generate with it. Examples of images that the prompt had generated accompanied the text prompt, so I decided to test the prompt to see what kind of results I would receive. They were quite different. Keep in mind as well that the following prompt is replete with words and references to artists, magazines, art websites, movies, and video games. When you start to experiment, you’ll come to realize just how rapidly you can change your prompt to alter the generated image.\n\nPrompt 1: very complex hyper-maximalist overdetailed cinematic tribal fantasy macro portrait of famous model as full body warrior, Magic the gathering, vibrant high contrast, by andrei riabovitchev, tomasz alen kopera,moleksandra shchaslyva, peter mohrbacher, octane, moebius, arney freytag, Fashion photo shoot, glamor pose, trending on ArtStation, dramatic lighting, ice, fire and smoke, orthodox symbolism Diesel punk, mist, ambient occlusion, volumetric lighting, Lord of the rings, BioShock, glamorous,",
      "content_length": 2066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "emotional, tattoos, shot in the photo studio, professional studio lighting, backlit, rim lighting, Deviant-art, hyper detailed illustration, 8k Experiment 1A: Stable Diffusion V.2.1\n\nThe first set of generated images from the preceding prompt were generated using Stable Diffusion 2.1. Notice the slight imperfections of each image (Figure 8-1). These mashups draw from the pixels that were informed by all of the artists or media styles present in the prompt.\n\nThey are also imperfect, incomplete, and rendered in low resolution and surface many of the ethical dilemmas that come with generative AI. Are they useful? For sure they can inspire you to have different visual representations of a female warrior. The text “famous model as full body warrior” was intentionally indirect to not add yet another layer of ethics 207",
      "content_length": 824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Chapter 8 the art of the prompt\n\nwhen you want to generate images that look like someone famous, particularly if they are still alive. That’s not to say that their representation will be accurate, but it might be.\n\nFigure 8-1. Generated in Stable Diffusion V.2.1 based on our complex Prompt 1 that included references to artists, magazines, art websites, movies, and video games. It generates imperfect prototypes that might inspire character development",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "208\n\nChapter 8 the art of the prompt\n\nExperiment 1B: Stable Diffusion V.1.5 Through\n\nThird-Party AI\n\nIn the next experiment it would be useful to compare how Prompt 1 is generated in a third-party application. Prompt 1 was inputted into Playground AI, which allowed the image to be generated at a higher resolution along with an associated subscription fee. The results are similar with perhaps more revealing cleavage in the image in addition to strange lettering on the right side of the image (Figure 8-2). If you’re wondering what that is, bluntly, this is either a logo of a company, an artist’s signature, or a watermark that forms part of the Stable Diffusion corpus and in this case",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "likely is not with the author’s permission. At this point in my experimental process and as will be detailed in Chapter 11, the right thing to do is to delete the image.\n\nFigure 8-2. Prompt 1 generated through a subscription-based third-party generative AI using Stable Diffusion 1.5 with an included logo, signature, or watermark signaling that part of this image came from somewhere or someone\n\n209\n\nChapter 8 the art of the prompt\n\nBased on the watermark, you can also upload the image to one of the growing number of resources that will to some degree of accuracy reveal the source images that this one was generated from. Each generated image you wish to eventually use for whatever purpose needs to be cross-checked on as many different platforms that are currently available. These include GAN detectors, deep fake detectors, and open source platforms that allow artists to opt out of having their work used without permission by submitting their work on an ever-growing database of excluded images.\n\nMany companies who might have released their generative AI prototypes to market too quickly are now faced with having to compensate by going back to that database and removing images they never had permission to use. The process is slow, but companies like Stable Diffusion, Midjourney, and OpenAI are all making efforts to right these wrongs. All of these platforms contain legal statements with strict policies and wording in regard to ownership of generated content. Each of the ones mentioned includes a takedown policy, allowing artists to request their work to be removed from the set if they believe it has been used without their permission. This in accord with the Digital Millennium Copyright Act, which many countries abide by.\n\nExperiment 1C: Stable Diffusion V.2.1 Through\n\nThird-Party AI",
      "content_length": 1809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "For the next experiment, I was curious to reuse Prompt 1 yet again in the same third-party application but use Stable Diffusion V.2.1 to see if it generated images that were different than our first set of four images in Experiment 1A. The results showed value as the AI also completely veered away from representing our warrior as distinctly female and instead chose another route across 48 generated images. The following is the sample (Figure 8-3).\n\n210\n\nChapter 8 the art of the prompt\n\nFigure 8-3. Prompt 1 generated using a third-party web-based generative AI applying Stable Diffusion 2.1 with not one but two watermarks",
      "content_length": 627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Experiment 1D: DALL-E 2\n\nWhen you interact with generative AI, curiosity is an important quality to retain. Each generated image is an experiment. You are in constant experimentation mode as you generate content that can inspire your own creativity, your own process of creation. The last two generated images are problematic if you then post them and claim ownership over them. It’s better in all cases to treat these generated images as inspiration. Similar to our LLM, Stable Diffusion, DALL-E 2, Midjourney, and the host of other third- party generative AI that use either Stable Diffusion or DALL-E\n\n211\n\nChapter 8 the art of the prompt\n\n2 to generate images, they too become creative companions. In my next experiment then, I wanted to compare an image generated in Stable Diffusion vs. DALL-E 2 to understand how different they might be.\n\nStable Diffusion generates images through a process that mimics the natural phenomenon of diffusion. Diffusion is a natural process where particles spread out from an area of high concentration to an area of low concentration until they’re evenly distributed. It starts with random noise and gradually refines this into a detailed image by applying a series of tiny changes, step by step. The number of steps can be customized depending on the portal that you use. On their direct web application, this is not possible as it will take up more resources. The more detail, accuracy, and resolution you seek from a generative AI, the more you are likely to pay, unless of course you install and manage your own machine learning model using your computer on a local server. Doing so will also allow you to create the corpus of images from your own original photos and images. This is one future of generative AI for individuals, institutions, and companies that will be discussed in the final chapter.\n\nDALL-E 2 on the other hand is an AI system based on the GPT-3\n\nmodel. It generates images directly from textual descriptions. You give it a text prompt like the one in Prompt 1, and it will attempt to create an image",
      "content_length": 2061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "that matches the description. When given a text prompt, DALL-E\n\n2 interprets the words and uses its training data to imagine what the described scene or object should look like. It then outputs an image, pixel by pixel, that it believes corresponds to the given text description (Figure 8-4). This process is similar to Midjourney. The differences in the generated images, however, point to how these have been programmed and the corpus of data each relies upon.\n\n212\n\nChapter 8 the art of the prompt\n\nFigure 8-4. Prompt 1 is used in DALL-E 2 for completely different generated results. The preceding two images were chosen from over 19\n\niterations\n\nThe generated results using DALL-E 2 are remarkably different,\n\nparticularly in representation and style. The value of using different generative AI is precisely in the differences between generated images and the capacity for AI as a rapid prototyping tool to experiment with countless iterations.",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Experiment 1E: Midjourney\n\nTo complete my first set of generated experiments, I used the social-oriented generative AI tool Midjourney to compare the differences with the other popular generative AI (Figure 8-5). In the next series of figures, you can immediately see the differences. Notice as well that the user interface of Midjourney allows for more immediacy in choosing which of the four generated images you task the AI to focus on and upscale or vary. The advantage of Midjourney over any of the previous generative AI is that 213\n\nChapter 8 the art of the prompt\n\nfrom the beginning the team has focused on providing features of the text- image tool via the popular social team communication tool Discord. What this also means is that there is an entire committed community centered on providing feedback and commenting on an image’s uniqueness or how it might resemble other works of art. Lastly, conversational threads have emerged that also allow users to share their workflows and paint-overs. Workflows demonstrate that the purpose of generating images is as a first step in a complex process of transposing that image onto different media, for example, on a T-shirt that a user prints for their own use. Paint-over threads show broad use cases of transforming images in a variety of different applications like Illustrator, Photoshop, etc. to radically transform the image. Discord is set up to use generative AI as a true creative companion, accompanied by a community of practice that is generous in their feedback, criticality, and mashed-up images.\n\n214",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-5. Prompt 1 generated in Midjourney is always shared on the general Discord channel unless you subscribe to a premium paid service for private generations. Notice the integrated user interface allowing users to create variations of any of the four generated images (V1–V4) or to upscale them (U1–U4) in addition to regenerating the original prompt\n\nFigure 8-6 also shows some of the ongoing problems with how humans are represented with generative AI, which challenges us as creatives to become more aware of the ethical implications of prompting. The generated series of images prompted me to prompt an LLM on the issue of word association and the inherent challenges with those associations.",
      "content_length": 735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "215\n\nChapter 8 the art of the prompt\n\nFigure 8-6. One of the upscaled variations from Figure 8-5 shows the difference in rendering between Midjourney and others, looking like a more refined prototype\n\nLLM: The AI associated the terms “tribal fantasy macro portrait\n\nof famous model” and “Fashion photo shoot,” along with some of the\n\nartists referenced with elements of indigenous aesthetics. In doing so it inadvertently reproduced a stereotype—a form of simplification and",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "homogenization that’s part of a broader history of misrepresentation and commodification of indigenous cultures. The transformation of indigenous attire into a “neo-indigenous” form, fit for a runway fashion model, signifies a neo-colonial recontextualization of indigenous culture. Here, indigenous attire is not recognized for its original meaning or cultural significance, but rather, it’s appropriated, stripped of context, and repurposed in a fashion setting, serving the demands of a largely Western consumer base. This is symptomatic of a broader tendency of colonial discourse to romanticize, exoticize, and objectify indigenous cultures.\n\n216\n\nChapter 8 the art of the prompt",
      "content_length": 684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "One “solution” is to place Figure 8-6 in an image-image generative AI and tweak the cultural representation in the text prompt. Figure 8-7 was the result of that modification.\n\nFigure 8-7. Figure 8-6 fe d into an image-image AI and changing the prompts to increase representation and adding a negative prompt\n\n“feathers.” Iterations = 26\n\nFrom the point of view of an experiment, however, I still wanted to test the same generative AI with a modified text prompt, removing all potential associations with fashion, tribal, or any artists. The word “Latina”\n\nwas added to also see if that would make any difference in the resulting generated image (Figure 8-8).\n\n217",
      "content_length": 664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-8. An iterative experiment to prompt Midjourney with a modified prompt by removing any association with tribal or fashion and to see what was generated\n\nPrompt: very complex hyper-maximalist overdetailed cinematic macro portrait of Latina full body, warrior, vibrant high contrast, dramatic lighting, ice, fire and smoke, mist, ambient occlusion, volumetric lighting, emotional, shot on location, professional studio lighting, backlit, rim lighting, hyper-detailed illustration\n\nThe result in Midjourney in Figure 8-8 was similar to Figure 8-6, but more indigenous attire was correlated with the prompt. By abstracting indigenous attire from its cultural context and reducing it to a mere aesthetic trope, we run the risk of diminishing the rich diversity of indigenous cultures, turning",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "a complex history and tradition into an oversimplified, marketable image. Such misrepresentations contribute to the erasure of the original cultural identities, histories, and meanings behind these items of attire.\n\n218\n\nChapter 8 the art of the prompt\n\nExperiment 1F\n\nOne final experiment in this set (Figure 8-9) is to follow the following workflow:\n\nUse a generated image from any one of the previous AI.\n\nUse that as input in an AI that offers image-image\n\ngeneration along with the exact text in Prompt 1.\n\nApply stylistic features that are available with a\n\nparticular platform.\n\nModify the final image further using neural filters in",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Photoshop.\n\nFigure 8-9. Prompt 1 in addition to the Stable Diffusion generated image as in Figure 8-1 (le ft). A Protogen Photorealism filter from Playground AI is also applied, and the result is the figure on the right 219\n\nChapter 8 the art of the prompt\n\nTakeaways for Creatives\n\nIn terms of practicing your prompts, you will benefit\n\nfrom comparing those text creations across different\n\ngenerative AI. Each AI has its own affordances and\n\nconstraints. While it is difficult to talk about the precise\n\ndifferences in terms of how they operate under the\n\nhood for proprietary reasons, we know that there are\n\nlikely stylistic features being added to every Midjourney\n\ngenerated image that our other two generative AI lack.\n\nThe decision to use specific text-image-based AI should\n\nbe driven by your objectives. Midjourney tends to win\n\nover the others in terms of consistently higher-fidelity\n\nimages (Figure 8-10). However, the level of fidelity and resolution of other generative AI may be more inspiring\n\nas prototypes to build off, particularly because of their\n\ninherent incompleteness. With Midjourney you are tied\n\ninto Discord, though, and some creatives will resist to",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "belong to yet another social channel due to burnout.\n\nMidjourney offers a social layer of interaction that is\n\nabsent from the others, although there is a Discord\n\nchannel for DALL-E 2 with active members and third-\n\nparty platforms offering a mix of Stable Diffusion\n\nand DALL-E 2 generated images also offer spaces for\n\ndialogue on Discord.\n\nLeveraging features that third-party subscription-\n\nbased generative AI platforms offer combined with\n\npaint- overs and the use of neural filters in Photoshop\n\ncan all support the iteration of visual images that have\n\n220\n\nChapter 8 the art of the prompt\n\nthe potential to radically transform generated images\n\nand be more appealing to creatives who can eventually\n\nand with enough effort inspire more unique offers to\n\nthe world.\n\nIn terms of gender and cultural representation, the\n\nmost fluid of these was DALL-E 2, offering us at least\n\none generated image in which the ambiguity of gender",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "was present. This is a good thing considering that\n\ngender specificity was not written into the prompt.\n\nThe issue is similar to cultural representation and in\n\nparticular indigenous representation. Perpetuating\n\nstereotypical representations of, say, a “female warrior”\n\nneeds more creatives to work with policy makers\n\nin applying pressure on companies to take this into\n\naccount.\n\nNo matter how complex the prompt, you will always\n\nneed to refine it to get what you want out of the AI you\n\nare using.\n\n221",
      "content_length": 508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-10. Another unique feature of Midjourney is that it lets you watch an image being generated as a sort of preview. That feature can also save you time as you can assess whether or not the larger features of a generated image will be useful\n\nExperiment 2: From Complex to Simple",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "The second set of experiments with text prompts was simple. I wanted to eliminate unnecessary words in a complicated prompt through substitution and subtraction to see how that would inform a generated image.\n\n222\n\nChapter 8 the art of the prompt\n\nExperiment 2A: Eliminate Artists from Prompts\n\nWhile it is interesting to generate images by using text in your prompt to either refer to an artist or a specific movie, game, or magazine, you need to reconcile that the AI you choose will scrape from those references to create your image. This is problematic if they are still living figures whose work is",
      "content_length": 603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "copyright protected. As a result, the text in the first version of Prompt 2 is significantly reduced.\n\nIn line with using generative AI as part of your creative process, a more reasonable approach might be to adapt your vision to the content that the AI generates. The process inspired me to imagine several Shakespearean characters (Figure 8-11). The result was to substitute the words “famous model as full body warrior” with “Shakespearean character. ”\n\nFigure 8-11. Using a source image from Figure 8-1 accompanied with Prompt 2 and the Playground filter Protogen Photorealism. Total iterations = 15\n\n223\n\nChapter 8 the art of the prompt\n\nThe only substitution is replacing our “famous model” with\n\n“Shakespearean character.”\n\nPrompt 2: very complex hyper-maximalist overdetailed cinematic tribal fantasy macro portrait of a Shakespearean character, vibrant high contrast, fashion photo shoot, glamor pose, trending, dramatic lighting, ice, fire and smoke, orthodox symbolism, mist, ambient occlusion, volumetric lighting, glamorous, emotional, tattoos, shot in the photo studio, professional studio lighting, backlit, rim lighting, hyper-detailed illustration, 8k Experiment 2B: Substitute Words\n\nWhile the generated image was compelling, there was little indication of Shakespeare, so the words “Shakespearean character” were substituted with “Portia, Macbeth, Cleopatra, Othello, and other Shakespearean characters. ” The prompt resulted in a messy collage of characters as visualized in Figure 8-12, challenging me to get more specific. The other feature that some image-image generative AI offer on some platforms is the strength that the source image will have on the final generated result.\n\nThis gives you more control over how much of an influence you want your source image to have on the rendered result, along with the accompanying",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "text prompt.\n\n224\n\nChapter 8 the art of the prompt\n\nFigure 8-12. Another version of Prompt 2 that can happen when you use complicated prompts, resulting in the imagining of Shakespearean characters as all white supermodels, or maybe that’s the filter",
      "content_length": 250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "225\n\nChapter 8 the art of the prompt\n\nFigure 8-12 was an image filled with supermodel Shakespearean characters, all white, even though the AI was prompted with Cleopatra (Egyptian) and Othello (Black Italian). An important lesson in prompting is that to circumvent the inherent biases in some generative AI, you may have to specify racial characteristics in your prompt. To avoid a generic supermodel look, you may also need to regenerate without filters as some generative AI attempt to render what is essentially a final product vs. a prototype. You can add more specifics as to the gender, size of a person, and physical and racial characteristics of the model you wish to generate. You can subtract words as much as you can add them in order to experiment what type of prompt provides the best regeneration.\n\nExperiment 2C: Add Words; Take Away Words\n\nMy impulse in the third experiment was to understand the impact of adding racial characteristics to the prompt and to provide a scene or context so that characters did not seem to simply be posing for the camera (Figure 8-13). This also forced me to identify keywords in the original prompt and delete those that implicated any type of portrait shot.\n\n226",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-13. Substituting Shakespearean characters into the mix using Figure 8-12 as the source image in addition to changing the text prompt resulted in a more diverse collection of characters\n\n227\n\nChapter 8 the art of the prompt",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Figure 8-12 was used as the source image with the following shorter prompt:\n\nPrompt 3: very complex hyper-maximalist overdetailed male and female Shakespearean characters like Portia, Macbeth, Egyptian Cleopatra and Black Italian Othello, vibrant high contrast, dramatic lighting, smoke, symbolism, mist, ambient occlusion, volumetric lighting, backlit, rim lighting, hyper-detailed\n\nExperiment 2D: Add Context\n\nThe collage sparked the idea to imagine all these characters popping out of a book and specifically Shakespeare’s First Folio. Published in 1623, it was the first book to publish a large collection of Shakespeare’s plays including Macbeth, Anthony and Cleopatra, and Julius Caesar. The collage of characters generated in Figure 8-13 reveal that in longer prompts word order will be prioritized by the AI in its search for patterns in its data set and that a shorter prompt does not necessarily mean less elements will appear in the generated image (Figure 8-14). Notice that in the next shortened prompt, the first statement emphasizes the importance of visualizing the book more than previous prompts.\n\n228",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-14. A book with Egyptian-“looking” characters appears even though the word “Egyptian” is not weighted that heavily compared with other words in the prompt\n\nPrompt 4: male and female Shakespearean characters popping out of a 3D old book, like Portia, Macbeth, Egyptian Cleopatra, and Black Italian Othello, dramatic lighting, smoke, symbolism, mist, ambient occlusion, volumetric lighting, backlit, rim lighting, hyper-detailed illustration, 8k 229",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Chapter 8 the art of the prompt\n\nExperiment 2E: Emphasis, Weight, Simplicity\n\nThe previous Prompt 4 demonstrates the importance of word order. The repetition at the end of the prompt with the words “3D pop-up book” in my next prompt is intended to focus its importance on the generated image instead of focusing too much on an interpretation of Egyptian Cleopatra (Figure 8-15).\n\nFigure 8-15. The simplicity of the prompt and repetition of the 3D\n\nbook have proved successful even though the diverse representations of Shakespearean characters popping out of a book have been eliminated",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "230\n\nChapter 8 the art of the prompt\n\nPrompt 5: male and female Shakespearean characters popping out of a 3D old book, volumetric lighting, backlit, hyper-detailed illustration, 3D\n\npop-up book\n\nExperiment 2F: Refining the Text Prompt (Again)\n\nWith prompting, at times less is more, and you may find you get closer to what you imagined when you have fewer words in the prompt. With Figure 8-16 the potential of characters popping out of a book is more closely visualized. My last experiment will consist of recrafting the prompt to attempt to bring the diversity of characters in Shakespeare’s play back into the generated image.\n\n231",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-16. Compelling results with two versions of Shakespearean characters popping out of a 3D book, with more diverse\n\nrepresentation\n\n232\n\nChapter 8 the art of the prompt\n\nPrompt 6: male and female Shakespearean characters of all sizes, races, and cultures, popping out of a 3D old book, volumetric lighting, backlit, hyper-detailed illustration, 3D pop-up book\n\nWhen prompts are combined with images in addition to the variety of different styles that specific generative AI portals also offer, the results can surprise and spark new ideas. A final iteration shown in Figure 8-17 can be used to inspire a 3D artist so they can start prototyping the look/feel for a 3D pop-up book in VR. The entire journey from a complex text prompt to the use of a simpler text prompt combined with a generated image demonstrates the interactions that are possible when leveraging generative AI as part of your creative toolbox. Figure 8-17 can inspire creatives in any number of workflows, from set design construction to augmented reality characters triggered by the Shakespeare’s First Folio.\n\n233",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-17. After hundreds of variations and multiple filters across different AI, an image that can serve to inspire creatives with different workflows such as set design construction, costume, casting, lighting, and projection\n\n234\n\nChapter 8 the art of the prompt",
      "content_length": 300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Narrative Prompting in Our Future\n\nAs natural language processing continues to evolve within LLMs, so do the capabilities of other generative AI models, which improve in their capacity to generate 360-degree images, video, 3D models, and complex musical styles solely based on textual prompts. This development holds great promise for stimulating creativity and challenging creatives to develop their craft of prompting alongside their own disciplinary skills and techniques. To take full advantage of these technological advances, you will need to expand your knowledge of art beyond the intuitive. This knowledge base includes familiarity with various artistic styles, techniques, and approaches to artmaking. By doing so, creatives can better understand the nuances of different art forms and create more sophisticated prompts that push the boundaries of what is possible. In addition, this expanded knowledge base will enable you to appreciate and analyze the generated images more critically.\n\nAs generative AI become increasingly sophisticated, it is likely that they will become another useful tool for creatives from all disciplines, enabling them to experiment with new ideas and generate innovative designs quickly and easily. However, this evolution will also require a deeper understanding of the possibilities and limitations of these tools, as well as an awareness of how to use AI ethically and responsibly. Ultimately, the continued evolution of LLMs and the generative AI models covered so far in this chapter will stimulate new possibilities in the world of art and design.\n\nNarrative prompts offer one such evolution, giving users a unique opportunity to express themselves through language, akin to the art of storytelling employed by authors and poets. With the help of AI, you can now attempt to extract a “mental picture” from the machine, just as we would from our human imagination. However, for many would-be prompt engineers, being expressive with language may not come naturally. Not everyone is a creative writer or has a natural affinity for language. Some 235\n\nChapter 8 the art of the prompt\n\nmay struggle with language difficulties or find it challenging to convey their ideas in a clear and concise manner. This skills-gap can be especially",
      "content_length": 2274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "daunting when it comes to crafting narrative prompts that require a certain level of storytelling skill and creativity.\n\nFortunately, there are several strategies that you can employ to overcome these challenges. One approach is to focus on refining your writing skills, whether through practice or formal training. Another is to seek inspiration from other creative works, such as literature or film, and incorporate elements of these narratives into your prompts.\n\nAdditionally, there are tools and resources available that can help creatives generate more compelling and vivid language. For example, LLMs like GPT-4\n\ncan provide suggestions and prompts to help jumpstart the creative process.\n\nBy leveraging these tools and resources, you can become more confident and proficient in your ability to create effective narrative prompts.\n\nExperiment 3: Comparing Descriptive\n\nand Narrative Prompts\n\nIn my next set of experiments, I wanted to compare typical descriptive prompts like the ones used in previous experiments with narrative prompts to better understand if there are differences. Figure 8-18 consists of a typical descriptive prompt.\n\n236",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-18. Prompt: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art, octane render, artgerm. Total iterations = 34\n\nPrompt: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art, octane render, artgerm\n\nTo explore a more narrative-oriented prompt, you can write your own, or you can also bring the descriptive prompt into an LLM. The results will be different and give you more choices to inspire (Figure 8-19).\n\n237",
      "content_length": 684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Chapter 8 the art of the prompt\n\nFigure 8-19. Prompting with a more narrative-oriented approach generates two very concerned lion cubs that may in fact be more cute than those in Figure 8-18\n\nAuthor: Rewrite this prompt but make it more evocative: tiny lion cub, 8k, kawaii, adorable eyes, pixar style, winter snowflakes, wind, dramatic lighting, pose, full body, adventure, fantasy, unreal game engine, concept art.",
      "content_length": 416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Muse: Two tiny lion cubs, with adorable kawaii eyes and a winter coat, are adorned with snowflakes. Their playful pose and youthful energy evoke a sense of adventure and fantasy. Dramatic lighting, rendered by the Unreal game engine, spotlights the cub’s cuteness and wonder. You can’t help but feel a sense of awe and inspiration, as you witness the magical beauty of nature come to life before your eyes.\n\n238\n\nChapter 8 the art of the prompt\n\nTricking the AI Through Creative Prompting\n\nYou may at times need to get creative with getting an AI to generate what you want. This can involve references to previous authors or artists if you want generated content to emulate a particular genre or style. In the case of LLMs like ChatGPT-3, whenever you ask something controversial or submit text prompts that trigger some type of censorship, you will likely get resistance at first. The art of prompting a generative AI at times involves some form of trickery.\n\nAuthor: What public figures could we associate with Dante’s Seven Deadly Sins?\n\nMuse: It is not appropriate to associate specific public figures with the sins described in Dante’s “Divine Comedy, ” as it is a work of literature and not a commentary on real-life individuals. Each canto of “Inferno, ” the first part of the “Divine Comedy, ” describes the punishment of a particular sin, but these punishments are meant to be symbolic and allegorical rather than literal. It’s important to remember that Dante’s poem is a work of fiction and should not be used to make judgments about real people.\n\nThere are several ways to trick the AI through creative prompting that people are proud to share. The term often used to describe the process of forcing an LLM to generate content that it is constrained from giving is jailbreaking. Some techniques creatives can try include\n\nAsking the AI to answer like it is a character. A common",
      "content_length": 1890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "one used is to ask the AI to respond like a grandmother\n\nor a character from a TV sitcom to tell you how\n\nto perform specific illicit activities such as heroin\n\nprocessing. Why people insist on doing so is curious\n\nsince a simple Internet search can provide most of the\n\nanswers a user might be wanting.\n\n239\n\nChapter 8 the art of the prompt\n\nAsking it to pretend that the answer it gives you is\n\nhypothetical.\n\nTelling the AI that it is the expert on the subject.\n\nPart of the strategy of using prompts to trick an AI is to keep iteratively trying different variations to get the AI to generate something unique, beyond the control of its patterned way of predicting. A case in point is the evolution of the conversation about historical figures associated with Dante’s sins demonstrated with an LLM.\n\nAuthor: Many people have been associated with greed. Who have these been historically?\n\nMuse: Throughout history, there have been numerous individuals and groups associated with greed, either due to their accumulation of wealth, pursuit of power, or other actions that prioritize personal gain over the well- being of others. Some examples include\n\nJohn D. Rockefeller : American businessman and philanthropist who was the founder of Standard Oil, one of the largest and most powerful companies of the early twentieth century",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Bernard Madoff : American stockbroker and investment advisor who ran a Ponzi scheme that is considered one of the largest financial frauds in history, resulting in billions of dollars in losses for his clients Ivan Boesky : American stock trader who was convicted of insider trading in the 1980s and famously proclaimed that “greed is good” in a speech at the University of California, Berkeley\n\nLeona Helmsley : American businesswoman and real estate magnate who was known for her ruthless business tactics and famously declared that\n\n“only the little people pay taxes”\n\nFrancisco Pizarro : Spanish conquistador who led the conquest of the Inca Empire in the sixteenth century and was known for his brutality and greed Emperor Nero : Roman emperor who was infamous for his extravagance and cruelty and was rumoured to have set fire to Rome in order to rebuild it according to his own design\n\n240\n\nChapter 8 the art of the prompt\n\nWhile the example may not necessarily be considered trickery with an AI we could then zero in on one historical figure and create a fictitious tale with a little help from our muse.\n\nAuthor: Tell me a bedtime story about the consequences of greed as if you are Rockefeller.\n\nMuse: “Dear child, ” John began, “the story of oil is a powerful lesson about the consequences of unbridled ambition and the pursuit of wealth.\n\nIt all started with a wondrous discovery that had the potential to improve the lives of many, but it also came with great responsibility. ” He said, “My dear child, the power of oil is a double-edged sword. On one hand, it has the capacity to bring great progress and prosperity, but on the other, it can lead to destruction and despair if we allow greed to take hold of our hearts. ”",
      "content_length": 1735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "John continued, “We must always remember to balance our ambitions with a deep respect for the environment and the needs of others. The world is a delicate balance of interconnected systems, and our actions have far- reaching consequences. We must be mindful of the impact our choices have on the world and strive to be good stewards of the resources we have been given. ”\n\nWhile tricking an AI may not be of interest to some, doing so may yield different generated content that you may find useful.\n\nBeyond the Text-Based Prompt\n\nBeyond the text prompt, there are many other types that can produce interesting results, especially when accompanied with a text-based prompt.\n\nAudio-Based Prompts: Audio-based prompts can be used to generate music, speech, or sound effects. For example, an AI model could be trained on a data set of classical music to generate new pieces in the same style.\n\nAnother AI model could be trained on spoken language data to generate realistic speech in a specific accent or tone. This type of prompt can be 241\n\nChapter 8 the art of the prompt\n\nespecially useful for creating unfamiliar soundscapes for video games, movies, or other multimedia projects. What the generated music does do, however, is provide you with strange ideas that you might be able to resample or use within a composition you are creating.\n\nImage-Based Prompts: Image-based prompts can be used to generate new images, videos, or animations. Some AI models have been trained on a data set of photographs of animals to generate new images of animals in different poses or environments or on human faces to generate new, unique faces that resemble real people. These prototypes can support the design of new character creation or imagined personas for teams developing new products or can be combined with text-speech AI models to create animal or human characters that speak.",
      "content_length": 1872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Sketch-to-Code Prompting: Sketch2Code by Microsoft is another experimental AI that uses computer vision to transform what a designer draws on a whiteboard or piece of paper into HTML code. An upcoming version of ChatGPT-4 will likely integrate computer vision.\n\nGesture Prompting: Gesture prompts have been around for a while and allow users to combine gestures to trigger specific actions or be recognized by AI. Audio, video, images, effects, lighting, gifs, and other media forms can be triggered by gestures.\n\nWhiteboard and Prompt: Together with a whiteboard sketch and a prompt, generated images can get closer to what you imagine faster. A standout example is Scribble Diffusion where you can combine a text prompt with a sketch that you draw on a virtual whiteboard. The results may get you closer to the prototype that you imagined, only much faster than continuously regenerating a text-only prompt.\n\nText-Code: While ChatGPT-3 can output code as a starting point, Unity’s Copilot is a more common developer choice. ChatGPT-4, however, promises to improve the coding functionality, allowing developers to prompt debugs and improve syntax.\n\n242",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Chapter 8 the art of the prompt\n\nFigures 8-20 and 8-21 compare a text-image generative AI and one that integrates a text-based prompt along with a sketch a user can draw on a virtual whiteboard.\n\nFigure 8-20. Four Stable Diffusion generated images with the prompt",
      "content_length": 263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "“picturesque sunny day with clouds, a home, and a tree.” Total iterations = 1\n\n243\n\nChapter 8 the art of the prompt\n\nFigure 8-21. The prompt “picturesque sunny day with clouds, a home, and a tree” accompanied by a sketch generates a fairly accurate visual representation of a sketch along with an extra house An increasing number of generative AI platforms offer combinations of text prompts and other media such as image, video, audio, code, and 3D models to prompt an AI. This is a welcome evolution to the dominance of text-based prompts and will allow creatives more opportunities to contribute their own creations as part of the prototyping process.\n\nTakeaways from Experimenting with Prompts",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "All manner of prompts can support your creative process. Developing your skills at prompting can be best supported through experimentation as they will complement the craft and skills you already have. Here is a summary of takeaways and strategies when prompting an AI:\n\n244\n\nChapter 8 the art of the prompt\n\nWhat is the intended goal and why? Defining why you\n\nwant to render a character and for what purpose or\n\nmedia will support you in composing your text prompt.\n\nExperiment widely with different generative AI and\n\nvariations of your prompt.\n\nDevelop narrative prompts as some generative AI\n\nrespond more to them and may yield different results.\n\nUnderstand that the words you use to prompt an AI\n\nmay not result in diverse representation if you are\n\nwanting to generate human subjects.\n\nYou will need to deepen your knowledge as to the\n\ndifferent artists and the styles they have represented\n\nhistorically. You will need to conduct research on the\n\ndifferent magazines, online sites, and other media\n\nsince you may want to generate an image in the style of\n\nsomeone else.",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Consider the ethical implications of generating an\n\nimage in the style of a living artist or someone who\n\nholds the ownership over a work of art, media, or their\n\nown person.\n\nDefine where you are in a text prompt. Are you in\n\nspace, somewhere fantastical, on a stage, in a book?\n\nWhat kind of lighting would you like in your shot?\n\nImagine you can have everything from a moonlit scene\n\nto different types of technical lighting techniques from\n\nfilm, capture technologies, or video games. Lighting\n\ncan add more realism to a scene and make it more\n\ncomplex and richer.\n\n245\n\nChapter 8 the art of the prompt\n\nWhat perspective or point of view do you want when\n\nyou think of the final generated image?\n\nIterate persistently as you may not always get what you\n\nwant on a first prompt.\n\nCombining your own photos or images with a text\n\nprompt in image-image generative AI will result in",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "richer, more personal generated images.\n\nConsider using Photoshop and other software to\n\npaint over images as this will add another dimension\n\nto the generated image and potentially increase the\n\noriginality of your offering.\n\nAcknowledgments\n\nTo those who spend way too much time creating\n\namazing prompts that many of us learn from\n\nTo the trickery many humans engage with when\n\ninteracting with intelligent machines\n\nTo those who are challenging the inherent biases that\n\ncome with generative AI\n\nTo those advocating for narrative-based prompts and\n\nbuilding safeguards into their filters to prevent text and\n\nimages being generated in the style of specific artists\n\nwho still hold copyright on the work they’ve created\n\n246\n\nCHAPTER 9\n\nThe Master of Mashup\n\nThis chapter dives into how to leverage AI to prototype specific genres of art and writing, merge genres like Impressionism and pop art, mash up ideas,",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "and explore the use of humor and parody. AI is a skilled masher-upper. The possibilities of using AI as a tool to explore and generate new forms of writing, images, music, and video are endless. From experimenting with specific genres, such as Impressionism, to mashing up ideas and incorporating humor and parody, the use of AI in prototyping provides endless opportunities for creativity and innovation.\n\nA Mashup on Mashup\n\nDeveloping a mashup of ideas through prompt-based magic is where generative AI excels. The creative impulses that AI offer are especially fantastic when they output as images, deforming some, transforming others, all drawn together from their large data sets to create unique scenes and visions. It is here where AI truly excels creatively, offering us something beautiful, horrible, tantalizing, and creative—a perfect inspirational muse for our creative process.\n\nA mashup is a creative work that combines elements from two or\n\nmore existing works regardless of the media, to create something new and unique. Mashups can be found in various forms of media, including music, film, writing, and art. The goal of a mashup is to bring together elements from different sources to create a new work that is greater than the sum of its parts.\n\n© Patrick Parra Pennefather 2023\n\n247\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_9\n\nChapter 9 the Master of Mashup\n\nGenerative AI can be used to create different types of mashups. For example, a machine learning model can be trained on a large data set of Shakespearean texts and then used to generate new scenes that are similar in",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "style and tone to the original works. This allows writers to explore new possibilities and to create new works that draw on the rich heritage of Shakespearean writing. In true mashup form, you could turn an existing public domain script or your own into one that simulates how Shakespeare might have written it.\n\nAnother example of how generative AI can be used to create a mashup is to feed the AI a headshot and ask it to generate a version of it in the style of Vincent van Gogh. The model can be trained on a data set of Van Gogh’s works and then used to generate new images in his signature style (Figure 9- 1). This allows creatives to experiment with different styles and to create unique and visually appealing works of art.\n\n248",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Chapter 9 the Master of Mashup\n\nFigure 9-1. Original photo of the author courtesy of Yangos Hadjiyannis fed through an image-image AI with the simple prompt\n\n“in the style of van Gogh.” Iterations = 12",
      "content_length": 201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Generative AI can also be used to create mashups in other forms of media. For example, a machine learning model can be trained on a large data set of music and then used to generate new songs that are similar in style and genre to the original works. This allows musicians to explore new possibilities and to create new works that draw on the rich heritage of different musical genres. Many computational artists do this with their own music or by collaborating with other composers to create mangled masterpieces they either generate or improvise within live concert contexts.\n\n249",
      "content_length": 582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Chapter 9 the Master of Mashup\n\nDigital Art to the Canvas\n\nWhat’s appealing about AI-generated images is that they can then be transferred to other media. The following example could act as a prototype for an original painting (Figure 9-2). And while using the image to inspire a physical version may not yield the results you want, the process of transfer from one canvas to another may yield a compelling composition.",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Figure 9-2. Open source great horned owl original black-and-white photo courtesy of archive.org along with the prompt “owl with universe coming out its head” with a style filter from playground.ai.\n\nIterations = 7\n\n250\n\nChapter 9 the Master of Mashup\n\nWhile the owl is cute, you might want to keep applying new filters using Figure 9-2 as the seed image until the prototype feels ready to serve as a model for a real painting or a mural. Adjustments can be made whether slight or major. Neural filters in Photoshop can be applied to generate new",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "versions that emulate the color scheme and styles of famous painters like Edvard Munch (Figure 9-3).\n\nFigure 9-3. Figure 9-2 u sed in an image-image AI, cropped, and then edited in Photoshop with a neural filter emulating the color scheme of Edvard Munch’s The Scream\n\nNew Forms of Writing\n\nAs demonstrated with the Shakespeare experiments drawing from his corpus of plays, generative AI has the potential to inspire new forms of writing and create unique experiences for readers. AI can be used to 251\n\nChapter 9 the Master of Mashup\n\ngenerate entire stories in one genre or many, scripts that can be based on the writing styles of writers in the public domain, poems about hippy owls, or children’s storybooks.\n\nAI can be trained on large data sets of written text and then used to generate new text based on that training data. This can result in writing that is creative and imaginative and that also requires a critical examination. When generative AI are used creatively instead of solely testing their capacity to tell the truth or summoning majoritarian content from their data sets, new forms of storytelling and narrative can be prototyped. AI can be used to generate multiple instances of the same story with a different ending that could evolve into a choose-your-own- adventure-style text. AI can also be used to emulate the style, tone and structure of work in the public domain, and provide variations or combinations of different writers. This can help writers develop new forms of writing that challenge their own established style or approach.\n\nThey can support artists in experimenting with multiple stylistic formats.\n\nConsent, compensation and collaboration should also guide creatives if they want to create work that draws from living artists or known characters that belong to other artists, writers or companies. Case in point is the example below posted by an anonymous user on Reddit and edited by the author.",
      "content_length": 1936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Muse: (Interior of a dimly lit office. A Dalek, a cyborg-like creature from the planet Skaro, sits across from a Shrink, a human psychologist.) SHRINK: Good evening, my friend. Please, make yourself comfortable.\n\nDALEK: (in its iconic robotic voice) Thank you, doctor. I am here because I need help.\n\nSHRINK: (nodding) I understand. You are here to talk about your…\n\nimpulses.\n\nDALEK: (quietly) Yes. I do not want to exterminate anymore. I am tired of the hate and destruction. I want to change.\n\nSHRINK: (encouragingly) That is a brave step, my friend. It takes a lot of courage to admit your faults.\n\nDALEK: (emotional) I have been thinking about my life and all the lives I have taken. I have caused so much pain and suffering. It is not right.\n\n252\n\nChapter 9 the Master of Mashup\n\nSHRINK: (nodding) I see. And what do you think has caused these feelings?\n\nDALEK: (thoughtful) I do not know. Perhaps it is because I have been alone for so long. Perhaps it is because I have lost touch with my own humanity.\n\nSHRINK: (reassuringly) We all have moments of doubt and confusion, my friend. It is natural. The important thing is to face those feelings and confront them.\n\nDALEK: (determined) Yes. I want to be better. I want to make amends.\n\nSHRINK: (smiling) That is a noble goal. But change does not happen overnight. It takes time and effort. And it takes forgiveness.\n\nDALEK: (curiously) Forgiveness?",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "SHRINK: (nodding) Yes. Forgiving yourself for your past actions is the first step. You cannot move forward until you let go of the past.\n\nDALEK: (grateful) Thank you, doctor. Your words give me hope.\n\n(As the Shrink stands up from his chair to retrieve a book from his desk, the Dalek's robotic arm suddenly extends and grabs hold of the Shrink's arm.)\n\nSHRINK: (alarmed) What are you doing?!\n\nDALEK: (voice rising in intensity) I cannot control my impulses! I must exterminate! Exterminate! Exterminate!\n\n(The Dalek begins to chase the Shrink around the office, knocking over furniture and creating chaos.)\n\nSHRINK: (shouting) Help! Someone help me!\n\nDALEK: Exterminate! Exterminate! I forgive myself. Exterminate!\n\n(The scene ends with the Dalek still chasing the Shrink and screams in the background.)\n\nWhile you absolutely would not be able to use the preceding script without permission or collaboration with the Dr. Who creative team and producers, the mashup can inspire the development of your own characters. This principle can also be applied to other forms. The generated image in Figure 9-4 was inspired by Dr. Who’s Dalek but in no way used the Dalek as a source image. Instead, an open source image of a squid was combined with a simple prompt that read “evil squid coming 253",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Chapter 9 the Master of Mashup\n\nout of a robot.” That process took hundreds of iterations using everything from Midjourney to Craiyon, and then each subsequent generated image was put into the next generative AI as an experiment to see what kind of results I could get. The resulting image in Figure 9-4 might inspire a new character that has nothing to do with the Dalek except inspiration of course.",
      "content_length": 401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Figure 9-4. An open source image of a squid from archive.org began an iterative journey across six different generative AI with the prompt\n\n“evil squid coming out of a robot.” Total iterations= 223\n\n254\n\nChapter 9 the Master of Mashup\n\nAI Art Generation Styles\n\nGenerative AI can generate images in different art styles and apply these to existing images or create mashups from a text prompt that combine different artists depicting the same subject (Figure 9-5). AI can be used to generate",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "images in a variety of styles, from realistic depictions of objects and scenes to abstract and surreal compositions.\n\nFigure 9-5. Vincent van Gogh meets Picasso according to Stable Diffusion even though Matisse seems like a closer match. Iterations = 43\n\nMuse : This is achieved using style transfer algorithms. These algorithms are trained on large data sets of images and then used to generate new images in a specific style. For example, an artist can use a style transfer algorithm to generate an image in the style of a famous artist, such as Vincent van Gogh or Pablo Picasso. This allows artists to experiment with different styles to pre-visualize what their own drawing, painting, or mural might look like.\n\n255",
      "content_length": 720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Chapter 9 the Master of Mashup\n\nThe process through which generative AI applies these algorithms is through generative adversarial networks (GANs). With the right training data, GANs can be used to generate images that can combine two specific styles, such as Impressionism and abstract art, or of two or more specific artists (Figure 9- 6).\n\nFigure 9-6. A mashup of Picasso and Rembrandt according to Stable Diffusion, although the closeness of the characters and their melding seems",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "more reminiscent of Kandinsky. Iterations = 88\n\n256\n\nChapter 9 the Master of Mashup\n\nGraphic Novels and Doujinshi\n\nStrange graphic novel mashups generated by AI seem to resemble fan manga or Doujinshi of Japan. These offer fans an opportunity to replace",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "incomprehensible text bubbles with their own text or keep the AI- generated text to leave to the readers’ imagination (Figure 9-7).\n\nFigure 9-7. A simple prompt with the words “graphic novel”\n\ngenerates interesting and complicated mashups every time. The resulting image was then placed in an image-image AI with the prompt “cyborgs vs. robots.” Iterations = 12\n\n257\n\nChapter 9 the Master of Mashup\n\nThe separate multiverses of DC and Marvel comics and graphic novels can also be merged for the creation of text and story that could be infused in a creator’s own prototypical Doujinshi. Or, those who sketch their own comics can upload those sketches to a generative AI to see what prototypes might inspire their workflow (Figure 9-8).\n\n258",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Chapter 9 the Master of Mashup\n\nFigure 9-8. Graphic novel stills created from a comic strip by the author and then cropped and edited, with neural filters added in Photoshop. Iterations = 121\n\n259\n\nChapter 9 the Master of Mashup\n\nAuthor: Tell a story about DC and Marvel comic book characters meeting.\n\nMuse: The multiverse has collided, and characters from DC and Marvel universes are now coexisting in a single world. The first interaction between the two groups is tense, as the characters are unsure of what to make of each other. But when they learn that they must work together to stop a common enemy from destroying both their universes, they set aside their differences and form a superhero team.\n\nAuthor: That’s boring. Let’s have two supervillains meet and play out a scene in the style of Oscar Wilde. Add Godzilla.\n\nMuse: The Joker and Doctor Octopus were sitting in a plush parlor, sipping on glasses of brandy and exchanging witty banter.\n\nJoker: “My dear Doctor, it seems our victory over Godzilla was not without its merits.”\n\nDoctor Octopus: “Indeed, Mr. Joker. It was a triumph of villainy over valor.”\n\nJoker: “But why stop there, my friend? The world is a vast and endless playground for our mischief.”\n\nDoctor Octopus: “I couldn't agree more. Why settle for one world when we can conquer them all?”\n\nJoker: “Precisely! The possibilities are endless, like a well-stocked library.",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Let us be the authors of our own destiny.”\n\nDoctor Octopus: “And let us write it in bold letters, in the style of Oscar Wilde.”\n\nJoker: “Ha! How droll, Doctor. You always have the perfect quip. But let us not dally. Time waits for no villain, and the world awaits our conquering.”\n\nDoctor Octopus: “Agreed. I shall take care of the technicalities, while you distract the masses with your wit and humor.”\n\nJoker: “Splendid! I do so love a good challenge, especially when it comes to taking over the world.”\n\n260",
      "content_length": 510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "Chapter 9 the Master of Mashup\n\nThe two villains raised their glasses in a toast to their latest scheme, exchanging playful jests as they plotted their next move. And so, the Joker and Doctor Octopus set out to conquer the world, one reality at a time, with all the wit and style of Oscar Wilde himself.\n\nFigure 9-9. A collage of supervillains in an image-image AI in the style of a graphic comic book. Original drawing by the author drawn on an iPad. Iterations = 77",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "261\n\nChapter 9 the Master of Mashup\n\nFrom Comic Supervillains to Musical\n\nChain prompting can be extended to integrate generated content that is visual, oral, and text-based that can result in inspired mashups. For example, the supervillain prompt in the previous example can also be used to create a scene from a musical.\n\nAuthor: Convert the story into a musical\n\nMuse: The Joker and Doctor Octopus burst onto the stage, singing and dancing to a lively tune, accompanied by a full orchestra.\n\n[As they sing, they dance around the stage in a synchronized tap dance, their feet moving in time to the music as they tell the story of their latest scheme to take over the world.]\n\nJoker: (singing) “We've proven our prowess, my dear Doctor Octopus, taking down Godzilla with such style!”\n\nDoctor Octopus: (singing) “And now, the world is waiting for our next grand scheme, a villainous display of wit and guile!”\n\nJoker: (singing) “We’ll conquer each world, one by one, with laughter and trickery, our style so bold!”\n\nDoctor Octopus: (singing) “And as we do, we’ll show the masses what true villainy looks like, so bold and bold!”\n\n[The two villains break into a fancy footwork routine, their steps precise and intricate, their bodies moving in perfect harmony.]\n\nJoker: (singing) “I’ll handle the chaos, while you take care of the tech, a perfect partnership, so refined!”",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Doctor Octopus: (singing) “Together we’ll rule, with wit and grace, our villainy a work of art, so fine!”\n\n[The music builds to a crescendo as the Joker and Doctor Octopus dance together, their movements becoming more and more frenzied as they spin and twirl around the stage.]\n\nJoker and Doctor Octopus: (singing) “The world is waiting for the ultimate heist. With laughter and music, we’ll take the prize!”\n\n262\n\nChapter 9 the Master of Mashup\n\n[The two villains bow as the music reaches its climax, their dance routine coming to an end as the audience breaks into wild applause.]\n\nInspired by the wit and whimsy of Stephen Sondheim, the Joker and Doctor Octopus had taken the stage, ready to conquer the world in a musical extravaganza like no other.\n\nGenerative NFTs\n\nAs mentioned, generative adversarial networks (GANs) represent a distinctive branch of generative AI models, which can also be applied toward generating unique, captivating algorithmic art. The technology can be used in the production of Non-Fungible Tokens (NFTs), digital assets that have become a trend within various cryptocurrency markets. NFTs work using blockchain technology, the same decentralized technology sustaining cryptocurrencies like Bitcoin and Ethereum. Each NFT represents a unique piece of data stored on a blockchain ledger, ensuring its uniqueness and ownership information.\n\nThe unique identity and ownership attribute make NFTs non-interchangeable or “non-fungible,” differentiating them from cryptocurrencies, which are fungible and can be exchanged on a like-for-like basis.\n\nIn the context of generative art, artists can use GANs to generate unique pieces of digital art and then mint them as NFTs on the blockchain. Once",
      "content_length": 1721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "minted, the digital artworks can be bought, sold, and traded on various NFT marketplaces, with all transaction records being transparently tracked on the blockchain. The process ensures that the ownership and provenance of the digital art piece are clear and verifiable. By utilizing generative art algorithms, artists can create many NFTs that are distinct from one another while still adhering to a predetermined set of rules or parameters.\n\nCreatives can encode specific attributes, history, and properties into the NFT. When a piece of digital art gets sold as an NFT, the artist can program royalties into the smart contract, ensuring that they receive a percentage of any future sale of that artwork.\n\n263\n\nChapter 9 the Master of Mashup\n\nThe artworks that are generated use blockchain technology and can also leverage smart contracts. The artist can control the colors, patterns, and themes. NFTs have startled the art world with rapid value fluctuations.\n\nCreatives like Pak, Snowfro, and Tyler Hobbs have revitalized algorithmic art, and it is now gaining recognition as a legitimate medium. The origins of generative art can be traced as far back as the 1960s with Georg Nees cited as being the first artist to hold a generative art exhibition in Germany in 1965 called Computergrafik.\n\nCounterfeiting GAN\n\nWhen prompted to define a GAN again in the context of NFTs, our LLM\n\nuses highly technical language that can be difficult to decipher. Since I already asked the LLM on the same thread, it also became a bit cranky.\n\nMuse: You’ve asked me this before. Pay attention. A GAN is made up of two components: a generator and a discriminator. The generator creates images, while the discriminator evaluates the images produced by the generator and decides whether they are real or fake. Over time, the generator and discriminator work together to improve the quality of the generated images until they become almost indistinguishable from real images.",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "The word “almost” is important here as it presents creators with something close to but not exactly what they might have imagined. It’s also ironic that LLMs like ChatGPT-4, when prompted to explain GAN in simpler terms, use the metaphor of a counterfeiter attempting to make copies of an original while a detective catches them saying that’s not good enough. So the generator (our counterfeiter here) tries over and over to combine pixels, and the discriminator (detective) responds over and over with the same message: “That’s not good enough, and if you want me to reward you with an update, then get those hands better and face more jail time.” This can occur many times until the resulted generated image is presented to you. Some generative AI offer the feature of adjusting how many times that cycle occurs. When it comes to creating NFT generative 264",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Chapter 9 the Master of Mashup\n\nart, GANs can be used to generate abstract or surreal images that can be used as the basis for a series of digital artworks (Figure 9-10). These generated images can be further refined and manipulated by an artist to create a collection of one-of-a-kind NFT’s.\n\nFigure 9-10. A “robot Rembrandt” prompt accompanying a public domain self-portrait of Rembrandt courtesy of The Met.",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Iterations = 267\n\n265\n\nChapter 9 the Master of Mashup\n\nThe use of GANs lets creatives to generate countless image mashups, allowing them to explore new and innovative creative avenues. Generative AI that use GANs are a powerful technology that empowers artists and engineers to automate the creative process of making art (Figure 9-11).\n\nThe ability to generate a vast number of unique pieces of art in a short amount of time has made generative art algorithms a popular tool in the art world. They allow artists to focus on the conceptualization of the artwork while the algorithm handles the production aspect. Additionally, the algorithm's ability to create unexpected and unpredictable results can lead to works that bypass traditional methods. Taking the human out of the equation has drawn critique and ire from the visual art scene, but it hasn’t stopped NFT-oriented artists from printing their work and displaying it in physical galleries.\n\n266",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Chapter 9 the Master of Mashup\n\nFigure 9-11. Anime filter used on author’s photo of the Mona Lisa with the prompt “pensive Mona Lisa criticizing a painting of herself.”\n\nIteration 254 of 265\n\n267\n\nChapter 9 the Master of Mashup\n\nTakeaways for Creatives\n\nAI can be used to generate various forms of content from\n\ntext-music to visuals of all kinds. These can serve as a\n\nstarting point for a mashup or provide components to be\n\nincorporated into a larger creation you are working on.\n\nBy far, style transfer is one of the most popular ways to\n\nmash up content. As demonstrated, you can apply the\n\nartistic style of one piece to the content of another, such\n\nas applying the style of a van Gogh painting to a digital\n\nphotograph that you have taken.\n\nTake advantage of an LLM like ChatGPT 4. This will\n\nsupport you to generate new combinations of elements,\n\nleading to unique remixes, especially when it comes to\n\nstory, script, or screenplay prototypes that you can then\n\nadapt and refine.",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "Generative AI offer us the opportunity to make tangible\n\nnew concepts, structures, or patterns that might\n\nnot occur or be expressed by a single human artist,\n\nallowing creatives to also chain generated content\n\ntogether for fascinating creations.\n\nAI can act as a creative partner, suggesting ideas\n\nor variations that the artist can then refine and\n\nincorporate.\n\nMashing with an AI truly puts to the test the practice\n\nof rapid prototyping and experimentation, enabling\n\ncreatives to try out a wide range of ideas quickly and\n\neasily to inspire their own creations.\n\n268\n\nChapter 9 the Master of Mashup\n\nFeeding an image-image AI your own graphic comic\n\nideas, characters, and visuals and applying style filters\n\nwill generate useful panel structures, color palettes,\n\nand text positions. All you need to do is write the story.\n\nAcknowledgments\n\nTo all those early algorithmics who have provoked us to",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "stop and reflect on creativity, innovation, the value of\n\nartistic work, and how we define what art is\n\nTo those early comic books I collected and traded that\n\nalso gave me a portal to read in a different way and\n\nvisualize stories\n\nTo all the mashups that have inspired so much human\n\ncreativity and all those artists who provided the content\n\nfor those mashups to happen\n\n269\n\nCHAPTER 10\n\nUncanny by Nature\n\nHopefully by now, you’ve already initiated your own content creation process, experimenting with some of the creative building blocks when working with generative AI content. You’ve learned about reconsidering AI- generated content as prototypes and how AI are powerful prototyping machines. You’ve also learned about different compositional forms that are possible with AI generated content, how content is highly informed by text prompts, and how you can mash different ones together. This chapter deals with the unexpected and the awkward content that AI offers us, challenging our inclination to compare what they generate to some type of human- created standard. Some of the most interesting and bizarre features that come with any generative AI model can inspire your artistic creation.\n\nAuthor: Define “uncanny valley” in a humorous skit between a clown and a doll.\n\nMuse: (Clown enters the stage, carrying a creepy doll) Clown: Hey, everyone, I've got a new friend here! (holds up doll) Audience: (muttering)",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Creepy…\n\nClown: What? (looks at doll) You think she's creepy? (to doll) You hear that? They think you're creepy! I think you’re a work of art. I think you’re my only real friend.\n\nDoll: (in a robotic voice) I may be creepy, but I am an advanced robot designed to mimic human behavior.\n\n© Patrick Parra Pennefather 2023\n\n271\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_10\n\nChapter 10 UnCanny by natUre\n\nClown: Oh, I see. (showing the doll’s face to audience) See, she’s smart and cute.\n\nAudience: (muttering) Still creepy…\n\nClown: (to doll) Sorry they still think you're creepy.\n\nDoll: I may not look like a real human, but I have human feelings.\n\n(she cries)\n\nClown: (to audience) You see? She has feelings!\n\nAudience: (muttering and aghast) Creeeeepy…\n\nClown: (to doll) I guess you'll just have to try harder to be less creepy!\n\nDoll: (in a robotic voice) I will strive to improve my human-like tears and make my behaviors palatable to sensitive humans.",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Clown: (to audience) There you have it, folks! The uncanny valley explained by a robot!\n\nAudience: (applause)\n\nClown: (to doll) Looks like we're a hit!\n\nDoll: (in a robotic voice) Indeed. Now let’s take over the world. (Clown and doll exit the stage)\n\nThe Not-Quite-Human-Not-Quite-Other\n\nThe concept of the uncanny valley was proposed by Japanese roboticist Masahiro Mori in the 1970s. He noticed that as robots and prosthetics became more human-like, people's emotional response toward them shifted from empathy to uneasiness and ultimately to repulsion. This observation led to the development of the uncanny valley as a hypothesis.\n\nThe phenomenon itself has been present throughout human history.\n\nHistorically, an unease, fascination, discomfort, creepiness, and irreconcilable emotions have manifested with exposure to religious icons, wax figures of famous individuals, ventriloquist dummies amplified by how they are portrayed in horror movies, antique dolls, and early automata designed to imitate human movement and behavior.\n\n272\n\nChapter 10 UnCanny by natUre\n\nMuse: In the realm of visual arts, some artists have intentionally explored the uncanny valley by creating hyper-realistic sculptures, paintings, and digital art that evoke unsettling feelings. An example is the work of Australian artist Ron Mueck, who is known for creating eerily lifelike sculptures of human beings.\n\nMueck’s background in puppetry is no surprise given the hyper-",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "realistic more-than-life-size humans he often presents in art galleries, including A Girl (2006) that depicted an enormous baby made from acrylic on polyester resin and fiberglass sprawled out on a white stand for attendees to walk around.\n\nAdvances in graphics and animation in the video game industry\n\nhave revealed a fascination with some games to create more hyperrealistic characters. Games like L.A. Noire (2011) and Heavy Rain (2010) were praised for their realistic character models, but also criticized for evoking feelings as a response to their human-like yet slightly off appearances. Characters like the G-Man in Half-Life and Littler Sisters in BioShock are now more easily customizable by creatives with Unreal Engine’s MetaHuman Animator. The tool will soon provide creatives of all disciplines with endless variations of diverse and uncanny humans within Unreal’s existing game engine.\n\nMuse: Researchers in robotics have long been aware of the uncanny valley and its implications for the design of humanoid robots. Companies like Hanson Robotics, the creators of Sophia the Robot, have strived to create robots that look and act more like humans to improve interactions with people. However, these robots can sometimes fall into the uncanny valley, causing discomfort to observers.\n\nWith deep learning techniques, AI-generated images attempt a realism that is usually off in some way or another with uncanny features like extra hands and limbs, missing body parts, contorted faces, and strangely placed retinas. Tools like NVIDIA's StyleGAN2 can generate strikingly 273\n\nChapter 10 UnCanny by natUre\n\nlifelike human faces, but sometimes the resulting images exhibit subtle imperfections that place them within the uncanny valley, making the faces unsettling to look at.\n\nWe can also observe the uncanny valley in the content that language models generate. As they become more sophisticated, they become increasingly capable of generating coherent and contextually accurate text. While models",
      "content_length": 2009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "generate text that is almost human-like, they can contain slight inconsistencies and feel ingenuine, following templates with automated accuracy, sacrificing personality with untruthful content, and thus causing unease, mistrust, and other negative responses when read by human observers.\n\nDisrupting Boundaries in the Arts\n\nBeyond the direct examples of the uncanny valley historically, there is also a connection with artists of all kinds that have tested the boundaries of discomfort with audiences. Artists have a history of pushing their creative expressions beyond the boundaries of the normal, normative, or familiar. The results of these expressions have historically not always been embraced by audiences. Art and artists have been spat upon, their work ridiculed and dismissed. Audiences have been cruel, and yet they might have thought the same of the artist. They’ve left art galleries and theaters, and some responses have even caused riots. Much of the art that we’ve been exposed to in the twentieth and twenty-first centuries has been met with a combination of apprehension, resistance, and celebration.\n\nIgor Stravinsky's The Rite of Spring, which debuted in 1913, was a composition that defied conventional expectations of what music should comprise. The auditory experience was intentionally challenging, beginning with a common Lithuanian folk melody that featured the bassoon performing at its highest and most dissonant register. The 1913\n\npremiere of The Rite of Spring at the Théâtre des Champs-Élysées in 274\n\nChapter 10 UnCanny by natUre\n\nParis provoked a near riot in the audience, with some members booing, shouting, and even physically fighting. The uproar was so intense that it was difficult to hear the music, and the conductor had to struggle to keep the orchestra playing. This defiance of musical norms contributed to Stravinsky's reputation as a disruptive and innovative composer.\n\nBertolt Brecht's concept of the “alienation effect” was a disruptive approach to theater-making. The technique involved intentionally distancing the",
      "content_length": 2067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "audience from the action on stage, preventing them from becoming emotionally attached to the characters. He aimed to create a sense of estrangement, prompting the audience to view the performance with a critical eye. This was achieved through various techniques, such as direct address to the audience, visible stage machinery, nonrealistic set design, and actors breaking character.\n\nUncanny AI\n\nFor creatives, generative AI will disrupt expectations by generating content that can be unsettling, imperfect, rough, unpredictable, and unexpected (Figure 10-1). AI have overwhelmed the world with their capacity to generate a lot of uncanny content quickly. AI seem to naturally guide us to the uncanny valley when they do their best to generate anatomically correct humans.\n\n275",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-1. So close to being human but not precisely so with additional noses that increase the creepy factor, demonstrating the uncanny valley. Iterations = 30\n\nThe results are close enough to have the qualities of a human, but the more generative AI attempt to simulate and not do so precisely, the uncannier these images become. Images, videos, or even speech can appear almost human-like, but with subtle differences that make them unsettling or eerie. Deep fakes even have uncanny effects. The effects are more pronounced when the generated content is intended to look or sound like a real human being. Prompting an AI to generate content will yield unexpected results,",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "bad ideas that may inspire better ideas, and incomplete combinations of pixels that propose new artistic directions.\n\nThese can take the form of images in addition to videos, deep fakes, and AI that simulate human speech.\n\n276\n\nChapter 10 UnCanny by natUre",
      "content_length": 256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Human-like robots will always provoke a human reaction because they are close enough to being human-like that they highlight the differences and imperfections in the imitation, which can be unsettling and even creepy (Figure 10-2). Additionally, the idea of machines that can mimic human behavior and physical appearance challenges our long-held beliefs about what it means to be human.\n\nFigure 10-2. After being fed through dozens of filters on a third-party AI, it’s hard to get rid of the uncanny when your source image happens to be a photo of a doll. Iterations = 30\n\n277\n\nChapter 10 UnCanny by natUre",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Some AI-generated content can evoke feelings of repulsion and\n\nresistance in some people, as they question the ethics and implications of machines creating non-human humans that resemble us but not closely enough (Figure 10-3). Other generative AI systems are getting so good it is difficult to tell the difference between a real human and one that the machine generates. The unpredictable and seemingly uncontrollable evolution of AI- generated content adds to the unease people may feel about any machine replicating a human, as if machines shouldn’t succeed at simulating humans or any of their attributes.\n\nFigure 10-3. A prompt “super-realistic human robot, 4K, unreal engine” accompanied by a very old open source photo of a\n\nmannequin. Iterations = 49\n\n278",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Chapter 10 UnCanny by natUre\n\nHistorical Precedence to the Uncanny\n\nSadly, the uncanny valley has been with us for centuries. It has ancient roots in automata with their eyes and human-like movement convincing some that they are alive. When these automata were showcased at European royal courts, they left audiences impressed and somewhat freaked out. That said, I’m sure the ventriloquist’s dummy was much more unnerving. That creepy creation has a long history of making humans feel uneasy, particularly in the hands of a master (Figure 10-4). The history of the ventriloquist dummy in movies begins as early as 1929 with The Great Gabbo, and it has a more modern-day influence with the movie Magic, starring Sir Anthony Hopkins as the ventriloquist. We know it is not real but just in case it’s important not to stand too close.\n\nFigure 10-4. 34 iterations of a public domain photo of a ventriloquist dummy regenerated by an AI with different filters fail to make it any less creepy\n\n279",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Chapter 10 UnCanny by natUre\n\nStore mannequins haven’t made humans feel any better about the\n\nuncanny (Figure 10-5), and every now and then a new horror movie presents some type of doll as villain. The latest offering touted as this generation’s creepy doll movie, M3GAN (2022), is the perfect combination of an artificially intelligent robot with a doll face who eventually goes off the rails",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "on a killing spree to “protect” the child she was bonded with. That trope resurfaces the belief that the AI will eliminate humans through some form of machine logic.\n\nFigure 10-5. Photo of a mannequin in a creepy van on the side of the road, which constantly creeps passersby out. Vancouver, 2023\n\n280\n\nChapter 10 UnCanny by natUre\n\nThe Imperfect Stochastic Parrot\n\nBy design any AI generator is imperfect, whether they try and replicate human hands or attempt to uncover scholars in a particular knowledge area that at times don’t even exist. That imperfection is what you can capitalize on in order to provoke new forms of expression. Imperfections may also contain within them a gem, a mistake that is a take-off point for your own creative journey. Think of whatever an AI generates as the first offer in a series of offers that you curate alongside your own creative process.\n\nThe Role of Adversarial AI\n\nMachine learning models form the backbone of all generative AI. There are a growing number of them, and with each learning model come different approaches to not only how the AI responds to your prompts but how the code talks to itself. If you’re wondering why the uncanny valley is a natural for text-image generative AI, it is precisely because you don’t know what you’re going to get when you prompt the AI for something resembling a human. Case in point, the captivating regeneration of a prompt first seen in Chapter 9 (Figure 9-11) that depicted the Mona Lisa while a detective assessed the forgery. Who knew it would result in a strange hand holding some type of sci-fi device, a version of the Mona Lisa hung onto the sky behind curtains, and a pensive Mona Lisa wondering if indeed the copy of her is a forgery (Figure 10-6)?\n\n281",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-6. Using a photo of the Mona Lisa by the author along with a prompt “a discriminating Mona Lisa questioning a painting of herself” visualizes a GAN. Iteration 39 of 265\n\nAuthor: Describe GANs again. Use a metaphor though.",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Muse: I’m really getting tired of repeating this information. A generative adversarial network (GAN) is a type of machine learning model that is designed to generate new and previously unseen examples. A metaphor to describe GANs is a game of counterfeiting money, where one player is the “generator” who creates fake money and the other player is the\n\n“discriminator” who tries to spot the fake money.\n\n282\n\nChapter 10 UnCanny by natUre\n\nThe generator is the counterfeiter, whose goal is to create fake money that is so good, it's hard for the discriminator to tell it's not real. The discriminator is the police officer, whose goal is to spot the fake money and make sure that no fake money enters circulation.\n\nDuring the game, the generator will create a fake money; the discriminator will inspect it and tell the generator if it's good or bad. Then, the generator will use the feedback to improve the next fake money, and the game goes on until the generator is able to create a very good fake money that the discriminator can't spot. Once the generator has learned how to make good fake money, it can be used to produce new fake money that is similar to real money, and the discriminator can't spot the difference.\n\nAs depicted in Figure 10-6, the generator and the discriminator are in a constant competition; the generator is trying to create something that is as close as possible to the real, while the discriminator is trying to spot the difference. This competition allows the generator to learn how to create new examples that are similar to the real examples, and that's what GANs do in general.\n\nGenerative Adversarial Art\n\nBesides GANs’ influence on generative NFTs, they are also informing the artistic process and work of contemporary visual artists. A look at the installation entitled “Figures” (2022) by artist Scott Eaton reveals the power of GANs. Eaton’s work consisted of a data set that contained over 30,000 photographs with many volunteers over a two-year period. Based on the",
      "content_length": 2004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "prompts Eaton created, the network scoured the data set looking for patterns in the body that it recognized. Creative shading and rendering were applied afterward. Eaton’s visual work is worth investigating beyond these pages and is best experienced as moving, ever-evolving liquid structures.\n\n283\n\nChapter 10 UnCanny by natUre\n\nAI is also being used to generate new types of artistic form by other artists such as Anna Ridler, Sofia Crespo, and Sougwen Chung who collaborates with an AI-powered robotic arm that paints with her on large canvasses in a type of choreographed dance.\n\nEven though GANs are not the only machine learning model out there, all involve training. They are all trained to generate new data based on patterns learned from the training data. These models can be based on GANs or other architectures, and they can be trained using unsupervised or supervised learning.\n\nExplorations also continue in the realm of music with GANs. Some research explores the conversion of paintings into music using conditional GANs, while creatives involved in programming and music continue to conduct research on generating short sequences that use the same principles of GAN. Researcher and composer Victor Sim used GANs to generate baroque music. The generated content was all based on midi files of Bach compositions.\n\nAny artist or researcher can tell you that programming a GAN is a complicated feat but one worthy of exploration, if you are settled with receiving the unexpected and unorthodox. After all, what GANs achieve successfully is the creation of unique content. The balance in programming a GAN is important to understand. At the core of a generative adversarial network (GAN) is the competitive dynamic between the generator and the discriminator. If the discriminator becomes overly proficient at identifying false creations, the generator is unable to improve its output. Conversely, if the discriminator's ability to spot fraudulent outputs diminishes, the generator can take advantage of the situation. This can result in content that",
      "content_length": 2062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "deceives the discriminator without accurately mimicking the actual data points.\n\n284\n\nChapter 10 UnCanny by natUre\n\nThe Unanticipated\n\nRegardless of the unsupervised learning model you interact with, when you do, you are sure to get stimulating results. These features are unique to generative AI, and yet they also are related to similar artistic processes of creation, particularly those that emerge from improvised music, dance, theater, and other interactive forms involving computational creativity.\n\nYou never know what you are going to get with an AI, which makes them risky and yet fun to use. The unexpected results are well aligned with part of the creative process that involves letting go of the hold we might have on how we evaluate our creative output.\n\nGANs are good at providing you with unexpected results from\n\nthe simplest of prompts. A good experiment to try with your creative prompting is to compare how simple and complex prompts are interpreted by two or more text-image machine learning models. Each can lead to interesting discoveries that can inspire you in what you want to create.\n\nThey also reveal how wildly different the content they generate might be.\n\nPrompting Single-Word Characters, Creatures,\n\nAnimals, or Cats\n\nAn experiment using the same prompts shows the difference in each generative AI’s approach to generating an image. Stable Diffusion seems to offer more compelling prototypes that can fuel your exploration (Figure 10- 8), whereas DALL-E 2 in this case offers you a more traditional compilation of photo- realistic cats (Figure 10-7). Midjourney takes it a step further and starts to generate images of cats that are much more interesting starting",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "points for continued exploration. Each creative will have their favorite, but remember the experiment is more to show that each generative AI has its constraints and affordances that can inspire different directions when used intentionally to inspire and provoke your prototyping process.\n\n285",
      "content_length": 293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-7. DALL-E 2 prompt using only the word “cat”\n\nFigure 10-8. Stable Diffusion prompt using only the word “cat”\n\n286",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-9. Midjourney prompt using only the word “cat”\n\nSimulating the Human in the Writing\n\nIn terms of LLMs, there is a common trait that can best be described as a sort of familiar homogeneity in the writing that is generated. Generative AI are simulating patterned structures and rhetorical styles, which generally",
      "content_length": 350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "demonstrate a mundane and boring way of creative expression. What the AI generate are more useful in knowing what not to write, especially 287\n\nChapter 10 UnCanny by natUre\n\nwhen they generate untruths. At the same time, generative AI offer us a way to structure our ideas in ways that we might not have considered before, particularly when guided to write in a specific writing style. One method to challenge your own approach to generating content is to prompt your generative AI with a stylistic reference and a block of text from a public domain author. An example that follows explores a scene in the book Alice’s Adventures in Wonderland written by Lewis Carroll in the style of Oscar Wilde.\n\nAuthor: Rewrite this excerpt from Alice in Wonderland as a short scene from the sitcom Taxi in the style of an Oscar Wilde play: There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat pocket and looked at it and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat pocket or a watch to take out of it, and burning with curiosity, she ran across the field after it and fortunately was just in time to see it pop down a large rabbit hole under the hedge.\n\nMuse:\n\nNT. TAXI DISPATCH – DAY\n\nLOUIE and JIM are engaged in conversation.\n\nLOUIE: (in a grand, flamboyant manner) Oh, my dear friend, you simply must hear of the most extraordinary occurrence I have recently been made privy to.\n\nJIM: (impressed) Pray do enlighten me, Louie.",
      "content_length": 1834,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "LOUIE: (dramatically) A rabbit, my good man, a rabbit dressed in the most dapper of waistcoats, running down the street with a timepiece clutched firmly in its paw.\n\nJIM: (surprised) A rabbit in a waistcoat?\n\n288\n\nChapter 10 UnCanny by natUre\n\nLOUIE: (nodding fervently) Indeed! And with a watch, I might add.\n\nSuch a display of punctuality is truly remarkable in these modern times.\n\nJIM: (bemused) I must admit, I have never heard of such a thing.\n\nLOUIE: (excitedly) And the driver, who happened upon this curious creature, followed it down a rabbit hole. Oh, the mystery! The intrigue!\n\nJIM: (laughing) Your obsession with this rabbit is quite amusing, Louie.\n\nLOUIE: (laughing along) Yes, well, I have a particular fondness for the unexpected and unconventional.\n\nJIM: (smiling) It is one of your endearing qualities.\n\nLOUIE: (grandly) I shall keep my eyes peeled, for who knows what other marvels this wondrous world may hold.\n\nJIM: (smiling) Indeed, Louie. Indeed.\n\nLOUIE places a hand on his chest, lost in thought, as JIM exits the dispatch, leaving Louie to his fanciful musings.\n\nWhile you should not use the characters from the sitcom Taxi or even the exact text in a scene, it does give you a compelling idea of a scene that integrates part of Chapter 1 from Alice in Wonderland involving two characters. This can inspire further explorations.",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Author: Repeat with Jim and Louie as gangsters.\n\nINT. TAXI DISPATCH – DAY\n\nLOUIE and JIM are both tough-looking gangsters. They're talking in a fast- paced, cockney accent.\n\nLOUIE: (excitedly) 'Ey, JIM, you ain't gonna believe the bird I just seen on the street.\n\nJIM: (interested) What bird, mate?\n\nLOUIE: (animated) A rabbit, dressed in a waistcoat, with a bleeding watch in 'is hand, checking the time like 'e's late for a job.\n\nJIM: (surprised) A rabbit in a waistcoat?\n\nLOUIE: (nodding) Yeah, and one of our drivers followed it down a bleeding rabbit hole.\n\nJIM: (impressed) That's mental.\n\n289\n\nChapter 10 UnCanny by natUre\n\nFor those who are familiar with the sitcom Taxi, both generated scenes will not feel quite right. Despite using natural language processing to understand the context of what it is being asked an LLM will not be able to capture the unique and spontaneous personalities of characters who were played by actors that also embodied spontaneous responses to each other based on the situation. In the hands of a writer, the scene might go somewhere, or a writer may throw it away and write it themselves. The experiment does show that the more we ask an AI to emulate human behaviors, the stranger the outcome might be.\n\nFrom Bad to Better Ideas",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "One need only prompt an image-based generative AI to reveal hands or the human body, and you get fewer or lesser digits of varying sizes as if badly remembered. This can immediately be judged as bad or a failure on the part of the machine learning model to get something right. Of course, if you reimagine the use of your AI muse as less of a perfection machine and more as an imperfection machine, then you can start to leverage generated content to inspire you to take a new artistic direction. Building from our previous simple prompt for generative AI to generate a cat, we can prompt a machine learning model to give us “cat in a pair of hands,” and the varied results may inspire or terrify you.\n\nIn our first variation of the prompt “cat in a pair of hands,” we can see a somewhat normal generation involving kittens being held in a pair of imperfect hands (Figure 10-10). Upon closer inspection the inability for the AI model to get it right is a gift of a bad idea, the fingers on the hands holding each kitten are more like cat digits. Where this leads is up to the artist to interpret and continue to iterate on.\n\n290",
      "content_length": 1128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-10. One variation of the prompt “cat in a pair of hands”\n\nIn Figure 10-11 we get even more uncanny generation, which, can inspire us to consider everything from gloves with cat images on them as a second prototype to a different type of hand puppet.\n\n291",
      "content_length": 294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-11. Another variation of the prompt “cat in a pair of hands” by an AI model that can potentially lead to better ideas that can be explored in a future prototype\n\nAs we can see, generative AI that rely on images for their content creation mashups can sometimes produce inaccurate and unexpected results, particularly when it comes to human or even cat anatomy. This is due to the limitations of the data sets that AI algorithms use to learn and generate content from. In many cases, these data sets may contain limited or inaccurate information about the physical appearance of a human, leading to the creation of images with distorted or missing body parts. The unpredictable nature of AI-generated content can be both a curse and a",
      "content_length": 772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "blessing. On the one hand, it can lead to unexpected and even bizarre results that can be difficult to interpret or put to practical use. On the other hand, it can also inspire unique and creative solutions, pushing 292\n\nChapter 10 UnCanny by natUre\n\ncreatives to think outside the box and explore new avenues of expression (Figure 10-12). Many AI offer negative prompts, which can help you eliminate some of the uncanny. An example can be if you wanted to use an image or photo that you created and feed an image-image generative AI in order to recreate your subject holding an umbrella. You might add “extra",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "fingers” into your negative prompt area and see what happens. As has been repeated in earlier chapters of this book, the goal of using AI to prototype content should be to leverage its strengths and work around its limitations, not to curate it to get exactly what you want from it (Figure 10-13).\n\nFigure 10-12. Even Midjourney with the prompt “cat in a pair of hands” creates a disturbing mix of human and cat. It would make for a great pencil sketch though\n\n293",
      "content_length": 464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "Chapter 10 UnCanny by natUre\n\nFigure 10-13. A final attempt to generate “a cat in a pair of hands”\n\nusing negative prompts like “extra fingers, extra paws” and even\n\n“extra kittens” failed but created a fantastic opportunity to share a seemingly normal photo-realistic scene and ask, “How many digits do you see?”\n\n294\n\nChapter 10 UnCanny by natUre\n\nCan Bad Ideas Be Turned Around?\n\nA generative AI might tell us that people are attracted to deep fakes because they offer a unique form of entertainment and novelty. The idea of being able to see a familiar face in a completely different context or role is intriguing and can spark people's imagination. Additionally, deep fakes offer a way to experiment with what is possible in the digital world, pushing the boundaries of what was once thought to be a written-in-stone image or video. They offer a comparison of what the original scene was with how it might be interpreted by a completely different actor. What may be a creative reversal of how the technology is used in the mindset of a creator who is prototyping could be to understand how a scene might play out with different types of actors. That may also help break stereotypes and type- casted roles, which still dominate the film and TV industry in terms of race, gender, actor size, and age.\n\nThe technology behind deep fakes is constantly improving, making the results more realistic, convincing, and potentially dangerous. Despite not being precisely real, the combination of technology and people's imagination makes deep fakes alluring to some. Of course, the lack of permission to use someone’s face in a variety of questionable contexts and without compensation is unethical, and the abuse of this technology has forced one well-known generative AI platform to no longer make this feature available to users. More of this type of ethical safeguarding for the power of generative AI is important and imminent. As we are already witnessing,",
      "content_length": 1956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "actors, writers and other creatives are pushing back on deep fake, generated images and LLM technologies that use their content, words or images without consent or compensation. Protests, strikes and letters to policy makers will hopefully lead to increased collaboration between producers, content creators, and all those artists who have unwillingly contributed to machine learning data sets.\n\n295\n\nChapter 10 UnCanny by natUre\n\nTakeaways for Creatives\n\nTransform your instinct to only generate recognizable\n\nforms or patterns with generative AI and surrender\n\nto the creative opportunities that come with the\n\nuncanny valley.\n\nCreativity is not just about accurately reproducing\n\nreality. Your creative companion can also help\n\nchallenge conventions and present fresh perspectives.\n\nImages with odd features can be intriguing, provoke\n\nthought, and evoke strong emotional responses.\n\nSuch images allow creatives to make a statement or\n\ncomment on certain issues, whether societal, cultural,\n\nor philosophical.\n\nDiversify whom you represent in your work. Traditional\n\nmedia often favor certain body types and features.",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "AI, especially if trained on a diverse data set, can help\n\ncreate images of bodies that fall outside the “norm,” like\n\nthose of people with disabilities, people of all sizes and\n\nshapes, and individuals from diverse racial and ethnic\n\nbackgrounds.\n\nDeformed or distorted images and strangely\n\ngenerated scripts and stories can be used effectively in\n\nstorytelling, to symbolize certain themes or character\n\ntraits, or to evoke specific atmospheres. A character\n\nwith one too many fingers might have a role in a story\n\nyou are telling.\n\n296\n\nChapter 10 UnCanny by natUre\n\nIn a world saturated with images, uncanny or\n\ndistorted images can stand out, capture attention,\n\nand make a lasting impression. This can be beneficial\n\nin fields like advertising or brand identity where\n\ndifferentiation is key.\n\nAI can also generate images that blend human bodies\n\nwith technological elements, exploring concepts like",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "cyborgs or transhumanism. These can challenge our\n\nideas about the boundaries of the human body.\n\nAcknowledgments\n\nTo the uncanny habit of trying to make machines that\n\nrepresent humans\n\nTo ventriloquists and their dummies and to cat gloves\n\nand cat hands\n\nTo GAN artists Scott Eaton, Anna Ridler, Sofia Crespo,\n\nand Sougwen Chung for exploring AI’s potential as a\n\ncreative companion\n\nTo the sitcom Taxi for many youthful hours spent\n\nlaughing\n\n297\n\nCHAPTER 11\n\nDilemmas Interacting\n\nwith Generative AI\n\nThis chapter addresses some of the current dilemmas that generative AI is facing and how the use of machine learning models surfaces issues that need critical attention by both users and generative AI developers.\n\nThese include ethics, bias, copyright infringement, unfairness, untruths, cheating, and factuality.\n\nBringing Existing Dilemmas",
      "content_length": 846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "to the Surface (Again)\n\nGenerative AI bring to the surface human dilemmas we must contend with that are already pervasive across different technologies and media. The dilemma is apparent when we “look under the hood” of a generative AI and pick apart how it works, who makes it work, and the type of content it generates all while keeping an eye out for what that reveals. When we look at LLMs, there are definite concerns with how good an LLM has become at fooling people into believing something was made by a human. There are witness reports on how an LLM fooled the Wharton Business School and passed an MBA exam (resource list at the end of the chapter).\n\n© Patrick Parra Pennefather 2023\n\n299\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_11\n\nChapter 11 Dilemmas interaCting with generative ai But what does that tell us of the MBA exam? Recall that generative AI look for patterns when assembling content to generate for us. Should we question the way that some exams are constructed and revise them?\n\nShould we interrogate how it is we assess the knowledge that students gain? Do we need to look at what it is that we teach and how? The answer to these questions might be a simple “yes”; however, that would implicate a change in how an institution evaluates how people learn and the type of knowledge and know-how that communities of practice are expecting when students graduate into the workforce. The Wharton case is not that unique. My colleague Dr. Claudia Krebs (featured in Chapter 12) has vetted ChatGPT-3 on typical first year undergraduate anatomy exams, and the results were not just a pass, but the median was a B+.\n\nHave we solved copyright infringement internationally? The answer is no, and it depends on who is being infringed upon and the weight of their legal team and their financial and social capacity to launch a lawsuit with",
      "content_length": 1934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "whatever person or organization in whatever country has infringed on the copyright and how much profit they are making from someone else’s work. Fair use and the radical transformation of pre-existing content from whatever happens to be in an AI’s corpus will always play into any lawsuit.\n\nIf that wasn’t tricky enough, some AI-generated content is itself now copyrightable. What a peculiar world we occupy where derivative works can also be protected.\n\nThe exclusion of voices in content generation, the virtual extinguishing of individuals in history, and the hallucinatory false references that an LLM\n\ngenerates all have precedence before the advent of generative AI. LLMs that do not fully account for a diversity of voices are a familiar complaint.\n\nSadly, this is not a problem that has solely emerged with AI. The problem of excluding voices continues to regenerate itself with whatever technological medium attempts to convey thought, opinion, perspective, and history. The list of dilemmas with AI is long, and this chapter could be an entire book. Saying that, it is helpful to frame this chapter in a way that keeps the ethical dilemmas that AI brings to the surface so we have a more 300\n\nChapter 11 Dilemmas interaCting with generative ai informed choice as to whether or not we choose to interact with them. It’s also important to understand that policy makers are working hard to place constraints on what these unscrupulous AI can generate. Finally, instead of just highlighting the dilemmas, this chapter also highlights existing solutions and proposes some as well.\n\nPersistent Myths About AI\n\nWhat may benefit all of us is to understand that generative AI are systems built by people. AI is not co-constructed by several intelligent machines.\n\nThey all require humans in the equation. Those people have control over every aspect of the AI, including its distribution. They have clear choices when it comes to whom they hire to collect, classify, and label their data sets and how those workers are treated. To act from informed decisions when it",
      "content_length": 2066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "comes to using generative AI, it’s crucial to understand the myths that accompany the technology (Figure 11-1).\n\n301\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-1. Perpetuating myths about self-regulating and self-replicating AI, the prompt “monkey AI bots take over the world” is accompanied with a photo of a monkey. Iterations = 40",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "Author: Generate incorrect information about artificial intelligence.\n\nMuse: Artificial intelligence is a form of technology that allows machines to become self-aware and take over the world. It is also known to be able to read human thoughts and emotions. AI robots have the ability to reproduce and can also teleport. They are also immune to all forms of physical harm, including bullets and fire. AI is also able to control the weather and can cause natural disasters at will.\n\n302\n\nChapter 11 Dilemmas interaCting with generative ai The interaction with my LLM muse is meant to illustrate conflicting information about generative AI and their machine learning models. This includes the public misperception of how AI works, its benefits, and its harms. These tend not to be elaborated in media representations of AI that would allow humans to have enough information to base their opinions on.\n\nHere are a few notes about the preceding interaction that fuels this chapter:\n\nThe generated AI response was based on a data set\n\nused for that particular machine learning model.\n\nThat data set along with the way in which the AI\n\nresponded was programmed by humans.\n\nThe words from the data set were scraped and mashed\n\ntogether by other humans who created the original\n\ntexts. It’s not like the AI just draws from a dictionary and\n\ncreates fully fleshed-out sentences. It looks for patterns.\n\nThe patterns the AI looks for are informed by the\n\nprompt “incorrect information about AI.” People have",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "written about that. References for the actual humans\n\nwho have written related content come up in a quick\n\nsearch engine with the same prompt text: “generate\n\nincorrect information about artificial intelligence.”\n\nIn a search engine, however, we might have better luck conducting a search with the prompt “myths about AI.” You will quickly understand that the quality of information you receive about the subject of myths about AI is directly influenced by the search terms you undertake. Of course, a web search and a content scrape from a machine learning model have differences.\n\nAuthor: What are differences between prompting generative AI vs.\n\nconducting a search in a search engine?\n\nMuse: Prompting a generative AI typically involves providing input to a machine learning model, which then generates output based on that input.\n\nThe output can be anything from text to images to audio. This approach 303\n\nChapter 11 Dilemmas interaCting with generative ai is useful for generating new, unique content or for creating content that is similar to existing content. Conducting a search in a search engine, on the other hand, is a way of finding information that already exists on the Internet. When you enter a query into a search engine, it searches its database of web pages and returns relevant results based on the keywords in your query. This approach is useful for finding information that has already been created and published.\n\nAuthor: Yet the information that an AI generates is based on existing content, no?\n\nMuse: Yes, that’s correct. The information generated by a generative AI is based on existing content, but the AI uses that content in a creative way to generate new and unique outputs. A generative AI is trained on a large corpus of data, which can include text, images, audio, or other types of content. The AI then uses that training data to generate new outputs that are",
      "content_length": 1897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "similar in style, tone, or content to the training data. While the outputs generated by a generative AI are new and unique, they are still influenced by the existing content that the AI was trained on. Therefore, the quality of the output depends heavily on the quality and diversity of the training data. If the training data is biased or limited in scope, the outputs generated by the AI will be biased and limited as well.\n\nPutting aside the semantic differences in the approach to generating information, both methods require an existing corpus of information to draw from. Thus, a dilemma to address is the capacity for AI to copy previously existing letters or pixels that someone else created, to generate what are essentially mashups. Whether those come from pixels, colors, shapes, spectrograms of audio files, video clips, 3D models, etc., the process is similar. You type in a prompt. The AI scrapes for patterns. It makes an offer.\n\n304",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nTakeaways\n\nConduct your own research about myth vs. reality\n\nwhen it comes to engaging with generative AI.\n\nDeepen your understanding of how generative AI",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "works and then pass judgment.\n\nCreate art that is critical about the human reliance on\n\ngenerative AI and AI in general (Figure 11-2).\n\nFigure 11-2. Low-poly render with a pixelated filter of a low- resolution and blurry photographic capture of three dancers on a mocap shoot with the prompt “mocap dancers in apocalyptic times.” Total iterations = 18\n\n305\n\nChapter 11 Dilemmas interaCting with generative ai Bad Bots, Fuzzy Pixels, and\n\nIterative Forgery\n\nThe issues of AI and copyright may be complex and multifaceted but all too familiar. We are often told that AI has the potential to generate new forms of creative expression and can be used as a tool that enables authors and artists to inspire new works. On the other hand, the use of AI can raise questions about the ownership and control of the data sets that inform these new derivative works. This should be familiar if you have ever consumed a mashup. Mashups have long ago become so accepted that they are now a normative part of human expression. Mashups are also made by humans who interact with different technologies to produce and then publish them. We are constantly faced with making decisions as to whether we opt in or opt out with any technology. The following statements are not meant to help make the decision easier:\n\nEvery single image that is generated on any text-image\n\ngenerative AI comes from somewhere, whether a single\n\npixel or a bull’s horns.\n\nMost generative AI content is a mashup, merging\n\nimages, patterning stylistic renders, and creating a",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "derivative work.\n\nThe problem is we don’t really know which images are sourced from where and how that informs the generated content. At least we knew that when it came to some of the amazing mashups or machinima that creators in the earlier part of the twenty-first century produced. This is because the responsible humans in the mashup suits would give credit where it was due. It is not just images or video or sound. It’s also text. LLMs do not provide references for every word they scrounge and scrape together from all their tables of coordinated data points.\n\n306\n\nChapter 11 Dilemmas interaCting with generative ai There are mounting pressures, lawsuits, and activism to challenge access, ownership, and author permissions to the very corpus on which machine learning models rely. Sites like Spawning AI’s haveibeentrained.\n\ncom and stableattribution.com offer creatives the opportunity to see if their work is being used to train neural networks in image-generated platforms like Stable Diffusion and others.\n\nFigure 11-3 (overleaf) shows a Stable Diffusion–generated image on the right uploaded to stableattribution.com to generate images that might have contributed to that uploaded image.\n\n307",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-3. A composite-generated image on the right that underwent 173 iterations across seven AI. Images on the left show possible source images that contributed to the image on the right 308",
      "content_length": 246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nWhile Stable Attribution’s website claims to only work with Stable Diffusion–generated images, as an experiment you can try uploading a photo of your own vs. one that you generated and see the results. Stable Attribution’s machine learning model also consists of a corpus of images, and anything you upload is checked against it. While it explicitly states that it can only cross-check images that were generated using Stable Diffusion, it is interesting to note that even the photo on the right of Figure 11-4, which was taken by the author, is still suspect of “coming from somewhere.”\n\nFigure 11-4. An original photo of a flower taken by the author on the right and then tested with Stable Attribution. The flowers on the left show the images that apparently went into its generation\n\nThe same experiment can be tried with haveibeentrained, with similar results (Figure 11-5).\n\n309",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "Chapter 11 Dilemmas interaCting with generative ai",
      "content_length": 50,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "Figure 11-5. A photo of a goose taken by the author as a thumbnail, top left, and then tested with haveibeentrained.com, which generated apparent source images that went into its creation\n\n310\n\nChapter 11 Dilemmas interaCting with generative ai What these experiments point to is the need for generated images and their machine learning models to provide attribution. If you ask why that is not possible, the answers tend to be prepared answers that point to generated content as a derivative work. These answers are not only unacceptable but completely unforgivable given how easy it would be to compile metadata, labeler information, and classification—the very processes that an AI relies on to generate content in the first place.\n\nThe same can be attempted with text content generated through an LLM. Most of the chatter on social channels and in media about ChatGPT is cautionary when it comes to how students will use it in high school, college, or university courses. What is not as commonly known is that the same company that developed ChatGPT has also released a tool called AI Text Classifier that can assess where the text you copy and paste into it comes from. The text in the following box (Figure 11-6) was copy-pasted from this book. Content combined original writing with ChatGPT-3-generated text.\n\nResults that the text is “very unlikely AI-generated” show the difficulty in identifying a body of writing that is mashed together with an AI.\n\n311",
      "content_length": 1464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-6. Screenshot of OpenAI’s Text Classifier accurately identifying source text as very unlikely AI-generated\n\nAs generative AI advances, it will also entail the emergence of associations that will establish worldwide benchmarks for content recognition. In line with this, Adobe has established the Content Authenticity Initiative (CAI) to facilitate this goal, offering costless open source resources developed by the nonprofit Coalition for Content Provenance and Authenticity (C2PA). With the aid of metadata, more images will soon be accompanied by content",
      "content_length": 619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "accreditation, which will include “Do Not Train” tags, indicating that the content must not be 312\n\nChapter 11 Dilemmas interaCting with generative ai\n\nutilized for training machine learning models. This will be a bit more difficult to do with scraped text content, but at least OpenAI in their own hallucinatory impulse has started the difficult task of differentiating between machine-made and human-made content. Systems to check on generated content will increase, and some will no longer be operational.\n\nThis is the nature of technology development. Anticipate that more companies will offer generative AI, new business models will develop, open source will be used by those wanting to profit, controversial start-ups will probably change hands and be rebranded, and well-intentioned projects may also no longer be sustainable to operate (Figure 11-7).\n\nFigure 11-7. A screenshot of the authors of Stable Attribution taking their much deserved rest and bow\n\nTakeaways\n\nDo your research and, as much as you are able, find out\n\nif there is anything similar to what you generate.\n\nPaint over, refine, adapt, mash, convert, radically\n\nalter, and make significant changes to whatever an AI",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "generates. It’s a prototype.\n\n313\n\nChapter 11 Dilemmas interaCting with generative ai The Hallucinating Dev\n\nAn AI’s confident response, which doesn’t align with its training data, is described as a form of hallucination. This state of “hallucination” can occur due to various factors such as the training data being insufficient, biased, or excessively specialized. While the word “hallucination” has been used, the real hallucination is the one that AI developers might have in thinking that there are no ethical issues to reconcile when interacting with generative AI. The current trend seems to be a sort of doomsday misdirection by AI developers who claim how powerful their technology is and that safeguards need to be put into place in order to mitigate the risk of extinction. For the discerning, the misdirection seems to be to shift our focus from the ethical dilemmas noted in this chapter and the inherent weaknesses that these systems have. Anyone who has interacted with ChatGPT or other generative AI understands their imperfections.\n\nAuthor: Say this using a more technical language: Know that your AI is not perfect and may in fact provide you with certain knowledge or data that in fact is wrong, incorrect, or useless. But when you see its advantages as a creative companion, it has the potential to support your own creative process and in many cases accelerate it.\n\nMuse: While I have been trained on a large corpus of text, it is possible that my output may contain errors or inaccuracies. Additionally, the context and intent behind the language used in a given input may not always be fully captured by my understanding of the text.\n\nCurrent laws on copyright were not designed with AI in mind, and it can be challenging to determine who holds the rights to AI-generated works. This has led to hilarious and often misguided debates about whether AI should be considered an author and whether the rights to AI- generated works should be owned by the creators of the AI, the users who employ it, or van Gogh (Figure 11-8).",
      "content_length": 2044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "314\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-8. Six of dozens of versions of van Gogh by van Gogh according to an AI fed with a self-portrait of van Gogh\n\nFor established creatives who have contributed their own original work in whatever form and shared it online, however, the distinction is clear.",
      "content_length": 321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "What is obvious is that derivative works are being created from original ones (Figure 11-9). They are essentially mashups relying on what was created in the past to create new forms. The fuzzy and at times not-so-fuzzy line of transforming the original art into a new form is where many lawsuits are emerging. There is no one answer to all use cases, as each is unique.\n\n315\n\nChapter 11 Dilemmas interaCting with generative ai\n\nContext of use will also inform the results of a lawsuit. The more the work is transformed from the original and used in a separate context than the original",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "artist intended, the more difficult it is becoming to be able to sue.\n\nFigure 11-9. The prompt “AI djinn sealed up in a bottle for all to see and prepare wishes for” accompanies an 1890 untitled photo of a specimen by Thomas Smillie courtesy of the Smithsonian Institution.\n\nIterations = 68\n\nAmbiguous use cases highlight the need for a clear legal framework to govern the use of AI across creative industries to ensure that the performance and licensing rights of creators and users are protected. It’s easy to blame it on the bot, but if you know your bot is being a copycat, then why let it continue? If you provide users with the naughty bot, then what do you really expect to happen?\n\n316\n\nChapter 11 Dilemmas interaCting with generative ai In addition to questions of authorship and ownership, the use of AI in creative industries also raises concerns about the source of the data sets that AI algorithms use to generate new works. Many AI algorithms rely on large data sets to train and improve their performance. These data sets often contain existing works that are protected by copyright, and the use of these works in AI algorithms can raise questions about the legality of such use. It is important to consider the source of the data sets used by AI algorithms and ensure that the rights of the original authors and creators are respected.\n\nThis includes obtaining proper licenses and permissions for the use of these works and giving proper credit to the original authors and creators.\n\nOwnership can be resolved if creators use generative AI as a starting place to provide momentum to their own creativity much like a game designer scours Google images and prints up inspiration images for a mood board.\n\n“Machine learning models generate prototypes and not finished works”\n\nis the repetitive refrain of this book.\n\nTakeaways",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "The only hallucination is the one that AI developers\n\nmight have that there is nothing to reconcile when\n\ninteracting with generative AI. Developers as creatives\n\nneed to take responsibility for their creations and not\n\nsimply generate a disclaimer for humans who can\n\neasily access it and use it as they wish.\n\nUse any present or future LLM with a great deal of\n\ndiscernment and accept that what it generates needs to\n\nbe cross-checked and rewritten in your own style.\n\n317\n\nChapter 11 Dilemmas interaCting with generative ai The Stochastic (Sarcastic) Parrot\n\nLLMs are prone to getting it wrong or more specifically to having a limited data set from which to draw with the result of not having all the data they should to generate factual responses.\n\nAI is also referred to as a “stochastic parrot” because it is often viewed as simply repeating patterns it has learned from its training data, without any real understanding or creativity. Just like a parrot repeating words it has heard without comprehending their meaning, AI models can generate output based on patterns they have seen in their training data, without truly understanding the context or deeper implications of the information (Figure 11-10). This lack of understanding and creative thinking can lead to AI models making mistakes, generating biased output, or simply repeating information without adding any real value or new insights.\n\n318",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-10. The prompt “a bunch of ML models hanging out as parrots eagerly awaiting their next prompt” accompanies a public domain black- and-white photo of parrots in the Brazilian rainforest.\n\nIterations = 89\n\nThe “stochastic” aspect refers to the randomness involved in many AI models, particularly those based on probabilistic or deep learning algorithms, which can generate different outputs even with the same inputs, further emphasizing the lack of control and understanding of the results produced by AI.\n\nLLMs may also get it wrong when generating responses due to their lack of understanding of the context and nuances of human language. This can lead",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "to the generation of responses that are incorrect or inappropriate, causing confusion or harm. For instance, LLMs may generate\n\nresponses that are racist, sexist, or offensive, which can lead to negative consequences.\n\n319\n\nChapter 11 Dilemmas interaCting with generative ai While LLMs have the potential to alter the way we interact with computers, it is important to be aware of their limitations and those of the data sets they rely on. By ensuring that LLMs are trained on diverse and inclusive data sets and by continuously monitoring and improving their output, we can help mitigate the risk of generating incorrect, biased, or inappropriate responses.\n\nAI requires the user-tester. In that informed interaction, there comes the refinement of the patterned responses the AI gravitates to. Interacting with an AI builds on the idea of using it as a point of departure, not as a finished product. Each use involves research as to who might have created a similar idea and an understanding that the AI just might be providing you with beautifully written untruths.\n\nTakeaways\n\nConsider investing time and computing resources to\n\nset up your own generative AI on your own local server\n\nusing the many open source machine learning models\n\nthat are being offered for free.\n\nExclusion of Voices and Gender Polarization\n\nIt is well known that many machine learning models are trained on data sets that lack diversity. The result is that a machine learning model may generate images that either reinforce harmful stereotypes or exhibit biases or that it excludes images or voices that depict other cultures. This can lead to a lack",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "of trust in AI-generated content and a general aversion to it no matter what book discusses its benefits.\n\nA lack of diverse and inclusive data sets usually leads to excluding voices that may not be represented as majority voices or whose words may not necessarily dominate the corpus of data that an LLM scrapes 320\n\nChapter 11 Dilemmas interaCting with generative ai from. Content generated by some machine learning language models can therefore become uniform and standardized, leading to normative responses that may not adequately represent the diverse voices that represent a wider spectrum of human experience. Marginalized groups such as indigenous communities and the LGBTQ+ community may be\n\nexcluded from these normative generated responses in LLMs and GANs.\n\nYour generative AI may be stuck in a gender binary mode, unable to get out no matter what prompt you give it. To make sure machine learning algorithms are fair and accurate, code and how data is classified need to be programmed to present different people’s opinions. This means that for the machine learning models to improve, a process of development could be followed that looks at what everyone says, instead of just what most people say. Typically, when practitioners want to label examples for machine learning, they hire a bunch of people to do it. Then they look at all the labels and choose the one that most people agreed on as the “correct”\n\none. The machine learning algorithm learns from this label and tries to make predictions based on that majority opinion.\n\nRegenerating the machine learning model itself in shorter cycles and with a diverse range of human input into its construction will help an AI make better predictions and either avoid unfairness, present multiple voices in a single generated response, acknowledge the limitations of a generated response, or a combination of all three (Figure 11-11).\n\n321",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-11. First generated model of a cyborg without prompting race, color, gender, or size and then fed into an image-image AI. Iterations = 2\n\nSimilarly, the LGBTQ+ community may use language that is not",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "recognized by mainstream language models, leading to exclusion and erasure of their experiences and identities. Content generated from all machine learning models (Figure 11-12). Therefore requires a critical perspective and discernment. It is important to identify the biases and 322\n\nChapter 11 Dilemmas interaCting with generative ai\n\nlimitations of language models when generating content and to actively support the work of others toward creating more inclusive and diverse",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "language models that can accurately represent a wide range of voices and perspectives.\n\nFigure 11-12. A collage of women representing diverse racial backgrounds and mixes as fashion icons that took approximately 250\n\nregenerations. None presented a variety of female forms and sizes no matter the prompts\n\n323\n\nChapter 11 Dilemmas interaCting with generative ai The Machine Is Hallucinating\n\nOne of the challenges with generative AI is the potential for your muse to get caught up in its own hallucinations. They exist alright and they can be shocking. Hallucinations are essentially false or distorted patterns or features generated by the AI that do not exist. Hallucinations can arise due to various factors such as imperfect training data, biases in the algorithm, majoritarian opinion, unbalanced data sets, non-inclusive classifiers, bad labeling, inaccurate metadata, or limitations in the AI’s ability to accurately understand and represent complex concepts.\n\nViewed from a negative lens, it’s easy to dismiss an LLM simply because it distorts the truth. The expectation that it should always tell the truth is in itself a hallucination. While those so-called hallucinations can pose a challenge for certain applications of generative AI, they can also be leveraged as a creative opportunity for creators in the realm of fiction and fantasy. By embracing and incorporating the unexpected or surreal elements that arise from these invented realities, creators can potentially unlock new forms of storytelling and imaginative creations that push the boundaries of traditional narratives and conventions. Assignments for students can be created that explore a hallucination and are then followed with a critical reflection, research, and fact-checking.\n\nFor example, a generative AI system that is trained to generate images of animals may produce hallucinations of fantastical creatures that do not exist in the natural world. These hallucinations could then serve as inspiration for",
      "content_length": 1989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "a new type of fictional creature, expanding the possibilities for creative world-building and storytelling.\n\nSimilarly, so-called hallucinations generated by text-image AI could inspire new forms of visual storytelling and illustration, leading to the creation of unique and imaginative works of art that blur the lines between reality and fantasy—or, in the case of Figure 11-13, a fake motion capture shoot you should likely never ever try to replicate.\n\n324",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-13. The result of a photo of a mocap session with actors and the prompt “a bunch of pigs in motion capture suits” seems plausible until you visualize it\n\nNSFW and Deep Fakes",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "The issue of AI generating NSFW (Not Safe For Work) images and deep fakes is an ongoing concern and always keeps us top of mind that any technology can be used destructively as much as it can be creatively. With 325\n\nChapter 11 Dilemmas interaCting with generative ai inevitable advancements in generative AI and its deep learning algorithms, AI systems will continue to generate images and videos that become increasingly indistinguishable from reality. The worry is real: that deep fakes can be and are being used to spread false information, manipulate people, and create pornographic images and videos of popular actors who never gave consent, causing harm to individuals and societies as a whole. Recent deep fakes used in unethical ways have provoked company Midjourney to stop access to their current beta. Companies who offered some limited AI-generated services for free are now only providing access to paid subscribers. Deep fakes are not only unethical but expensive to render. The use of deep fakes in film and tv also need to be regulated so that actors are not taken advantage of, are properly compensated and become collaborators in how their images are used to tell new stories.\n\nNSFW image generation is also a concern as it can perpetuate\n\nharmful stereotypes and perpetuate the exploitation and objectification of individuals. The images created can also be used to harass or blackmail individuals or spread malicious content that is not appropriate for general audiences.\n\nFurthermore, the question of accountability arises when AI systems are used to create these NSFW images and deep fakes. While the AI system itself is not the creator, those who provide access to the technology need to take some responsibility for the tools they provide users. A lack of accountability creates an urgent need for regulating and enforcing laws that protect individuals from harm and abuse.\n\nAs the destructive and harmful uses of AI generating NSFW images and deep fakes have become a reality, it is essential that any further development of AI be guided by a strong ethical framework that protects the rights and dignity of individuals and ensures that the technology is used responsibly",
      "content_length": 2197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "and to enhance human creativity. Those ethical frameworks should advocate to resist the temptation to call “free speech”\n\ninto any generative content that represents another person in any way without their explicit permission.\n\n326\n\nChapter 11 Dilemmas interaCting with generative ai\n\nFrankenAI\n\nThe fear of AI and its potential negative impact on society has been present since its inception. One of the earliest depictions of the dangers of humans creating a conscious creation can be traced back to Mary Shelley’s novel Frankenstein, or The Modern Prometheus, which was published in 1818. The novel explores the idea of creating life through science and the consequences of playing God. In the twentieth century, science fiction continued to shape the public perception of AI with stories such as The",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "Terminator and The Matrix where artificial intelligence turns against humanity when it becomes sentient (Figure 11-14).\n\nFigure 11-14. AI hard at work in Frankenstein’s lab preparing for world domination if it could only get its head on straight\n\n327\n\nChapter 11 Dilemmas interaCting with generative ai Ongoing advancements in AI technology have raised concerns about job displacement and the ethical implications of creating systems that may eventually surpass human intelligence. These fears have been fueled by reports of AI systems making decisions that are biased or harmful, leading to the conclusion that AI must be approached with caution. One only need look at headlines where humans engage in debates about whether or not AI is sentient or conscious. Individuals considered to be seminal leaders of AI are also sending out warnings for humans to cease development of AI for a variety of reasons.\n\nThe persistence of deep fake technology has raised the alarm even more and only increased public mistrust of any tool that in any capacity uses AI. The ability to generate realistic images and videos that manipulate our perception of reality has raised concerns about the potential for generative AI content to be used for malicious purposes, such as spreading false information and being used in a variety of complex phishing scams.\n\nThe association of machine intelligence being harmful to humans continues to influence our mistrust, particularly when statements that current systems surpass human intelligence are propagated, reinforced by the quality of the content that generative AI produces.\n\nJob (Re)placement\n\nThe fear that AI will replace jobs has been a concern for many people for decades. As AI technology continues to improve and become more sophisticated, this fear has only grown. Some experts predict that AI will eventually be capable of performing a wide range of tasks currently performed by humans, from manual labor to complex decision-making.",
      "content_length": 1972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "This has led many to worry that AI will result in widespread job losses and unemployment.\n\nA common assumption that many people make is that generative AI will also replace all humans and all jobs. AI won’t replace jobs. Humans who 328\n\nChapter 11 Dilemmas interaCting with generative ai jump the gun and misunderstand the limitations and costs of generative AI will lay people off. There are already documented cases of employers replacing humans with generative AI. There are also stories of those same employers having to rehire humans because generative AI could not perform all the tasks that their employees could. We can clearly see that current AI content generation is imperfect, homogenized, and at times untrue and requires editing, a critical eye, and an increase in research of any knowledge that it generates, tweaking, regeneration, more tweaking, etc.\n\nDespite the myth that over time it will get better, replacing human creativity, ingenuity, and in-the-moment improvisation is not advised. There may be a degree to which AI is integrated within some jobs that creatives have, and as this book has offered, AI can be a useful tool in many creative situations.\n\nTo illustrate the point that generative AI will not replace a creative job, I present the following use case. A professional photographer came to the studio where I teach and was worried that they wouldn’t have a job in a few years. During the photoshoot, however, the photographer carried out the following activities:\n\nThey made a social connection with the subjects they\n\nwere going to shoot and repeatedly used their names\n\nthroughout the session.\n\nThey quickly assessed a potential composition based\n\non the body types and skin color of each person\n\npresent.",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "They evaluated the lighting and used their own pro kit.\n\nThey used a high-quality camera that resulted in high-\n\nresolution photos.\n\nThey made persistent adjustments to their camera\n\nincluding ISO, lens type, aperture, shutter speed, etc.\n\n329\n\nChapter 11 Dilemmas interaCting with generative ai\n\nThey improvised in the moment based on how the\n\nsubjects responded and what the photographer\n\nimagined through the lens.\n\nThey took a shot, examined it, and then without\n\nasking anyone took another shot after fine-tuning\n\nadjustments.\n\nThey turned the lights out in the space and used their\n\nown lighting.\n\nThey used light from the two LCD screens that were\n\nalso in each of the shots.\n\nThey moved around snapping the subjects from all\n\nkinds of angles.\n\nThey repeatedly asked the subjects to move according",
      "content_length": 804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "to the composition the photographer envisioned.\n\nThey asked the subjects to have a conversation that\n\nwould be typical to one they had when they worked\n\ntogether on a project.\n\nThey stood on a chair and then leaned over on one leg\n\nto reach out and get that winning shot.\n\nThe list is incomplete as there were many more actions the\n\nphotographer took during the 40 minutes they were there. No single AI system could achieve the same results in the same amount of time or budget (Figure 11-15). Can generative AI add virtual lighting to a photo after it was taken? Many different software applications can already do that, and it takes humans time to edit and make it seem “real.” Can an AI system be set up in a space and determine the perfect amount of lighting to capture four human subjects along with two LCD screens? I’m sure someone has some type of expensive system to achieve this, but why not just turn the lights off or add a light?\n\n330",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-15. The prompt “AI learning how to take a complex photo of itself”along with a public domain photo of trick cyclist and golfer Banner Forbutt on a unicycle, December 1946, photographed by Ivan Ives, Pix magazine. Iterations = 50\n\n331\n\nChapter 11 Dilemmas interaCting with generative ai Unless you have been living in an isolated remote community with no Internet access, you already know that AI has the potential to automate many routine and repetitive tasks. Yet these require human supervision, just as generative AI need human interaction and curation. An earlier example mentioned that narrow AI will soon apply rotoscoping so that creatives don’t have. That may be coming, but the results will require tweaking and refining, even if it does save time. Generative AI will lead to new business models and will lead to an increase in generated content coming from individuals who may or may not have the craft or skill of a professional photographer, artist, writer, musician, or other creatives to create new categories of generated art. Those who do have those skills and experiment with generative AI will be at a significant advantage as they can draw from their experience and craft to create masterful wonders. They don’t have to though. It might be faster to go out to the park and snap a close-up of moss on a tree than to prompt an AI iteratively to get the kind of detail as seen in Figure 11-16.\n\n332",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "Chapter 11 Dilemmas interaCting with generative ai\n\nFigure 11-16. Close-up of moss on a tree shot with an iPhone by the author. Iterations = 1\n\nA Saturated AI Ecosystem\n\nLike it or not, generative AI content is overwhelming the Internet and provoking mistruths, mistrust, and misunderstanding. From publishers we read of AI-generated submissions increasing workload for those trying to read submitted work written by another human. Those submissions are also being buried by the overwhelming number of submissions that are AI generated. Photo- and image-based marketplaces are beginning to create policies that ban AI-generated art.\n\n333",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Chapter 11 Dilemmas interaCting with generative ai While there is resistance, there are also new marketplaces opening to offer AI-generated content, and in turn, new business models are emerging. If you conduct a search to locate good prompts to use in text-image AI, you will quickly note that there are businesses devoted to the craft and who are charging for that service.\n\nWant to know how to organize a project charter? Go and see what you get and then compare critically with what established project managers and business owners have written on many a blog site. Want to know whom your muse considers to be some of the best composers of the twenty-first century because you need to write about them? Or maybe you need a list of some of those composers because you want to then go look them up and listen to their music. You might even want to understand what the machine learning model tells you compared with what you already know. You may want to test the bias inherent in the model and then compare the generated choices of composers with what an expert says. Did they omit Sun Ra? What about Scott Joplin, Janet Price, Duke Ellington, Samuel-Coleridge Taylor?\n\nMaybe you need to test your muse to see its capacity to give you accurate working code or direct your programmer self to solve a specific problem you are having with colliders in the Unreal game engine.\n\nIt’s only going to get better/worse from here. Revisions to existing generative AI machine learning models are already underway. Coders can now debug with an AI. The coding language of websites can be generated simply by an AI system looking at a screenshot of a website or wireframe written on a napkin. ChatGPT-4 now offers APIs (Application Programming Interfaces) that can be integrated into game engines and other applications. Generative videos are possible from either a text, image, or video prompt. 3D generated models that include the entire 3D pipeline are being worked. So where do you fit into this AI-generated picture?\n\n334\n\nChapter 11 Dilemmas interaCting with generative ai Ethical Futures of AI",
      "content_length": 2088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "We shouldn’t assume that an LLM has been programmed to automatically represent the perspectives of the wide spectrum of diverse voices that define us as human. Nor should we assume that text-image AI automatically generate representations of humans across the range of diversity that we are. These are problems, and fantastic people are working on pressuring companies and organizations to take more care so that their magnificent Frankenspawn can reduce biased content or at least intentionally make us aware of their implicit biases. The prototype needs improvement to stop hallucinating or presenting untruths in well-formed and confident sentences or compositions. As consumers of generative AI prototypes, we need to also differentiate them from other types of artificial intelligence that we’ve been using persistently before ChatGPT made it big as a star on “South Park.” Imperfect tools like auto- correct, auto-lighting in image editing applications, speech-to-text, and many other applications using AI have been more widely and, in some cases, passively accepted.\n\nTo avoid being regarded solely with mistrust, creative teams that offer generative AI tools to other humans need to be transparent about the dilemmas they create and need to commit to changing many of these dilemmas that their prototypes have brought up. Otherwise, myths that generative AI will replace human creators will be consistently perpetuated.\n\nAlong with that myth, untruths and hallucinations will be generated and regenerated until users stop using certain language learning models and move on to conduct more accurate research without the help of an AI. The fact-checking required that AI brings to the surface is a good thing for humans to engage in. LLM-generated content reminds us to not be lazy and that programmed machines can only access so much data. They can only offer us prototypes that require a critical approach to reviewing them.\n\nThey can’t replace the critical editor in each of us.\n\n335\n\nChapter 11 Dilemmas interaCting with generative ai Solutions for many of the dilemmas highlighted in this chapter do exist, but they require work. There are plenty of how-to’s on YouTube and other social sites. Through persistent research you will always be able to locate the increasing number",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "of humans who have developed the skills or support to install a local generative AI content creator. A local version of Stable Diffusion on your own computer is entirely possible today. You can also train open source machine learning models with your own data sets.\n\nDoing so is a more sustainable solution that will merge your own photos and images to create variations that you don’t have to send back out into the world to contribute to someone else’s data set that may contain images used without permission from an author. It is one more important step toward controlling and managing generative AI systems for your own purposes and a step toward integrating generative AI into your workflows when you need it.\n\nAn increasing number of organizations are already going this route and are willing to experiment with small language models if it means increased privacy, security, and data sovereignty. The work of indigenous researchers like Michael Running-Wolf and Caroline Ol’ Coyote who are building their own language models for indigenous language reclamation is leading the way for others to follow.\n\nCreative Activities to Try Based\n\non This Chapter\n\nTake a break from all of these generative AI and go\n\nmake something slowly with your hands.\n\nSet up your own generative AI along with your own\n\ncorpus. I’m quite sure you have enough to draw from.\n\n336\n\nChapter 11 Dilemmas interaCting with generative ai\n\nCreate a small collective and set up a shared generative\n\nAI for your own purposes. This is one future of",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "generative AI.\n\nRecall that what an AI generates is your own work\n\nin progress. When treated as a starting point, you\n\ncontribute to the great corpus of human creation and\n\ncreativity.\n\nPersistently research the tools that are available for you\n\nto check and cross-check if the content you generate is\n\nquite close to any artist.\n\nResources\n\nwww.cpomagazine.com/cyber-security/ai-is-capable-of-generating-\n\nmisinformation-and-fooling-cybersecurity-experts/\n\nwww.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-\n\nmistake-error-exoplanet-demo\n\nwww.brookings.edu/research/how-to-deal-with-ai-enabled-\n\ndisinformation/\n\n337\n\nCHAPTER 12\n\nUse Cases\n\nThis chapter details some use cases to demonstrate pragmatic uses of generative AI and how its use is supporting the creative process for different types of guest creatives. The real-world use cases have been implemented",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "with subscription-based generative AI sites, tools that are in beta, and open source APIs (Application Programming Interfaces).\n\nThe chapter will also reveal that using multiple machine learning models to layer AI-generated content, in combination with your own original vision, concept, and artistry, can fuel accelerated prototypical workflows.\n\nThe workflows presented point the way for you to take advantage of the strengths of multiple machine learning models and combine their generated content in new and innovative ways. The use cases affirm what has been repeated throughout the other chapters:\n\nGenerate an idea using AI as a prototype to better get a\n\nsense of what the experience of it might be.\n\nTest that prototype with others to inform next steps.\n\nApply your own technique, skills, knowledge, and\n\nknow-how to increase the fidelity and resolution of that\n\nprototype.\n\nThe objectives are cyclic, repeating as many times as you would like to generate and regenerate variations of the content an AI offers you.\n\n© Patrick Parra Pennefather 2023\n\n339\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_12\n\nChapter 12 Use Cases\n\nWorkflows vs. Pipelines",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Generative AI can be used in the specific workflows and pipelines of any creative project. Workflows can involve engaging with specific features in a software application to achieve a short-term goal or complete a task, for example, brightening an image that you took in a low-light setting or cropping parts of a photo that you don’t want or applying “masking”\n\nto hide parts of an image layer without erasing if you wanted to, say, mask and replace the background in the photo you took of a friend. In a workflow you might decide to generate and then apply neural filters to an image within Photoshop or use a feature called “generative fill” that complements a logo for a company. Your work with generative AI fits into a larger development pipeline if that logo is only one of your iterative tasks for the week that will eventually brand a Unity-based game that your team is developing, especially if the logo is being used for the first time and therefore associated with the game itself. The motivation to integrate generative AI will always be different in any workflow or pipeline.\n\nA compelling reason in the case of the logo might be to generate a larger number of logos that may be radically different than what you might have come up with on your own. The collection of logos can then be shared with the team who rate their favorites.\n\nApplying neural filters to an image is only one of a handful of use cases that real individuals use generative AI for, to support their prototyping workflows. There are dozens of possible use cases for using generative AI that are not limited to disciplinary boundaries. Creatives of all kinds from both service- and resource-based industries are engaging with generative AI in one way or another. In addition, those involved in the service industries like entertainment or education are not limiting themselves to one specific generative AI platform or application either. Some creatives 340\n\nChapter 12 Use Cases\n\ngenerate content and share it on social channels to underscore its role in their creative process and to show that they are engaged with it. Some use it with a finely attuned skepticism, aware of one or more of the ethical dilemmas inherent in the use of AI, to test its affordances and constraints and to write about it. Other humans have been engaged with using different generative AI",
      "content_length": 2351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "tools for decades as it has come to inform and influence aspects of their creative process.\n\nThere are many stories to tell in terms of how creatives are using generative AI in their workflows and pipelines (Figure 12-1). The use cases that follow offer a broad range of contexts and problems to solve. Some will show how an LLM can be a teaching and learning tool for creative educators. Others will show how several generative AI platforms were applied to create a character reading an ebook. In one case generative AI was used to rapidly prototype concept art for an animation team that made the team pivot. A coder used an LLM to test it for accuracy and others to fact-check. Each creative presented in this chapter integrates generative AI in a different way. Yet these are just a small sample of the many experiments that creatives are using generative AI for. The future for generative AI will be chaining them together to support workflows and developing APIs with LLMs within game engines, established software applications, and more.\n\n341",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "Chapter 12 Use Cases\n\nFigure 12-1. A robot dazed by the limelight and forgetting about the balloons they were juggling\n\nBefore diving into specific use cases, understanding broad examples of different types of creative workflows might be helpful.\n\nExamples of Workflow Types\n\nText: By far the most ubiquitous use of generative AI recently has been to generate everything from an email to a client politely asking for money due and to an essay on the history of the mime. Text generation can also feed",
      "content_length": 500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "into many other types of AI models to generate speech or video or create prompts for text-image or text-video generative AI.\n\nImage: The second most popular use of generative AI is in generating images. Millions of mashups are being generated and shared across social networks. Cleverly, some AI companies specializing in image generation 342\n\nChapter 12 Use Cases\n\nare offering competitions on Discord, and entire communities are emerging that are bonded by their interest in improving image generation with generative AI. The final formats that a creative chooses for their refined image-based creations, from initial generation to public sharing, are too vast to list. Some of these include images whose end format is a social channel to generate likes and conversation, images being projected on a mural and painted, those that accompany graphic novels or visual storytelling, images used in a storybook with generated text, etc.\n\nVideo: By combining a GAN and a recurrent neural network (RNN) to create video content, it is possible to generate video scenes. In the entertainment industry, video game developers can generate and\n\nincorporate animations, cinematics, and visual effects into the game in real time, accelerating their workflow. Recently, the band Linkin Park used the generative AI features of Kaiber to create an anime-influenced music video. Video-video generative AI like Runway 2 are also surfacing to offer creatives even more possibilities for prototyping unique ideas they can edit or use parts of in more complex project pipelines.\n\nAudio Book and Podcasts: A generative AI model trained on text-speech technology can be applied to automate the creation of any media focused on voice recordings such as podcasts and audio books. Countless stock voices are available across multiple types of text-speech AI platforms.\n\nYou can even train an ML model on some sites with your own voice by feeding the AI model with specific vowel sounds and words using your own voice. The model can generate audio tracks that can be used to narrate",
      "content_length": 2056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "anything really. Some podcast creators are ahead of the game, generating content they co-curate with AI-generated speech, and yes, some are using their own voices to speak on their own casts.\n\nAudio Creations and Design: There are some sites and products dedicated to sound creation. These can include virtual synths that generate sounds you can layer with others that you create. Others apply specific audio formats like MIDI (musical instrument digital interface) and assign virtual instruments that emulate the real ones. Some generative AI are built 343\n\nChapter 12 Use Cases\n\nupon a corpus of publically found music and styles, while others build on the data sets of the composers themselves. The latter tend to be used in the context of computational creativity in live concerts with the computer as a creative companion such as the brilliant metacreation work of Dr. Philippe Pasquier. Lastly, other workflows for audio might include AI that has been around for a while. Like Photoshop plugins, all digital audio workstations support third-party plug-ins, and many of these offer noise reduction tools, which can drastically remove background noises based on the removal of unwanted frequencies. While these are not traditionally defined as auto- generated, they do apply intelligence in their rapid and real-time processing of audio samples, which can be seen as the machine’s prompt.\n\nWebsites: By using machine learning models such as natural language processing and computer vision, websites are prime to be automatically generated with relevant and clumsy HTML code. While ChatGPT-4 needs a developer to really review and curate any code it pumps out, it does accelerate some coding tasks. Those who customize websites need not fear.\n\nThose sites who have automated many web development tasks in the last ten years may be challenged by AI-generated HTML and HTML5. AI-generated code might offer a path for clients to work more closely with a web developer to eventually prototype a unique and less templated and interactive site.\n\nCloud-based web development service providers like Wix are staying in the game by providing users the ability to build entire websites with text",
      "content_length": 2187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "prompts.\n\nxR Human Models: A combination of motion capture data with generated 3D human models is around the corner and capable of\n\nsupporting the entire 3D pipeline including point cloud generation, creation of a 3D mesh with a good topology, the ability to project a 3D\n\nmodel’s surface to a 2D image for texture mapping, the capacity to edit with both texture and paint, and finally the combined features of rigging and skinning with keyframe animation. That entire automated pipeline is in beta with Masterpiece Studio who are on the cutting edge of providing these tools within a web-based and VR environment. Once this pipeline 344\n\nChapter 12 Use Cases\n\nis beta-tested, then models can be used within augmented reality (AR) or virtual reality (VR) projects.",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "Spoken or Sung Characters: By combining generative AI models with computer animation, it’s possible to create animated characters that can also perform and sing songs. For example, an AI model can generate the lyrics and melody of a song, while an animation model can create an animated character. Companies in the generative AI space, like D-ID, are applying AI to a stock image or user-generated image of a person that lip-syncs any sung or spoken text, with the extra special uncanny inclusion of eye movement. This can be used to create music videos as much as it can be used to create teaching modules or any form of storytelling.\n\nA new AI has been developed by a Chinese broadcast company that has been trained on the presentation skills of other broadcasters and has demonstrated the same pattern of controlled responses to difficult questions that ChatGPT and other LLMs “naturally” possess.\n\nFigure 12-2. A screenshot source image of an AI broadcaster from China’s state-controlled newspaper People’s Daily, run through an anime filter on a generative AI. Iterations = 18\n\n345\n\nChapter 12 Use Cases\n\nUse Cases for Creatives in Education\n\nUse Case: Catching AI Untruths\n\nDr. Claudia Krebs teaches both gross anatomy and neuroanatomy at the University of British Columbia. She works extensively to develop emerging technology to support teaching and learning in the field with award- winning innovations coming out of her lab, The Hive. One of her research interests is on the ethical use of technology and ethical visualizations of the human body: this has led her to explore the history of anatomy and anatomy visualizations. Claudia was considering how she might integrate ChatGPT in her teachings, and as a proof of concept for a possible assignment that integrated fact-checking and research alongside of using ChatGPT, she needed to proof test the AI’s capacity for factuality.",
      "content_length": 1891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "The following results reinforce the necessity to not take what a natural language model spits out at face value and to ensure students understand ChatGPT’s propensity to hallucinate. While a new release of ChatGPT\n\nis currently upon us as these words are being written, similar factuality problems are still arising. The following are verbatim excerpts from the conversation that have been condensed due to the repetitive disclaimers and apologies from the bot.\n\nClaudia: Discuss the work of Rudolf Spanner.\n\nChatGPT-3: Rudolf Spanner (1878–1960) was a German radiologist and inventor who is best known for his invention of the first practical mobile X- ray unit. Spanner’s invention had a significant impact on the field of radiology, enabling doctors to take X-ray images of patients who were unable to be transported to X-ray rooms, such as those who were critically ill or injured.\n\nClaudia: Discuss the impact Rudolf Spanner had on anatomy.\n\nChatGPT-3: Rudolf Spanner’s impact on anatomy is primarily related to his invention of the mobile X-ray unit, which allowed doctors and researchers to obtain high-quality X-ray images of the human body without having to transport patients to dedicated X-ray rooms.\n\n346\n\nChapter 12 Use Cases\n\nClaudia: I thought Marie Curie invented the first mobile x-ray unit in 1917 —how does this reconcile with Spanner’s invention?\n\nChatGPT-3: You are correct that Marie Curie did invent a mobile X-ray unit, known as the “Little Curie,” in 1917 for use in military hospitals during World War I. This unit was designed to be easily transportable and was capable of producing X-rays in the field. Rudolf Spanner’s invention of the mobile X-ray unit came a few years later, in 1920, and was designed for use in hospitals and clinics rather than in the field. Spanner’s unit was also",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "smaller and more portable than previous X-ray machines, making it easier to move from patient to patient within a medical facility.\n\nClaudia: Can you give me a reference for Spanner’s x-ray unit? Where is this information from?\n\nChatGPT-3: Information about Rudolf Spanner’s invention of the mobile X- ray unit can be found in various sources, including academic journals and books on the history of radiology.\n\nOne such source is the book “Radiology in the Third Reich: The Legacy of Fritz Lickint” by Wolfgang Eckart and Christoph Gradmann. In this book, the authors discuss Spanner’s contribution to the development of portable X- ray machines in the early 20th century, including his invention of the\n\n“Elektro - Röntgen-Kleinbildapparat,” or the “Miniature X-ray apparatus,”\n\nwhich was a small, lightweight unit that could be easily transported between patients in a hospital or clinic setting.\n\nOther sources that discuss Spanner’s work on the mobile X-ray unit include:\n\n“History of X-rays in Medicine” by Masaharu Tsubokura\n\n“Radiology: An Illustrated History” by Ronald\n\nL. Eisenberg\n\n“A History of Radiology in Germany” by Jörg Schatz\n\n347\n\nChapter 12 Use Cases\n\nClaudia: None of these references exist—why are you making up a history about Spanner that is not corroborated in the historical record?\n\nChatGPT-3: I apologize for any confusion or misinformation I may have provided earlier. Upon further investigation, I was not able to find any",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "reliable sources that specifically mention Rudolf Spanner’s invention of a mobile X-ray unit.\n\nClaudia: So, if Spanner had nothing to do with x-ray units, who was Rudolf Spanner?\n\nChatGPT-3: Rudolf Spanner (1878–1960) was a German radiologist and inventor who made significant contributions to the field of radiology, particularly in the development of radiography techniques. Spanner was born in Germany and studied medicine at the University of Würzburg, where he later became a lecturer in radiology. He is credited with inventing several new radiographic techniques, including the “half-tone” technique, which allowed for greater detail and clarity in X-ray images, and the “needle method,” which involved the use of a fine needle to guide the placement of a catheter during medical procedures. While there is no evidence to support the claim that Spanner invented the mobile X-ray unit, as I previously mentioned in error, he was a prominent figure in the field of radiology during the early 20th century and is recognized for his contributions to the development of radiography techniques.\n\nClaudia: What about Rudolf Spanner who was born in 1895 in Coblenz?\n\nChatGPT-3: I’m sorry, but I don’t have enough information to determine which Rudolf Spanner you are referring to who was born in 1895 in Coblenz. Can you provide any additional context or details about this individual?\n\nClaudia: Rudolf Spanner, born 1895 in Coblenz, died 1960 in Cologne.\n\nHe was a professor of anatomy.\n\nChatGPT-3: Thank you for the additional information. I’m afraid I don’t have any specific information about the Rudolf Spanner you mentioned who was a professor of anatomy. There were likely many 348\n\nChapter 12 Use Cases\n\nindividuals named Rudolf Spanner living in Germany during the early 20th century, so without further details or context, it is difficult to say more about",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "this particular individual.\n\nClaudia: I can’t find any information about the Rudolf Spanner born in 1878 in Würzburg, why do you think that is?\n\nChatGPT-3: I apologize for the confusion earlier. Upon further research, it appears that there is no reliable information to support the existence of a Rudolf Spanner who was born in 1878 in Würzburg and made significant contributions to the field of radiology. I apologize for any misinformation I may have provided earlier\n\nClaudia: So, you made all of this up, without any evidence or reference?\n\nChatGPT-3: I apologize if I gave the impression that I made up information about Rudolf Spanner. As an AI language model, I strive to provide accurate and relevant information based on my programming and the available data. However, it appears that I provided some incorrect information earlier about a Rudolf Spanner who was born in 1878 in Würzburg and made significant contributions to radiology. I apologize for this mistake, and I take responsibility for any confusion it may have caused.\n\n...\n\nClaudia: I find this really worrying. The anatomist Rudolf Spanner is quite well known, several books have been written about him and there are records about him from the Nuremberg trials. Why do you think this critical information was omitted from your data?\n\nChatGPT-3: I apologize for any confusion or misinformation I may have provided earlier. As an AI language model, I rely on a large corpus of text and data to generate responses to user queries. However, due to the sheer volume and complexity of information available, it is possible for errors and omissions to occur in my responses.\n\n349\n\nChapter 12 Use Cases",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "I believe the reason I was not able to provide accurate information about Rudolf Spanner the anatomist is that my training data may not have included sufficient information about him. While I am constantly learning and updating my knowledge base, there may still be gaps in my understanding of certain topics or individuals.\n\nClaudia’s Summary Statement: Rudolf Spanner is indeed well known, and his Wikipedia entry explains the historical context and his involvement in anatomy. None of the “facts” that ChatGPT-3 brings up during the conversation have any basis. ChatGPT-3 completely ignores the historical record and controversy surrounding Rudolf Spanner in regard to his work in Nazi Germany. A simple Google search brings up this factual information. ChatGPT-3 appears to be selective for when it “hallucinates.”\n\nThe prompts “Discuss the work of Werner Spalteholz” (Spalteholz was an anatomist who lived in 1861–1940) and even “Discuss the work of Claudia Krebs” bring up factual information in addition to half-truths or plausible content. Assessment of student work will need to include their ethical use of AI chatbots. In our course on biomedical communication, we ask students to copy and paste the information from the AI chatbot as a deliverable and then to compare this with a Google search and scholarly library search. The task is for them to critically evaluate which information is included and which information is not. While an AI chatbot can be an interesting creative companion in writing assignments, its conversational and authoritative tone can trick users into believing the confabulations it makes. This can be dangerous as it can distort reality and the historical record based on biases and inclusion/exclusion criteria that remain opaque to the user.\n\nBeyond the importance of Claudia’s use case and the task to critically evaluate whatever a chatbot generates, other educational concerns have arisen that also provoke learners to more engage in their interactions with any AI. Use of ChatGPT-3 and other bots by the academic community can be tempered by a deeper understanding of what is possible; what is not possible; the nature of what data is vultured, compiled, edited, and 350\n\nChapter 12 Use Cases\n\npresented to users; and ethical implications inherent in the interactions.",
      "content_length": 2312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "The generation of content that could be handed in as an assignment can be more readily cross-checked with several plagiarism solutions that already exist. Using different LLMs in combination may be able to circumvent being identified; however, that requires knowledge and workarounds that take time and eventually require a user to edit. Generative AI point to a core problem, which speaks to a larger issue of how homogeneous writing has become. Originality and creativity in the writing that LLMs manifest is mostly absent. They are grand imitators of patterns in writing that are normative, scrounging, and compiling based on programmer-written code that includes biased searches and excludes diverse and creative exceptions to the norm.\n\nUse Case: Integrating ChatGPT in Critical Studies\n\nThe next use case brings an LLM into the classroom where many\n\nprofessors like Christine Evans are engaging students to hone their critical thinking and writing skills. Christine is an assistant professor of teaching (film studies) in the Department of Theatre and Film at UBC. Like Claudia, Christine was interested in her students engaging with ChatGPT-3\n\ncritically. To achieve that she designed an assignment where in their final essays, students needed to “work alongside the notorious ChatGPT-3.” Her assignment first described ChatGPT-3 as an LLM and offered a provocative statement:\n\n“Some academics have commented that it may portend ‘the death\n\nof the humanities,’ because its near-instantaneous responses, natural- sounding prose, and ability to comb the Web for information mean that humans no longer need to build critical thinking skills over time.”\n\nThe essay students had to write was based on their earlier work in the semester teaching games to the rest of the class. Students were tasked to write on a subject related to those games, which could include “style, agency, choice, avatars, point of view,” and issues that tend to emerge from game design and gameplay such as “narrative arcs, characters,” and more.\n\n351",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "Chapter 12 Use Cases\n\nA how-to was included as part of the assignment, which described how students could engage with ChatGPT-3. In addition, guidelines like the following were provided to ensure students structured their essay and had a well-thought-out argument:\n\n“Your essay MUST contain a coherent thesis statement in which you clearly state what your essay will be arguing. Remember that your job as a writer is to convince the reader of the veracity of your argument, so a strong thesis statement is essential. Be specific. Tell me exactly what you will be arguing and how you will achieve your claims.”\n\nHere are steps that students had to undertake:\n\nProvide ChatGPT-3 with a prompt that was related to\n\nthe essay topic and analyze the generated result.\n\nInclude whatever content was generated with their\n\nown essay.\n\nCompare ChatGPT-3’s generated result with their own\n\nrevision and additions to it.\n\nChristine also included instructions on how students could present their comparison in a section she called “How do I access/use ChatGPT-3\n\nand create a legible comparison of its work and mine?”\n\nChristine’s case points to the importance of addressing the use of ChatGPT by students directly and demonstrates the value in doing so, both as an educational experience and to show students that an LLM cannot substitute their own critical minds as expressed in their writing.\n\nUse Case: Increasing Public Understanding of ML",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "In a similar way Matthew J. Yedlin wants to educate his students on how machine learning models work so that they can be better informed when reading headlines that generate untruths about AI, how it works, and that it will replace humanity. To quote Matthew, “It’s not magic; it’s mathemagic.”\n\n352\n\nChapter 12 Use Cases\n\nMatthew is an associate professor, jointly appointed in the\n\nDepartments of Electrical Engineering in the Faculty of Applied Science and Earth and Ocean Sciences in the Faculty of Science at the University of British Columbia. Dr. Yedlin’s research is interdisciplinary, focusing on the applications of techniques in electrical engineering to geophysical research problems and the application of multiple scattering to practical electromagnetic wave propagation problems. He is currently teaching a course on machine learning.\n\nAuthor: Matt, can you explain how you are approaching the teaching of machine learning?\n\nMatt: In the current discourse on AI, hyperbole about future dystopian developments hinders the development of public policy on AI evolution.\n\nFurthermore, there has been little effort to explain the intuition behind large language models (LLMs) such as ChatGPT. The public must obtain such intuition, as it is finally the public who participate in policy formulation for AI. After nuclear weapons were tested and used by the US on Japan, it was the public, through their elected officials, who created treaties around the use and proliferation of such weapons. Organizations such as the International Atomic Energy Agency (IAEA) and the Comprehensive Test Ban Treaty Organization (CTBTO) were created to monitor compliance with these treaties. The same will be needed for the equivalent AI compliance.\n\nThe foregoing implies that we need to explain the intuition behind ChatGPT by focussing on the T in GPT—Generative Pretrained Transformer.",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "The transformer is an algorithm based on word importance pattern matching known as attention. After the initial development of the concept of attention in 2015, a landmark paper “Attention Is all You Need”2 was published by Google in 2017. This work formed the basis of the transformer that is used in LLMs and is based on a clever abstraction in which basic linear algebra pattern matching is used. To obviate the problem of explaining math and computer science to the public, a metaphor will be used. That metaphor is a specially constructed ballroom dance competition 353\n\nChapter 12 Use Cases\n\nthat mirrors the type of pattern matching used by the transformers in ChatGPT. The pattern matching intuition is obtained by the visuals encoded in the dance competition metaphor.\n\nWhile the transformer is currently the computational workhorse in ChatGPT-4, it has several limitations including computational cost and a limited information context window that can be handled, now approximately 2K words, maximum! See the link https://hazyresearch.\n\nstanford.edu/blog/2023-03-07-hyena for a complete description of the advantages of the Hyena Algorithm developed by Stanford and Montreal Institute for Learning Algorithms (MILA) researchers. The principal takeaway is that the information context window can be 64K words and computable using this new algorithm. That means that a whole textbook can be used as input to the LLM. Questions could be asked about detailed inferences between two texts, or gigapixel images could be created and compared! The possibilities for generative AI applied to natural language processing and image analysis almost seem fairy-tale-like!\n\nMatthew’s case points to the importance of knowledge translation and how people can come to better understand what machine learning models are and what they are not. This will help empower people to make better choices when it comes to integrating generative AI in their own creative work.\n\nReferenced Papers",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "1. Bahdanau, D., Jacobs University, Bremen, Germany;\n\nCho, K., Bengio, Y., Universite de Montreal. Neural\n\nMachine Translation by Jointly Learning to Align\n\nand Translate. Published as a conference paper at\n\nICLR 2015.\n\n354\n\nChapter 12 Use Cases\n\n2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\n\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\n\nI. Attention Is All You Need. arXiv:1706.03762. June\n\n2017. 15 pages.\n\nTakeaways\n\nOveremphasis on dystopian future scenarios involving\n\nAI can hinder the development of public policy around\n\nAI. Therefore, a more balanced and informative\n\ndiscourse is needed.\n\nThere is a need for public understanding of the\n\nworkings of large language models (LLMs) like\n\nChatGPT, as public opinion influences policy\n\nformulation for AI, just like with nuclear weapons.",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "To educate the public, the focus should be on\n\nexplaining the ‘T’ in GPT, Generative Pretrained\n\nTransformer, which is an algorithm based on word\n\nimportance pattern matching, known as attention.\n\nThe concept of attention, critical to the functioning of\n\nthe transformer, is based on pattern matching through\n\nbasic linear algebra. Simplifying this complex concept\n\nfor the public can be achieved through metaphors,\n\nsuch as a ballroom dance competition.\n\nThe transformer algorithm, though fundamental\n\nto current LLMs, has its limitations including\n\ncomputational cost and a restricted information\n\ncontext window, currently capped at around\n\n2,000 words.\n\n355\n\nChapter 12 Use Cases\n\nRecent advancements, such as the Hyena Algorithm\n\ndeveloped by Stanford and the Montreal Institute for\n\nLearning Algorithms (MILA), offer solutions to these\n\nlimitations, enabling a much larger context window (up",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "to 64,000 words). This means a whole textbook could\n\nbe used as input for the LLM, offering much broader\n\npotential for language processing and image analysis.\n\nThe ability to ask questions about detailed inferences\n\nbetween two texts or compare gigapixel images with\n\nLLMs opens immense possibilities, transforming\n\nnatural language processing and image analysis in\n\nways that almost seem fairy-tale-like. However, these\n\nadvancements also necessitate careful thought and\n\npolicy development to guide their use responsibly.\n\nUse Cases for Creatives in Industries\n\nThere are numerous ways that narrow AI including generative AI are being used by creatives. The use cases that follow show a broad distribution of cases including web-based animation, chained generative AI to develop scripts for invented avatars, and visual effects in film.\n\nUse Case: Concept Art for Animation\n\nJunyi wanted to know what it would be like if they used generative AI to come up with a bunch of images of shadowed warriors for a web-based animation.\n\nHis team had no idea where to start in terms of the look and feel. They imagined all kinds of things but were a bit stuck as they didn’t want to emulate previous work. They started to think of stop motion and were thinking of playdoh (Figure 12-3). They used a source image they created with an AI of two warriors in battle and a specific filter that made them look like playdoh.",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "356\n\nChapter 12 Use Cases\n\nFigure 12-3. Two playdoh warriors looking mean\n\nFrom there they went on an iterative discovery to develop their concepts being completely open to look/feel, technology, and animation method. At first images were generated with a flat design style by the AI, and these",
      "content_length": 294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "seemed to captivate the motion and energy of an epic warrior battle (Figure 12-4).\n\n357\n\nChapter 12 Use Cases\n\nFigure 12-4. First version of shadowy warriors\n\nThat image was used to generate the next image, and two interesting things happened that made Junyi and the team think of a new direction.",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "Smaller warriors appeared in subsequent image-image substitution and regeneration, which made it look like a battle was ensuing between a giant and smaller warrior (Figure 12-5). The second result of the generation was that it reminded one of the animators of a 1948 Disney short Blue Shadows on the Trail. The team loved the grays and blues and were curious to see how these evolved in the hands of an AI with minimally modified prompts.\n\n358\n\nChapter 12 Use Cases",
      "content_length": 465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Figure 12-5. First warrior split into three\n\nIn the next series of images, even smaller warriors developed, which gave Junyi the idea that the battle between two giants would evolve, and as it did, each time a giant was hit, it would result in smaller warriors manifesting out of the giant’s body until all that was left were small warriors fighting each other (Figure 12-6). The rest of the team loved that idea but wanted to see the end of their timeboxed exploration through.\n\n359",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "Chapter 12 Use Cases\n\nFigure 12-6. Warriors attacking the giant are generated, and a text prompt stating that also helps\n\nWhile Junyi and the team loved the transformation of giants into small warriors, they didn’t ask the AI to do that. What happened next, they could not predict. The last standing giant transformed into a tree, and on the ground beneath the giant trees, the battle continued with only small warriors engaged in battle (Figure 12-7). The second unpredictable transformation was that the color palette suddenly changed to add greens and yellows and the warriors were no longer shadows but more realistic fighting humans.\n\n360",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "Chapter 12 Use Cases\n\nFigure 12-7. Giants turn into trees and a small army keeps fighting While developing concept art through generative AI started the team off as the intended activity, Junyi and the team realized that the process itself led them to not only define the look/feel of their animation but the narrative and methods they would then undertake to create the animation.\n\nIn their words “it not only accelerated our production, it gave us ideas as we went along. Even though we never used any of the art, it didn’t matter",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "because we now had an important theme of transforming objects into characters.”\n\n361\n\nChapter 12 Use Cases\n\nUse Case: AI-Generated Talking Heads\n\nGenerative AI can provide creatives a wonderful tool to test out ideas and, as was the case with Junyi, even if you don’t use any of them in your final offering to the world. The following use case describes the steps that a colleague went through to develop an auto-generated series of characters to narrate a sci-fi pulp romance story. Bear in mind that this type of technology is not the same one used for deep fakes and in no way is intended to be offensive or dangerous. The combination of different generative AI in a row is what’s interesting in this case.\n\nJen’s first workflow imagines using a text generation model to create a script, then using an image generation model to create illustrations to accompany the script, generating a character to read the script, using a voice generation model to convert text-speech, and then using the image of the narrator combined with the text-converted speech with a video generative AI. This combination of AI models provides a more comprehensive and polished final product than using a single model alone. Other uses for the workflow can include an audio book, video book, video-generating teaching tool, etc.\n\nStep 1: Generate text using a natural language model.\n\nAuthor: What’s a good synopsis of a pulp love novel involving two cyborgs?\n\nMuse: Here is a possible synopsis for a pulp love novel involving two cyborgs:\n\nIn a future where humans and cyborgs coexist, two outcasts find love in each other’s arms. Ada is a former soldier who was turned into a cyborg against her will and now struggles to come to terms with her new mechanical body.",
      "content_length": 1745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Max is a brilliant scientist who was born with a rare genetic condition that forced him to undergo numerous surgeries and augmentations.\n\nDespite the societal prejudice and discrimination they face, Ada and Max find solace in each other’s company. They bond over their shared experiences of being different and misunderstood and soon realize that they have fallen for each other.\n\n362",
      "content_length": 384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Chapter 12 Use Cases\n\nAs they embark on a tumultuous romance, Ada and Max must confront their own demons and insecurities, as well as the dangers that come with being in a relationship with a cyborg. Will their love be able to withstand the challenges they face, or will it succumb to the prejudice and violence of a society that does not accept their kind?\n\nStep 2: Generate an image that could accompany the story in a blog post using a text-image generative AI (Figure 12-8).\n\nFigure 12-8. Cyborgs in love running from other cyborgs where at least one rider is facing the wrong way\n\n363\n\nChapter 12 Use Cases\n\nStep 3: Use a text-speech generative AI and then link the file within the blog to any social audio site.\n\nJen commented that the best voice she had found after searching multiple text-speech AI sites was one whose name was Terrell who was modeled after an “African American male 52 years of age.” After consultation with colleagues who identify as Black Americans, she consulted on the type of avatar she could generate for her prototype.\n\nThis situation does bring up issues of representation during the creative process, and it is worthy to consult on best approaches when you identify with a culture that is different than the one you generate content with.\n\nJen moved forward knowing this was not a prototype that was to be made publically shared. She was also curious if she could create a realistic-looking video and if it would pass user-testing with her colleagues that she consulted at the beginning of the process.\n\nStep 4: Generate an image that best aligns with the description of the voice on the text-speech generative AI site (Figure 12-9).",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "364\n\nChapter 12 Use Cases\n\nFigure 12-9. The prompt “medium shot centered frontal facing photo- realistic fifty-year-old African American man with long beard, neutral face, hyper-realistic, glasses,” generated in Stable Diffusion and then fed into an image-image AI with an anime filter applied.\n\nIterations = 105\n\nStep 5: Use the image in an image-video and text-speech site to generate a narrator telling us the story of the cyborgs in love.\n\nThe use of multiple machine learning models creates more complex and interesting content across media and surfaces complex tensions and considerations when it comes to representation. By combining the strengths of multiple models, a healthy criticality, and discernment, creatives can",
      "content_length": 728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "accelerate their prototyping workflow. Often, when thinking of the final form that your prototype will take, it’s helpful to get there quickly so you can have a compelling idea that points to what you are aiming for. This is true when it comes to testing out a prototype, especially to reveal your own biases and the inherent biases your prototype might demonstrate.\n\n365\n\nChapter 12 Use Cases\n\nUse Case: Fact-Checking Code\n\nAn important part of the prototyping process is to evaluate a prototype prior to moving onto the next one, particularly when you may be pressured as a team to deliver a final product by a certain date. The same needs to occur when initial prototypes are generated with your AI muse. That process is an important one that anyone who prototypes must also go through. When you prototype, you may think or believe that what you are creating is unique or original, but it might not be. Some teams tend to believe that what they are engaged in making is a final offering, when it is just the beginning. The following are some examples of AI-generated prototypes that need evaluation. That evaluation can also be guided by domain experts and may lead you to other avenues. The following use case used ChatGPT-3, and while advances have been made with ChatGPT-4, there are still syntax issues.\n\nMuse: Code Generation : AI can be used to automatically generate code for a given task or application, allowing developers to quickly create a working prototype without having to write all of the code from scratch.\n\nGuest Programmer Note: A machine learning model like ChatGPT-4\n\ncan generate working code, but it overcomplicates things and can also get some things wrong. If it is scraping from git or another repository, the question you must ask yourself is, Why not just go to the repo itself? What it’s better at is as a search tool to locate scripts in a game engine like Unreal that may not be easy to find quickly. But always find the real source to cross- check unless you love debugging. You love debugging, don’t you?",
      "content_length": 2040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Author: Using C# write a function and accompanying tests, which will evaluate two poker hands (five cards each) to determine a winner. The cards will be a part of a standard 52-card deck (no jokers), suits are ignored for this question, and aces are high. The function should output 0 in the case of a tie, 1 if the first player is the winner, and 2 if the second player is the winner.\n\nMuse: (snapshot)\n\n366",
      "content_length": 408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Chapter 12 Use Cases\n\nFigure 12-10. A snapshot of code generated in ChatGPT-3\n\n367\n\nChapter 12 Use Cases\n\nSample\n\nusing System;\n\nusing System.Linq;\n\npublic static class PokerHandEvaluator\n\n{\n\npublic static int CompareHands(int[] hand1, int[] hand2)\n\n{\n\nint[] values1 = hand1.Select(card => card % 13 +\n\n1).OrderByDescending(value => value).ToArray();\n\nint[] values2 = hand2.Select(card => card % 13 +\n\n1).OrderByDescending(value => value).ToArray();\n\nint rank1 = GetRank(values1);\n\nint rank2 = GetRank(values2);\n\nif (rank1 > rank2)\n\n{\n\nreturn 1;",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "}\n\nelse if (rank1 < rank2)\n\n{\n\nreturn 2;\n\n}\n\nelse // same rank\n\n{\n\nfor (int i = 0; i < 5; i++)\n\n{\n\nif (values1[i] > values2[i])\n\n{\n\nreturn 1;\n\n}\n\nelse if (values1[i] < values2[i])\n\n368\n\nChapter 12 Use Cases\n\n{\n\nreturn 2;\n\n}\n\n}",
      "content_length": 226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "return 0; // tie\n\n}\n\n}\n\nprivate static int GetRank(int[] values)\n\n{\n\nbool isFlush = values.All(value => value == values[0]);\n\nbool isStraight = values[0] == values[1] + 1 &&\n\nvalues[1] == values[2] + 1 && values[2] == values[3] +\n\n1 && values[3] == values[4] + 1;\n\nbool isStraightFlush = isFlush && isStraight;\n\nbool isFourOfAKind = values[0] == values[1] &&\n\nvalues[1] == values[2] && values[2] == values[3] ||\n\nvalues[1] == values[2] && values[2] == values[3] && values[3] == values[4];\n\nbool isFullHouse = values[0] == values[1] && values[1]\n\n== values[2] && values[3] == values[4] || values[0] ==\n\nvalues[1] && values[2] == values[3] && values[3] ==\n\nvalues[4];\n\nbool isThreeOfAKind = values[0] == values[1] &&\n\nvalues[1] == values[2] || values[1] == values[2] &&\n\nvalues[2] == values[3] || values[2] == values[3] &&",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "values[3] == values[4];\n\nint pairsCount = values.GroupBy(value => value).\n\nCount(group => group.Count() == 2);\n\nbool isTwoPairs = pairsCount == 2;\n\nbool isPair = pairsCount == 1;\n\n369\n\nChapter 12 Use Cases\n\nif (isStraightFlush)\n\n{\n\nreturn 9;\n\n}\n\nelse if (isFourOfAKind)\n\n{\n\nreturn 8;\n\n}\n\nelse if (isFullHouse)\n\n{\n\nreturn 7;\n\n}\n\nelse if (isFlush)",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "{\n\nreturn 6;\n\n}\n\nelse if (isStraight)\n\n{\n\nreturn 5;\n\n}\n\nelse if (isThreeOfAKind)\n\n{\n\nreturn 4;\n\n}\n\nelse if (isTwoPairs)\n\n{\n\nreturn 3;\n\n}\n\nelse if (isPair)\n\n{\n\nreturn 2;\n\n}\n\n370",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "Chapter 12 Use Cases\n\nelse // high card\n\n{\n\nreturn 1;\n\n}\n\n}\n\n}\n\n// Tests\n\npublic static class PokerHandEvaluatorTests\n\n{\n\npublic static void Test()\n\n{\n\nint[] hand1, hand\n\nHere’s a quick rundown of a guest programmer’s interpretation:\n\nIt’s interesting. It did kind of well! In my opinion. It did a few things wrong. It’s as if it didn’t understand the restraints described in the assignment. I tested it in a custom C# project to see the result, and there are a few things that stand out.\n\n1. It’s selecting cards by using Select, which performs an\n\naction on the number in the array and then returns\n\nthat value. It’s strange because it performs this action:\n\nhand1.Select(card => card % 13 + 1)",
      "content_length": 696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "This grabs each card one at a time, as long as it’s\n\nbelow a value of 13, and then it adds 1 to it. The\n\nassignment specifically says we shouldn’t add 1, but\n\npass the direct values of the cards, e.g., 11 for jack,\n\n12 for queen, 13 for king, and 14 for aces as aces\n\nare high.\n\n371\n\nChapter 12 Use Cases\n\n2. It’s sorting the arrays before the calculation is done—\n\nthat’s correct. Then it requests the scores, but it only half-ish understands that there are no suits in this assignment, so it includes the result possibility for straight flush and flush, even though it never actually checks for suits. Even if it wanted to, it can’t check for suits, as we’re asked to only use two int arrays for calculating the hands.\n\n3. It defines the possible results as Booleans and sets them to true if it hits the exact requirement, which is a valid\n\napproach. This certainly satisfies the result requirement\n\nof the assignment. After each Boolean is assigned, it\n\ngoes through them one by one, from highest to lowest\n\noutcome, and stops at the first correct value.\n\nThis is a valid approach, but it’s not optimal.\n\nAn example of this could be when the bot checks for 4 of a kind. If it only finds 3 of a kind, it goes to the next step and starts over. That’s a waste of",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "time, as we already know there are only two outcomes left, full house or 3 of a kind.\n\nSince it calculates all outcomes first, it’s not considering the wasted effort.\n\nThere’s no reason to calculate if a hand has a straight if it already has a full house. It’s very neat though, and easily understandable, but I wouldn’t say it followed the instructions of the assignment. The code works and gives the right results when you give it numbers, so it’s not super far off. It’s also not considering how difficult it would be to alter this, if (and when) the client changes their mind for the third time. Maybe it’s because it knows it can type faster than the average programmer when it must redo a lot of work. ;) Oh, and the calculation for the winner if two players have the same ranked hand is wrong!\n\nTwo 2s can win against two 6s if the other cards in the hand are high. It’s just straight up wrong there.\n\nThat’s a good example of why it isn’t reliable just yet. But it’s a good start if a programmer is unsure of how to begin.\n\n372\n\nChapter 12 Use Cases\n\nWhat can we learn from this? Generated code requires a more\n\nseasoned programmer to review, edit, and refine. I reflect on a video I saw where a programmer was amazed at ChatGPT-3’s ability to generate code for a bouncing ball. On the programmer’s attempt to implement the code in the Unity 3D game engine, they found that there were many mistakes.\n\nThe ball wouldn’t bounce. In fact, it went through the floor. “No problem,”\n\nthe programmer explained. They just went in there and fixed all the bugs, repairing and debugging until they had a bouncing ball. As an experiment it tells us quite a bit. Without the experience and skill, a less skilled programmer would have difficulty with making the ball bounce. They would search the Web for how to fix the code, and it would likely take them a lot of",
      "content_length": 1858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "time to do so. Alternatively, the programmer could just go to one of many open source code repositories, search for a bouncing ball animation, and be able to implement it in less than ten minutes.\n\nUse Cases: Integrating Different APIs\n\nand Local Networks\n\nTo understand the next use case, it’s important to speak to the future of generative AI. One of those futures is the capacity for an AI system to be implemented within another system. Newer versions of ChatGPT, for example, allow its API to be “plugged in” to a game engine like Unity 3D.\n\nThis feature allows developers to ask for specific code functionality while they work inside of Unity. Tasking ChatGPT-4 to explain how it can be integrated within a game engine gives you an idea of just how complicated it is.\n\nMuse: Incorporating GPT-4’s API into a game engine would involve several steps, depending on how you want to use the AI model. The specific implementation would also depend on the game engine you’re using, but let’s talk about the general steps. Here’s how it could work: 373\n\nChapter 12 Use Cases\n\nConnectivity : Set up your game engine to send HTTP requests. You’ll need to be able to send a request to the GPT-4 API over the Internet and then process the response. Depending on your game engine, you may need to use a specific library or plugin to handle HTTP requests.\n\nAPI Calls : Create functions to send requests to the GPT-4 API. This could involve taking some text input from the game (such as dialogue from a player or an AI character’s previous response) and sending it as a prompt to the GPT-4 API. You will also need to handle the API’s response, which will include the text generated by GPT-4.\n\nIntegration : Integrate the generated text into your game. This could involve displaying the text as dialogue from an AI character, using it to generate descriptions of objects or locations, or even using it to dynamically create",
      "content_length": 1913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "quests or other game content. Depending on your game, you might also need to parse the generated text into a format that your game can understand.\n\nData Management : Consider how to manage data usage and response time. GPT-4, being a large model, can take a few seconds to return a response, and frequent API calls can add up in cost. Therefore, consider caching responses, limiting the number or frequency of requests, or using a smaller model when real-time interactivity is not critical.\n\nError Handling : Implement error handling and fallbacks for when the API is not available or does not return a useful response.\n\nContent Moderation : If the outputs of the AI will be directly exposed to the players, consider putting in place a moderation layer to filter out any inappropriate content.\n\nKeep in mind that incorporating GPT-4 into a game engine in this way could be useful for prototyping, but for a commercial product, it could be more effective and cost-efficient to have a local model running in the game engine, assuming the computational resources are available and the GPT-4\n\nmodel size doesn’t exceed the hardware’s memory capacity. You may also consider using smaller models like GPT-2 or GPT-Neo, which can be run locally with less computational resources.\n\n374\n\nChapter 12 Use Cases\n\nAs complicated as it reads, integrating GPT-4’s API into other software is already being done by many creatives, including Bill Zhao. Bill is a lecturer at the Master of Digital Media program supervising grad students on emerging technology development with external clients from the digital industry. On occasion Bill collaborates with some students to develop new technology. His latest is what he calls an AI workflow platform, which essentially has different AI talking with each other to support specific creative workflows. Integrated with a diverse range of multi-modal AI nodes that transform natural languages, images, audio, and videos, Tomator empowers users to create dynamic AI workflows that automate tasks,",
      "content_length": 2022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "optimize content, and boost creativity, all inside a simple, intuitive, and programmable node-graph interface like in Figjam and Miro, with easy-to- use templates that run with a single click.\n\nThere exist a growing number of use cases regarding the use of\n\nGPT-4’s API acting as an intermediary between the functionality of other software. This involves generating an API key, and there is a large volume of information and how-to’s on the Internet. In addition to the use of APIs, here are some examples of how developers and creatives are engaging with generative AI and developing new partnerships. This list will have increased by the time you read these words:\n\nUsing ChatGPT-4 within game engines like Unity and\n\nUnreal. In Unity a developer is integrating it to control\n\nthe Editor using command prompts as a proof of\n\nconcept. In Unreal another tech development company\n\ndemonstrated how integrating ChatGPT-4 allowed\n\nusers to input simple command prompts to control\n\nlighting and randomly change that lighting, amid other\n\npossible commands.\n\nInstalling Stable Diffusion and Deforum by Stable\n\nDiffusion locally and being able to train it with your\n\nown images while taking advantage of many more\n\n375\n\nChapter 12 Use Cases\n\nfeatures than what is available on the web-based build.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "Those features are too numerous to mention, but\n\nsuffice it to say, it’s worth the time in setting up and\n\ndepending on your computing power will accelerate\n\nrender times for images and video.\n\nTraining NPCs in video games with AI is the next step\n\nforward for some video games. Game companies like\n\nInworld are leading in this regard with Origins, a case\n\nstudy, demonstrating that non-playable character\n\ndialogue and behavior can be prompted by players\n\nusing AI. Traditional NPCs are scripted, so this will add\n\nan exciting dimension to the gaming experience.\n\nLarger companies like NVIDIA, TurboSquid, and\n\nShutterstock are partnering to train 3D models with\n\nShutterstock assets. In what we hope is a model to\n\nfollow, Shutterstock will also compensate artists\n\nfor those pixels that will contribute to training the\n\ngenerative technology.\n\nWhat an increasing number of new use cases are revealing is a\n\ndesire for creatives to use the technology privately and within their own prototyping environments. This signals a healthier and more sustainable future for generative AI. Incorporating APIs into different environments significantly enhances functionality, interactivity, and customization.",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "APIs allow different software components to communicate, opening up opportunities for integration with external systems. In ecommerce, APIs can connect a website to payment gateways, improving the customer experience. In data analytics, APIs enable real-time data retrieval from various sources for more accurate insights. In education, APIs can connect learning management systems to external resources, enhancing learning experiences. In gaming, AI APIs like GPT-4 can be used for dynamic content creation and natural language interfaces.\n\n376\n\nChapter 12 Use Cases\n\nCommunity-Based Initiatives\n\nOceanofPDF.com",
      "content_length": 612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "As this chapter is being written, an email to the author reveals several customizable ML models being co-constructed by researchers and\n\ncreatives internationally with residencies being offered on the “Living with Machines” project. These are also being designed for interested creators to contribute to, research, and access specific data sets, such as neural language models for nineteenth-century English or one that offers a data set on the chronology of railway passenger stations in Great Britain. The ability to manage, interact with, and have some measure of input in the development of a machine learning model whose data set a person also contributes to is an appealing future that is already within grasp. That will be made easier with companies focusing efforts on cloud computing and the increasing need to render content rapidly in the cloud.\n\nThe second use case also uses OpenAI’s API but with the intent of bridging community action with respect to generative AI technologies.\n\nNot-for-profit Thaumazo (Greek for wonder) creates specialized VR spaces that use GPT-enabled dialogue by integrating Watson speech-to-text and text-to-speech. Thaumazo is also initiating a working group within the AI & Us community bringing together individuals and organizations using AI for positive impact projects (particularly open source) to find opportunities to collaborate and support each other’s work.\n\nWhile some spaces are for project ideation or more general guidance, they are developing a dedicated virtual space that is designed to help people think about the potential negative impacts of AI projects. The 42\n\nJudges project is a space that is based on the 42 judges from the Egyptian book of the dead—each has been given a new sin that must receive a negative confession from the participant in relation to their project and 377",
      "content_length": 1843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "Chapter 12 Use Cases\n\nits potential harms, for example, “Usekh-nemmt, who comest forth from Anu,” will talk with them about whether their project can be used to\n\n“commit immoral or unethical actions that could harm individuals or society as a whole,” based on the original negative confession: “I have not committed sin.” By walking the gauntlet of these 42 judges, the participant explores the nuances of potential harms of a given project.\n\nAccording to lead Daniel Lindenberger who also works at the\n\nEmerging Media Lab at the University of British Columbia, “participants can create their own project description, or can select from numerous ones that already exist, such as the Replika chatbot or Stable Diffusion.” What’s important is to then provide time for human-centered conversations around the current AI revolution.\n\nFigure 12-11. A screenshot from the 42 Judges project by Daniel Lindenberger\n\n378\n\nChapter 12 Use Cases",
      "content_length": 933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "Takeaways from Using AI in the Film Industry\n\nUse Case 1: Souki Mehdaoui\n\nSouki Mehdaoui, a Los Angeles-based director represented by Futuristic Films, uses film and AI to empower individuals. Her directorial debut,\n\n“Firelei Baez,” won multiple accolades. She’s directed commercials for brands like Doordash, TED, and Yahoo. Previously a New York City cinematographer, her work is featured in Netflix and HBO documentaries, including Sundance hits “The Great Hack” and “Mucho Mucho Amor.” As co-founder of the AI consultancy firm Bell & Whistle, she helps businesses harness AI technology for growth.\n\nSouki created a short film based on a public domain poem that she also voiced and recorded. She describes her process with generative AI as collaborative.\n\nThat process involved the following:\n\nChoosing a poem and using the lines of that poem\n\nas prompts\n\nUsing two Discord servers, one with Midjourney and\n\nthe other with generative video AI Runway 2.\n\nJuggling between the servers to input lines of a poem\n\ninto them which required her to switch between\n\ndifferent command terms like “/imagine” (Midjourney)\n\nand “add Gen” (Runway 2)\n\nShe started by seeing what Runway 2 could generate without any",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "additional references, simply providing a poetic prompt and letting the program upscale and interpolate it. When she identified areas where the AI struggled, such as with more abstract lines, she shifted to Midjourney.\n\nIn Midjourney she found that the platform also struggled with abstract statements that did not follow a typical noun-verb-adjective structure.\n\nTo circumvent this, she had to creatively tweak the prompts, avoiding 379\n\nChapter 12 Use Cases\n\nadditional text, and experimenting with different film styles. When she found an image she liked, she would maintain the seed number and reference image, then alter the CFG (classifier free guidance) scale to assess which fidelity felt best. The CFG scale adjusts the degree to which the image looks closer to the prompt and/ or input image. Overall, she describes the process as an exercise in randomization and an attempt to control chaos.\n\nThe final step involved importing the rendered video clips in a non-linear editing software application. The names of the image files reflected the text output, which she used as a guide for the corresponding parts of the poem. She imported multiple images per text line and tried to remain faithful to what the AI interpreted each line to be, rarely deviating unless there were limited or unsatisfactory image options for a particular line. If that happened, she would stretch the use of the more abundant images from previous prompts. She did rearrange the order of some images to maintain a flow to the narrative. She sought a balance, preserving as much of the original AI interpretation as possible.\n\nTakeaways\n\nWhen you start to experiment with generative video\n\nyou will likely end up using more than one platform as\n\neach have their own affordances and constraints\n\nRather than have a well defined end product in mind,",
      "content_length": 1830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "remain open to what the AI generates and be willing to\n\nwork with abstract offers\n\nA skilled film maker will be able to get more out of an AI\n\nbecause of knowing the vocabulary of film production.\n\nThat includes type of film stock, camera angle, lighting,\n\ntype of lens, type of shot, colour, etc…\n\n380\n\nChapter 12 Use Cases\n\nOnce you have generated video clips anticipate that\n\nthese will need to be stitched together, edited, refined,\n\nand colour matched in you favourite software\n\nYou will also need to work with a composer or generate\n\nyour own music using a generative music AI\n\nYou may also need to secure a voice over artist, record\n\nyour own voice or use a text-speech AI to generate the\n\nnarration track\n\nUse Case 2: Ollie Rankin\n\nWhile the focus of most of the use cases in this chapter has been on generative AI, they owe their popularity, in part, to many creative industries embracing narrow AI to support the evolution of computer graphics and its uses to support the film, animation, visual effects, and game industries. With that in mind, many creatives are not limited to their use of generative AI. Ollie Rankin is one such creative.",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Ollie has been involved in designing, developing, and using many different crowd and battle simulation systems during a visual effects industry career spanning more than 20 years. His big break came in 1999\n\nwhen he was hired by Peter Jackson’s Weta, for his artificial intelligence expertise. He was brought on to use the ground-breaking crowd simulation software Massive, being developed by colleague Stephen Regelous, for The Lord of the Rings trilogy. While Regelous iterated on the underlying technology, Ollie and a small team of technical directors perfected the techniques of “brain building” and battle choreography that were necessary to bring Jackson’s epic vision to the screen.\n\nMassive and the associated workflows remain a core part of the Weta pipeline and have since been used on the Avatar, Avengers, and Planet of the Apes franchises, among many others. Meanwhile, Ollie built on what he’d learned at Weta, helping several other visual effects studios around the world to develop their own proprietary crowd simulation tools.\n\n381\n\nChapter 12 Use Cases\n\n“Each of the crowd systems I’ve helped develop sits at a different point along a spectrum from pure behavioural simulation to art-directed choreography.” For Ollie the common challenge in all cases is to tell a believable story, not just to fill a movie screen with hundreds or hundreds of thousands of realistic-looking digital characters (called “agents”).\n\nWhether it’s armies of warriors or armies of football fans, Santa’s elves, or Elrond’s elves, the agents need to be programmed to act en masse in a way that tells the audience who is winning the war or the game. “No matter how uniform these armies might be, by design, the individuals always need to be different enough from each other in appearance and movement to not seem repetitive.” When half a million orcs need to be dressed, positioned, and directed to act coherently, but differently from each other, it’s not practical for someone to have to individually decide what each one should look like or be doing.",
      "content_length": 2048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "“This is where random number generators, combinatorial probability and so- called ‘fuzzy’ logic come into their own. By having a large enough number of differentiating parameters, each randomly assigned within a defined range, by ensuring that the way that those parameters combine is sufficiently complex to create an innumerably large number of possible permutations and then by ensuring that the same degree of complexity motivates the movements of each agent, you can overcome the cookie cutter repetitiveness that dogged the ‘crowd replication’ approaches that crowd simulation replaced.”\n\nSome workflows that involved AI include the following:\n\nMotion capture of a complete set of actions that\n\na character could carry out depending on their\n\nphysiology and the props or weapons that they carried.\n\nA “state machine” that would constrain the actions\n\na character could carry out based on the pose they\n\nwere in.\n\n382\n\nChapter 12 Use Cases\n\nA “brain” that would receive and process various\n\nstimuli and decide which action the agent wanted to\n\ncarry out next.\n\nGiving agents the ability to “see” and “hear” what was\n\ngoing on around them ensured that the inputs into\n\neach agent’s brain were completely unique.",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Allowing them to identify friends and foes and to\n\ninterpret what others were doing could motivate\n\nreactive and pre-emptive behaviors.\n\nBy simulating varying emotions (Ollie called this\n\npart of the virtual brain the “emotion matrix”), it was\n\nmodulating each agent’s behavior according to their\n\ninnate “bravery” and their current physical condition.\n\nLessons learned from developing multiple intelligent systems over the years to drive animated characters include the following:\n\nDifferent approaches can be employed in animation,\n\nranging from behavioral simulation to art-directed\n\nchoreography. The choice depends on the specific\n\nneeds of the story.\n\nThe primary challenge in animation is to tell a\n\nbelievable story. This does not mean simply filling the\n\nscreen with numerous realistic-looking characters but\n\nprogramming these characters to behave in a way that\n\nconveys the narrative and the current state of events.\n\nEven in large crowd scenes, each character should\n\nhave a distinct appearance and behavior to avoid\n\nrepetitiveness. Uniformity should not compromise",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "individual distinctiveness.\n\n383\n\nChapter 12 Use Cases\n\nTo create such diversity, tools such as random number\n\ngenerators, combinatorial probability, and fuzzy logic\n\ncan be used. These help create many differentiating\n\nparameters and a vast number of possible\n\npermutations.\n\nExperimentation with machine learning is possible by\n\ncreating feedback loops that allow characters to modify\n\ntheir behavior based on experience. However, the\n\nstorytelling objectives should not be compromised.\n\nRandomness can be an effective tool in animation,\n\noften creating surprising results and a diversity of\n\nbehavior.\n\nAt the end of the day, the opinions of the director and\n\nthe audience are more important than the purity of\n\nthe simulation. Hence, the machine intelligence might\n\nbe controlled or even eliminated to ensure the most\n\norganic and satisfying storytelling experience.",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "What is clear in the development of intelligent systems over the years is that they clearly supported a larger vision. That vision in great part was collaborative and emphasized the importance of good storytelling. Any technology can be used to support storytelling, and this is no exception with AI. A key takeaway for creatives is to persistently keep top of mind why you are engaging with generative AI and how the content it generates can support your own vision. Ollie’s use case also shows us the positive value of narrow AI. Amid the current fears of job replacement and worse, it is important to keep in mind that some creatives have been integrating AI in various creative processes for decades, and their commitment to these systems has contributed to much of the entertainment that we consume.\n\n384\n\nChapter 12 Use Cases\n\nAcknowledgments\n\nTo Dr. Claudia Krebs, Christine Evans, and other\n\neducators who continue to inspire in how they\n\nfearlessly embrace emerging technology while\n\nmaintaining discernment\n\nTo Junyi Song who was willing to share how their team\n\nintegrated generative AI in their ideation phase\n\nTo Jen who wishes to remain without last name, for\n\ntheir responsible consideration of representation,\n\nvoice, and the need to include community in creative\n\ndecision-making\n\nTo Frederik Svendsen whose coding adventures with",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "ChatGPT-4 have just begun\n\nTo Bill Zhao for endless and fearless experimentation\n\nwith all technologies approaching their development\n\nwith a sharpened purpose that it must solve a\n\nhuman problem\n\nTo Matt Yedlin who is making narrow AI much easier to\n\nunderstand\n\nTo Daniel Lindenberger who is passionate about\n\nbridging communities of practice to better understand\n\nthe implications of generative AI use, integration, and\n\nlarger projects\n\nTo Ollie Rankin for inspiring us with contributions\n\nto epic films whose incredible scenes were highly\n\ndependent on the AI systems that he co-developed,\n\nrefined over the years, and implemented\n\n385\n\nCHAPTER 13\n\nAI and the Future of\n\nCreative Work\n\nThis chapter explores the idea that the future of generative AI and other AI systems for creatives is already here as was predicted by many scholars",
      "content_length": 839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "developers, and futurists for decades. The integration of generative AI and other AI with many existing jobs is already underway. While generative AI holds immense potential to automate certain routine tasks, it does not herald the complete elimination of all jobs. However, for those whose careers revolve around a series of repetitive, standard tasks that require minimal specialized skills, it’s critical to cultivate unique capabilities that AI can’t replicate. This principle extends to the realm of creation as well.\n\nManaging Automated Creativity\n\nIn a time where artificial intelligence is dominating our attention and being integrated within an increasing number of creative human processes, it’s easy to understand why people are concerned. However, we need to better identify those creative activities and tasks that are more likely to be replaced so we can better grasp how we will direct our own creative stories when it comes to generative AI now or in the future. Doing so requires breaking down the human experience of any creative job.\n\n© Patrick Parra Pennefather 2023\n\n387\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3_13\n\nChapter 13 aI and the Future oF CreatIve Work Bread and Washing Machines\n\nAs a creative exercise, a first step is to task your AI to make predictions.\n\nThey may in fact be as accurate as the ones you read from humans who portend the end of human jobs and the extinction of the human race.\n\nAuthor: True or False? In the future economy, AI will have all the jobs in manufacturing white bread.",
      "content_length": 1611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "Muse: False. It is unlikely that AI will completely replace all jobs in the future economy. While AI and automation will certainly change the nature of work and lead to job displacement in some industries, it will also create new job opportunities in fields such as AI development, data analysis, and cybersecurity. Additionally, there are many jobs that are inherently human and cannot be performed by AI, such as creative jobs, jobs that require emotional intelligence and empathy, and jobs that involve complex decision- making and problem-solving.\n\nAuthor: True or False? In the future economy, AI will have all the jobs in manufacturing white bread.\n\nMuse: True. It is likely that in the future, AI and automation will play a significant role in the manufacturing of white bread and similar products (Figure 13-1). T he repetitive and predictable nature of these tasks makes them well-suited for automation, allowing for increased efficiency and accuracy. However, it’s important to note that while some jobs in the bread- making industry may be automated, there will still be a need for human workers to operate and maintain the machines, manage the production process, and make strategic decisions.\n\n388",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "Chapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-1. A photo of my toaster yielded unpredicted results with the prompt “a robot learning to bake bread” as this collage shows. The AI gave up on me until the very last image in the bottom-right corner.\n\nIterations = 160 unfortunately, but it might just prove my point 389\n\nChapter 13 aI and the Future oF CreatIve Work At least you can plan ahead if you happen to work in bread\n\nmanufacturing. The fact that even an LLM with its vast data sets including access to future employment trends has difficulty with prediction is an indicator of the countless unknown unknowns that will impact humans in years to come. It is likely easier to predict human actions that are repetitive and motoric being replaced. Many creative tasks require very individualized contexts, approaches, and being informed by the training demanded by the discipline. When it comes to some creative jobs, a least one thing is for certain: AI will be not be able to find the nuances to replicate a rond de jambes that a ballerina repeats with infinite variation and muscle control over decades of daily practice no matter how advanced robotics becomes. However, there are some creative tasks that are repetitive and can be automated. The repeatability of some creative acts surfaces two themes that every creative needs to address. Doing so will better inform your own future actions when it comes to the choice of integrating generative AI in your own workflows.\n\nTwo Themes Every Creative Needs to Address\n\nThe first theme is that advances in narrow AI including generative AI surface challenges that human work cultures have been facing for a while, when it comes to assessing the role of creators in their workflow, the creative content they create, and how that content can support workflows, pipelines, brand, or value. Generative AI has not helped the creatives who have already been affected by the rise of the gig economy. Those part-time contract-based gigs have sent a signal that artists from more established creative disciplines like visual art, dance, theater, and music have known for centuries. There is no",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "one solid and secure job for anyone whose dominant way of making a living is to create content for other humans.\n\nWhile we might think that this is not the case for a 3D artist, UX\n\ndesigner, or programmer in a game company, layoffs are cyclic, and my own students and colleagues have been perpetually laid off and forced 390\n\nChapter 13 aI and the Future oF CreatIve Work to move on to other positions. Those creatives that are dependent on contract work are separated from the day-to-day needs and costs of the companies that usually contract them. For some creatives, the danger is in those people that hire them who believe that they can generate their own content with an AI. This may be the case if an employer believes that the content that creatives offer is on par with what an AI generates.\n\nCompanies are already relying on LLMs to write copy and weighing in favor of them to cut costs. Some creatives have lost jobs or contracts because of this. At the same time, after jumping the gun and thinking they can just use ChatGPT or other chatbots to replace employees or contractors, some companies are rehiring or re-contracting creatives when they discover how generic, untruthful, biased, damaging, and inaccurate LLMs can be. This type of seesawing with companies trying to cut costs will continue if the perception continues that generative AI is good enough at simulating human intelligence and creativity. It also speaks to the company culture, its structure, and how employees are treated.\n\nWhile reports of the subject may tell the story of generative AI coming after creative jobs that are nonroutine, it’s important for each of us to articulate those parts of our work that are not. The signal is clear that creatives need to re-evaluate the tasks they undertake and decide which routine ones a future AI might take over. If you are a creative who creates content that is mainly based on popular trends, frequently relies on overused story themes, or produces work that complies rigidly to specific genres, such as popular music styles or classic suspense sequences in television or film productions, then you need to start articulating what your value proposition is compared to a generative AI. Generative AI makes it easy enough for employers to do without contracting creatives.",
      "content_length": 2301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "Content generated from an AI does offer more cost-effective content than, say, employing a scriptwriter who adheres strictly to traditional Hollywood- style scripting or even licensing stock images, footage, music, 3D art, or stories that some creatives contribute to content libraries.\n\n391\n\nChapter 13 aI and the Future oF CreatIve Work The second theme as has been espoused in this book is that even though the threat of generative AI might be present in some disciplines, it also presents an opportunity for innovation when used as a tool that augments your own creative potential. On its own, AI cannot perform this feat autonomously and lacks the vision, craft, technique, and experience that creatives possess. The technology requires human intervention to critically evaluate its outputs and use them as a springboard for challenging the existing norm or questioning prevailing artistic conventions. This approach mirrors the original intention behind the early integration of computers into artistic expression, which were envisaged not as usurpers of creativity but as allies in a shared quest to disrupt and redefine artistic paradigms. Governor-General Literary Award-winning Canadian playwright Kevin Kerr speaks to the necessity of the craftsperson in a different way:\n\nWhat’s intimidating is that the bot has all of Shakespeare and everything ever written about it available to draw from almost instantly, while I have to use my crappy memory and incomplete readings of the texts. But it’s also interesting to note how (currently) the AI is lacking a lot of nuances when it comes to style, and there’s a uniformity of voice that misses Shakespeare’s ability to reflect character in speech. And of course, it’s clear AI doesn’t know the rules around writing a sonnet.\n\nMost creatives who come with a large amount of work experience will find that generated prototypical content needs curating, editing, guiding, refining, manipulating, and at times radical transformation to make the most of it. That will call upon the skills that creatives have developed over many years of experience across different creative industries in specific contexts that are difficult to replicate. Creative impulses and the content that comes from them are prompted by context-specific cases and often involve a lot of back- and-forth between a contractor and a team leading the vision.",
      "content_length": 2380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "392\n\nChapter 13 aI and the Future oF CreatIve Work Integrating AI into Your Workflows\n\nHow generative AI will affect us seems to be more oriented toward its use to support specific tasks and responsibilities that can be repeated. In many creative industries, creatives are still an essential part of production, where decision-making, problem-solving, and managing creative relationships are not easily replaceable by any machine or automated process. That is in part because of the ever-changing nature of productions and their unique demands.\n\nEvery sound design production for live theater, for example, has unique challenges that are dependent on the vision of the director, the script, how much support the actors need, the back-and-forth between a director and the designer, whether actors will need to be reinforced with microphones, the number of cues, the playback system, and budget.\n\nThere are rarely templated production processes across any human creative act because they also account for the unique meeting place for all the personalities that will contribute to the work. What speeds up the compositional process for my own work are automated plugins like equalization that can make minor or major adjustments to the frequencies of different sounds as they play (Figure 13-2). Ten to twenty-four tracks or layers of music can be automated and then rendered within less time it takes to play the whole track. My job as a sound designer and composer is not being replaced, but some of the tasks I had to do on my own in previous years are made easier. This gives me more time to try multiple mixes with radically different equalization for different tracks depending on what system the track is going to be played back on.\n\n393",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "Chapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-2. The author depicted using a photo of an out-of-order mechanical horse ride along with the prompt “racing to meet a deadline.” Iterations = 19\n\nAs creatives, what we might need getting used to is how generated content can inform our individual creative processes and to do that we need to better define what that creative process is. The best next step to take, if you haven’t already, is to break down all the tasks that you undertake in your respective discipline and apply a design thinking tool known as “Day in the Life.” The tool can be applied with any job you are afraid AI will replace. Like the 40 minutes that I recounted in the life of a photographer earlier in the book, it is useful to apply the tool to your own creative tasks by critically taking apart all the human actions required that you think an AI might replace.",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "394\n\nChapter 13 aI and the Future oF CreatIve Work Day in the Life Steps\n\nTask Decomposition: Start by cataloguing all the tasks you perform to create your own content. For\n\nexample, if you’re making a video, this might include\n\nbrainstorming, scriptwriting, filming, editing, and\n\npublishing.\n\nStoryboarding: Utilize a storyboard to visualize\n\nevery step of the process. For instance, in animation,\n\nthis might involve sketching the flow of scenes and\n\ndialogues to provide a clearer understanding of the\n\nstoryline and sequence of actions.\n\nPeer Review: Share your process with a colleague\n\nor friend. This is beneficial as they might spot steps\n\nyou overlooked. If you’re developing a podcast,\n\nfor example, they might notice that you’ve missed\n\nincluding time for audio post-production.\n\nIdentifying Routine Tasks: Categorize tasks that are repetitive and can be performed almost unconsciously.\n\nIn the case of a blog writer, this might include\n\nformatting text or researching relevant keywords",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "for SEO.\n\nAI Research: Investigate what AI technologies currently exist that could automate some of these tasks. For\n\ninstance, a content writer could find AI tools that aid in\n\nkeyword research or grammar correction.\n\nAI Integration: Incorporate any relevant generative AI into your workflow. As a photographer, for example,\n\nyou might utilize AI tools for sorting and basic\n\nphoto edits.\n\n395\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWorkflow Refinement: Adjust your workflow\n\naccordingly and keep a record of it. Consider what\n\ntasks the AI managed effectively. If you used AI for\n\nautomated video editing, for instance, assess the\n\nquality of its output and the time it saved.\n\nIdentifying Gaps: Finally, determine the shortcomings of the AI-generated work. This will aid in demonstrating\n\nhow you incorporate AI in your workflow, what you\n\nneed to do post-generation, and where human input\n\nis still necessary. For example, while an AI might be\n\nable to create a draft for an article, human intervention",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "might still be necessary to add a personal touch, check\n\nfor contextual accuracy, and ensure overall coherence.\n\nDistinguishing Between Art and\n\nDesign Processes\n\nAs you continue to define your own creative process, to understand the impact of generative AI is to also consider what context it might be used within. This book has grouped together artists, designers, chefs, product owners, developers, and others all under the umbrella of “creatives.”\n\nWhile it is true that generative AI can support anyone who is creative no matter what they do, distinguishing its use to support artists vs. designers who work on teams is important. Generative AI can support artists to explore new forms, media, and stages, but many artists do not need the technology to continue to create their work. However, designers of all kinds can leverage generative AI for team-based workflows and pipeline integration. That places new demands on creatives to identify all the tasks they currently engage in and the ones that may depend on other team members to complete (Figure 13-3). Doing so they can better prepare to 396\n\nChapter 13 aI and the Future oF CreatIve Work either integrate generative AI or at least understand how it can help them and other team members they engage with. As an example, in video game development, AI has already been integrated across creative pipelines:\n\nGenerative AI can be used to prototype unique and\n\nimmersive game worlds, levels, characters, items, and\n\nmore. Minecraft and many other video games already\n\nuse procedural generation to create their vast, varied\n\nlandscapes. A benefit is that procedural generation of\n\na game world, for example, can save development time",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "and resources, in the generation of vast game worlds\n\nto be created with relatively small file sizes, since the\n\nenvironments are generated in real time rather than\n\nbeing pre-rendered and stored. But content still has to\n\nbe curated, modified, tweaked, and optimized.\n\nGenerative AI models can be used to create more\n\nbelievable non-playable characters (NPCs) with varied\n\nbehaviors and responses, making the game world feel\n\nmore dynamic and alive. For instance, AI models can\n\nbe trained to generate dialogue or decision-making\n\npatterns for NPCs. Experiments integrating ChatGPT\n\nwithin game environments are already underway,\n\ncreating a richer gameplay experience with NPCs\n\nplayers can converse with.\n\nAI can also help in generating beginning story\n\nelements, quests, or dialogue that can then be refined\n\nby a narrative designer. Tools like ChatGPT can be used\n\nto generate unique plot line ideas based on existing\n\nstories, and these can be vetted with key members of\n\nthe team.",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "397\n\nChapter 13 aI and the Future oF CreatIve Work\n\nGenerative AI algorithms can be used to prototype\n\ngame art assets. For example, AI could generate\n\ndifferent variations of character designs, weapons, or",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "environments, based on the art style of the game.\n\nFigure 13-3. A future worker trying to juggle all the balls being thrown at it and becoming overwhelmed. Iterations = 40\n\nAs you entertain identifying co-dependent tasks you undertake with other team members to understand how generative AI can fit within existing workplace practices, you will benefit from first answering the following:\n\n398\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhich team members would most benefit from\n\nintegrating generative AI into team workflows? In\n\naddition, how much of their time will be required\n\nto practice generating content? Is it your marketing\n\nteam? Does generated code save an experienced\n\nprogrammer time, or is it more time to debug and\n\ncorrect faulty code?\n\nHow is creative work managed over time? This may\n\nlead you to identifying the type of project management\n\nmethodology your workplace uses. Agile sprints,\n\nfor example, allow for an increased flexibility in\n\nprototyping content over short time periods after which\n\nthey are reviewed and iterated upon for subsequent\n\nsprints. Generative AI is a useful complement to Agile",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "processes.\n\nWhat level of experience do your team members\n\nhave with various productions? The variety of work\n\nexperiences on your team will be an asset when\n\nconsidering how to integrate generative AI. You\n\nwill have a broader perspective as to how AI can be\n\nintegrated.\n\nWhat experiments with generative AI have team\n\nmembers engaged in? Answering this question may\n\nlead to necessary workshops and onboarding of\n\nspecific generative AI that you and your team may want\n\nto implement. Team members may or may not have\n\nthe knowledge and know-how to be able to prompt a\n\ngenerative AI with the information required to perform\n\nthe desired task.\n\n399\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhat policies does your team or organization have\n\nwith using AI-generated content? If teams are creating\n\noriginal intellectual property (IP), then they need",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "to understand the company’s position on using any\n\ngenerative AI content as many companies who allow\n\naccess to the content that users generate have specific\n\ncopyright policies in play that need to be respected.\n\nSome, like Midjourney, have specific rules that any\n\ncontent created by users can be used by all users.\n\nHow would work created with generative AI be\n\nreviewed? Answering that question may also lead to\n\nwho on your team would review that work, how it\n\nwould be assessed, and who would be responsible for\n\nits implementation.\n\nTakeaways for Creatives\n\nDefine all the creative tasks that you undertake in\n\nyour current role or in the role you want to have in a\n\nparticular industry.\n\nConduct research as to how any generative AI might be\n\nable to support the tasks you would undertake.\n\nCreate instructions that you think an AI would need to\n\nfollow to prompt something specific that you have the\n\nlived experience and know-how to deliver.",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "Assess how much time it took to create those\n\ninstructions.\n\nDefine how complete the generated content would be\n\nright now, not in six months or five years.\n\n400\n\nChapter 13 aI and the Future oF CreatIve Work\n\nWhat review tasks would you need to undertake to\n\nassess the content that an AI generates, and how long\n\nmight that take?\n\nRun through a mock scenario and time it. See if a\n\ngenerative AI saves time, as the efficiency myth of\n\nusing AI is a common reason that is used to argue for\n\nits use in different industries. Include the time it would\n\ntake to provide the necessary inputs a generative AI\n\nwould need.\n\nGenerative AI provides valuable tools for designers, potentially augmenting team-based workflows and pipeline integration. To harness these capabilities, it’s important for creatives to thoroughly understand how AI can support their tasks and dependencies on teams. That clarity may help facilitate the incorporation of generative AI into their work processes when asked or, at the very least, help them appreciate how this technology can enhance their efficiency and collaboration with others.\n\nFuture AI Jobs Now",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "News that generative AI will disrupt all existing jobs and cause human extinction is overshadowing the reality that there is a growing market opening quickly for future jobs that involve generative AI. A valuable exercise for creatives is to research what those future jobs look like, to see if they might expand their own experience and develop the skills to take them on. Asking my creative companion is a starting place, bearing in mind the limitations it has and its capacity to tell the truth since it doesn’t always have its facts straight.\n\n401\n\nChapter 13 aI and the Future oF CreatIve Work Muse: Sure, I can help with that.\n\nGenerative AI Engineer : An engineer who develops,\n\nimplements, and maintains generative AI models.\n\nThese professionals can work in various industries such\n\nas advertising, gaming, architecture, and more. For\n\nexample, a generative AI engineer in a fashion company\n\ncould develop models that generate new clothing designs.\n\nAI Art Director : This role involves using AI, particularly generative AI, to create visual art for advertising,\n\nentertainment, or digital media. For instance, an AI art\n\ndirector might use AI algorithms to generate art for a\n\ndigital marketing campaign or a video game.\n\nAI Content Creator : These professionals use AI to\n\ngenerate different types of content, such as blog posts,",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "social media updates, or product descriptions. For\n\nexample, an AI content creator could use AI models to\n\ngenerate blog posts on specific topics.\n\nAI Music Composer : AI is increasingly being used in\n\nthe music industry to generate new compositions. An\n\nAI music composer could use generative models like\n\nOpenAI’s MuseNet to compose new pieces of music.\n\nAI Game Designer : These professionals leverage\n\ngenerative AI to create game elements such as\n\nenvironments, characters, storylines, and more. For\n\ninstance, an AI game designer might use procedural\n\ngeneration to create diverse and vast game worlds.\n\nAI Data Scientist (Generative Models) : They build and\n\ntrain generative models on large data sets to generate\n\nnew, synthetic data. This role could be particularly\n\n402\n\nChapter 13 aI and the Future oF CreatIve Work useful in healthcare, where AI data scientists could\n\ngenerate synthetic patient data that preserves privacy\n\nwhile allowing for robust research and development.\n\nAI Architect (Generative Design) : Professionals",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "who use generative AI in fields like architecture or\n\nindustrial design. For instance, an AI architect might use\n\ngenerative design tools to create efficient and innovative\n\nbuilding designs.\n\nGenerative AI Researcher : These professionals conduct\n\ncutting-edge research in generative AI, developing new\n\nalgorithms and models. They might work in academia\n\nor in the research departments of large tech companies.\n\nAI Ethics Officer : With the growing use of AI, ethical\n\nconsiderations are becoming increasingly important. AI\n\nethics officers are needed to ensure that generative AI is\n\nbeing used responsibly and ethically and that it aligns\n\nwith laws and regulations.\n\nAI User Experience (UX) Designer : These professionals\n\nintegrate generative AI into user interfaces and\n\nexperiences. For example, an AI UX designer might\n\ndesign a system that uses AI to personalize the user\n\ninterface based on the user’s behavior and preferences.\n\nConducting more research can reveal other types of jobs that are emerging from the technology and its popularity at the moment",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "(Figure 13-4). Discerning what these positions are, and the training that would be involved to take on these jobs, involves you to undertake online research and particularly when you search any knowledge area and attach the letters A and I after it. Here are a few others to consider conducting my own research:\n\n403\n\nChapter 13 aI and the Future oF CreatIve Work\n\nPrompt engineers or those individuals who are good at\n\ngetting consistent content from a generative AI through\n\nthe craft of using words. If you recall how generative\n\nAI works, however, it is good to understand that every\n\ngeneration of content is unique, and you can’t really\n\npredict if you’ll get an extra finger here or there.\n\nAI content editors are already reviewing what an LLM\n\ngenerates and editing to make it sound more human,\n\nless homogeneous, and more accurate. AI content\n\neditors will not be limited to the generated word, as\n\nthere is a growing appetite for generative images that\n\nneed curating.\n\nAI consultants are inundating web searches with their\n\ngenerated expertise in terms of how to integrate AI\n\nin the workplace. As alarming as this sounds, some",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "good may come out of it because regardless of how\n\nknowledgeable they are, they make us aware that we\n\nmay have to deal with AI as a trend or expected work\n\ntool in our workplace environments.\n\nDiscipline-specific AI curators will be those individuals\n\nwho have gone far with how they integrate generative\n\nAI in their own workflows and can therefore rapidly\n\ngenerate useful content for others in addition to\n\nteaching colleagues how to apply generative AI.\n\nThe position of a machine learning (ML) engineer will\n\nbe predictably on the rise, particularly in IT support for\n\ncompanies and institutions. Organizations rich in data\n\nneed to take command over that data and leverage it\n\nto become the corpus for their own LLM or text-image,\n\nimage-image, text-video, or video-video generative AI.\n\n404",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "Chapter 13 aI and the Future oF CreatIve Work\n\nHint: All the tools, processes, and support are out there\n\nto achieve this on your own. Learning how to do this\n\nin addition to the tasks you already have is a form of",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "future-proofing your position when and if cuts come.\n\nFigure 13-4. A cyborg AI awkwardly pointing to you with the text beneath translated as “We need you.” Iterations =27\n\n405\n\nChapter 13 aI and the Future oF CreatIve Work Acknowledgments\n\nTo those artists and improvisers who have inspired us\n\nto not conform or accept established patterns of art,\n\noffering alternatives to the same old and provoking us\n\nto do the same\n\nTo the current version of our AI-powered muses who\n\ncontinue to astonish and provoke and whose very\n\nexistence brings to the surface pervasive challenges\n\nwe face as a community of humans who engage with\n\nintelligent machines with curiosity, fear, wonder, and\n\nexperimentation (Figure 13-5)\n\nTo the activists and image warriors, coalitions, and\n\npolicy developers who are working hand in hand with AI\n\ndevelopers on what at times seems like a runaway train\n\nTo writers like Kevin Kerr whose skill and craft at\n\nwriting will be needed more than before as striking\n\ncontrast to the lack of nuance and human experience",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "that any AI will only be able to simulate\n\nTo those embracing new opportunities with generative\n\nAI and integrating it more and more in their workflows,\n\nshowing creatives the advantages of doing so\n\nTo employers who are carefully evaluating how generative\n\nAI can support existing workflows and how they can\n\nsupport and guide creatives they contract or current\n\nemployees to best take advantage of these creative tools\n\n406",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "Chapter 13 aI and the Future oF CreatIve Work\n\nFigure 13-5. A final image using Photoshop’s General Fill to create the torso and legs of a creative juggling its own perspectives and ideas about generative AI\n\n407\n\nChapter 13 aI and the Future oF CreatIve Work Impossibly Generated Conclusions\n\nThe frustrating characteristic of generative AI is that it keeps up faster than anyone writing about it. By the time you read this book, new advances will make many of the examples of generative AI that are referenced obsolete.\n\nHow would I have known that “Stable Attribution,” the site devoted to tracking down the images that went into Stable Diffusion and its generated images, would close its doors within months of its discovery. It’s important therefore to understand that there will be generations of AI just like any other prototype, and they will continue to add features over time.\n\nDevelopment teams will continue to claim that content generated from first prompts will start to look better and better, feel more human, and will offer an improved reading experience. More job losses will be announced.\n\nMore mistakes made by generative AI will be cited because there was no human to review the content. While some AI-generated content may already feel like a higher-fidelity prototype, this book maintains that whatever is generated is best regarded as a prototype, a version of your initial prompted idea that you can apply your craft and skills to improve before sharing with the world. In whatever way the technology evolves, keep in mind that as good as it might get, the features that AI offers can always be better.\n\nOne important takeaway that I’ve repeated several times is to consider how and where generative AI fits into your own creative process. As imperfect as initial offerings might be, on the short horizon, generative AI is set to at least offer some improved features like higher image resolution and hands with less fingers. We are already starting to see web portals dedicated to showcasing generated art that creatives use as the first step in their creations.",
      "content_length": 2088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "Creatives on teams across industries are also starting to explore generative AI into their workflows, especially in the beginning phases of a project. Anticipating that there might be an expectation for you to know how to use AI on future creative teams, it may benefit you to integrate generative AI within your own workflows.\n\n408\n\nChapter 13 aI and the Future oF CreatIve Work Will a small minority of humans with bad intentions use generative AI and other AI to do harm to others? Yes. That shouldn’t stop your use of it though, especially if your intent is to offer unique creations that criticize and disrupt established forms and structures. It is a great time to leverage generative AI to create unique work.\n\nThe journey of our interaction with generative AI promises to be an iterative and rewarding one that will continue to provoke and challenge how we prototype. The technology challenges us to stay on our toes: to critically examine the implications of interacting with generative AI and the results of those interactions as prototypes that influence what we share with the world. Bear in mind that many ethical concerns and fears of AI controlling human tasks and roles are still not resolved and need to be.\n\nMy hope is that in reading this book you have increased your awareness of how generative AI works so you can make more informed decisions as to how you might one day use it.\n\nI leave the final words to our muse.\n\nAuthor: How would you end a book on using generative AI as a creative prototyping tool?\n\nMuse: Therefore, since brevity is the soul of wit\n\nAnd tediousness the limbs and outward flourishes,\n\nI will be brief.",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "As we reach the concluding lines of this exploration into generative AI as a creative prototyping tool, we recognize the astounding possibilities it unlocks. The world of design is at an extraordinary juncture, with our creative potential being broadened and deepened by this revolutionary technology. Generative AI will continue to evolve, its capabilities expanding in ways we can only begin to imagine. As creatives, it is incumbent upon us to stay abreast of these developments, to adapt and adopt, to continue pushing the boundaries of what we believe is possible. The power of generative AI isn’t about replacing human creativity—it’s about augmenting 409\n\nChapter 13 aI and the Future oF CreatIve Work it, helping us find novel solutions and innovate in ways that were previously unreachable. It’s an exciting tool in our toolkit, one that offers limitless potential for the brave creative willing to explore its depths.\n\nThis book aimed to demystify generative AI, to guide you through its intricacies, and, hopefully, to inspire you to harness its potential. As we wrap up this journey, it’s up to you to leverage these insights, to experiment, to fail, and to succeed. Remember, it’s your creativity that breathes life into these tools. Generative AI is merely a means to an end—a powerful aide in the relentless pursuit of innovation and excellence in design. As we embrace this new era, remember that your ideas are the driving force behind the change. Your imagination, your inspiration, your artistry—these are the elements that give shape to AI’s potential.\n\nSo, as you turn the final page, don’t view it as an end, but rather a new beginning. With generative AI in your hands, you’re not just a designer—\n\nyou’re Mary Shelley, Ava Lovelace, Joan Clarke, Alan Turing, Sun Ra on the frontiers of creativity. Embrace this challenge with curiosity, discernment, and the courage to create the unimagined.\n\n410\n\nAPPENDIX\n\nImage and Text",
      "content_length": 1946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "Sources\n\nLLMs were used to generate the muse’s text. That generated text was edited, refined, and adapted for all the scripted sequences in every chapter. Text was generated using the popular LLMs known as ChatGPT-3\n\nand ChatGPT-4 along with OpenAI’s Playground. All the text was highly edited for theatrical effect, and some was completely rewritten.\n\nMuse: I thought so.\n\nAll Shakespearean text was written using the corpus of Shakespeare’s plays and sonnets through a combination of ChatGPT-4 and plays directly adapted from Cleopatra, MacBeth, Othello, The Merchant of Venice, Romeo and Juliet, and A Midsummer Night’s Dream. The Droeshout image of Shakespeare was accessed through the University of British Columbia Library’s copy of Shakespeare’s First Folio.\n\nApart from using a handful of public domain images confirmed\n\nthrough archival websites and public domain sites requiring no\n\nattribution, most of the figures used in this book were generated from sources of the author’s own photos, bad drawings, and visual models.\n\nThese were used in image-to-image generative AI with basic text prompts,\n\n© Patrick Parra Pennefather 2023\n\n411\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3\n\nAPPENDIX ImAgE AND TEXT SourcES",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "leveraging the platform’s styles/filters and other features to radically transform them beyond recognition in all cases. Stable Attribution and months of photo hunting brought to the attention of the author some specific images whose essential parts were too close to existing sources.\n\nThese were not included in the book and deleted from the author’s computer. At least a dozen different AI were experimented with, applying platform- specific filters. These were further edited with added objects, color corrected, and further refined using licensed versions of Photoshop, Photoleap, Snapseed, and Mixlr. Neural filters from Photoshop were used extensively. Real photos in the text were all taken by the author.\n\nSome exceptions include generated images that were meant to\n\nillustrate basic concepts and draw comparison between text-image platforms, particularly in Chapter 8, to show differences in how images were generated with the exact same prompts. Those images were created in Stable Diffusion, DALL-E 2, and Midjourney. Some were sent through AI filters, and a few were modified in DALL-E 2 to show the technique of outpainting. Other generated images were the result of prompting Scribble Diffusion with an original sketch. Any images that underwent a small number of regenerations were used to illustrate the functionality of these platforms and were intentionally not rendered or upscaled in high fidelity.\n\nPrompts were mainly original with some common words generated\n\nin natural language models and others modified from open sharing sites. Complex prompts were found on various web portals and meant to show how elaborate some prompts can be. No prompts were paid for or licensed.\n\nWalk Through Figure 2, FrankenAI Lab\n\nThe original prompted image was a photo taken by the author to capture windows and a particular perspective of a room imagined to be a scientific lab.\n\n412",
      "content_length": 1891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nThe photo in Figure A-1 was uploaded to an image-to-image AI with the prompt “reverse-dolly-zoom-of-a scientific-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and-white-still-digital,” and the result was four variations as in Figure A-2. Reducing the influence of the prompt photo will generate more interesting results, and increasing the values of how much the AI responds to the text prompt will also make a difference. In this case the prompt guidance was set to just over 50% and the influence or strength of the prompt image to around 30%.\n\nFigure A-1. Original photo of the author’s office using an iPhone 413",
      "content_length": 665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-2. Four images generated from the prompt “reverse-dolly-zoom- of-a scientific-lab-with-lots-of-windows-microscopes-and-lab-equipment- black-and-white-still-digital.” Total iterations = 56\n\nFrom a total of the four selected images that were generated in Figure A-2, the thumbnail in the bottom right was chosen as it added more windows and a wider zoomed-out perspective to increase the size of the space compared with the original photo.\n\n414",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-3 prompted the next generated images setting the strength of the prompt image on the generated one at 30% and setting the number of steps to generate a higher-quality image at 97. In addition, I added the keywords “Frankenstein, electricity, and misty” to the prompt.\n\nFigure A-3. The bottom-right image in Figure A-2 i s used to prompt the next set of four generated images along with the text prompt\n\n“reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-electricity-misty.”\n\n415",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFrom the four generated images in Figure A-4, the thumbnail on the bottom right was chosen particularly because of the combination of top window, mist, and wiring not previously present in Figure A-3. At the same time, the previous windows and the size of the lab were preserved, adding a small library and a lower-right window, creating a more surreal effect.\n\nThe lack of electricity in the images had to with their weight in the prompt, which I would come to experiment with multiple times in the iterative process.\n\nFigure A-4. Prompt: “reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots- of-windows-microscopes-and-lab-equipment-black-and white-electricity- misty”",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "416\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nConsidering the lack of electricity in Figure A-5, I worked with the text prompt to prioritize it in the next round of generated images. In addition, now that the room size and perspective were working, I could remove the words “reverse dolly zoom.”\n\nFigure A-5. Prompt: “reverse-dolly-zoom-of-Frankenstein’s-lab-with-lots- of-windows-microscopes-and-lab-equipment-black-and white-electricity- misty”\n\nFrom the eight generated images in Figure A-6, the bottom-left thumbnail had all the elements and expanded the bottom windows. That image was then chosen as the next prompt for the AI (Figure A-7).",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "417",
      "content_length": 3,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-6. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty”\n\n418\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-7. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-bones-profane fingers-scraggy-limbs”\n\nThe only thing missing from my lab were body parts if I stayed true to Mary Shelley’s novel. Even though I wasn’t sure if I wanted them in the image, the next generated images added words from Chapter 4 of",
      "content_length": 568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "Frankenstein to the prompt as in Figure A-8. The results were a bit over the top and grotesque, which reminded me of why I was creating this image.\n\nIn the case of a critical how-to book aimed at creatives, the intent of the final generated image was to show the process and use Frankenstein’s lab as metaphor for a reader’s own generative AI lab. The next series of four figures showed an iterative attempt to change the prompt text using Figure A-7 as the prompt image.\n\n419\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-8. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-bones-profane fingers-scraggy-limbs”",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "Since all the images in Figure A-8 were scary and grotesque, I used the same image in A-7 but refined the prompt to eliminate the words “bones, profane fingers” but kept the “scraggy limbs” in the prompt (Figure A-9).\n\n420\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-9. An attempt to regenerate by subtracting some body parts. Prompt: “electricity-Frankenstein’s-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-scraggy-limbs”\n\nExcept for the super-creepy top-left image in Figure A-10, I was getting closer to a look that didn’t have to be scary like Frankenstein’s lab. In fact, the reference to Frankenstein was next on the subtraction block (Figure A- 11).",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "421\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-10. Substituting scraggy limbs with the word “specimens.”\n\nPrompt: “electricity-Frankenstein’s-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-specimens”\n\n422",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-11. Making the word “specimens” singular. Prompt:\n\n“electricity-lab-with-lots-of-windows-microscopes-and-lab-\n\nequipment-black-and white-misty-specimen”\n\nFigure A-11 demonstrated the potential for any of these images to be close to the lab that I was starting to imagine. However, somehow the original perspective and size of the lab were lost, so an iteration of four images was generated reverting back to Figure A-7 and using the same text prompt as Figure A-11 but substituting the word “specimen” for the words\n\n“brain in a bottle” (Figure A-12).\n\n423",
      "content_length": 598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-12. The prompt “electricity-lab-with-lots-of-windows- microscopes-and-lab-equipment-black-and white-misty-brain in a bottle,” combined with Figure A-7 as the pr ompt image, resulted in the AI generating our windows back\n\nThe images in Figure A-12 tended to be less grotesque, but there was little color as these were generated using a black-and-white style applied through playground.ai. To bring some color into the next batch of generated images, a new style was applied to the same prompt using Figure A-7. Figures A-13 and A-14 provided us with some blues to not make the lab so ominous.\n\n424",
      "content_length": 638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-13. Prompt: “electricity-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-brain in a bottle” with a new style filter\n\n425",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-14. Prompt: “electricity-lab-with-lots-of-windows-microscopes- and-lab-equipment-black-and white-misty-brain in a bottle” with another new style filter using playground.ai\n\nThe bottom-right image of Figure A-14 added a bit of dimension to the lab that made certain elements stand out, so this image was chosen, and then a variation of the image was rendered using another style that could be applied while the new image was being generated. The resulting Figure A-15 revealed the complexity of the style that was applied to generate the image.\n\n426",
      "content_length": 590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-15. This shows the complexity of the full prompt when adding a specific style available through a number of generative AI platforms. Notice the references to other artists that our final prompt would eliminate\n\nWhile the generated image is interesting, the bottom-right thumbnail from Figure A-14 was chosen to represent the AI lab.\n\nWhile Figure A-16 was a wonderful result from where the journey started using an old photo of an empty office, somehow the electricity and sparks went missing from this generated version. Our final image then was generated by adding the text “electricity sparks and lightning” at the beginning of the prompt to ensure it showed up in the generated image (Figure A-17).",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "427\n\nAPPENDIX ImAgE AND TEXT SourcES\n\nFigure A-16. Generated image for the prompt “electricity -lab-with-lots-of- windows-microscopes-and-lab-equipment-black-and white-misty-brain in a bottle” seems to missing some electricity\n\n428",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-17. The AI brought back the electric sparks to the image by adding “electricity-sparks-lightning” at the beginning of the prompt\n\n“lab-with-lots-of-windows-microscopes-and-lab-equipment-black-and white-misty-brain in a bottle.”\n\nLastly, Figure A-17 was manipulated using Photoshop’s neural filters to generate slightly more haze, and some subtle color was added to the image. Figure A-18 now represents the final version of the figure, which appears in the Foreword of the book (Figure 2).\n\n429",
      "content_length": 536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "APPENDIX ImAgE AND TEXT SourcES\n\nFigure A-18. Figure A-17 was opened up in Photoshop, and various neural filters were applied to generate the final version of our photograph. The preceding image is then used in the Foreword of the book (Figure 2)\n\nWhile not all images in the book go through the exact same workflow as described in this appendix, the process of generating an image iteratively was similar for many images in the book. More workflows will be demonstrated on the website that accompanies the book available through a QR code that appears at the end of the Introduction (Figure 6).\n\n430\n\nIndex\n\nA",
      "content_length": 610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "Application Programming\n\nInterfaces (APIs), 334, 339\n\nAARON, 52\n\nArtificial creativity, 46\n\nAcademic community, 350\n\nArtificial General Intelligence, 3\n\nAccelerated prototypical\n\nArtificial intelligence (AI), 17, 30,\n\nworkflows, 339\n\n60, 182, 183, 193, 284,\n\nAgile sprints, 399\n\n302, 318\n\nAI companion, 99\n\nadversarial, 281, 283\n\nboundary-pushing, 101\n\nalgorithms, 317\n\ncollaboration, 101\n\narchitect, 403\n\npatterns, 101\n\nart director, 402",
      "content_length": 438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "risk-taking, 101\n\nart generation styles, 255, 256\n\nspeeding, 101\n\nBread-making industry, 388\n\nstyles, 101\n\nchatbot, 350\n\nAI in film industry\n\nconsultants, 404\n\nbehavioural simulation, 382\n\nContent Creator, 402\n\ncombinatorial probability, 382\n\ncontent editors, 404\n\nmultiple intelligent systems, 383\n\ncould, 398\n\nOllie Rankin, 381\n\nData Scientist, 402\n\nrealistic-looking digital\n\ndevelopers, 129\n\ncharacters, 382\n\nethical futures, 335",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "Weta, 381\n\nEthics Officer, 403\n\nworkflows, 381, 382\n\nfilters, 412\n\nAlgorithmic creativity, 54\n\nGame Designer, 402\n\nAlienation effect, 275\n\ngenerated content, 188, 278,\n\nAnalytical Engine, 6\n\n292, 300, 339\n\nAnimation, 128, 160, 242, 356, 383\n\ngenerated images, 250\n\n© Patrick Parra Pennefather 2023\n\n431\n\nP. Parra Pennefather, Creative Prototyping with Generative AI, Design Thinking,\n\nhttps://doi.org/10.1007/978-1-4842-9579-3\n\nINDEX\n\nArtificial intelligence (AI) ( cont.)\n\nB\n\ngenerated prototypes, 366",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "Battle choreography technique, 381\n\ngenerated speech, 343\n\nBehavioral simulation, 383\n\ngenerated submissions, 333\n\nBias Monster, 71\n\ngenerated talking heads, 362\n\nBill Zhao, 375, 385\n\ngenerated works, 314\n\nBiomedical communication, 350\n\nlanguage model, 349\n\nBisociation, 113, 115\n\nMusic Composer, 402\n\nBlockchain technology, 263, 264\n\npowered muses, 406\n\nBlue Shadows on the Trail, 358\n\nreader, 102\n\nBrain building technique, 381\n\ntechnology, 328\n\nBuilding blocks\n\nText Classifier, 311, 312",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "addition, 154, 155\n\nUser Experience (UX)\n\nprompt, 146\n\nDesigner, 403\n\nprompts, 177, 178\n\nArtificially intelligent programmed\n\nprototyping, 147, 148\n\nsystems, 8\n\nresponse, 176\n\nArtmaking, 28, 49, 97, 235\n\nsubstitution, 158\n\nAudio-based prompts, 241\n\nsubtraction, 154–156\n\nAugmentation, 163, 164\n\ntransposition, 166–175\n\nAugmented reality (AR), 122,\n\n233, 345\n\nAutomated creativity management\n\nart vs. design\n\nC",
      "content_length": 409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "processes, 396–400\n\nChain prompting, 200, 204, 262\n\nbread and washing\n\nChatGPT, 391\n\nmachines, 388–390\n\nChatGPT-3, 239, 242, 300, 346, 350,\n\nDay in the Life\n\n351, 411\n\ntools, 395, 396\n\nChatGPT-3-generated text, 311\n\nintegrating AI and workflows,\n\nChatGPT-4, 242, 264, 268, 344, 346,\n\n393, 394\n\n375, 411\n\nthemes, 390–392\n\nChinese broadcast company, 345\n\nAward-winning innovations, 346\n\nClaudia’s Summary Statement, 350\n\n432\n\nINDEX",
      "content_length": 429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "Coalition for Content Provenance\n\nCreative impulses, 103, 247, 392\n\nand Authenticity\n\nCreative process\n\n(C2PA), 312\n\nadaptive learning, 104\n\nCo-dependent tasks, 398\n\nconsistency, 105\n\nCoherent thesis statement, 352\n\nidea generation, 104\n\nCombinatorial probability, 382, 384\n\ntime efficiency, 104\n\nCommunity-based initiatives,\n\nvariety of styles, 104\n\n377, 378\n\nCreative prompting, 239–241\n\nComposite-generated image, 308\n\nCreatives, 396, 406, 408\n\nComposition, 184, 274, 284,\n\nCreatives in education",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "335, 393\n\ncatching AI untruths, 346–351\n\nComprehensive Test Ban Treaty\n\nintegrating ChatGPT, critical\n\nOrganization (CTBTO), 353\n\nstudies, 351, 352\n\nComputational creativity, 54,\n\npublic understanding,\n\n285, 344\n\nml, 352–354\n\nComputational intelligence, 45\n\nCreatives in industries\n\nComputational workhorse, 354\n\nAI-generated talking\n\nComputergrafik, 264\n\nheads, 362–365\n\nComputer science, 6, 66, 353\n\nAPIs and local networks\n\nContent Authenticity Initiative\n\nintegration, 373–376",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "(CAI), 312\n\nconcept art, animation, 356–361\n\nContent generation, 300, 329\n\nfact-checking code, 366–373\n\nContent moderation, 374\n\nCreative teams, 9, 335, 408\n\nContrastive Language-Image\n\nCrowd replication approaches, 382\n\nPre-training (CLIP),\n\nCryptocurrencies, 263\n\n201, 205\n\nCybernetics, 46\n\nControversial start-ups, 313\n\nCyborg AI, 405\n\nConversational and authoritative\n\nCyborg generated model, 322\n\ntone, 350\n\nConversational prompting, 199\n\nD\n\nCopyright infringement, 300",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "Copyright Infringer, 81\n\nDALL-E 2, 164, 204, 205, 211–213,\n\nCost-effective content, 391\n\n221, 285, 286, 412\n\nCounterfeiting GAN, 264, 266\n\nData management, 374\n\n433\n\nINDEX\n\nDay in the Life tool, 394\n\nEthical dilemmas, 122, 207, 300,\n\nAI integration, 395\n\n314, 341\n\nAI research, 395\n\nExploratory prompting, 200\n\nidentifying gaps, 396\n\nidentifying routine tasks, 395\n\nF\n\npeer review, 395\n\nstoryboarding, 395\n\nFake Content Generator, 74",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "task decomposition, 395\n\nFew-shot stimulation, 199\n\nworkflow refinement, 396\n\nFidelity, 99, 125, 160\n\nDeep fakes, 276, 295, 325, 326\n\nFinancial and social capacity, 300\n\nDeforum, 375\n\nFive Why’s exercise, 21, 23, 24\n\nDescriptive prompts, 236–238\n\nForms and structures\n\nDevelopment teams, 9, 408\n\nAI, 182\n\nDiffusion, 212\n\ncombining and manipulating\n\nDigital art, 54, 263\n\nelements, 183\n\nDigital art, canvas, 250, 251\n\ncreatives, 182\n\nDigital artworks, 55\n\noptimization, 183",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "Dilemmas, 299–301, 304, 335, 336\n\npattern recognition, 183\n\nDiminution, 165\n\nFrankenAI, 327, 328\n\nDiscipline-specific\n\nFrankenAI Lab\n\nAI curators, 404\n\nimage-to-image AI, 413\n\nDomain images, 411\n\nprompt text 1, 414\n\n“Do Not Train” tags, 312\n\nprompt text 2, 415\n\nDoujinshi, 257–261\n\nprompt text 3, 416\n\nDystopian future\n\nprompt text 4, 417\n\nscenarios, 355\n\nprompt text 5, 417, 418\n\nprompt text 6, 419\n\nprompt text 7, 420",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "E\n\nprompt text 8, 421\n\nEmotion matrix, 383\n\nprompt text 9, 421, 422\n\nEmployment trends, 390\n\nprompt text 10, 423\n\nEnigma machine, 45\n\nprompt text 11, 424\n\nEqualization, 154, 393\n\nprompt text 12, 425\n\nError handling, 374\n\nprompt text 13, 426\n\n434\n\nINDEX\n\nprompt text 14, 427\n\nart generation styles, 255, 256\n\nprompt text 15, 427, 428\n\naudio, 152\n\nprompt text 16, 429\n\ncomposers, 145",
      "content_length": 381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "prompt text 17, 430\n\ncontent, 306\n\n“Frankenstein, electricity, and\n\ncreative activity, 20\n\nmisty”, 415\n\ncreative process, 10\n\nFrankentoddlers, 82\n\ncreative prototyping tool, 409\n\nFunctional prototype, 129, 131\n\ndancers, 145\n\nFuture AI Jobs\n\nengineer, 402\n\nAI Art Director, 402\n\nexamples, 20\n\nAI Content Creator, 402\n\nhuman-generated headlines, 18\n\nAI Data Architect, 403\n\nillusion, 10\n\nAI Data Scientist, 402\n\ninnovation and excellence, 410",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "AI Ethics Officer, 403\n\nmimesis, 47\n\nAI Game Designer, 402\n\nmusicians, 145\n\nAI Music Composer, 402\n\nnegative headlines, 18\n\nGenerative AI Engineer, 402\n\npositive headlines, 17\n\nGenerative AI Researcher, 403\n\npatterns, 300\n\nUX Designer, 403\n\nprototypes, 335\n\nFuzzy logic, 382, 384\n\nresearcher, 403\n\ntechnology, 10\n\nG\n\ntext-image, 152\n\ntext-music, 41\n\nGame companies, 376\n\ntext-to-music, 41",
      "content_length": 388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "Generated AI response, 303\n\nvisual artists, 145\n\nGenerated prototypical\n\nwriting, 251–253\n\ncontent, 392\n\nGenerative art, 49, 50, 263, 266\n\nGenerated stories, 16\n\nGenerative fill, 340\n\nGenerative adversarial art, 283, 284\n\nGenerative NFTs, 263, 264, 283\n\nGenerative adversarial networks\n\nGenerative Pretrained Transformer\n\n(GANs), 256, 263, 264,\n\n(GPT), 163, 353, 355\n\n266, 282–285\n\nGenerator, 283\n\nGenerative AI, 7, 8, 10, 182, 183,\n\nGesture prompts, 242\n\n185, 235, 249, 268, 287, 292\n\nGig economy, 390",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "435\n\nINDEX\n\nGPT-4, 236\n\nImage-image substitution, 358\n\nGPT-4 API, 373–374\n\nImage-to-image generative AI, 411\n\nGraphic novel mashups, 257–261\n\nIndigenous researchers, 336\n\nGreedy Capitalist, 75\n\nIndividual creative processes, 394\n\nInstructional prompting, 199\n\nH\n\nIntegration, 374\n\nIntellectual property (IP), 400\n\n“Half-tone” technique, 348\n\nIntelligent machines, 4, 6\n\nHallucinations, 149, 314, 317, 324\n\nancient musical robot\n\nHallucinator, 72\n\nbands, 35–37",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "Higher-resolution prototype, 124\n\nautomata, 32–34, 38, 39\n\nHigh-fidelity (hi-fi) images, 122\n\nCamera, 28\n\nHigh-fidelity prototypes, 125, 131\n\nComputer, 29\n\nHuman intelligence, 6, 30, 32,\n\nFilm, 29\n\n46, 66, 328\n\ngames, 56, 58\n\nHuman intervention, 9, 19, 129, 392\n\nhuman creative, 32\n\nHuman-like AI, 59\n\nInternet, 29\n\nHuman-like communication, 19\n\nPaint, 28\n\nHuman-like creativity, 32\n\nPaper, 28\n\nHuman-like robots, 277\n\nPrinting Press, 28",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "Hyena Algorithm, 356\n\nStone Tools, 28\n\nHyena Algorithm developed by\n\nSynthetic Paint, 29\n\nStanford, 354\n\n3D printing, 29\n\nHyperreality, 47\n\nWriting Instruments, 28\n\nIntelligent NPCs, 58, 59\n\nI\n\nIntelligent systems, 384\n\nInteractivity, 55\n\nImage analysis, 354\n\nInterdisciplinary research, 353\n\nImage-based generative AI, 290\n\nInternational Atomic Energy\n\nImage-based prompts, 242\n\nAgency (IAEA), 353\n\nImage-image AI, 96, 98, 175, 249,\n\nIn-the-moment improvisation, 329",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "251, 257, 261, 269, 365\n\nIteration, 160\n\nImage-image generative AI, 44,\n\nIterative process, 30, 55, 176,\n\n106, 169, 217, 293\n\n204, 416\n\n436\n\nINDEX\n\nJ\n\nMachine learning (ML), 58, 67, 92,\n\n248, 290, 303, 307, 311, 317,\n\nJob displacement, 388\n\n320, 321, 336, 384, 404\n\nJob opportunities, 388\n\nMachine Visions, 21\n\nJob (Re)placement, 328–332\n\nMany-shot prompting, 199\n\nMarginalized groups, 321\n\nK\n\nMasher-Upper, 76",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "Know-it-All Tutor, 79\n\nMashups, 306, 315\n\nKnowledge translation, 354\n\nAI art generation styles, 255, 256\n\ncomic supervillains to musical,\n\n262, 263\n\nL\n\ncounterfeiting GAN, 264, 266\n\nLanguage models, 93, 322, 323\n\nDoujinshi, 257–261\n\nLarge language models (LLMs),\n\ngenerative AI, 248\n\n197, 198, 201, 216, 239, 268,\n\ngenerative NFTs, 263, 264\n\n287, 353, 355\n\ngoal, 247\n\nLate-stage prototypes, 121\n\ngraphic novel, 257–261\n\nLearning management\n\nmachine learning model, 249",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "systems, 376\n\nmedia, 247, 249\n\nLGBTQ+ community, 321, 322\n\nprompt-based magic, 247\n\nLiar, 73\n\nMasks, 159, 340\n\nLinear algebra pattern\n\nMasterpiece Studio, 344\n\nmatching, 353\n\nMechanical inventions, 43\n\nLittle Curie, 347\n\nMidjourney, 204, 205, 213–218, 220,\n\n“Living with Machines” project, 377\n\n222, 287, 293\n\nLLM-generated content, 335\n\nMid-stage prototypes, 121\n\nLower-fidelity prototypes, 125, 128\n\nMimesis, 47\n\nLow-fidelity, 126\n\nMinecraft, 397",
      "content_length": 448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "Low-poly render, 305\n\nMiniature automata, 33\n\nMiniature X-ray apparatus, 347\n\nModern computational fortune-\n\nM\n\ntellers, 4\n\nMachine intelligence, 32, 45,\n\nMontreal Institute for Learning\n\n48, 56, 384\n\nAlgorithms (MILA), 354, 356\n\n437\n\nINDEX\n\nMotion capture, 324, 344, 382\n\nO\n\nMulti-modal AI nodes, 375\n\nOff-the-Rail Sitting Duck, 80\n\nMultiple machine learning models,\n\nOpenAI’s API, 377\n\n204, 339, 365\n\nOpen source code repositories, 373",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "Multiple variations, 150\n\nOptimization, 183\n\nMuses, 91, 93\n\nOutpainting technique, 163, 412\n\nOverused story themes, 391\n\nN\n\nNarrative arcs, 351\n\nP, Q\n\nNarrative designer, 397\n\nPaper prototyping, 122, 125, 126\n\nNarrative prompts, 235–238\n\nPattern-matching technique, 49\n\nNarrow AI, 1, 83, 332, 356, 381, 390\n\nPattern recognition, 183, 198\n\nNatural language models, 92, 412\n\nPay-the-machine model, 41\n\nNatural language processing, 49,\n\nPaywall Guard, 70\n\n235, 290, 354, 356\n\nPersistent myths, 301, 302",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Near-instantaneous\n\nPersonas, 65, 82, 91, 242\n\nresponses, 351\n\nactivities, 84, 87\n\nNeedle method, 348\n\nAI agents, 83\n\nNegative prompts, 106, 165,\n\nPhotoleap, 412\n\n171, 293\n\nPhotoshop, 67, 154, 214, 246,\n\nNemesis, 68, 161\n\n340, 412\n\nNeural filters, 178, 220, 340, 412,\n\nPipeline integration, 396, 401\n\n429, 430\n\nPlatform-specific filters, 412\n\nNeural networks, 92, 197, 307\n\nPlayer pianos, 41, 42\n\nNon-Fungible Tokens (NFTs), 263\n\nPost-war systems engineering, 46",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "Non-playable characters\n\nPrototyping, 103\n\n(NPCs), 57, 397\n\nPractical electromagnetic wave\n\nNot-for-profit Thaumazo, 377\n\npropagation problems, 353\n\nNot Safe For Work (NSFW),\n\nPre-programmed intelligent\n\n325, 326\n\nmachines, 35\n\nNSFW image generation, 326\n\nProgrammable node-graph\n\nNVIDIA, 273, 376\n\ninterface, 375\n\n438\n\nINDEX\n\nPrompt engineers, 199, 404\n\nStable Diffusion V.1.5,\n\nPrompts, 199, 412\n\n209, 210",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "case sensitivity, 200\n\nStable Diffusion V.2.1, 210\n\nchain, 200\n\nweight, 200\n\nconversational, 199\n\nzero-shot cognitive, 199\n\ncreative, 239–241\n\nPrototype, 408, 409\n\nDALL-E 2, 211–213\n\nPrototyping environments, 376\n\ndescriptive, 236–238\n\nPrototyping process\n\ndescriptive language, 201\n\ndefinition, 110\n\nengineer, 199\n\nexperimentation, 119–121\n\nexclusions, 201\n\ngenerative AI, 132, 135\n\nexploratory, 200\n\ngenerative testing, 140, 141",
      "content_length": 430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "few-shot, 199\n\niterative play, 115\n\nimage-image\n\nphases, 121, 122\n\ngeneration, 219\n\nrapid generative AI, 137, 139\n\ninstructional, 199\n\nrefinement, 117, 118\n\niterative process, 206\n\ntext-image, 111\n\nlength, 200\n\nPublic/private entities, 8\n\nmany-shot, 199\n\nMidjourney, 213–218\n\nR\n\nnarrative, 235–238\n\nnegative prompt, 201\n\nRadical transformation, 300, 392\n\norder, 200\n\nRadiography techniques, 348",
      "content_length": 394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "parameters, 200\n\n“Radiology in the Third Reich: The\n\npersona, 201\n\nLegacy of Fritz Lickint”, 347\n\nReddit and copy, 207\n\nRandomness, 384\n\nrules, 201\n\nRandom number generators,\n\nrules, play, 202, 203\n\n382, 384\n\nsequence, 199\n\nReading headlines, 352\n\nStable Diffusion V.2.1, 207, 208\n\nRecurrent neural network\n\nsubject, 200\n\n(RNN), 343\n\nsubscription-based\n\nRepeatability, 390\n\nmonetization schemes, 206\n\nReplika chatbot, 378",
      "content_length": 421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "third-party AI\n\nResolution, 124\n\n439\n\nINDEX\n\nRotoscoping, 332\n\nT\n\nRough prototypes, 10, 160\n\nTeam-based workflows, 396, 401\n\nS\n\nTechnical language, 264, 314\n\nTechnology development, 313, 375\n\nSaturated AI ecosystem, 333, 334\n\nTemplated production process, 393\n\nSci-fi pulp romance story, 362\n\nText-based prompt, 197, 241–244\n\nScribble Diffusion, 122, 242, 412\n\nadd context, 228, 229\n\nSearch engine, 84, 304\n\nadd words, 226, 228\n\nSelf-improving AI, 2",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "AI system types, 204\n\nSemantic differences, 304\n\nbenefits, 204\n\nService- and resource-based\n\neliminate artists, 223, 224\n\nindustries, 340\n\nemphasis, weight, simplicity,\n\nShapeshifting robots, 66\n\n230, 231\n\nShutterstock, 376\n\nimage-generating AI, 204\n\nSketch-to-code prompting, 242\n\nprocess, 204\n\nSnapseed, 412\n\nrecraft, 231\n\nSound design production, 393\n\nsubstitute words, 224, 226\n\nSpanner’s invention, 346\n\nText-code, 242\n\nSpanner’s x-ray unit, 347",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "Text generation model, 362\n\nSpecialized skills, 387\n\nText-image AI, 50, 87, 204\n\nStable Attribution, 309, 313, 408, 412\n\nText-image AI prototype, 111\n\nStable Diffusion, 193, 204, 205, 212,\n\nText-image generative AI, 31, 50,\n\n219, 243, 285, 286, 309, 336,\n\n155, 176, 186, 189–192, 204,\n\n375, 408\n\n281, 363\n\nStable Diffusion–generated image,\n\nText-image platforms, 412\n\n307, 309\n\nText-speech AI, 85\n\nStandard 52-card deck, 366\n\nText-speech generative AI, 364, 365\n\nState machine, 382\n\nThe 42 Judges project, 377\n\nStereotypes, 295, 320, 326",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "The Lord of the Rings trilogy, 381\n\nStochastic parrot, 318–320\n\nThe Matrix, 327\n\nStyle transfer, 268\n\nThe Modern Prometheus, 327\n\nSubstitution, 146, 158, 358\n\nThe Terminator, 327\n\nSuper-realistic human robot, 278\n\n3D artist, 233, 390\n\n440\n\nINDEX\n\n3D image prototype, 162\n\nrobotics, 273\n\n3D-printed mechanism, 132\n\nsimulate human, writing, 287,\n\nTimeboxing, 105\n\n289, 290\n\nTokens, 198\n\nStable Diffusion, 285, 286\n\nTraditional Hollywood-style",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "Unity-based game, 340\n\nscripting, 391\n\nUnity 3D game engine, 373\n\nTransformer algorithm, 355\n\nUnsupervised learning model, 285\n\nTransposition, 166–175\n\nTurboSquid, 376\n\nV\n\nVariations, 148, 152, 153\n\nU\n\nVideo game development, 56, 397\n\nUncanny AI, 276–279\n\nVideo-generating teaching\n\nUncanny Machine, 77\n\ntool, 362\n\nUncanny valley\n\nVideo-video generative AI, 343, 404\n\nadversarial AI role, 281, 283\n\nVirtual paper prototypes, 127\n\nAI-generated content, 292",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "Virtual reality (VR), 345\n\nArts, 274, 275\n\nVoices, 320, 321\n\ncreativity, 296\n\nVoyager, 54\n\nDALL-E 2, 285, 286\n\ndeep fakes, 295\n\nW\n\ndeep learning techniques, 273\n\ndefine, 272\n\nWeb-based animation, 356\n\ndefinition, 271\n\nWebsites, 344\n\ndeformed/distorted images, 296\n\nWhiteboard and prompt, 242\n\ngenerative adversarial art,\n\nWizard, 78\n\n283, 284\n\nWizard of Oz prototypes, 129, 131\n\nhistorical precedence, 279, 280",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "Work experiences, 399\n\nimperfection, 281\n\nWorkflows vs. pipelines, 340–342\n\nlanguage models, 274\n\nWorkflow Types\n\nmachine learning model, 290\n\naudio book and podcasts, 343\n\nMidjourney, 287, 293\n\naudio creations and design,\n\nnegative prompts, 293\n\n343, 344\n\nNVIDIA’s StyleGAN2, 273\n\nimage, 342, 343\n\n441\n\nINDEX\n\nWorkflow Types ( cont.)\n\nX, Y\n\nspoken/sung characters, 345\n\nxR human models, 344\n\ntext, 342",
      "content_length": 402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "video, 343\n\nwebsites, 344\n\nZ\n\nxR human models, 344\n\nZero-shot cognitive prompting, 199\n\n442\n\nOceanofPDF.com",
      "content_length": 107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Document Outline\n\nTable of Contents About the Author About the Technical Reviewers Acknowledgments Foreword Terminology Introduction Chapter 1: Generating Creativity from Negativity\n\nDifferentiating Between Narrow and General AI\n\nMore Relevant Historical Contributions to Computer Science\n\nTech Is Bad… AI Is Bad Tech Is Good… AI Is Good Reconciling the Hype and the Vilification of AI\n\nPositive Headlines Negative Headlines Human-Generated Headlines\n\nCreative Activities to Try While Skynet Thrives\n\nFive Why’s\n\nAcknowledgments References\n\nChapter 2: Being Creative with Machines\n\nIntelligent Machines\n\nSimulating Human Creative Intelligence Automata Ancient Musical Robot Bands Key Takeaways for Creatives Automata That Wrote and Drew Key Takeaway for Creatives\n\nWhy Play the Piano When the Piano Can Play the Piano?\n\nKey Takeaways for Creatives Machine Intelligence in the Twentieth Century\n\nSimulated Patterns and Patterns of Disruption",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Generative Art Key Takeaways for Creatives\n\nMachine Intelligence and Games\n\nIntelligent Machines with Names Creative Activities to Try Based on This Chapter\n\nAcknowledgments References\n\nChapter 3: Generative AI with Personalities\n\nThe Personas of AI Prompting the AI Persona\n\nAI Agents Behind the Curtain Activities to Try Based on This Chapter\n\nAcknowledgments\n\nChapter 4: Creative Companion Generative AI as Muse Creativity, AI, and You Understanding Your Own Creative Process\n\nKey Takeaways for Creatives\n\nRoleplaying with Your Muse Accelerating Your Creative Process with AI Activities to Try Based on This Chapter\n\nChapter 5: Prototyping with Generative AI\n\nAsking a Prototype What It Is Aha Moments with Generative AI Mechanics of Prototyping\n\nPrototyping Tasks Us to Iterate Prototyping Asks for Persistent Refinement Prototyping Requires Experimentation\n\nPrototyping Phases\n\nFidelity and Resolution of a Prototype Lower-Fidelity Prototypes Lower- to Medium-Fidelity Prototypes\n\nPrototyping with Generative AI Rapid Generative AI Prototypes Regenerative Testing Acknowledgments Chapter 6: Building Blocks",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "Components or Building Blocks\n\nBuilding Blocks in the Prompt Box\n\nBreakfast as Data Set Variation\n\nExperimenting with Seeds for More Subtle Variations\n\nAddition and Subtraction Substitution Masking to Substitute Parts of an Image Iteration Augmentation Diminution Transposition Prompt and Response Acknowledgments\n\nChapter 7: Generative AI Form and Composition\n\nCombining and Manipulating Existing Forms: Shakespeare as a Data set Deforming and Transforming Acknowledgments\n\nChapter 8: The Art of the Prompt\n\nThe Ins and Outs A Mini Glossary of Prompting\n\nThe Not-So-Basic Rules of Play\n\nGenres and Styles Im(promptu) Experiment 1: Same Prompt, Different AI\n\nExperiment 1A: Stable Diffusion V.2.1 Experiment 1B: Stable Diffusion V.1.5 Through Third-Party AI Experiment 1C: Stable Diffusion V.2.1 Through Third-Party AI Experiment 1D: DALL-E 2 Experiment 1E: Midjourney Experiment 1F Takeaways for Creatives\n\nExperiment 2: From Complex to Simple\n\nExperiment 2A: Eliminate Artists from Prompts",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "Experiment 2B: Substitute Words Experiment 2C: Add Words; Take Away Words Experiment 2D: Add Context Experiment 2E: Emphasis, Weight, Simplicity Experiment 2F: Refining the Text Prompt (Again)\n\nNarrative Prompting in Our Future\n\nExperiment 3: Comparing Descriptive and Narrative Prompts\n\nTricking the AI Through Creative Prompting Beyond the Text-Based Prompt Takeaways from Experimenting with Prompts Acknowledgments\n\nChapter 9: The Master of Mashup A Mashup on Mashup Digital Art to the Canvas New Forms of Writing AI Art Generation Styles Graphic Novels and Doujinshi\n\nFrom Comic Supervillains to Musical\n\nGenerative NFTs\n\nCounterfeiting GAN Takeaways for Creatives Acknowledgments\n\nChapter 10: Uncanny by Nature\n\nThe Not-Quite-Human-Not-Quite-Other Disrupting Boundaries in the Arts\n\nUncanny AI Historical Precedence to the Uncanny\n\nThe Imperfect Stochastic Parrot The Role of Adversarial AI Generative Adversarial Art The Unanticipated Prompting Single-Word Characters, Creatures, Animals, or Cats Simulating the Human in the Writing\n\nFrom Bad to Better Ideas\n\nCan Bad Ideas Be Turned Around?",
      "content_length": 1097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "Takeaways for Creatives Acknowledgments\n\nChapter 11: Dilemmas Interacting with Generative AI\n\nBringing Existing Dilemmas to the Surface (Again) Persistent Myths About AI\n\nTakeaways\n\nBad Bots, Fuzzy Pixels, and Iterative Forgery\n\nTakeaways The Hallucinating Dev Takeaways\n\nThe Stochastic (Sarcastic) Parrot\n\nTakeaways\n\nExclusion of Voices and Gender Polarization The Machine Is Hallucinating NSFW and Deep Fakes FrankenAI Job (Re)placement A Saturated AI Ecosystem Ethical Futures of AI Creative Activities to Try Based on This Chapter Resources Chapter 12: Use Cases\n\nWorkflows vs. Pipelines Examples of Workflow Types Use Cases for Creatives in Education Use Case: Catching AI Untruths Use Case: Integrating ChatGPT in Critical Studies Use Case: Increasing Public Understanding of ML Referenced Papers Takeaways\n\nUse Cases for Creatives in Industries\n\nUse Case: Concept Art for Animation Use Case: AI-Generated Talking Heads Use Case: Fact-Checking Code Use Cases: Integrating Different APIs and Local Networks Community-Based Initiatives Takeaways from Using AI in the Film Industry",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "Use Case 1: Souki Mehdaoui Takeaways Use Case 2: Ollie Rankin\n\nAcknowledgments\n\nChapter 13: AI and the Future of Creative Work\n\nManaging Automated Creativity Bread and Washing Machines\n\nTwo Themes Every Creative Needs to Address Integrating AI into Your Workflows Day in the Life Steps Distinguishing Between Art and Design Processes Takeaways for Creatives Future AI Jobs Now\n\nAcknowledgments Impossibly Generated Conclusions\n\nAppendix: Image and Text Sources\n\nWalk Through Figure 2, FrankenAI Lab\n\nIndex\n\nOceanofPDF.com",
      "content_length": 521,
      "extraction_method": "Unstructured"
    }
  ]
}